[
    {
        "Model":"ShiningValiant",
        "URL":"https:\/\/huggingface.co\/ValiantLabs\/ShiningValiant",
        "full_model_name":"ValiantLabs\/ShiningValiant",
        "Parameters":null,
        "MMLU_average":0.7096585255,
        "arc:challenge|25":0.6860068259,
        "hellaswag|10":0.6943835889,
        "MMLU_abstract_algebra":0.38,
        "MMLU_anatomy":0.637037037,
        "MMLU_astronomy":0.8092105263,
        "MMLU_business_ethics":0.75,
        "MMLU_clinical_knowledge":0.7433962264,
        "MMLU_college_biology":0.8194444444,
        "MMLU_college_chemistry":0.48,
        "MMLU_college_computer_science":0.59,
        "MMLU_college_mathematics":0.4,
        "MMLU_college_medicine":0.6878612717,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.78,
        "MMLU_conceptual_physics":0.7021276596,
        "MMLU_econometrics":0.4649122807,
        "MMLU_electrical_engineering":0.6551724138,
        "MMLU_elementary_mathematics":0.4761904762,
        "MMLU_formal_logic":0.5,
        "MMLU_global_facts":0.48,
        "MMLU_high_school_biology":0.8129032258,
        "MMLU_high_school_chemistry":0.5615763547,
        "MMLU_high_school_computer_science":0.78,
        "MMLU_high_school_european_history":0.8363636364,
        "MMLU_high_school_geography":0.904040404,
        "MMLU_high_school_government_and_politics":0.9430051813,
        "MMLU_high_school_macroeconomics":0.7102564103,
        "MMLU_high_school_mathematics":0.3296296296,
        "MMLU_high_school_microeconomics":0.768907563,
        "MMLU_high_school_physics":0.4834437086,
        "MMLU_high_school_psychology":0.904587156,
        "MMLU_high_school_statistics":0.6064814815,
        "MMLU_high_school_us_history":0.9264705882,
        "MMLU_high_school_world_history":0.8860759494,
        "MMLU_human_aging":0.7892376682,
        "MMLU_human_sexuality":0.8473282443,
        "MMLU_international_law":0.867768595,
        "MMLU_jurisprudence":0.8425925926,
        "MMLU_logical_fallacies":0.8404907975,
        "MMLU_machine_learning":0.5535714286,
        "MMLU_management":0.8446601942,
        "MMLU_marketing":0.9230769231,
        "MMLU_medical_genetics":0.72,
        "MMLU_miscellaneous":0.8684546616,
        "MMLU_moral_disputes":0.7774566474,
        "MMLU_moral_scenarios":0.6446927374,
        "MMLU_nutrition":0.7549019608,
        "MMLU_philosophy":0.7781350482,
        "MMLU_prehistory":0.8333333333,
        "MMLU_professional_accounting":0.5957446809,
        "MMLU_professional_law":0.5880052151,
        "MMLU_professional_medicine":0.7389705882,
        "MMLU_professional_psychology":0.7663398693,
        "MMLU_public_relations":0.7454545455,
        "MMLU_security_studies":0.7959183673,
        "MMLU_sociology":0.8756218905,
        "MMLU_us_foreign_policy":0.87,
        "MMLU_virology":0.5240963855,
        "MMLU_world_religions":0.8830409357
    },
    {
        "Model":"sheep-duck-llama-2-70b-v1.1",
        "URL":"https:\/\/huggingface.co\/Riiid\/sheep-duck-llama-2-70b-v1.1",
        "full_model_name":"Riiid\/sheep-duck-llama-2-70b-v1.1",
        "Parameters":70.0,
        "MMLU_average":0.7084075843,
        "arc:challenge|25":0.6877133106,
        "hellaswag|10":0.691495718,
        "MMLU_abstract_algebra":0.38,
        "MMLU_anatomy":0.6296296296,
        "MMLU_astronomy":0.8092105263,
        "MMLU_business_ethics":0.75,
        "MMLU_clinical_knowledge":0.7509433962,
        "MMLU_college_biology":0.8194444444,
        "MMLU_college_chemistry":0.48,
        "MMLU_college_computer_science":0.6,
        "MMLU_college_mathematics":0.4,
        "MMLU_college_medicine":0.6820809249,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.76,
        "MMLU_conceptual_physics":0.6936170213,
        "MMLU_econometrics":0.4649122807,
        "MMLU_electrical_engineering":0.6413793103,
        "MMLU_elementary_mathematics":0.4814814815,
        "MMLU_formal_logic":0.5,
        "MMLU_global_facts":0.48,
        "MMLU_high_school_biology":0.8193548387,
        "MMLU_high_school_chemistry":0.5566502463,
        "MMLU_high_school_computer_science":0.78,
        "MMLU_high_school_european_history":0.8484848485,
        "MMLU_high_school_geography":0.8939393939,
        "MMLU_high_school_government_and_politics":0.9430051813,
        "MMLU_high_school_macroeconomics":0.7128205128,
        "MMLU_high_school_mathematics":0.3407407407,
        "MMLU_high_school_microeconomics":0.768907563,
        "MMLU_high_school_physics":0.4834437086,
        "MMLU_high_school_psychology":0.9009174312,
        "MMLU_high_school_statistics":0.6018518519,
        "MMLU_high_school_us_history":0.9264705882,
        "MMLU_high_school_world_history":0.8987341772,
        "MMLU_human_aging":0.7892376682,
        "MMLU_human_sexuality":0.8473282443,
        "MMLU_international_law":0.867768595,
        "MMLU_jurisprudence":0.8333333333,
        "MMLU_logical_fallacies":0.8282208589,
        "MMLU_machine_learning":0.5535714286,
        "MMLU_management":0.8446601942,
        "MMLU_marketing":0.9188034188,
        "MMLU_medical_genetics":0.72,
        "MMLU_miscellaneous":0.8659003831,
        "MMLU_moral_disputes":0.774566474,
        "MMLU_moral_scenarios":0.6368715084,
        "MMLU_nutrition":0.7516339869,
        "MMLU_philosophy":0.7749196141,
        "MMLU_prehistory":0.8333333333,
        "MMLU_professional_accounting":0.5780141844,
        "MMLU_professional_law":0.5899608866,
        "MMLU_professional_medicine":0.7426470588,
        "MMLU_professional_psychology":0.7696078431,
        "MMLU_public_relations":0.7454545455,
        "MMLU_security_studies":0.7959183673,
        "MMLU_sociology":0.8756218905,
        "MMLU_us_foreign_policy":0.87,
        "MMLU_virology":0.5240963855,
        "MMLU_world_religions":0.8771929825
    },
    {
        "Model":"LLaMA_2_70B_LoRA",
        "URL":"https:\/\/huggingface.co\/adonlee\/LLaMA_2_70B_LoRA",
        "full_model_name":"adonlee\/LLaMA_2_70B_LoRA",
        "Parameters":70.0,
        "MMLU_average":0.7083507034,
        "arc:challenge|25":0.6902730375,
        "hellaswag|10":0.688607847,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.637037037,
        "MMLU_astronomy":0.8223684211,
        "MMLU_business_ethics":0.76,
        "MMLU_clinical_knowledge":0.7358490566,
        "MMLU_college_biology":0.8263888889,
        "MMLU_college_chemistry":0.49,
        "MMLU_college_computer_science":0.56,
        "MMLU_college_mathematics":0.41,
        "MMLU_college_medicine":0.6936416185,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.78,
        "MMLU_conceptual_physics":0.7106382979,
        "MMLU_econometrics":0.4561403509,
        "MMLU_electrical_engineering":0.6206896552,
        "MMLU_elementary_mathematics":0.4761904762,
        "MMLU_formal_logic":0.5079365079,
        "MMLU_global_facts":0.47,
        "MMLU_high_school_biology":0.8096774194,
        "MMLU_high_school_chemistry":0.5714285714,
        "MMLU_high_school_computer_science":0.78,
        "MMLU_high_school_european_history":0.8545454545,
        "MMLU_high_school_geography":0.898989899,
        "MMLU_high_school_government_and_politics":0.932642487,
        "MMLU_high_school_macroeconomics":0.7102564103,
        "MMLU_high_school_mathematics":0.337037037,
        "MMLU_high_school_microeconomics":0.781512605,
        "MMLU_high_school_physics":0.4900662252,
        "MMLU_high_school_psychology":0.9009174312,
        "MMLU_high_school_statistics":0.5833333333,
        "MMLU_high_school_us_history":0.9019607843,
        "MMLU_high_school_world_history":0.8818565401,
        "MMLU_human_aging":0.7847533632,
        "MMLU_human_sexuality":0.8473282443,
        "MMLU_international_law":0.8760330579,
        "MMLU_jurisprudence":0.8425925926,
        "MMLU_logical_fallacies":0.8466257669,
        "MMLU_machine_learning":0.5714285714,
        "MMLU_management":0.8252427184,
        "MMLU_marketing":0.9145299145,
        "MMLU_medical_genetics":0.75,
        "MMLU_miscellaneous":0.8697318008,
        "MMLU_moral_disputes":0.7687861272,
        "MMLU_moral_scenarios":0.6469273743,
        "MMLU_nutrition":0.7516339869,
        "MMLU_philosophy":0.768488746,
        "MMLU_prehistory":0.8271604938,
        "MMLU_professional_accounting":0.5992907801,
        "MMLU_professional_law":0.5814863103,
        "MMLU_professional_medicine":0.7316176471,
        "MMLU_professional_psychology":0.7679738562,
        "MMLU_public_relations":0.7454545455,
        "MMLU_security_studies":0.8081632653,
        "MMLU_sociology":0.8756218905,
        "MMLU_us_foreign_policy":0.86,
        "MMLU_virology":0.5301204819,
        "MMLU_world_religions":0.8713450292
    },
    {
        "Model":"sheep-duck-llama-2",
        "URL":"https:\/\/huggingface.co\/Riiid\/sheep-duck-llama-2",
        "full_model_name":"Riiid\/sheep-duck-llama-2",
        "Parameters":null,
        "MMLU_average":0.7081660999,
        "arc:challenge|25":0.6843003413,
        "hellaswag|10":0.6925911173,
        "MMLU_abstract_algebra":0.39,
        "MMLU_anatomy":0.6296296296,
        "MMLU_astronomy":0.8026315789,
        "MMLU_business_ethics":0.74,
        "MMLU_clinical_knowledge":0.7509433962,
        "MMLU_college_biology":0.8194444444,
        "MMLU_college_chemistry":0.47,
        "MMLU_college_computer_science":0.57,
        "MMLU_college_mathematics":0.4,
        "MMLU_college_medicine":0.6705202312,
        "MMLU_college_physics":0.3823529412,
        "MMLU_computer_security":0.77,
        "MMLU_conceptual_physics":0.6808510638,
        "MMLU_econometrics":0.4649122807,
        "MMLU_electrical_engineering":0.6482758621,
        "MMLU_elementary_mathematics":0.4761904762,
        "MMLU_formal_logic":0.5238095238,
        "MMLU_global_facts":0.48,
        "MMLU_high_school_biology":0.8193548387,
        "MMLU_high_school_chemistry":0.5714285714,
        "MMLU_high_school_computer_science":0.77,
        "MMLU_high_school_european_history":0.8484848485,
        "MMLU_high_school_geography":0.8787878788,
        "MMLU_high_school_government_and_politics":0.9378238342,
        "MMLU_high_school_macroeconomics":0.7128205128,
        "MMLU_high_school_mathematics":0.3333333333,
        "MMLU_high_school_microeconomics":0.7773109244,
        "MMLU_high_school_physics":0.4768211921,
        "MMLU_high_school_psychology":0.904587156,
        "MMLU_high_school_statistics":0.6064814815,
        "MMLU_high_school_us_history":0.931372549,
        "MMLU_high_school_world_history":0.9029535865,
        "MMLU_human_aging":0.798206278,
        "MMLU_human_sexuality":0.8625954198,
        "MMLU_international_law":0.8760330579,
        "MMLU_jurisprudence":0.8333333333,
        "MMLU_logical_fallacies":0.8220858896,
        "MMLU_machine_learning":0.5535714286,
        "MMLU_management":0.8446601942,
        "MMLU_marketing":0.9145299145,
        "MMLU_medical_genetics":0.71,
        "MMLU_miscellaneous":0.87100894,
        "MMLU_moral_disputes":0.7832369942,
        "MMLU_moral_scenarios":0.6167597765,
        "MMLU_nutrition":0.7614379085,
        "MMLU_philosophy":0.7877813505,
        "MMLU_prehistory":0.8395061728,
        "MMLU_professional_accounting":0.5744680851,
        "MMLU_professional_law":0.5873533246,
        "MMLU_professional_medicine":0.7316176471,
        "MMLU_professional_psychology":0.7696078431,
        "MMLU_public_relations":0.7545454545,
        "MMLU_security_studies":0.7959183673,
        "MMLU_sociology":0.8706467662,
        "MMLU_us_foreign_policy":0.87,
        "MMLU_virology":0.5240963855,
        "MMLU_world_religions":0.8713450292
    },
    {
        "Model":"genz-70b",
        "URL":"https:\/\/huggingface.co\/budecosystem\/genz-70b",
        "full_model_name":"budecosystem\/genz-70b",
        "Parameters":70.0,
        "MMLU_average":0.7077570717,
        "arc:challenge|25":0.6697952218,
        "hellaswag|10":0.6941844254,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.6296296296,
        "MMLU_astronomy":0.8289473684,
        "MMLU_business_ethics":0.74,
        "MMLU_clinical_knowledge":0.7283018868,
        "MMLU_college_biology":0.8472222222,
        "MMLU_college_chemistry":0.5,
        "MMLU_college_computer_science":0.58,
        "MMLU_college_mathematics":0.41,
        "MMLU_college_medicine":0.676300578,
        "MMLU_college_physics":0.431372549,
        "MMLU_computer_security":0.78,
        "MMLU_conceptual_physics":0.6936170213,
        "MMLU_econometrics":0.4473684211,
        "MMLU_electrical_engineering":0.6344827586,
        "MMLU_elementary_mathematics":0.4603174603,
        "MMLU_formal_logic":0.4682539683,
        "MMLU_global_facts":0.5,
        "MMLU_high_school_biology":0.8193548387,
        "MMLU_high_school_chemistry":0.5517241379,
        "MMLU_high_school_computer_science":0.79,
        "MMLU_high_school_european_history":0.8484848485,
        "MMLU_high_school_geography":0.8787878788,
        "MMLU_high_school_government_and_politics":0.932642487,
        "MMLU_high_school_macroeconomics":0.7205128205,
        "MMLU_high_school_mathematics":0.3185185185,
        "MMLU_high_school_microeconomics":0.7773109244,
        "MMLU_high_school_physics":0.4768211921,
        "MMLU_high_school_psychology":0.8990825688,
        "MMLU_high_school_statistics":0.6111111111,
        "MMLU_high_school_us_history":0.9411764706,
        "MMLU_high_school_world_history":0.9029535865,
        "MMLU_human_aging":0.798206278,
        "MMLU_human_sexuality":0.8549618321,
        "MMLU_international_law":0.8595041322,
        "MMLU_jurisprudence":0.8240740741,
        "MMLU_logical_fallacies":0.8098159509,
        "MMLU_machine_learning":0.5357142857,
        "MMLU_management":0.8252427184,
        "MMLU_marketing":0.8931623932,
        "MMLU_medical_genetics":0.72,
        "MMLU_miscellaneous":0.8761174968,
        "MMLU_moral_disputes":0.7832369942,
        "MMLU_moral_scenarios":0.6134078212,
        "MMLU_nutrition":0.7745098039,
        "MMLU_philosophy":0.7845659164,
        "MMLU_prehistory":0.8425925926,
        "MMLU_professional_accounting":0.5815602837,
        "MMLU_professional_law":0.5560625815,
        "MMLU_professional_medicine":0.7536764706,
        "MMLU_professional_psychology":0.7761437908,
        "MMLU_public_relations":0.7363636364,
        "MMLU_security_studies":0.8040816327,
        "MMLU_sociology":0.8656716418,
        "MMLU_us_foreign_policy":0.89,
        "MMLU_virology":0.5361445783,
        "MMLU_world_religions":0.8830409357
    },
    {
        "Model":"Marcoroni-70B-v1",
        "URL":"https:\/\/huggingface.co\/AIDC-ai-business\/Marcoroni-70B-v1",
        "full_model_name":"AIDC-ai-business\/Marcoroni-70B-v1",
        "Parameters":70.0,
        "MMLU_average":0.7067318138,
        "arc:challenge|25":0.6877133106,
        "hellaswag|10":0.6876120295,
        "MMLU_abstract_algebra":0.38,
        "MMLU_anatomy":0.6444444444,
        "MMLU_astronomy":0.8157894737,
        "MMLU_business_ethics":0.75,
        "MMLU_clinical_knowledge":0.7471698113,
        "MMLU_college_biology":0.8194444444,
        "MMLU_college_chemistry":0.45,
        "MMLU_college_computer_science":0.58,
        "MMLU_college_mathematics":0.41,
        "MMLU_college_medicine":0.7052023121,
        "MMLU_college_physics":0.362745098,
        "MMLU_computer_security":0.73,
        "MMLU_conceptual_physics":0.7106382979,
        "MMLU_econometrics":0.4561403509,
        "MMLU_electrical_engineering":0.6275862069,
        "MMLU_elementary_mathematics":0.4682539683,
        "MMLU_formal_logic":0.4920634921,
        "MMLU_global_facts":0.46,
        "MMLU_high_school_biology":0.8290322581,
        "MMLU_high_school_chemistry":0.5566502463,
        "MMLU_high_school_computer_science":0.79,
        "MMLU_high_school_european_history":0.8424242424,
        "MMLU_high_school_geography":0.8888888889,
        "MMLU_high_school_government_and_politics":0.9378238342,
        "MMLU_high_school_macroeconomics":0.7307692308,
        "MMLU_high_school_mathematics":0.3148148148,
        "MMLU_high_school_microeconomics":0.7857142857,
        "MMLU_high_school_physics":0.4900662252,
        "MMLU_high_school_psychology":0.8972477064,
        "MMLU_high_school_statistics":0.5925925926,
        "MMLU_high_school_us_history":0.9215686275,
        "MMLU_high_school_world_history":0.8945147679,
        "MMLU_human_aging":0.802690583,
        "MMLU_human_sexuality":0.8549618321,
        "MMLU_international_law":0.867768595,
        "MMLU_jurisprudence":0.8333333333,
        "MMLU_logical_fallacies":0.8343558282,
        "MMLU_machine_learning":0.5892857143,
        "MMLU_management":0.8349514563,
        "MMLU_marketing":0.8974358974,
        "MMLU_medical_genetics":0.72,
        "MMLU_miscellaneous":0.8684546616,
        "MMLU_moral_disputes":0.7687861272,
        "MMLU_moral_scenarios":0.6681564246,
        "MMLU_nutrition":0.7581699346,
        "MMLU_philosophy":0.768488746,
        "MMLU_prehistory":0.8179012346,
        "MMLU_professional_accounting":0.5638297872,
        "MMLU_professional_law":0.5788787484,
        "MMLU_professional_medicine":0.7279411765,
        "MMLU_professional_psychology":0.7663398693,
        "MMLU_public_relations":0.7363636364,
        "MMLU_security_studies":0.7836734694,
        "MMLU_sociology":0.8706467662,
        "MMLU_us_foreign_policy":0.87,
        "MMLU_virology":0.5542168675,
        "MMLU_world_religions":0.865497076
    },
    {
        "Model":"airoboros-l2-70b-gpt4-m2.0",
        "URL":"https:\/\/huggingface.co\/jondurbin\/airoboros-l2-70b-gpt4-m2.0",
        "full_model_name":"jondurbin\/airoboros-l2-70b-gpt4-m2.0",
        "Parameters":70.0,
        "MMLU_average":0.7067196081,
        "arc:challenge|25":0.6535836177,
        "hellaswag|10":0.6906990639,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.6444444444,
        "MMLU_astronomy":0.8355263158,
        "MMLU_business_ethics":0.77,
        "MMLU_clinical_knowledge":0.7320754717,
        "MMLU_college_biology":0.8402777778,
        "MMLU_college_chemistry":0.51,
        "MMLU_college_computer_science":0.57,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.6647398844,
        "MMLU_college_physics":0.431372549,
        "MMLU_computer_security":0.77,
        "MMLU_conceptual_physics":0.6893617021,
        "MMLU_econometrics":0.4649122807,
        "MMLU_electrical_engineering":0.6413793103,
        "MMLU_elementary_mathematics":0.4444444444,
        "MMLU_formal_logic":0.5158730159,
        "MMLU_global_facts":0.5,
        "MMLU_high_school_biology":0.8387096774,
        "MMLU_high_school_chemistry":0.5369458128,
        "MMLU_high_school_computer_science":0.76,
        "MMLU_high_school_european_history":0.8303030303,
        "MMLU_high_school_geography":0.8888888889,
        "MMLU_high_school_government_and_politics":0.9430051813,
        "MMLU_high_school_macroeconomics":0.7358974359,
        "MMLU_high_school_mathematics":0.3185185185,
        "MMLU_high_school_microeconomics":0.781512605,
        "MMLU_high_school_physics":0.4437086093,
        "MMLU_high_school_psychology":0.895412844,
        "MMLU_high_school_statistics":0.6157407407,
        "MMLU_high_school_us_history":0.9117647059,
        "MMLU_high_school_world_history":0.9029535865,
        "MMLU_human_aging":0.7937219731,
        "MMLU_human_sexuality":0.8625954198,
        "MMLU_international_law":0.8595041322,
        "MMLU_jurisprudence":0.8333333333,
        "MMLU_logical_fallacies":0.8036809816,
        "MMLU_machine_learning":0.5089285714,
        "MMLU_management":0.8252427184,
        "MMLU_marketing":0.8974358974,
        "MMLU_medical_genetics":0.69,
        "MMLU_miscellaneous":0.87100894,
        "MMLU_moral_disputes":0.7919075145,
        "MMLU_moral_scenarios":0.5899441341,
        "MMLU_nutrition":0.7712418301,
        "MMLU_philosophy":0.7749196141,
        "MMLU_prehistory":0.8425925926,
        "MMLU_professional_accounting":0.6028368794,
        "MMLU_professional_law":0.5580182529,
        "MMLU_professional_medicine":0.7352941176,
        "MMLU_professional_psychology":0.7450980392,
        "MMLU_public_relations":0.7636363636,
        "MMLU_security_studies":0.7959183673,
        "MMLU_sociology":0.8905472637,
        "MMLU_us_foreign_policy":0.92,
        "MMLU_virology":0.5481927711,
        "MMLU_world_religions":0.8596491228
    },
    {
        "Model":"SOLAR-0-70b-16bit",
        "URL":"https:\/\/huggingface.co\/upstage\/SOLAR-0-70b-16bit",
        "full_model_name":"upstage\/SOLAR-0-70b-16bit",
        "Parameters":70.0,
        "MMLU_average":0.7057664899,
        "arc:challenge|25":0.6732081911,
        "hellaswag|10":0.6974706234,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.6518518519,
        "MMLU_astronomy":0.8421052632,
        "MMLU_business_ethics":0.74,
        "MMLU_clinical_knowledge":0.7320754717,
        "MMLU_college_biology":0.8402777778,
        "MMLU_college_chemistry":0.49,
        "MMLU_college_computer_science":0.6,
        "MMLU_college_mathematics":0.42,
        "MMLU_college_medicine":0.6589595376,
        "MMLU_college_physics":0.4215686275,
        "MMLU_computer_security":0.77,
        "MMLU_conceptual_physics":0.7063829787,
        "MMLU_econometrics":0.4649122807,
        "MMLU_electrical_engineering":0.6482758621,
        "MMLU_elementary_mathematics":0.4682539683,
        "MMLU_formal_logic":0.4682539683,
        "MMLU_global_facts":0.47,
        "MMLU_high_school_biology":0.8096774194,
        "MMLU_high_school_chemistry":0.5615763547,
        "MMLU_high_school_computer_science":0.79,
        "MMLU_high_school_european_history":0.8424242424,
        "MMLU_high_school_geography":0.8737373737,
        "MMLU_high_school_government_and_politics":0.9378238342,
        "MMLU_high_school_macroeconomics":0.7102564103,
        "MMLU_high_school_mathematics":0.3185185185,
        "MMLU_high_school_microeconomics":0.7647058824,
        "MMLU_high_school_physics":0.4701986755,
        "MMLU_high_school_psychology":0.9027522936,
        "MMLU_high_school_statistics":0.6018518519,
        "MMLU_high_school_us_history":0.9264705882,
        "MMLU_high_school_world_history":0.8945147679,
        "MMLU_human_aging":0.7937219731,
        "MMLU_human_sexuality":0.8625954198,
        "MMLU_international_law":0.8595041322,
        "MMLU_jurisprudence":0.8240740741,
        "MMLU_logical_fallacies":0.8036809816,
        "MMLU_machine_learning":0.5089285714,
        "MMLU_management":0.8252427184,
        "MMLU_marketing":0.9017094017,
        "MMLU_medical_genetics":0.71,
        "MMLU_miscellaneous":0.8684546616,
        "MMLU_moral_disputes":0.7803468208,
        "MMLU_moral_scenarios":0.6044692737,
        "MMLU_nutrition":0.7679738562,
        "MMLU_philosophy":0.7781350482,
        "MMLU_prehistory":0.8333333333,
        "MMLU_professional_accounting":0.5744680851,
        "MMLU_professional_law":0.5521512386,
        "MMLU_professional_medicine":0.7389705882,
        "MMLU_professional_psychology":0.7647058824,
        "MMLU_public_relations":0.7454545455,
        "MMLU_security_studies":0.8204081633,
        "MMLU_sociology":0.8756218905,
        "MMLU_us_foreign_policy":0.9,
        "MMLU_virology":0.5301204819,
        "MMLU_world_religions":0.8771929825
    },
    {
        "Model":"MelangeA-70b",
        "URL":"https:\/\/huggingface.co\/chargoddard\/MelangeA-70b",
        "full_model_name":"chargoddard\/MelangeA-70b",
        "Parameters":70.0,
        "MMLU_average":0.7056243242,
        "arc:challenge|25":0.6817406143,
        "hellaswag|10":0.6904999004,
        "MMLU_abstract_algebra":0.39,
        "MMLU_anatomy":0.6148148148,
        "MMLU_astronomy":0.7894736842,
        "MMLU_business_ethics":0.74,
        "MMLU_clinical_knowledge":0.7358490566,
        "MMLU_college_biology":0.8333333333,
        "MMLU_college_chemistry":0.48,
        "MMLU_college_computer_science":0.58,
        "MMLU_college_mathematics":0.42,
        "MMLU_college_medicine":0.6705202312,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.72,
        "MMLU_conceptual_physics":0.6936170213,
        "MMLU_econometrics":0.5087719298,
        "MMLU_electrical_engineering":0.6620689655,
        "MMLU_elementary_mathematics":0.462962963,
        "MMLU_formal_logic":0.5396825397,
        "MMLU_global_facts":0.51,
        "MMLU_high_school_biology":0.8193548387,
        "MMLU_high_school_chemistry":0.5665024631,
        "MMLU_high_school_computer_science":0.78,
        "MMLU_high_school_european_history":0.8424242424,
        "MMLU_high_school_geography":0.8636363636,
        "MMLU_high_school_government_and_politics":0.9378238342,
        "MMLU_high_school_macroeconomics":0.7128205128,
        "MMLU_high_school_mathematics":0.337037037,
        "MMLU_high_school_microeconomics":0.781512605,
        "MMLU_high_school_physics":0.4768211921,
        "MMLU_high_school_psychology":0.9009174312,
        "MMLU_high_school_statistics":0.5740740741,
        "MMLU_high_school_us_history":0.9068627451,
        "MMLU_high_school_world_history":0.9071729958,
        "MMLU_human_aging":0.7757847534,
        "MMLU_human_sexuality":0.8473282443,
        "MMLU_international_law":0.867768595,
        "MMLU_jurisprudence":0.7962962963,
        "MMLU_logical_fallacies":0.8036809816,
        "MMLU_machine_learning":0.5714285714,
        "MMLU_management":0.8349514563,
        "MMLU_marketing":0.8974358974,
        "MMLU_medical_genetics":0.72,
        "MMLU_miscellaneous":0.8659003831,
        "MMLU_moral_disputes":0.789017341,
        "MMLU_moral_scenarios":0.6134078212,
        "MMLU_nutrition":0.7745098039,
        "MMLU_philosophy":0.7749196141,
        "MMLU_prehistory":0.8148148148,
        "MMLU_professional_accounting":0.5531914894,
        "MMLU_professional_law":0.6010430248,
        "MMLU_professional_medicine":0.7352941176,
        "MMLU_professional_psychology":0.7565359477,
        "MMLU_public_relations":0.7454545455,
        "MMLU_security_studies":0.787755102,
        "MMLU_sociology":0.8756218905,
        "MMLU_us_foreign_policy":0.88,
        "MMLU_virology":0.5481927711,
        "MMLU_world_religions":0.8596491228
    },
    {
        "Model":"falcon-180B",
        "URL":"https:\/\/huggingface.co\/tiiuae\/falcon-180B",
        "full_model_name":"tiiuae\/falcon-180B",
        "Parameters":180.0,
        "MMLU_average":0.7054468833,
        "arc:challenge|25":0.6510238908,
        "hellaswag|10":0.7065325632,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.6518518519,
        "MMLU_astronomy":0.7894736842,
        "MMLU_business_ethics":0.73,
        "MMLU_clinical_knowledge":0.7245283019,
        "MMLU_college_biology":0.8194444444,
        "MMLU_college_chemistry":0.49,
        "MMLU_college_computer_science":0.58,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.6820809249,
        "MMLU_college_physics":0.3823529412,
        "MMLU_computer_security":0.8,
        "MMLU_conceptual_physics":0.6638297872,
        "MMLU_econometrics":0.4385964912,
        "MMLU_electrical_engineering":0.6344827586,
        "MMLU_elementary_mathematics":0.4894179894,
        "MMLU_formal_logic":0.4761904762,
        "MMLU_global_facts":0.49,
        "MMLU_high_school_biology":0.8612903226,
        "MMLU_high_school_chemistry":0.5911330049,
        "MMLU_high_school_computer_science":0.73,
        "MMLU_high_school_european_history":0.8242424242,
        "MMLU_high_school_geography":0.8636363636,
        "MMLU_high_school_government_and_politics":0.9533678756,
        "MMLU_high_school_macroeconomics":0.7076923077,
        "MMLU_high_school_mathematics":0.3888888889,
        "MMLU_high_school_microeconomics":0.7773109244,
        "MMLU_high_school_physics":0.4304635762,
        "MMLU_high_school_psychology":0.9064220183,
        "MMLU_high_school_statistics":0.6203703704,
        "MMLU_high_school_us_history":0.8970588235,
        "MMLU_high_school_world_history":0.8987341772,
        "MMLU_human_aging":0.8340807175,
        "MMLU_human_sexuality":0.9007633588,
        "MMLU_international_law":0.8347107438,
        "MMLU_jurisprudence":0.8425925926,
        "MMLU_logical_fallacies":0.8036809816,
        "MMLU_machine_learning":0.5625,
        "MMLU_management":0.8446601942,
        "MMLU_marketing":0.9102564103,
        "MMLU_medical_genetics":0.78,
        "MMLU_miscellaneous":0.8761174968,
        "MMLU_moral_disputes":0.8005780347,
        "MMLU_moral_scenarios":0.5329608939,
        "MMLU_nutrition":0.7875816993,
        "MMLU_philosophy":0.7877813505,
        "MMLU_prehistory":0.8117283951,
        "MMLU_professional_accounting":0.5709219858,
        "MMLU_professional_law":0.5436766623,
        "MMLU_professional_medicine":0.7536764706,
        "MMLU_professional_psychology":0.7614379085,
        "MMLU_public_relations":0.7363636364,
        "MMLU_security_studies":0.7836734694,
        "MMLU_sociology":0.8955223881,
        "MMLU_us_foreign_policy":0.92,
        "MMLU_virology":0.5602409639,
        "MMLU_world_religions":0.8421052632
    },
    {
        "Model":"quantumairk-llama-2-70B-instruct",
        "URL":"https:\/\/huggingface.co\/quantumaikr\/quantumairk-llama-2-70B-instruct",
        "full_model_name":"quantumaikr\/quantumairk-llama-2-70B-instruct",
        "Parameters":70.0,
        "MMLU_average":0.7046276417,
        "arc:challenge|25":0.662116041,
        "hellaswag|10":0.6787492531,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.6148148148,
        "MMLU_astronomy":0.7960526316,
        "MMLU_business_ethics":0.71,
        "MMLU_clinical_knowledge":0.758490566,
        "MMLU_college_biology":0.8402777778,
        "MMLU_college_chemistry":0.5,
        "MMLU_college_computer_science":0.54,
        "MMLU_college_mathematics":0.42,
        "MMLU_college_medicine":0.6936416185,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.78,
        "MMLU_conceptual_physics":0.6638297872,
        "MMLU_econometrics":0.4736842105,
        "MMLU_electrical_engineering":0.5862068966,
        "MMLU_elementary_mathematics":0.462962963,
        "MMLU_formal_logic":0.5396825397,
        "MMLU_global_facts":0.48,
        "MMLU_high_school_biology":0.8258064516,
        "MMLU_high_school_chemistry":0.5665024631,
        "MMLU_high_school_computer_science":0.77,
        "MMLU_high_school_european_history":0.8787878788,
        "MMLU_high_school_geography":0.8636363636,
        "MMLU_high_school_government_and_politics":0.9481865285,
        "MMLU_high_school_macroeconomics":0.7205128205,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.7773109244,
        "MMLU_high_school_physics":0.5099337748,
        "MMLU_high_school_psychology":0.9009174312,
        "MMLU_high_school_statistics":0.625,
        "MMLU_high_school_us_history":0.9215686275,
        "MMLU_high_school_world_history":0.9029535865,
        "MMLU_human_aging":0.802690583,
        "MMLU_human_sexuality":0.8091603053,
        "MMLU_international_law":0.8842975207,
        "MMLU_jurisprudence":0.8333333333,
        "MMLU_logical_fallacies":0.8159509202,
        "MMLU_machine_learning":0.6160714286,
        "MMLU_management":0.854368932,
        "MMLU_marketing":0.9188034188,
        "MMLU_medical_genetics":0.71,
        "MMLU_miscellaneous":0.87100894,
        "MMLU_moral_disputes":0.7687861272,
        "MMLU_moral_scenarios":0.5988826816,
        "MMLU_nutrition":0.7647058824,
        "MMLU_philosophy":0.768488746,
        "MMLU_prehistory":0.8302469136,
        "MMLU_professional_accounting":0.5886524823,
        "MMLU_professional_law":0.5840938722,
        "MMLU_professional_medicine":0.7132352941,
        "MMLU_professional_psychology":0.7532679739,
        "MMLU_public_relations":0.7272727273,
        "MMLU_security_studies":0.7673469388,
        "MMLU_sociology":0.8706467662,
        "MMLU_us_foreign_policy":0.87,
        "MMLU_virology":0.5421686747,
        "MMLU_world_religions":0.8421052632
    },
    {
        "Model":"kssht-euripedes-70b",
        "URL":"https:\/\/huggingface.co\/lloorree\/kssht-euripedes-70b",
        "full_model_name":"lloorree\/kssht-euripedes-70b",
        "Parameters":70.0,
        "MMLU_average":0.7043409954,
        "arc:challenge|25":0.6587030717,
        "hellaswag|10":0.6872137024,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.6592592593,
        "MMLU_astronomy":0.8092105263,
        "MMLU_business_ethics":0.77,
        "MMLU_clinical_knowledge":0.720754717,
        "MMLU_college_biology":0.8333333333,
        "MMLU_college_chemistry":0.48,
        "MMLU_college_computer_science":0.57,
        "MMLU_college_mathematics":0.4,
        "MMLU_college_medicine":0.6531791908,
        "MMLU_college_physics":0.3921568627,
        "MMLU_computer_security":0.77,
        "MMLU_conceptual_physics":0.6723404255,
        "MMLU_econometrics":0.4649122807,
        "MMLU_electrical_engineering":0.6620689655,
        "MMLU_elementary_mathematics":0.4444444444,
        "MMLU_formal_logic":0.4920634921,
        "MMLU_global_facts":0.53,
        "MMLU_high_school_biology":0.8129032258,
        "MMLU_high_school_chemistry":0.5566502463,
        "MMLU_high_school_computer_science":0.76,
        "MMLU_high_school_european_history":0.8363636364,
        "MMLU_high_school_geography":0.8686868687,
        "MMLU_high_school_government_and_politics":0.9222797927,
        "MMLU_high_school_macroeconomics":0.7230769231,
        "MMLU_high_school_mathematics":0.337037037,
        "MMLU_high_school_microeconomics":0.7605042017,
        "MMLU_high_school_physics":0.4701986755,
        "MMLU_high_school_psychology":0.8972477064,
        "MMLU_high_school_statistics":0.5972222222,
        "MMLU_high_school_us_history":0.9264705882,
        "MMLU_high_school_world_history":0.8902953586,
        "MMLU_human_aging":0.7937219731,
        "MMLU_human_sexuality":0.8778625954,
        "MMLU_international_law":0.867768595,
        "MMLU_jurisprudence":0.8518518519,
        "MMLU_logical_fallacies":0.8036809816,
        "MMLU_machine_learning":0.5446428571,
        "MMLU_management":0.8058252427,
        "MMLU_marketing":0.9017094017,
        "MMLU_medical_genetics":0.74,
        "MMLU_miscellaneous":0.8620689655,
        "MMLU_moral_disputes":0.7947976879,
        "MMLU_moral_scenarios":0.5575418994,
        "MMLU_nutrition":0.7679738562,
        "MMLU_philosophy":0.7717041801,
        "MMLU_prehistory":0.8395061728,
        "MMLU_professional_accounting":0.5531914894,
        "MMLU_professional_law":0.5580182529,
        "MMLU_professional_medicine":0.7352941176,
        "MMLU_professional_psychology":0.7614379085,
        "MMLU_public_relations":0.7545454545,
        "MMLU_security_studies":0.812244898,
        "MMLU_sociology":0.8955223881,
        "MMLU_us_foreign_policy":0.91,
        "MMLU_virology":0.5542168675,
        "MMLU_world_religions":0.8596491228
    },
    {
        "Model":"kssht-dahj-70b",
        "URL":"https:\/\/huggingface.co\/lloorree\/kssht-dahj-70b",
        "full_model_name":"lloorree\/kssht-dahj-70b",
        "Parameters":70.0,
        "MMLU_average":0.7043298967,
        "arc:challenge|25":0.6612627986,
        "hellaswag|10":0.6867157937,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.6592592593,
        "MMLU_astronomy":0.8289473684,
        "MMLU_business_ethics":0.76,
        "MMLU_clinical_knowledge":0.7283018868,
        "MMLU_college_biology":0.8194444444,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.54,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.6705202312,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.76,
        "MMLU_conceptual_physics":0.7021276596,
        "MMLU_econometrics":0.4824561404,
        "MMLU_electrical_engineering":0.6413793103,
        "MMLU_elementary_mathematics":0.4470899471,
        "MMLU_formal_logic":0.5079365079,
        "MMLU_global_facts":0.47,
        "MMLU_high_school_biology":0.8193548387,
        "MMLU_high_school_chemistry":0.5418719212,
        "MMLU_high_school_computer_science":0.81,
        "MMLU_high_school_european_history":0.8424242424,
        "MMLU_high_school_geography":0.8636363636,
        "MMLU_high_school_government_and_politics":0.932642487,
        "MMLU_high_school_macroeconomics":0.7282051282,
        "MMLU_high_school_mathematics":0.3592592593,
        "MMLU_high_school_microeconomics":0.781512605,
        "MMLU_high_school_physics":0.4635761589,
        "MMLU_high_school_psychology":0.9082568807,
        "MMLU_high_school_statistics":0.5833333333,
        "MMLU_high_school_us_history":0.9215686275,
        "MMLU_high_school_world_history":0.8987341772,
        "MMLU_human_aging":0.798206278,
        "MMLU_human_sexuality":0.8702290076,
        "MMLU_international_law":0.8760330579,
        "MMLU_jurisprudence":0.8240740741,
        "MMLU_logical_fallacies":0.8159509202,
        "MMLU_machine_learning":0.5089285714,
        "MMLU_management":0.8155339806,
        "MMLU_marketing":0.8931623932,
        "MMLU_medical_genetics":0.72,
        "MMLU_miscellaneous":0.8735632184,
        "MMLU_moral_disputes":0.7832369942,
        "MMLU_moral_scenarios":0.6458100559,
        "MMLU_nutrition":0.7679738562,
        "MMLU_philosophy":0.7749196141,
        "MMLU_prehistory":0.8240740741,
        "MMLU_professional_accounting":0.5744680851,
        "MMLU_professional_law":0.5612777053,
        "MMLU_professional_medicine":0.7389705882,
        "MMLU_professional_psychology":0.7696078431,
        "MMLU_public_relations":0.7090909091,
        "MMLU_security_studies":0.812244898,
        "MMLU_sociology":0.8955223881,
        "MMLU_us_foreign_policy":0.9,
        "MMLU_virology":0.5481927711,
        "MMLU_world_religions":0.8713450292
    },
    {
        "Model":"airoboros-l2-70b-gpt4-2.0",
        "URL":"https:\/\/huggingface.co\/jondurbin\/airoboros-l2-70b-gpt4-2.0",
        "full_model_name":"jondurbin\/airoboros-l2-70b-gpt4-2.0",
        "Parameters":70.0,
        "MMLU_average":0.7040547417,
        "arc:challenge|25":0.638225256,
        "hellaswag|10":0.6891057558,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.6222222222,
        "MMLU_astronomy":0.8289473684,
        "MMLU_business_ethics":0.77,
        "MMLU_clinical_knowledge":0.7320754717,
        "MMLU_college_biology":0.8402777778,
        "MMLU_college_chemistry":0.54,
        "MMLU_college_computer_science":0.56,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.6820809249,
        "MMLU_college_physics":0.3823529412,
        "MMLU_computer_security":0.78,
        "MMLU_conceptual_physics":0.6936170213,
        "MMLU_econometrics":0.4298245614,
        "MMLU_electrical_engineering":0.6482758621,
        "MMLU_elementary_mathematics":0.4312169312,
        "MMLU_formal_logic":0.5,
        "MMLU_global_facts":0.5,
        "MMLU_high_school_biology":0.8096774194,
        "MMLU_high_school_chemistry":0.5024630542,
        "MMLU_high_school_computer_science":0.78,
        "MMLU_high_school_european_history":0.8363636364,
        "MMLU_high_school_geography":0.8939393939,
        "MMLU_high_school_government_and_politics":0.9430051813,
        "MMLU_high_school_macroeconomics":0.7153846154,
        "MMLU_high_school_mathematics":0.3407407407,
        "MMLU_high_school_microeconomics":0.7857142857,
        "MMLU_high_school_physics":0.4370860927,
        "MMLU_high_school_psychology":0.8972477064,
        "MMLU_high_school_statistics":0.6296296296,
        "MMLU_high_school_us_history":0.9117647059,
        "MMLU_high_school_world_history":0.8902953586,
        "MMLU_human_aging":0.7847533632,
        "MMLU_human_sexuality":0.8778625954,
        "MMLU_international_law":0.8595041322,
        "MMLU_jurisprudence":0.8425925926,
        "MMLU_logical_fallacies":0.8343558282,
        "MMLU_machine_learning":0.5089285714,
        "MMLU_management":0.8349514563,
        "MMLU_marketing":0.9017094017,
        "MMLU_medical_genetics":0.7,
        "MMLU_miscellaneous":0.8633461047,
        "MMLU_moral_disputes":0.7919075145,
        "MMLU_moral_scenarios":0.5687150838,
        "MMLU_nutrition":0.7712418301,
        "MMLU_philosophy":0.7813504823,
        "MMLU_prehistory":0.8425925926,
        "MMLU_professional_accounting":0.5744680851,
        "MMLU_professional_law":0.5482398957,
        "MMLU_professional_medicine":0.7389705882,
        "MMLU_professional_psychology":0.7516339869,
        "MMLU_public_relations":0.7272727273,
        "MMLU_security_studies":0.8040816327,
        "MMLU_sociology":0.8805970149,
        "MMLU_us_foreign_policy":0.92,
        "MMLU_virology":0.5481927711,
        "MMLU_world_religions":0.8596491228
    },
    {
        "Model":"kssht-castor-70b",
        "URL":"https:\/\/huggingface.co\/lloorree\/kssht-castor-70b",
        "full_model_name":"lloorree\/kssht-castor-70b",
        "Parameters":70.0,
        "MMLU_average":0.7037777006,
        "arc:challenge|25":0.6501706485,
        "hellaswag|10":0.6857199761,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.6518518519,
        "MMLU_astronomy":0.8486842105,
        "MMLU_business_ethics":0.75,
        "MMLU_clinical_knowledge":0.720754717,
        "MMLU_college_biology":0.8472222222,
        "MMLU_college_chemistry":0.5,
        "MMLU_college_computer_science":0.56,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.6473988439,
        "MMLU_college_physics":0.3529411765,
        "MMLU_computer_security":0.78,
        "MMLU_conceptual_physics":0.6978723404,
        "MMLU_econometrics":0.4649122807,
        "MMLU_electrical_engineering":0.6551724138,
        "MMLU_elementary_mathematics":0.4365079365,
        "MMLU_formal_logic":0.4841269841,
        "MMLU_global_facts":0.48,
        "MMLU_high_school_biology":0.8193548387,
        "MMLU_high_school_chemistry":0.5566502463,
        "MMLU_high_school_computer_science":0.77,
        "MMLU_high_school_european_history":0.8303030303,
        "MMLU_high_school_geography":0.8737373737,
        "MMLU_high_school_government_and_politics":0.9430051813,
        "MMLU_high_school_macroeconomics":0.7128205128,
        "MMLU_high_school_mathematics":0.3407407407,
        "MMLU_high_school_microeconomics":0.7647058824,
        "MMLU_high_school_physics":0.4701986755,
        "MMLU_high_school_psychology":0.9027522936,
        "MMLU_high_school_statistics":0.6064814815,
        "MMLU_high_school_us_history":0.9264705882,
        "MMLU_high_school_world_history":0.8860759494,
        "MMLU_human_aging":0.7937219731,
        "MMLU_human_sexuality":0.8778625954,
        "MMLU_international_law":0.8760330579,
        "MMLU_jurisprudence":0.8425925926,
        "MMLU_logical_fallacies":0.8159509202,
        "MMLU_machine_learning":0.5089285714,
        "MMLU_management":0.8155339806,
        "MMLU_marketing":0.905982906,
        "MMLU_medical_genetics":0.72,
        "MMLU_miscellaneous":0.8684546616,
        "MMLU_moral_disputes":0.7919075145,
        "MMLU_moral_scenarios":0.5463687151,
        "MMLU_nutrition":0.7777777778,
        "MMLU_philosophy":0.7717041801,
        "MMLU_prehistory":0.8425925926,
        "MMLU_professional_accounting":0.5638297872,
        "MMLU_professional_law":0.55410691,
        "MMLU_professional_medicine":0.7426470588,
        "MMLU_professional_psychology":0.7679738562,
        "MMLU_public_relations":0.7181818182,
        "MMLU_security_studies":0.8163265306,
        "MMLU_sociology":0.8905472637,
        "MMLU_us_foreign_policy":0.91,
        "MMLU_virology":0.5542168675,
        "MMLU_world_religions":0.8713450292
    },
    {
        "Model":"Synthia-70B-v1.1",
        "URL":"https:\/\/huggingface.co\/migtissera\/Synthia-70B-v1.1",
        "full_model_name":"migtissera\/Synthia-70B-v1.1",
        "Parameters":70.0,
        "MMLU_average":0.7033770426,
        "arc:challenge|25":0.6441979522,
        "hellaswag|10":0.680442143,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.6518518519,
        "MMLU_astronomy":0.8355263158,
        "MMLU_business_ethics":0.75,
        "MMLU_clinical_knowledge":0.7245283019,
        "MMLU_college_biology":0.8263888889,
        "MMLU_college_chemistry":0.53,
        "MMLU_college_computer_science":0.59,
        "MMLU_college_mathematics":0.42,
        "MMLU_college_medicine":0.6878612717,
        "MMLU_college_physics":0.4019607843,
        "MMLU_computer_security":0.77,
        "MMLU_conceptual_physics":0.6638297872,
        "MMLU_econometrics":0.4035087719,
        "MMLU_electrical_engineering":0.6620689655,
        "MMLU_elementary_mathematics":0.4338624339,
        "MMLU_formal_logic":0.5079365079,
        "MMLU_global_facts":0.5,
        "MMLU_high_school_biology":0.8225806452,
        "MMLU_high_school_chemistry":0.5467980296,
        "MMLU_high_school_computer_science":0.79,
        "MMLU_high_school_european_history":0.8242424242,
        "MMLU_high_school_geography":0.898989899,
        "MMLU_high_school_government_and_politics":0.932642487,
        "MMLU_high_school_macroeconomics":0.7179487179,
        "MMLU_high_school_mathematics":0.3148148148,
        "MMLU_high_school_microeconomics":0.7521008403,
        "MMLU_high_school_physics":0.4834437086,
        "MMLU_high_school_psychology":0.8899082569,
        "MMLU_high_school_statistics":0.6111111111,
        "MMLU_high_school_us_history":0.9166666667,
        "MMLU_high_school_world_history":0.8902953586,
        "MMLU_human_aging":0.7802690583,
        "MMLU_human_sexuality":0.8549618321,
        "MMLU_international_law":0.8760330579,
        "MMLU_jurisprudence":0.8333333333,
        "MMLU_logical_fallacies":0.8098159509,
        "MMLU_machine_learning":0.4821428571,
        "MMLU_management":0.8349514563,
        "MMLU_marketing":0.905982906,
        "MMLU_medical_genetics":0.68,
        "MMLU_miscellaneous":0.8748403576,
        "MMLU_moral_disputes":0.8005780347,
        "MMLU_moral_scenarios":0.5597765363,
        "MMLU_nutrition":0.7647058824,
        "MMLU_philosophy":0.7781350482,
        "MMLU_prehistory":0.8271604938,
        "MMLU_professional_accounting":0.5744680851,
        "MMLU_professional_law":0.5560625815,
        "MMLU_professional_medicine":0.7389705882,
        "MMLU_professional_psychology":0.7467320261,
        "MMLU_public_relations":0.7363636364,
        "MMLU_security_studies":0.8163265306,
        "MMLU_sociology":0.8805970149,
        "MMLU_us_foreign_policy":0.91,
        "MMLU_virology":0.5180722892,
        "MMLU_world_religions":0.8713450292
    },
    {
        "Model":"airoboros-l2-70b-gpt4-1.4.1",
        "URL":"https:\/\/huggingface.co\/jondurbin\/airoboros-l2-70b-gpt4-1.4.1",
        "full_model_name":"jondurbin\/airoboros-l2-70b-gpt4-1.4.1",
        "Parameters":70.0,
        "MMLU_average":0.7031210261,
        "arc:challenge|25":0.662116041,
        "hellaswag|10":0.69557857,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.6666666667,
        "MMLU_astronomy":0.8157894737,
        "MMLU_business_ethics":0.78,
        "MMLU_clinical_knowledge":0.7094339623,
        "MMLU_college_biology":0.8263888889,
        "MMLU_college_chemistry":0.49,
        "MMLU_college_computer_science":0.59,
        "MMLU_college_mathematics":0.39,
        "MMLU_college_medicine":0.676300578,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.78,
        "MMLU_conceptual_physics":0.7021276596,
        "MMLU_econometrics":0.4561403509,
        "MMLU_electrical_engineering":0.6275862069,
        "MMLU_elementary_mathematics":0.4523809524,
        "MMLU_formal_logic":0.4841269841,
        "MMLU_global_facts":0.47,
        "MMLU_high_school_biology":0.8225806452,
        "MMLU_high_school_chemistry":0.5566502463,
        "MMLU_high_school_computer_science":0.75,
        "MMLU_high_school_european_history":0.8363636364,
        "MMLU_high_school_geography":0.8686868687,
        "MMLU_high_school_government_and_politics":0.9481865285,
        "MMLU_high_school_macroeconomics":0.7205128205,
        "MMLU_high_school_mathematics":0.3296296296,
        "MMLU_high_school_microeconomics":0.756302521,
        "MMLU_high_school_physics":0.4701986755,
        "MMLU_high_school_psychology":0.8935779817,
        "MMLU_high_school_statistics":0.5694444444,
        "MMLU_high_school_us_history":0.9019607843,
        "MMLU_high_school_world_history":0.8987341772,
        "MMLU_human_aging":0.8251121076,
        "MMLU_human_sexuality":0.8702290076,
        "MMLU_international_law":0.867768595,
        "MMLU_jurisprudence":0.8148148148,
        "MMLU_logical_fallacies":0.8036809816,
        "MMLU_machine_learning":0.5089285714,
        "MMLU_management":0.8155339806,
        "MMLU_marketing":0.8974358974,
        "MMLU_medical_genetics":0.75,
        "MMLU_miscellaneous":0.8697318008,
        "MMLU_moral_disputes":0.789017341,
        "MMLU_moral_scenarios":0.574301676,
        "MMLU_nutrition":0.7745098039,
        "MMLU_philosophy":0.7845659164,
        "MMLU_prehistory":0.8148148148,
        "MMLU_professional_accounting":0.5780141844,
        "MMLU_professional_law":0.5534550196,
        "MMLU_professional_medicine":0.7132352941,
        "MMLU_professional_psychology":0.7614379085,
        "MMLU_public_relations":0.7272727273,
        "MMLU_security_studies":0.8204081633,
        "MMLU_sociology":0.8855721393,
        "MMLU_us_foreign_policy":0.92,
        "MMLU_virology":0.5602409639,
        "MMLU_world_religions":0.865497076
    },
    {
        "Model":"Genz-70b-GPTQ",
        "URL":"https:\/\/huggingface.co\/TheBloke\/Genz-70b-GPTQ",
        "full_model_name":"TheBloke\/Genz-70b-GPTQ",
        "Parameters":70.0,
        "MMLU_average":0.7026095385,
        "arc:challenge|25":0.6638225256,
        "hellaswag|10":0.6892053376,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.637037037,
        "MMLU_astronomy":0.8223684211,
        "MMLU_business_ethics":0.76,
        "MMLU_clinical_knowledge":0.720754717,
        "MMLU_college_biology":0.8194444444,
        "MMLU_college_chemistry":0.51,
        "MMLU_college_computer_science":0.58,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.6473988439,
        "MMLU_college_physics":0.4019607843,
        "MMLU_computer_security":0.77,
        "MMLU_conceptual_physics":0.6765957447,
        "MMLU_econometrics":0.4561403509,
        "MMLU_electrical_engineering":0.6344827586,
        "MMLU_elementary_mathematics":0.4417989418,
        "MMLU_formal_logic":0.4682539683,
        "MMLU_global_facts":0.49,
        "MMLU_high_school_biology":0.8290322581,
        "MMLU_high_school_chemistry":0.5270935961,
        "MMLU_high_school_computer_science":0.8,
        "MMLU_high_school_european_history":0.8545454545,
        "MMLU_high_school_geography":0.8686868687,
        "MMLU_high_school_government_and_politics":0.9378238342,
        "MMLU_high_school_macroeconomics":0.7051282051,
        "MMLU_high_school_mathematics":0.3185185185,
        "MMLU_high_school_microeconomics":0.7731092437,
        "MMLU_high_school_physics":0.5099337748,
        "MMLU_high_school_psychology":0.8917431193,
        "MMLU_high_school_statistics":0.6018518519,
        "MMLU_high_school_us_history":0.931372549,
        "MMLU_high_school_world_history":0.9029535865,
        "MMLU_human_aging":0.8116591928,
        "MMLU_human_sexuality":0.8396946565,
        "MMLU_international_law":0.8595041322,
        "MMLU_jurisprudence":0.8240740741,
        "MMLU_logical_fallacies":0.8159509202,
        "MMLU_machine_learning":0.5,
        "MMLU_management":0.8446601942,
        "MMLU_marketing":0.9102564103,
        "MMLU_medical_genetics":0.71,
        "MMLU_miscellaneous":0.8722860792,
        "MMLU_moral_disputes":0.8005780347,
        "MMLU_moral_scenarios":0.5754189944,
        "MMLU_nutrition":0.7745098039,
        "MMLU_philosophy":0.7717041801,
        "MMLU_prehistory":0.8487654321,
        "MMLU_professional_accounting":0.5638297872,
        "MMLU_professional_law":0.5534550196,
        "MMLU_professional_medicine":0.7389705882,
        "MMLU_professional_psychology":0.7630718954,
        "MMLU_public_relations":0.7363636364,
        "MMLU_security_studies":0.8,
        "MMLU_sociology":0.8606965174,
        "MMLU_us_foreign_policy":0.9,
        "MMLU_virology":0.5180722892,
        "MMLU_world_religions":0.8771929825
    },
    {
        "Model":"llama-2-70b-fb16-guanaco-1k",
        "URL":"https:\/\/huggingface.co\/quantumaikr\/llama-2-70b-fb16-guanaco-1k",
        "full_model_name":"quantumaikr\/llama-2-70b-fb16-guanaco-1k",
        "Parameters":70.0,
        "MMLU_average":0.7024958114,
        "arc:challenge|25":0.6510238908,
        "hellaswag|10":0.6860187214,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.637037037,
        "MMLU_astronomy":0.7894736842,
        "MMLU_business_ethics":0.76,
        "MMLU_clinical_knowledge":0.7320754717,
        "MMLU_college_biology":0.8402777778,
        "MMLU_college_chemistry":0.46,
        "MMLU_college_computer_science":0.6,
        "MMLU_college_mathematics":0.4,
        "MMLU_college_medicine":0.6705202312,
        "MMLU_college_physics":0.3921568627,
        "MMLU_computer_security":0.75,
        "MMLU_conceptual_physics":0.6638297872,
        "MMLU_econometrics":0.4561403509,
        "MMLU_electrical_engineering":0.6275862069,
        "MMLU_elementary_mathematics":0.4497354497,
        "MMLU_formal_logic":0.4761904762,
        "MMLU_global_facts":0.47,
        "MMLU_high_school_biology":0.835483871,
        "MMLU_high_school_chemistry":0.5320197044,
        "MMLU_high_school_computer_science":0.8,
        "MMLU_high_school_european_history":0.8242424242,
        "MMLU_high_school_geography":0.898989899,
        "MMLU_high_school_government_and_politics":0.9378238342,
        "MMLU_high_school_macroeconomics":0.7333333333,
        "MMLU_high_school_mathematics":0.3222222222,
        "MMLU_high_school_microeconomics":0.7605042017,
        "MMLU_high_school_physics":0.4569536424,
        "MMLU_high_school_psychology":0.8935779817,
        "MMLU_high_school_statistics":0.625,
        "MMLU_high_school_us_history":0.931372549,
        "MMLU_high_school_world_history":0.8860759494,
        "MMLU_human_aging":0.7847533632,
        "MMLU_human_sexuality":0.8549618321,
        "MMLU_international_law":0.867768595,
        "MMLU_jurisprudence":0.8425925926,
        "MMLU_logical_fallacies":0.7914110429,
        "MMLU_machine_learning":0.5,
        "MMLU_management":0.8349514563,
        "MMLU_marketing":0.8974358974,
        "MMLU_medical_genetics":0.7,
        "MMLU_miscellaneous":0.8646232439,
        "MMLU_moral_disputes":0.7832369942,
        "MMLU_moral_scenarios":0.5910614525,
        "MMLU_nutrition":0.7679738562,
        "MMLU_philosophy":0.7749196141,
        "MMLU_prehistory":0.8364197531,
        "MMLU_professional_accounting":0.5709219858,
        "MMLU_professional_law":0.5560625815,
        "MMLU_professional_medicine":0.7536764706,
        "MMLU_professional_psychology":0.7565359477,
        "MMLU_public_relations":0.7454545455,
        "MMLU_security_studies":0.8204081633,
        "MMLU_sociology":0.8905472637,
        "MMLU_us_foreign_policy":0.89,
        "MMLU_virology":0.5301204819,
        "MMLU_world_religions":0.8538011696
    },
    {
        "Model":"ORCA_LLaMA_70B_QLoRA",
        "URL":"https:\/\/huggingface.co\/fangloveskari\/ORCA_LLaMA_70B_QLoRA",
        "full_model_name":"fangloveskari\/ORCA_LLaMA_70B_QLoRA",
        "Parameters":70.0,
        "MMLU_average":0.702253558,
        "arc:challenge|25":0.683447099,
        "hellaswag|10":0.6881099383,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.6444444444,
        "MMLU_astronomy":0.8026315789,
        "MMLU_business_ethics":0.75,
        "MMLU_clinical_knowledge":0.7471698113,
        "MMLU_college_biology":0.8263888889,
        "MMLU_college_chemistry":0.5,
        "MMLU_college_computer_science":0.62,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.6647398844,
        "MMLU_college_physics":0.3823529412,
        "MMLU_computer_security":0.77,
        "MMLU_conceptual_physics":0.685106383,
        "MMLU_econometrics":0.4561403509,
        "MMLU_electrical_engineering":0.6413793103,
        "MMLU_elementary_mathematics":0.4788359788,
        "MMLU_formal_logic":0.5,
        "MMLU_global_facts":0.46,
        "MMLU_high_school_biology":0.8096774194,
        "MMLU_high_school_chemistry":0.5418719212,
        "MMLU_high_school_computer_science":0.78,
        "MMLU_high_school_european_history":0.8484848485,
        "MMLU_high_school_geography":0.8888888889,
        "MMLU_high_school_government_and_politics":0.932642487,
        "MMLU_high_school_macroeconomics":0.7025641026,
        "MMLU_high_school_mathematics":0.3444444444,
        "MMLU_high_school_microeconomics":0.7647058824,
        "MMLU_high_school_physics":0.4768211921,
        "MMLU_high_school_psychology":0.9027522936,
        "MMLU_high_school_statistics":0.5925925926,
        "MMLU_high_school_us_history":0.9264705882,
        "MMLU_high_school_world_history":0.8945147679,
        "MMLU_human_aging":0.7757847534,
        "MMLU_human_sexuality":0.8396946565,
        "MMLU_international_law":0.8760330579,
        "MMLU_jurisprudence":0.8240740741,
        "MMLU_logical_fallacies":0.8220858896,
        "MMLU_machine_learning":0.4821428571,
        "MMLU_management":0.8155339806,
        "MMLU_marketing":0.9017094017,
        "MMLU_medical_genetics":0.71,
        "MMLU_miscellaneous":0.8659003831,
        "MMLU_moral_disputes":0.7716763006,
        "MMLU_moral_scenarios":0.5843575419,
        "MMLU_nutrition":0.7516339869,
        "MMLU_philosophy":0.7620578778,
        "MMLU_prehistory":0.8271604938,
        "MMLU_professional_accounting":0.5673758865,
        "MMLU_professional_law":0.5710560626,
        "MMLU_professional_medicine":0.7205882353,
        "MMLU_professional_psychology":0.7614379085,
        "MMLU_public_relations":0.7363636364,
        "MMLU_security_studies":0.8,
        "MMLU_sociology":0.8905472637,
        "MMLU_us_foreign_policy":0.89,
        "MMLU_virology":0.5301204819,
        "MMLU_world_religions":0.865497076
    },
    {
        "Model":"orca_mini_v3_70b",
        "URL":"https:\/\/huggingface.co\/pankajmathur\/orca_mini_v3_70b",
        "full_model_name":"pankajmathur\/orca_mini_v3_70b",
        "Parameters":70.0,
        "MMLU_average":0.7017920055,
        "arc:challenge|25":0.6646757679,
        "hellaswag|10":0.695180243,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.6296296296,
        "MMLU_astronomy":0.8157894737,
        "MMLU_business_ethics":0.77,
        "MMLU_clinical_knowledge":0.7396226415,
        "MMLU_college_biology":0.8125,
        "MMLU_college_chemistry":0.5,
        "MMLU_college_computer_science":0.56,
        "MMLU_college_mathematics":0.39,
        "MMLU_college_medicine":0.6589595376,
        "MMLU_college_physics":0.3823529412,
        "MMLU_computer_security":0.76,
        "MMLU_conceptual_physics":0.6936170213,
        "MMLU_econometrics":0.4473684211,
        "MMLU_electrical_engineering":0.6344827586,
        "MMLU_elementary_mathematics":0.4814814815,
        "MMLU_formal_logic":0.4444444444,
        "MMLU_global_facts":0.52,
        "MMLU_high_school_biology":0.8161290323,
        "MMLU_high_school_chemistry":0.5665024631,
        "MMLU_high_school_computer_science":0.78,
        "MMLU_high_school_european_history":0.8424242424,
        "MMLU_high_school_geography":0.8737373737,
        "MMLU_high_school_government_and_politics":0.932642487,
        "MMLU_high_school_macroeconomics":0.7025641026,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.7478991597,
        "MMLU_high_school_physics":0.5033112583,
        "MMLU_high_school_psychology":0.9009174312,
        "MMLU_high_school_statistics":0.587962963,
        "MMLU_high_school_us_history":0.9264705882,
        "MMLU_high_school_world_history":0.9029535865,
        "MMLU_human_aging":0.7757847534,
        "MMLU_human_sexuality":0.8625954198,
        "MMLU_international_law":0.8760330579,
        "MMLU_jurisprudence":0.8240740741,
        "MMLU_logical_fallacies":0.8159509202,
        "MMLU_machine_learning":0.5178571429,
        "MMLU_management":0.8155339806,
        "MMLU_marketing":0.9145299145,
        "MMLU_medical_genetics":0.75,
        "MMLU_miscellaneous":0.8748403576,
        "MMLU_moral_disputes":0.7919075145,
        "MMLU_moral_scenarios":0.5575418994,
        "MMLU_nutrition":0.7418300654,
        "MMLU_philosophy":0.7749196141,
        "MMLU_prehistory":0.8209876543,
        "MMLU_professional_accounting":0.585106383,
        "MMLU_professional_law":0.5599739244,
        "MMLU_professional_medicine":0.7169117647,
        "MMLU_professional_psychology":0.7549019608,
        "MMLU_public_relations":0.7181818182,
        "MMLU_security_studies":0.8,
        "MMLU_sociology":0.8756218905,
        "MMLU_us_foreign_policy":0.91,
        "MMLU_virology":0.5240963855,
        "MMLU_world_religions":0.865497076
    },
    {
        "Model":"orca_mini_v3_70b",
        "URL":"https:\/\/huggingface.co\/psmathur\/orca_mini_v3_70b",
        "full_model_name":"psmathur\/orca_mini_v3_70b",
        "Parameters":70.0,
        "MMLU_average":0.7017920055,
        "arc:challenge|25":0.6646757679,
        "hellaswag|10":0.695180243,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.6296296296,
        "MMLU_astronomy":0.8157894737,
        "MMLU_business_ethics":0.77,
        "MMLU_clinical_knowledge":0.7396226415,
        "MMLU_college_biology":0.8125,
        "MMLU_college_chemistry":0.5,
        "MMLU_college_computer_science":0.56,
        "MMLU_college_mathematics":0.39,
        "MMLU_college_medicine":0.6589595376,
        "MMLU_college_physics":0.3823529412,
        "MMLU_computer_security":0.76,
        "MMLU_conceptual_physics":0.6936170213,
        "MMLU_econometrics":0.4473684211,
        "MMLU_electrical_engineering":0.6344827586,
        "MMLU_elementary_mathematics":0.4814814815,
        "MMLU_formal_logic":0.4444444444,
        "MMLU_global_facts":0.52,
        "MMLU_high_school_biology":0.8161290323,
        "MMLU_high_school_chemistry":0.5665024631,
        "MMLU_high_school_computer_science":0.78,
        "MMLU_high_school_european_history":0.8424242424,
        "MMLU_high_school_geography":0.8737373737,
        "MMLU_high_school_government_and_politics":0.932642487,
        "MMLU_high_school_macroeconomics":0.7025641026,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.7478991597,
        "MMLU_high_school_physics":0.5033112583,
        "MMLU_high_school_psychology":0.9009174312,
        "MMLU_high_school_statistics":0.587962963,
        "MMLU_high_school_us_history":0.9264705882,
        "MMLU_high_school_world_history":0.9029535865,
        "MMLU_human_aging":0.7757847534,
        "MMLU_human_sexuality":0.8625954198,
        "MMLU_international_law":0.8760330579,
        "MMLU_jurisprudence":0.8240740741,
        "MMLU_logical_fallacies":0.8159509202,
        "MMLU_machine_learning":0.5178571429,
        "MMLU_management":0.8155339806,
        "MMLU_marketing":0.9145299145,
        "MMLU_medical_genetics":0.75,
        "MMLU_miscellaneous":0.8748403576,
        "MMLU_moral_disputes":0.7919075145,
        "MMLU_moral_scenarios":0.5575418994,
        "MMLU_nutrition":0.7418300654,
        "MMLU_philosophy":0.7749196141,
        "MMLU_prehistory":0.8209876543,
        "MMLU_professional_accounting":0.585106383,
        "MMLU_professional_law":0.5599739244,
        "MMLU_professional_medicine":0.7169117647,
        "MMLU_professional_psychology":0.7549019608,
        "MMLU_public_relations":0.7181818182,
        "MMLU_security_studies":0.8,
        "MMLU_sociology":0.8756218905,
        "MMLU_us_foreign_policy":0.91,
        "MMLU_virology":0.5240963855,
        "MMLU_world_religions":0.865497076
    },
    {
        "Model":"qCammel-70-x",
        "URL":"https:\/\/huggingface.co\/augtoma\/qCammel-70-x",
        "full_model_name":"augtoma\/qCammel-70-x",
        "Parameters":null,
        "MMLU_average":0.7017892288,
        "arc:challenge|25":0.6407849829,
        "hellaswag|10":0.687811193,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.5925925926,
        "MMLU_astronomy":0.8421052632,
        "MMLU_business_ethics":0.75,
        "MMLU_clinical_knowledge":0.7169811321,
        "MMLU_college_biology":0.8472222222,
        "MMLU_college_chemistry":0.49,
        "MMLU_college_computer_science":0.6,
        "MMLU_college_mathematics":0.39,
        "MMLU_college_medicine":0.6705202312,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.76,
        "MMLU_conceptual_physics":0.6680851064,
        "MMLU_econometrics":0.4561403509,
        "MMLU_electrical_engineering":0.6689655172,
        "MMLU_elementary_mathematics":0.4470899471,
        "MMLU_formal_logic":0.4603174603,
        "MMLU_global_facts":0.53,
        "MMLU_high_school_biology":0.8,
        "MMLU_high_school_chemistry":0.5221674877,
        "MMLU_high_school_computer_science":0.8,
        "MMLU_high_school_european_history":0.8424242424,
        "MMLU_high_school_geography":0.8686868687,
        "MMLU_high_school_government_and_politics":0.9430051813,
        "MMLU_high_school_macroeconomics":0.7230769231,
        "MMLU_high_school_mathematics":0.3444444444,
        "MMLU_high_school_microeconomics":0.7521008403,
        "MMLU_high_school_physics":0.4768211921,
        "MMLU_high_school_psychology":0.8972477064,
        "MMLU_high_school_statistics":0.5925925926,
        "MMLU_high_school_us_history":0.9166666667,
        "MMLU_high_school_world_history":0.8945147679,
        "MMLU_human_aging":0.7802690583,
        "MMLU_human_sexuality":0.8549618321,
        "MMLU_international_law":0.8512396694,
        "MMLU_jurisprudence":0.8240740741,
        "MMLU_logical_fallacies":0.8404907975,
        "MMLU_machine_learning":0.5089285714,
        "MMLU_management":0.8252427184,
        "MMLU_marketing":0.9102564103,
        "MMLU_medical_genetics":0.69,
        "MMLU_miscellaneous":0.8646232439,
        "MMLU_moral_disputes":0.8034682081,
        "MMLU_moral_scenarios":0.5608938547,
        "MMLU_nutrition":0.7712418301,
        "MMLU_philosophy":0.7717041801,
        "MMLU_prehistory":0.8395061728,
        "MMLU_professional_accounting":0.5602836879,
        "MMLU_professional_law":0.5593220339,
        "MMLU_professional_medicine":0.7573529412,
        "MMLU_professional_psychology":0.7549019608,
        "MMLU_public_relations":0.7363636364,
        "MMLU_security_studies":0.7918367347,
        "MMLU_sociology":0.855721393,
        "MMLU_us_foreign_policy":0.9,
        "MMLU_virology":0.5240963855,
        "MMLU_world_religions":0.8888888889
    },
    {
        "Model":"qCammel70",
        "URL":"https:\/\/huggingface.co\/augtoma\/qCammel70",
        "full_model_name":"augtoma\/qCammel70",
        "Parameters":null,
        "MMLU_average":0.7017892288,
        "arc:challenge|25":0.6407849829,
        "hellaswag|10":0.687811193,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.5925925926,
        "MMLU_astronomy":0.8421052632,
        "MMLU_business_ethics":0.75,
        "MMLU_clinical_knowledge":0.7169811321,
        "MMLU_college_biology":0.8472222222,
        "MMLU_college_chemistry":0.49,
        "MMLU_college_computer_science":0.6,
        "MMLU_college_mathematics":0.39,
        "MMLU_college_medicine":0.6705202312,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.76,
        "MMLU_conceptual_physics":0.6680851064,
        "MMLU_econometrics":0.4561403509,
        "MMLU_electrical_engineering":0.6689655172,
        "MMLU_elementary_mathematics":0.4470899471,
        "MMLU_formal_logic":0.4603174603,
        "MMLU_global_facts":0.53,
        "MMLU_high_school_biology":0.8,
        "MMLU_high_school_chemistry":0.5221674877,
        "MMLU_high_school_computer_science":0.8,
        "MMLU_high_school_european_history":0.8424242424,
        "MMLU_high_school_geography":0.8686868687,
        "MMLU_high_school_government_and_politics":0.9430051813,
        "MMLU_high_school_macroeconomics":0.7230769231,
        "MMLU_high_school_mathematics":0.3444444444,
        "MMLU_high_school_microeconomics":0.7521008403,
        "MMLU_high_school_physics":0.4768211921,
        "MMLU_high_school_psychology":0.8972477064,
        "MMLU_high_school_statistics":0.5925925926,
        "MMLU_high_school_us_history":0.9166666667,
        "MMLU_high_school_world_history":0.8945147679,
        "MMLU_human_aging":0.7802690583,
        "MMLU_human_sexuality":0.8549618321,
        "MMLU_international_law":0.8512396694,
        "MMLU_jurisprudence":0.8240740741,
        "MMLU_logical_fallacies":0.8404907975,
        "MMLU_machine_learning":0.5089285714,
        "MMLU_management":0.8252427184,
        "MMLU_marketing":0.9102564103,
        "MMLU_medical_genetics":0.69,
        "MMLU_miscellaneous":0.8646232439,
        "MMLU_moral_disputes":0.8034682081,
        "MMLU_moral_scenarios":0.5608938547,
        "MMLU_nutrition":0.7712418301,
        "MMLU_philosophy":0.7717041801,
        "MMLU_prehistory":0.8395061728,
        "MMLU_professional_accounting":0.5602836879,
        "MMLU_professional_law":0.5593220339,
        "MMLU_professional_medicine":0.7573529412,
        "MMLU_professional_psychology":0.7549019608,
        "MMLU_public_relations":0.7363636364,
        "MMLU_security_studies":0.7918367347,
        "MMLU_sociology":0.855721393,
        "MMLU_us_foreign_policy":0.9,
        "MMLU_virology":0.5240963855,
        "MMLU_world_religions":0.8888888889
    },
    {
        "Model":"FashionGPT-70B-V1.2",
        "URL":"https:\/\/huggingface.co\/ICBU-NPU\/FashionGPT-70B-V1.2",
        "full_model_name":"ICBU-NPU\/FashionGPT-70B-V1.2",
        "Parameters":70.0,
        "MMLU_average":0.7011144497,
        "arc:challenge|25":0.6979522184,
        "hellaswag|10":0.6938856801,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.6074074074,
        "MMLU_astronomy":0.7828947368,
        "MMLU_business_ethics":0.75,
        "MMLU_clinical_knowledge":0.7245283019,
        "MMLU_college_biology":0.7916666667,
        "MMLU_college_chemistry":0.49,
        "MMLU_college_computer_science":0.62,
        "MMLU_college_mathematics":0.4,
        "MMLU_college_medicine":0.6647398844,
        "MMLU_college_physics":0.3921568627,
        "MMLU_computer_security":0.77,
        "MMLU_conceptual_physics":0.6808510638,
        "MMLU_econometrics":0.4649122807,
        "MMLU_electrical_engineering":0.6551724138,
        "MMLU_elementary_mathematics":0.4735449735,
        "MMLU_formal_logic":0.4841269841,
        "MMLU_global_facts":0.54,
        "MMLU_high_school_biology":0.8064516129,
        "MMLU_high_school_chemistry":0.5221674877,
        "MMLU_high_school_computer_science":0.82,
        "MMLU_high_school_european_history":0.8363636364,
        "MMLU_high_school_geography":0.8838383838,
        "MMLU_high_school_government_and_politics":0.9222797927,
        "MMLU_high_school_macroeconomics":0.6974358974,
        "MMLU_high_school_mathematics":0.3148148148,
        "MMLU_high_school_microeconomics":0.7731092437,
        "MMLU_high_school_physics":0.4569536424,
        "MMLU_high_school_psychology":0.9027522936,
        "MMLU_high_school_statistics":0.6203703704,
        "MMLU_high_school_us_history":0.9166666667,
        "MMLU_high_school_world_history":0.9071729958,
        "MMLU_human_aging":0.8071748879,
        "MMLU_human_sexuality":0.8396946565,
        "MMLU_international_law":0.826446281,
        "MMLU_jurisprudence":0.8055555556,
        "MMLU_logical_fallacies":0.8343558282,
        "MMLU_machine_learning":0.5535714286,
        "MMLU_management":0.854368932,
        "MMLU_marketing":0.9017094017,
        "MMLU_medical_genetics":0.68,
        "MMLU_miscellaneous":0.8646232439,
        "MMLU_moral_disputes":0.8034682081,
        "MMLU_moral_scenarios":0.5899441341,
        "MMLU_nutrition":0.7745098039,
        "MMLU_philosophy":0.7556270096,
        "MMLU_prehistory":0.8024691358,
        "MMLU_professional_accounting":0.585106383,
        "MMLU_professional_law":0.5580182529,
        "MMLU_professional_medicine":0.7169117647,
        "MMLU_professional_psychology":0.7549019608,
        "MMLU_public_relations":0.7181818182,
        "MMLU_security_studies":0.7959183673,
        "MMLU_sociology":0.8507462687,
        "MMLU_us_foreign_policy":0.89,
        "MMLU_virology":0.5481927711,
        "MMLU_world_religions":0.8596491228
    },
    {
        "Model":"mythospice-70b",
        "URL":"https:\/\/huggingface.co\/Doctor-Shotgun\/mythospice-70b",
        "full_model_name":"Doctor-Shotgun\/mythospice-70b",
        "Parameters":70.0,
        "MMLU_average":0.7009513874,
        "arc:challenge|25":0.656996587,
        "hellaswag|10":0.6870145389,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.6592592593,
        "MMLU_astronomy":0.8157894737,
        "MMLU_business_ethics":0.77,
        "MMLU_clinical_knowledge":0.720754717,
        "MMLU_college_biology":0.8333333333,
        "MMLU_college_chemistry":0.47,
        "MMLU_college_computer_science":0.6,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.6705202312,
        "MMLU_college_physics":0.3529411765,
        "MMLU_computer_security":0.78,
        "MMLU_conceptual_physics":0.6765957447,
        "MMLU_econometrics":0.4649122807,
        "MMLU_electrical_engineering":0.6344827586,
        "MMLU_elementary_mathematics":0.4470899471,
        "MMLU_formal_logic":0.4920634921,
        "MMLU_global_facts":0.5,
        "MMLU_high_school_biology":0.8161290323,
        "MMLU_high_school_chemistry":0.5270935961,
        "MMLU_high_school_computer_science":0.76,
        "MMLU_high_school_european_history":0.8242424242,
        "MMLU_high_school_geography":0.8686868687,
        "MMLU_high_school_government_and_politics":0.9274611399,
        "MMLU_high_school_macroeconomics":0.7128205128,
        "MMLU_high_school_mathematics":0.3111111111,
        "MMLU_high_school_microeconomics":0.7731092437,
        "MMLU_high_school_physics":0.4635761589,
        "MMLU_high_school_psychology":0.8917431193,
        "MMLU_high_school_statistics":0.5833333333,
        "MMLU_high_school_us_history":0.9264705882,
        "MMLU_high_school_world_history":0.8818565401,
        "MMLU_human_aging":0.8071748879,
        "MMLU_human_sexuality":0.8778625954,
        "MMLU_international_law":0.8760330579,
        "MMLU_jurisprudence":0.8425925926,
        "MMLU_logical_fallacies":0.8036809816,
        "MMLU_machine_learning":0.5446428571,
        "MMLU_management":0.8155339806,
        "MMLU_marketing":0.905982906,
        "MMLU_medical_genetics":0.73,
        "MMLU_miscellaneous":0.8633461047,
        "MMLU_moral_disputes":0.7919075145,
        "MMLU_moral_scenarios":0.5296089385,
        "MMLU_nutrition":0.7679738562,
        "MMLU_philosophy":0.7620578778,
        "MMLU_prehistory":0.8456790123,
        "MMLU_professional_accounting":0.5638297872,
        "MMLU_professional_law":0.5514993481,
        "MMLU_professional_medicine":0.7205882353,
        "MMLU_professional_psychology":0.7549019608,
        "MMLU_public_relations":0.7363636364,
        "MMLU_security_studies":0.8163265306,
        "MMLU_sociology":0.9054726368,
        "MMLU_us_foreign_policy":0.91,
        "MMLU_virology":0.5361445783,
        "MMLU_world_religions":0.8596491228
    },
    {
        "Model":"Platypus2-70B",
        "URL":"https:\/\/huggingface.co\/garage-bAInd\/Platypus2-70B",
        "full_model_name":"garage-bAInd\/Platypus2-70B",
        "Parameters":70.0,
        "MMLU_average":0.7008313588,
        "arc:challenge|25":0.6604095563,
        "hellaswag|10":0.6739693288,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.5925925926,
        "MMLU_astronomy":0.7697368421,
        "MMLU_business_ethics":0.72,
        "MMLU_clinical_knowledge":0.7283018868,
        "MMLU_college_biology":0.8194444444,
        "MMLU_college_chemistry":0.5,
        "MMLU_college_computer_science":0.56,
        "MMLU_college_mathematics":0.41,
        "MMLU_college_medicine":0.7052023121,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.77,
        "MMLU_conceptual_physics":0.6723404255,
        "MMLU_econometrics":0.5087719298,
        "MMLU_electrical_engineering":0.5655172414,
        "MMLU_elementary_mathematics":0.462962963,
        "MMLU_formal_logic":0.5634920635,
        "MMLU_global_facts":0.45,
        "MMLU_high_school_biology":0.8096774194,
        "MMLU_high_school_chemistry":0.5714285714,
        "MMLU_high_school_computer_science":0.74,
        "MMLU_high_school_european_history":0.8666666667,
        "MMLU_high_school_geography":0.8737373737,
        "MMLU_high_school_government_and_politics":0.9378238342,
        "MMLU_high_school_macroeconomics":0.7153846154,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.7941176471,
        "MMLU_high_school_physics":0.4768211921,
        "MMLU_high_school_psychology":0.9009174312,
        "MMLU_high_school_statistics":0.625,
        "MMLU_high_school_us_history":0.8823529412,
        "MMLU_high_school_world_history":0.9113924051,
        "MMLU_human_aging":0.802690583,
        "MMLU_human_sexuality":0.8396946565,
        "MMLU_international_law":0.8760330579,
        "MMLU_jurisprudence":0.8333333333,
        "MMLU_logical_fallacies":0.8098159509,
        "MMLU_machine_learning":0.6071428571,
        "MMLU_management":0.8349514563,
        "MMLU_marketing":0.905982906,
        "MMLU_medical_genetics":0.7,
        "MMLU_miscellaneous":0.8671775223,
        "MMLU_moral_disputes":0.7687861272,
        "MMLU_moral_scenarios":0.5497206704,
        "MMLU_nutrition":0.7712418301,
        "MMLU_philosophy":0.7942122186,
        "MMLU_prehistory":0.8333333333,
        "MMLU_professional_accounting":0.5354609929,
        "MMLU_professional_law":0.5788787484,
        "MMLU_professional_medicine":0.7205882353,
        "MMLU_professional_psychology":0.7647058824,
        "MMLU_public_relations":0.7,
        "MMLU_security_studies":0.7673469388,
        "MMLU_sociology":0.8905472637,
        "MMLU_us_foreign_policy":0.91,
        "MMLU_virology":0.5843373494,
        "MMLU_world_religions":0.8538011696
    },
    {
        "Model":"Platypus2-70B-Instruct-GPTQ",
        "URL":"https:\/\/huggingface.co\/TheBloke\/Platypus2-70B-Instruct-GPTQ",
        "full_model_name":"TheBloke\/Platypus2-70B-Instruct-GPTQ",
        "Parameters":70.0,
        "MMLU_average":0.6988587856,
        "arc:challenge|25":0.6919795222,
        "hellaswag|10":0.6863174666,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.5925925926,
        "MMLU_astronomy":0.7894736842,
        "MMLU_business_ethics":0.71,
        "MMLU_clinical_knowledge":0.7471698113,
        "MMLU_college_biology":0.8333333333,
        "MMLU_college_chemistry":0.47,
        "MMLU_college_computer_science":0.55,
        "MMLU_college_mathematics":0.44,
        "MMLU_college_medicine":0.6705202312,
        "MMLU_college_physics":0.3529411765,
        "MMLU_computer_security":0.76,
        "MMLU_conceptual_physics":0.6680851064,
        "MMLU_econometrics":0.4385964912,
        "MMLU_electrical_engineering":0.6137931034,
        "MMLU_elementary_mathematics":0.4656084656,
        "MMLU_formal_logic":0.5396825397,
        "MMLU_global_facts":0.48,
        "MMLU_high_school_biology":0.8064516129,
        "MMLU_high_school_chemistry":0.5467980296,
        "MMLU_high_school_computer_science":0.74,
        "MMLU_high_school_european_history":0.8787878788,
        "MMLU_high_school_geography":0.8585858586,
        "MMLU_high_school_government_and_politics":0.9481865285,
        "MMLU_high_school_macroeconomics":0.7025641026,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.781512605,
        "MMLU_high_school_physics":0.4701986755,
        "MMLU_high_school_psychology":0.9082568807,
        "MMLU_high_school_statistics":0.5972222222,
        "MMLU_high_school_us_history":0.9068627451,
        "MMLU_high_school_world_history":0.8987341772,
        "MMLU_human_aging":0.798206278,
        "MMLU_human_sexuality":0.8091603053,
        "MMLU_international_law":0.8760330579,
        "MMLU_jurisprudence":0.8055555556,
        "MMLU_logical_fallacies":0.8282208589,
        "MMLU_machine_learning":0.5714285714,
        "MMLU_management":0.8446601942,
        "MMLU_marketing":0.9017094017,
        "MMLU_medical_genetics":0.71,
        "MMLU_miscellaneous":0.8659003831,
        "MMLU_moral_disputes":0.7687861272,
        "MMLU_moral_scenarios":0.6469273743,
        "MMLU_nutrition":0.7647058824,
        "MMLU_philosophy":0.7749196141,
        "MMLU_prehistory":0.8271604938,
        "MMLU_professional_accounting":0.5673758865,
        "MMLU_professional_law":0.5860495437,
        "MMLU_professional_medicine":0.7132352941,
        "MMLU_professional_psychology":0.7565359477,
        "MMLU_public_relations":0.7181818182,
        "MMLU_security_studies":0.7795918367,
        "MMLU_sociology":0.8706467662,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.5481927711,
        "MMLU_world_religions":0.8421052632
    },
    {
        "Model":"GodziLLa2-70B",
        "URL":"https:\/\/huggingface.co\/MayaPH\/GodziLLa2-70B",
        "full_model_name":"MayaPH\/GodziLLa2-70B",
        "Parameters":70.0,
        "MMLU_average":0.6988177292,
        "arc:challenge|25":0.6868600683,
        "hellaswag|10":0.6891057558,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.5703703704,
        "MMLU_astronomy":0.7960526316,
        "MMLU_business_ethics":0.71,
        "MMLU_clinical_knowledge":0.7660377358,
        "MMLU_college_biology":0.8194444444,
        "MMLU_college_chemistry":0.51,
        "MMLU_college_computer_science":0.55,
        "MMLU_college_mathematics":0.45,
        "MMLU_college_medicine":0.6878612717,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.77,
        "MMLU_conceptual_physics":0.6680851064,
        "MMLU_econometrics":0.4736842105,
        "MMLU_electrical_engineering":0.5931034483,
        "MMLU_elementary_mathematics":0.4735449735,
        "MMLU_formal_logic":0.5079365079,
        "MMLU_global_facts":0.52,
        "MMLU_high_school_biology":0.8193548387,
        "MMLU_high_school_chemistry":0.5665024631,
        "MMLU_high_school_computer_science":0.78,
        "MMLU_high_school_european_history":0.8666666667,
        "MMLU_high_school_geography":0.8535353535,
        "MMLU_high_school_government_and_politics":0.932642487,
        "MMLU_high_school_macroeconomics":0.7128205128,
        "MMLU_high_school_mathematics":0.3074074074,
        "MMLU_high_school_microeconomics":0.7647058824,
        "MMLU_high_school_physics":0.4834437086,
        "MMLU_high_school_psychology":0.9082568807,
        "MMLU_high_school_statistics":0.625,
        "MMLU_high_school_us_history":0.8970588235,
        "MMLU_high_school_world_history":0.9071729958,
        "MMLU_human_aging":0.8116591928,
        "MMLU_human_sexuality":0.8244274809,
        "MMLU_international_law":0.8595041322,
        "MMLU_jurisprudence":0.8333333333,
        "MMLU_logical_fallacies":0.8159509202,
        "MMLU_machine_learning":0.5803571429,
        "MMLU_management":0.8155339806,
        "MMLU_marketing":0.9017094017,
        "MMLU_medical_genetics":0.67,
        "MMLU_miscellaneous":0.8684546616,
        "MMLU_moral_disputes":0.7543352601,
        "MMLU_moral_scenarios":0.6312849162,
        "MMLU_nutrition":0.7385620915,
        "MMLU_philosophy":0.768488746,
        "MMLU_prehistory":0.8148148148,
        "MMLU_professional_accounting":0.5744680851,
        "MMLU_professional_law":0.5814863103,
        "MMLU_professional_medicine":0.7022058824,
        "MMLU_professional_psychology":0.7581699346,
        "MMLU_public_relations":0.7090909091,
        "MMLU_security_studies":0.7591836735,
        "MMLU_sociology":0.8756218905,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.5361445783,
        "MMLU_world_religions":0.8538011696
    },
    {
        "Model":"Camel-Platypus2-70B",
        "URL":"https:\/\/huggingface.co\/garage-bAInd\/Camel-Platypus2-70B",
        "full_model_name":"garage-bAInd\/Camel-Platypus2-70B",
        "Parameters":70.0,
        "MMLU_average":0.6983421591,
        "arc:challenge|25":0.6697952218,
        "hellaswag|10":0.6862178849,
        "MMLU_abstract_algebra":0.38,
        "MMLU_anatomy":0.5851851852,
        "MMLU_astronomy":0.8092105263,
        "MMLU_business_ethics":0.73,
        "MMLU_clinical_knowledge":0.7320754717,
        "MMLU_college_biology":0.8402777778,
        "MMLU_college_chemistry":0.46,
        "MMLU_college_computer_science":0.55,
        "MMLU_college_mathematics":0.4,
        "MMLU_college_medicine":0.6820809249,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.78,
        "MMLU_conceptual_physics":0.6595744681,
        "MMLU_econometrics":0.5,
        "MMLU_electrical_engineering":0.6137931034,
        "MMLU_elementary_mathematics":0.4365079365,
        "MMLU_formal_logic":0.5793650794,
        "MMLU_global_facts":0.49,
        "MMLU_high_school_biology":0.8,
        "MMLU_high_school_chemistry":0.5714285714,
        "MMLU_high_school_computer_science":0.76,
        "MMLU_high_school_european_history":0.8363636364,
        "MMLU_high_school_geography":0.8686868687,
        "MMLU_high_school_government_and_politics":0.932642487,
        "MMLU_high_school_macroeconomics":0.7051282051,
        "MMLU_high_school_mathematics":0.2888888889,
        "MMLU_high_school_microeconomics":0.7773109244,
        "MMLU_high_school_physics":0.4900662252,
        "MMLU_high_school_psychology":0.9009174312,
        "MMLU_high_school_statistics":0.6111111111,
        "MMLU_high_school_us_history":0.8921568627,
        "MMLU_high_school_world_history":0.9113924051,
        "MMLU_human_aging":0.798206278,
        "MMLU_human_sexuality":0.786259542,
        "MMLU_international_law":0.8595041322,
        "MMLU_jurisprudence":0.8240740741,
        "MMLU_logical_fallacies":0.8036809816,
        "MMLU_machine_learning":0.5982142857,
        "MMLU_management":0.854368932,
        "MMLU_marketing":0.9145299145,
        "MMLU_medical_genetics":0.68,
        "MMLU_miscellaneous":0.8633461047,
        "MMLU_moral_disputes":0.7716763006,
        "MMLU_moral_scenarios":0.5229050279,
        "MMLU_nutrition":0.7450980392,
        "MMLU_philosophy":0.7781350482,
        "MMLU_prehistory":0.8179012346,
        "MMLU_professional_accounting":0.5425531915,
        "MMLU_professional_law":0.5951760104,
        "MMLU_professional_medicine":0.7205882353,
        "MMLU_professional_psychology":0.7663398693,
        "MMLU_public_relations":0.7272727273,
        "MMLU_security_studies":0.7714285714,
        "MMLU_sociology":0.8805970149,
        "MMLU_us_foreign_policy":0.88,
        "MMLU_virology":0.5542168675,
        "MMLU_world_religions":0.8713450292
    },
    {
        "Model":"test-llama2-70b",
        "URL":"https:\/\/huggingface.co\/bongchoi\/test-llama2-70b",
        "full_model_name":"bongchoi\/test-llama2-70b",
        "Parameters":70.0,
        "MMLU_average":0.6983208921,
        "arc:challenge|25":0.6262798635,
        "hellaswag|10":0.6760605457,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.6296296296,
        "MMLU_astronomy":0.8092105263,
        "MMLU_business_ethics":0.72,
        "MMLU_clinical_knowledge":0.7169811321,
        "MMLU_college_biology":0.8472222222,
        "MMLU_college_chemistry":0.51,
        "MMLU_college_computer_science":0.6,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.6416184971,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.77,
        "MMLU_conceptual_physics":0.6638297872,
        "MMLU_econometrics":0.4473684211,
        "MMLU_electrical_engineering":0.6551724138,
        "MMLU_elementary_mathematics":0.4338624339,
        "MMLU_formal_logic":0.4761904762,
        "MMLU_global_facts":0.46,
        "MMLU_high_school_biology":0.8193548387,
        "MMLU_high_school_chemistry":0.5123152709,
        "MMLU_high_school_computer_science":0.79,
        "MMLU_high_school_european_history":0.8303030303,
        "MMLU_high_school_geography":0.8787878788,
        "MMLU_high_school_government_and_politics":0.9430051813,
        "MMLU_high_school_macroeconomics":0.741025641,
        "MMLU_high_school_mathematics":0.3555555556,
        "MMLU_high_school_microeconomics":0.7647058824,
        "MMLU_high_school_physics":0.4304635762,
        "MMLU_high_school_psychology":0.8733944954,
        "MMLU_high_school_statistics":0.6342592593,
        "MMLU_high_school_us_history":0.8970588235,
        "MMLU_high_school_world_history":0.8776371308,
        "MMLU_human_aging":0.802690583,
        "MMLU_human_sexuality":0.8778625954,
        "MMLU_international_law":0.8760330579,
        "MMLU_jurisprudence":0.8333333333,
        "MMLU_logical_fallacies":0.8036809816,
        "MMLU_machine_learning":0.5357142857,
        "MMLU_management":0.8349514563,
        "MMLU_marketing":0.905982906,
        "MMLU_medical_genetics":0.74,
        "MMLU_miscellaneous":0.8620689655,
        "MMLU_moral_disputes":0.7774566474,
        "MMLU_moral_scenarios":0.4547486034,
        "MMLU_nutrition":0.7810457516,
        "MMLU_philosophy":0.7877813505,
        "MMLU_prehistory":0.8364197531,
        "MMLU_professional_accounting":0.5673758865,
        "MMLU_professional_law":0.5319426336,
        "MMLU_professional_medicine":0.75,
        "MMLU_professional_psychology":0.7565359477,
        "MMLU_public_relations":0.6909090909,
        "MMLU_security_studies":0.7918367347,
        "MMLU_sociology":0.9004975124,
        "MMLU_us_foreign_policy":0.92,
        "MMLU_virology":0.5301204819,
        "MMLU_world_religions":0.8538011696
    },
    {
        "Model":"Llama-2-70B-fp16",
        "URL":"https:\/\/huggingface.co\/TheBloke\/Llama-2-70B-fp16",
        "full_model_name":"TheBloke\/Llama-2-70B-fp16",
        "Parameters":70.0,
        "MMLU_average":0.6983208921,
        "arc:challenge|25":0.6262798635,
        "hellaswag|10":0.6760605457,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.6296296296,
        "MMLU_astronomy":0.8092105263,
        "MMLU_business_ethics":0.72,
        "MMLU_clinical_knowledge":0.7169811321,
        "MMLU_college_biology":0.8472222222,
        "MMLU_college_chemistry":0.51,
        "MMLU_college_computer_science":0.6,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.6416184971,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.77,
        "MMLU_conceptual_physics":0.6638297872,
        "MMLU_econometrics":0.4473684211,
        "MMLU_electrical_engineering":0.6551724138,
        "MMLU_elementary_mathematics":0.4338624339,
        "MMLU_formal_logic":0.4761904762,
        "MMLU_global_facts":0.46,
        "MMLU_high_school_biology":0.8193548387,
        "MMLU_high_school_chemistry":0.5123152709,
        "MMLU_high_school_computer_science":0.79,
        "MMLU_high_school_european_history":0.8303030303,
        "MMLU_high_school_geography":0.8787878788,
        "MMLU_high_school_government_and_politics":0.9430051813,
        "MMLU_high_school_macroeconomics":0.741025641,
        "MMLU_high_school_mathematics":0.3555555556,
        "MMLU_high_school_microeconomics":0.7647058824,
        "MMLU_high_school_physics":0.4304635762,
        "MMLU_high_school_psychology":0.8733944954,
        "MMLU_high_school_statistics":0.6342592593,
        "MMLU_high_school_us_history":0.8970588235,
        "MMLU_high_school_world_history":0.8776371308,
        "MMLU_human_aging":0.802690583,
        "MMLU_human_sexuality":0.8778625954,
        "MMLU_international_law":0.8760330579,
        "MMLU_jurisprudence":0.8333333333,
        "MMLU_logical_fallacies":0.8036809816,
        "MMLU_machine_learning":0.5357142857,
        "MMLU_management":0.8349514563,
        "MMLU_marketing":0.905982906,
        "MMLU_medical_genetics":0.74,
        "MMLU_miscellaneous":0.8620689655,
        "MMLU_moral_disputes":0.7774566474,
        "MMLU_moral_scenarios":0.4547486034,
        "MMLU_nutrition":0.7810457516,
        "MMLU_philosophy":0.7877813505,
        "MMLU_prehistory":0.8364197531,
        "MMLU_professional_accounting":0.5673758865,
        "MMLU_professional_law":0.5319426336,
        "MMLU_professional_medicine":0.75,
        "MMLU_professional_psychology":0.7565359477,
        "MMLU_public_relations":0.6909090909,
        "MMLU_security_studies":0.7918367347,
        "MMLU_sociology":0.9004975124,
        "MMLU_us_foreign_policy":0.92,
        "MMLU_virology":0.5301204819,
        "MMLU_world_religions":0.8538011696
    },
    {
        "Model":"Llama-2-70b-hf",
        "URL":"https:\/\/huggingface.co\/meta-llama\/Llama-2-70b-hf",
        "full_model_name":"meta-llama\/Llama-2-70b-hf",
        "Parameters":70.0,
        "MMLU_average":0.6983208921,
        "arc:challenge|25":0.6262798635,
        "hellaswag|10":0.6760605457,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.6296296296,
        "MMLU_astronomy":0.8092105263,
        "MMLU_business_ethics":0.72,
        "MMLU_clinical_knowledge":0.7169811321,
        "MMLU_college_biology":0.8472222222,
        "MMLU_college_chemistry":0.51,
        "MMLU_college_computer_science":0.6,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.6416184971,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.77,
        "MMLU_conceptual_physics":0.6638297872,
        "MMLU_econometrics":0.4473684211,
        "MMLU_electrical_engineering":0.6551724138,
        "MMLU_elementary_mathematics":0.4338624339,
        "MMLU_formal_logic":0.4761904762,
        "MMLU_global_facts":0.46,
        "MMLU_high_school_biology":0.8193548387,
        "MMLU_high_school_chemistry":0.5123152709,
        "MMLU_high_school_computer_science":0.79,
        "MMLU_high_school_european_history":0.8303030303,
        "MMLU_high_school_geography":0.8787878788,
        "MMLU_high_school_government_and_politics":0.9430051813,
        "MMLU_high_school_macroeconomics":0.741025641,
        "MMLU_high_school_mathematics":0.3555555556,
        "MMLU_high_school_microeconomics":0.7647058824,
        "MMLU_high_school_physics":0.4304635762,
        "MMLU_high_school_psychology":0.8733944954,
        "MMLU_high_school_statistics":0.6342592593,
        "MMLU_high_school_us_history":0.8970588235,
        "MMLU_high_school_world_history":0.8776371308,
        "MMLU_human_aging":0.802690583,
        "MMLU_human_sexuality":0.8778625954,
        "MMLU_international_law":0.8760330579,
        "MMLU_jurisprudence":0.8333333333,
        "MMLU_logical_fallacies":0.8036809816,
        "MMLU_machine_learning":0.5357142857,
        "MMLU_management":0.8349514563,
        "MMLU_marketing":0.905982906,
        "MMLU_medical_genetics":0.74,
        "MMLU_miscellaneous":0.8620689655,
        "MMLU_moral_disputes":0.7774566474,
        "MMLU_moral_scenarios":0.4547486034,
        "MMLU_nutrition":0.7810457516,
        "MMLU_philosophy":0.7877813505,
        "MMLU_prehistory":0.8364197531,
        "MMLU_professional_accounting":0.5673758865,
        "MMLU_professional_law":0.5319426336,
        "MMLU_professional_medicine":0.75,
        "MMLU_professional_psychology":0.7565359477,
        "MMLU_public_relations":0.6909090909,
        "MMLU_security_studies":0.7918367347,
        "MMLU_sociology":0.9004975124,
        "MMLU_us_foreign_policy":0.92,
        "MMLU_virology":0.5301204819,
        "MMLU_world_religions":0.8538011696
    },
    {
        "Model":"fiction.live-Kimiko-V2-70B-fp16",
        "URL":"https:\/\/huggingface.co\/TheBloke\/fiction.live-Kimiko-V2-70B-fp16",
        "full_model_name":"TheBloke\/fiction.live-Kimiko-V2-70B-fp16",
        "Parameters":70.0,
        "MMLU_average":0.6981608402,
        "arc:challenge|25":0.6399317406,
        "hellaswag|10":0.6801433977,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.6296296296,
        "MMLU_astronomy":0.8223684211,
        "MMLU_business_ethics":0.73,
        "MMLU_clinical_knowledge":0.7320754717,
        "MMLU_college_biology":0.8333333333,
        "MMLU_college_chemistry":0.48,
        "MMLU_college_computer_science":0.58,
        "MMLU_college_mathematics":0.4,
        "MMLU_college_medicine":0.6473988439,
        "MMLU_college_physics":0.3529411765,
        "MMLU_computer_security":0.79,
        "MMLU_conceptual_physics":0.6723404255,
        "MMLU_econometrics":0.4298245614,
        "MMLU_electrical_engineering":0.6206896552,
        "MMLU_elementary_mathematics":0.4365079365,
        "MMLU_formal_logic":0.5,
        "MMLU_global_facts":0.5,
        "MMLU_high_school_biology":0.8096774194,
        "MMLU_high_school_chemistry":0.5270935961,
        "MMLU_high_school_computer_science":0.76,
        "MMLU_high_school_european_history":0.8303030303,
        "MMLU_high_school_geography":0.8787878788,
        "MMLU_high_school_government_and_politics":0.9378238342,
        "MMLU_high_school_macroeconomics":0.7153846154,
        "MMLU_high_school_mathematics":0.3555555556,
        "MMLU_high_school_microeconomics":0.7605042017,
        "MMLU_high_school_physics":0.4105960265,
        "MMLU_high_school_psychology":0.8844036697,
        "MMLU_high_school_statistics":0.6157407407,
        "MMLU_high_school_us_history":0.9068627451,
        "MMLU_high_school_world_history":0.8776371308,
        "MMLU_human_aging":0.802690583,
        "MMLU_human_sexuality":0.8702290076,
        "MMLU_international_law":0.8760330579,
        "MMLU_jurisprudence":0.8240740741,
        "MMLU_logical_fallacies":0.8220858896,
        "MMLU_machine_learning":0.5714285714,
        "MMLU_management":0.8349514563,
        "MMLU_marketing":0.8974358974,
        "MMLU_medical_genetics":0.74,
        "MMLU_miscellaneous":0.8659003831,
        "MMLU_moral_disputes":0.7803468208,
        "MMLU_moral_scenarios":0.4357541899,
        "MMLU_nutrition":0.7745098039,
        "MMLU_philosophy":0.768488746,
        "MMLU_prehistory":0.8333333333,
        "MMLU_professional_accounting":0.5425531915,
        "MMLU_professional_law":0.536505867,
        "MMLU_professional_medicine":0.7536764706,
        "MMLU_professional_psychology":0.7565359477,
        "MMLU_public_relations":0.7,
        "MMLU_security_studies":0.7959183673,
        "MMLU_sociology":0.8955223881,
        "MMLU_us_foreign_policy":0.92,
        "MMLU_virology":0.5542168675,
        "MMLU_world_religions":0.865497076
    },
    {
        "Model":"Nous-Puffin-70B",
        "URL":"https:\/\/huggingface.co\/NousResearch\/Nous-Puffin-70B",
        "full_model_name":"NousResearch\/Nous-Puffin-70B",
        "Parameters":70.0,
        "MMLU_average":0.697689922,
        "arc:challenge|25":0.6288395904,
        "hellaswag|10":0.6766580362,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.6592592593,
        "MMLU_astronomy":0.8092105263,
        "MMLU_business_ethics":0.73,
        "MMLU_clinical_knowledge":0.7320754717,
        "MMLU_college_biology":0.8402777778,
        "MMLU_college_chemistry":0.5,
        "MMLU_college_computer_science":0.6,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.6358381503,
        "MMLU_college_physics":0.3529411765,
        "MMLU_computer_security":0.78,
        "MMLU_conceptual_physics":0.6595744681,
        "MMLU_econometrics":0.4385964912,
        "MMLU_electrical_engineering":0.6482758621,
        "MMLU_elementary_mathematics":0.4338624339,
        "MMLU_formal_logic":0.4682539683,
        "MMLU_global_facts":0.48,
        "MMLU_high_school_biology":0.8161290323,
        "MMLU_high_school_chemistry":0.5172413793,
        "MMLU_high_school_computer_science":0.78,
        "MMLU_high_school_european_history":0.8363636364,
        "MMLU_high_school_geography":0.8787878788,
        "MMLU_high_school_government_and_politics":0.9430051813,
        "MMLU_high_school_macroeconomics":0.7333333333,
        "MMLU_high_school_mathematics":0.3592592593,
        "MMLU_high_school_microeconomics":0.768907563,
        "MMLU_high_school_physics":0.4105960265,
        "MMLU_high_school_psychology":0.871559633,
        "MMLU_high_school_statistics":0.6296296296,
        "MMLU_high_school_us_history":0.9166666667,
        "MMLU_high_school_world_history":0.8776371308,
        "MMLU_human_aging":0.798206278,
        "MMLU_human_sexuality":0.8702290076,
        "MMLU_international_law":0.8760330579,
        "MMLU_jurisprudence":0.8333333333,
        "MMLU_logical_fallacies":0.7975460123,
        "MMLU_machine_learning":0.5446428571,
        "MMLU_management":0.8349514563,
        "MMLU_marketing":0.905982906,
        "MMLU_medical_genetics":0.74,
        "MMLU_miscellaneous":0.8633461047,
        "MMLU_moral_disputes":0.774566474,
        "MMLU_moral_scenarios":0.4458100559,
        "MMLU_nutrition":0.7810457516,
        "MMLU_philosophy":0.7717041801,
        "MMLU_prehistory":0.8333333333,
        "MMLU_professional_accounting":0.5567375887,
        "MMLU_professional_law":0.5371577575,
        "MMLU_professional_medicine":0.7426470588,
        "MMLU_professional_psychology":0.7565359477,
        "MMLU_public_relations":0.7,
        "MMLU_security_studies":0.7959183673,
        "MMLU_sociology":0.8955223881,
        "MMLU_us_foreign_policy":0.92,
        "MMLU_virology":0.5361445783,
        "MMLU_world_religions":0.8596491228
    },
    {
        "Model":"Nous-Hermes-Llama2-70b",
        "URL":"https:\/\/huggingface.co\/NousResearch\/Nous-Hermes-Llama2-70b",
        "full_model_name":"NousResearch\/Nous-Hermes-Llama2-70b",
        "Parameters":70.0,
        "MMLU_average":0.6972126141,
        "arc:challenge|25":0.6313993174,
        "hellaswag|10":0.6777534356,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.637037037,
        "MMLU_astronomy":0.8026315789,
        "MMLU_business_ethics":0.75,
        "MMLU_clinical_knowledge":0.720754717,
        "MMLU_college_biology":0.8402777778,
        "MMLU_college_chemistry":0.47,
        "MMLU_college_computer_science":0.53,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.6705202312,
        "MMLU_college_physics":0.362745098,
        "MMLU_computer_security":0.77,
        "MMLU_conceptual_physics":0.714893617,
        "MMLU_econometrics":0.4298245614,
        "MMLU_electrical_engineering":0.6344827586,
        "MMLU_elementary_mathematics":0.4391534392,
        "MMLU_formal_logic":0.4761904762,
        "MMLU_global_facts":0.53,
        "MMLU_high_school_biology":0.8032258065,
        "MMLU_high_school_chemistry":0.5467980296,
        "MMLU_high_school_computer_science":0.74,
        "MMLU_high_school_european_history":0.8484848485,
        "MMLU_high_school_geography":0.8737373737,
        "MMLU_high_school_government_and_politics":0.9378238342,
        "MMLU_high_school_macroeconomics":0.7025641026,
        "MMLU_high_school_mathematics":0.3111111111,
        "MMLU_high_school_microeconomics":0.7605042017,
        "MMLU_high_school_physics":0.4834437086,
        "MMLU_high_school_psychology":0.8990825688,
        "MMLU_high_school_statistics":0.5972222222,
        "MMLU_high_school_us_history":0.9264705882,
        "MMLU_high_school_world_history":0.8818565401,
        "MMLU_human_aging":0.7892376682,
        "MMLU_human_sexuality":0.8625954198,
        "MMLU_international_law":0.8760330579,
        "MMLU_jurisprudence":0.8333333333,
        "MMLU_logical_fallacies":0.8466257669,
        "MMLU_machine_learning":0.5446428571,
        "MMLU_management":0.8252427184,
        "MMLU_marketing":0.9017094017,
        "MMLU_medical_genetics":0.71,
        "MMLU_miscellaneous":0.8582375479,
        "MMLU_moral_disputes":0.7803468208,
        "MMLU_moral_scenarios":0.4446927374,
        "MMLU_nutrition":0.7712418301,
        "MMLU_philosophy":0.7717041801,
        "MMLU_prehistory":0.8271604938,
        "MMLU_professional_accounting":0.5602836879,
        "MMLU_professional_law":0.5508474576,
        "MMLU_professional_medicine":0.7389705882,
        "MMLU_professional_psychology":0.7630718954,
        "MMLU_public_relations":0.7181818182,
        "MMLU_security_studies":0.8081632653,
        "MMLU_sociology":0.8706467662,
        "MMLU_us_foreign_policy":0.91,
        "MMLU_virology":0.5301204819,
        "MMLU_world_religions":0.8771929825
    },
    {
        "Model":"llama-2-70B-ensemble-v4",
        "URL":"https:\/\/huggingface.co\/yeontaek\/llama-2-70B-ensemble-v4",
        "full_model_name":"yeontaek\/llama-2-70B-ensemble-v4",
        "Parameters":70.0,
        "MMLU_average":0.6970988497,
        "arc:challenge|25":0.680887372,
        "hellaswag|10":0.6838279227,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.5925925926,
        "MMLU_astronomy":0.7763157895,
        "MMLU_business_ethics":0.69,
        "MMLU_clinical_knowledge":0.7433962264,
        "MMLU_college_biology":0.8541666667,
        "MMLU_college_chemistry":0.47,
        "MMLU_college_computer_science":0.54,
        "MMLU_college_mathematics":0.45,
        "MMLU_college_medicine":0.676300578,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.76,
        "MMLU_conceptual_physics":0.6680851064,
        "MMLU_econometrics":0.4561403509,
        "MMLU_electrical_engineering":0.5655172414,
        "MMLU_elementary_mathematics":0.4708994709,
        "MMLU_formal_logic":0.5238095238,
        "MMLU_global_facts":0.46,
        "MMLU_high_school_biology":0.8225806452,
        "MMLU_high_school_chemistry":0.5369458128,
        "MMLU_high_school_computer_science":0.78,
        "MMLU_high_school_european_history":0.8606060606,
        "MMLU_high_school_geography":0.8737373737,
        "MMLU_high_school_government_and_politics":0.9481865285,
        "MMLU_high_school_macroeconomics":0.7205128205,
        "MMLU_high_school_mathematics":0.3296296296,
        "MMLU_high_school_microeconomics":0.7941176471,
        "MMLU_high_school_physics":0.4635761589,
        "MMLU_high_school_psychology":0.8880733945,
        "MMLU_high_school_statistics":0.6064814815,
        "MMLU_high_school_us_history":0.8970588235,
        "MMLU_high_school_world_history":0.8902953586,
        "MMLU_human_aging":0.8161434978,
        "MMLU_human_sexuality":0.8320610687,
        "MMLU_international_law":0.8595041322,
        "MMLU_jurisprudence":0.8148148148,
        "MMLU_logical_fallacies":0.8159509202,
        "MMLU_machine_learning":0.5892857143,
        "MMLU_management":0.8155339806,
        "MMLU_marketing":0.8931623932,
        "MMLU_medical_genetics":0.7,
        "MMLU_miscellaneous":0.8659003831,
        "MMLU_moral_disputes":0.7514450867,
        "MMLU_moral_scenarios":0.6502793296,
        "MMLU_nutrition":0.7418300654,
        "MMLU_philosophy":0.7556270096,
        "MMLU_prehistory":0.8240740741,
        "MMLU_professional_accounting":0.5673758865,
        "MMLU_professional_law":0.591916558,
        "MMLU_professional_medicine":0.7095588235,
        "MMLU_professional_psychology":0.7516339869,
        "MMLU_public_relations":0.7090909091,
        "MMLU_security_studies":0.7795918367,
        "MMLU_sociology":0.8756218905,
        "MMLU_us_foreign_policy":0.86,
        "MMLU_virology":0.5180722892,
        "MMLU_world_religions":0.8538011696
    },
    {
        "Model":"llama-2-70B-ensemble-v5",
        "URL":"https:\/\/huggingface.co\/yeontaek\/llama-2-70B-ensemble-v5",
        "full_model_name":"yeontaek\/llama-2-70B-ensemble-v5",
        "Parameters":70.0,
        "MMLU_average":0.695958264,
        "arc:challenge|25":0.6774744027,
        "hellaswag|10":0.680043816,
        "MMLU_abstract_algebra":0.4,
        "MMLU_anatomy":0.6222222222,
        "MMLU_astronomy":0.8092105263,
        "MMLU_business_ethics":0.72,
        "MMLU_clinical_knowledge":0.7396226415,
        "MMLU_college_biology":0.8333333333,
        "MMLU_college_chemistry":0.49,
        "MMLU_college_computer_science":0.59,
        "MMLU_college_mathematics":0.42,
        "MMLU_college_medicine":0.6820809249,
        "MMLU_college_physics":0.3529411765,
        "MMLU_computer_security":0.76,
        "MMLU_conceptual_physics":0.6723404255,
        "MMLU_econometrics":0.4298245614,
        "MMLU_electrical_engineering":0.6137931034,
        "MMLU_elementary_mathematics":0.4788359788,
        "MMLU_formal_logic":0.4682539683,
        "MMLU_global_facts":0.49,
        "MMLU_high_school_biology":0.8193548387,
        "MMLU_high_school_chemistry":0.5369458128,
        "MMLU_high_school_computer_science":0.76,
        "MMLU_high_school_european_history":0.8363636364,
        "MMLU_high_school_geography":0.904040404,
        "MMLU_high_school_government_and_politics":0.932642487,
        "MMLU_high_school_macroeconomics":0.7179487179,
        "MMLU_high_school_mathematics":0.337037037,
        "MMLU_high_school_microeconomics":0.768907563,
        "MMLU_high_school_physics":0.4635761589,
        "MMLU_high_school_psychology":0.8880733945,
        "MMLU_high_school_statistics":0.6018518519,
        "MMLU_high_school_us_history":0.8970588235,
        "MMLU_high_school_world_history":0.8776371308,
        "MMLU_human_aging":0.7802690583,
        "MMLU_human_sexuality":0.8320610687,
        "MMLU_international_law":0.8181818182,
        "MMLU_jurisprudence":0.8148148148,
        "MMLU_logical_fallacies":0.8282208589,
        "MMLU_machine_learning":0.4642857143,
        "MMLU_management":0.8252427184,
        "MMLU_marketing":0.8846153846,
        "MMLU_medical_genetics":0.67,
        "MMLU_miscellaneous":0.8697318008,
        "MMLU_moral_disputes":0.7630057803,
        "MMLU_moral_scenarios":0.6067039106,
        "MMLU_nutrition":0.7222222222,
        "MMLU_philosophy":0.7717041801,
        "MMLU_prehistory":0.8148148148,
        "MMLU_professional_accounting":0.5567375887,
        "MMLU_professional_law":0.5638852673,
        "MMLU_professional_medicine":0.7352941176,
        "MMLU_professional_psychology":0.7401960784,
        "MMLU_public_relations":0.7,
        "MMLU_security_studies":0.812244898,
        "MMLU_sociology":0.8955223881,
        "MMLU_us_foreign_policy":0.89,
        "MMLU_virology":0.5421686747,
        "MMLU_world_religions":0.8538011696
    },
    {
        "Model":"Instruct_Llama70B_Dolly15k",
        "URL":"https:\/\/huggingface.co\/Brillibits\/Instruct_Llama70B_Dolly15k",
        "full_model_name":"Brillibits\/Instruct_Llama70B_Dolly15k",
        "Parameters":70.0,
        "MMLU_average":0.6952348074,
        "arc:challenge|25":0.638225256,
        "hellaswag|10":0.6777534356,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.6148148148,
        "MMLU_astronomy":0.8157894737,
        "MMLU_business_ethics":0.72,
        "MMLU_clinical_knowledge":0.720754717,
        "MMLU_college_biology":0.8402777778,
        "MMLU_college_chemistry":0.52,
        "MMLU_college_computer_science":0.56,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.6473988439,
        "MMLU_college_physics":0.3529411765,
        "MMLU_computer_security":0.77,
        "MMLU_conceptual_physics":0.6893617021,
        "MMLU_econometrics":0.4561403509,
        "MMLU_electrical_engineering":0.6413793103,
        "MMLU_elementary_mathematics":0.4365079365,
        "MMLU_formal_logic":0.4603174603,
        "MMLU_global_facts":0.46,
        "MMLU_high_school_biology":0.8193548387,
        "MMLU_high_school_chemistry":0.5024630542,
        "MMLU_high_school_computer_science":0.74,
        "MMLU_high_school_european_history":0.8181818182,
        "MMLU_high_school_geography":0.8737373737,
        "MMLU_high_school_government_and_politics":0.932642487,
        "MMLU_high_school_macroeconomics":0.7,
        "MMLU_high_school_mathematics":0.3074074074,
        "MMLU_high_school_microeconomics":0.7773109244,
        "MMLU_high_school_physics":0.4370860927,
        "MMLU_high_school_psychology":0.8899082569,
        "MMLU_high_school_statistics":0.5740740741,
        "MMLU_high_school_us_history":0.9166666667,
        "MMLU_high_school_world_history":0.8776371308,
        "MMLU_human_aging":0.802690583,
        "MMLU_human_sexuality":0.8702290076,
        "MMLU_international_law":0.867768595,
        "MMLU_jurisprudence":0.8148148148,
        "MMLU_logical_fallacies":0.8098159509,
        "MMLU_machine_learning":0.5,
        "MMLU_management":0.8349514563,
        "MMLU_marketing":0.905982906,
        "MMLU_medical_genetics":0.75,
        "MMLU_miscellaneous":0.8595146871,
        "MMLU_moral_disputes":0.7947976879,
        "MMLU_moral_scenarios":0.4536312849,
        "MMLU_nutrition":0.7810457516,
        "MMLU_philosophy":0.7781350482,
        "MMLU_prehistory":0.8425925926,
        "MMLU_professional_accounting":0.5673758865,
        "MMLU_professional_law":0.5410691004,
        "MMLU_professional_medicine":0.7426470588,
        "MMLU_professional_psychology":0.7516339869,
        "MMLU_public_relations":0.7272727273,
        "MMLU_security_studies":0.8,
        "MMLU_sociology":0.8905472637,
        "MMLU_us_foreign_policy":0.94,
        "MMLU_virology":0.5542168675,
        "MMLU_world_religions":0.865497076
    },
    {
        "Model":"SharpBalance",
        "URL":"https:\/\/huggingface.co\/sequelbox\/SharpBalance",
        "full_model_name":"sequelbox\/SharpBalance",
        "Parameters":null,
        "MMLU_average":0.6950772546,
        "arc:challenge|25":0.6527303754,
        "hellaswag|10":0.687811193,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.6444444444,
        "MMLU_astronomy":0.8092105263,
        "MMLU_business_ethics":0.75,
        "MMLU_clinical_knowledge":0.720754717,
        "MMLU_college_biology":0.8055555556,
        "MMLU_college_chemistry":0.49,
        "MMLU_college_computer_science":0.55,
        "MMLU_college_mathematics":0.41,
        "MMLU_college_medicine":0.6647398844,
        "MMLU_college_physics":0.3921568627,
        "MMLU_computer_security":0.77,
        "MMLU_conceptual_physics":0.6723404255,
        "MMLU_econometrics":0.4122807018,
        "MMLU_electrical_engineering":0.6,
        "MMLU_elementary_mathematics":0.4391534392,
        "MMLU_formal_logic":0.5238095238,
        "MMLU_global_facts":0.48,
        "MMLU_high_school_biology":0.8161290323,
        "MMLU_high_school_chemistry":0.5369458128,
        "MMLU_high_school_computer_science":0.75,
        "MMLU_high_school_european_history":0.8303030303,
        "MMLU_high_school_geography":0.8838383838,
        "MMLU_high_school_government_and_politics":0.9274611399,
        "MMLU_high_school_macroeconomics":0.7076923077,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.7521008403,
        "MMLU_high_school_physics":0.4768211921,
        "MMLU_high_school_psychology":0.8880733945,
        "MMLU_high_school_statistics":0.5462962963,
        "MMLU_high_school_us_history":0.9166666667,
        "MMLU_high_school_world_history":0.8776371308,
        "MMLU_human_aging":0.798206278,
        "MMLU_human_sexuality":0.8473282443,
        "MMLU_international_law":0.8760330579,
        "MMLU_jurisprudence":0.8148148148,
        "MMLU_logical_fallacies":0.8098159509,
        "MMLU_machine_learning":0.5,
        "MMLU_management":0.8252427184,
        "MMLU_marketing":0.9188034188,
        "MMLU_medical_genetics":0.72,
        "MMLU_miscellaneous":0.8735632184,
        "MMLU_moral_disputes":0.7976878613,
        "MMLU_moral_scenarios":0.5296089385,
        "MMLU_nutrition":0.7352941176,
        "MMLU_philosophy":0.7909967846,
        "MMLU_prehistory":0.8148148148,
        "MMLU_professional_accounting":0.5425531915,
        "MMLU_professional_law":0.5462842243,
        "MMLU_professional_medicine":0.7169117647,
        "MMLU_professional_psychology":0.75,
        "MMLU_public_relations":0.7272727273,
        "MMLU_security_studies":0.8,
        "MMLU_sociology":0.8756218905,
        "MMLU_us_foreign_policy":0.92,
        "MMLU_virology":0.5481927711,
        "MMLU_world_religions":0.8596491228
    },
    {
        "Model":"QuantumLM-llama2-70B-Korean-LoRA",
        "URL":"https:\/\/huggingface.co\/quantumaikr\/QuantumLM-llama2-70B-Korean-LoRA",
        "full_model_name":"quantumaikr\/QuantumLM-llama2-70B-Korean-LoRA",
        "Parameters":70.0,
        "MMLU_average":0.6940756769,
        "arc:challenge|25":0.6749146758,
        "hellaswag|10":0.6743676558,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.5925925926,
        "MMLU_astronomy":0.8289473684,
        "MMLU_business_ethics":0.75,
        "MMLU_clinical_knowledge":0.7245283019,
        "MMLU_college_biology":0.8333333333,
        "MMLU_college_chemistry":0.51,
        "MMLU_college_computer_science":0.56,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.6705202312,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.73,
        "MMLU_conceptual_physics":0.6808510638,
        "MMLU_econometrics":0.4473684211,
        "MMLU_electrical_engineering":0.5862068966,
        "MMLU_elementary_mathematics":0.4391534392,
        "MMLU_formal_logic":0.4682539683,
        "MMLU_global_facts":0.5,
        "MMLU_high_school_biology":0.8451612903,
        "MMLU_high_school_chemistry":0.5320197044,
        "MMLU_high_school_computer_science":0.73,
        "MMLU_high_school_european_history":0.8242424242,
        "MMLU_high_school_geography":0.8636363636,
        "MMLU_high_school_government_and_politics":0.9430051813,
        "MMLU_high_school_macroeconomics":0.7256410256,
        "MMLU_high_school_mathematics":0.3518518519,
        "MMLU_high_school_microeconomics":0.7857142857,
        "MMLU_high_school_physics":0.4569536424,
        "MMLU_high_school_psychology":0.8844036697,
        "MMLU_high_school_statistics":0.587962963,
        "MMLU_high_school_us_history":0.9068627451,
        "MMLU_high_school_world_history":0.8734177215,
        "MMLU_human_aging":0.7757847534,
        "MMLU_human_sexuality":0.8320610687,
        "MMLU_international_law":0.8842975207,
        "MMLU_jurisprudence":0.8518518519,
        "MMLU_logical_fallacies":0.8159509202,
        "MMLU_machine_learning":0.5267857143,
        "MMLU_management":0.7961165049,
        "MMLU_marketing":0.8717948718,
        "MMLU_medical_genetics":0.69,
        "MMLU_miscellaneous":0.8531289911,
        "MMLU_moral_disputes":0.7687861272,
        "MMLU_moral_scenarios":0.5094972067,
        "MMLU_nutrition":0.7679738562,
        "MMLU_philosophy":0.7588424437,
        "MMLU_prehistory":0.8055555556,
        "MMLU_professional_accounting":0.5567375887,
        "MMLU_professional_law":0.5560625815,
        "MMLU_professional_medicine":0.7536764706,
        "MMLU_professional_psychology":0.7630718954,
        "MMLU_public_relations":0.7181818182,
        "MMLU_security_studies":0.7755102041,
        "MMLU_sociology":0.8955223881,
        "MMLU_us_foreign_policy":0.93,
        "MMLU_virology":0.5361445783,
        "MMLU_world_religions":0.8538011696
    },
    {
        "Model":"llama-2-70B-LoRA-assemble",
        "URL":"https:\/\/huggingface.co\/oh-yeontaek\/llama-2-70B-LoRA-assemble",
        "full_model_name":"oh-yeontaek\/llama-2-70B-LoRA-assemble",
        "Parameters":70.0,
        "MMLU_average":0.6939756538,
        "arc:challenge|25":0.6851535836,
        "hellaswag|10":0.6707827126,
        "MMLU_abstract_algebra":0.44,
        "MMLU_anatomy":0.6222222222,
        "MMLU_astronomy":0.7763157895,
        "MMLU_business_ethics":0.73,
        "MMLU_clinical_knowledge":0.7358490566,
        "MMLU_college_biology":0.8125,
        "MMLU_college_chemistry":0.5,
        "MMLU_college_computer_science":0.62,
        "MMLU_college_mathematics":0.39,
        "MMLU_college_medicine":0.6705202312,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.73,
        "MMLU_conceptual_physics":0.6638297872,
        "MMLU_econometrics":0.4210526316,
        "MMLU_electrical_engineering":0.6275862069,
        "MMLU_elementary_mathematics":0.4656084656,
        "MMLU_formal_logic":0.4523809524,
        "MMLU_global_facts":0.46,
        "MMLU_high_school_biology":0.8290322581,
        "MMLU_high_school_chemistry":0.5320197044,
        "MMLU_high_school_computer_science":0.77,
        "MMLU_high_school_european_history":0.8242424242,
        "MMLU_high_school_geography":0.8939393939,
        "MMLU_high_school_government_and_politics":0.9274611399,
        "MMLU_high_school_macroeconomics":0.6948717949,
        "MMLU_high_school_mathematics":0.3185185185,
        "MMLU_high_school_microeconomics":0.7731092437,
        "MMLU_high_school_physics":0.4900662252,
        "MMLU_high_school_psychology":0.8862385321,
        "MMLU_high_school_statistics":0.5740740741,
        "MMLU_high_school_us_history":0.8970588235,
        "MMLU_high_school_world_history":0.8734177215,
        "MMLU_human_aging":0.7757847534,
        "MMLU_human_sexuality":0.8396946565,
        "MMLU_international_law":0.8512396694,
        "MMLU_jurisprudence":0.8240740741,
        "MMLU_logical_fallacies":0.8220858896,
        "MMLU_machine_learning":0.4732142857,
        "MMLU_management":0.8252427184,
        "MMLU_marketing":0.8888888889,
        "MMLU_medical_genetics":0.7,
        "MMLU_miscellaneous":0.8633461047,
        "MMLU_moral_disputes":0.7716763006,
        "MMLU_moral_scenarios":0.574301676,
        "MMLU_nutrition":0.7385620915,
        "MMLU_philosophy":0.7556270096,
        "MMLU_prehistory":0.799382716,
        "MMLU_professional_accounting":0.5567375887,
        "MMLU_professional_law":0.5645371578,
        "MMLU_professional_medicine":0.7426470588,
        "MMLU_professional_psychology":0.7418300654,
        "MMLU_public_relations":0.7545454545,
        "MMLU_security_studies":0.787755102,
        "MMLU_sociology":0.8805970149,
        "MMLU_us_foreign_policy":0.9,
        "MMLU_virology":0.5240963855,
        "MMLU_world_religions":0.865497076
    },
    {
        "Model":"llama-2-70B-LoRA-assemble-v2",
        "URL":"https:\/\/huggingface.co\/oh-yeontaek\/llama-2-70B-LoRA-assemble-v2",
        "full_model_name":"oh-yeontaek\/llama-2-70B-LoRA-assemble-v2",
        "Parameters":70.0,
        "MMLU_average":0.6937152178,
        "arc:challenge|25":0.683447099,
        "hellaswag|10":0.6721768572,
        "MMLU_abstract_algebra":0.43,
        "MMLU_anatomy":0.6148148148,
        "MMLU_astronomy":0.7828947368,
        "MMLU_business_ethics":0.75,
        "MMLU_clinical_knowledge":0.7433962264,
        "MMLU_college_biology":0.8194444444,
        "MMLU_college_chemistry":0.48,
        "MMLU_college_computer_science":0.61,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.6589595376,
        "MMLU_college_physics":0.362745098,
        "MMLU_computer_security":0.74,
        "MMLU_conceptual_physics":0.6553191489,
        "MMLU_econometrics":0.4122807018,
        "MMLU_electrical_engineering":0.6344827586,
        "MMLU_elementary_mathematics":0.4656084656,
        "MMLU_formal_logic":0.4603174603,
        "MMLU_global_facts":0.47,
        "MMLU_high_school_biology":0.8322580645,
        "MMLU_high_school_chemistry":0.5467980296,
        "MMLU_high_school_computer_science":0.78,
        "MMLU_high_school_european_history":0.8303030303,
        "MMLU_high_school_geography":0.904040404,
        "MMLU_high_school_government_and_politics":0.9274611399,
        "MMLU_high_school_macroeconomics":0.6974358974,
        "MMLU_high_school_mathematics":0.3259259259,
        "MMLU_high_school_microeconomics":0.781512605,
        "MMLU_high_school_physics":0.4701986755,
        "MMLU_high_school_psychology":0.8844036697,
        "MMLU_high_school_statistics":0.5648148148,
        "MMLU_high_school_us_history":0.8970588235,
        "MMLU_high_school_world_history":0.8776371308,
        "MMLU_human_aging":0.7668161435,
        "MMLU_human_sexuality":0.8244274809,
        "MMLU_international_law":0.8512396694,
        "MMLU_jurisprudence":0.8333333333,
        "MMLU_logical_fallacies":0.8220858896,
        "MMLU_machine_learning":0.4910714286,
        "MMLU_management":0.8252427184,
        "MMLU_marketing":0.8888888889,
        "MMLU_medical_genetics":0.71,
        "MMLU_miscellaneous":0.8659003831,
        "MMLU_moral_disputes":0.7774566474,
        "MMLU_moral_scenarios":0.5597765363,
        "MMLU_nutrition":0.7352941176,
        "MMLU_philosophy":0.7588424437,
        "MMLU_prehistory":0.7962962963,
        "MMLU_professional_accounting":0.5531914894,
        "MMLU_professional_law":0.5586701434,
        "MMLU_professional_medicine":0.7352941176,
        "MMLU_professional_psychology":0.7434640523,
        "MMLU_public_relations":0.7454545455,
        "MMLU_security_studies":0.7795918367,
        "MMLU_sociology":0.8855721393,
        "MMLU_us_foreign_policy":0.88,
        "MMLU_virology":0.5240963855,
        "MMLU_world_religions":0.8596491228
    },
    {
        "Model":"chronos007-70b",
        "URL":"https:\/\/huggingface.co\/elinas\/chronos007-70b",
        "full_model_name":"elinas\/chronos007-70b",
        "Parameters":70.0,
        "MMLU_average":0.6932563855,
        "arc:challenge|25":0.6527303754,
        "hellaswag|10":0.687412866,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.6444444444,
        "MMLU_astronomy":0.7828947368,
        "MMLU_business_ethics":0.74,
        "MMLU_clinical_knowledge":0.720754717,
        "MMLU_college_biology":0.8333333333,
        "MMLU_college_chemistry":0.5,
        "MMLU_college_computer_science":0.58,
        "MMLU_college_mathematics":0.42,
        "MMLU_college_medicine":0.6358381503,
        "MMLU_college_physics":0.3431372549,
        "MMLU_computer_security":0.79,
        "MMLU_conceptual_physics":0.6212765957,
        "MMLU_econometrics":0.4736842105,
        "MMLU_electrical_engineering":0.6068965517,
        "MMLU_elementary_mathematics":0.4365079365,
        "MMLU_formal_logic":0.5079365079,
        "MMLU_global_facts":0.47,
        "MMLU_high_school_biology":0.835483871,
        "MMLU_high_school_chemistry":0.5467980296,
        "MMLU_high_school_computer_science":0.75,
        "MMLU_high_school_european_history":0.8363636364,
        "MMLU_high_school_geography":0.8636363636,
        "MMLU_high_school_government_and_politics":0.9119170984,
        "MMLU_high_school_macroeconomics":0.6974358974,
        "MMLU_high_school_mathematics":0.3296296296,
        "MMLU_high_school_microeconomics":0.756302521,
        "MMLU_high_school_physics":0.5033112583,
        "MMLU_high_school_psychology":0.8825688073,
        "MMLU_high_school_statistics":0.5509259259,
        "MMLU_high_school_us_history":0.9117647059,
        "MMLU_high_school_world_history":0.8691983122,
        "MMLU_human_aging":0.7757847534,
        "MMLU_human_sexuality":0.8549618321,
        "MMLU_international_law":0.8842975207,
        "MMLU_jurisprudence":0.8240740741,
        "MMLU_logical_fallacies":0.7975460123,
        "MMLU_machine_learning":0.4910714286,
        "MMLU_management":0.8155339806,
        "MMLU_marketing":0.9145299145,
        "MMLU_medical_genetics":0.7,
        "MMLU_miscellaneous":0.8480204342,
        "MMLU_moral_disputes":0.7803468208,
        "MMLU_moral_scenarios":0.5508379888,
        "MMLU_nutrition":0.7418300654,
        "MMLU_philosophy":0.7491961415,
        "MMLU_prehistory":0.8086419753,
        "MMLU_professional_accounting":0.5425531915,
        "MMLU_professional_law":0.5417209909,
        "MMLU_professional_medicine":0.7169117647,
        "MMLU_professional_psychology":0.75,
        "MMLU_public_relations":0.7545454545,
        "MMLU_security_studies":0.787755102,
        "MMLU_sociology":0.8855721393,
        "MMLU_us_foreign_policy":0.9,
        "MMLU_virology":0.5481927711,
        "MMLU_world_religions":0.8596491228
    },
    {
        "Model":"MetaMath-70B-V1.0",
        "URL":"https:\/\/huggingface.co\/meta-math\/MetaMath-70B-V1.0",
        "full_model_name":"meta-math\/MetaMath-70B-V1.0",
        "Parameters":70.0,
        "MMLU_average":0.6930831213,
        "arc:challenge|25":0.6416382253,
        "hellaswag|10":0.6786496714,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.6148148148,
        "MMLU_astronomy":0.8223684211,
        "MMLU_business_ethics":0.75,
        "MMLU_clinical_knowledge":0.7283018868,
        "MMLU_college_biology":0.8541666667,
        "MMLU_college_chemistry":0.5,
        "MMLU_college_computer_science":0.59,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.6705202312,
        "MMLU_college_physics":0.362745098,
        "MMLU_computer_security":0.76,
        "MMLU_conceptual_physics":0.6808510638,
        "MMLU_econometrics":0.4210526316,
        "MMLU_electrical_engineering":0.5931034483,
        "MMLU_elementary_mathematics":0.4603174603,
        "MMLU_formal_logic":0.4523809524,
        "MMLU_global_facts":0.41,
        "MMLU_high_school_biology":0.8032258065,
        "MMLU_high_school_chemistry":0.5221674877,
        "MMLU_high_school_computer_science":0.75,
        "MMLU_high_school_european_history":0.8363636364,
        "MMLU_high_school_geography":0.8787878788,
        "MMLU_high_school_government_and_politics":0.9430051813,
        "MMLU_high_school_macroeconomics":0.7076923077,
        "MMLU_high_school_mathematics":0.3333333333,
        "MMLU_high_school_microeconomics":0.7647058824,
        "MMLU_high_school_physics":0.4238410596,
        "MMLU_high_school_psychology":0.8880733945,
        "MMLU_high_school_statistics":0.5740740741,
        "MMLU_high_school_us_history":0.931372549,
        "MMLU_high_school_world_history":0.8734177215,
        "MMLU_human_aging":0.798206278,
        "MMLU_human_sexuality":0.8625954198,
        "MMLU_international_law":0.867768595,
        "MMLU_jurisprudence":0.8333333333,
        "MMLU_logical_fallacies":0.8159509202,
        "MMLU_machine_learning":0.5267857143,
        "MMLU_management":0.8446601942,
        "MMLU_marketing":0.9017094017,
        "MMLU_medical_genetics":0.71,
        "MMLU_miscellaneous":0.8659003831,
        "MMLU_moral_disputes":0.7832369942,
        "MMLU_moral_scenarios":0.417877095,
        "MMLU_nutrition":0.7418300654,
        "MMLU_philosophy":0.7974276527,
        "MMLU_prehistory":0.8395061728,
        "MMLU_professional_accounting":0.5460992908,
        "MMLU_professional_law":0.5371577575,
        "MMLU_professional_medicine":0.7389705882,
        "MMLU_professional_psychology":0.7401960784,
        "MMLU_public_relations":0.7363636364,
        "MMLU_security_studies":0.8081632653,
        "MMLU_sociology":0.8955223881,
        "MMLU_us_foreign_policy":0.93,
        "MMLU_virology":0.5361445783,
        "MMLU_world_religions":0.8596491228
    },
    {
        "Model":"model_51",
        "URL":"https:\/\/huggingface.co\/psmathur\/model_51",
        "full_model_name":"psmathur\/model_51",
        "Parameters":null,
        "MMLU_average":0.6930797217,
        "arc:challenge|25":0.6484641638,
        "hellaswag|10":0.6812387971,
        "MMLU_abstract_algebra":0.38,
        "MMLU_anatomy":0.6148148148,
        "MMLU_astronomy":0.7828947368,
        "MMLU_business_ethics":0.75,
        "MMLU_clinical_knowledge":0.7320754717,
        "MMLU_college_biology":0.8472222222,
        "MMLU_college_chemistry":0.51,
        "MMLU_college_computer_science":0.55,
        "MMLU_college_mathematics":0.39,
        "MMLU_college_medicine":0.6242774566,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.78,
        "MMLU_conceptual_physics":0.6382978723,
        "MMLU_econometrics":0.4473684211,
        "MMLU_electrical_engineering":0.6137931034,
        "MMLU_elementary_mathematics":0.4656084656,
        "MMLU_formal_logic":0.4761904762,
        "MMLU_global_facts":0.46,
        "MMLU_high_school_biology":0.8258064516,
        "MMLU_high_school_chemistry":0.5270935961,
        "MMLU_high_school_computer_science":0.76,
        "MMLU_high_school_european_history":0.8181818182,
        "MMLU_high_school_geography":0.8737373737,
        "MMLU_high_school_government_and_politics":0.9378238342,
        "MMLU_high_school_macroeconomics":0.7128205128,
        "MMLU_high_school_mathematics":0.3333333333,
        "MMLU_high_school_microeconomics":0.756302521,
        "MMLU_high_school_physics":0.4768211921,
        "MMLU_high_school_psychology":0.8880733945,
        "MMLU_high_school_statistics":0.5925925926,
        "MMLU_high_school_us_history":0.9215686275,
        "MMLU_high_school_world_history":0.8734177215,
        "MMLU_human_aging":0.7713004484,
        "MMLU_human_sexuality":0.8396946565,
        "MMLU_international_law":0.8512396694,
        "MMLU_jurisprudence":0.8148148148,
        "MMLU_logical_fallacies":0.8036809816,
        "MMLU_machine_learning":0.4821428571,
        "MMLU_management":0.8155339806,
        "MMLU_marketing":0.8974358974,
        "MMLU_medical_genetics":0.7,
        "MMLU_miscellaneous":0.8722860792,
        "MMLU_moral_disputes":0.789017341,
        "MMLU_moral_scenarios":0.5709497207,
        "MMLU_nutrition":0.7385620915,
        "MMLU_philosophy":0.7717041801,
        "MMLU_prehistory":0.8209876543,
        "MMLU_professional_accounting":0.5531914894,
        "MMLU_professional_law":0.5456323338,
        "MMLU_professional_medicine":0.7316176471,
        "MMLU_professional_psychology":0.7581699346,
        "MMLU_public_relations":0.7272727273,
        "MMLU_security_studies":0.8,
        "MMLU_sociology":0.8457711443,
        "MMLU_us_foreign_policy":0.9,
        "MMLU_virology":0.5180722892,
        "MMLU_world_religions":0.8538011696
    },
    {
        "Model":"llama2-70B-qlora-gpt4",
        "URL":"https:\/\/huggingface.co\/liuxiang886\/llama2-70B-qlora-gpt4",
        "full_model_name":"liuxiang886\/llama2-70B-qlora-gpt4",
        "Parameters":70.0,
        "MMLU_average":0.6929151851,
        "arc:challenge|25":0.6535836177,
        "hellaswag|10":0.6748655646,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.6222222222,
        "MMLU_astronomy":0.7894736842,
        "MMLU_business_ethics":0.76,
        "MMLU_clinical_knowledge":0.7622641509,
        "MMLU_college_biology":0.8055555556,
        "MMLU_college_chemistry":0.48,
        "MMLU_college_computer_science":0.56,
        "MMLU_college_mathematics":0.43,
        "MMLU_college_medicine":0.6705202312,
        "MMLU_college_physics":0.4117647059,
        "MMLU_computer_security":0.77,
        "MMLU_conceptual_physics":0.6765957447,
        "MMLU_econometrics":0.4210526316,
        "MMLU_electrical_engineering":0.6344827586,
        "MMLU_elementary_mathematics":0.4497354497,
        "MMLU_formal_logic":0.4841269841,
        "MMLU_global_facts":0.44,
        "MMLU_high_school_biology":0.8451612903,
        "MMLU_high_school_chemistry":0.5073891626,
        "MMLU_high_school_computer_science":0.75,
        "MMLU_high_school_european_history":0.8363636364,
        "MMLU_high_school_geography":0.8888888889,
        "MMLU_high_school_government_and_politics":0.932642487,
        "MMLU_high_school_macroeconomics":0.7205128205,
        "MMLU_high_school_mathematics":0.3555555556,
        "MMLU_high_school_microeconomics":0.8025210084,
        "MMLU_high_school_physics":0.417218543,
        "MMLU_high_school_psychology":0.8880733945,
        "MMLU_high_school_statistics":0.5509259259,
        "MMLU_high_school_us_history":0.8921568627,
        "MMLU_high_school_world_history":0.8523206751,
        "MMLU_human_aging":0.7713004484,
        "MMLU_human_sexuality":0.8244274809,
        "MMLU_international_law":0.8512396694,
        "MMLU_jurisprudence":0.8148148148,
        "MMLU_logical_fallacies":0.7791411043,
        "MMLU_machine_learning":0.4553571429,
        "MMLU_management":0.8349514563,
        "MMLU_marketing":0.8888888889,
        "MMLU_medical_genetics":0.68,
        "MMLU_miscellaneous":0.8684546616,
        "MMLU_moral_disputes":0.7803468208,
        "MMLU_moral_scenarios":0.6592178771,
        "MMLU_nutrition":0.7810457516,
        "MMLU_philosophy":0.7556270096,
        "MMLU_prehistory":0.7839506173,
        "MMLU_professional_accounting":0.5390070922,
        "MMLU_professional_law":0.5462842243,
        "MMLU_professional_medicine":0.7022058824,
        "MMLU_professional_psychology":0.7450980392,
        "MMLU_public_relations":0.7181818182,
        "MMLU_security_studies":0.7755102041,
        "MMLU_sociology":0.8855721393,
        "MMLU_us_foreign_policy":0.9,
        "MMLU_virology":0.5542168675,
        "MMLU_world_religions":0.8538011696
    },
    {
        "Model":"llama-2-70b-fb16-korean",
        "URL":"https:\/\/huggingface.co\/quantumaikr\/llama-2-70b-fb16-korean",
        "full_model_name":"quantumaikr\/llama-2-70b-fb16-korean",
        "Parameters":70.0,
        "MMLU_average":0.692856135,
        "arc:challenge|25":0.6305460751,
        "hellaswag|10":0.6704839673,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.6444444444,
        "MMLU_astronomy":0.8223684211,
        "MMLU_business_ethics":0.75,
        "MMLU_clinical_knowledge":0.7320754717,
        "MMLU_college_biology":0.8125,
        "MMLU_college_chemistry":0.46,
        "MMLU_college_computer_science":0.61,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.6647398844,
        "MMLU_college_physics":0.3921568627,
        "MMLU_computer_security":0.77,
        "MMLU_conceptual_physics":0.685106383,
        "MMLU_econometrics":0.4912280702,
        "MMLU_electrical_engineering":0.6344827586,
        "MMLU_elementary_mathematics":0.4232804233,
        "MMLU_formal_logic":0.4523809524,
        "MMLU_global_facts":0.45,
        "MMLU_high_school_biology":0.7870967742,
        "MMLU_high_school_chemistry":0.5517241379,
        "MMLU_high_school_computer_science":0.79,
        "MMLU_high_school_european_history":0.8242424242,
        "MMLU_high_school_geography":0.8939393939,
        "MMLU_high_school_government_and_politics":0.9222797927,
        "MMLU_high_school_macroeconomics":0.7102564103,
        "MMLU_high_school_mathematics":0.3333333333,
        "MMLU_high_school_microeconomics":0.7478991597,
        "MMLU_high_school_physics":0.4900662252,
        "MMLU_high_school_psychology":0.8862385321,
        "MMLU_high_school_statistics":0.5694444444,
        "MMLU_high_school_us_history":0.887254902,
        "MMLU_high_school_world_history":0.8691983122,
        "MMLU_human_aging":0.8116591928,
        "MMLU_human_sexuality":0.8320610687,
        "MMLU_international_law":0.867768595,
        "MMLU_jurisprudence":0.8333333333,
        "MMLU_logical_fallacies":0.7914110429,
        "MMLU_machine_learning":0.5267857143,
        "MMLU_management":0.8058252427,
        "MMLU_marketing":0.9102564103,
        "MMLU_medical_genetics":0.68,
        "MMLU_miscellaneous":0.8633461047,
        "MMLU_moral_disputes":0.7601156069,
        "MMLU_moral_scenarios":0.4636871508,
        "MMLU_nutrition":0.7320261438,
        "MMLU_philosophy":0.7845659164,
        "MMLU_prehistory":0.8333333333,
        "MMLU_professional_accounting":0.5319148936,
        "MMLU_professional_law":0.5260756193,
        "MMLU_professional_medicine":0.7095588235,
        "MMLU_professional_psychology":0.7647058824,
        "MMLU_public_relations":0.7454545455,
        "MMLU_security_studies":0.7959183673,
        "MMLU_sociology":0.855721393,
        "MMLU_us_foreign_policy":0.9,
        "MMLU_virology":0.5481927711,
        "MMLU_world_religions":0.8713450292
    },
    {
        "Model":"llama-2-70B-chat",
        "URL":"https:\/\/huggingface.co\/quantumaikr\/llama-2-70B-chat",
        "full_model_name":"quantumaikr\/llama-2-70B-chat",
        "Parameters":70.0,
        "MMLU_average":0.6917697928,
        "arc:challenge|25":0.6407849829,
        "hellaswag|10":0.6809400518,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.6,
        "MMLU_astronomy":0.7960526316,
        "MMLU_business_ethics":0.72,
        "MMLU_clinical_knowledge":0.7056603774,
        "MMLU_college_biology":0.8194444444,
        "MMLU_college_chemistry":0.46,
        "MMLU_college_computer_science":0.53,
        "MMLU_college_mathematics":0.41,
        "MMLU_college_medicine":0.6473988439,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.75,
        "MMLU_conceptual_physics":0.6340425532,
        "MMLU_econometrics":0.4473684211,
        "MMLU_electrical_engineering":0.5862068966,
        "MMLU_elementary_mathematics":0.455026455,
        "MMLU_formal_logic":0.5555555556,
        "MMLU_global_facts":0.46,
        "MMLU_high_school_biology":0.7935483871,
        "MMLU_high_school_chemistry":0.5467980296,
        "MMLU_high_school_computer_science":0.77,
        "MMLU_high_school_european_history":0.8545454545,
        "MMLU_high_school_geography":0.8585858586,
        "MMLU_high_school_government_and_politics":0.9430051813,
        "MMLU_high_school_macroeconomics":0.7025641026,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.7647058824,
        "MMLU_high_school_physics":0.4966887417,
        "MMLU_high_school_psychology":0.8917431193,
        "MMLU_high_school_statistics":0.5972222222,
        "MMLU_high_school_us_history":0.9117647059,
        "MMLU_high_school_world_history":0.9071729958,
        "MMLU_human_aging":0.7937219731,
        "MMLU_human_sexuality":0.7786259542,
        "MMLU_international_law":0.8925619835,
        "MMLU_jurisprudence":0.7962962963,
        "MMLU_logical_fallacies":0.8159509202,
        "MMLU_machine_learning":0.5982142857,
        "MMLU_management":0.8446601942,
        "MMLU_marketing":0.9145299145,
        "MMLU_medical_genetics":0.71,
        "MMLU_miscellaneous":0.8595146871,
        "MMLU_moral_disputes":0.7572254335,
        "MMLU_moral_scenarios":0.5251396648,
        "MMLU_nutrition":0.7418300654,
        "MMLU_philosophy":0.7588424437,
        "MMLU_prehistory":0.8271604938,
        "MMLU_professional_accounting":0.5744680851,
        "MMLU_professional_law":0.5664928292,
        "MMLU_professional_medicine":0.7095588235,
        "MMLU_professional_psychology":0.7581699346,
        "MMLU_public_relations":0.7454545455,
        "MMLU_security_studies":0.7673469388,
        "MMLU_sociology":0.8706467662,
        "MMLU_us_foreign_policy":0.86,
        "MMLU_virology":0.5421686747,
        "MMLU_world_religions":0.865497076
    },
    {
        "Model":"llama-2-70b-dolphin-peft",
        "URL":"https:\/\/huggingface.co\/dfurman\/llama-2-70b-dolphin-peft",
        "full_model_name":"dfurman\/llama-2-70b-dolphin-peft",
        "Parameters":70.0,
        "MMLU_average":0.6917628186,
        "arc:challenge|25":0.6527303754,
        "hellaswag|10":0.6691894045,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.637037037,
        "MMLU_astronomy":0.8223684211,
        "MMLU_business_ethics":0.7,
        "MMLU_clinical_knowledge":0.7169811321,
        "MMLU_college_biology":0.8055555556,
        "MMLU_college_chemistry":0.48,
        "MMLU_college_computer_science":0.57,
        "MMLU_college_mathematics":0.39,
        "MMLU_college_medicine":0.6416184971,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.79,
        "MMLU_conceptual_physics":0.6638297872,
        "MMLU_econometrics":0.4298245614,
        "MMLU_electrical_engineering":0.6206896552,
        "MMLU_elementary_mathematics":0.4391534392,
        "MMLU_formal_logic":0.4603174603,
        "MMLU_global_facts":0.46,
        "MMLU_high_school_biology":0.7870967742,
        "MMLU_high_school_chemistry":0.5123152709,
        "MMLU_high_school_computer_science":0.75,
        "MMLU_high_school_european_history":0.8363636364,
        "MMLU_high_school_geography":0.898989899,
        "MMLU_high_school_government_and_politics":0.932642487,
        "MMLU_high_school_macroeconomics":0.6871794872,
        "MMLU_high_school_mathematics":0.3407407407,
        "MMLU_high_school_microeconomics":0.7521008403,
        "MMLU_high_school_physics":0.4966887417,
        "MMLU_high_school_psychology":0.8917431193,
        "MMLU_high_school_statistics":0.5601851852,
        "MMLU_high_school_us_history":0.9019607843,
        "MMLU_high_school_world_history":0.8776371308,
        "MMLU_human_aging":0.7892376682,
        "MMLU_human_sexuality":0.8320610687,
        "MMLU_international_law":0.8842975207,
        "MMLU_jurisprudence":0.8425925926,
        "MMLU_logical_fallacies":0.7914110429,
        "MMLU_machine_learning":0.5,
        "MMLU_management":0.8252427184,
        "MMLU_marketing":0.905982906,
        "MMLU_medical_genetics":0.72,
        "MMLU_miscellaneous":0.8659003831,
        "MMLU_moral_disputes":0.7514450867,
        "MMLU_moral_scenarios":0.5363128492,
        "MMLU_nutrition":0.7320261438,
        "MMLU_philosophy":0.7717041801,
        "MMLU_prehistory":0.8209876543,
        "MMLU_professional_accounting":0.5390070922,
        "MMLU_professional_law":0.5404172099,
        "MMLU_professional_medicine":0.7352941176,
        "MMLU_professional_psychology":0.7434640523,
        "MMLU_public_relations":0.7272727273,
        "MMLU_security_studies":0.7959183673,
        "MMLU_sociology":0.8706467662,
        "MMLU_us_foreign_policy":0.91,
        "MMLU_virology":0.5481927711,
        "MMLU_world_religions":0.865497076
    },
    {
        "Model":"jfdslijsijdgis",
        "URL":"https:\/\/huggingface.co\/lloorree\/jfdslijsijdgis",
        "full_model_name":"lloorree\/jfdslijsijdgis",
        "Parameters":null,
        "MMLU_average":0.6917345225,
        "arc:challenge|25":0.6518771331,
        "hellaswag|10":0.6760605457,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.6296296296,
        "MMLU_astronomy":0.8223684211,
        "MMLU_business_ethics":0.74,
        "MMLU_clinical_knowledge":0.7132075472,
        "MMLU_college_biology":0.8125,
        "MMLU_college_chemistry":0.5,
        "MMLU_college_computer_science":0.54,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.6705202312,
        "MMLU_college_physics":0.3823529412,
        "MMLU_computer_security":0.78,
        "MMLU_conceptual_physics":0.6638297872,
        "MMLU_econometrics":0.4385964912,
        "MMLU_electrical_engineering":0.6206896552,
        "MMLU_elementary_mathematics":0.4444444444,
        "MMLU_formal_logic":0.4444444444,
        "MMLU_global_facts":0.47,
        "MMLU_high_school_biology":0.8064516129,
        "MMLU_high_school_chemistry":0.5221674877,
        "MMLU_high_school_computer_science":0.77,
        "MMLU_high_school_european_history":0.8484848485,
        "MMLU_high_school_geography":0.8636363636,
        "MMLU_high_school_government_and_politics":0.9533678756,
        "MMLU_high_school_macroeconomics":0.7153846154,
        "MMLU_high_school_mathematics":0.2888888889,
        "MMLU_high_school_microeconomics":0.781512605,
        "MMLU_high_school_physics":0.4238410596,
        "MMLU_high_school_psychology":0.8935779817,
        "MMLU_high_school_statistics":0.6018518519,
        "MMLU_high_school_us_history":0.9117647059,
        "MMLU_high_school_world_history":0.8565400844,
        "MMLU_human_aging":0.798206278,
        "MMLU_human_sexuality":0.8473282443,
        "MMLU_international_law":0.8595041322,
        "MMLU_jurisprudence":0.8425925926,
        "MMLU_logical_fallacies":0.8159509202,
        "MMLU_machine_learning":0.5357142857,
        "MMLU_management":0.8252427184,
        "MMLU_marketing":0.8760683761,
        "MMLU_medical_genetics":0.72,
        "MMLU_miscellaneous":0.8659003831,
        "MMLU_moral_disputes":0.7658959538,
        "MMLU_moral_scenarios":0.5117318436,
        "MMLU_nutrition":0.7385620915,
        "MMLU_philosophy":0.7845659164,
        "MMLU_prehistory":0.8240740741,
        "MMLU_professional_accounting":0.5177304965,
        "MMLU_professional_law":0.5397653194,
        "MMLU_professional_medicine":0.7169117647,
        "MMLU_professional_psychology":0.75,
        "MMLU_public_relations":0.6909090909,
        "MMLU_security_studies":0.812244898,
        "MMLU_sociology":0.9004975124,
        "MMLU_us_foreign_policy":0.89,
        "MMLU_virology":0.5180722892,
        "MMLU_world_religions":0.8713450292
    },
    {
        "Model":"airoboros-l2-70b-2.1",
        "URL":"https:\/\/huggingface.co\/jondurbin\/airoboros-l2-70b-2.1",
        "full_model_name":"jondurbin\/airoboros-l2-70b-2.1",
        "Parameters":70.0,
        "MMLU_average":0.6917324589,
        "arc:challenge|25":0.6697952218,
        "hellaswag|10":0.6769567815,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.5851851852,
        "MMLU_astronomy":0.8289473684,
        "MMLU_business_ethics":0.75,
        "MMLU_clinical_knowledge":0.7132075472,
        "MMLU_college_biology":0.8263888889,
        "MMLU_college_chemistry":0.52,
        "MMLU_college_computer_science":0.51,
        "MMLU_college_mathematics":0.4,
        "MMLU_college_medicine":0.6705202312,
        "MMLU_college_physics":0.3823529412,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.6978723404,
        "MMLU_econometrics":0.4122807018,
        "MMLU_electrical_engineering":0.5793103448,
        "MMLU_elementary_mathematics":0.4285714286,
        "MMLU_formal_logic":0.4841269841,
        "MMLU_global_facts":0.46,
        "MMLU_high_school_biology":0.8451612903,
        "MMLU_high_school_chemistry":0.5221674877,
        "MMLU_high_school_computer_science":0.73,
        "MMLU_high_school_european_history":0.8181818182,
        "MMLU_high_school_geography":0.8737373737,
        "MMLU_high_school_government_and_politics":0.9481865285,
        "MMLU_high_school_macroeconomics":0.7333333333,
        "MMLU_high_school_mathematics":0.3555555556,
        "MMLU_high_school_microeconomics":0.7731092437,
        "MMLU_high_school_physics":0.4370860927,
        "MMLU_high_school_psychology":0.8880733945,
        "MMLU_high_school_statistics":0.587962963,
        "MMLU_high_school_us_history":0.9068627451,
        "MMLU_high_school_world_history":0.8691983122,
        "MMLU_human_aging":0.7937219731,
        "MMLU_human_sexuality":0.8320610687,
        "MMLU_international_law":0.8925619835,
        "MMLU_jurisprudence":0.8240740741,
        "MMLU_logical_fallacies":0.8098159509,
        "MMLU_machine_learning":0.5178571429,
        "MMLU_management":0.8058252427,
        "MMLU_marketing":0.8675213675,
        "MMLU_medical_genetics":0.67,
        "MMLU_miscellaneous":0.8492975734,
        "MMLU_moral_disputes":0.7774566474,
        "MMLU_moral_scenarios":0.505027933,
        "MMLU_nutrition":0.7712418301,
        "MMLU_philosophy":0.7749196141,
        "MMLU_prehistory":0.8117283951,
        "MMLU_professional_accounting":0.5602836879,
        "MMLU_professional_law":0.5508474576,
        "MMLU_professional_medicine":0.7389705882,
        "MMLU_professional_psychology":0.7565359477,
        "MMLU_public_relations":0.7272727273,
        "MMLU_security_studies":0.787755102,
        "MMLU_sociology":0.9104477612,
        "MMLU_us_foreign_policy":0.91,
        "MMLU_virology":0.5481927711,
        "MMLU_world_religions":0.8479532164
    },
    {
        "Model":"model_007",
        "URL":"https:\/\/huggingface.co\/pankajmathur\/model_007",
        "full_model_name":"pankajmathur\/model_007",
        "Parameters":null,
        "MMLU_average":0.6904044577,
        "arc:challenge|25":0.6749146758,
        "hellaswag|10":0.6908982274,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.6444444444,
        "MMLU_astronomy":0.8157894737,
        "MMLU_business_ethics":0.76,
        "MMLU_clinical_knowledge":0.7320754717,
        "MMLU_college_biology":0.8125,
        "MMLU_college_chemistry":0.48,
        "MMLU_college_computer_science":0.56,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.6531791908,
        "MMLU_college_physics":0.362745098,
        "MMLU_computer_security":0.76,
        "MMLU_conceptual_physics":0.6680851064,
        "MMLU_econometrics":0.4298245614,
        "MMLU_electrical_engineering":0.6068965517,
        "MMLU_elementary_mathematics":0.455026455,
        "MMLU_formal_logic":0.4682539683,
        "MMLU_global_facts":0.45,
        "MMLU_high_school_biology":0.8129032258,
        "MMLU_high_school_chemistry":0.5369458128,
        "MMLU_high_school_computer_science":0.75,
        "MMLU_high_school_european_history":0.8666666667,
        "MMLU_high_school_geography":0.8888888889,
        "MMLU_high_school_government_and_politics":0.932642487,
        "MMLU_high_school_macroeconomics":0.7102564103,
        "MMLU_high_school_mathematics":0.3222222222,
        "MMLU_high_school_microeconomics":0.743697479,
        "MMLU_high_school_physics":0.4768211921,
        "MMLU_high_school_psychology":0.8899082569,
        "MMLU_high_school_statistics":0.5416666667,
        "MMLU_high_school_us_history":0.9215686275,
        "MMLU_high_school_world_history":0.8734177215,
        "MMLU_human_aging":0.798206278,
        "MMLU_human_sexuality":0.8167938931,
        "MMLU_international_law":0.867768595,
        "MMLU_jurisprudence":0.8240740741,
        "MMLU_logical_fallacies":0.7791411043,
        "MMLU_machine_learning":0.4910714286,
        "MMLU_management":0.8058252427,
        "MMLU_marketing":0.9017094017,
        "MMLU_medical_genetics":0.69,
        "MMLU_miscellaneous":0.8748403576,
        "MMLU_moral_disputes":0.7630057803,
        "MMLU_moral_scenarios":0.5497206704,
        "MMLU_nutrition":0.7254901961,
        "MMLU_philosophy":0.768488746,
        "MMLU_prehistory":0.8055555556,
        "MMLU_professional_accounting":0.5460992908,
        "MMLU_professional_law":0.5456323338,
        "MMLU_professional_medicine":0.7316176471,
        "MMLU_professional_psychology":0.75,
        "MMLU_public_relations":0.7363636364,
        "MMLU_security_studies":0.7918367347,
        "MMLU_sociology":0.855721393,
        "MMLU_us_foreign_policy":0.86,
        "MMLU_virology":0.5421686747,
        "MMLU_world_religions":0.865497076
    },
    {
        "Model":"model_007",
        "URL":"https:\/\/huggingface.co\/psmathur\/model_007",
        "full_model_name":"psmathur\/model_007",
        "Parameters":null,
        "MMLU_average":0.6904044577,
        "arc:challenge|25":0.6749146758,
        "hellaswag|10":0.6908982274,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.6444444444,
        "MMLU_astronomy":0.8157894737,
        "MMLU_business_ethics":0.76,
        "MMLU_clinical_knowledge":0.7320754717,
        "MMLU_college_biology":0.8125,
        "MMLU_college_chemistry":0.48,
        "MMLU_college_computer_science":0.56,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.6531791908,
        "MMLU_college_physics":0.362745098,
        "MMLU_computer_security":0.76,
        "MMLU_conceptual_physics":0.6680851064,
        "MMLU_econometrics":0.4298245614,
        "MMLU_electrical_engineering":0.6068965517,
        "MMLU_elementary_mathematics":0.455026455,
        "MMLU_formal_logic":0.4682539683,
        "MMLU_global_facts":0.45,
        "MMLU_high_school_biology":0.8129032258,
        "MMLU_high_school_chemistry":0.5369458128,
        "MMLU_high_school_computer_science":0.75,
        "MMLU_high_school_european_history":0.8666666667,
        "MMLU_high_school_geography":0.8888888889,
        "MMLU_high_school_government_and_politics":0.932642487,
        "MMLU_high_school_macroeconomics":0.7102564103,
        "MMLU_high_school_mathematics":0.3222222222,
        "MMLU_high_school_microeconomics":0.743697479,
        "MMLU_high_school_physics":0.4768211921,
        "MMLU_high_school_psychology":0.8899082569,
        "MMLU_high_school_statistics":0.5416666667,
        "MMLU_high_school_us_history":0.9215686275,
        "MMLU_high_school_world_history":0.8734177215,
        "MMLU_human_aging":0.798206278,
        "MMLU_human_sexuality":0.8167938931,
        "MMLU_international_law":0.867768595,
        "MMLU_jurisprudence":0.8240740741,
        "MMLU_logical_fallacies":0.7791411043,
        "MMLU_machine_learning":0.4910714286,
        "MMLU_management":0.8058252427,
        "MMLU_marketing":0.9017094017,
        "MMLU_medical_genetics":0.69,
        "MMLU_miscellaneous":0.8748403576,
        "MMLU_moral_disputes":0.7630057803,
        "MMLU_moral_scenarios":0.5497206704,
        "MMLU_nutrition":0.7254901961,
        "MMLU_philosophy":0.768488746,
        "MMLU_prehistory":0.8055555556,
        "MMLU_professional_accounting":0.5460992908,
        "MMLU_professional_law":0.5456323338,
        "MMLU_professional_medicine":0.7316176471,
        "MMLU_professional_psychology":0.75,
        "MMLU_public_relations":0.7363636364,
        "MMLU_security_studies":0.7918367347,
        "MMLU_sociology":0.855721393,
        "MMLU_us_foreign_policy":0.86,
        "MMLU_virology":0.5421686747,
        "MMLU_world_religions":0.865497076
    },
    {
        "Model":"LLaMA-2-Wizard-70B-QLoRA",
        "URL":"https:\/\/huggingface.co\/v2ray\/LLaMA-2-Wizard-70B-QLoRA",
        "full_model_name":"v2ray\/LLaMA-2-Wizard-70B-QLoRA",
        "Parameters":70.0,
        "MMLU_average":0.6895859316,
        "arc:challenge|25":0.6433447099,
        "hellaswag|10":0.6948814977,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.6148148148,
        "MMLU_astronomy":0.7894736842,
        "MMLU_business_ethics":0.69,
        "MMLU_clinical_knowledge":0.7169811321,
        "MMLU_college_biology":0.8333333333,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.53,
        "MMLU_college_mathematics":0.44,
        "MMLU_college_medicine":0.6358381503,
        "MMLU_college_physics":0.4019607843,
        "MMLU_computer_security":0.78,
        "MMLU_conceptual_physics":0.6723404255,
        "MMLU_econometrics":0.4210526316,
        "MMLU_electrical_engineering":0.5931034483,
        "MMLU_elementary_mathematics":0.4391534392,
        "MMLU_formal_logic":0.4603174603,
        "MMLU_global_facts":0.49,
        "MMLU_high_school_biology":0.8161290323,
        "MMLU_high_school_chemistry":0.4876847291,
        "MMLU_high_school_computer_science":0.76,
        "MMLU_high_school_european_history":0.8242424242,
        "MMLU_high_school_geography":0.8636363636,
        "MMLU_high_school_government_and_politics":0.9430051813,
        "MMLU_high_school_macroeconomics":0.7051282051,
        "MMLU_high_school_mathematics":0.3148148148,
        "MMLU_high_school_microeconomics":0.7605042017,
        "MMLU_high_school_physics":0.4238410596,
        "MMLU_high_school_psychology":0.8917431193,
        "MMLU_high_school_statistics":0.5740740741,
        "MMLU_high_school_us_history":0.9166666667,
        "MMLU_high_school_world_history":0.864978903,
        "MMLU_human_aging":0.7802690583,
        "MMLU_human_sexuality":0.8473282443,
        "MMLU_international_law":0.867768595,
        "MMLU_jurisprudence":0.8055555556,
        "MMLU_logical_fallacies":0.8282208589,
        "MMLU_machine_learning":0.5267857143,
        "MMLU_management":0.8252427184,
        "MMLU_marketing":0.9017094017,
        "MMLU_medical_genetics":0.7,
        "MMLU_miscellaneous":0.8531289911,
        "MMLU_moral_disputes":0.7803468208,
        "MMLU_moral_scenarios":0.4972067039,
        "MMLU_nutrition":0.7483660131,
        "MMLU_philosophy":0.7652733119,
        "MMLU_prehistory":0.8086419753,
        "MMLU_professional_accounting":0.5567375887,
        "MMLU_professional_law":0.5547588005,
        "MMLU_professional_medicine":0.7389705882,
        "MMLU_professional_psychology":0.7516339869,
        "MMLU_public_relations":0.7272727273,
        "MMLU_security_studies":0.7714285714,
        "MMLU_sociology":0.8855721393,
        "MMLU_us_foreign_policy":0.91,
        "MMLU_virology":0.5421686747,
        "MMLU_world_religions":0.8771929825
    },
    {
        "Model":"WizardMath-70B-V1.0",
        "URL":"https:\/\/huggingface.co\/WizardLM\/WizardMath-70B-V1.0",
        "full_model_name":"WizardLM\/WizardMath-70B-V1.0",
        "Parameters":70.0,
        "MMLU_average":0.6892451427,
        "arc:challenge|25":0.6407849829,
        "hellaswag|10":0.684126668,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.6444444444,
        "MMLU_astronomy":0.7960526316,
        "MMLU_business_ethics":0.71,
        "MMLU_clinical_knowledge":0.7245283019,
        "MMLU_college_biology":0.8125,
        "MMLU_college_chemistry":0.48,
        "MMLU_college_computer_science":0.54,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.6589595376,
        "MMLU_college_physics":0.3823529412,
        "MMLU_computer_security":0.74,
        "MMLU_conceptual_physics":0.6723404255,
        "MMLU_econometrics":0.4035087719,
        "MMLU_electrical_engineering":0.6206896552,
        "MMLU_elementary_mathematics":0.4312169312,
        "MMLU_formal_logic":0.4603174603,
        "MMLU_global_facts":0.45,
        "MMLU_high_school_biology":0.8193548387,
        "MMLU_high_school_chemistry":0.5369458128,
        "MMLU_high_school_computer_science":0.7,
        "MMLU_high_school_european_history":0.8,
        "MMLU_high_school_geography":0.8787878788,
        "MMLU_high_school_government_and_politics":0.9378238342,
        "MMLU_high_school_macroeconomics":0.7179487179,
        "MMLU_high_school_mathematics":0.3333333333,
        "MMLU_high_school_microeconomics":0.8025210084,
        "MMLU_high_school_physics":0.3973509934,
        "MMLU_high_school_psychology":0.895412844,
        "MMLU_high_school_statistics":0.5694444444,
        "MMLU_high_school_us_history":0.9068627451,
        "MMLU_high_school_world_history":0.864978903,
        "MMLU_human_aging":0.7757847534,
        "MMLU_human_sexuality":0.8778625954,
        "MMLU_international_law":0.8595041322,
        "MMLU_jurisprudence":0.8148148148,
        "MMLU_logical_fallacies":0.8036809816,
        "MMLU_machine_learning":0.4821428571,
        "MMLU_management":0.8349514563,
        "MMLU_marketing":0.8888888889,
        "MMLU_medical_genetics":0.71,
        "MMLU_miscellaneous":0.8620689655,
        "MMLU_moral_disputes":0.7658959538,
        "MMLU_moral_scenarios":0.548603352,
        "MMLU_nutrition":0.7581699346,
        "MMLU_philosophy":0.768488746,
        "MMLU_prehistory":0.8364197531,
        "MMLU_professional_accounting":0.5354609929,
        "MMLU_professional_law":0.5684485007,
        "MMLU_professional_medicine":0.75,
        "MMLU_professional_psychology":0.7516339869,
        "MMLU_public_relations":0.7363636364,
        "MMLU_security_studies":0.7714285714,
        "MMLU_sociology":0.8955223881,
        "MMLU_us_foreign_policy":0.92,
        "MMLU_virology":0.5301204819,
        "MMLU_world_religions":0.8830409357
    },
    {
        "Model":"Synthia-70B",
        "URL":"https:\/\/huggingface.co\/migtissera\/Synthia-70B",
        "full_model_name":"migtissera\/Synthia-70B",
        "Parameters":70.0,
        "MMLU_average":0.6890922391,
        "arc:challenge|25":0.6587030717,
        "hellaswag|10":0.6826329416,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.6222222222,
        "MMLU_astronomy":0.7960526316,
        "MMLU_business_ethics":0.75,
        "MMLU_clinical_knowledge":0.720754717,
        "MMLU_college_biology":0.8055555556,
        "MMLU_college_chemistry":0.48,
        "MMLU_college_computer_science":0.55,
        "MMLU_college_mathematics":0.44,
        "MMLU_college_medicine":0.6242774566,
        "MMLU_college_physics":0.362745098,
        "MMLU_computer_security":0.76,
        "MMLU_conceptual_physics":0.6468085106,
        "MMLU_econometrics":0.4561403509,
        "MMLU_electrical_engineering":0.5931034483,
        "MMLU_elementary_mathematics":0.4391534392,
        "MMLU_formal_logic":0.4682539683,
        "MMLU_global_facts":0.48,
        "MMLU_high_school_biology":0.8064516129,
        "MMLU_high_school_chemistry":0.5172413793,
        "MMLU_high_school_computer_science":0.77,
        "MMLU_high_school_european_history":0.8181818182,
        "MMLU_high_school_geography":0.8585858586,
        "MMLU_high_school_government_and_politics":0.9274611399,
        "MMLU_high_school_macroeconomics":0.6871794872,
        "MMLU_high_school_mathematics":0.3481481481,
        "MMLU_high_school_microeconomics":0.756302521,
        "MMLU_high_school_physics":0.4768211921,
        "MMLU_high_school_psychology":0.8880733945,
        "MMLU_high_school_statistics":0.5833333333,
        "MMLU_high_school_us_history":0.9166666667,
        "MMLU_high_school_world_history":0.8691983122,
        "MMLU_human_aging":0.8161434978,
        "MMLU_human_sexuality":0.8244274809,
        "MMLU_international_law":0.8512396694,
        "MMLU_jurisprudence":0.8148148148,
        "MMLU_logical_fallacies":0.7975460123,
        "MMLU_machine_learning":0.5535714286,
        "MMLU_management":0.8058252427,
        "MMLU_marketing":0.9188034188,
        "MMLU_medical_genetics":0.74,
        "MMLU_miscellaneous":0.8722860792,
        "MMLU_moral_disputes":0.7716763006,
        "MMLU_moral_scenarios":0.4391061453,
        "MMLU_nutrition":0.7450980392,
        "MMLU_philosophy":0.7588424437,
        "MMLU_prehistory":0.8086419753,
        "MMLU_professional_accounting":0.5425531915,
        "MMLU_professional_law":0.5528031291,
        "MMLU_professional_medicine":0.7132352941,
        "MMLU_professional_psychology":0.7516339869,
        "MMLU_public_relations":0.7363636364,
        "MMLU_security_studies":0.7795918367,
        "MMLU_sociology":0.8855721393,
        "MMLU_us_foreign_policy":0.88,
        "MMLU_virology":0.5301204819,
        "MMLU_world_religions":0.8596491228
    },
    {
        "Model":"Euryale-L2-70B",
        "URL":"https:\/\/huggingface.co\/Sao10K\/Euryale-L2-70B",
        "full_model_name":"Sao10K\/Euryale-L2-70B",
        "Parameters":70.0,
        "MMLU_average":0.6884428008,
        "arc:challenge|25":0.6493174061,
        "hellaswag|10":0.6826329416,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.6148148148,
        "MMLU_astronomy":0.75,
        "MMLU_business_ethics":0.73,
        "MMLU_clinical_knowledge":0.7094339623,
        "MMLU_college_biology":0.8472222222,
        "MMLU_college_chemistry":0.48,
        "MMLU_college_computer_science":0.58,
        "MMLU_college_mathematics":0.39,
        "MMLU_college_medicine":0.6300578035,
        "MMLU_college_physics":0.3921568627,
        "MMLU_computer_security":0.78,
        "MMLU_conceptual_physics":0.6382978723,
        "MMLU_econometrics":0.4561403509,
        "MMLU_electrical_engineering":0.6137931034,
        "MMLU_elementary_mathematics":0.4417989418,
        "MMLU_formal_logic":0.4841269841,
        "MMLU_global_facts":0.49,
        "MMLU_high_school_biology":0.8225806452,
        "MMLU_high_school_chemistry":0.5123152709,
        "MMLU_high_school_computer_science":0.74,
        "MMLU_high_school_european_history":0.8121212121,
        "MMLU_high_school_geography":0.8838383838,
        "MMLU_high_school_government_and_politics":0.932642487,
        "MMLU_high_school_macroeconomics":0.7102564103,
        "MMLU_high_school_mathematics":0.3481481481,
        "MMLU_high_school_microeconomics":0.7478991597,
        "MMLU_high_school_physics":0.4635761589,
        "MMLU_high_school_psychology":0.8733944954,
        "MMLU_high_school_statistics":0.5601851852,
        "MMLU_high_school_us_history":0.9215686275,
        "MMLU_high_school_world_history":0.8607594937,
        "MMLU_human_aging":0.7668161435,
        "MMLU_human_sexuality":0.8167938931,
        "MMLU_international_law":0.8760330579,
        "MMLU_jurisprudence":0.8055555556,
        "MMLU_logical_fallacies":0.8159509202,
        "MMLU_machine_learning":0.4821428571,
        "MMLU_management":0.8252427184,
        "MMLU_marketing":0.8931623932,
        "MMLU_medical_genetics":0.69,
        "MMLU_miscellaneous":0.8441890166,
        "MMLU_moral_disputes":0.774566474,
        "MMLU_moral_scenarios":0.505027933,
        "MMLU_nutrition":0.7450980392,
        "MMLU_philosophy":0.7845659164,
        "MMLU_prehistory":0.799382716,
        "MMLU_professional_accounting":0.5531914894,
        "MMLU_professional_law":0.5306388527,
        "MMLU_professional_medicine":0.7169117647,
        "MMLU_professional_psychology":0.7205882353,
        "MMLU_public_relations":0.7636363636,
        "MMLU_security_studies":0.7714285714,
        "MMLU_sociology":0.8955223881,
        "MMLU_us_foreign_policy":0.93,
        "MMLU_virology":0.5421686747,
        "MMLU_world_religions":0.865497076
    },
    {
        "Model":"StableBeluga2",
        "URL":"https:\/\/huggingface.co\/stabilityai\/StableBeluga2",
        "full_model_name":"stabilityai\/StableBeluga2",
        "Parameters":null,
        "MMLU_average":0.6879488672,
        "arc:challenge|25":0.659556314,
        "hellaswag|10":0.6744672376,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.6296296296,
        "MMLU_astronomy":0.7894736842,
        "MMLU_business_ethics":0.75,
        "MMLU_clinical_knowledge":0.758490566,
        "MMLU_college_biology":0.7916666667,
        "MMLU_college_chemistry":0.46,
        "MMLU_college_computer_science":0.54,
        "MMLU_college_mathematics":0.44,
        "MMLU_college_medicine":0.6531791908,
        "MMLU_college_physics":0.3921568627,
        "MMLU_computer_security":0.75,
        "MMLU_conceptual_physics":0.6510638298,
        "MMLU_econometrics":0.4649122807,
        "MMLU_electrical_engineering":0.6275862069,
        "MMLU_elementary_mathematics":0.4391534392,
        "MMLU_formal_logic":0.5,
        "MMLU_global_facts":0.43,
        "MMLU_high_school_biology":0.8322580645,
        "MMLU_high_school_chemistry":0.5024630542,
        "MMLU_high_school_computer_science":0.75,
        "MMLU_high_school_european_history":0.8424242424,
        "MMLU_high_school_geography":0.8737373737,
        "MMLU_high_school_government_and_politics":0.9274611399,
        "MMLU_high_school_macroeconomics":0.7076923077,
        "MMLU_high_school_mathematics":0.3296296296,
        "MMLU_high_school_microeconomics":0.8109243697,
        "MMLU_high_school_physics":0.4238410596,
        "MMLU_high_school_psychology":0.8880733945,
        "MMLU_high_school_statistics":0.5694444444,
        "MMLU_high_school_us_history":0.887254902,
        "MMLU_high_school_world_history":0.8607594937,
        "MMLU_human_aging":0.7802690583,
        "MMLU_human_sexuality":0.8473282443,
        "MMLU_international_law":0.8347107438,
        "MMLU_jurisprudence":0.8240740741,
        "MMLU_logical_fallacies":0.7975460123,
        "MMLU_machine_learning":0.4642857143,
        "MMLU_management":0.8252427184,
        "MMLU_marketing":0.8931623932,
        "MMLU_medical_genetics":0.67,
        "MMLU_miscellaneous":0.8607918263,
        "MMLU_moral_disputes":0.774566474,
        "MMLU_moral_scenarios":0.6078212291,
        "MMLU_nutrition":0.7712418301,
        "MMLU_philosophy":0.7556270096,
        "MMLU_prehistory":0.7962962963,
        "MMLU_professional_accounting":0.5177304965,
        "MMLU_professional_law":0.5462842243,
        "MMLU_professional_medicine":0.7095588235,
        "MMLU_professional_psychology":0.7467320261,
        "MMLU_public_relations":0.6818181818,
        "MMLU_security_studies":0.7551020408,
        "MMLU_sociology":0.8955223881,
        "MMLU_us_foreign_policy":0.89,
        "MMLU_virology":0.5361445783,
        "MMLU_world_religions":0.8479532164
    },
    {
        "Model":"llama2_70b_chat_uncensored",
        "URL":"https:\/\/huggingface.co\/jarradh\/llama2_70b_chat_uncensored",
        "full_model_name":"jarradh\/llama2_70b_chat_uncensored",
        "Parameters":70.0,
        "MMLU_average":0.6876148772,
        "arc:challenge|25":0.6424914676,
        "hellaswag|10":0.6794463254,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.6222222222,
        "MMLU_astronomy":0.8026315789,
        "MMLU_business_ethics":0.73,
        "MMLU_clinical_knowledge":0.7245283019,
        "MMLU_college_biology":0.7986111111,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.56,
        "MMLU_college_mathematics":0.43,
        "MMLU_college_medicine":0.6416184971,
        "MMLU_college_physics":0.3529411765,
        "MMLU_computer_security":0.8,
        "MMLU_conceptual_physics":0.6127659574,
        "MMLU_econometrics":0.4385964912,
        "MMLU_electrical_engineering":0.6,
        "MMLU_elementary_mathematics":0.4444444444,
        "MMLU_formal_logic":0.4523809524,
        "MMLU_global_facts":0.47,
        "MMLU_high_school_biology":0.8161290323,
        "MMLU_high_school_chemistry":0.5270935961,
        "MMLU_high_school_computer_science":0.73,
        "MMLU_high_school_european_history":0.8484848485,
        "MMLU_high_school_geography":0.8535353535,
        "MMLU_high_school_government_and_politics":0.9274611399,
        "MMLU_high_school_macroeconomics":0.7102564103,
        "MMLU_high_school_mathematics":0.3185185185,
        "MMLU_high_school_microeconomics":0.7731092437,
        "MMLU_high_school_physics":0.4304635762,
        "MMLU_high_school_psychology":0.8844036697,
        "MMLU_high_school_statistics":0.5925925926,
        "MMLU_high_school_us_history":0.9117647059,
        "MMLU_high_school_world_history":0.8734177215,
        "MMLU_human_aging":0.7802690583,
        "MMLU_human_sexuality":0.8549618321,
        "MMLU_international_law":0.8595041322,
        "MMLU_jurisprudence":0.8055555556,
        "MMLU_logical_fallacies":0.8159509202,
        "MMLU_machine_learning":0.5535714286,
        "MMLU_management":0.8155339806,
        "MMLU_marketing":0.9017094017,
        "MMLU_medical_genetics":0.7,
        "MMLU_miscellaneous":0.846743295,
        "MMLU_moral_disputes":0.7716763006,
        "MMLU_moral_scenarios":0.4134078212,
        "MMLU_nutrition":0.7516339869,
        "MMLU_philosophy":0.7813504823,
        "MMLU_prehistory":0.8117283951,
        "MMLU_professional_accounting":0.5319148936,
        "MMLU_professional_law":0.5606258149,
        "MMLU_professional_medicine":0.7205882353,
        "MMLU_professional_psychology":0.7516339869,
        "MMLU_public_relations":0.7363636364,
        "MMLU_security_studies":0.7959183673,
        "MMLU_sociology":0.855721393,
        "MMLU_us_foreign_policy":0.91,
        "MMLU_virology":0.5542168675,
        "MMLU_world_religions":0.865497076
    },
    {
        "Model":"model_007_v2",
        "URL":"https:\/\/huggingface.co\/psmathur\/model_007_v2",
        "full_model_name":"psmathur\/model_007_v2",
        "Parameters":null,
        "MMLU_average":0.6857936692,
        "arc:challenge|25":0.6774744027,
        "hellaswag|10":0.6884086835,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.6,
        "MMLU_astronomy":0.7565789474,
        "MMLU_business_ethics":0.71,
        "MMLU_clinical_knowledge":0.7245283019,
        "MMLU_college_biology":0.7916666667,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.56,
        "MMLU_college_mathematics":0.4,
        "MMLU_college_medicine":0.6647398844,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.76,
        "MMLU_conceptual_physics":0.6425531915,
        "MMLU_econometrics":0.4561403509,
        "MMLU_electrical_engineering":0.5724137931,
        "MMLU_elementary_mathematics":0.4708994709,
        "MMLU_formal_logic":0.5,
        "MMLU_global_facts":0.43,
        "MMLU_high_school_biology":0.8096774194,
        "MMLU_high_school_chemistry":0.4975369458,
        "MMLU_high_school_computer_science":0.76,
        "MMLU_high_school_european_history":0.8424242424,
        "MMLU_high_school_geography":0.8737373737,
        "MMLU_high_school_government_and_politics":0.9170984456,
        "MMLU_high_school_macroeconomics":0.7025641026,
        "MMLU_high_school_mathematics":0.3074074074,
        "MMLU_high_school_microeconomics":0.7647058824,
        "MMLU_high_school_physics":0.4635761589,
        "MMLU_high_school_psychology":0.8899082569,
        "MMLU_high_school_statistics":0.5277777778,
        "MMLU_high_school_us_history":0.9166666667,
        "MMLU_high_school_world_history":0.8860759494,
        "MMLU_human_aging":0.7892376682,
        "MMLU_human_sexuality":0.8015267176,
        "MMLU_international_law":0.8512396694,
        "MMLU_jurisprudence":0.8148148148,
        "MMLU_logical_fallacies":0.8036809816,
        "MMLU_machine_learning":0.5892857143,
        "MMLU_management":0.8058252427,
        "MMLU_marketing":0.9188034188,
        "MMLU_medical_genetics":0.7,
        "MMLU_miscellaneous":0.8569604087,
        "MMLU_moral_disputes":0.7485549133,
        "MMLU_moral_scenarios":0.6469273743,
        "MMLU_nutrition":0.7287581699,
        "MMLU_philosophy":0.768488746,
        "MMLU_prehistory":0.8086419753,
        "MMLU_professional_accounting":0.5744680851,
        "MMLU_professional_law":0.5762711864,
        "MMLU_professional_medicine":0.7058823529,
        "MMLU_professional_psychology":0.7434640523,
        "MMLU_public_relations":0.7090909091,
        "MMLU_security_studies":0.7469387755,
        "MMLU_sociology":0.8756218905,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.5240963855,
        "MMLU_world_religions":0.8596491228
    },
    {
        "Model":"llama-2-70B-ensemble-v7",
        "URL":"https:\/\/huggingface.co\/yeontaek\/llama-2-70B-ensemble-v7",
        "full_model_name":"yeontaek\/llama-2-70B-ensemble-v7",
        "Parameters":70.0,
        "MMLU_average":0.6834103806,
        "arc:challenge|25":0.6749146758,
        "hellaswag|10":0.6818362876,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.637037037,
        "MMLU_astronomy":0.7828947368,
        "MMLU_business_ethics":0.75,
        "MMLU_clinical_knowledge":0.7283018868,
        "MMLU_college_biology":0.8194444444,
        "MMLU_college_chemistry":0.5,
        "MMLU_college_computer_science":0.55,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.6300578035,
        "MMLU_college_physics":0.362745098,
        "MMLU_computer_security":0.76,
        "MMLU_conceptual_physics":0.6595744681,
        "MMLU_econometrics":0.4385964912,
        "MMLU_electrical_engineering":0.5862068966,
        "MMLU_elementary_mathematics":0.4814814815,
        "MMLU_formal_logic":0.4682539683,
        "MMLU_global_facts":0.47,
        "MMLU_high_school_biology":0.8032258065,
        "MMLU_high_school_chemistry":0.5270935961,
        "MMLU_high_school_computer_science":0.75,
        "MMLU_high_school_european_history":0.8545454545,
        "MMLU_high_school_geography":0.8939393939,
        "MMLU_high_school_government_and_politics":0.9274611399,
        "MMLU_high_school_macroeconomics":0.6948717949,
        "MMLU_high_school_mathematics":0.3333333333,
        "MMLU_high_school_microeconomics":0.7478991597,
        "MMLU_high_school_physics":0.4503311258,
        "MMLU_high_school_psychology":0.8880733945,
        "MMLU_high_school_statistics":0.5416666667,
        "MMLU_high_school_us_history":0.8970588235,
        "MMLU_high_school_world_history":0.8818565401,
        "MMLU_human_aging":0.7623318386,
        "MMLU_human_sexuality":0.8015267176,
        "MMLU_international_law":0.826446281,
        "MMLU_jurisprudence":0.8240740741,
        "MMLU_logical_fallacies":0.8098159509,
        "MMLU_machine_learning":0.4821428571,
        "MMLU_management":0.7961165049,
        "MMLU_marketing":0.8846153846,
        "MMLU_medical_genetics":0.66,
        "MMLU_miscellaneous":0.8646232439,
        "MMLU_moral_disputes":0.7456647399,
        "MMLU_moral_scenarios":0.5910614525,
        "MMLU_nutrition":0.7091503268,
        "MMLU_philosophy":0.7717041801,
        "MMLU_prehistory":0.8024691358,
        "MMLU_professional_accounting":0.5460992908,
        "MMLU_professional_law":0.5430247718,
        "MMLU_professional_medicine":0.6838235294,
        "MMLU_professional_psychology":0.7238562092,
        "MMLU_public_relations":0.7090909091,
        "MMLU_security_studies":0.787755102,
        "MMLU_sociology":0.8706467662,
        "MMLU_us_foreign_policy":0.86,
        "MMLU_virology":0.5361445783,
        "MMLU_world_religions":0.8362573099
    },
    {
        "Model":"chronos-70b-v2",
        "URL":"https:\/\/huggingface.co\/elinas\/chronos-70b-v2",
        "full_model_name":"elinas\/chronos-70b-v2",
        "Parameters":70.0,
        "MMLU_average":0.6827865621,
        "arc:challenge|25":0.6450511945,
        "hellaswag|10":0.6743676558,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.6074074074,
        "MMLU_astronomy":0.7631578947,
        "MMLU_business_ethics":0.68,
        "MMLU_clinical_knowledge":0.7056603774,
        "MMLU_college_biology":0.8333333333,
        "MMLU_college_chemistry":0.48,
        "MMLU_college_computer_science":0.58,
        "MMLU_college_mathematics":0.43,
        "MMLU_college_medicine":0.6069364162,
        "MMLU_college_physics":0.3431372549,
        "MMLU_computer_security":0.76,
        "MMLU_conceptual_physics":0.629787234,
        "MMLU_econometrics":0.4561403509,
        "MMLU_electrical_engineering":0.6275862069,
        "MMLU_elementary_mathematics":0.4259259259,
        "MMLU_formal_logic":0.5079365079,
        "MMLU_global_facts":0.5,
        "MMLU_high_school_biology":0.7967741935,
        "MMLU_high_school_chemistry":0.5123152709,
        "MMLU_high_school_computer_science":0.73,
        "MMLU_high_school_european_history":0.8363636364,
        "MMLU_high_school_geography":0.8585858586,
        "MMLU_high_school_government_and_politics":0.9222797927,
        "MMLU_high_school_macroeconomics":0.6871794872,
        "MMLU_high_school_mathematics":0.3296296296,
        "MMLU_high_school_microeconomics":0.7226890756,
        "MMLU_high_school_physics":0.5099337748,
        "MMLU_high_school_psychology":0.8697247706,
        "MMLU_high_school_statistics":0.5509259259,
        "MMLU_high_school_us_history":0.9068627451,
        "MMLU_high_school_world_history":0.8396624473,
        "MMLU_human_aging":0.7668161435,
        "MMLU_human_sexuality":0.8244274809,
        "MMLU_international_law":0.8595041322,
        "MMLU_jurisprudence":0.7962962963,
        "MMLU_logical_fallacies":0.8159509202,
        "MMLU_machine_learning":0.5,
        "MMLU_management":0.8058252427,
        "MMLU_marketing":0.9102564103,
        "MMLU_medical_genetics":0.69,
        "MMLU_miscellaneous":0.846743295,
        "MMLU_moral_disputes":0.7572254335,
        "MMLU_moral_scenarios":0.5061452514,
        "MMLU_nutrition":0.7385620915,
        "MMLU_philosophy":0.7491961415,
        "MMLU_prehistory":0.7777777778,
        "MMLU_professional_accounting":0.524822695,
        "MMLU_professional_law":0.5254237288,
        "MMLU_professional_medicine":0.7205882353,
        "MMLU_professional_psychology":0.7238562092,
        "MMLU_public_relations":0.7454545455,
        "MMLU_security_studies":0.7510204082,
        "MMLU_sociology":0.8855721393,
        "MMLU_us_foreign_policy":0.89,
        "MMLU_virology":0.5602409639,
        "MMLU_world_religions":0.8771929825
    },
    {
        "Model":"llama-2-70B-ensemble-v3",
        "URL":"https:\/\/huggingface.co\/yeontaek\/llama-2-70B-ensemble-v3",
        "full_model_name":"yeontaek\/llama-2-70B-ensemble-v3",
        "Parameters":70.0,
        "MMLU_average":0.6815002471,
        "arc:challenge|25":0.662116041,
        "hellaswag|10":0.6936865166,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.5925925926,
        "MMLU_astronomy":0.7828947368,
        "MMLU_business_ethics":0.69,
        "MMLU_clinical_knowledge":0.7622641509,
        "MMLU_college_biology":0.8333333333,
        "MMLU_college_chemistry":0.45,
        "MMLU_college_computer_science":0.55,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.6531791908,
        "MMLU_college_physics":0.4215686275,
        "MMLU_computer_security":0.79,
        "MMLU_conceptual_physics":0.6340425532,
        "MMLU_econometrics":0.4210526316,
        "MMLU_electrical_engineering":0.5655172414,
        "MMLU_elementary_mathematics":0.4841269841,
        "MMLU_formal_logic":0.4682539683,
        "MMLU_global_facts":0.41,
        "MMLU_high_school_biology":0.8225806452,
        "MMLU_high_school_chemistry":0.5270935961,
        "MMLU_high_school_computer_science":0.75,
        "MMLU_high_school_european_history":0.8484848485,
        "MMLU_high_school_geography":0.8535353535,
        "MMLU_high_school_government_and_politics":0.9430051813,
        "MMLU_high_school_macroeconomics":0.6923076923,
        "MMLU_high_school_mathematics":0.3296296296,
        "MMLU_high_school_microeconomics":0.7731092437,
        "MMLU_high_school_physics":0.3774834437,
        "MMLU_high_school_psychology":0.880733945,
        "MMLU_high_school_statistics":0.537037037,
        "MMLU_high_school_us_history":0.9068627451,
        "MMLU_high_school_world_history":0.8902953586,
        "MMLU_human_aging":0.7802690583,
        "MMLU_human_sexuality":0.8167938931,
        "MMLU_international_law":0.8347107438,
        "MMLU_jurisprudence":0.7592592593,
        "MMLU_logical_fallacies":0.8343558282,
        "MMLU_machine_learning":0.5625,
        "MMLU_management":0.8155339806,
        "MMLU_marketing":0.8931623932,
        "MMLU_medical_genetics":0.68,
        "MMLU_miscellaneous":0.8607918263,
        "MMLU_moral_disputes":0.7196531792,
        "MMLU_moral_scenarios":0.5787709497,
        "MMLU_nutrition":0.7385620915,
        "MMLU_philosophy":0.7524115756,
        "MMLU_prehistory":0.799382716,
        "MMLU_professional_accounting":0.5709219858,
        "MMLU_professional_law":0.5645371578,
        "MMLU_professional_medicine":0.6875,
        "MMLU_professional_psychology":0.7336601307,
        "MMLU_public_relations":0.7090909091,
        "MMLU_security_studies":0.7306122449,
        "MMLU_sociology":0.8656716418,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.5301204819,
        "MMLU_world_religions":0.8362573099
    },
    {
        "Model":"llama-2-70B-ensemble-v6",
        "URL":"https:\/\/huggingface.co\/yeontaek\/llama-2-70B-ensemble-v6",
        "full_model_name":"yeontaek\/llama-2-70B-ensemble-v6",
        "Parameters":70.0,
        "MMLU_average":0.6806734464,
        "arc:challenge|25":0.6774744027,
        "hellaswag|10":0.6856203943,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.6296296296,
        "MMLU_astronomy":0.7763157895,
        "MMLU_business_ethics":0.7,
        "MMLU_clinical_knowledge":0.720754717,
        "MMLU_college_biology":0.8055555556,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.52,
        "MMLU_college_mathematics":0.39,
        "MMLU_college_medicine":0.6589595376,
        "MMLU_college_physics":0.3529411765,
        "MMLU_computer_security":0.75,
        "MMLU_conceptual_physics":0.6382978723,
        "MMLU_econometrics":0.4122807018,
        "MMLU_electrical_engineering":0.5517241379,
        "MMLU_elementary_mathematics":0.4603174603,
        "MMLU_formal_logic":0.5,
        "MMLU_global_facts":0.41,
        "MMLU_high_school_biology":0.8161290323,
        "MMLU_high_school_chemistry":0.5172413793,
        "MMLU_high_school_computer_science":0.76,
        "MMLU_high_school_european_history":0.8363636364,
        "MMLU_high_school_geography":0.8585858586,
        "MMLU_high_school_government_and_politics":0.9274611399,
        "MMLU_high_school_macroeconomics":0.6923076923,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.7521008403,
        "MMLU_high_school_physics":0.4569536424,
        "MMLU_high_school_psychology":0.8844036697,
        "MMLU_high_school_statistics":0.5277777778,
        "MMLU_high_school_us_history":0.9166666667,
        "MMLU_high_school_world_history":0.8860759494,
        "MMLU_human_aging":0.7757847534,
        "MMLU_human_sexuality":0.8015267176,
        "MMLU_international_law":0.8347107438,
        "MMLU_jurisprudence":0.8055555556,
        "MMLU_logical_fallacies":0.7914110429,
        "MMLU_machine_learning":0.5803571429,
        "MMLU_management":0.8349514563,
        "MMLU_marketing":0.8974358974,
        "MMLU_medical_genetics":0.71,
        "MMLU_miscellaneous":0.8607918263,
        "MMLU_moral_disputes":0.7341040462,
        "MMLU_moral_scenarios":0.6424581006,
        "MMLU_nutrition":0.7222222222,
        "MMLU_philosophy":0.7491961415,
        "MMLU_prehistory":0.8024691358,
        "MMLU_professional_accounting":0.5744680851,
        "MMLU_professional_law":0.5671447197,
        "MMLU_professional_medicine":0.6801470588,
        "MMLU_professional_psychology":0.7352941176,
        "MMLU_public_relations":0.7090909091,
        "MMLU_security_studies":0.7387755102,
        "MMLU_sociology":0.8756218905,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.5180722892,
        "MMLU_world_religions":0.8479532164
    },
    {
        "Model":"llama-2-70B-ensemble-v2",
        "URL":"https:\/\/huggingface.co\/yeontaek\/llama-2-70B-ensemble-v2",
        "full_model_name":"yeontaek\/llama-2-70B-ensemble-v2",
        "Parameters":70.0,
        "MMLU_average":0.6803978172,
        "arc:challenge|25":0.6501706485,
        "hellaswag|10":0.6491734714,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.6148148148,
        "MMLU_astronomy":0.7894736842,
        "MMLU_business_ethics":0.74,
        "MMLU_clinical_knowledge":0.7773584906,
        "MMLU_college_biology":0.8055555556,
        "MMLU_college_chemistry":0.45,
        "MMLU_college_computer_science":0.56,
        "MMLU_college_mathematics":0.42,
        "MMLU_college_medicine":0.5895953757,
        "MMLU_college_physics":0.3921568627,
        "MMLU_computer_security":0.75,
        "MMLU_conceptual_physics":0.6553191489,
        "MMLU_econometrics":0.4561403509,
        "MMLU_electrical_engineering":0.5931034483,
        "MMLU_elementary_mathematics":0.4444444444,
        "MMLU_formal_logic":0.4603174603,
        "MMLU_global_facts":0.44,
        "MMLU_high_school_biology":0.8161290323,
        "MMLU_high_school_chemistry":0.5172413793,
        "MMLU_high_school_computer_science":0.72,
        "MMLU_high_school_european_history":0.8424242424,
        "MMLU_high_school_geography":0.8686868687,
        "MMLU_high_school_government_and_politics":0.9222797927,
        "MMLU_high_school_macroeconomics":0.6820512821,
        "MMLU_high_school_mathematics":0.3333333333,
        "MMLU_high_school_microeconomics":0.756302521,
        "MMLU_high_school_physics":0.417218543,
        "MMLU_high_school_psychology":0.8825688073,
        "MMLU_high_school_statistics":0.5416666667,
        "MMLU_high_school_us_history":0.9019607843,
        "MMLU_high_school_world_history":0.8776371308,
        "MMLU_human_aging":0.7533632287,
        "MMLU_human_sexuality":0.8396946565,
        "MMLU_international_law":0.8347107438,
        "MMLU_jurisprudence":0.787037037,
        "MMLU_logical_fallacies":0.8159509202,
        "MMLU_machine_learning":0.4910714286,
        "MMLU_management":0.8155339806,
        "MMLU_marketing":0.8803418803,
        "MMLU_medical_genetics":0.68,
        "MMLU_miscellaneous":0.8582375479,
        "MMLU_moral_disputes":0.7601156069,
        "MMLU_moral_scenarios":0.5463687151,
        "MMLU_nutrition":0.7222222222,
        "MMLU_philosophy":0.7588424437,
        "MMLU_prehistory":0.7839506173,
        "MMLU_professional_accounting":0.5319148936,
        "MMLU_professional_law":0.5501955671,
        "MMLU_professional_medicine":0.7242647059,
        "MMLU_professional_psychology":0.7287581699,
        "MMLU_public_relations":0.7181818182,
        "MMLU_security_studies":0.7632653061,
        "MMLU_sociology":0.8606965174,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.5180722892,
        "MMLU_world_religions":0.8421052632
    },
    {
        "Model":"falcon-180B-chat",
        "URL":"https:\/\/huggingface.co\/tiiuae\/falcon-180B-chat",
        "full_model_name":"tiiuae\/falcon-180B-chat",
        "Parameters":180.0,
        "MMLU_average":0.6802909925,
        "arc:challenge|25":0.614334471,
        "hellaswag|10":0.6904999004,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.6518518519,
        "MMLU_astronomy":0.7434210526,
        "MMLU_business_ethics":0.7,
        "MMLU_clinical_knowledge":0.7018867925,
        "MMLU_college_biology":0.8263888889,
        "MMLU_college_chemistry":0.5,
        "MMLU_college_computer_science":0.53,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.6473988439,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.78,
        "MMLU_conceptual_physics":0.6510638298,
        "MMLU_econometrics":0.4824561404,
        "MMLU_electrical_engineering":0.6137931034,
        "MMLU_elementary_mathematics":0.455026455,
        "MMLU_formal_logic":0.4365079365,
        "MMLU_global_facts":0.45,
        "MMLU_high_school_biology":0.7967741935,
        "MMLU_high_school_chemistry":0.5418719212,
        "MMLU_high_school_computer_science":0.75,
        "MMLU_high_school_european_history":0.8,
        "MMLU_high_school_geography":0.8484848485,
        "MMLU_high_school_government_and_politics":0.9481865285,
        "MMLU_high_school_macroeconomics":0.6820512821,
        "MMLU_high_school_mathematics":0.3222222222,
        "MMLU_high_school_microeconomics":0.768907563,
        "MMLU_high_school_physics":0.3973509934,
        "MMLU_high_school_psychology":0.871559633,
        "MMLU_high_school_statistics":0.5416666667,
        "MMLU_high_school_us_history":0.8578431373,
        "MMLU_high_school_world_history":0.8607594937,
        "MMLU_human_aging":0.7668161435,
        "MMLU_human_sexuality":0.8396946565,
        "MMLU_international_law":0.8181818182,
        "MMLU_jurisprudence":0.8611111111,
        "MMLU_logical_fallacies":0.8343558282,
        "MMLU_machine_learning":0.5446428571,
        "MMLU_management":0.8446601942,
        "MMLU_marketing":0.8931623932,
        "MMLU_medical_genetics":0.78,
        "MMLU_miscellaneous":0.8671775223,
        "MMLU_moral_disputes":0.774566474,
        "MMLU_moral_scenarios":0.4659217877,
        "MMLU_nutrition":0.7287581699,
        "MMLU_philosophy":0.7877813505,
        "MMLU_prehistory":0.787037037,
        "MMLU_professional_accounting":0.5460992908,
        "MMLU_professional_law":0.5345501956,
        "MMLU_professional_medicine":0.6875,
        "MMLU_professional_psychology":0.727124183,
        "MMLU_public_relations":0.6909090909,
        "MMLU_security_studies":0.7632653061,
        "MMLU_sociology":0.8258706468,
        "MMLU_us_foreign_policy":0.89,
        "MMLU_virology":0.5240963855,
        "MMLU_world_religions":0.8421052632
    },
    {
        "Model":"llama2-70b-oasst-sft-v10",
        "URL":"https:\/\/huggingface.co\/OpenAssistant\/llama2-70b-oasst-sft-v10",
        "full_model_name":"OpenAssistant\/llama2-70b-oasst-sft-v10",
        "Parameters":70.0,
        "MMLU_average":0.6770234693,
        "arc:challenge|25":0.638225256,
        "hellaswag|10":0.668990241,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.5555555556,
        "MMLU_astronomy":0.7763157895,
        "MMLU_business_ethics":0.73,
        "MMLU_clinical_knowledge":0.7132075472,
        "MMLU_college_biology":0.75,
        "MMLU_college_chemistry":0.48,
        "MMLU_college_computer_science":0.63,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.6473988439,
        "MMLU_college_physics":0.4411764706,
        "MMLU_computer_security":0.74,
        "MMLU_conceptual_physics":0.6170212766,
        "MMLU_econometrics":0.3947368421,
        "MMLU_electrical_engineering":0.5586206897,
        "MMLU_elementary_mathematics":0.4391534392,
        "MMLU_formal_logic":0.4682539683,
        "MMLU_global_facts":0.46,
        "MMLU_high_school_biology":0.8258064516,
        "MMLU_high_school_chemistry":0.5320197044,
        "MMLU_high_school_computer_science":0.75,
        "MMLU_high_school_european_history":0.8424242424,
        "MMLU_high_school_geography":0.8585858586,
        "MMLU_high_school_government_and_politics":0.9170984456,
        "MMLU_high_school_macroeconomics":0.7128205128,
        "MMLU_high_school_mathematics":0.3333333333,
        "MMLU_high_school_microeconomics":0.7731092437,
        "MMLU_high_school_physics":0.4304635762,
        "MMLU_high_school_psychology":0.871559633,
        "MMLU_high_school_statistics":0.5925925926,
        "MMLU_high_school_us_history":0.8921568627,
        "MMLU_high_school_world_history":0.864978903,
        "MMLU_human_aging":0.7668161435,
        "MMLU_human_sexuality":0.8625954198,
        "MMLU_international_law":0.8925619835,
        "MMLU_jurisprudence":0.7962962963,
        "MMLU_logical_fallacies":0.754601227,
        "MMLU_machine_learning":0.5,
        "MMLU_management":0.8058252427,
        "MMLU_marketing":0.8717948718,
        "MMLU_medical_genetics":0.68,
        "MMLU_miscellaneous":0.8569604087,
        "MMLU_moral_disputes":0.7485549133,
        "MMLU_moral_scenarios":0.4960893855,
        "MMLU_nutrition":0.7352941176,
        "MMLU_philosophy":0.7459807074,
        "MMLU_prehistory":0.7530864198,
        "MMLU_professional_accounting":0.5283687943,
        "MMLU_professional_law":0.5371577575,
        "MMLU_professional_medicine":0.6838235294,
        "MMLU_professional_psychology":0.7091503268,
        "MMLU_public_relations":0.7272727273,
        "MMLU_security_studies":0.7469387755,
        "MMLU_sociology":0.9004975124,
        "MMLU_us_foreign_policy":0.9,
        "MMLU_virology":0.5060240964,
        "MMLU_world_religions":0.8362573099
    },
    {
        "Model":"Llama-2-70b-orca-200k",
        "URL":"https:\/\/huggingface.co\/ddobokki\/Llama-2-70b-orca-200k",
        "full_model_name":"ddobokki\/Llama-2-70b-orca-200k",
        "Parameters":70.0,
        "MMLU_average":0.6689155585,
        "arc:challenge|25":0.5989761092,
        "hellaswag|10":0.6584345748,
        "MMLU_abstract_algebra":0.38,
        "MMLU_anatomy":0.6148148148,
        "MMLU_astronomy":0.7697368421,
        "MMLU_business_ethics":0.71,
        "MMLU_clinical_knowledge":0.6981132075,
        "MMLU_college_biology":0.7847222222,
        "MMLU_college_chemistry":0.45,
        "MMLU_college_computer_science":0.57,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.612716763,
        "MMLU_college_physics":0.3431372549,
        "MMLU_computer_security":0.74,
        "MMLU_conceptual_physics":0.6127659574,
        "MMLU_econometrics":0.4298245614,
        "MMLU_electrical_engineering":0.5931034483,
        "MMLU_elementary_mathematics":0.4153439153,
        "MMLU_formal_logic":0.4365079365,
        "MMLU_global_facts":0.43,
        "MMLU_high_school_biology":0.8129032258,
        "MMLU_high_school_chemistry":0.5123152709,
        "MMLU_high_school_computer_science":0.72,
        "MMLU_high_school_european_history":0.8121212121,
        "MMLU_high_school_geography":0.8636363636,
        "MMLU_high_school_government_and_politics":0.9067357513,
        "MMLU_high_school_macroeconomics":0.6820512821,
        "MMLU_high_school_mathematics":0.3555555556,
        "MMLU_high_school_microeconomics":0.7142857143,
        "MMLU_high_school_physics":0.4039735099,
        "MMLU_high_school_psychology":0.8568807339,
        "MMLU_high_school_statistics":0.5416666667,
        "MMLU_high_school_us_history":0.8725490196,
        "MMLU_high_school_world_history":0.8523206751,
        "MMLU_human_aging":0.7488789238,
        "MMLU_human_sexuality":0.7938931298,
        "MMLU_international_law":0.8760330579,
        "MMLU_jurisprudence":0.8333333333,
        "MMLU_logical_fallacies":0.7852760736,
        "MMLU_machine_learning":0.4732142857,
        "MMLU_management":0.8155339806,
        "MMLU_marketing":0.9017094017,
        "MMLU_medical_genetics":0.72,
        "MMLU_miscellaneous":0.8595146871,
        "MMLU_moral_disputes":0.774566474,
        "MMLU_moral_scenarios":0.5363128492,
        "MMLU_nutrition":0.6895424837,
        "MMLU_philosophy":0.7395498392,
        "MMLU_prehistory":0.7685185185,
        "MMLU_professional_accounting":0.4893617021,
        "MMLU_professional_law":0.5084745763,
        "MMLU_professional_medicine":0.6838235294,
        "MMLU_professional_psychology":0.7075163399,
        "MMLU_public_relations":0.7363636364,
        "MMLU_security_studies":0.7632653061,
        "MMLU_sociology":0.855721393,
        "MMLU_us_foreign_policy":0.85,
        "MMLU_virology":0.5120481928,
        "MMLU_world_religions":0.8479532164
    },
    {
        "Model":"14B-DPO-alpha",
        "URL":"https:\/\/huggingface.co\/CausalLM\/14B-DPO-alpha",
        "full_model_name":"CausalLM\/14B-DPO-alpha",
        "Parameters":14.0,
        "MMLU_average":0.6661698976,
        "arc:challenge|25":0.5392491468,
        "hellaswag|10":0.6023700458,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.6,
        "MMLU_astronomy":0.6907894737,
        "MMLU_business_ethics":0.7,
        "MMLU_clinical_knowledge":0.7056603774,
        "MMLU_college_biology":0.7638888889,
        "MMLU_college_chemistry":0.49,
        "MMLU_college_computer_science":0.52,
        "MMLU_college_mathematics":0.43,
        "MMLU_college_medicine":0.6820809249,
        "MMLU_college_physics":0.4509803922,
        "MMLU_computer_security":0.79,
        "MMLU_conceptual_physics":0.6382978723,
        "MMLU_econometrics":0.5263157895,
        "MMLU_electrical_engineering":0.6068965517,
        "MMLU_elementary_mathematics":0.5052910053,
        "MMLU_formal_logic":0.5238095238,
        "MMLU_global_facts":0.44,
        "MMLU_high_school_biology":0.7935483871,
        "MMLU_high_school_chemistry":0.5763546798,
        "MMLU_high_school_computer_science":0.74,
        "MMLU_high_school_european_history":0.8,
        "MMLU_high_school_geography":0.8787878788,
        "MMLU_high_school_government_and_politics":0.8445595855,
        "MMLU_high_school_macroeconomics":0.6487179487,
        "MMLU_high_school_mathematics":0.3666666667,
        "MMLU_high_school_microeconomics":0.7394957983,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.8256880734,
        "MMLU_high_school_statistics":0.537037037,
        "MMLU_high_school_us_history":0.8333333333,
        "MMLU_high_school_world_history":0.8396624473,
        "MMLU_human_aging":0.7040358744,
        "MMLU_human_sexuality":0.7480916031,
        "MMLU_international_law":0.826446281,
        "MMLU_jurisprudence":0.7685185185,
        "MMLU_logical_fallacies":0.7607361963,
        "MMLU_machine_learning":0.5089285714,
        "MMLU_management":0.786407767,
        "MMLU_marketing":0.8888888889,
        "MMLU_medical_genetics":0.73,
        "MMLU_miscellaneous":0.8199233716,
        "MMLU_moral_disputes":0.725433526,
        "MMLU_moral_scenarios":0.4290502793,
        "MMLU_nutrition":0.7385620915,
        "MMLU_philosophy":0.729903537,
        "MMLU_prehistory":0.7222222222,
        "MMLU_professional_accounting":0.5070921986,
        "MMLU_professional_law":0.517601043,
        "MMLU_professional_medicine":0.7058823529,
        "MMLU_professional_psychology":0.6748366013,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.7755102041,
        "MMLU_sociology":0.8706467662,
        "MMLU_us_foreign_policy":0.86,
        "MMLU_virology":0.5240963855,
        "MMLU_world_religions":0.8187134503
    },
    {
        "Model":"14B",
        "URL":"https:\/\/huggingface.co\/CausalLM\/14B",
        "full_model_name":"CausalLM\/14B",
        "Parameters":14.0,
        "MMLU_average":0.6586320847,
        "arc:challenge|25":0.5307167235,
        "hellaswag|10":0.6042620992,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.5925925926,
        "MMLU_astronomy":0.6644736842,
        "MMLU_business_ethics":0.68,
        "MMLU_clinical_knowledge":0.6905660377,
        "MMLU_college_biology":0.7916666667,
        "MMLU_college_chemistry":0.49,
        "MMLU_college_computer_science":0.57,
        "MMLU_college_mathematics":0.43,
        "MMLU_college_medicine":0.6936416185,
        "MMLU_college_physics":0.431372549,
        "MMLU_computer_security":0.76,
        "MMLU_conceptual_physics":0.6,
        "MMLU_econometrics":0.4912280702,
        "MMLU_electrical_engineering":0.6,
        "MMLU_elementary_mathematics":0.5291005291,
        "MMLU_formal_logic":0.5,
        "MMLU_global_facts":0.44,
        "MMLU_high_school_biology":0.7677419355,
        "MMLU_high_school_chemistry":0.5911330049,
        "MMLU_high_school_computer_science":0.65,
        "MMLU_high_school_european_history":0.7818181818,
        "MMLU_high_school_geography":0.8484848485,
        "MMLU_high_school_government_and_politics":0.8704663212,
        "MMLU_high_school_macroeconomics":0.658974359,
        "MMLU_high_school_mathematics":0.3851851852,
        "MMLU_high_school_microeconomics":0.756302521,
        "MMLU_high_school_physics":0.357615894,
        "MMLU_high_school_psychology":0.8348623853,
        "MMLU_high_school_statistics":0.6064814815,
        "MMLU_high_school_us_history":0.8039215686,
        "MMLU_high_school_world_history":0.7974683544,
        "MMLU_human_aging":0.7130044843,
        "MMLU_human_sexuality":0.7557251908,
        "MMLU_international_law":0.7851239669,
        "MMLU_jurisprudence":0.7314814815,
        "MMLU_logical_fallacies":0.7423312883,
        "MMLU_machine_learning":0.5178571429,
        "MMLU_management":0.7961165049,
        "MMLU_marketing":0.8760683761,
        "MMLU_medical_genetics":0.72,
        "MMLU_miscellaneous":0.8173690932,
        "MMLU_moral_disputes":0.7167630058,
        "MMLU_moral_scenarios":0.3910614525,
        "MMLU_nutrition":0.7124183007,
        "MMLU_philosophy":0.7202572347,
        "MMLU_prehistory":0.6851851852,
        "MMLU_professional_accounting":0.5106382979,
        "MMLU_professional_law":0.5104302477,
        "MMLU_professional_medicine":0.6911764706,
        "MMLU_professional_psychology":0.658496732,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.7591836735,
        "MMLU_sociology":0.8955223881,
        "MMLU_us_foreign_policy":0.88,
        "MMLU_virology":0.5240963855,
        "MMLU_world_religions":0.8011695906
    },
    {
        "Model":"StableBeluga1-Delta",
        "URL":"https:\/\/huggingface.co\/stabilityai\/StableBeluga1-Delta",
        "full_model_name":"stabilityai\/StableBeluga1-Delta",
        "Parameters":null,
        "MMLU_average":0.6482943911,
        "arc:challenge|25":0.6459044369,
        "hellaswag|10":0.669786895,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.5703703704,
        "MMLU_astronomy":0.7236842105,
        "MMLU_business_ethics":0.63,
        "MMLU_clinical_knowledge":0.6867924528,
        "MMLU_college_biology":0.7222222222,
        "MMLU_college_chemistry":0.48,
        "MMLU_college_computer_science":0.51,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.5895953757,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.73,
        "MMLU_conceptual_physics":0.6,
        "MMLU_econometrics":0.4385964912,
        "MMLU_electrical_engineering":0.6,
        "MMLU_elementary_mathematics":0.4206349206,
        "MMLU_formal_logic":0.4682539683,
        "MMLU_global_facts":0.42,
        "MMLU_high_school_biology":0.7677419355,
        "MMLU_high_school_chemistry":0.4088669951,
        "MMLU_high_school_computer_science":0.71,
        "MMLU_high_school_european_history":0.8181818182,
        "MMLU_high_school_geography":0.8484848485,
        "MMLU_high_school_government_and_politics":0.896373057,
        "MMLU_high_school_macroeconomics":0.6641025641,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.6932773109,
        "MMLU_high_school_physics":0.417218543,
        "MMLU_high_school_psychology":0.8366972477,
        "MMLU_high_school_statistics":0.5416666667,
        "MMLU_high_school_us_history":0.8529411765,
        "MMLU_high_school_world_history":0.8523206751,
        "MMLU_human_aging":0.6860986547,
        "MMLU_human_sexuality":0.8015267176,
        "MMLU_international_law":0.8181818182,
        "MMLU_jurisprudence":0.787037037,
        "MMLU_logical_fallacies":0.7791411043,
        "MMLU_machine_learning":0.4732142857,
        "MMLU_management":0.7961165049,
        "MMLU_marketing":0.8888888889,
        "MMLU_medical_genetics":0.65,
        "MMLU_miscellaneous":0.8186462324,
        "MMLU_moral_disputes":0.7572254335,
        "MMLU_moral_scenarios":0.4793296089,
        "MMLU_nutrition":0.7026143791,
        "MMLU_philosophy":0.729903537,
        "MMLU_prehistory":0.7222222222,
        "MMLU_professional_accounting":0.5035460993,
        "MMLU_professional_law":0.4960886571,
        "MMLU_professional_medicine":0.6360294118,
        "MMLU_professional_psychology":0.6977124183,
        "MMLU_public_relations":0.7272727273,
        "MMLU_security_studies":0.7469387755,
        "MMLU_sociology":0.8756218905,
        "MMLU_us_foreign_policy":0.86,
        "MMLU_virology":0.5180722892,
        "MMLU_world_religions":0.8304093567
    },
    {
        "Model":"llama-65b-instruct",
        "URL":"https:\/\/huggingface.co\/upstage\/llama-65b-instruct",
        "full_model_name":"upstage\/llama-65b-instruct",
        "Parameters":65.0,
        "MMLU_average":0.6477170394,
        "arc:challenge|25":0.6527303754,
        "hellaswag|10":0.6733718383,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.5555555556,
        "MMLU_astronomy":0.7434210526,
        "MMLU_business_ethics":0.68,
        "MMLU_clinical_knowledge":0.6981132075,
        "MMLU_college_biology":0.7291666667,
        "MMLU_college_chemistry":0.47,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5606936416,
        "MMLU_college_physics":0.4509803922,
        "MMLU_computer_security":0.74,
        "MMLU_conceptual_physics":0.6255319149,
        "MMLU_econometrics":0.3684210526,
        "MMLU_electrical_engineering":0.5793103448,
        "MMLU_elementary_mathematics":0.4206349206,
        "MMLU_formal_logic":0.4444444444,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.7677419355,
        "MMLU_high_school_chemistry":0.4137931034,
        "MMLU_high_school_computer_science":0.71,
        "MMLU_high_school_european_history":0.8,
        "MMLU_high_school_geography":0.8434343434,
        "MMLU_high_school_government_and_politics":0.9015544041,
        "MMLU_high_school_macroeconomics":0.6538461538,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.7058823529,
        "MMLU_high_school_physics":0.417218543,
        "MMLU_high_school_psychology":0.8275229358,
        "MMLU_high_school_statistics":0.4953703704,
        "MMLU_high_school_us_history":0.8480392157,
        "MMLU_high_school_world_history":0.8565400844,
        "MMLU_human_aging":0.6995515695,
        "MMLU_human_sexuality":0.7633587786,
        "MMLU_international_law":0.8347107438,
        "MMLU_jurisprudence":0.7962962963,
        "MMLU_logical_fallacies":0.7607361963,
        "MMLU_machine_learning":0.4821428571,
        "MMLU_management":0.8252427184,
        "MMLU_marketing":0.8803418803,
        "MMLU_medical_genetics":0.68,
        "MMLU_miscellaneous":0.8301404853,
        "MMLU_moral_disputes":0.7514450867,
        "MMLU_moral_scenarios":0.4905027933,
        "MMLU_nutrition":0.7124183007,
        "MMLU_philosophy":0.7202572347,
        "MMLU_prehistory":0.7407407407,
        "MMLU_professional_accounting":0.5283687943,
        "MMLU_professional_law":0.4960886571,
        "MMLU_professional_medicine":0.6139705882,
        "MMLU_professional_psychology":0.6960784314,
        "MMLU_public_relations":0.7363636364,
        "MMLU_security_studies":0.7795918367,
        "MMLU_sociology":0.8606965174,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.5361445783,
        "MMLU_world_religions":0.8011695906
    },
    {
        "Model":"Lemur-70B-Chat-v1-GPTQ",
        "URL":"https:\/\/huggingface.co\/TheBloke\/Lemur-70B-Chat-v1-GPTQ",
        "full_model_name":"TheBloke\/Lemur-70B-Chat-v1-GPTQ",
        "Parameters":70.0,
        "MMLU_average":0.6474833909,
        "arc:challenge|25":0.6075085324,
        "hellaswag|10":0.6475801633,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.5407407407,
        "MMLU_astronomy":0.6973684211,
        "MMLU_business_ethics":0.67,
        "MMLU_clinical_knowledge":0.6754716981,
        "MMLU_college_biology":0.7430555556,
        "MMLU_college_chemistry":0.47,
        "MMLU_college_computer_science":0.49,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.6358381503,
        "MMLU_college_physics":0.4117647059,
        "MMLU_computer_security":0.78,
        "MMLU_conceptual_physics":0.5914893617,
        "MMLU_econometrics":0.4298245614,
        "MMLU_electrical_engineering":0.5448275862,
        "MMLU_elementary_mathematics":0.4603174603,
        "MMLU_formal_logic":0.4761904762,
        "MMLU_global_facts":0.5,
        "MMLU_high_school_biology":0.7612903226,
        "MMLU_high_school_chemistry":0.4630541872,
        "MMLU_high_school_computer_science":0.72,
        "MMLU_high_school_european_history":0.7878787879,
        "MMLU_high_school_geography":0.8131313131,
        "MMLU_high_school_government_and_politics":0.896373057,
        "MMLU_high_school_macroeconomics":0.6615384615,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.6596638655,
        "MMLU_high_school_physics":0.417218543,
        "MMLU_high_school_psychology":0.8348623853,
        "MMLU_high_school_statistics":0.5231481481,
        "MMLU_high_school_us_history":0.8382352941,
        "MMLU_high_school_world_history":0.835443038,
        "MMLU_human_aging":0.730941704,
        "MMLU_human_sexuality":0.7404580153,
        "MMLU_international_law":0.8429752066,
        "MMLU_jurisprudence":0.7777777778,
        "MMLU_logical_fallacies":0.7852760736,
        "MMLU_machine_learning":0.4910714286,
        "MMLU_management":0.786407767,
        "MMLU_marketing":0.858974359,
        "MMLU_medical_genetics":0.62,
        "MMLU_miscellaneous":0.8135376756,
        "MMLU_moral_disputes":0.7427745665,
        "MMLU_moral_scenarios":0.5072625698,
        "MMLU_nutrition":0.7058823529,
        "MMLU_philosophy":0.7073954984,
        "MMLU_prehistory":0.7469135802,
        "MMLU_professional_accounting":0.4964539007,
        "MMLU_professional_law":0.4993481095,
        "MMLU_professional_medicine":0.6580882353,
        "MMLU_professional_psychology":0.6830065359,
        "MMLU_public_relations":0.7,
        "MMLU_security_studies":0.7918367347,
        "MMLU_sociology":0.8457711443,
        "MMLU_us_foreign_policy":0.86,
        "MMLU_virology":0.5120481928,
        "MMLU_world_religions":0.783625731
    },
    {
        "Model":"yayi-70b-llama2",
        "URL":"https:\/\/huggingface.co\/wenge-research\/yayi-70b-llama2",
        "full_model_name":"wenge-research\/yayi-70b-llama2",
        "Parameters":70.0,
        "MMLU_average":0.6442069275,
        "arc:challenge|25":0.5614334471,
        "hellaswag|10":0.6402111133,
        "MMLU_abstract_algebra":0.38,
        "MMLU_anatomy":0.5777777778,
        "MMLU_astronomy":0.6973684211,
        "MMLU_business_ethics":0.69,
        "MMLU_clinical_knowledge":0.6943396226,
        "MMLU_college_biology":0.7013888889,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.53,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.5606936416,
        "MMLU_college_physics":0.4019607843,
        "MMLU_computer_security":0.76,
        "MMLU_conceptual_physics":0.6170212766,
        "MMLU_econometrics":0.4210526316,
        "MMLU_electrical_engineering":0.6068965517,
        "MMLU_elementary_mathematics":0.4259259259,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.46,
        "MMLU_high_school_biology":0.7870967742,
        "MMLU_high_school_chemistry":0.5369458128,
        "MMLU_high_school_computer_science":0.7,
        "MMLU_high_school_european_history":0.7939393939,
        "MMLU_high_school_geography":0.803030303,
        "MMLU_high_school_government_and_politics":0.9067357513,
        "MMLU_high_school_macroeconomics":0.6487179487,
        "MMLU_high_school_mathematics":0.3444444444,
        "MMLU_high_school_microeconomics":0.6386554622,
        "MMLU_high_school_physics":0.417218543,
        "MMLU_high_school_psychology":0.8128440367,
        "MMLU_high_school_statistics":0.4907407407,
        "MMLU_high_school_us_history":0.8431372549,
        "MMLU_high_school_world_history":0.8143459916,
        "MMLU_human_aging":0.735426009,
        "MMLU_human_sexuality":0.7938931298,
        "MMLU_international_law":0.867768595,
        "MMLU_jurisprudence":0.7962962963,
        "MMLU_logical_fallacies":0.754601227,
        "MMLU_machine_learning":0.4642857143,
        "MMLU_management":0.7961165049,
        "MMLU_marketing":0.8632478632,
        "MMLU_medical_genetics":0.64,
        "MMLU_miscellaneous":0.8199233716,
        "MMLU_moral_disputes":0.7312138728,
        "MMLU_moral_scenarios":0.2737430168,
        "MMLU_nutrition":0.6993464052,
        "MMLU_philosophy":0.7363344051,
        "MMLU_prehistory":0.7314814815,
        "MMLU_professional_accounting":0.5177304965,
        "MMLU_professional_law":0.5110821382,
        "MMLU_professional_medicine":0.5772058824,
        "MMLU_professional_psychology":0.6764705882,
        "MMLU_public_relations":0.7181818182,
        "MMLU_security_studies":0.7673469388,
        "MMLU_sociology":0.8407960199,
        "MMLU_us_foreign_policy":0.93,
        "MMLU_virology":0.5361445783,
        "MMLU_world_religions":0.8538011696
    },
    {
        "Model":"Mistral-v0.1-PeanutButter-v0.0.2-7B",
        "URL":"https:\/\/huggingface.co\/PeanutJar\/Mistral-v0.1-PeanutButter-v0.0.2-7B",
        "full_model_name":"PeanutJar\/Mistral-v0.1-PeanutButter-v0.0.2-7B",
        "Parameters":7.0,
        "MMLU_average":0.6437709051,
        "arc:challenge|25":0.5844709898,
        "hellaswag|10":0.6389165505,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.6296296296,
        "MMLU_astronomy":0.6710526316,
        "MMLU_business_ethics":0.6,
        "MMLU_clinical_knowledge":0.6943396226,
        "MMLU_college_biology":0.7291666667,
        "MMLU_college_chemistry":0.5,
        "MMLU_college_computer_science":0.58,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.6416184971,
        "MMLU_college_physics":0.4411764706,
        "MMLU_computer_security":0.78,
        "MMLU_conceptual_physics":0.5659574468,
        "MMLU_econometrics":0.5,
        "MMLU_electrical_engineering":0.5586206897,
        "MMLU_elementary_mathematics":0.3835978836,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.7580645161,
        "MMLU_high_school_chemistry":0.5369458128,
        "MMLU_high_school_computer_science":0.71,
        "MMLU_high_school_european_history":0.7757575758,
        "MMLU_high_school_geography":0.7676767677,
        "MMLU_high_school_government_and_politics":0.8756476684,
        "MMLU_high_school_macroeconomics":0.6461538462,
        "MMLU_high_school_mathematics":0.3518518519,
        "MMLU_high_school_microeconomics":0.6302521008,
        "MMLU_high_school_physics":0.3642384106,
        "MMLU_high_school_psychology":0.8220183486,
        "MMLU_high_school_statistics":0.5509259259,
        "MMLU_high_school_us_history":0.7892156863,
        "MMLU_high_school_world_history":0.7679324895,
        "MMLU_human_aging":0.6905829596,
        "MMLU_human_sexuality":0.7938931298,
        "MMLU_international_law":0.8016528926,
        "MMLU_jurisprudence":0.7777777778,
        "MMLU_logical_fallacies":0.7852760736,
        "MMLU_machine_learning":0.5,
        "MMLU_management":0.8349514563,
        "MMLU_marketing":0.8717948718,
        "MMLU_medical_genetics":0.74,
        "MMLU_miscellaneous":0.8122605364,
        "MMLU_moral_disputes":0.7167630058,
        "MMLU_moral_scenarios":0.3329608939,
        "MMLU_nutrition":0.7549019608,
        "MMLU_philosophy":0.7138263666,
        "MMLU_prehistory":0.75,
        "MMLU_professional_accounting":0.5070921986,
        "MMLU_professional_law":0.4556714472,
        "MMLU_professional_medicine":0.6727941176,
        "MMLU_professional_psychology":0.6830065359,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.7306122449,
        "MMLU_sociology":0.8308457711,
        "MMLU_us_foreign_policy":0.88,
        "MMLU_virology":0.5542168675,
        "MMLU_world_religions":0.8187134503
    },
    {
        "Model":"airoboros-65b-gpt4-m2.0",
        "URL":"https:\/\/huggingface.co\/jondurbin\/airoboros-65b-gpt4-m2.0",
        "full_model_name":"jondurbin\/airoboros-65b-gpt4-m2.0",
        "Parameters":65.0,
        "MMLU_average":0.6437325306,
        "arc:challenge|25":0.6322525597,
        "hellaswag|10":0.6721768572,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.5777777778,
        "MMLU_astronomy":0.7368421053,
        "MMLU_business_ethics":0.64,
        "MMLU_clinical_knowledge":0.6603773585,
        "MMLU_college_biology":0.7013888889,
        "MMLU_college_chemistry":0.48,
        "MMLU_college_computer_science":0.51,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.5664739884,
        "MMLU_college_physics":0.431372549,
        "MMLU_computer_security":0.79,
        "MMLU_conceptual_physics":0.6085106383,
        "MMLU_econometrics":0.4298245614,
        "MMLU_electrical_engineering":0.5655172414,
        "MMLU_elementary_mathematics":0.3862433862,
        "MMLU_formal_logic":0.4444444444,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.7677419355,
        "MMLU_high_school_chemistry":0.4581280788,
        "MMLU_high_school_computer_science":0.72,
        "MMLU_high_school_european_history":0.8121212121,
        "MMLU_high_school_geography":0.8181818182,
        "MMLU_high_school_government_and_politics":0.8756476684,
        "MMLU_high_school_macroeconomics":0.6769230769,
        "MMLU_high_school_mathematics":0.3222222222,
        "MMLU_high_school_microeconomics":0.6890756303,
        "MMLU_high_school_physics":0.3973509934,
        "MMLU_high_school_psychology":0.8348623853,
        "MMLU_high_school_statistics":0.5648148148,
        "MMLU_high_school_us_history":0.8382352941,
        "MMLU_high_school_world_history":0.8312236287,
        "MMLU_human_aging":0.668161435,
        "MMLU_human_sexuality":0.7328244275,
        "MMLU_international_law":0.7933884298,
        "MMLU_jurisprudence":0.712962963,
        "MMLU_logical_fallacies":0.7484662577,
        "MMLU_machine_learning":0.4553571429,
        "MMLU_management":0.8349514563,
        "MMLU_marketing":0.8632478632,
        "MMLU_medical_genetics":0.68,
        "MMLU_miscellaneous":0.816091954,
        "MMLU_moral_disputes":0.7283236994,
        "MMLU_moral_scenarios":0.4636871508,
        "MMLU_nutrition":0.6960784314,
        "MMLU_philosophy":0.7331189711,
        "MMLU_prehistory":0.75,
        "MMLU_professional_accounting":0.5141843972,
        "MMLU_professional_law":0.4863102999,
        "MMLU_professional_medicine":0.6323529412,
        "MMLU_professional_psychology":0.6683006536,
        "MMLU_public_relations":0.7545454545,
        "MMLU_security_studies":0.7632653061,
        "MMLU_sociology":0.8208955224,
        "MMLU_us_foreign_policy":0.85,
        "MMLU_virology":0.5180722892,
        "MMLU_world_religions":0.8128654971
    },
    {
        "Model":"Platypus-30B",
        "URL":"https:\/\/huggingface.co\/garage-bAInd\/Platypus-30B",
        "full_model_name":"garage-bAInd\/Platypus-30B",
        "Parameters":30.0,
        "MMLU_average":0.6422907709,
        "arc:challenge|25":0.6126279863,
        "hellaswag|10":0.6362278431,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.5407407407,
        "MMLU_astronomy":0.6973684211,
        "MMLU_business_ethics":0.64,
        "MMLU_clinical_knowledge":0.6377358491,
        "MMLU_college_biology":0.7222222222,
        "MMLU_college_chemistry":0.48,
        "MMLU_college_computer_science":0.53,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.5722543353,
        "MMLU_college_physics":0.4019607843,
        "MMLU_computer_security":0.73,
        "MMLU_conceptual_physics":0.570212766,
        "MMLU_econometrics":0.3596491228,
        "MMLU_electrical_engineering":0.5931034483,
        "MMLU_elementary_mathematics":0.417989418,
        "MMLU_formal_logic":0.4444444444,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.7516129032,
        "MMLU_high_school_chemistry":0.4975369458,
        "MMLU_high_school_computer_science":0.72,
        "MMLU_high_school_european_history":0.8,
        "MMLU_high_school_geography":0.8232323232,
        "MMLU_high_school_government_and_politics":0.8860103627,
        "MMLU_high_school_macroeconomics":0.6333333333,
        "MMLU_high_school_mathematics":0.3555555556,
        "MMLU_high_school_microeconomics":0.7100840336,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.8495412844,
        "MMLU_high_school_statistics":0.5231481481,
        "MMLU_high_school_us_history":0.8578431373,
        "MMLU_high_school_world_history":0.8438818565,
        "MMLU_human_aging":0.7174887892,
        "MMLU_human_sexuality":0.7251908397,
        "MMLU_international_law":0.8347107438,
        "MMLU_jurisprudence":0.75,
        "MMLU_logical_fallacies":0.7484662577,
        "MMLU_machine_learning":0.5178571429,
        "MMLU_management":0.7961165049,
        "MMLU_marketing":0.8760683761,
        "MMLU_medical_genetics":0.7,
        "MMLU_miscellaneous":0.8020434227,
        "MMLU_moral_disputes":0.7225433526,
        "MMLU_moral_scenarios":0.4916201117,
        "MMLU_nutrition":0.6993464052,
        "MMLU_philosophy":0.7234726688,
        "MMLU_prehistory":0.7469135802,
        "MMLU_professional_accounting":0.5106382979,
        "MMLU_professional_law":0.5280312907,
        "MMLU_professional_medicine":0.5992647059,
        "MMLU_professional_psychology":0.6895424837,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.7714285714,
        "MMLU_sociology":0.8208955224,
        "MMLU_us_foreign_policy":0.9,
        "MMLU_virology":0.5361445783,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"Mistral-7B-OpenOrca-lora",
        "URL":"https:\/\/huggingface.co\/uukuguy\/Mistral-7B-OpenOrca-lora",
        "full_model_name":"uukuguy\/Mistral-7B-OpenOrca-lora",
        "Parameters":7.0,
        "MMLU_average":0.6416230888,
        "arc:challenge|25":0.5742320819,
        "hellaswag|10":0.6357299343,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.6444444444,
        "MMLU_astronomy":0.6644736842,
        "MMLU_business_ethics":0.58,
        "MMLU_clinical_knowledge":0.679245283,
        "MMLU_college_biology":0.7361111111,
        "MMLU_college_chemistry":0.49,
        "MMLU_college_computer_science":0.52,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.6358381503,
        "MMLU_college_physics":0.3823529412,
        "MMLU_computer_security":0.79,
        "MMLU_conceptual_physics":0.5787234043,
        "MMLU_econometrics":0.5087719298,
        "MMLU_electrical_engineering":0.5793103448,
        "MMLU_elementary_mathematics":0.373015873,
        "MMLU_formal_logic":0.4047619048,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.7580645161,
        "MMLU_high_school_chemistry":0.5270935961,
        "MMLU_high_school_computer_science":0.7,
        "MMLU_high_school_european_history":0.7757575758,
        "MMLU_high_school_geography":0.7626262626,
        "MMLU_high_school_government_and_politics":0.8860103627,
        "MMLU_high_school_macroeconomics":0.6666666667,
        "MMLU_high_school_mathematics":0.3481481481,
        "MMLU_high_school_microeconomics":0.6638655462,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.8220183486,
        "MMLU_high_school_statistics":0.5509259259,
        "MMLU_high_school_us_history":0.8039215686,
        "MMLU_high_school_world_history":0.7637130802,
        "MMLU_human_aging":0.6950672646,
        "MMLU_human_sexuality":0.786259542,
        "MMLU_international_law":0.7933884298,
        "MMLU_jurisprudence":0.7777777778,
        "MMLU_logical_fallacies":0.7975460123,
        "MMLU_machine_learning":0.4910714286,
        "MMLU_management":0.8058252427,
        "MMLU_marketing":0.8888888889,
        "MMLU_medical_genetics":0.76,
        "MMLU_miscellaneous":0.8109833972,
        "MMLU_moral_disputes":0.7196531792,
        "MMLU_moral_scenarios":0.3318435754,
        "MMLU_nutrition":0.7516339869,
        "MMLU_philosophy":0.7073954984,
        "MMLU_prehistory":0.7283950617,
        "MMLU_professional_accounting":0.4858156028,
        "MMLU_professional_law":0.4524119948,
        "MMLU_professional_medicine":0.6764705882,
        "MMLU_professional_psychology":0.6781045752,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.7346938776,
        "MMLU_sociology":0.8407960199,
        "MMLU_us_foreign_policy":0.87,
        "MMLU_virology":0.5542168675,
        "MMLU_world_religions":0.8304093567
    },
    {
        "Model":"mistral-7b-platypus-fp16",
        "URL":"https:\/\/huggingface.co\/bhenrym14\/mistral-7b-platypus-fp16",
        "full_model_name":"bhenrym14\/mistral-7b-platypus-fp16",
        "Parameters":7.0,
        "MMLU_average":0.6411285501,
        "arc:challenge|25":0.587883959,
        "hellaswag|10":0.6414060944,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.6222222222,
        "MMLU_astronomy":0.6842105263,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.6641509434,
        "MMLU_college_biology":0.7222222222,
        "MMLU_college_chemistry":0.5,
        "MMLU_college_computer_science":0.53,
        "MMLU_college_mathematics":0.41,
        "MMLU_college_medicine":0.6416184971,
        "MMLU_college_physics":0.3921568627,
        "MMLU_computer_security":0.77,
        "MMLU_conceptual_physics":0.5531914894,
        "MMLU_econometrics":0.4385964912,
        "MMLU_electrical_engineering":0.524137931,
        "MMLU_elementary_mathematics":0.4285714286,
        "MMLU_formal_logic":0.4523809524,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.7612903226,
        "MMLU_high_school_chemistry":0.5123152709,
        "MMLU_high_school_computer_science":0.69,
        "MMLU_high_school_european_history":0.7696969697,
        "MMLU_high_school_geography":0.7929292929,
        "MMLU_high_school_government_and_politics":0.896373057,
        "MMLU_high_school_macroeconomics":0.6512820513,
        "MMLU_high_school_mathematics":0.3296296296,
        "MMLU_high_school_microeconomics":0.6806722689,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.8275229358,
        "MMLU_high_school_statistics":0.5185185185,
        "MMLU_high_school_us_history":0.7892156863,
        "MMLU_high_school_world_history":0.7848101266,
        "MMLU_human_aging":0.6905829596,
        "MMLU_human_sexuality":0.7938931298,
        "MMLU_international_law":0.8181818182,
        "MMLU_jurisprudence":0.7962962963,
        "MMLU_logical_fallacies":0.7484662577,
        "MMLU_machine_learning":0.4553571429,
        "MMLU_management":0.8252427184,
        "MMLU_marketing":0.8632478632,
        "MMLU_medical_genetics":0.72,
        "MMLU_miscellaneous":0.8173690932,
        "MMLU_moral_disputes":0.7225433526,
        "MMLU_moral_scenarios":0.3932960894,
        "MMLU_nutrition":0.7712418301,
        "MMLU_philosophy":0.7041800643,
        "MMLU_prehistory":0.7469135802,
        "MMLU_professional_accounting":0.4574468085,
        "MMLU_professional_law":0.4778357236,
        "MMLU_professional_medicine":0.6875,
        "MMLU_professional_psychology":0.6846405229,
        "MMLU_public_relations":0.6272727273,
        "MMLU_security_studies":0.7183673469,
        "MMLU_sociology":0.8208955224,
        "MMLU_us_foreign_policy":0.89,
        "MMLU_virology":0.5481927711,
        "MMLU_world_religions":0.8362573099
    },
    {
        "Model":"Tess-XS-v1.0",
        "URL":"https:\/\/huggingface.co\/migtissera\/Tess-XS-v1.0",
        "full_model_name":"migtissera\/Tess-XS-v1.0",
        "Parameters":null,
        "MMLU_average":0.641038422,
        "arc:challenge|25":0.5750853242,
        "hellaswag|10":0.6381198964,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.637037037,
        "MMLU_astronomy":0.6644736842,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.6981132075,
        "MMLU_college_biology":0.7013888889,
        "MMLU_college_chemistry":0.48,
        "MMLU_college_computer_science":0.53,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.6358381503,
        "MMLU_college_physics":0.4215686275,
        "MMLU_computer_security":0.79,
        "MMLU_conceptual_physics":0.5829787234,
        "MMLU_econometrics":0.4736842105,
        "MMLU_electrical_engineering":0.5448275862,
        "MMLU_elementary_mathematics":0.3994708995,
        "MMLU_formal_logic":0.3968253968,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.7677419355,
        "MMLU_high_school_chemistry":0.5024630542,
        "MMLU_high_school_computer_science":0.69,
        "MMLU_high_school_european_history":0.7515151515,
        "MMLU_high_school_geography":0.7727272727,
        "MMLU_high_school_government_and_politics":0.8860103627,
        "MMLU_high_school_macroeconomics":0.6564102564,
        "MMLU_high_school_mathematics":0.3481481481,
        "MMLU_high_school_microeconomics":0.6764705882,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.8275229358,
        "MMLU_high_school_statistics":0.5324074074,
        "MMLU_high_school_us_history":0.8137254902,
        "MMLU_high_school_world_history":0.7721518987,
        "MMLU_human_aging":0.6950672646,
        "MMLU_human_sexuality":0.786259542,
        "MMLU_international_law":0.7933884298,
        "MMLU_jurisprudence":0.7777777778,
        "MMLU_logical_fallacies":0.8098159509,
        "MMLU_machine_learning":0.5178571429,
        "MMLU_management":0.8058252427,
        "MMLU_marketing":0.8547008547,
        "MMLU_medical_genetics":0.73,
        "MMLU_miscellaneous":0.8173690932,
        "MMLU_moral_disputes":0.7167630058,
        "MMLU_moral_scenarios":0.3351955307,
        "MMLU_nutrition":0.7385620915,
        "MMLU_philosophy":0.6945337621,
        "MMLU_prehistory":0.7314814815,
        "MMLU_professional_accounting":0.5212765957,
        "MMLU_professional_law":0.4485006519,
        "MMLU_professional_medicine":0.6801470588,
        "MMLU_professional_psychology":0.6781045752,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.7469387755,
        "MMLU_sociology":0.8457711443,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.5301204819,
        "MMLU_world_religions":0.8245614035
    },
    {
        "Model":"Mistral-11B-TestBench7",
        "URL":"https:\/\/huggingface.co\/Undi95\/Mistral-11B-TestBench7",
        "full_model_name":"Undi95\/Mistral-11B-TestBench7",
        "Parameters":11.0,
        "MMLU_average":0.6408707446,
        "arc:challenge|25":0.590443686,
        "hellaswag|10":0.6343357897,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.6444444444,
        "MMLU_astronomy":0.6710526316,
        "MMLU_business_ethics":0.58,
        "MMLU_clinical_knowledge":0.6943396226,
        "MMLU_college_biology":0.7083333333,
        "MMLU_college_chemistry":0.47,
        "MMLU_college_computer_science":0.54,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.6647398844,
        "MMLU_college_physics":0.4411764706,
        "MMLU_computer_security":0.79,
        "MMLU_conceptual_physics":0.5361702128,
        "MMLU_econometrics":0.4912280702,
        "MMLU_electrical_engineering":0.5379310345,
        "MMLU_elementary_mathematics":0.4153439153,
        "MMLU_formal_logic":0.4206349206,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.7806451613,
        "MMLU_high_school_chemistry":0.4975369458,
        "MMLU_high_school_computer_science":0.7,
        "MMLU_high_school_european_history":0.7696969697,
        "MMLU_high_school_geography":0.803030303,
        "MMLU_high_school_government_and_politics":0.8808290155,
        "MMLU_high_school_macroeconomics":0.6846153846,
        "MMLU_high_school_mathematics":0.3333333333,
        "MMLU_high_school_microeconomics":0.6596638655,
        "MMLU_high_school_physics":0.3708609272,
        "MMLU_high_school_psychology":0.8293577982,
        "MMLU_high_school_statistics":0.6064814815,
        "MMLU_high_school_us_history":0.8039215686,
        "MMLU_high_school_world_history":0.7721518987,
        "MMLU_human_aging":0.6547085202,
        "MMLU_human_sexuality":0.7633587786,
        "MMLU_international_law":0.7685950413,
        "MMLU_jurisprudence":0.7685185185,
        "MMLU_logical_fallacies":0.7607361963,
        "MMLU_machine_learning":0.4642857143,
        "MMLU_management":0.7961165049,
        "MMLU_marketing":0.8632478632,
        "MMLU_medical_genetics":0.71,
        "MMLU_miscellaneous":0.8109833972,
        "MMLU_moral_disputes":0.6965317919,
        "MMLU_moral_scenarios":0.3810055866,
        "MMLU_nutrition":0.7320261438,
        "MMLU_philosophy":0.7041800643,
        "MMLU_prehistory":0.7191358025,
        "MMLU_professional_accounting":0.4822695035,
        "MMLU_professional_law":0.4367666232,
        "MMLU_professional_medicine":0.6911764706,
        "MMLU_professional_psychology":0.660130719,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.7265306122,
        "MMLU_sociology":0.8457711443,
        "MMLU_us_foreign_policy":0.86,
        "MMLU_virology":0.5361445783,
        "MMLU_world_religions":0.8362573099
    },
    {
        "Model":"WizardLM-70B-V1.0",
        "URL":"https:\/\/huggingface.co\/WizardLM\/WizardLM-70B-V1.0",
        "full_model_name":"WizardLM\/WizardLM-70B-V1.0",
        "Parameters":70.0,
        "MMLU_average":0.6404982727,
        "arc:challenge|25":0.614334471,
        "hellaswag|10":0.653455487,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.6973684211,
        "MMLU_business_ethics":0.66,
        "MMLU_clinical_knowledge":0.679245283,
        "MMLU_college_biology":0.7708333333,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.52,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.5953757225,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.5829787234,
        "MMLU_econometrics":0.4122807018,
        "MMLU_electrical_engineering":0.5586206897,
        "MMLU_elementary_mathematics":0.4126984127,
        "MMLU_formal_logic":0.4682539683,
        "MMLU_global_facts":0.46,
        "MMLU_high_school_biology":0.7580645161,
        "MMLU_high_school_chemistry":0.4876847291,
        "MMLU_high_school_computer_science":0.71,
        "MMLU_high_school_european_history":0.8181818182,
        "MMLU_high_school_geography":0.797979798,
        "MMLU_high_school_government_and_politics":0.896373057,
        "MMLU_high_school_macroeconomics":0.6333333333,
        "MMLU_high_school_mathematics":0.3111111111,
        "MMLU_high_school_microeconomics":0.7100840336,
        "MMLU_high_school_physics":0.4437086093,
        "MMLU_high_school_psychology":0.8458715596,
        "MMLU_high_school_statistics":0.5,
        "MMLU_high_school_us_history":0.8480392157,
        "MMLU_high_school_world_history":0.8481012658,
        "MMLU_human_aging":0.7040358744,
        "MMLU_human_sexuality":0.7709923664,
        "MMLU_international_law":0.7933884298,
        "MMLU_jurisprudence":0.7592592593,
        "MMLU_logical_fallacies":0.754601227,
        "MMLU_machine_learning":0.4910714286,
        "MMLU_management":0.8155339806,
        "MMLU_marketing":0.8717948718,
        "MMLU_medical_genetics":0.65,
        "MMLU_miscellaneous":0.8275862069,
        "MMLU_moral_disputes":0.725433526,
        "MMLU_moral_scenarios":0.4625698324,
        "MMLU_nutrition":0.6928104575,
        "MMLU_philosophy":0.6977491961,
        "MMLU_prehistory":0.725308642,
        "MMLU_professional_accounting":0.5177304965,
        "MMLU_professional_law":0.5019556714,
        "MMLU_professional_medicine":0.5735294118,
        "MMLU_professional_psychology":0.6928104575,
        "MMLU_public_relations":0.7272727273,
        "MMLU_security_studies":0.7551020408,
        "MMLU_sociology":0.8358208955,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.5120481928,
        "MMLU_world_religions":0.8304093567
    },
    {
        "Model":"WizardLM-70B-V1.0-HF",
        "URL":"https:\/\/huggingface.co\/simsim314\/WizardLM-70B-V1.0-HF",
        "full_model_name":"simsim314\/WizardLM-70B-V1.0-HF",
        "Parameters":70.0,
        "MMLU_average":0.6404982727,
        "arc:challenge|25":0.614334471,
        "hellaswag|10":0.653455487,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.6973684211,
        "MMLU_business_ethics":0.66,
        "MMLU_clinical_knowledge":0.679245283,
        "MMLU_college_biology":0.7708333333,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.52,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.5953757225,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.5829787234,
        "MMLU_econometrics":0.4122807018,
        "MMLU_electrical_engineering":0.5586206897,
        "MMLU_elementary_mathematics":0.4126984127,
        "MMLU_formal_logic":0.4682539683,
        "MMLU_global_facts":0.46,
        "MMLU_high_school_biology":0.7580645161,
        "MMLU_high_school_chemistry":0.4876847291,
        "MMLU_high_school_computer_science":0.71,
        "MMLU_high_school_european_history":0.8181818182,
        "MMLU_high_school_geography":0.797979798,
        "MMLU_high_school_government_and_politics":0.896373057,
        "MMLU_high_school_macroeconomics":0.6333333333,
        "MMLU_high_school_mathematics":0.3111111111,
        "MMLU_high_school_microeconomics":0.7100840336,
        "MMLU_high_school_physics":0.4437086093,
        "MMLU_high_school_psychology":0.8458715596,
        "MMLU_high_school_statistics":0.5,
        "MMLU_high_school_us_history":0.8480392157,
        "MMLU_high_school_world_history":0.8481012658,
        "MMLU_human_aging":0.7040358744,
        "MMLU_human_sexuality":0.7709923664,
        "MMLU_international_law":0.7933884298,
        "MMLU_jurisprudence":0.7592592593,
        "MMLU_logical_fallacies":0.754601227,
        "MMLU_machine_learning":0.4910714286,
        "MMLU_management":0.8155339806,
        "MMLU_marketing":0.8717948718,
        "MMLU_medical_genetics":0.65,
        "MMLU_miscellaneous":0.8275862069,
        "MMLU_moral_disputes":0.725433526,
        "MMLU_moral_scenarios":0.4625698324,
        "MMLU_nutrition":0.6928104575,
        "MMLU_philosophy":0.6977491961,
        "MMLU_prehistory":0.725308642,
        "MMLU_professional_accounting":0.5177304965,
        "MMLU_professional_law":0.5019556714,
        "MMLU_professional_medicine":0.5735294118,
        "MMLU_professional_psychology":0.6928104575,
        "MMLU_public_relations":0.7272727273,
        "MMLU_security_studies":0.7551020408,
        "MMLU_sociology":0.8358208955,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.5120481928,
        "MMLU_world_religions":0.8304093567
    },
    {
        "Model":"llama-65b",
        "URL":"https:\/\/huggingface.co\/huggyllama\/llama-65b",
        "full_model_name":"huggyllama\/llama-65b",
        "Parameters":65.0,
        "MMLU_average":0.6393064176,
        "arc:challenge|25":0.5938566553,
        "hellaswag|10":0.665305716,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.5777777778,
        "MMLU_astronomy":0.7171052632,
        "MMLU_business_ethics":0.6,
        "MMLU_clinical_knowledge":0.6603773585,
        "MMLU_college_biology":0.7013888889,
        "MMLU_college_chemistry":0.49,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.5549132948,
        "MMLU_college_physics":0.3823529412,
        "MMLU_computer_security":0.79,
        "MMLU_conceptual_physics":0.5829787234,
        "MMLU_econometrics":0.3771929825,
        "MMLU_electrical_engineering":0.5586206897,
        "MMLU_elementary_mathematics":0.4047619048,
        "MMLU_formal_logic":0.4365079365,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.7483870968,
        "MMLU_high_school_chemistry":0.4137931034,
        "MMLU_high_school_computer_science":0.68,
        "MMLU_high_school_european_history":0.7878787879,
        "MMLU_high_school_geography":0.803030303,
        "MMLU_high_school_government_and_politics":0.8808290155,
        "MMLU_high_school_macroeconomics":0.6615384615,
        "MMLU_high_school_mathematics":0.3407407407,
        "MMLU_high_school_microeconomics":0.6806722689,
        "MMLU_high_school_physics":0.3774834437,
        "MMLU_high_school_psychology":0.8293577982,
        "MMLU_high_school_statistics":0.6064814815,
        "MMLU_high_school_us_history":0.8382352941,
        "MMLU_high_school_world_history":0.8396624473,
        "MMLU_human_aging":0.66367713,
        "MMLU_human_sexuality":0.7709923664,
        "MMLU_international_law":0.8181818182,
        "MMLU_jurisprudence":0.7592592593,
        "MMLU_logical_fallacies":0.7791411043,
        "MMLU_machine_learning":0.4910714286,
        "MMLU_management":0.8252427184,
        "MMLU_marketing":0.8760683761,
        "MMLU_medical_genetics":0.68,
        "MMLU_miscellaneous":0.816091954,
        "MMLU_moral_disputes":0.7369942197,
        "MMLU_moral_scenarios":0.4804469274,
        "MMLU_nutrition":0.6830065359,
        "MMLU_philosophy":0.7266881029,
        "MMLU_prehistory":0.7469135802,
        "MMLU_professional_accounting":0.4929078014,
        "MMLU_professional_law":0.4954367666,
        "MMLU_professional_medicine":0.6213235294,
        "MMLU_professional_psychology":0.6650326797,
        "MMLU_public_relations":0.7454545455,
        "MMLU_security_studies":0.7142857143,
        "MMLU_sociology":0.8009950249,
        "MMLU_us_foreign_policy":0.88,
        "MMLU_virology":0.5421686747,
        "MMLU_world_religions":0.8070175439
    },
    {
        "Model":"samantha-1.2-mistral-7b",
        "URL":"https:\/\/huggingface.co\/ehartford\/samantha-1.2-mistral-7b",
        "full_model_name":"ehartford\/samantha-1.2-mistral-7b",
        "Parameters":7.0,
        "MMLU_average":0.6391082947,
        "arc:challenge|25":0.5989761092,
        "hellaswag|10":0.6651065525,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.6296296296,
        "MMLU_astronomy":0.6776315789,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.6830188679,
        "MMLU_college_biology":0.7291666667,
        "MMLU_college_chemistry":0.46,
        "MMLU_college_computer_science":0.55,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.612716763,
        "MMLU_college_physics":0.4215686275,
        "MMLU_computer_security":0.76,
        "MMLU_conceptual_physics":0.5914893617,
        "MMLU_econometrics":0.4824561404,
        "MMLU_electrical_engineering":0.5586206897,
        "MMLU_elementary_mathematics":0.4047619048,
        "MMLU_formal_logic":0.4365079365,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.7612903226,
        "MMLU_high_school_chemistry":0.5369458128,
        "MMLU_high_school_computer_science":0.72,
        "MMLU_high_school_european_history":0.7515151515,
        "MMLU_high_school_geography":0.7828282828,
        "MMLU_high_school_government_and_politics":0.8860103627,
        "MMLU_high_school_macroeconomics":0.6435897436,
        "MMLU_high_school_mathematics":0.3555555556,
        "MMLU_high_school_microeconomics":0.6638655462,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.8165137615,
        "MMLU_high_school_statistics":0.5231481481,
        "MMLU_high_school_us_history":0.7843137255,
        "MMLU_high_school_world_history":0.7890295359,
        "MMLU_human_aging":0.6860986547,
        "MMLU_human_sexuality":0.7633587786,
        "MMLU_international_law":0.7851239669,
        "MMLU_jurisprudence":0.7962962963,
        "MMLU_logical_fallacies":0.7791411043,
        "MMLU_machine_learning":0.4910714286,
        "MMLU_management":0.8252427184,
        "MMLU_marketing":0.8675213675,
        "MMLU_medical_genetics":0.74,
        "MMLU_miscellaneous":0.8148148148,
        "MMLU_moral_disputes":0.7225433526,
        "MMLU_moral_scenarios":0.3106145251,
        "MMLU_nutrition":0.7581699346,
        "MMLU_philosophy":0.691318328,
        "MMLU_prehistory":0.7469135802,
        "MMLU_professional_accounting":0.4716312057,
        "MMLU_professional_law":0.4556714472,
        "MMLU_professional_medicine":0.6801470588,
        "MMLU_professional_psychology":0.6846405229,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.7183673469,
        "MMLU_sociology":0.8208955224,
        "MMLU_us_foreign_policy":0.87,
        "MMLU_virology":0.5361445783,
        "MMLU_world_religions":0.8187134503
    },
    {
        "Model":"Llama-2-70b-chat-hf",
        "URL":"https:\/\/huggingface.co\/meta-llama\/Llama-2-70b-chat-hf",
        "full_model_name":"meta-llama\/Llama-2-70b-chat-hf",
        "Parameters":70.0,
        "MMLU_average":0.6390701953,
        "arc:challenge|25":0.6049488055,
        "hellaswag|10":0.669388568,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.5185185185,
        "MMLU_astronomy":0.7302631579,
        "MMLU_business_ethics":0.65,
        "MMLU_clinical_knowledge":0.6377358491,
        "MMLU_college_biology":0.75,
        "MMLU_college_chemistry":0.48,
        "MMLU_college_computer_science":0.59,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.6011560694,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.5829787234,
        "MMLU_econometrics":0.4122807018,
        "MMLU_electrical_engineering":0.5793103448,
        "MMLU_elementary_mathematics":0.4100529101,
        "MMLU_formal_logic":0.4126984127,
        "MMLU_global_facts":0.43,
        "MMLU_high_school_biology":0.764516129,
        "MMLU_high_school_chemistry":0.4630541872,
        "MMLU_high_school_computer_science":0.65,
        "MMLU_high_school_european_history":0.8181818182,
        "MMLU_high_school_geography":0.8080808081,
        "MMLU_high_school_government_and_politics":0.8911917098,
        "MMLU_high_school_macroeconomics":0.641025641,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.6596638655,
        "MMLU_high_school_physics":0.4238410596,
        "MMLU_high_school_psychology":0.8385321101,
        "MMLU_high_school_statistics":0.4814814815,
        "MMLU_high_school_us_history":0.8578431373,
        "MMLU_high_school_world_history":0.8438818565,
        "MMLU_human_aging":0.7264573991,
        "MMLU_human_sexuality":0.7099236641,
        "MMLU_international_law":0.8016528926,
        "MMLU_jurisprudence":0.8240740741,
        "MMLU_logical_fallacies":0.7607361963,
        "MMLU_machine_learning":0.4821428571,
        "MMLU_management":0.8058252427,
        "MMLU_marketing":0.8717948718,
        "MMLU_medical_genetics":0.65,
        "MMLU_miscellaneous":0.8275862069,
        "MMLU_moral_disputes":0.7167630058,
        "MMLU_moral_scenarios":0.3955307263,
        "MMLU_nutrition":0.6993464052,
        "MMLU_philosophy":0.7041800643,
        "MMLU_prehistory":0.7098765432,
        "MMLU_professional_accounting":0.5070921986,
        "MMLU_professional_law":0.4771838331,
        "MMLU_professional_medicine":0.5772058824,
        "MMLU_professional_psychology":0.6699346405,
        "MMLU_public_relations":0.6909090909,
        "MMLU_security_studies":0.787755102,
        "MMLU_sociology":0.8706467662,
        "MMLU_us_foreign_policy":0.87,
        "MMLU_virology":0.5120481928,
        "MMLU_world_religions":0.8187134503
    },
    {
        "Model":"Mistral-11B-TestBench10",
        "URL":"https:\/\/huggingface.co\/Undi95\/Mistral-11B-TestBench10",
        "full_model_name":"Undi95\/Mistral-11B-TestBench10",
        "Parameters":11.0,
        "MMLU_average":0.6389740633,
        "arc:challenge|25":0.6220136519,
        "hellaswag|10":0.6533559052,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.6222222222,
        "MMLU_astronomy":0.6710526316,
        "MMLU_business_ethics":0.58,
        "MMLU_clinical_knowledge":0.7056603774,
        "MMLU_college_biology":0.7222222222,
        "MMLU_college_chemistry":0.47,
        "MMLU_college_computer_science":0.53,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.6705202312,
        "MMLU_college_physics":0.4117647059,
        "MMLU_computer_security":0.77,
        "MMLU_conceptual_physics":0.5446808511,
        "MMLU_econometrics":0.4736842105,
        "MMLU_electrical_engineering":0.5517241379,
        "MMLU_elementary_mathematics":0.3941798942,
        "MMLU_formal_logic":0.4285714286,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.7451612903,
        "MMLU_high_school_chemistry":0.4926108374,
        "MMLU_high_school_computer_science":0.69,
        "MMLU_high_school_european_history":0.7636363636,
        "MMLU_high_school_geography":0.797979798,
        "MMLU_high_school_government_and_politics":0.8860103627,
        "MMLU_high_school_macroeconomics":0.6820512821,
        "MMLU_high_school_mathematics":0.3666666667,
        "MMLU_high_school_microeconomics":0.6512605042,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.8348623853,
        "MMLU_high_school_statistics":0.5416666667,
        "MMLU_high_school_us_history":0.8137254902,
        "MMLU_high_school_world_history":0.7890295359,
        "MMLU_human_aging":0.668161435,
        "MMLU_human_sexuality":0.786259542,
        "MMLU_international_law":0.7685950413,
        "MMLU_jurisprudence":0.7777777778,
        "MMLU_logical_fallacies":0.7852760736,
        "MMLU_machine_learning":0.5,
        "MMLU_management":0.7961165049,
        "MMLU_marketing":0.8717948718,
        "MMLU_medical_genetics":0.71,
        "MMLU_miscellaneous":0.8122605364,
        "MMLU_moral_disputes":0.7080924855,
        "MMLU_moral_scenarios":0.4078212291,
        "MMLU_nutrition":0.7124183007,
        "MMLU_philosophy":0.6977491961,
        "MMLU_prehistory":0.7098765432,
        "MMLU_professional_accounting":0.4609929078,
        "MMLU_professional_law":0.4498044329,
        "MMLU_professional_medicine":0.6838235294,
        "MMLU_professional_psychology":0.6633986928,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.7346938776,
        "MMLU_sociology":0.855721393,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.5421686747,
        "MMLU_world_religions":0.8362573099
    },
    {
        "Model":"airoboros-65b-gpt4-1.3",
        "URL":"https:\/\/huggingface.co\/jondurbin\/airoboros-65b-gpt4-1.3",
        "full_model_name":"jondurbin\/airoboros-65b-gpt4-1.3",
        "Parameters":65.0,
        "MMLU_average":0.6388563701,
        "arc:challenge|25":0.635665529,
        "hellaswag|10":0.68084047,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.5185185185,
        "MMLU_astronomy":0.7434210526,
        "MMLU_business_ethics":0.63,
        "MMLU_clinical_knowledge":0.6452830189,
        "MMLU_college_biology":0.6666666667,
        "MMLU_college_chemistry":0.52,
        "MMLU_college_computer_science":0.54,
        "MMLU_college_mathematics":0.4,
        "MMLU_college_medicine":0.5722543353,
        "MMLU_college_physics":0.3921568627,
        "MMLU_computer_security":0.78,
        "MMLU_conceptual_physics":0.5659574468,
        "MMLU_econometrics":0.3859649123,
        "MMLU_electrical_engineering":0.5724137931,
        "MMLU_elementary_mathematics":0.3703703704,
        "MMLU_formal_logic":0.4682539683,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.7612903226,
        "MMLU_high_school_chemistry":0.4482758621,
        "MMLU_high_school_computer_science":0.68,
        "MMLU_high_school_european_history":0.7818181818,
        "MMLU_high_school_geography":0.8080808081,
        "MMLU_high_school_government_and_politics":0.9067357513,
        "MMLU_high_school_macroeconomics":0.6641025641,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.7016806723,
        "MMLU_high_school_physics":0.3774834437,
        "MMLU_high_school_psychology":0.8110091743,
        "MMLU_high_school_statistics":0.6157407407,
        "MMLU_high_school_us_history":0.8333333333,
        "MMLU_high_school_world_history":0.835443038,
        "MMLU_human_aging":0.6412556054,
        "MMLU_human_sexuality":0.7404580153,
        "MMLU_international_law":0.7933884298,
        "MMLU_jurisprudence":0.7407407407,
        "MMLU_logical_fallacies":0.7852760736,
        "MMLU_machine_learning":0.4285714286,
        "MMLU_management":0.786407767,
        "MMLU_marketing":0.8803418803,
        "MMLU_medical_genetics":0.64,
        "MMLU_miscellaneous":0.7892720307,
        "MMLU_moral_disputes":0.7543352601,
        "MMLU_moral_scenarios":0.4837988827,
        "MMLU_nutrition":0.6895424837,
        "MMLU_philosophy":0.7202572347,
        "MMLU_prehistory":0.7314814815,
        "MMLU_professional_accounting":0.5070921986,
        "MMLU_professional_law":0.4915254237,
        "MMLU_professional_medicine":0.6654411765,
        "MMLU_professional_psychology":0.6486928105,
        "MMLU_public_relations":0.7272727273,
        "MMLU_security_studies":0.7673469388,
        "MMLU_sociology":0.815920398,
        "MMLU_us_foreign_policy":0.88,
        "MMLU_virology":0.5240963855,
        "MMLU_world_religions":0.8245614035
    },
    {
        "Model":"Aria-70B",
        "URL":"https:\/\/huggingface.co\/Faradaylab\/Aria-70B",
        "full_model_name":"Faradaylab\/Aria-70B",
        "Parameters":70.0,
        "MMLU_average":0.6387572189,
        "arc:challenge|25":0.6049488055,
        "hellaswag|10":0.6690898227,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.5185185185,
        "MMLU_astronomy":0.7302631579,
        "MMLU_business_ethics":0.65,
        "MMLU_clinical_knowledge":0.6377358491,
        "MMLU_college_biology":0.75,
        "MMLU_college_chemistry":0.48,
        "MMLU_college_computer_science":0.59,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.6069364162,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.5829787234,
        "MMLU_econometrics":0.4122807018,
        "MMLU_electrical_engineering":0.5793103448,
        "MMLU_elementary_mathematics":0.4100529101,
        "MMLU_formal_logic":0.4126984127,
        "MMLU_global_facts":0.44,
        "MMLU_high_school_biology":0.7677419355,
        "MMLU_high_school_chemistry":0.4630541872,
        "MMLU_high_school_computer_science":0.65,
        "MMLU_high_school_european_history":0.7939393939,
        "MMLU_high_school_geography":0.803030303,
        "MMLU_high_school_government_and_politics":0.8911917098,
        "MMLU_high_school_macroeconomics":0.641025641,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.6596638655,
        "MMLU_high_school_physics":0.4304635762,
        "MMLU_high_school_psychology":0.8385321101,
        "MMLU_high_school_statistics":0.4814814815,
        "MMLU_high_school_us_history":0.8529411765,
        "MMLU_high_school_world_history":0.8438818565,
        "MMLU_human_aging":0.7264573991,
        "MMLU_human_sexuality":0.7099236641,
        "MMLU_international_law":0.8016528926,
        "MMLU_jurisprudence":0.8240740741,
        "MMLU_logical_fallacies":0.7607361963,
        "MMLU_machine_learning":0.4821428571,
        "MMLU_management":0.8058252427,
        "MMLU_marketing":0.8760683761,
        "MMLU_medical_genetics":0.65,
        "MMLU_miscellaneous":0.8275862069,
        "MMLU_moral_disputes":0.7167630058,
        "MMLU_moral_scenarios":0.3932960894,
        "MMLU_nutrition":0.6993464052,
        "MMLU_philosophy":0.7041800643,
        "MMLU_prehistory":0.7098765432,
        "MMLU_professional_accounting":0.5070921986,
        "MMLU_professional_law":0.4791395046,
        "MMLU_professional_medicine":0.5772058824,
        "MMLU_professional_psychology":0.6666666667,
        "MMLU_public_relations":0.6909090909,
        "MMLU_security_studies":0.787755102,
        "MMLU_sociology":0.8706467662,
        "MMLU_us_foreign_policy":0.87,
        "MMLU_virology":0.5120481928,
        "MMLU_world_religions":0.8187134503
    },
    {
        "Model":"Mistral-7B-claude-instruct",
        "URL":"https:\/\/huggingface.co\/Norquinal\/Mistral-7B-claude-instruct",
        "full_model_name":"Norquinal\/Mistral-7B-claude-instruct",
        "Parameters":7.0,
        "MMLU_average":0.6383713385,
        "arc:challenge|25":0.6023890785,
        "hellaswag|10":0.6502688707,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.6296296296,
        "MMLU_astronomy":0.6710526316,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.6905660377,
        "MMLU_college_biology":0.7152777778,
        "MMLU_college_chemistry":0.48,
        "MMLU_college_computer_science":0.54,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.676300578,
        "MMLU_college_physics":0.3921568627,
        "MMLU_computer_security":0.78,
        "MMLU_conceptual_physics":0.5872340426,
        "MMLU_econometrics":0.5175438596,
        "MMLU_electrical_engineering":0.5586206897,
        "MMLU_elementary_mathematics":0.4047619048,
        "MMLU_formal_logic":0.4206349206,
        "MMLU_global_facts":0.41,
        "MMLU_high_school_biology":0.7774193548,
        "MMLU_high_school_chemistry":0.5418719212,
        "MMLU_high_school_computer_science":0.7,
        "MMLU_high_school_european_history":0.7454545455,
        "MMLU_high_school_geography":0.7828282828,
        "MMLU_high_school_government_and_politics":0.8652849741,
        "MMLU_high_school_macroeconomics":0.6333333333,
        "MMLU_high_school_mathematics":0.3777777778,
        "MMLU_high_school_microeconomics":0.6596638655,
        "MMLU_high_school_physics":0.3642384106,
        "MMLU_high_school_psychology":0.8110091743,
        "MMLU_high_school_statistics":0.5509259259,
        "MMLU_high_school_us_history":0.7745098039,
        "MMLU_high_school_world_history":0.7637130802,
        "MMLU_human_aging":0.668161435,
        "MMLU_human_sexuality":0.786259542,
        "MMLU_international_law":0.8099173554,
        "MMLU_jurisprudence":0.7407407407,
        "MMLU_logical_fallacies":0.8036809816,
        "MMLU_machine_learning":0.4285714286,
        "MMLU_management":0.7961165049,
        "MMLU_marketing":0.8888888889,
        "MMLU_medical_genetics":0.75,
        "MMLU_miscellaneous":0.8122605364,
        "MMLU_moral_disputes":0.7080924855,
        "MMLU_moral_scenarios":0.3463687151,
        "MMLU_nutrition":0.7549019608,
        "MMLU_philosophy":0.6945337621,
        "MMLU_prehistory":0.7222222222,
        "MMLU_professional_accounting":0.475177305,
        "MMLU_professional_law":0.4426336375,
        "MMLU_professional_medicine":0.6764705882,
        "MMLU_professional_psychology":0.6683006536,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.7183673469,
        "MMLU_sociology":0.8208955224,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.5361445783,
        "MMLU_world_religions":0.8421052632
    },
    {
        "Model":"OpenHermes-2.5-Mistral-7B",
        "URL":"https:\/\/huggingface.co\/teknium\/OpenHermes-2.5-Mistral-7B",
        "full_model_name":"teknium\/OpenHermes-2.5-Mistral-7B",
        "Parameters":7.0,
        "MMLU_average":0.6382166828,
        "arc:challenge|25":0.6126279863,
        "hellaswag|10":0.6519617606,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.6074074074,
        "MMLU_astronomy":0.6973684211,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.6867924528,
        "MMLU_college_biology":0.7569444444,
        "MMLU_college_chemistry":0.48,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.6242774566,
        "MMLU_college_physics":0.3823529412,
        "MMLU_computer_security":0.76,
        "MMLU_conceptual_physics":0.5659574468,
        "MMLU_econometrics":0.4912280702,
        "MMLU_electrical_engineering":0.524137931,
        "MMLU_elementary_mathematics":0.4259259259,
        "MMLU_formal_logic":0.4682539683,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.7935483871,
        "MMLU_high_school_chemistry":0.5172413793,
        "MMLU_high_school_computer_science":0.68,
        "MMLU_high_school_european_history":0.7818181818,
        "MMLU_high_school_geography":0.8080808081,
        "MMLU_high_school_government_and_politics":0.8911917098,
        "MMLU_high_school_macroeconomics":0.6128205128,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.6806722689,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.8330275229,
        "MMLU_high_school_statistics":0.5092592593,
        "MMLU_high_school_us_history":0.7990196078,
        "MMLU_high_school_world_history":0.8143459916,
        "MMLU_human_aging":0.7040358744,
        "MMLU_human_sexuality":0.7938931298,
        "MMLU_international_law":0.7603305785,
        "MMLU_jurisprudence":0.787037037,
        "MMLU_logical_fallacies":0.7852760736,
        "MMLU_machine_learning":0.5089285714,
        "MMLU_management":0.7766990291,
        "MMLU_marketing":0.858974359,
        "MMLU_medical_genetics":0.71,
        "MMLU_miscellaneous":0.8301404853,
        "MMLU_moral_disputes":0.7167630058,
        "MMLU_moral_scenarios":0.3083798883,
        "MMLU_nutrition":0.7549019608,
        "MMLU_philosophy":0.6848874598,
        "MMLU_prehistory":0.7530864198,
        "MMLU_professional_accounting":0.5070921986,
        "MMLU_professional_law":0.4693611473,
        "MMLU_professional_medicine":0.6764705882,
        "MMLU_professional_psychology":0.6732026144,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.7346938776,
        "MMLU_sociology":0.815920398,
        "MMLU_us_foreign_policy":0.87,
        "MMLU_virology":0.5542168675,
        "MMLU_world_religions":0.8362573099
    },
    {
        "Model":"Dans-TotSirocco-7b",
        "URL":"https:\/\/huggingface.co\/PocketDoc\/Dans-TotSirocco-7b",
        "full_model_name":"PocketDoc\/Dans-TotSirocco-7b",
        "Parameters":7.0,
        "MMLU_average":0.6380139853,
        "arc:challenge|25":0.5895904437,
        "hellaswag|10":0.6389165505,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.6222222222,
        "MMLU_astronomy":0.6644736842,
        "MMLU_business_ethics":0.58,
        "MMLU_clinical_knowledge":0.6943396226,
        "MMLU_college_biology":0.7152777778,
        "MMLU_college_chemistry":0.5,
        "MMLU_college_computer_science":0.54,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.6242774566,
        "MMLU_college_physics":0.431372549,
        "MMLU_computer_security":0.78,
        "MMLU_conceptual_physics":0.5531914894,
        "MMLU_econometrics":0.4824561404,
        "MMLU_electrical_engineering":0.5586206897,
        "MMLU_elementary_mathematics":0.4206349206,
        "MMLU_formal_logic":0.4047619048,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.7516129032,
        "MMLU_high_school_chemistry":0.5024630542,
        "MMLU_high_school_computer_science":0.7,
        "MMLU_high_school_european_history":0.7454545455,
        "MMLU_high_school_geography":0.7626262626,
        "MMLU_high_school_government_and_politics":0.8652849741,
        "MMLU_high_school_macroeconomics":0.6615384615,
        "MMLU_high_school_mathematics":0.3481481481,
        "MMLU_high_school_microeconomics":0.6554621849,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.8256880734,
        "MMLU_high_school_statistics":0.5138888889,
        "MMLU_high_school_us_history":0.8088235294,
        "MMLU_high_school_world_history":0.7637130802,
        "MMLU_human_aging":0.6591928251,
        "MMLU_human_sexuality":0.786259542,
        "MMLU_international_law":0.8099173554,
        "MMLU_jurisprudence":0.787037037,
        "MMLU_logical_fallacies":0.7852760736,
        "MMLU_machine_learning":0.4910714286,
        "MMLU_management":0.8349514563,
        "MMLU_marketing":0.8504273504,
        "MMLU_medical_genetics":0.7,
        "MMLU_miscellaneous":0.8148148148,
        "MMLU_moral_disputes":0.6994219653,
        "MMLU_moral_scenarios":0.3284916201,
        "MMLU_nutrition":0.7418300654,
        "MMLU_philosophy":0.7073954984,
        "MMLU_prehistory":0.7345679012,
        "MMLU_professional_accounting":0.4858156028,
        "MMLU_professional_law":0.444589309,
        "MMLU_professional_medicine":0.6654411765,
        "MMLU_professional_psychology":0.681372549,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.7428571429,
        "MMLU_sociology":0.8457711443,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.5421686747,
        "MMLU_world_religions":0.8304093567
    },
    {
        "Model":"Mistral-7B-v0.1-Open-Platypus",
        "URL":"https:\/\/huggingface.co\/akjindal53244\/Mistral-7B-v0.1-Open-Platypus",
        "full_model_name":"akjindal53244\/Mistral-7B-v0.1-Open-Platypus",
        "Parameters":7.0,
        "MMLU_average":0.6379097738,
        "arc:challenge|25":0.5836177474,
        "hellaswag|10":0.6590320653,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.562962963,
        "MMLU_astronomy":0.6973684211,
        "MMLU_business_ethics":0.58,
        "MMLU_clinical_knowledge":0.6754716981,
        "MMLU_college_biology":0.7013888889,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.54,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.676300578,
        "MMLU_college_physics":0.4019607843,
        "MMLU_computer_security":0.75,
        "MMLU_conceptual_physics":0.5574468085,
        "MMLU_econometrics":0.4473684211,
        "MMLU_electrical_engineering":0.5448275862,
        "MMLU_elementary_mathematics":0.4047619048,
        "MMLU_formal_logic":0.4920634921,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.7225806452,
        "MMLU_high_school_chemistry":0.5024630542,
        "MMLU_high_school_computer_science":0.69,
        "MMLU_high_school_european_history":0.7575757576,
        "MMLU_high_school_geography":0.7525252525,
        "MMLU_high_school_government_and_politics":0.896373057,
        "MMLU_high_school_macroeconomics":0.6179487179,
        "MMLU_high_school_mathematics":0.3666666667,
        "MMLU_high_school_microeconomics":0.6596638655,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.8146788991,
        "MMLU_high_school_statistics":0.4861111111,
        "MMLU_high_school_us_history":0.7892156863,
        "MMLU_high_school_world_history":0.7890295359,
        "MMLU_human_aging":0.7264573991,
        "MMLU_human_sexuality":0.7709923664,
        "MMLU_international_law":0.8016528926,
        "MMLU_jurisprudence":0.8055555556,
        "MMLU_logical_fallacies":0.7852760736,
        "MMLU_machine_learning":0.5,
        "MMLU_management":0.8252427184,
        "MMLU_marketing":0.8888888889,
        "MMLU_medical_genetics":0.7,
        "MMLU_miscellaneous":0.8071519796,
        "MMLU_moral_disputes":0.7167630058,
        "MMLU_moral_scenarios":0.3530726257,
        "MMLU_nutrition":0.7450980392,
        "MMLU_philosophy":0.7138263666,
        "MMLU_prehistory":0.7561728395,
        "MMLU_professional_accounting":0.4858156028,
        "MMLU_professional_law":0.498696219,
        "MMLU_professional_medicine":0.6838235294,
        "MMLU_professional_psychology":0.6862745098,
        "MMLU_public_relations":0.6818181818,
        "MMLU_security_studies":0.706122449,
        "MMLU_sociology":0.8358208955,
        "MMLU_us_foreign_policy":0.87,
        "MMLU_virology":0.5481927711,
        "MMLU_world_religions":0.8070175439
    },
    {
        "Model":"speechless-mistral-dolphin-orca-platypus-samantha-7b",
        "URL":"https:\/\/huggingface.co\/uukuguy\/speechless-mistral-dolphin-orca-platypus-samantha-7b",
        "full_model_name":"uukuguy\/speechless-mistral-dolphin-orca-platypus-samantha-7b",
        "Parameters":7.0,
        "MMLU_average":0.6372371984,
        "arc:challenge|25":0.6083617747,
        "hellaswag|10":0.6489743079,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.6074074074,
        "MMLU_astronomy":0.6973684211,
        "MMLU_business_ethics":0.63,
        "MMLU_clinical_knowledge":0.6905660377,
        "MMLU_college_biology":0.7291666667,
        "MMLU_college_chemistry":0.47,
        "MMLU_college_computer_science":0.52,
        "MMLU_college_mathematics":0.41,
        "MMLU_college_medicine":0.6300578035,
        "MMLU_college_physics":0.3529411765,
        "MMLU_computer_security":0.79,
        "MMLU_conceptual_physics":0.5574468085,
        "MMLU_econometrics":0.4561403509,
        "MMLU_electrical_engineering":0.5862068966,
        "MMLU_elementary_mathematics":0.4021164021,
        "MMLU_formal_logic":0.4285714286,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.7774193548,
        "MMLU_high_school_chemistry":0.5123152709,
        "MMLU_high_school_computer_science":0.72,
        "MMLU_high_school_european_history":0.7515151515,
        "MMLU_high_school_geography":0.7676767677,
        "MMLU_high_school_government_and_politics":0.8601036269,
        "MMLU_high_school_macroeconomics":0.6461538462,
        "MMLU_high_school_mathematics":0.337037037,
        "MMLU_high_school_microeconomics":0.6638655462,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.8275229358,
        "MMLU_high_school_statistics":0.5,
        "MMLU_high_school_us_history":0.7990196078,
        "MMLU_high_school_world_history":0.8016877637,
        "MMLU_human_aging":0.6905829596,
        "MMLU_human_sexuality":0.7786259542,
        "MMLU_international_law":0.8099173554,
        "MMLU_jurisprudence":0.787037037,
        "MMLU_logical_fallacies":0.7484662577,
        "MMLU_machine_learning":0.4642857143,
        "MMLU_management":0.8058252427,
        "MMLU_marketing":0.8675213675,
        "MMLU_medical_genetics":0.75,
        "MMLU_miscellaneous":0.8237547893,
        "MMLU_moral_disputes":0.7080924855,
        "MMLU_moral_scenarios":0.3687150838,
        "MMLU_nutrition":0.6862745098,
        "MMLU_philosophy":0.6945337621,
        "MMLU_prehistory":0.7222222222,
        "MMLU_professional_accounting":0.4645390071,
        "MMLU_professional_law":0.4674054759,
        "MMLU_professional_medicine":0.6617647059,
        "MMLU_professional_psychology":0.6486928105,
        "MMLU_public_relations":0.6727272727,
        "MMLU_security_studies":0.7306122449,
        "MMLU_sociology":0.8358208955,
        "MMLU_us_foreign_policy":0.87,
        "MMLU_virology":0.5421686747,
        "MMLU_world_religions":0.8362573099
    },
    {
        "Model":"WizardLM-70B-V1.0-GPTQ",
        "URL":"https:\/\/huggingface.co\/TheBloke\/WizardLM-70B-V1.0-GPTQ",
        "full_model_name":"TheBloke\/WizardLM-70B-V1.0-GPTQ",
        "Parameters":70.0,
        "MMLU_average":0.6367595228,
        "arc:challenge|25":0.6075085324,
        "hellaswag|10":0.6486755626,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.5185185185,
        "MMLU_astronomy":0.7171052632,
        "MMLU_business_ethics":0.7,
        "MMLU_clinical_knowledge":0.6603773585,
        "MMLU_college_biology":0.7430555556,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.53,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.6184971098,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.5617021277,
        "MMLU_econometrics":0.3684210526,
        "MMLU_electrical_engineering":0.5655172414,
        "MMLU_elementary_mathematics":0.4100529101,
        "MMLU_formal_logic":0.4365079365,
        "MMLU_global_facts":0.4,
        "MMLU_high_school_biology":0.7516129032,
        "MMLU_high_school_chemistry":0.5024630542,
        "MMLU_high_school_computer_science":0.71,
        "MMLU_high_school_european_history":0.8181818182,
        "MMLU_high_school_geography":0.7878787879,
        "MMLU_high_school_government_and_politics":0.896373057,
        "MMLU_high_school_macroeconomics":0.6128205128,
        "MMLU_high_school_mathematics":0.3148148148,
        "MMLU_high_school_microeconomics":0.6848739496,
        "MMLU_high_school_physics":0.4370860927,
        "MMLU_high_school_psychology":0.8422018349,
        "MMLU_high_school_statistics":0.4814814815,
        "MMLU_high_school_us_history":0.8382352941,
        "MMLU_high_school_world_history":0.8438818565,
        "MMLU_human_aging":0.7174887892,
        "MMLU_human_sexuality":0.786259542,
        "MMLU_international_law":0.8181818182,
        "MMLU_jurisprudence":0.75,
        "MMLU_logical_fallacies":0.7484662577,
        "MMLU_machine_learning":0.5089285714,
        "MMLU_management":0.7961165049,
        "MMLU_marketing":0.8846153846,
        "MMLU_medical_genetics":0.68,
        "MMLU_miscellaneous":0.8186462324,
        "MMLU_moral_disputes":0.6878612717,
        "MMLU_moral_scenarios":0.3519553073,
        "MMLU_nutrition":0.6764705882,
        "MMLU_philosophy":0.6881028939,
        "MMLU_prehistory":0.7067901235,
        "MMLU_professional_accounting":0.4893617021,
        "MMLU_professional_law":0.5097783572,
        "MMLU_professional_medicine":0.5808823529,
        "MMLU_professional_psychology":0.6764705882,
        "MMLU_public_relations":0.7363636364,
        "MMLU_security_studies":0.7632653061,
        "MMLU_sociology":0.8656716418,
        "MMLU_us_foreign_policy":0.85,
        "MMLU_virology":0.5421686747,
        "MMLU_world_religions":0.8362573099
    },
    {
        "Model":"Mistral-v0.1-PeanutButter-v0.0.5-SFT-7B-QLoRA",
        "URL":"https:\/\/huggingface.co\/PeanutJar\/Mistral-v0.1-PeanutButter-v0.0.5-SFT-7B-QLoRA",
        "full_model_name":"PeanutJar\/Mistral-v0.1-PeanutButter-v0.0.5-SFT-7B-QLoRA",
        "Parameters":7.0,
        "MMLU_average":0.6366450229,
        "arc:challenge|25":0.5733788396,
        "hellaswag|10":0.639514041,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.6222222222,
        "MMLU_astronomy":0.6710526316,
        "MMLU_business_ethics":0.61,
        "MMLU_clinical_knowledge":0.7056603774,
        "MMLU_college_biology":0.7291666667,
        "MMLU_college_chemistry":0.49,
        "MMLU_college_computer_science":0.53,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.6011560694,
        "MMLU_college_physics":0.4215686275,
        "MMLU_computer_security":0.76,
        "MMLU_conceptual_physics":0.5744680851,
        "MMLU_econometrics":0.5087719298,
        "MMLU_electrical_engineering":0.5103448276,
        "MMLU_elementary_mathematics":0.3994708995,
        "MMLU_formal_logic":0.4047619048,
        "MMLU_global_facts":0.41,
        "MMLU_high_school_biology":0.7483870968,
        "MMLU_high_school_chemistry":0.5024630542,
        "MMLU_high_school_computer_science":0.7,
        "MMLU_high_school_european_history":0.7757575758,
        "MMLU_high_school_geography":0.7727272727,
        "MMLU_high_school_government_and_politics":0.8756476684,
        "MMLU_high_school_macroeconomics":0.6307692308,
        "MMLU_high_school_mathematics":0.3333333333,
        "MMLU_high_school_microeconomics":0.6218487395,
        "MMLU_high_school_physics":0.3708609272,
        "MMLU_high_school_psychology":0.8146788991,
        "MMLU_high_school_statistics":0.5,
        "MMLU_high_school_us_history":0.7696078431,
        "MMLU_high_school_world_history":0.7679324895,
        "MMLU_human_aging":0.6816143498,
        "MMLU_human_sexuality":0.8167938931,
        "MMLU_international_law":0.7768595041,
        "MMLU_jurisprudence":0.787037037,
        "MMLU_logical_fallacies":0.7852760736,
        "MMLU_machine_learning":0.5357142857,
        "MMLU_management":0.8252427184,
        "MMLU_marketing":0.8760683761,
        "MMLU_medical_genetics":0.7,
        "MMLU_miscellaneous":0.8148148148,
        "MMLU_moral_disputes":0.7196531792,
        "MMLU_moral_scenarios":0.3117318436,
        "MMLU_nutrition":0.7745098039,
        "MMLU_philosophy":0.7009646302,
        "MMLU_prehistory":0.7438271605,
        "MMLU_professional_accounting":0.5141843972,
        "MMLU_professional_law":0.4569752282,
        "MMLU_professional_medicine":0.6286764706,
        "MMLU_professional_psychology":0.6732026144,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.7346938776,
        "MMLU_sociology":0.8308457711,
        "MMLU_us_foreign_policy":0.88,
        "MMLU_virology":0.5481927711,
        "MMLU_world_religions":0.8128654971
    },
    {
        "Model":"Mistral-v0.1-PeanutButter-v0.0.5-DPO-7B-QLoRA",
        "URL":"https:\/\/huggingface.co\/PeanutJar\/Mistral-v0.1-PeanutButter-v0.0.5-DPO-7B-QLoRA",
        "full_model_name":"PeanutJar\/Mistral-v0.1-PeanutButter-v0.0.5-DPO-7B-QLoRA",
        "Parameters":7.0,
        "MMLU_average":0.6362833878,
        "arc:challenge|25":0.5793515358,
        "hellaswag|10":0.6418044214,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.6222222222,
        "MMLU_astronomy":0.6710526316,
        "MMLU_business_ethics":0.6,
        "MMLU_clinical_knowledge":0.7094339623,
        "MMLU_college_biology":0.7291666667,
        "MMLU_college_chemistry":0.49,
        "MMLU_college_computer_science":0.55,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.6011560694,
        "MMLU_college_physics":0.4117647059,
        "MMLU_computer_security":0.76,
        "MMLU_conceptual_physics":0.5744680851,
        "MMLU_econometrics":0.5087719298,
        "MMLU_electrical_engineering":0.5172413793,
        "MMLU_elementary_mathematics":0.4074074074,
        "MMLU_formal_logic":0.3968253968,
        "MMLU_global_facts":0.4,
        "MMLU_high_school_biology":0.7483870968,
        "MMLU_high_school_chemistry":0.5024630542,
        "MMLU_high_school_computer_science":0.69,
        "MMLU_high_school_european_history":0.7757575758,
        "MMLU_high_school_geography":0.7727272727,
        "MMLU_high_school_government_and_politics":0.8756476684,
        "MMLU_high_school_macroeconomics":0.6333333333,
        "MMLU_high_school_mathematics":0.3185185185,
        "MMLU_high_school_microeconomics":0.6176470588,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.8146788991,
        "MMLU_high_school_statistics":0.4953703704,
        "MMLU_high_school_us_history":0.7843137255,
        "MMLU_high_school_world_history":0.7637130802,
        "MMLU_human_aging":0.6860986547,
        "MMLU_human_sexuality":0.8167938931,
        "MMLU_international_law":0.7768595041,
        "MMLU_jurisprudence":0.787037037,
        "MMLU_logical_fallacies":0.7975460123,
        "MMLU_machine_learning":0.5357142857,
        "MMLU_management":0.8349514563,
        "MMLU_marketing":0.8846153846,
        "MMLU_medical_genetics":0.71,
        "MMLU_miscellaneous":0.816091954,
        "MMLU_moral_disputes":0.7138728324,
        "MMLU_moral_scenarios":0.3162011173,
        "MMLU_nutrition":0.7712418301,
        "MMLU_philosophy":0.7009646302,
        "MMLU_prehistory":0.75,
        "MMLU_professional_accounting":0.4964539007,
        "MMLU_professional_law":0.4563233377,
        "MMLU_professional_medicine":0.6433823529,
        "MMLU_professional_psychology":0.6732026144,
        "MMLU_public_relations":0.6272727273,
        "MMLU_security_studies":0.7306122449,
        "MMLU_sociology":0.8308457711,
        "MMLU_us_foreign_policy":0.88,
        "MMLU_virology":0.5361445783,
        "MMLU_world_religions":0.8128654971
    },
    {
        "Model":"h2ogpt-research-oasst1-llama-65b",
        "URL":"https:\/\/huggingface.co\/h2oai\/h2ogpt-research-oasst1-llama-65b",
        "full_model_name":"h2oai\/h2ogpt-research-oasst1-llama-65b",
        "Parameters":65.0,
        "MMLU_average":0.6356872582,
        "arc:challenge|25":0.6177474403,
        "hellaswag|10":0.6664011153,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.562962963,
        "MMLU_astronomy":0.75,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.6339622642,
        "MMLU_college_biology":0.7083333333,
        "MMLU_college_chemistry":0.51,
        "MMLU_college_computer_science":0.52,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5549132948,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.75,
        "MMLU_conceptual_physics":0.6,
        "MMLU_econometrics":0.3859649123,
        "MMLU_electrical_engineering":0.5310344828,
        "MMLU_elementary_mathematics":0.3783068783,
        "MMLU_formal_logic":0.4206349206,
        "MMLU_global_facts":0.4,
        "MMLU_high_school_biology":0.7483870968,
        "MMLU_high_school_chemistry":0.4039408867,
        "MMLU_high_school_computer_science":0.67,
        "MMLU_high_school_european_history":0.8,
        "MMLU_high_school_geography":0.803030303,
        "MMLU_high_school_government_and_politics":0.8756476684,
        "MMLU_high_school_macroeconomics":0.6256410256,
        "MMLU_high_school_mathematics":0.3185185185,
        "MMLU_high_school_microeconomics":0.6722689076,
        "MMLU_high_school_physics":0.3973509934,
        "MMLU_high_school_psychology":0.8275229358,
        "MMLU_high_school_statistics":0.5740740741,
        "MMLU_high_school_us_history":0.8333333333,
        "MMLU_high_school_world_history":0.8396624473,
        "MMLU_human_aging":0.6995515695,
        "MMLU_human_sexuality":0.7404580153,
        "MMLU_international_law":0.8099173554,
        "MMLU_jurisprudence":0.7314814815,
        "MMLU_logical_fallacies":0.7791411043,
        "MMLU_machine_learning":0.5178571429,
        "MMLU_management":0.8446601942,
        "MMLU_marketing":0.8717948718,
        "MMLU_medical_genetics":0.68,
        "MMLU_miscellaneous":0.8148148148,
        "MMLU_moral_disputes":0.7196531792,
        "MMLU_moral_scenarios":0.4770949721,
        "MMLU_nutrition":0.6764705882,
        "MMLU_philosophy":0.7459807074,
        "MMLU_prehistory":0.7530864198,
        "MMLU_professional_accounting":0.4893617021,
        "MMLU_professional_law":0.4830508475,
        "MMLU_professional_medicine":0.6066176471,
        "MMLU_professional_psychology":0.6732026144,
        "MMLU_public_relations":0.7272727273,
        "MMLU_security_studies":0.6653061224,
        "MMLU_sociology":0.8358208955,
        "MMLU_us_foreign_policy":0.87,
        "MMLU_virology":0.5481927711,
        "MMLU_world_religions":0.8245614035
    },
    {
        "Model":"llama-2-70B-ensemble-v8",
        "URL":"https:\/\/huggingface.co\/yeontaek\/llama-2-70B-ensemble-v8",
        "full_model_name":"yeontaek\/llama-2-70B-ensemble-v8",
        "Parameters":70.0,
        "MMLU_average":0.6355505048,
        "arc:challenge|25":0.6561433447,
        "hellaswag|10":0.6623182633,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.5333333333,
        "MMLU_astronomy":0.6842105263,
        "MMLU_business_ethics":0.68,
        "MMLU_clinical_knowledge":0.7056603774,
        "MMLU_college_biology":0.75,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.6069364162,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.72,
        "MMLU_conceptual_physics":0.629787234,
        "MMLU_econometrics":0.3684210526,
        "MMLU_electrical_engineering":0.5517241379,
        "MMLU_elementary_mathematics":0.3941798942,
        "MMLU_formal_logic":0.4285714286,
        "MMLU_global_facts":0.44,
        "MMLU_high_school_biology":0.7741935484,
        "MMLU_high_school_chemistry":0.4532019704,
        "MMLU_high_school_computer_science":0.6,
        "MMLU_high_school_european_history":0.7878787879,
        "MMLU_high_school_geography":0.8535353535,
        "MMLU_high_school_government_and_politics":0.9067357513,
        "MMLU_high_school_macroeconomics":0.6641025641,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.6890756303,
        "MMLU_high_school_physics":0.4039735099,
        "MMLU_high_school_psychology":0.8440366972,
        "MMLU_high_school_statistics":0.5277777778,
        "MMLU_high_school_us_history":0.8431372549,
        "MMLU_high_school_world_history":0.8396624473,
        "MMLU_human_aging":0.7443946188,
        "MMLU_human_sexuality":0.7480916031,
        "MMLU_international_law":0.7851239669,
        "MMLU_jurisprudence":0.7777777778,
        "MMLU_logical_fallacies":0.773006135,
        "MMLU_machine_learning":0.5357142857,
        "MMLU_management":0.786407767,
        "MMLU_marketing":0.8547008547,
        "MMLU_medical_genetics":0.65,
        "MMLU_miscellaneous":0.8263090677,
        "MMLU_moral_disputes":0.7341040462,
        "MMLU_moral_scenarios":0.3687150838,
        "MMLU_nutrition":0.6830065359,
        "MMLU_philosophy":0.7266881029,
        "MMLU_prehistory":0.7654320988,
        "MMLU_professional_accounting":0.4964539007,
        "MMLU_professional_law":0.5208604954,
        "MMLU_professional_medicine":0.5955882353,
        "MMLU_professional_psychology":0.6797385621,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.6693877551,
        "MMLU_sociology":0.7960199005,
        "MMLU_us_foreign_policy":0.88,
        "MMLU_virology":0.5060240964,
        "MMLU_world_religions":0.8362573099
    },
    {
        "Model":"vigostral-7b-chat",
        "URL":"https:\/\/huggingface.co\/bofenghuang\/vigostral-7b-chat",
        "full_model_name":"bofenghuang\/vigostral-7b-chat",
        "Parameters":7.0,
        "MMLU_average":0.6353227161,
        "arc:challenge|25":0.5921501706,
        "hellaswag|10":0.6408086039,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.6074074074,
        "MMLU_astronomy":0.6973684211,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.6905660377,
        "MMLU_college_biology":0.7013888889,
        "MMLU_college_chemistry":0.49,
        "MMLU_college_computer_science":0.57,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.6069364162,
        "MMLU_college_physics":0.4117647059,
        "MMLU_computer_security":0.78,
        "MMLU_conceptual_physics":0.5574468085,
        "MMLU_econometrics":0.5087719298,
        "MMLU_electrical_engineering":0.5793103448,
        "MMLU_elementary_mathematics":0.3968253968,
        "MMLU_formal_logic":0.4285714286,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.7709677419,
        "MMLU_high_school_chemistry":0.5024630542,
        "MMLU_high_school_computer_science":0.7,
        "MMLU_high_school_european_history":0.7515151515,
        "MMLU_high_school_geography":0.803030303,
        "MMLU_high_school_government_and_politics":0.8704663212,
        "MMLU_high_school_macroeconomics":0.6512820513,
        "MMLU_high_school_mathematics":0.3851851852,
        "MMLU_high_school_microeconomics":0.6722689076,
        "MMLU_high_school_physics":0.3708609272,
        "MMLU_high_school_psychology":0.8220183486,
        "MMLU_high_school_statistics":0.4768518519,
        "MMLU_high_school_us_history":0.8137254902,
        "MMLU_high_school_world_history":0.7637130802,
        "MMLU_human_aging":0.6726457399,
        "MMLU_human_sexuality":0.7938931298,
        "MMLU_international_law":0.7768595041,
        "MMLU_jurisprudence":0.7314814815,
        "MMLU_logical_fallacies":0.7791411043,
        "MMLU_machine_learning":0.4732142857,
        "MMLU_management":0.786407767,
        "MMLU_marketing":0.858974359,
        "MMLU_medical_genetics":0.7,
        "MMLU_miscellaneous":0.8058748404,
        "MMLU_moral_disputes":0.6965317919,
        "MMLU_moral_scenarios":0.3039106145,
        "MMLU_nutrition":0.7549019608,
        "MMLU_philosophy":0.7009646302,
        "MMLU_prehistory":0.7314814815,
        "MMLU_professional_accounting":0.4858156028,
        "MMLU_professional_law":0.4452411995,
        "MMLU_professional_medicine":0.6654411765,
        "MMLU_professional_psychology":0.6683006536,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.7387755102,
        "MMLU_sociology":0.8407960199,
        "MMLU_us_foreign_policy":0.87,
        "MMLU_virology":0.5421686747,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"jackalope-7b",
        "URL":"https:\/\/huggingface.co\/openaccess-ai-collective\/jackalope-7b",
        "full_model_name":"openaccess-ai-collective\/jackalope-7b",
        "Parameters":7.0,
        "MMLU_average":0.6349721233,
        "arc:challenge|25":0.5981228669,
        "hellaswag|10":0.6398127863,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.6222222222,
        "MMLU_astronomy":0.6907894737,
        "MMLU_business_ethics":0.64,
        "MMLU_clinical_knowledge":0.6830188679,
        "MMLU_college_biology":0.7638888889,
        "MMLU_college_chemistry":0.45,
        "MMLU_college_computer_science":0.51,
        "MMLU_college_mathematics":0.4,
        "MMLU_college_medicine":0.6531791908,
        "MMLU_college_physics":0.3529411765,
        "MMLU_computer_security":0.77,
        "MMLU_conceptual_physics":0.5659574468,
        "MMLU_econometrics":0.4824561404,
        "MMLU_electrical_engineering":0.5517241379,
        "MMLU_elementary_mathematics":0.4259259259,
        "MMLU_formal_logic":0.4365079365,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.7483870968,
        "MMLU_high_school_chemistry":0.4729064039,
        "MMLU_high_school_computer_science":0.66,
        "MMLU_high_school_european_history":0.7393939394,
        "MMLU_high_school_geography":0.7777777778,
        "MMLU_high_school_government_and_politics":0.8704663212,
        "MMLU_high_school_macroeconomics":0.6461538462,
        "MMLU_high_school_mathematics":0.3148148148,
        "MMLU_high_school_microeconomics":0.6890756303,
        "MMLU_high_school_physics":0.3642384106,
        "MMLU_high_school_psychology":0.8256880734,
        "MMLU_high_school_statistics":0.5092592593,
        "MMLU_high_school_us_history":0.8137254902,
        "MMLU_high_school_world_history":0.805907173,
        "MMLU_human_aging":0.6995515695,
        "MMLU_human_sexuality":0.7480916031,
        "MMLU_international_law":0.7851239669,
        "MMLU_jurisprudence":0.8055555556,
        "MMLU_logical_fallacies":0.754601227,
        "MMLU_machine_learning":0.5178571429,
        "MMLU_management":0.786407767,
        "MMLU_marketing":0.8675213675,
        "MMLU_medical_genetics":0.71,
        "MMLU_miscellaneous":0.8250319285,
        "MMLU_moral_disputes":0.7283236994,
        "MMLU_moral_scenarios":0.3106145251,
        "MMLU_nutrition":0.7385620915,
        "MMLU_philosophy":0.6945337621,
        "MMLU_prehistory":0.7037037037,
        "MMLU_professional_accounting":0.4929078014,
        "MMLU_professional_law":0.4543676662,
        "MMLU_professional_medicine":0.6691176471,
        "MMLU_professional_psychology":0.660130719,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.7142857143,
        "MMLU_sociology":0.8258706468,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.5180722892,
        "MMLU_world_religions":0.8304093567
    },
    {
        "Model":"GPlatty-30B",
        "URL":"https:\/\/huggingface.co\/lilloukas\/GPlatty-30B",
        "full_model_name":"lilloukas\/GPlatty-30B",
        "Parameters":30.0,
        "MMLU_average":0.6348906462,
        "arc:challenge|25":0.6365187713,
        "hellaswag|10":0.650169289,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.5407407407,
        "MMLU_astronomy":0.6776315789,
        "MMLU_business_ethics":0.64,
        "MMLU_clinical_knowledge":0.6566037736,
        "MMLU_college_biology":0.6944444444,
        "MMLU_college_chemistry":0.45,
        "MMLU_college_computer_science":0.52,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.5549132948,
        "MMLU_college_physics":0.362745098,
        "MMLU_computer_security":0.72,
        "MMLU_conceptual_physics":0.5489361702,
        "MMLU_econometrics":0.3421052632,
        "MMLU_electrical_engineering":0.5793103448,
        "MMLU_elementary_mathematics":0.3968253968,
        "MMLU_formal_logic":0.4206349206,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.7483870968,
        "MMLU_high_school_chemistry":0.4778325123,
        "MMLU_high_school_computer_science":0.69,
        "MMLU_high_school_european_history":0.7939393939,
        "MMLU_high_school_geography":0.8181818182,
        "MMLU_high_school_government_and_politics":0.8756476684,
        "MMLU_high_school_macroeconomics":0.6333333333,
        "MMLU_high_school_mathematics":0.3185185185,
        "MMLU_high_school_microeconomics":0.6974789916,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.8495412844,
        "MMLU_high_school_statistics":0.5231481481,
        "MMLU_high_school_us_history":0.8578431373,
        "MMLU_high_school_world_history":0.8523206751,
        "MMLU_human_aging":0.7219730942,
        "MMLU_human_sexuality":0.6946564885,
        "MMLU_international_law":0.8181818182,
        "MMLU_jurisprudence":0.7407407407,
        "MMLU_logical_fallacies":0.754601227,
        "MMLU_machine_learning":0.5089285714,
        "MMLU_management":0.8155339806,
        "MMLU_marketing":0.8760683761,
        "MMLU_medical_genetics":0.7,
        "MMLU_miscellaneous":0.7969348659,
        "MMLU_moral_disputes":0.7023121387,
        "MMLU_moral_scenarios":0.4837988827,
        "MMLU_nutrition":0.6764705882,
        "MMLU_philosophy":0.729903537,
        "MMLU_prehistory":0.7469135802,
        "MMLU_professional_accounting":0.5106382979,
        "MMLU_professional_law":0.5156453716,
        "MMLU_professional_medicine":0.5882352941,
        "MMLU_professional_psychology":0.6764705882,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.7755102041,
        "MMLU_sociology":0.8308457711,
        "MMLU_us_foreign_policy":0.92,
        "MMLU_virology":0.5481927711,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"ARIA-70B-V2",
        "URL":"https:\/\/huggingface.co\/Faradaylab\/ARIA-70B-V2",
        "full_model_name":"Faradaylab\/ARIA-70B-V2",
        "Parameters":70.0,
        "MMLU_average":0.6348848193,
        "arc:challenge|25":0.5784982935,
        "hellaswag|10":0.6558454491,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.5333333333,
        "MMLU_astronomy":0.7039473684,
        "MMLU_business_ethics":0.67,
        "MMLU_clinical_knowledge":0.6377358491,
        "MMLU_college_biology":0.7361111111,
        "MMLU_college_chemistry":0.46,
        "MMLU_college_computer_science":0.57,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.5838150289,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.67,
        "MMLU_conceptual_physics":0.6085106383,
        "MMLU_econometrics":0.4122807018,
        "MMLU_electrical_engineering":0.5655172414,
        "MMLU_elementary_mathematics":0.3915343915,
        "MMLU_formal_logic":0.3888888889,
        "MMLU_global_facts":0.46,
        "MMLU_high_school_biology":0.764516129,
        "MMLU_high_school_chemistry":0.5024630542,
        "MMLU_high_school_computer_science":0.63,
        "MMLU_high_school_european_history":0.7696969697,
        "MMLU_high_school_geography":0.7878787879,
        "MMLU_high_school_government_and_politics":0.896373057,
        "MMLU_high_school_macroeconomics":0.6384615385,
        "MMLU_high_school_mathematics":0.3518518519,
        "MMLU_high_school_microeconomics":0.6596638655,
        "MMLU_high_school_physics":0.4238410596,
        "MMLU_high_school_psychology":0.8513761468,
        "MMLU_high_school_statistics":0.4907407407,
        "MMLU_high_school_us_history":0.862745098,
        "MMLU_high_school_world_history":0.8565400844,
        "MMLU_human_aging":0.7399103139,
        "MMLU_human_sexuality":0.7251908397,
        "MMLU_international_law":0.826446281,
        "MMLU_jurisprudence":0.8055555556,
        "MMLU_logical_fallacies":0.7484662577,
        "MMLU_machine_learning":0.5,
        "MMLU_management":0.786407767,
        "MMLU_marketing":0.8547008547,
        "MMLU_medical_genetics":0.67,
        "MMLU_miscellaneous":0.8250319285,
        "MMLU_moral_disputes":0.6965317919,
        "MMLU_moral_scenarios":0.3374301676,
        "MMLU_nutrition":0.6830065359,
        "MMLU_philosophy":0.7073954984,
        "MMLU_prehistory":0.712962963,
        "MMLU_professional_accounting":0.5035460993,
        "MMLU_professional_law":0.4850065189,
        "MMLU_professional_medicine":0.5477941176,
        "MMLU_professional_psychology":0.6781045752,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.7551020408,
        "MMLU_sociology":0.8706467662,
        "MMLU_us_foreign_policy":0.87,
        "MMLU_virology":0.4879518072,
        "MMLU_world_religions":0.8362573099
    },
    {
        "Model":"SynthIA-7B-v1.5",
        "URL":"https:\/\/huggingface.co\/migtissera\/SynthIA-7B-v1.5",
        "full_model_name":"migtissera\/SynthIA-7B-v1.5",
        "Parameters":7.0,
        "MMLU_average":0.6348049447,
        "arc:challenge|25":0.5870307167,
        "hellaswag|10":0.6432981478,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.6148148148,
        "MMLU_astronomy":0.6644736842,
        "MMLU_business_ethics":0.61,
        "MMLU_clinical_knowledge":0.6981132075,
        "MMLU_college_biology":0.6944444444,
        "MMLU_college_chemistry":0.49,
        "MMLU_college_computer_science":0.52,
        "MMLU_college_mathematics":0.4,
        "MMLU_college_medicine":0.6473988439,
        "MMLU_college_physics":0.3823529412,
        "MMLU_computer_security":0.76,
        "MMLU_conceptual_physics":0.5957446809,
        "MMLU_econometrics":0.4385964912,
        "MMLU_electrical_engineering":0.5448275862,
        "MMLU_elementary_mathematics":0.3941798942,
        "MMLU_formal_logic":0.4126984127,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.764516129,
        "MMLU_high_school_chemistry":0.4975369458,
        "MMLU_high_school_computer_science":0.68,
        "MMLU_high_school_european_history":0.7575757576,
        "MMLU_high_school_geography":0.7878787879,
        "MMLU_high_school_government_and_politics":0.8652849741,
        "MMLU_high_school_macroeconomics":0.6641025641,
        "MMLU_high_school_mathematics":0.3592592593,
        "MMLU_high_school_microeconomics":0.7058823529,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.8183486239,
        "MMLU_high_school_statistics":0.5231481481,
        "MMLU_high_school_us_history":0.7990196078,
        "MMLU_high_school_world_history":0.7932489451,
        "MMLU_human_aging":0.6816143498,
        "MMLU_human_sexuality":0.7633587786,
        "MMLU_international_law":0.7851239669,
        "MMLU_jurisprudence":0.787037037,
        "MMLU_logical_fallacies":0.7607361963,
        "MMLU_machine_learning":0.4464285714,
        "MMLU_management":0.7669902913,
        "MMLU_marketing":0.8760683761,
        "MMLU_medical_genetics":0.69,
        "MMLU_miscellaneous":0.8199233716,
        "MMLU_moral_disputes":0.6994219653,
        "MMLU_moral_scenarios":0.3765363128,
        "MMLU_nutrition":0.7156862745,
        "MMLU_philosophy":0.7041800643,
        "MMLU_prehistory":0.7283950617,
        "MMLU_professional_accounting":0.4716312057,
        "MMLU_professional_law":0.4589308996,
        "MMLU_professional_medicine":0.6580882353,
        "MMLU_professional_psychology":0.658496732,
        "MMLU_public_relations":0.6727272727,
        "MMLU_security_studies":0.693877551,
        "MMLU_sociology":0.8358208955,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.5361445783,
        "MMLU_world_religions":0.8187134503
    },
    {
        "Model":"UltraLM-65b",
        "URL":"https:\/\/huggingface.co\/openbmb\/UltraLM-65b",
        "full_model_name":"openbmb\/UltraLM-65b",
        "Parameters":65.0,
        "MMLU_average":0.634797334,
        "arc:challenge|25":0.6313993174,
        "hellaswag|10":0.6514638518,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.5777777778,
        "MMLU_astronomy":0.7434210526,
        "MMLU_business_ethics":0.58,
        "MMLU_clinical_knowledge":0.6566037736,
        "MMLU_college_biology":0.7013888889,
        "MMLU_college_chemistry":0.51,
        "MMLU_college_computer_science":0.56,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.5549132948,
        "MMLU_college_physics":0.362745098,
        "MMLU_computer_security":0.74,
        "MMLU_conceptual_physics":0.5914893617,
        "MMLU_econometrics":0.3596491228,
        "MMLU_electrical_engineering":0.5379310345,
        "MMLU_elementary_mathematics":0.3915343915,
        "MMLU_formal_logic":0.4285714286,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.7451612903,
        "MMLU_high_school_chemistry":0.3891625616,
        "MMLU_high_school_computer_science":0.67,
        "MMLU_high_school_european_history":0.7818181818,
        "MMLU_high_school_geography":0.8080808081,
        "MMLU_high_school_government_and_politics":0.896373057,
        "MMLU_high_school_macroeconomics":0.6282051282,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.6890756303,
        "MMLU_high_school_physics":0.4105960265,
        "MMLU_high_school_psychology":0.8201834862,
        "MMLU_high_school_statistics":0.5185185185,
        "MMLU_high_school_us_history":0.8088235294,
        "MMLU_high_school_world_history":0.8270042194,
        "MMLU_human_aging":0.6726457399,
        "MMLU_human_sexuality":0.7633587786,
        "MMLU_international_law":0.7933884298,
        "MMLU_jurisprudence":0.7314814815,
        "MMLU_logical_fallacies":0.7668711656,
        "MMLU_machine_learning":0.4732142857,
        "MMLU_management":0.8058252427,
        "MMLU_marketing":0.8717948718,
        "MMLU_medical_genetics":0.68,
        "MMLU_miscellaneous":0.8084291188,
        "MMLU_moral_disputes":0.7167630058,
        "MMLU_moral_scenarios":0.4793296089,
        "MMLU_nutrition":0.6862745098,
        "MMLU_philosophy":0.7427652733,
        "MMLU_prehistory":0.7469135802,
        "MMLU_professional_accounting":0.4964539007,
        "MMLU_professional_law":0.4830508475,
        "MMLU_professional_medicine":0.625,
        "MMLU_professional_psychology":0.6666666667,
        "MMLU_public_relations":0.7181818182,
        "MMLU_security_studies":0.7020408163,
        "MMLU_sociology":0.855721393,
        "MMLU_us_foreign_policy":0.86,
        "MMLU_virology":0.5361445783,
        "MMLU_world_religions":0.8421052632
    },
    {
        "Model":"Mistral-11B-SynthIAirOmniMix",
        "URL":"https:\/\/huggingface.co\/NeverSleep\/Mistral-11B-SynthIAirOmniMix",
        "full_model_name":"NeverSleep\/Mistral-11B-SynthIAirOmniMix",
        "Parameters":11.0,
        "MMLU_average":0.6346609566,
        "arc:challenge|25":0.5921501706,
        "hellaswag|10":0.6396136228,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.6074074074,
        "MMLU_astronomy":0.625,
        "MMLU_business_ethics":0.58,
        "MMLU_clinical_knowledge":0.6754716981,
        "MMLU_college_biology":0.7222222222,
        "MMLU_college_chemistry":0.48,
        "MMLU_college_computer_science":0.53,
        "MMLU_college_mathematics":0.41,
        "MMLU_college_medicine":0.6300578035,
        "MMLU_college_physics":0.362745098,
        "MMLU_computer_security":0.76,
        "MMLU_conceptual_physics":0.5574468085,
        "MMLU_econometrics":0.4561403509,
        "MMLU_electrical_engineering":0.5655172414,
        "MMLU_elementary_mathematics":0.3888888889,
        "MMLU_formal_logic":0.3888888889,
        "MMLU_global_facts":0.4,
        "MMLU_high_school_biology":0.7677419355,
        "MMLU_high_school_chemistry":0.4975369458,
        "MMLU_high_school_computer_science":0.69,
        "MMLU_high_school_european_history":0.7575757576,
        "MMLU_high_school_geography":0.8080808081,
        "MMLU_high_school_government_and_politics":0.8756476684,
        "MMLU_high_school_macroeconomics":0.6743589744,
        "MMLU_high_school_mathematics":0.3333333333,
        "MMLU_high_school_microeconomics":0.6848739496,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.8220183486,
        "MMLU_high_school_statistics":0.5092592593,
        "MMLU_high_school_us_history":0.8235294118,
        "MMLU_high_school_world_history":0.7890295359,
        "MMLU_human_aging":0.7085201794,
        "MMLU_human_sexuality":0.7786259542,
        "MMLU_international_law":0.7768595041,
        "MMLU_jurisprudence":0.7592592593,
        "MMLU_logical_fallacies":0.7607361963,
        "MMLU_machine_learning":0.4642857143,
        "MMLU_management":0.7961165049,
        "MMLU_marketing":0.8632478632,
        "MMLU_medical_genetics":0.7,
        "MMLU_miscellaneous":0.816091954,
        "MMLU_moral_disputes":0.6820809249,
        "MMLU_moral_scenarios":0.3731843575,
        "MMLU_nutrition":0.7222222222,
        "MMLU_philosophy":0.6848874598,
        "MMLU_prehistory":0.725308642,
        "MMLU_professional_accounting":0.4822695035,
        "MMLU_professional_law":0.4543676662,
        "MMLU_professional_medicine":0.6838235294,
        "MMLU_professional_psychology":0.6503267974,
        "MMLU_public_relations":0.6727272727,
        "MMLU_security_studies":0.7387755102,
        "MMLU_sociology":0.8457711443,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.5542168675,
        "MMLU_world_religions":0.8245614035
    },
    {
        "Model":"mamba-gpt-7b-v1",
        "URL":"https:\/\/huggingface.co\/CobraMamba\/mamba-gpt-7b-v1",
        "full_model_name":"CobraMamba\/mamba-gpt-7b-v1",
        "Parameters":7.0,
        "MMLU_average":0.6346233132,
        "arc:challenge|25":0.5759385666,
        "hellaswag|10":0.635431189,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.637037037,
        "MMLU_astronomy":0.6315789474,
        "MMLU_business_ethics":0.61,
        "MMLU_clinical_knowledge":0.6905660377,
        "MMLU_college_biology":0.75,
        "MMLU_college_chemistry":0.48,
        "MMLU_college_computer_science":0.57,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.6300578035,
        "MMLU_college_physics":0.4509803922,
        "MMLU_computer_security":0.81,
        "MMLU_conceptual_physics":0.5404255319,
        "MMLU_econometrics":0.5,
        "MMLU_electrical_engineering":0.5793103448,
        "MMLU_elementary_mathematics":0.3994708995,
        "MMLU_formal_logic":0.380952381,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.7612903226,
        "MMLU_high_school_chemistry":0.4778325123,
        "MMLU_high_school_computer_science":0.71,
        "MMLU_high_school_european_history":0.7454545455,
        "MMLU_high_school_geography":0.7878787879,
        "MMLU_high_school_government_and_politics":0.8756476684,
        "MMLU_high_school_macroeconomics":0.6461538462,
        "MMLU_high_school_mathematics":0.3592592593,
        "MMLU_high_school_microeconomics":0.6470588235,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.823853211,
        "MMLU_high_school_statistics":0.4953703704,
        "MMLU_high_school_us_history":0.7843137255,
        "MMLU_high_school_world_history":0.7932489451,
        "MMLU_human_aging":0.6771300448,
        "MMLU_human_sexuality":0.7938931298,
        "MMLU_international_law":0.826446281,
        "MMLU_jurisprudence":0.75,
        "MMLU_logical_fallacies":0.7975460123,
        "MMLU_machine_learning":0.4910714286,
        "MMLU_management":0.786407767,
        "MMLU_marketing":0.8760683761,
        "MMLU_medical_genetics":0.74,
        "MMLU_miscellaneous":0.8148148148,
        "MMLU_moral_disputes":0.7080924855,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.7516339869,
        "MMLU_philosophy":0.691318328,
        "MMLU_prehistory":0.712962963,
        "MMLU_professional_accounting":0.4858156028,
        "MMLU_professional_law":0.4478487614,
        "MMLU_professional_medicine":0.6691176471,
        "MMLU_professional_psychology":0.658496732,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.7183673469,
        "MMLU_sociology":0.8308457711,
        "MMLU_us_foreign_policy":0.85,
        "MMLU_virology":0.5421686747,
        "MMLU_world_religions":0.8304093567
    },
    {
        "Model":"OpenHermes-2-Mistral-7B",
        "URL":"https:\/\/huggingface.co\/teknium\/OpenHermes-2-Mistral-7B",
        "full_model_name":"teknium\/OpenHermes-2-Mistral-7B",
        "Parameters":7.0,
        "MMLU_average":0.6346113592,
        "arc:challenge|25":0.6006825939,
        "hellaswag|10":0.6379207329,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.6074074074,
        "MMLU_astronomy":0.7171052632,
        "MMLU_business_ethics":0.6,
        "MMLU_clinical_knowledge":0.6754716981,
        "MMLU_college_biology":0.7569444444,
        "MMLU_college_chemistry":0.45,
        "MMLU_college_computer_science":0.5,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.6011560694,
        "MMLU_college_physics":0.3823529412,
        "MMLU_computer_security":0.76,
        "MMLU_conceptual_physics":0.5617021277,
        "MMLU_econometrics":0.4736842105,
        "MMLU_electrical_engineering":0.5310344828,
        "MMLU_elementary_mathematics":0.4126984127,
        "MMLU_formal_logic":0.4285714286,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.7516129032,
        "MMLU_high_school_chemistry":0.5024630542,
        "MMLU_high_school_computer_science":0.67,
        "MMLU_high_school_european_history":0.7818181818,
        "MMLU_high_school_geography":0.7929292929,
        "MMLU_high_school_government_and_politics":0.8756476684,
        "MMLU_high_school_macroeconomics":0.5974358974,
        "MMLU_high_school_mathematics":0.3148148148,
        "MMLU_high_school_microeconomics":0.6218487395,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.8348623853,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.8235294118,
        "MMLU_high_school_world_history":0.8101265823,
        "MMLU_human_aging":0.6995515695,
        "MMLU_human_sexuality":0.7633587786,
        "MMLU_international_law":0.7933884298,
        "MMLU_jurisprudence":0.787037037,
        "MMLU_logical_fallacies":0.7914110429,
        "MMLU_machine_learning":0.5178571429,
        "MMLU_management":0.7766990291,
        "MMLU_marketing":0.8846153846,
        "MMLU_medical_genetics":0.7,
        "MMLU_miscellaneous":0.8326947637,
        "MMLU_moral_disputes":0.7341040462,
        "MMLU_moral_scenarios":0.3642458101,
        "MMLU_nutrition":0.7418300654,
        "MMLU_philosophy":0.7009646302,
        "MMLU_prehistory":0.7191358025,
        "MMLU_professional_accounting":0.5,
        "MMLU_professional_law":0.4687092568,
        "MMLU_professional_medicine":0.6470588235,
        "MMLU_professional_psychology":0.6617647059,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.7306122449,
        "MMLU_sociology":0.8457711443,
        "MMLU_us_foreign_policy":0.88,
        "MMLU_virology":0.5240963855,
        "MMLU_world_religions":0.8362573099
    },
    {
        "Model":"mistral-7b-slimorcaboros",
        "URL":"https:\/\/huggingface.co\/openaccess-ai-collective\/mistral-7b-slimorcaboros",
        "full_model_name":"openaccess-ai-collective\/mistral-7b-slimorcaboros",
        "Parameters":7.0,
        "MMLU_average":0.6345547635,
        "arc:challenge|25":0.611774744,
        "hellaswag|10":0.6503684525,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.5851851852,
        "MMLU_astronomy":0.6447368421,
        "MMLU_business_ethics":0.58,
        "MMLU_clinical_knowledge":0.6603773585,
        "MMLU_college_biology":0.7361111111,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.54,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.6184971098,
        "MMLU_college_physics":0.3823529412,
        "MMLU_computer_security":0.77,
        "MMLU_conceptual_physics":0.570212766,
        "MMLU_econometrics":0.4736842105,
        "MMLU_electrical_engineering":0.5517241379,
        "MMLU_elementary_mathematics":0.4206349206,
        "MMLU_formal_logic":0.4206349206,
        "MMLU_global_facts":0.27,
        "MMLU_high_school_biology":0.7806451613,
        "MMLU_high_school_chemistry":0.4729064039,
        "MMLU_high_school_computer_science":0.67,
        "MMLU_high_school_european_history":0.7939393939,
        "MMLU_high_school_geography":0.803030303,
        "MMLU_high_school_government_and_politics":0.8808290155,
        "MMLU_high_school_macroeconomics":0.6435897436,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.6848739496,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.8403669725,
        "MMLU_high_school_statistics":0.5,
        "MMLU_high_school_us_history":0.8284313725,
        "MMLU_high_school_world_history":0.8185654008,
        "MMLU_human_aging":0.730941704,
        "MMLU_human_sexuality":0.7709923664,
        "MMLU_international_law":0.8099173554,
        "MMLU_jurisprudence":0.787037037,
        "MMLU_logical_fallacies":0.7607361963,
        "MMLU_machine_learning":0.5,
        "MMLU_management":0.8349514563,
        "MMLU_marketing":0.8547008547,
        "MMLU_medical_genetics":0.7,
        "MMLU_miscellaneous":0.8224776501,
        "MMLU_moral_disputes":0.7398843931,
        "MMLU_moral_scenarios":0.3787709497,
        "MMLU_nutrition":0.7385620915,
        "MMLU_philosophy":0.7073954984,
        "MMLU_prehistory":0.7283950617,
        "MMLU_professional_accounting":0.5,
        "MMLU_professional_law":0.4843546284,
        "MMLU_professional_medicine":0.6838235294,
        "MMLU_professional_psychology":0.6388888889,
        "MMLU_public_relations":0.6,
        "MMLU_security_studies":0.7224489796,
        "MMLU_sociology":0.855721393,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.5180722892,
        "MMLU_world_religions":0.8245614035
    },
    {
        "Model":"dolphin-2.1-mistral-7b",
        "URL":"https:\/\/huggingface.co\/ehartford\/dolphin-2.1-mistral-7b",
        "full_model_name":"ehartford\/dolphin-2.1-mistral-7b",
        "Parameters":7.0,
        "MMLU_average":0.6344297962,
        "arc:challenge|25":0.5998293515,
        "hellaswag|10":0.6652061342,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.6222222222,
        "MMLU_astronomy":0.6513157895,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.6754716981,
        "MMLU_college_biology":0.7291666667,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.54,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.6184971098,
        "MMLU_college_physics":0.3823529412,
        "MMLU_computer_security":0.77,
        "MMLU_conceptual_physics":0.5361702128,
        "MMLU_econometrics":0.4210526316,
        "MMLU_electrical_engineering":0.5862068966,
        "MMLU_elementary_mathematics":0.3862433862,
        "MMLU_formal_logic":0.3888888889,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.7709677419,
        "MMLU_high_school_chemistry":0.4926108374,
        "MMLU_high_school_computer_science":0.68,
        "MMLU_high_school_european_history":0.7757575758,
        "MMLU_high_school_geography":0.7727272727,
        "MMLU_high_school_government_and_politics":0.8808290155,
        "MMLU_high_school_macroeconomics":0.6538461538,
        "MMLU_high_school_mathematics":0.337037037,
        "MMLU_high_school_microeconomics":0.6680672269,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.823853211,
        "MMLU_high_school_statistics":0.5138888889,
        "MMLU_high_school_us_history":0.7745098039,
        "MMLU_high_school_world_history":0.7848101266,
        "MMLU_human_aging":0.668161435,
        "MMLU_human_sexuality":0.7786259542,
        "MMLU_international_law":0.7851239669,
        "MMLU_jurisprudence":0.8148148148,
        "MMLU_logical_fallacies":0.7668711656,
        "MMLU_machine_learning":0.5178571429,
        "MMLU_management":0.7669902913,
        "MMLU_marketing":0.8632478632,
        "MMLU_medical_genetics":0.71,
        "MMLU_miscellaneous":0.809706258,
        "MMLU_moral_disputes":0.7138728324,
        "MMLU_moral_scenarios":0.4,
        "MMLU_nutrition":0.7222222222,
        "MMLU_philosophy":0.6977491961,
        "MMLU_prehistory":0.7160493827,
        "MMLU_professional_accounting":0.4716312057,
        "MMLU_professional_law":0.4511082138,
        "MMLU_professional_medicine":0.6691176471,
        "MMLU_professional_psychology":0.6486928105,
        "MMLU_public_relations":0.7,
        "MMLU_security_studies":0.7224489796,
        "MMLU_sociology":0.8457711443,
        "MMLU_us_foreign_policy":0.88,
        "MMLU_virology":0.5602409639,
        "MMLU_world_religions":0.8479532164
    },
    {
        "Model":"7B-DPO-alpha",
        "URL":"https:\/\/huggingface.co\/CausalLM\/7B-DPO-alpha",
        "full_model_name":"CausalLM\/7B-DPO-alpha",
        "Parameters":7.0,
        "MMLU_average":0.6338732832,
        "arc:challenge|25":0.4735494881,
        "hellaswag|10":0.5332603067,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.5555555556,
        "MMLU_astronomy":0.625,
        "MMLU_business_ethics":0.7,
        "MMLU_clinical_knowledge":0.7396226415,
        "MMLU_college_biology":0.6944444444,
        "MMLU_college_chemistry":0.49,
        "MMLU_college_computer_science":0.54,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.6705202312,
        "MMLU_college_physics":0.4411764706,
        "MMLU_computer_security":0.74,
        "MMLU_conceptual_physics":0.5404255319,
        "MMLU_econometrics":0.3947368421,
        "MMLU_electrical_engineering":0.5448275862,
        "MMLU_elementary_mathematics":0.4761904762,
        "MMLU_formal_logic":0.4682539683,
        "MMLU_global_facts":0.41,
        "MMLU_high_school_biology":0.7580645161,
        "MMLU_high_school_chemistry":0.5024630542,
        "MMLU_high_school_computer_science":0.67,
        "MMLU_high_school_european_history":0.8181818182,
        "MMLU_high_school_geography":0.8434343434,
        "MMLU_high_school_government_and_politics":0.8704663212,
        "MMLU_high_school_macroeconomics":0.6307692308,
        "MMLU_high_school_mathematics":0.3222222222,
        "MMLU_high_school_microeconomics":0.6638655462,
        "MMLU_high_school_physics":0.3973509934,
        "MMLU_high_school_psychology":0.8256880734,
        "MMLU_high_school_statistics":0.5648148148,
        "MMLU_high_school_us_history":0.7450980392,
        "MMLU_high_school_world_history":0.776371308,
        "MMLU_human_aging":0.6502242152,
        "MMLU_human_sexuality":0.7404580153,
        "MMLU_international_law":0.8099173554,
        "MMLU_jurisprudence":0.75,
        "MMLU_logical_fallacies":0.7116564417,
        "MMLU_machine_learning":0.4821428571,
        "MMLU_management":0.786407767,
        "MMLU_marketing":0.8504273504,
        "MMLU_medical_genetics":0.68,
        "MMLU_miscellaneous":0.7969348659,
        "MMLU_moral_disputes":0.6791907514,
        "MMLU_moral_scenarios":0.3642458101,
        "MMLU_nutrition":0.7222222222,
        "MMLU_philosophy":0.7041800643,
        "MMLU_prehistory":0.6759259259,
        "MMLU_professional_accounting":0.4716312057,
        "MMLU_professional_law":0.5143415906,
        "MMLU_professional_medicine":0.6544117647,
        "MMLU_professional_psychology":0.6274509804,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.7387755102,
        "MMLU_sociology":0.8457711443,
        "MMLU_us_foreign_policy":0.86,
        "MMLU_virology":0.4518072289,
        "MMLU_world_religions":0.7894736842
    },
    {
        "Model":"airoboros-65b-gpt4-1.2",
        "URL":"https:\/\/huggingface.co\/jondurbin\/airoboros-65b-gpt4-1.2",
        "full_model_name":"jondurbin\/airoboros-65b-gpt4-1.2",
        "Parameters":65.0,
        "MMLU_average":0.6337291814,
        "arc:challenge|25":0.638225256,
        "hellaswag|10":0.6853216491,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.5555555556,
        "MMLU_astronomy":0.6973684211,
        "MMLU_business_ethics":0.64,
        "MMLU_clinical_knowledge":0.6603773585,
        "MMLU_college_biology":0.7222222222,
        "MMLU_college_chemistry":0.47,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.5433526012,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.77,
        "MMLU_conceptual_physics":0.6127659574,
        "MMLU_econometrics":0.3596491228,
        "MMLU_electrical_engineering":0.5862068966,
        "MMLU_elementary_mathematics":0.3915343915,
        "MMLU_formal_logic":0.4365079365,
        "MMLU_global_facts":0.41,
        "MMLU_high_school_biology":0.7516129032,
        "MMLU_high_school_chemistry":0.4137931034,
        "MMLU_high_school_computer_science":0.63,
        "MMLU_high_school_european_history":0.7757575758,
        "MMLU_high_school_geography":0.803030303,
        "MMLU_high_school_government_and_politics":0.8652849741,
        "MMLU_high_school_macroeconomics":0.6358974359,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.6806722689,
        "MMLU_high_school_physics":0.3708609272,
        "MMLU_high_school_psychology":0.8146788991,
        "MMLU_high_school_statistics":0.4907407407,
        "MMLU_high_school_us_history":0.8382352941,
        "MMLU_high_school_world_history":0.8438818565,
        "MMLU_human_aging":0.6995515695,
        "MMLU_human_sexuality":0.7709923664,
        "MMLU_international_law":0.8099173554,
        "MMLU_jurisprudence":0.7037037037,
        "MMLU_logical_fallacies":0.7975460123,
        "MMLU_machine_learning":0.4553571429,
        "MMLU_management":0.8349514563,
        "MMLU_marketing":0.8675213675,
        "MMLU_medical_genetics":0.69,
        "MMLU_miscellaneous":0.8199233716,
        "MMLU_moral_disputes":0.7341040462,
        "MMLU_moral_scenarios":0.4312849162,
        "MMLU_nutrition":0.7254901961,
        "MMLU_philosophy":0.7170418006,
        "MMLU_prehistory":0.75,
        "MMLU_professional_accounting":0.475177305,
        "MMLU_professional_law":0.4804432855,
        "MMLU_professional_medicine":0.6213235294,
        "MMLU_professional_psychology":0.6715686275,
        "MMLU_public_relations":0.7090909091,
        "MMLU_security_studies":0.7469387755,
        "MMLU_sociology":0.8059701493,
        "MMLU_us_foreign_policy":0.85,
        "MMLU_virology":0.5421686747,
        "MMLU_world_religions":0.8362573099
    },
    {
        "Model":"mistral-7b-ft-h4-no_robots_instructions",
        "URL":"https:\/\/huggingface.co\/mrm8488\/mistral-7b-ft-h4-no_robots_instructions",
        "full_model_name":"mrm8488\/mistral-7b-ft-h4-no_robots_instructions",
        "Parameters":7.0,
        "MMLU_average":0.6336917601,
        "arc:challenge|25":0.5759385666,
        "hellaswag|10":0.6289583748,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.6444444444,
        "MMLU_astronomy":0.6381578947,
        "MMLU_business_ethics":0.58,
        "MMLU_clinical_knowledge":0.6943396226,
        "MMLU_college_biology":0.75,
        "MMLU_college_chemistry":0.47,
        "MMLU_college_computer_science":0.58,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.6358381503,
        "MMLU_college_physics":0.3431372549,
        "MMLU_computer_security":0.78,
        "MMLU_conceptual_physics":0.5659574468,
        "MMLU_econometrics":0.4736842105,
        "MMLU_electrical_engineering":0.5655172414,
        "MMLU_elementary_mathematics":0.3597883598,
        "MMLU_formal_logic":0.4206349206,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.7548387097,
        "MMLU_high_school_chemistry":0.5123152709,
        "MMLU_high_school_computer_science":0.66,
        "MMLU_high_school_european_history":0.7757575758,
        "MMLU_high_school_geography":0.7575757576,
        "MMLU_high_school_government_and_politics":0.8704663212,
        "MMLU_high_school_macroeconomics":0.6179487179,
        "MMLU_high_school_mathematics":0.3666666667,
        "MMLU_high_school_microeconomics":0.6092436975,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.823853211,
        "MMLU_high_school_statistics":0.4907407407,
        "MMLU_high_school_us_history":0.7941176471,
        "MMLU_high_school_world_history":0.7721518987,
        "MMLU_human_aging":0.6905829596,
        "MMLU_human_sexuality":0.786259542,
        "MMLU_international_law":0.8181818182,
        "MMLU_jurisprudence":0.7592592593,
        "MMLU_logical_fallacies":0.7852760736,
        "MMLU_machine_learning":0.4732142857,
        "MMLU_management":0.8058252427,
        "MMLU_marketing":0.8888888889,
        "MMLU_medical_genetics":0.73,
        "MMLU_miscellaneous":0.8212005109,
        "MMLU_moral_disputes":0.7225433526,
        "MMLU_moral_scenarios":0.3418994413,
        "MMLU_nutrition":0.7385620915,
        "MMLU_philosophy":0.691318328,
        "MMLU_prehistory":0.7314814815,
        "MMLU_professional_accounting":0.4858156028,
        "MMLU_professional_law":0.4524119948,
        "MMLU_professional_medicine":0.6654411765,
        "MMLU_professional_psychology":0.6552287582,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.7265306122,
        "MMLU_sociology":0.8258706468,
        "MMLU_us_foreign_policy":0.89,
        "MMLU_virology":0.5421686747,
        "MMLU_world_religions":0.8187134503
    },
    {
        "Model":"speechless-mistral-six-in-one-7b",
        "URL":"https:\/\/huggingface.co\/uukuguy\/speechless-mistral-six-in-one-7b",
        "full_model_name":"uukuguy\/speechless-mistral-six-in-one-7b",
        "Parameters":7.0,
        "MMLU_average":0.6329099134,
        "arc:challenge|25":0.5981228669,
        "hellaswag|10":0.6524596694,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.6222222222,
        "MMLU_astronomy":0.6578947368,
        "MMLU_business_ethics":0.58,
        "MMLU_clinical_knowledge":0.679245283,
        "MMLU_college_biology":0.7361111111,
        "MMLU_college_chemistry":0.49,
        "MMLU_college_computer_science":0.53,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.6184971098,
        "MMLU_college_physics":0.362745098,
        "MMLU_computer_security":0.78,
        "MMLU_conceptual_physics":0.5319148936,
        "MMLU_econometrics":0.4561403509,
        "MMLU_electrical_engineering":0.6,
        "MMLU_elementary_mathematics":0.4074074074,
        "MMLU_formal_logic":0.4523809524,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.7677419355,
        "MMLU_high_school_chemistry":0.5270935961,
        "MMLU_high_school_computer_science":0.67,
        "MMLU_high_school_european_history":0.7575757576,
        "MMLU_high_school_geography":0.7828282828,
        "MMLU_high_school_government_and_politics":0.8704663212,
        "MMLU_high_school_macroeconomics":0.641025641,
        "MMLU_high_school_mathematics":0.3481481481,
        "MMLU_high_school_microeconomics":0.6764705882,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.8385321101,
        "MMLU_high_school_statistics":0.4953703704,
        "MMLU_high_school_us_history":0.7990196078,
        "MMLU_high_school_world_history":0.7637130802,
        "MMLU_human_aging":0.6905829596,
        "MMLU_human_sexuality":0.7557251908,
        "MMLU_international_law":0.7851239669,
        "MMLU_jurisprudence":0.787037037,
        "MMLU_logical_fallacies":0.754601227,
        "MMLU_machine_learning":0.4642857143,
        "MMLU_management":0.7766990291,
        "MMLU_marketing":0.8888888889,
        "MMLU_medical_genetics":0.71,
        "MMLU_miscellaneous":0.8122605364,
        "MMLU_moral_disputes":0.7080924855,
        "MMLU_moral_scenarios":0.4189944134,
        "MMLU_nutrition":0.7091503268,
        "MMLU_philosophy":0.6784565916,
        "MMLU_prehistory":0.700617284,
        "MMLU_professional_accounting":0.4574468085,
        "MMLU_professional_law":0.4550195567,
        "MMLU_professional_medicine":0.6544117647,
        "MMLU_professional_psychology":0.6388888889,
        "MMLU_public_relations":0.6727272727,
        "MMLU_security_studies":0.7224489796,
        "MMLU_sociology":0.8507462687,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.5421686747,
        "MMLU_world_religions":0.8479532164
    },
    {
        "Model":"dolphin-2.2.1-mistral-7b",
        "URL":"https:\/\/huggingface.co\/ehartford\/dolphin-2.2.1-mistral-7b",
        "full_model_name":"ehartford\/dolphin-2.2.1-mistral-7b",
        "Parameters":7.0,
        "MMLU_average":0.6327522678,
        "arc:challenge|25":0.6058020478,
        "hellaswag|10":0.6428998208,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.6222222222,
        "MMLU_astronomy":0.6710526316,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.679245283,
        "MMLU_college_biology":0.7569444444,
        "MMLU_college_chemistry":0.46,
        "MMLU_college_computer_science":0.49,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.6300578035,
        "MMLU_college_physics":0.362745098,
        "MMLU_computer_security":0.79,
        "MMLU_conceptual_physics":0.5404255319,
        "MMLU_econometrics":0.4561403509,
        "MMLU_electrical_engineering":0.5862068966,
        "MMLU_elementary_mathematics":0.3835978836,
        "MMLU_formal_logic":0.4047619048,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.7709677419,
        "MMLU_high_school_chemistry":0.4827586207,
        "MMLU_high_school_computer_science":0.69,
        "MMLU_high_school_european_history":0.7818181818,
        "MMLU_high_school_geography":0.7878787879,
        "MMLU_high_school_government_and_politics":0.8756476684,
        "MMLU_high_school_macroeconomics":0.6538461538,
        "MMLU_high_school_mathematics":0.3296296296,
        "MMLU_high_school_microeconomics":0.6764705882,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.8366972477,
        "MMLU_high_school_statistics":0.4907407407,
        "MMLU_high_school_us_history":0.7696078431,
        "MMLU_high_school_world_history":0.7805907173,
        "MMLU_human_aging":0.6816143498,
        "MMLU_human_sexuality":0.786259542,
        "MMLU_international_law":0.8016528926,
        "MMLU_jurisprudence":0.8055555556,
        "MMLU_logical_fallacies":0.7607361963,
        "MMLU_machine_learning":0.4732142857,
        "MMLU_management":0.7766990291,
        "MMLU_marketing":0.8717948718,
        "MMLU_medical_genetics":0.75,
        "MMLU_miscellaneous":0.8135376756,
        "MMLU_moral_disputes":0.7196531792,
        "MMLU_moral_scenarios":0.3843575419,
        "MMLU_nutrition":0.7091503268,
        "MMLU_philosophy":0.7170418006,
        "MMLU_prehistory":0.700617284,
        "MMLU_professional_accounting":0.475177305,
        "MMLU_professional_law":0.4452411995,
        "MMLU_professional_medicine":0.6433823529,
        "MMLU_professional_psychology":0.6535947712,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.7183673469,
        "MMLU_sociology":0.8407960199,
        "MMLU_us_foreign_policy":0.87,
        "MMLU_virology":0.5421686747,
        "MMLU_world_religions":0.8304093567
    },
    {
        "Model":"VicUnlocked-alpaca-65B-QLoRA-fp16",
        "URL":"https:\/\/huggingface.co\/TheBloke\/VicUnlocked-alpaca-65B-QLoRA-fp16",
        "full_model_name":"TheBloke\/VicUnlocked-alpaca-65B-QLoRA-fp16",
        "Parameters":65.0,
        "MMLU_average":0.6313206031,
        "arc:challenge|25":0.6194539249,
        "hellaswag|10":0.6573391755,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.5555555556,
        "MMLU_astronomy":0.7302631579,
        "MMLU_business_ethics":0.62,
        "MMLU_clinical_knowledge":0.6603773585,
        "MMLU_college_biology":0.7013888889,
        "MMLU_college_chemistry":0.48,
        "MMLU_college_computer_science":0.52,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.5549132948,
        "MMLU_college_physics":0.3921568627,
        "MMLU_computer_security":0.78,
        "MMLU_conceptual_physics":0.5914893617,
        "MMLU_econometrics":0.3947368421,
        "MMLU_electrical_engineering":0.5379310345,
        "MMLU_elementary_mathematics":0.3756613757,
        "MMLU_formal_logic":0.4365079365,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.7483870968,
        "MMLU_high_school_chemistry":0.4827586207,
        "MMLU_high_school_computer_science":0.65,
        "MMLU_high_school_european_history":0.7757575758,
        "MMLU_high_school_geography":0.8131313131,
        "MMLU_high_school_government_and_politics":0.8652849741,
        "MMLU_high_school_macroeconomics":0.6641025641,
        "MMLU_high_school_mathematics":0.3333333333,
        "MMLU_high_school_microeconomics":0.6932773109,
        "MMLU_high_school_physics":0.3708609272,
        "MMLU_high_school_psychology":0.8201834862,
        "MMLU_high_school_statistics":0.5601851852,
        "MMLU_high_school_us_history":0.8284313725,
        "MMLU_high_school_world_history":0.8143459916,
        "MMLU_human_aging":0.6547085202,
        "MMLU_human_sexuality":0.7251908397,
        "MMLU_international_law":0.7768595041,
        "MMLU_jurisprudence":0.7314814815,
        "MMLU_logical_fallacies":0.7300613497,
        "MMLU_machine_learning":0.4642857143,
        "MMLU_management":0.8252427184,
        "MMLU_marketing":0.8333333333,
        "MMLU_medical_genetics":0.65,
        "MMLU_miscellaneous":0.8135376756,
        "MMLU_moral_disputes":0.7283236994,
        "MMLU_moral_scenarios":0.4312849162,
        "MMLU_nutrition":0.7026143791,
        "MMLU_philosophy":0.7073954984,
        "MMLU_prehistory":0.7407407407,
        "MMLU_professional_accounting":0.5106382979,
        "MMLU_professional_law":0.4745762712,
        "MMLU_professional_medicine":0.6323529412,
        "MMLU_professional_psychology":0.6388888889,
        "MMLU_public_relations":0.7272727273,
        "MMLU_security_studies":0.7387755102,
        "MMLU_sociology":0.8258706468,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.4879518072,
        "MMLU_world_religions":0.8128654971
    },
    {
        "Model":"Mistral-11B-TestBench3",
        "URL":"https:\/\/huggingface.co\/Undi95\/Mistral-11B-TestBench3",
        "full_model_name":"Undi95\/Mistral-11B-TestBench3",
        "Parameters":11.0,
        "MMLU_average":0.6311161519,
        "arc:challenge|25":0.5784982935,
        "hellaswag|10":0.6485759809,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.6148148148,
        "MMLU_astronomy":0.6513157895,
        "MMLU_business_ethics":0.58,
        "MMLU_clinical_knowledge":0.6377358491,
        "MMLU_college_biology":0.7222222222,
        "MMLU_college_chemistry":0.49,
        "MMLU_college_computer_science":0.54,
        "MMLU_college_mathematics":0.42,
        "MMLU_college_medicine":0.6184971098,
        "MMLU_college_physics":0.3823529412,
        "MMLU_computer_security":0.75,
        "MMLU_conceptual_physics":0.570212766,
        "MMLU_econometrics":0.5,
        "MMLU_electrical_engineering":0.5655172414,
        "MMLU_elementary_mathematics":0.373015873,
        "MMLU_formal_logic":0.4206349206,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.7483870968,
        "MMLU_high_school_chemistry":0.4926108374,
        "MMLU_high_school_computer_science":0.67,
        "MMLU_high_school_european_history":0.7878787879,
        "MMLU_high_school_geography":0.7929292929,
        "MMLU_high_school_government_and_politics":0.8601036269,
        "MMLU_high_school_macroeconomics":0.6743589744,
        "MMLU_high_school_mathematics":0.3222222222,
        "MMLU_high_school_microeconomics":0.6638655462,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.8275229358,
        "MMLU_high_school_statistics":0.5,
        "MMLU_high_school_us_history":0.7843137255,
        "MMLU_high_school_world_history":0.7426160338,
        "MMLU_human_aging":0.6771300448,
        "MMLU_human_sexuality":0.7786259542,
        "MMLU_international_law":0.7933884298,
        "MMLU_jurisprudence":0.7407407407,
        "MMLU_logical_fallacies":0.773006135,
        "MMLU_machine_learning":0.5,
        "MMLU_management":0.786407767,
        "MMLU_marketing":0.8675213675,
        "MMLU_medical_genetics":0.7,
        "MMLU_miscellaneous":0.8007662835,
        "MMLU_moral_disputes":0.6734104046,
        "MMLU_moral_scenarios":0.3351955307,
        "MMLU_nutrition":0.7418300654,
        "MMLU_philosophy":0.7041800643,
        "MMLU_prehistory":0.7160493827,
        "MMLU_professional_accounting":0.4680851064,
        "MMLU_professional_law":0.4465449804,
        "MMLU_professional_medicine":0.6654411765,
        "MMLU_professional_psychology":0.6454248366,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.7428571429,
        "MMLU_sociology":0.8407960199,
        "MMLU_us_foreign_policy":0.85,
        "MMLU_virology":0.5301204819,
        "MMLU_world_religions":0.8362573099
    },
    {
        "Model":"dromedary-65b-lora-HF",
        "URL":"https:\/\/huggingface.co\/TheBloke\/dromedary-65b-lora-HF",
        "full_model_name":"TheBloke\/dromedary-65b-lora-HF",
        "Parameters":65.0,
        "MMLU_average":0.6307756589,
        "arc:challenge|25":0.5853242321,
        "hellaswag|10":0.6527584147,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.5555555556,
        "MMLU_astronomy":0.7368421053,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.6641509434,
        "MMLU_college_biology":0.6597222222,
        "MMLU_college_chemistry":0.54,
        "MMLU_college_computer_science":0.49,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.5375722543,
        "MMLU_college_physics":0.3529411765,
        "MMLU_computer_security":0.8,
        "MMLU_conceptual_physics":0.5659574468,
        "MMLU_econometrics":0.350877193,
        "MMLU_electrical_engineering":0.5517241379,
        "MMLU_elementary_mathematics":0.3835978836,
        "MMLU_formal_logic":0.4603174603,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.7322580645,
        "MMLU_high_school_chemistry":0.4433497537,
        "MMLU_high_school_computer_science":0.64,
        "MMLU_high_school_european_history":0.7696969697,
        "MMLU_high_school_geography":0.7878787879,
        "MMLU_high_school_government_and_politics":0.8756476684,
        "MMLU_high_school_macroeconomics":0.6461538462,
        "MMLU_high_school_mathematics":0.3333333333,
        "MMLU_high_school_microeconomics":0.6890756303,
        "MMLU_high_school_physics":0.3973509934,
        "MMLU_high_school_psychology":0.8275229358,
        "MMLU_high_school_statistics":0.537037037,
        "MMLU_high_school_us_history":0.8333333333,
        "MMLU_high_school_world_history":0.8396624473,
        "MMLU_human_aging":0.6816143498,
        "MMLU_human_sexuality":0.7633587786,
        "MMLU_international_law":0.8099173554,
        "MMLU_jurisprudence":0.7407407407,
        "MMLU_logical_fallacies":0.7914110429,
        "MMLU_machine_learning":0.4821428571,
        "MMLU_management":0.8252427184,
        "MMLU_marketing":0.8504273504,
        "MMLU_medical_genetics":0.65,
        "MMLU_miscellaneous":0.7994891443,
        "MMLU_moral_disputes":0.7369942197,
        "MMLU_moral_scenarios":0.4536312849,
        "MMLU_nutrition":0.6732026144,
        "MMLU_philosophy":0.7266881029,
        "MMLU_prehistory":0.7283950617,
        "MMLU_professional_accounting":0.5035460993,
        "MMLU_professional_law":0.4915254237,
        "MMLU_professional_medicine":0.6066176471,
        "MMLU_professional_psychology":0.6683006536,
        "MMLU_public_relations":0.6909090909,
        "MMLU_security_studies":0.7183673469,
        "MMLU_sociology":0.8109452736,
        "MMLU_us_foreign_policy":0.87,
        "MMLU_virology":0.5421686747,
        "MMLU_world_religions":0.8070175439
    },
    {
        "Model":"speechless-code-mistral-7b-v1.0",
        "URL":"https:\/\/huggingface.co\/uukuguy\/speechless-code-mistral-7b-v1.0",
        "full_model_name":"uukuguy\/speechless-code-mistral-7b-v1.0",
        "Parameters":7.0,
        "MMLU_average":0.6298464848,
        "arc:challenge|25":0.5742320819,
        "hellaswag|10":0.6404102768,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.562962963,
        "MMLU_astronomy":0.6776315789,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.679245283,
        "MMLU_college_biology":0.7361111111,
        "MMLU_college_chemistry":0.46,
        "MMLU_college_computer_science":0.5,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.612716763,
        "MMLU_college_physics":0.3823529412,
        "MMLU_computer_security":0.76,
        "MMLU_conceptual_physics":0.5446808511,
        "MMLU_econometrics":0.4473684211,
        "MMLU_electrical_engineering":0.5586206897,
        "MMLU_elementary_mathematics":0.4074074074,
        "MMLU_formal_logic":0.4206349206,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.7580645161,
        "MMLU_high_school_chemistry":0.5221674877,
        "MMLU_high_school_computer_science":0.69,
        "MMLU_high_school_european_history":0.7636363636,
        "MMLU_high_school_geography":0.7929292929,
        "MMLU_high_school_government_and_politics":0.8911917098,
        "MMLU_high_school_macroeconomics":0.6435897436,
        "MMLU_high_school_mathematics":0.2925925926,
        "MMLU_high_school_microeconomics":0.6806722689,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.8275229358,
        "MMLU_high_school_statistics":0.5138888889,
        "MMLU_high_school_us_history":0.7696078431,
        "MMLU_high_school_world_history":0.7721518987,
        "MMLU_human_aging":0.7040358744,
        "MMLU_human_sexuality":0.7633587786,
        "MMLU_international_law":0.8016528926,
        "MMLU_jurisprudence":0.712962963,
        "MMLU_logical_fallacies":0.736196319,
        "MMLU_machine_learning":0.4821428571,
        "MMLU_management":0.7572815534,
        "MMLU_marketing":0.8418803419,
        "MMLU_medical_genetics":0.73,
        "MMLU_miscellaneous":0.816091954,
        "MMLU_moral_disputes":0.7196531792,
        "MMLU_moral_scenarios":0.3318435754,
        "MMLU_nutrition":0.7352941176,
        "MMLU_philosophy":0.7170418006,
        "MMLU_prehistory":0.7469135802,
        "MMLU_professional_accounting":0.4787234043,
        "MMLU_professional_law":0.4954367666,
        "MMLU_professional_medicine":0.6727941176,
        "MMLU_professional_psychology":0.6699346405,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.7224489796,
        "MMLU_sociology":0.8208955224,
        "MMLU_us_foreign_policy":0.85,
        "MMLU_virology":0.5542168675,
        "MMLU_world_religions":0.8245614035
    },
    {
        "Model":"xDAN-L1-Thinking",
        "URL":"https:\/\/huggingface.co\/xDAN-AI\/xDAN-L1-Thinking",
        "full_model_name":"xDAN-AI\/xDAN-L1-Thinking",
        "Parameters":null,
        "MMLU_average":0.6290339344,
        "arc:challenge|25":0.5947098976,
        "hellaswag|10":0.6577375025,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.562962963,
        "MMLU_astronomy":0.6644736842,
        "MMLU_business_ethics":0.61,
        "MMLU_clinical_knowledge":0.6679245283,
        "MMLU_college_biology":0.7013888889,
        "MMLU_college_chemistry":0.49,
        "MMLU_college_computer_science":0.52,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.6473988439,
        "MMLU_college_physics":0.4117647059,
        "MMLU_computer_security":0.74,
        "MMLU_conceptual_physics":0.5659574468,
        "MMLU_econometrics":0.4561403509,
        "MMLU_electrical_engineering":0.5724137931,
        "MMLU_elementary_mathematics":0.4021164021,
        "MMLU_formal_logic":0.4285714286,
        "MMLU_global_facts":0.28,
        "MMLU_high_school_biology":0.7580645161,
        "MMLU_high_school_chemistry":0.4926108374,
        "MMLU_high_school_computer_science":0.7,
        "MMLU_high_school_european_history":0.7575757576,
        "MMLU_high_school_geography":0.7676767677,
        "MMLU_high_school_government_and_politics":0.8808290155,
        "MMLU_high_school_macroeconomics":0.6512820513,
        "MMLU_high_school_mathematics":0.337037037,
        "MMLU_high_school_microeconomics":0.6596638655,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.823853211,
        "MMLU_high_school_statistics":0.4953703704,
        "MMLU_high_school_us_history":0.7941176471,
        "MMLU_high_school_world_history":0.7974683544,
        "MMLU_human_aging":0.6771300448,
        "MMLU_human_sexuality":0.7404580153,
        "MMLU_international_law":0.7768595041,
        "MMLU_jurisprudence":0.75,
        "MMLU_logical_fallacies":0.7914110429,
        "MMLU_machine_learning":0.5089285714,
        "MMLU_management":0.786407767,
        "MMLU_marketing":0.8547008547,
        "MMLU_medical_genetics":0.69,
        "MMLU_miscellaneous":0.8058748404,
        "MMLU_moral_disputes":0.7052023121,
        "MMLU_moral_scenarios":0.3061452514,
        "MMLU_nutrition":0.7385620915,
        "MMLU_philosophy":0.7009646302,
        "MMLU_prehistory":0.725308642,
        "MMLU_professional_accounting":0.4574468085,
        "MMLU_professional_law":0.4680573664,
        "MMLU_professional_medicine":0.6397058824,
        "MMLU_professional_psychology":0.660130719,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.7142857143,
        "MMLU_sociology":0.8407960199,
        "MMLU_us_foreign_policy":0.88,
        "MMLU_virology":0.5240963855,
        "MMLU_world_religions":0.8011695906
    },
    {
        "Model":"openchat_3.5",
        "URL":"https:\/\/huggingface.co\/openchat\/openchat_3.5",
        "full_model_name":"openchat\/openchat_3.5",
        "Parameters":null,
        "MMLU_average":0.6289201603,
        "arc:challenge|25":0.5836177474,
        "hellaswag|10":0.6450906194,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.5851851852,
        "MMLU_astronomy":0.6776315789,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.6905660377,
        "MMLU_college_biology":0.7222222222,
        "MMLU_college_chemistry":0.47,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.6473988439,
        "MMLU_college_physics":0.4117647059,
        "MMLU_computer_security":0.75,
        "MMLU_conceptual_physics":0.5276595745,
        "MMLU_econometrics":0.4473684211,
        "MMLU_electrical_engineering":0.5448275862,
        "MMLU_elementary_mathematics":0.4021164021,
        "MMLU_formal_logic":0.5,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.7483870968,
        "MMLU_high_school_chemistry":0.4926108374,
        "MMLU_high_school_computer_science":0.68,
        "MMLU_high_school_european_history":0.7818181818,
        "MMLU_high_school_geography":0.7676767677,
        "MMLU_high_school_government_and_politics":0.8652849741,
        "MMLU_high_school_macroeconomics":0.6307692308,
        "MMLU_high_school_mathematics":0.3777777778,
        "MMLU_high_school_microeconomics":0.6512605042,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.847706422,
        "MMLU_high_school_statistics":0.4814814815,
        "MMLU_high_school_us_history":0.7843137255,
        "MMLU_high_school_world_history":0.8185654008,
        "MMLU_human_aging":0.6905829596,
        "MMLU_human_sexuality":0.7480916031,
        "MMLU_international_law":0.7851239669,
        "MMLU_jurisprudence":0.8055555556,
        "MMLU_logical_fallacies":0.736196319,
        "MMLU_machine_learning":0.4375,
        "MMLU_management":0.8252427184,
        "MMLU_marketing":0.8461538462,
        "MMLU_medical_genetics":0.69,
        "MMLU_miscellaneous":0.8071519796,
        "MMLU_moral_disputes":0.7052023121,
        "MMLU_moral_scenarios":0.348603352,
        "MMLU_nutrition":0.6764705882,
        "MMLU_philosophy":0.6945337621,
        "MMLU_prehistory":0.7407407407,
        "MMLU_professional_accounting":0.4929078014,
        "MMLU_professional_law":0.4680573664,
        "MMLU_professional_medicine":0.6617647059,
        "MMLU_professional_psychology":0.6405228758,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.6979591837,
        "MMLU_sociology":0.8109452736,
        "MMLU_us_foreign_policy":0.9,
        "MMLU_virology":0.5120481928,
        "MMLU_world_religions":0.8245614035
    },
    {
        "Model":"CollectiveCognition-v1-Mistral-7B",
        "URL":"https:\/\/huggingface.co\/teknium\/CollectiveCognition-v1-Mistral-7B",
        "full_model_name":"teknium\/CollectiveCognition-v1-Mistral-7B",
        "Parameters":7.0,
        "MMLU_average":0.6276456581,
        "arc:challenge|25":0.5981228669,
        "hellaswag|10":0.6682931687,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.6074074074,
        "MMLU_astronomy":0.6578947368,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.6943396226,
        "MMLU_college_biology":0.7361111111,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.54,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.6358381503,
        "MMLU_college_physics":0.3529411765,
        "MMLU_computer_security":0.78,
        "MMLU_conceptual_physics":0.5659574468,
        "MMLU_econometrics":0.4561403509,
        "MMLU_electrical_engineering":0.5862068966,
        "MMLU_elementary_mathematics":0.3835978836,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.7548387097,
        "MMLU_high_school_chemistry":0.5024630542,
        "MMLU_high_school_computer_science":0.66,
        "MMLU_high_school_european_history":0.7515151515,
        "MMLU_high_school_geography":0.7929292929,
        "MMLU_high_school_government_and_politics":0.8756476684,
        "MMLU_high_school_macroeconomics":0.6487179487,
        "MMLU_high_school_mathematics":0.3444444444,
        "MMLU_high_school_microeconomics":0.6176470588,
        "MMLU_high_school_physics":0.3642384106,
        "MMLU_high_school_psychology":0.8091743119,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.7696078431,
        "MMLU_high_school_world_history":0.7890295359,
        "MMLU_human_aging":0.66367713,
        "MMLU_human_sexuality":0.7480916031,
        "MMLU_international_law":0.7851239669,
        "MMLU_jurisprudence":0.7777777778,
        "MMLU_logical_fallacies":0.7607361963,
        "MMLU_machine_learning":0.4821428571,
        "MMLU_management":0.8058252427,
        "MMLU_marketing":0.8717948718,
        "MMLU_medical_genetics":0.74,
        "MMLU_miscellaneous":0.8135376756,
        "MMLU_moral_disputes":0.7138728324,
        "MMLU_moral_scenarios":0.269273743,
        "MMLU_nutrition":0.7352941176,
        "MMLU_philosophy":0.6752411576,
        "MMLU_prehistory":0.7191358025,
        "MMLU_professional_accounting":0.4858156028,
        "MMLU_professional_law":0.4426336375,
        "MMLU_professional_medicine":0.6727941176,
        "MMLU_professional_psychology":0.658496732,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.7265306122,
        "MMLU_sociology":0.8507462687,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.5180722892,
        "MMLU_world_religions":0.8538011696
    },
    {
        "Model":"llama-2-alpacagpt4-1000step",
        "URL":"https:\/\/huggingface.co\/Weyaxi\/llama-2-alpacagpt4-1000step",
        "full_model_name":"Weyaxi\/llama-2-alpacagpt4-1000step",
        "Parameters":null,
        "MMLU_average":0.6274838172,
        "arc:challenge|25":0.6126279863,
        "hellaswag|10":0.650169289,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.562962963,
        "MMLU_astronomy":0.6447368421,
        "MMLU_business_ethics":0.67,
        "MMLU_clinical_knowledge":0.679245283,
        "MMLU_college_biology":0.7430555556,
        "MMLU_college_chemistry":0.45,
        "MMLU_college_computer_science":0.52,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.6069364162,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.6170212766,
        "MMLU_econometrics":0.3684210526,
        "MMLU_electrical_engineering":0.5517241379,
        "MMLU_elementary_mathematics":0.3835978836,
        "MMLU_formal_logic":0.3968253968,
        "MMLU_global_facts":0.49,
        "MMLU_high_school_biology":0.7451612903,
        "MMLU_high_school_chemistry":0.4778325123,
        "MMLU_high_school_computer_science":0.64,
        "MMLU_high_school_european_history":0.7454545455,
        "MMLU_high_school_geography":0.8333333333,
        "MMLU_high_school_government_and_politics":0.896373057,
        "MMLU_high_school_macroeconomics":0.6435897436,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.6764705882,
        "MMLU_high_school_physics":0.3708609272,
        "MMLU_high_school_psychology":0.8366972477,
        "MMLU_high_school_statistics":0.5185185185,
        "MMLU_high_school_us_history":0.8284313725,
        "MMLU_high_school_world_history":0.8016877637,
        "MMLU_human_aging":0.7443946188,
        "MMLU_human_sexuality":0.7251908397,
        "MMLU_international_law":0.8181818182,
        "MMLU_jurisprudence":0.75,
        "MMLU_logical_fallacies":0.736196319,
        "MMLU_machine_learning":0.5267857143,
        "MMLU_management":0.8058252427,
        "MMLU_marketing":0.858974359,
        "MMLU_medical_genetics":0.64,
        "MMLU_miscellaneous":0.7982120051,
        "MMLU_moral_disputes":0.7341040462,
        "MMLU_moral_scenarios":0.2413407821,
        "MMLU_nutrition":0.6797385621,
        "MMLU_philosophy":0.7459807074,
        "MMLU_prehistory":0.737654321,
        "MMLU_professional_accounting":0.475177305,
        "MMLU_professional_law":0.498696219,
        "MMLU_professional_medicine":0.6066176471,
        "MMLU_professional_psychology":0.6911764706,
        "MMLU_public_relations":0.6818181818,
        "MMLU_security_studies":0.6653061224,
        "MMLU_sociology":0.7810945274,
        "MMLU_us_foreign_policy":0.89,
        "MMLU_virology":0.5421686747,
        "MMLU_world_religions":0.8187134503
    },
    {
        "Model":"Llama-2-70B-chat-GPTQ",
        "URL":"https:\/\/huggingface.co\/TheBloke\/Llama-2-70B-chat-GPTQ",
        "full_model_name":"TheBloke\/Llama-2-70B-chat-GPTQ",
        "Parameters":70.0,
        "MMLU_average":0.6274483928,
        "arc:challenge|25":0.5810580205,
        "hellaswag|10":0.6557458674,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.7302631579,
        "MMLU_business_ethics":0.64,
        "MMLU_clinical_knowledge":0.6377358491,
        "MMLU_college_biology":0.7291666667,
        "MMLU_college_chemistry":0.47,
        "MMLU_college_computer_science":0.6,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.6011560694,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.64,
        "MMLU_conceptual_physics":0.5361702128,
        "MMLU_econometrics":0.3684210526,
        "MMLU_electrical_engineering":0.5793103448,
        "MMLU_elementary_mathematics":0.3994708995,
        "MMLU_formal_logic":0.4047619048,
        "MMLU_global_facts":0.42,
        "MMLU_high_school_biology":0.7612903226,
        "MMLU_high_school_chemistry":0.4679802956,
        "MMLU_high_school_computer_science":0.65,
        "MMLU_high_school_european_history":0.7878787879,
        "MMLU_high_school_geography":0.803030303,
        "MMLU_high_school_government_and_politics":0.8860103627,
        "MMLU_high_school_macroeconomics":0.6282051282,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.6596638655,
        "MMLU_high_school_physics":0.3973509934,
        "MMLU_high_school_psychology":0.8366972477,
        "MMLU_high_school_statistics":0.4861111111,
        "MMLU_high_school_us_history":0.862745098,
        "MMLU_high_school_world_history":0.8396624473,
        "MMLU_human_aging":0.7040358744,
        "MMLU_human_sexuality":0.7328244275,
        "MMLU_international_law":0.7768595041,
        "MMLU_jurisprudence":0.8333333333,
        "MMLU_logical_fallacies":0.736196319,
        "MMLU_machine_learning":0.4732142857,
        "MMLU_management":0.8155339806,
        "MMLU_marketing":0.8632478632,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.809706258,
        "MMLU_moral_disputes":0.7167630058,
        "MMLU_moral_scenarios":0.3296089385,
        "MMLU_nutrition":0.6895424837,
        "MMLU_philosophy":0.7170418006,
        "MMLU_prehistory":0.7037037037,
        "MMLU_professional_accounting":0.4432624113,
        "MMLU_professional_law":0.4680573664,
        "MMLU_professional_medicine":0.5698529412,
        "MMLU_professional_psychology":0.660130719,
        "MMLU_public_relations":0.7,
        "MMLU_security_studies":0.7591836735,
        "MMLU_sociology":0.8457711443,
        "MMLU_us_foreign_policy":0.88,
        "MMLU_virology":0.5,
        "MMLU_world_religions":0.8128654971
    },
    {
        "Model":"gpt4-alpaca-lora_mlp-65B-HF",
        "URL":"https:\/\/huggingface.co\/TheBloke\/gpt4-alpaca-lora_mlp-65B-HF",
        "full_model_name":"TheBloke\/gpt4-alpaca-lora_mlp-65B-HF",
        "Parameters":65.0,
        "MMLU_average":0.627260763,
        "arc:challenge|25":0.6220136519,
        "hellaswag|10":0.6664011153,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.5481481481,
        "MMLU_astronomy":0.7105263158,
        "MMLU_business_ethics":0.63,
        "MMLU_clinical_knowledge":0.6641509434,
        "MMLU_college_biology":0.6875,
        "MMLU_college_chemistry":0.46,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.5375722543,
        "MMLU_college_physics":0.3823529412,
        "MMLU_computer_security":0.77,
        "MMLU_conceptual_physics":0.5914893617,
        "MMLU_econometrics":0.3947368421,
        "MMLU_electrical_engineering":0.524137931,
        "MMLU_elementary_mathematics":0.3756613757,
        "MMLU_formal_logic":0.4761904762,
        "MMLU_global_facts":0.4,
        "MMLU_high_school_biology":0.735483871,
        "MMLU_high_school_chemistry":0.3891625616,
        "MMLU_high_school_computer_science":0.66,
        "MMLU_high_school_european_history":0.7818181818,
        "MMLU_high_school_geography":0.7878787879,
        "MMLU_high_school_government_and_politics":0.8704663212,
        "MMLU_high_school_macroeconomics":0.6256410256,
        "MMLU_high_school_mathematics":0.3185185185,
        "MMLU_high_school_microeconomics":0.6764705882,
        "MMLU_high_school_physics":0.4039735099,
        "MMLU_high_school_psychology":0.823853211,
        "MMLU_high_school_statistics":0.5138888889,
        "MMLU_high_school_us_history":0.8235294118,
        "MMLU_high_school_world_history":0.8312236287,
        "MMLU_human_aging":0.6591928251,
        "MMLU_human_sexuality":0.6870229008,
        "MMLU_international_law":0.8016528926,
        "MMLU_jurisprudence":0.7592592593,
        "MMLU_logical_fallacies":0.7300613497,
        "MMLU_machine_learning":0.4553571429,
        "MMLU_management":0.8058252427,
        "MMLU_marketing":0.8717948718,
        "MMLU_medical_genetics":0.68,
        "MMLU_miscellaneous":0.7867177522,
        "MMLU_moral_disputes":0.7312138728,
        "MMLU_moral_scenarios":0.4480446927,
        "MMLU_nutrition":0.6633986928,
        "MMLU_philosophy":0.7331189711,
        "MMLU_prehistory":0.7191358025,
        "MMLU_professional_accounting":0.4716312057,
        "MMLU_professional_law":0.4719687093,
        "MMLU_professional_medicine":0.6176470588,
        "MMLU_professional_psychology":0.6519607843,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.706122449,
        "MMLU_sociology":0.8457711443,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.5602409639,
        "MMLU_world_religions":0.8187134503
    },
    {
        "Model":"Dolphin2.1-OpenOrca-7B",
        "URL":"https:\/\/huggingface.co\/Weyaxi\/Dolphin2.1-OpenOrca-7B",
        "full_model_name":"Weyaxi\/Dolphin2.1-OpenOrca-7B",
        "Parameters":7.0,
        "MMLU_average":0.626971471,
        "arc:challenge|25":0.6083617747,
        "hellaswag|10":0.6503684525,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.6,
        "MMLU_astronomy":0.6710526316,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.6981132075,
        "MMLU_college_biology":0.7222222222,
        "MMLU_college_chemistry":0.48,
        "MMLU_college_computer_science":0.54,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.5953757225,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.79,
        "MMLU_conceptual_physics":0.5531914894,
        "MMLU_econometrics":0.4298245614,
        "MMLU_electrical_engineering":0.5793103448,
        "MMLU_elementary_mathematics":0.3968253968,
        "MMLU_formal_logic":0.4206349206,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.764516129,
        "MMLU_high_school_chemistry":0.5024630542,
        "MMLU_high_school_computer_science":0.69,
        "MMLU_high_school_european_history":0.7515151515,
        "MMLU_high_school_geography":0.7727272727,
        "MMLU_high_school_government_and_politics":0.8652849741,
        "MMLU_high_school_macroeconomics":0.6333333333,
        "MMLU_high_school_mathematics":0.3259259259,
        "MMLU_high_school_microeconomics":0.6596638655,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.8128440367,
        "MMLU_high_school_statistics":0.4907407407,
        "MMLU_high_school_us_history":0.7990196078,
        "MMLU_high_school_world_history":0.7974683544,
        "MMLU_human_aging":0.668161435,
        "MMLU_human_sexuality":0.7709923664,
        "MMLU_international_law":0.7768595041,
        "MMLU_jurisprudence":0.7592592593,
        "MMLU_logical_fallacies":0.7423312883,
        "MMLU_machine_learning":0.4642857143,
        "MMLU_management":0.8058252427,
        "MMLU_marketing":0.8675213675,
        "MMLU_medical_genetics":0.72,
        "MMLU_miscellaneous":0.8058748404,
        "MMLU_moral_disputes":0.710982659,
        "MMLU_moral_scenarios":0.3340782123,
        "MMLU_nutrition":0.6993464052,
        "MMLU_philosophy":0.6848874598,
        "MMLU_prehistory":0.7191358025,
        "MMLU_professional_accounting":0.4468085106,
        "MMLU_professional_law":0.462190352,
        "MMLU_professional_medicine":0.6176470588,
        "MMLU_professional_psychology":0.6470588235,
        "MMLU_public_relations":0.6727272727,
        "MMLU_security_studies":0.7183673469,
        "MMLU_sociology":0.8457711443,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.5421686747,
        "MMLU_world_religions":0.8187134503
    },
    {
        "Model":"Chupacabra-7B",
        "URL":"https:\/\/huggingface.co\/perlthoughts\/Chupacabra-7B",
        "full_model_name":"perlthoughts\/Chupacabra-7B",
        "Parameters":7.0,
        "MMLU_average":0.6268060709,
        "arc:challenge|25":0.6365187713,
        "hellaswag|10":0.6437960566,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.5555555556,
        "MMLU_astronomy":0.7039473684,
        "MMLU_business_ethics":0.62,
        "MMLU_clinical_knowledge":0.7018867925,
        "MMLU_college_biology":0.6875,
        "MMLU_college_chemistry":0.51,
        "MMLU_college_computer_science":0.5,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.6416184971,
        "MMLU_college_physics":0.4215686275,
        "MMLU_computer_security":0.75,
        "MMLU_conceptual_physics":0.5617021277,
        "MMLU_econometrics":0.4122807018,
        "MMLU_electrical_engineering":0.6137931034,
        "MMLU_elementary_mathematics":0.3756613757,
        "MMLU_formal_logic":0.4841269841,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.7774193548,
        "MMLU_high_school_chemistry":0.4679802956,
        "MMLU_high_school_computer_science":0.66,
        "MMLU_high_school_european_history":0.7696969697,
        "MMLU_high_school_geography":0.7474747475,
        "MMLU_high_school_government_and_politics":0.8808290155,
        "MMLU_high_school_macroeconomics":0.6307692308,
        "MMLU_high_school_mathematics":0.3111111111,
        "MMLU_high_school_microeconomics":0.6764705882,
        "MMLU_high_school_physics":0.3774834437,
        "MMLU_high_school_psychology":0.8366972477,
        "MMLU_high_school_statistics":0.4814814815,
        "MMLU_high_school_us_history":0.7990196078,
        "MMLU_high_school_world_history":0.7932489451,
        "MMLU_human_aging":0.6726457399,
        "MMLU_human_sexuality":0.7633587786,
        "MMLU_international_law":0.826446281,
        "MMLU_jurisprudence":0.7592592593,
        "MMLU_logical_fallacies":0.717791411,
        "MMLU_machine_learning":0.4553571429,
        "MMLU_management":0.786407767,
        "MMLU_marketing":0.8675213675,
        "MMLU_medical_genetics":0.71,
        "MMLU_miscellaneous":0.8148148148,
        "MMLU_moral_disputes":0.6849710983,
        "MMLU_moral_scenarios":0.3709497207,
        "MMLU_nutrition":0.6928104575,
        "MMLU_philosophy":0.6881028939,
        "MMLU_prehistory":0.7037037037,
        "MMLU_professional_accounting":0.4539007092,
        "MMLU_professional_law":0.44589309,
        "MMLU_professional_medicine":0.6213235294,
        "MMLU_professional_psychology":0.6486928105,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.7265306122,
        "MMLU_sociology":0.7910447761,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.5421686747,
        "MMLU_world_religions":0.7894736842
    },
    {
        "Model":"SynthIA-7B-v1.3",
        "URL":"https:\/\/huggingface.co\/migtissera\/SynthIA-7B-v1.3",
        "full_model_name":"migtissera\/SynthIA-7B-v1.3",
        "Parameters":7.0,
        "MMLU_average":0.6264745571,
        "arc:challenge|25":0.5853242321,
        "hellaswag|10":0.6429994025,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.6,
        "MMLU_astronomy":0.6644736842,
        "MMLU_business_ethics":0.6,
        "MMLU_clinical_knowledge":0.679245283,
        "MMLU_college_biology":0.7013888889,
        "MMLU_college_chemistry":0.46,
        "MMLU_college_computer_science":0.51,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.6300578035,
        "MMLU_college_physics":0.3431372549,
        "MMLU_computer_security":0.79,
        "MMLU_conceptual_physics":0.5361702128,
        "MMLU_econometrics":0.4473684211,
        "MMLU_electrical_engineering":0.5379310345,
        "MMLU_elementary_mathematics":0.4021164021,
        "MMLU_formal_logic":0.4285714286,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.7290322581,
        "MMLU_high_school_chemistry":0.4482758621,
        "MMLU_high_school_computer_science":0.69,
        "MMLU_high_school_european_history":0.7636363636,
        "MMLU_high_school_geography":0.7878787879,
        "MMLU_high_school_government_and_politics":0.8652849741,
        "MMLU_high_school_macroeconomics":0.6358974359,
        "MMLU_high_school_mathematics":0.337037037,
        "MMLU_high_school_microeconomics":0.6428571429,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.8201834862,
        "MMLU_high_school_statistics":0.4814814815,
        "MMLU_high_school_us_history":0.7892156863,
        "MMLU_high_school_world_history":0.7805907173,
        "MMLU_human_aging":0.6905829596,
        "MMLU_human_sexuality":0.786259542,
        "MMLU_international_law":0.7933884298,
        "MMLU_jurisprudence":0.8148148148,
        "MMLU_logical_fallacies":0.7423312883,
        "MMLU_machine_learning":0.4732142857,
        "MMLU_management":0.786407767,
        "MMLU_marketing":0.8675213675,
        "MMLU_medical_genetics":0.7,
        "MMLU_miscellaneous":0.8071519796,
        "MMLU_moral_disputes":0.7023121387,
        "MMLU_moral_scenarios":0.4111731844,
        "MMLU_nutrition":0.6830065359,
        "MMLU_philosophy":0.7041800643,
        "MMLU_prehistory":0.7191358025,
        "MMLU_professional_accounting":0.4397163121,
        "MMLU_professional_law":0.464797914,
        "MMLU_professional_medicine":0.6764705882,
        "MMLU_professional_psychology":0.6715686275,
        "MMLU_public_relations":0.6909090909,
        "MMLU_security_studies":0.6857142857,
        "MMLU_sociology":0.8457711443,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.5060240964,
        "MMLU_world_religions":0.8070175439
    },
    {
        "Model":"SuperPlatty-30B",
        "URL":"https:\/\/huggingface.co\/ariellee\/SuperPlatty-30B",
        "full_model_name":"ariellee\/SuperPlatty-30B",
        "Parameters":30.0,
        "MMLU_average":0.62568701,
        "arc:challenge|25":0.6237201365,
        "hellaswag|10":0.6384186417,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.5333333333,
        "MMLU_astronomy":0.6907894737,
        "MMLU_business_ethics":0.62,
        "MMLU_clinical_knowledge":0.6377358491,
        "MMLU_college_biology":0.7013888889,
        "MMLU_college_chemistry":0.47,
        "MMLU_college_computer_science":0.5,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.5260115607,
        "MMLU_college_physics":0.3823529412,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.5531914894,
        "MMLU_econometrics":0.3859649123,
        "MMLU_electrical_engineering":0.5862068966,
        "MMLU_elementary_mathematics":0.4126984127,
        "MMLU_formal_logic":0.4444444444,
        "MMLU_global_facts":0.4,
        "MMLU_high_school_biology":0.7419354839,
        "MMLU_high_school_chemistry":0.4975369458,
        "MMLU_high_school_computer_science":0.65,
        "MMLU_high_school_european_history":0.7757575758,
        "MMLU_high_school_geography":0.8383838384,
        "MMLU_high_school_government_and_politics":0.8860103627,
        "MMLU_high_school_macroeconomics":0.6487179487,
        "MMLU_high_school_mathematics":0.3111111111,
        "MMLU_high_school_microeconomics":0.6932773109,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.8330275229,
        "MMLU_high_school_statistics":0.4953703704,
        "MMLU_high_school_us_history":0.8088235294,
        "MMLU_high_school_world_history":0.8438818565,
        "MMLU_human_aging":0.6860986547,
        "MMLU_human_sexuality":0.7251908397,
        "MMLU_international_law":0.8181818182,
        "MMLU_jurisprudence":0.7037037037,
        "MMLU_logical_fallacies":0.7484662577,
        "MMLU_machine_learning":0.4464285714,
        "MMLU_management":0.786407767,
        "MMLU_marketing":0.8803418803,
        "MMLU_medical_genetics":0.67,
        "MMLU_miscellaneous":0.7956577267,
        "MMLU_moral_disputes":0.7080924855,
        "MMLU_moral_scenarios":0.3497206704,
        "MMLU_nutrition":0.6830065359,
        "MMLU_philosophy":0.7073954984,
        "MMLU_prehistory":0.7314814815,
        "MMLU_professional_accounting":0.4822695035,
        "MMLU_professional_law":0.5006518905,
        "MMLU_professional_medicine":0.5845588235,
        "MMLU_professional_psychology":0.660130719,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.7306122449,
        "MMLU_sociology":0.815920398,
        "MMLU_us_foreign_policy":0.87,
        "MMLU_virology":0.5301204819,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"neural-chat-7b-v3-1",
        "URL":"https:\/\/huggingface.co\/Intel\/neural-chat-7b-v3-1",
        "full_model_name":"Intel\/neural-chat-7b-v3-1",
        "Parameters":7.0,
        "MMLU_average":0.624446041,
        "arc:challenge|25":0.6322525597,
        "hellaswag|10":0.6446922924,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.6148148148,
        "MMLU_astronomy":0.6842105263,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.6716981132,
        "MMLU_college_biology":0.7222222222,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.52,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.6300578035,
        "MMLU_college_physics":0.3921568627,
        "MMLU_computer_security":0.74,
        "MMLU_conceptual_physics":0.514893617,
        "MMLU_econometrics":0.4649122807,
        "MMLU_electrical_engineering":0.524137931,
        "MMLU_elementary_mathematics":0.3862433862,
        "MMLU_formal_logic":0.4285714286,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.7709677419,
        "MMLU_high_school_chemistry":0.5221674877,
        "MMLU_high_school_computer_science":0.68,
        "MMLU_high_school_european_history":0.7636363636,
        "MMLU_high_school_geography":0.7575757576,
        "MMLU_high_school_government_and_politics":0.9015544041,
        "MMLU_high_school_macroeconomics":0.6051282051,
        "MMLU_high_school_mathematics":0.3333333333,
        "MMLU_high_school_microeconomics":0.6764705882,
        "MMLU_high_school_physics":0.3642384106,
        "MMLU_high_school_psychology":0.8366972477,
        "MMLU_high_school_statistics":0.4861111111,
        "MMLU_high_school_us_history":0.8039215686,
        "MMLU_high_school_world_history":0.7974683544,
        "MMLU_human_aging":0.6816143498,
        "MMLU_human_sexuality":0.7328244275,
        "MMLU_international_law":0.7851239669,
        "MMLU_jurisprudence":0.7592592593,
        "MMLU_logical_fallacies":0.717791411,
        "MMLU_machine_learning":0.4821428571,
        "MMLU_management":0.8252427184,
        "MMLU_marketing":0.8632478632,
        "MMLU_medical_genetics":0.72,
        "MMLU_miscellaneous":0.8122605364,
        "MMLU_moral_disputes":0.6791907514,
        "MMLU_moral_scenarios":0.3810055866,
        "MMLU_nutrition":0.6960784314,
        "MMLU_philosophy":0.6752411576,
        "MMLU_prehistory":0.7191358025,
        "MMLU_professional_accounting":0.4432624113,
        "MMLU_professional_law":0.4439374185,
        "MMLU_professional_medicine":0.6544117647,
        "MMLU_professional_psychology":0.6421568627,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.7102040816,
        "MMLU_sociology":0.8407960199,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.5120481928,
        "MMLU_world_religions":0.8187134503
    },
    {
        "Model":"mistral-7b-finetuned-orca-dpo-v2",
        "URL":"https:\/\/huggingface.co\/lvkaokao\/mistral-7b-finetuned-orca-dpo-v2",
        "full_model_name":"lvkaokao\/mistral-7b-finetuned-orca-dpo-v2",
        "Parameters":7.0,
        "MMLU_average":0.6237420371,
        "arc:challenge|25":0.635665529,
        "hellaswag|10":0.6448914559,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.6222222222,
        "MMLU_astronomy":0.6710526316,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.6603773585,
        "MMLU_college_biology":0.7291666667,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.53,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.6242774566,
        "MMLU_college_physics":0.3921568627,
        "MMLU_computer_security":0.74,
        "MMLU_conceptual_physics":0.5234042553,
        "MMLU_econometrics":0.4561403509,
        "MMLU_electrical_engineering":0.5310344828,
        "MMLU_elementary_mathematics":0.3756613757,
        "MMLU_formal_logic":0.4365079365,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.7677419355,
        "MMLU_high_school_chemistry":0.5172413793,
        "MMLU_high_school_computer_science":0.66,
        "MMLU_high_school_european_history":0.7636363636,
        "MMLU_high_school_geography":0.7626262626,
        "MMLU_high_school_government_and_politics":0.896373057,
        "MMLU_high_school_macroeconomics":0.6076923077,
        "MMLU_high_school_mathematics":0.3444444444,
        "MMLU_high_school_microeconomics":0.6722689076,
        "MMLU_high_school_physics":0.357615894,
        "MMLU_high_school_psychology":0.8330275229,
        "MMLU_high_school_statistics":0.4953703704,
        "MMLU_high_school_us_history":0.7990196078,
        "MMLU_high_school_world_history":0.7890295359,
        "MMLU_human_aging":0.6816143498,
        "MMLU_human_sexuality":0.7328244275,
        "MMLU_international_law":0.8016528926,
        "MMLU_jurisprudence":0.7592592593,
        "MMLU_logical_fallacies":0.717791411,
        "MMLU_machine_learning":0.4821428571,
        "MMLU_management":0.8155339806,
        "MMLU_marketing":0.858974359,
        "MMLU_medical_genetics":0.72,
        "MMLU_miscellaneous":0.8122605364,
        "MMLU_moral_disputes":0.676300578,
        "MMLU_moral_scenarios":0.3865921788,
        "MMLU_nutrition":0.6928104575,
        "MMLU_philosophy":0.6752411576,
        "MMLU_prehistory":0.7222222222,
        "MMLU_professional_accounting":0.4361702128,
        "MMLU_professional_law":0.4478487614,
        "MMLU_professional_medicine":0.6580882353,
        "MMLU_professional_psychology":0.6405228758,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.7102040816,
        "MMLU_sociology":0.8358208955,
        "MMLU_us_foreign_policy":0.81,
        "MMLU_virology":0.5180722892,
        "MMLU_world_religions":0.8187134503
    },
    {
        "Model":"neural-chat-7b-v3",
        "URL":"https:\/\/huggingface.co\/Intel\/neural-chat-7b-v3",
        "full_model_name":"Intel\/neural-chat-7b-v3",
        "Parameters":7.0,
        "MMLU_average":0.6226105246,
        "arc:challenge|25":0.6399317406,
        "hellaswag|10":0.6532563234,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.5851851852,
        "MMLU_astronomy":0.6710526316,
        "MMLU_business_ethics":0.58,
        "MMLU_clinical_knowledge":0.6716981132,
        "MMLU_college_biology":0.7152777778,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.6300578035,
        "MMLU_college_physics":0.3823529412,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.5531914894,
        "MMLU_econometrics":0.4736842105,
        "MMLU_electrical_engineering":0.5586206897,
        "MMLU_elementary_mathematics":0.3994708995,
        "MMLU_formal_logic":0.4365079365,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.764516129,
        "MMLU_high_school_chemistry":0.4926108374,
        "MMLU_high_school_computer_science":0.68,
        "MMLU_high_school_european_history":0.7636363636,
        "MMLU_high_school_geography":0.7727272727,
        "MMLU_high_school_government_and_politics":0.8601036269,
        "MMLU_high_school_macroeconomics":0.6205128205,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.6764705882,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.8165137615,
        "MMLU_high_school_statistics":0.4305555556,
        "MMLU_high_school_us_history":0.7990196078,
        "MMLU_high_school_world_history":0.7932489451,
        "MMLU_human_aging":0.6816143498,
        "MMLU_human_sexuality":0.7251908397,
        "MMLU_international_law":0.826446281,
        "MMLU_jurisprudence":0.7685185185,
        "MMLU_logical_fallacies":0.7300613497,
        "MMLU_machine_learning":0.5267857143,
        "MMLU_management":0.7961165049,
        "MMLU_marketing":0.8376068376,
        "MMLU_medical_genetics":0.72,
        "MMLU_miscellaneous":0.8263090677,
        "MMLU_moral_disputes":0.6907514451,
        "MMLU_moral_scenarios":0.3508379888,
        "MMLU_nutrition":0.7189542484,
        "MMLU_philosophy":0.6977491961,
        "MMLU_prehistory":0.6975308642,
        "MMLU_professional_accounting":0.4255319149,
        "MMLU_professional_law":0.4537157757,
        "MMLU_professional_medicine":0.625,
        "MMLU_professional_psychology":0.6421568627,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.7020408163,
        "MMLU_sociology":0.8308457711,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.5240963855,
        "MMLU_world_religions":0.8128654971
    },
    {
        "Model":"Pwen-7B-Chat-20_30",
        "URL":"https:\/\/huggingface.co\/JosephusCheung\/Pwen-7B-Chat-20_30",
        "full_model_name":"JosephusCheung\/Pwen-7B-Chat-20_30",
        "Parameters":7.0,
        "MMLU_average":0.6208475291,
        "arc:challenge|25":0.4854948805,
        "hellaswag|10":0.5524795857,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.5555555556,
        "MMLU_astronomy":0.6513157895,
        "MMLU_business_ethics":0.71,
        "MMLU_clinical_knowledge":0.6905660377,
        "MMLU_college_biology":0.6944444444,
        "MMLU_college_chemistry":0.47,
        "MMLU_college_computer_science":0.51,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.6705202312,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.74,
        "MMLU_conceptual_physics":0.5787234043,
        "MMLU_econometrics":0.3947368421,
        "MMLU_electrical_engineering":0.524137931,
        "MMLU_elementary_mathematics":0.4444444444,
        "MMLU_formal_logic":0.4841269841,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.7516129032,
        "MMLU_high_school_chemistry":0.5418719212,
        "MMLU_high_school_computer_science":0.6,
        "MMLU_high_school_european_history":0.7272727273,
        "MMLU_high_school_geography":0.7929292929,
        "MMLU_high_school_government_and_politics":0.8911917098,
        "MMLU_high_school_macroeconomics":0.6051282051,
        "MMLU_high_school_mathematics":0.3222222222,
        "MMLU_high_school_microeconomics":0.6428571429,
        "MMLU_high_school_physics":0.3774834437,
        "MMLU_high_school_psychology":0.8275229358,
        "MMLU_high_school_statistics":0.5046296296,
        "MMLU_high_school_us_history":0.7892156863,
        "MMLU_high_school_world_history":0.7848101266,
        "MMLU_human_aging":0.6905829596,
        "MMLU_human_sexuality":0.6717557252,
        "MMLU_international_law":0.7933884298,
        "MMLU_jurisprudence":0.7407407407,
        "MMLU_logical_fallacies":0.736196319,
        "MMLU_machine_learning":0.4553571429,
        "MMLU_management":0.7572815534,
        "MMLU_marketing":0.8547008547,
        "MMLU_medical_genetics":0.73,
        "MMLU_miscellaneous":0.8058748404,
        "MMLU_moral_disputes":0.6676300578,
        "MMLU_moral_scenarios":0.3083798883,
        "MMLU_nutrition":0.7254901961,
        "MMLU_philosophy":0.7170418006,
        "MMLU_prehistory":0.6820987654,
        "MMLU_professional_accounting":0.4574468085,
        "MMLU_professional_law":0.4928292047,
        "MMLU_professional_medicine":0.6139705882,
        "MMLU_professional_psychology":0.6209150327,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.6979591837,
        "MMLU_sociology":0.815920398,
        "MMLU_us_foreign_policy":0.86,
        "MMLU_virology":0.4879518072,
        "MMLU_world_religions":0.7894736842
    },
    {
        "Model":"T2A",
        "URL":"https:\/\/huggingface.co\/AA051610\/T2A",
        "full_model_name":"AA051610\/T2A",
        "Parameters":null,
        "MMLU_average":0.6208475291,
        "arc:challenge|25":0.4854948805,
        "hellaswag|10":0.5524795857,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.5555555556,
        "MMLU_astronomy":0.6513157895,
        "MMLU_business_ethics":0.71,
        "MMLU_clinical_knowledge":0.6905660377,
        "MMLU_college_biology":0.6944444444,
        "MMLU_college_chemistry":0.47,
        "MMLU_college_computer_science":0.51,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.6705202312,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.74,
        "MMLU_conceptual_physics":0.5787234043,
        "MMLU_econometrics":0.3947368421,
        "MMLU_electrical_engineering":0.524137931,
        "MMLU_elementary_mathematics":0.4444444444,
        "MMLU_formal_logic":0.4841269841,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.7516129032,
        "MMLU_high_school_chemistry":0.5418719212,
        "MMLU_high_school_computer_science":0.6,
        "MMLU_high_school_european_history":0.7272727273,
        "MMLU_high_school_geography":0.7929292929,
        "MMLU_high_school_government_and_politics":0.8911917098,
        "MMLU_high_school_macroeconomics":0.6051282051,
        "MMLU_high_school_mathematics":0.3222222222,
        "MMLU_high_school_microeconomics":0.6428571429,
        "MMLU_high_school_physics":0.3774834437,
        "MMLU_high_school_psychology":0.8275229358,
        "MMLU_high_school_statistics":0.5046296296,
        "MMLU_high_school_us_history":0.7892156863,
        "MMLU_high_school_world_history":0.7848101266,
        "MMLU_human_aging":0.6905829596,
        "MMLU_human_sexuality":0.6717557252,
        "MMLU_international_law":0.7933884298,
        "MMLU_jurisprudence":0.7407407407,
        "MMLU_logical_fallacies":0.736196319,
        "MMLU_machine_learning":0.4553571429,
        "MMLU_management":0.7572815534,
        "MMLU_marketing":0.8547008547,
        "MMLU_medical_genetics":0.73,
        "MMLU_miscellaneous":0.8058748404,
        "MMLU_moral_disputes":0.6676300578,
        "MMLU_moral_scenarios":0.3083798883,
        "MMLU_nutrition":0.7254901961,
        "MMLU_philosophy":0.7170418006,
        "MMLU_prehistory":0.6820987654,
        "MMLU_professional_accounting":0.4574468085,
        "MMLU_professional_law":0.4928292047,
        "MMLU_professional_medicine":0.6139705882,
        "MMLU_professional_psychology":0.6209150327,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.6979591837,
        "MMLU_sociology":0.815920398,
        "MMLU_us_foreign_policy":0.86,
        "MMLU_virology":0.4879518072,
        "MMLU_world_religions":0.7894736842
    },
    {
        "Model":"SlimOpenOrca-Mistral-7B-v2",
        "URL":"https:\/\/huggingface.co\/PulsarAI\/SlimOpenOrca-Mistral-7B-v2",
        "full_model_name":"PulsarAI\/SlimOpenOrca-Mistral-7B-v2",
        "Parameters":7.0,
        "MMLU_average":0.6204940941,
        "arc:challenge|25":0.5938566553,
        "hellaswag|10":0.6448914559,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.5851851852,
        "MMLU_astronomy":0.6907894737,
        "MMLU_business_ethics":0.61,
        "MMLU_clinical_knowledge":0.6754716981,
        "MMLU_college_biology":0.7361111111,
        "MMLU_college_chemistry":0.51,
        "MMLU_college_computer_science":0.53,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.5549132948,
        "MMLU_college_physics":0.3823529412,
        "MMLU_computer_security":0.77,
        "MMLU_conceptual_physics":0.5319148936,
        "MMLU_econometrics":0.4210526316,
        "MMLU_electrical_engineering":0.5586206897,
        "MMLU_elementary_mathematics":0.4153439153,
        "MMLU_formal_logic":0.4523809524,
        "MMLU_global_facts":0.27,
        "MMLU_high_school_biology":0.7483870968,
        "MMLU_high_school_chemistry":0.4630541872,
        "MMLU_high_school_computer_science":0.72,
        "MMLU_high_school_european_history":0.7575757576,
        "MMLU_high_school_geography":0.7575757576,
        "MMLU_high_school_government_and_politics":0.8549222798,
        "MMLU_high_school_macroeconomics":0.5974358974,
        "MMLU_high_school_mathematics":0.3555555556,
        "MMLU_high_school_microeconomics":0.6386554622,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.8311926606,
        "MMLU_high_school_statistics":0.4907407407,
        "MMLU_high_school_us_history":0.8137254902,
        "MMLU_high_school_world_history":0.7890295359,
        "MMLU_human_aging":0.6547085202,
        "MMLU_human_sexuality":0.7633587786,
        "MMLU_international_law":0.7933884298,
        "MMLU_jurisprudence":0.7314814815,
        "MMLU_logical_fallacies":0.7423312883,
        "MMLU_machine_learning":0.5089285714,
        "MMLU_management":0.7961165049,
        "MMLU_marketing":0.8547008547,
        "MMLU_medical_genetics":0.74,
        "MMLU_miscellaneous":0.809706258,
        "MMLU_moral_disputes":0.6878612717,
        "MMLU_moral_scenarios":0.3474860335,
        "MMLU_nutrition":0.6993464052,
        "MMLU_philosophy":0.6655948553,
        "MMLU_prehistory":0.7160493827,
        "MMLU_professional_accounting":0.4361702128,
        "MMLU_professional_law":0.4530638853,
        "MMLU_professional_medicine":0.6066176471,
        "MMLU_professional_psychology":0.6421568627,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.7142857143,
        "MMLU_sociology":0.8258706468,
        "MMLU_us_foreign_policy":0.79,
        "MMLU_virology":0.5120481928,
        "MMLU_world_religions":0.8070175439
    },
    {
        "Model":"airoboros-65b-gpt4-1.4",
        "URL":"https:\/\/huggingface.co\/jondurbin\/airoboros-65b-gpt4-1.4",
        "full_model_name":"jondurbin\/airoboros-65b-gpt4-1.4",
        "Parameters":65.0,
        "MMLU_average":0.6194598386,
        "arc:challenge|25":0.6296928328,
        "hellaswag|10":0.6792471619,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.5777777778,
        "MMLU_astronomy":0.6973684211,
        "MMLU_business_ethics":0.66,
        "MMLU_clinical_knowledge":0.6603773585,
        "MMLU_college_biology":0.7222222222,
        "MMLU_college_chemistry":0.46,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.5202312139,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.73,
        "MMLU_conceptual_physics":0.5659574468,
        "MMLU_econometrics":0.350877193,
        "MMLU_electrical_engineering":0.5586206897,
        "MMLU_elementary_mathematics":0.3597883598,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.42,
        "MMLU_high_school_biology":0.7129032258,
        "MMLU_high_school_chemistry":0.4433497537,
        "MMLU_high_school_computer_science":0.65,
        "MMLU_high_school_european_history":0.7878787879,
        "MMLU_high_school_geography":0.7828282828,
        "MMLU_high_school_government_and_politics":0.8652849741,
        "MMLU_high_school_macroeconomics":0.6051282051,
        "MMLU_high_school_mathematics":0.3222222222,
        "MMLU_high_school_microeconomics":0.6302521008,
        "MMLU_high_school_physics":0.3907284768,
        "MMLU_high_school_psychology":0.8110091743,
        "MMLU_high_school_statistics":0.4814814815,
        "MMLU_high_school_us_history":0.8284313725,
        "MMLU_high_school_world_history":0.8565400844,
        "MMLU_human_aging":0.7040358744,
        "MMLU_human_sexuality":0.7251908397,
        "MMLU_international_law":0.8016528926,
        "MMLU_jurisprudence":0.7037037037,
        "MMLU_logical_fallacies":0.7484662577,
        "MMLU_machine_learning":0.4553571429,
        "MMLU_management":0.8155339806,
        "MMLU_marketing":0.858974359,
        "MMLU_medical_genetics":0.63,
        "MMLU_miscellaneous":0.8199233716,
        "MMLU_moral_disputes":0.6936416185,
        "MMLU_moral_scenarios":0.4648044693,
        "MMLU_nutrition":0.6797385621,
        "MMLU_philosophy":0.7395498392,
        "MMLU_prehistory":0.75,
        "MMLU_professional_accounting":0.5141843972,
        "MMLU_professional_law":0.4784876141,
        "MMLU_professional_medicine":0.5882352941,
        "MMLU_professional_psychology":0.6748366013,
        "MMLU_public_relations":0.6727272727,
        "MMLU_security_studies":0.6816326531,
        "MMLU_sociology":0.8358208955,
        "MMLU_us_foreign_policy":0.85,
        "MMLU_virology":0.5060240964,
        "MMLU_world_religions":0.8070175439
    },
    {
        "Model":"airoboros-m-7b-3.1.2",
        "URL":"https:\/\/huggingface.co\/jondurbin\/airoboros-m-7b-3.1.2",
        "full_model_name":"jondurbin\/airoboros-m-7b-3.1.2",
        "Parameters":7.0,
        "MMLU_average":0.6190699357,
        "arc:challenge|25":0.5921501706,
        "hellaswag|10":0.6340370444,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.5851851852,
        "MMLU_astronomy":0.6447368421,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.6641509434,
        "MMLU_college_biology":0.6805555556,
        "MMLU_college_chemistry":0.49,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.6069364162,
        "MMLU_college_physics":0.4509803922,
        "MMLU_computer_security":0.73,
        "MMLU_conceptual_physics":0.5276595745,
        "MMLU_econometrics":0.4736842105,
        "MMLU_electrical_engineering":0.5931034483,
        "MMLU_elementary_mathematics":0.3941798942,
        "MMLU_formal_logic":0.3968253968,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.7225806452,
        "MMLU_high_school_chemistry":0.4926108374,
        "MMLU_high_school_computer_science":0.63,
        "MMLU_high_school_european_history":0.7212121212,
        "MMLU_high_school_geography":0.8131313131,
        "MMLU_high_school_government_and_politics":0.8601036269,
        "MMLU_high_school_macroeconomics":0.6256410256,
        "MMLU_high_school_mathematics":0.3111111111,
        "MMLU_high_school_microeconomics":0.6386554622,
        "MMLU_high_school_physics":0.3642384106,
        "MMLU_high_school_psychology":0.8256880734,
        "MMLU_high_school_statistics":0.537037037,
        "MMLU_high_school_us_history":0.7990196078,
        "MMLU_high_school_world_history":0.7510548523,
        "MMLU_human_aging":0.6726457399,
        "MMLU_human_sexuality":0.7480916031,
        "MMLU_international_law":0.7438016529,
        "MMLU_jurisprudence":0.7314814815,
        "MMLU_logical_fallacies":0.7668711656,
        "MMLU_machine_learning":0.4107142857,
        "MMLU_management":0.8252427184,
        "MMLU_marketing":0.858974359,
        "MMLU_medical_genetics":0.7,
        "MMLU_miscellaneous":0.8109833972,
        "MMLU_moral_disputes":0.725433526,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.7156862745,
        "MMLU_philosophy":0.6720257235,
        "MMLU_prehistory":0.725308642,
        "MMLU_professional_accounting":0.4397163121,
        "MMLU_professional_law":0.444589309,
        "MMLU_professional_medicine":0.6801470588,
        "MMLU_professional_psychology":0.6470588235,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.6816326531,
        "MMLU_sociology":0.7960199005,
        "MMLU_us_foreign_policy":0.88,
        "MMLU_virology":0.4939759036,
        "MMLU_world_religions":0.8304093567
    },
    {
        "Model":"Dans-AdventurousWinds-Mk2-7b",
        "URL":"https:\/\/huggingface.co\/PocketDoc\/Dans-AdventurousWinds-Mk2-7b",
        "full_model_name":"PocketDoc\/Dans-AdventurousWinds-Mk2-7b",
        "Parameters":7.0,
        "MMLU_average":0.617993141,
        "arc:challenge|25":0.5383959044,
        "hellaswag|10":0.6399123681,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.5925925926,
        "MMLU_astronomy":0.625,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.6716981132,
        "MMLU_college_biology":0.7361111111,
        "MMLU_college_chemistry":0.52,
        "MMLU_college_computer_science":0.5,
        "MMLU_college_mathematics":0.4,
        "MMLU_college_medicine":0.6242774566,
        "MMLU_college_physics":0.3431372549,
        "MMLU_computer_security":0.74,
        "MMLU_conceptual_physics":0.5787234043,
        "MMLU_econometrics":0.4824561404,
        "MMLU_electrical_engineering":0.4896551724,
        "MMLU_elementary_mathematics":0.4047619048,
        "MMLU_formal_logic":0.4603174603,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.7548387097,
        "MMLU_high_school_chemistry":0.5024630542,
        "MMLU_high_school_computer_science":0.65,
        "MMLU_high_school_european_history":0.7696969697,
        "MMLU_high_school_geography":0.7929292929,
        "MMLU_high_school_government_and_politics":0.8652849741,
        "MMLU_high_school_macroeconomics":0.658974359,
        "MMLU_high_school_mathematics":0.3111111111,
        "MMLU_high_school_microeconomics":0.6848739496,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.7981651376,
        "MMLU_high_school_statistics":0.5787037037,
        "MMLU_high_school_us_history":0.7794117647,
        "MMLU_high_school_world_history":0.746835443,
        "MMLU_human_aging":0.66367713,
        "MMLU_human_sexuality":0.7557251908,
        "MMLU_international_law":0.7438016529,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.7423312883,
        "MMLU_machine_learning":0.4642857143,
        "MMLU_management":0.7669902913,
        "MMLU_marketing":0.8504273504,
        "MMLU_medical_genetics":0.7,
        "MMLU_miscellaneous":0.7892720307,
        "MMLU_moral_disputes":0.6445086705,
        "MMLU_moral_scenarios":0.3240223464,
        "MMLU_nutrition":0.7156862745,
        "MMLU_philosophy":0.6720257235,
        "MMLU_prehistory":0.6851851852,
        "MMLU_professional_accounting":0.4468085106,
        "MMLU_professional_law":0.4028683181,
        "MMLU_professional_medicine":0.6617647059,
        "MMLU_professional_psychology":0.6323529412,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.6489795918,
        "MMLU_sociology":0.7910447761,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.5361445783,
        "MMLU_world_religions":0.8070175439
    },
    {
        "Model":"7B",
        "URL":"https:\/\/huggingface.co\/CausalLM\/7B",
        "full_model_name":"CausalLM\/7B",
        "Parameters":7.0,
        "MMLU_average":0.6179183626,
        "arc:challenge|25":0.4701365188,
        "hellaswag|10":0.5603465445,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.5407407407,
        "MMLU_astronomy":0.625,
        "MMLU_business_ethics":0.66,
        "MMLU_clinical_knowledge":0.7132075472,
        "MMLU_college_biology":0.7361111111,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.56,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.7167630058,
        "MMLU_college_physics":0.3823529412,
        "MMLU_computer_security":0.73,
        "MMLU_conceptual_physics":0.5361702128,
        "MMLU_econometrics":0.4035087719,
        "MMLU_electrical_engineering":0.524137931,
        "MMLU_elementary_mathematics":0.4444444444,
        "MMLU_formal_logic":0.4444444444,
        "MMLU_global_facts":0.41,
        "MMLU_high_school_biology":0.7516129032,
        "MMLU_high_school_chemistry":0.4827586207,
        "MMLU_high_school_computer_science":0.64,
        "MMLU_high_school_european_history":0.7636363636,
        "MMLU_high_school_geography":0.8181818182,
        "MMLU_high_school_government_and_politics":0.8549222798,
        "MMLU_high_school_macroeconomics":0.5974358974,
        "MMLU_high_school_mathematics":0.3074074074,
        "MMLU_high_school_microeconomics":0.6008403361,
        "MMLU_high_school_physics":0.3907284768,
        "MMLU_high_school_psychology":0.8220183486,
        "MMLU_high_school_statistics":0.5046296296,
        "MMLU_high_school_us_history":0.7549019608,
        "MMLU_high_school_world_history":0.776371308,
        "MMLU_human_aging":0.6591928251,
        "MMLU_human_sexuality":0.7022900763,
        "MMLU_international_law":0.7933884298,
        "MMLU_jurisprudence":0.7314814815,
        "MMLU_logical_fallacies":0.6932515337,
        "MMLU_machine_learning":0.5,
        "MMLU_management":0.7766990291,
        "MMLU_marketing":0.8376068376,
        "MMLU_medical_genetics":0.71,
        "MMLU_miscellaneous":0.809706258,
        "MMLU_moral_disputes":0.6560693642,
        "MMLU_moral_scenarios":0.2759776536,
        "MMLU_nutrition":0.6764705882,
        "MMLU_philosophy":0.6848874598,
        "MMLU_prehistory":0.6913580247,
        "MMLU_professional_accounting":0.4219858156,
        "MMLU_professional_law":0.4954367666,
        "MMLU_professional_medicine":0.6360294118,
        "MMLU_professional_psychology":0.6160130719,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.7265306122,
        "MMLU_sociology":0.8507462687,
        "MMLU_us_foreign_policy":0.85,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.7894736842
    },
    {
        "Model":"mamba-gpt-7b-v2",
        "URL":"https:\/\/huggingface.co\/CobraMamba\/mamba-gpt-7b-v2",
        "full_model_name":"CobraMamba\/mamba-gpt-7b-v2",
        "Parameters":7.0,
        "MMLU_average":0.6174385055,
        "arc:challenge|25":0.5750853242,
        "hellaswag|10":0.6363274248,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.5777777778,
        "MMLU_astronomy":0.6315789474,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.6603773585,
        "MMLU_college_biology":0.7152777778,
        "MMLU_college_chemistry":0.45,
        "MMLU_college_computer_science":0.51,
        "MMLU_college_mathematics":0.41,
        "MMLU_college_medicine":0.5895953757,
        "MMLU_college_physics":0.3823529412,
        "MMLU_computer_security":0.74,
        "MMLU_conceptual_physics":0.5191489362,
        "MMLU_econometrics":0.4912280702,
        "MMLU_electrical_engineering":0.5586206897,
        "MMLU_elementary_mathematics":0.4074074074,
        "MMLU_formal_logic":0.3888888889,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.7225806452,
        "MMLU_high_school_chemistry":0.4630541872,
        "MMLU_high_school_computer_science":0.66,
        "MMLU_high_school_european_history":0.7636363636,
        "MMLU_high_school_geography":0.803030303,
        "MMLU_high_school_government_and_politics":0.8186528497,
        "MMLU_high_school_macroeconomics":0.6128205128,
        "MMLU_high_school_mathematics":0.3444444444,
        "MMLU_high_school_microeconomics":0.6092436975,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.7779816514,
        "MMLU_high_school_statistics":0.5185185185,
        "MMLU_high_school_us_history":0.7450980392,
        "MMLU_high_school_world_history":0.776371308,
        "MMLU_human_aging":0.66367713,
        "MMLU_human_sexuality":0.7480916031,
        "MMLU_international_law":0.7851239669,
        "MMLU_jurisprudence":0.7314814815,
        "MMLU_logical_fallacies":0.736196319,
        "MMLU_machine_learning":0.4642857143,
        "MMLU_management":0.7766990291,
        "MMLU_marketing":0.8547008547,
        "MMLU_medical_genetics":0.74,
        "MMLU_miscellaneous":0.7918263091,
        "MMLU_moral_disputes":0.676300578,
        "MMLU_moral_scenarios":0.3586592179,
        "MMLU_nutrition":0.7254901961,
        "MMLU_philosophy":0.691318328,
        "MMLU_prehistory":0.6604938272,
        "MMLU_professional_accounting":0.4397163121,
        "MMLU_professional_law":0.4400260756,
        "MMLU_professional_medicine":0.6397058824,
        "MMLU_professional_psychology":0.637254902,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.6979591837,
        "MMLU_sociology":0.7960199005,
        "MMLU_us_foreign_policy":0.86,
        "MMLU_virology":0.578313253,
        "MMLU_world_religions":0.8304093567
    },
    {
        "Model":"llama-30b-instruct-2048-PL-lora",
        "URL":"https:\/\/huggingface.co\/Aspik101\/llama-30b-instruct-2048-PL-lora",
        "full_model_name":"Aspik101\/llama-30b-instruct-2048-PL-lora",
        "Parameters":30.0,
        "MMLU_average":0.6169060066,
        "arc:challenge|25":0.6023890785,
        "hellaswag|10":0.6486755626,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.6,
        "MMLU_astronomy":0.6776315789,
        "MMLU_business_ethics":0.63,
        "MMLU_clinical_knowledge":0.6679245283,
        "MMLU_college_biology":0.6597222222,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.51,
        "MMLU_college_mathematics":0.39,
        "MMLU_college_medicine":0.5606936416,
        "MMLU_college_physics":0.4117647059,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.5404255319,
        "MMLU_econometrics":0.4122807018,
        "MMLU_electrical_engineering":0.5103448276,
        "MMLU_elementary_mathematics":0.3915343915,
        "MMLU_formal_logic":0.373015873,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.764516129,
        "MMLU_high_school_chemistry":0.4482758621,
        "MMLU_high_school_computer_science":0.63,
        "MMLU_high_school_european_history":0.7515151515,
        "MMLU_high_school_geography":0.8232323232,
        "MMLU_high_school_government_and_politics":0.896373057,
        "MMLU_high_school_macroeconomics":0.6051282051,
        "MMLU_high_school_mathematics":0.3185185185,
        "MMLU_high_school_microeconomics":0.6596638655,
        "MMLU_high_school_physics":0.357615894,
        "MMLU_high_school_psychology":0.8128440367,
        "MMLU_high_school_statistics":0.5046296296,
        "MMLU_high_school_us_history":0.8137254902,
        "MMLU_high_school_world_history":0.8227848101,
        "MMLU_human_aging":0.7040358744,
        "MMLU_human_sexuality":0.7022900763,
        "MMLU_international_law":0.7933884298,
        "MMLU_jurisprudence":0.7592592593,
        "MMLU_logical_fallacies":0.6625766871,
        "MMLU_machine_learning":0.4375,
        "MMLU_management":0.8252427184,
        "MMLU_marketing":0.8547008547,
        "MMLU_medical_genetics":0.64,
        "MMLU_miscellaneous":0.7931034483,
        "MMLU_moral_disputes":0.6907514451,
        "MMLU_moral_scenarios":0.4446927374,
        "MMLU_nutrition":0.660130719,
        "MMLU_philosophy":0.6623794212,
        "MMLU_prehistory":0.7407407407,
        "MMLU_professional_accounting":0.4645390071,
        "MMLU_professional_law":0.4465449804,
        "MMLU_professional_medicine":0.5845588235,
        "MMLU_professional_psychology":0.6470588235,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.7265306122,
        "MMLU_sociology":0.7960199005,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.4759036145,
        "MMLU_world_religions":0.8011695906
    },
    {
        "Model":"Mistral-7B-guanaco1k-ep2",
        "URL":"https:\/\/huggingface.co\/caisarl76\/Mistral-7B-guanaco1k-ep2",
        "full_model_name":"caisarl76\/Mistral-7B-guanaco1k-ep2",
        "Parameters":7.0,
        "MMLU_average":0.614974445,
        "arc:challenge|25":0.5631399317,
        "hellaswag|10":0.627265485,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.6444444444,
        "MMLU_astronomy":0.625,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.6566037736,
        "MMLU_college_biology":0.6944444444,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.51,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5722543353,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.77,
        "MMLU_conceptual_physics":0.5361702128,
        "MMLU_econometrics":0.4649122807,
        "MMLU_electrical_engineering":0.5586206897,
        "MMLU_elementary_mathematics":0.3888888889,
        "MMLU_formal_logic":0.4206349206,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.735483871,
        "MMLU_high_school_chemistry":0.4975369458,
        "MMLU_high_school_computer_science":0.7,
        "MMLU_high_school_european_history":0.7212121212,
        "MMLU_high_school_geography":0.7727272727,
        "MMLU_high_school_government_and_politics":0.8445595855,
        "MMLU_high_school_macroeconomics":0.6153846154,
        "MMLU_high_school_mathematics":0.3185185185,
        "MMLU_high_school_microeconomics":0.6848739496,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7963302752,
        "MMLU_high_school_statistics":0.4814814815,
        "MMLU_high_school_us_history":0.7990196078,
        "MMLU_high_school_world_history":0.776371308,
        "MMLU_human_aging":0.6547085202,
        "MMLU_human_sexuality":0.7633587786,
        "MMLU_international_law":0.7603305785,
        "MMLU_jurisprudence":0.7037037037,
        "MMLU_logical_fallacies":0.773006135,
        "MMLU_machine_learning":0.4107142857,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.8675213675,
        "MMLU_medical_genetics":0.73,
        "MMLU_miscellaneous":0.7943805875,
        "MMLU_moral_disputes":0.7052023121,
        "MMLU_moral_scenarios":0.2659217877,
        "MMLU_nutrition":0.6993464052,
        "MMLU_philosophy":0.7009646302,
        "MMLU_prehistory":0.7222222222,
        "MMLU_professional_accounting":0.4609929078,
        "MMLU_professional_law":0.4282920469,
        "MMLU_professional_medicine":0.6470588235,
        "MMLU_professional_psychology":0.6339869281,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.7265306122,
        "MMLU_sociology":0.8358208955,
        "MMLU_us_foreign_policy":0.85,
        "MMLU_virology":0.5301204819,
        "MMLU_world_religions":0.8011695906
    },
    {
        "Model":"llama-30b-2048-instruct-PL-lora_unload",
        "URL":"https:\/\/huggingface.co\/Aspik101\/llama-30b-2048-instruct-PL-lora_unload",
        "full_model_name":"Aspik101\/llama-30b-2048-instruct-PL-lora_unload",
        "Parameters":30.0,
        "MMLU_average":0.6148938165,
        "arc:challenge|25":0.6006825939,
        "hellaswag|10":0.6494722167,
        "MMLU_abstract_algebra":0.39,
        "MMLU_anatomy":0.5481481481,
        "MMLU_astronomy":0.6710526316,
        "MMLU_business_ethics":0.63,
        "MMLU_clinical_knowledge":0.6716981132,
        "MMLU_college_biology":0.6666666667,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.5,
        "MMLU_college_mathematics":0.4,
        "MMLU_college_medicine":0.5780346821,
        "MMLU_college_physics":0.4607843137,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.5106382979,
        "MMLU_econometrics":0.4122807018,
        "MMLU_electrical_engineering":0.5103448276,
        "MMLU_elementary_mathematics":0.3915343915,
        "MMLU_formal_logic":0.380952381,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.7548387097,
        "MMLU_high_school_chemistry":0.4581280788,
        "MMLU_high_school_computer_science":0.62,
        "MMLU_high_school_european_history":0.7575757576,
        "MMLU_high_school_geography":0.803030303,
        "MMLU_high_school_government_and_politics":0.8860103627,
        "MMLU_high_school_macroeconomics":0.5769230769,
        "MMLU_high_school_mathematics":0.3111111111,
        "MMLU_high_school_microeconomics":0.6596638655,
        "MMLU_high_school_physics":0.3642384106,
        "MMLU_high_school_psychology":0.8055045872,
        "MMLU_high_school_statistics":0.5138888889,
        "MMLU_high_school_us_history":0.8137254902,
        "MMLU_high_school_world_history":0.805907173,
        "MMLU_human_aging":0.6905829596,
        "MMLU_human_sexuality":0.7022900763,
        "MMLU_international_law":0.7933884298,
        "MMLU_jurisprudence":0.75,
        "MMLU_logical_fallacies":0.6625766871,
        "MMLU_machine_learning":0.4464285714,
        "MMLU_management":0.8155339806,
        "MMLU_marketing":0.8504273504,
        "MMLU_medical_genetics":0.62,
        "MMLU_miscellaneous":0.7943805875,
        "MMLU_moral_disputes":0.6791907514,
        "MMLU_moral_scenarios":0.4648044693,
        "MMLU_nutrition":0.6568627451,
        "MMLU_philosophy":0.6816720257,
        "MMLU_prehistory":0.725308642,
        "MMLU_professional_accounting":0.4716312057,
        "MMLU_professional_law":0.4498044329,
        "MMLU_professional_medicine":0.5882352941,
        "MMLU_professional_psychology":0.6503267974,
        "MMLU_public_relations":0.6272727273,
        "MMLU_security_studies":0.6816326531,
        "MMLU_sociology":0.8009950249,
        "MMLU_us_foreign_policy":0.87,
        "MMLU_virology":0.4759036145,
        "MMLU_world_religions":0.8070175439
    },
    {
        "Model":"Synatra-7B-v0.3-dpo",
        "URL":"https:\/\/huggingface.co\/maywell\/Synatra-7B-v0.3-dpo",
        "full_model_name":"maywell\/Synatra-7B-v0.3-dpo",
        "Parameters":7.0,
        "MMLU_average":0.6146292819,
        "arc:challenge|25":0.6006825939,
        "hellaswag|10":0.6278629755,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.5925925926,
        "MMLU_astronomy":0.6578947368,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.6603773585,
        "MMLU_college_biology":0.6875,
        "MMLU_college_chemistry":0.46,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.5953757225,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.79,
        "MMLU_conceptual_physics":0.5404255319,
        "MMLU_econometrics":0.4473684211,
        "MMLU_electrical_engineering":0.5655172414,
        "MMLU_elementary_mathematics":0.4232804233,
        "MMLU_formal_logic":0.3888888889,
        "MMLU_global_facts":0.41,
        "MMLU_high_school_biology":0.7451612903,
        "MMLU_high_school_chemistry":0.4729064039,
        "MMLU_high_school_computer_science":0.69,
        "MMLU_high_school_european_history":0.7818181818,
        "MMLU_high_school_geography":0.7929292929,
        "MMLU_high_school_government_and_politics":0.8393782383,
        "MMLU_high_school_macroeconomics":0.6076923077,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.6554621849,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.8073394495,
        "MMLU_high_school_statistics":0.4907407407,
        "MMLU_high_school_us_history":0.7990196078,
        "MMLU_high_school_world_history":0.7594936709,
        "MMLU_human_aging":0.7085201794,
        "MMLU_human_sexuality":0.6946564885,
        "MMLU_international_law":0.7933884298,
        "MMLU_jurisprudence":0.75,
        "MMLU_logical_fallacies":0.7423312883,
        "MMLU_machine_learning":0.5089285714,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.8547008547,
        "MMLU_medical_genetics":0.72,
        "MMLU_miscellaneous":0.7982120051,
        "MMLU_moral_disputes":0.6936416185,
        "MMLU_moral_scenarios":0.3407821229,
        "MMLU_nutrition":0.6732026144,
        "MMLU_philosophy":0.691318328,
        "MMLU_prehistory":0.7037037037,
        "MMLU_professional_accounting":0.475177305,
        "MMLU_professional_law":0.4491525424,
        "MMLU_professional_medicine":0.6397058824,
        "MMLU_professional_psychology":0.6323529412,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.6734693878,
        "MMLU_sociology":0.7960199005,
        "MMLU_us_foreign_policy":0.81,
        "MMLU_virology":0.4939759036,
        "MMLU_world_religions":0.8070175439
    },
    {
        "Model":"samantha-mistral-7b",
        "URL":"https:\/\/huggingface.co\/ehartford\/samantha-mistral-7b",
        "full_model_name":"ehartford\/samantha-mistral-7b",
        "Parameters":7.0,
        "MMLU_average":0.613641399,
        "arc:challenge|25":0.5895904437,
        "hellaswag|10":0.6470822545,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.6,
        "MMLU_astronomy":0.6776315789,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.6754716981,
        "MMLU_college_biology":0.7013888889,
        "MMLU_college_chemistry":0.45,
        "MMLU_college_computer_science":0.54,
        "MMLU_college_mathematics":0.4,
        "MMLU_college_medicine":0.6242774566,
        "MMLU_college_physics":0.431372549,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.5361702128,
        "MMLU_econometrics":0.5,
        "MMLU_electrical_engineering":0.4965517241,
        "MMLU_elementary_mathematics":0.3756613757,
        "MMLU_formal_logic":0.4206349206,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.7290322581,
        "MMLU_high_school_chemistry":0.4778325123,
        "MMLU_high_school_computer_science":0.66,
        "MMLU_high_school_european_history":0.7393939394,
        "MMLU_high_school_geography":0.7373737374,
        "MMLU_high_school_government_and_politics":0.8601036269,
        "MMLU_high_school_macroeconomics":0.6205128205,
        "MMLU_high_school_mathematics":0.3592592593,
        "MMLU_high_school_microeconomics":0.6218487395,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.7944954128,
        "MMLU_high_school_statistics":0.4907407407,
        "MMLU_high_school_us_history":0.7352941176,
        "MMLU_high_school_world_history":0.7679324895,
        "MMLU_human_aging":0.6547085202,
        "MMLU_human_sexuality":0.7022900763,
        "MMLU_international_law":0.7107438017,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.736196319,
        "MMLU_machine_learning":0.4821428571,
        "MMLU_management":0.7669902913,
        "MMLU_marketing":0.811965812,
        "MMLU_medical_genetics":0.68,
        "MMLU_miscellaneous":0.7918263091,
        "MMLU_moral_disputes":0.6734104046,
        "MMLU_moral_scenarios":0.3709497207,
        "MMLU_nutrition":0.7385620915,
        "MMLU_philosophy":0.6945337621,
        "MMLU_prehistory":0.700617284,
        "MMLU_professional_accounting":0.4397163121,
        "MMLU_professional_law":0.4498044329,
        "MMLU_professional_medicine":0.6397058824,
        "MMLU_professional_psychology":0.6503267974,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.7102040816,
        "MMLU_sociology":0.8258706468,
        "MMLU_us_foreign_policy":0.85,
        "MMLU_virology":0.5301204819,
        "MMLU_world_religions":0.8245614035
    },
    {
        "Model":"blossom-v3-mistral-7b",
        "URL":"https:\/\/huggingface.co\/Azure99\/blossom-v3-mistral-7b",
        "full_model_name":"Azure99\/blossom-v3-mistral-7b",
        "Parameters":7.0,
        "MMLU_average":0.6134680861,
        "arc:challenge|25":0.5639931741,
        "hellaswag|10":0.6340370444,
        "MMLU_abstract_algebra":0.21,
        "MMLU_anatomy":0.562962963,
        "MMLU_astronomy":0.6578947368,
        "MMLU_business_ethics":0.62,
        "MMLU_clinical_knowledge":0.6641509434,
        "MMLU_college_biology":0.6944444444,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.5895953757,
        "MMLU_college_physics":0.3529411765,
        "MMLU_computer_security":0.79,
        "MMLU_conceptual_physics":0.5319148936,
        "MMLU_econometrics":0.4736842105,
        "MMLU_electrical_engineering":0.5448275862,
        "MMLU_elementary_mathematics":0.3915343915,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.25,
        "MMLU_high_school_biology":0.7064516129,
        "MMLU_high_school_chemistry":0.4926108374,
        "MMLU_high_school_computer_science":0.7,
        "MMLU_high_school_european_history":0.7393939394,
        "MMLU_high_school_geography":0.7575757576,
        "MMLU_high_school_government_and_politics":0.8393782383,
        "MMLU_high_school_macroeconomics":0.6230769231,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.6470588235,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.7926605505,
        "MMLU_high_school_statistics":0.5092592593,
        "MMLU_high_school_us_history":0.7892156863,
        "MMLU_high_school_world_history":0.7890295359,
        "MMLU_human_aging":0.668161435,
        "MMLU_human_sexuality":0.7633587786,
        "MMLU_international_law":0.8016528926,
        "MMLU_jurisprudence":0.75,
        "MMLU_logical_fallacies":0.7484662577,
        "MMLU_machine_learning":0.4910714286,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.8675213675,
        "MMLU_medical_genetics":0.69,
        "MMLU_miscellaneous":0.7994891443,
        "MMLU_moral_disputes":0.676300578,
        "MMLU_moral_scenarios":0.3709497207,
        "MMLU_nutrition":0.7189542484,
        "MMLU_philosophy":0.6945337621,
        "MMLU_prehistory":0.700617284,
        "MMLU_professional_accounting":0.4432624113,
        "MMLU_professional_law":0.443285528,
        "MMLU_professional_medicine":0.6507352941,
        "MMLU_professional_psychology":0.6503267974,
        "MMLU_public_relations":0.6727272727,
        "MMLU_security_studies":0.7306122449,
        "MMLU_sociology":0.8059701493,
        "MMLU_us_foreign_policy":0.86,
        "MMLU_virology":0.5120481928,
        "MMLU_world_religions":0.8070175439
    },
    {
        "Model":"koOpenChat-sft",
        "URL":"https:\/\/huggingface.co\/maywell\/koOpenChat-sft",
        "full_model_name":"maywell\/koOpenChat-sft",
        "Parameters":null,
        "MMLU_average":0.6131723777,
        "arc:challenge|25":0.5682593857,
        "hellaswag|10":0.5913164708,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.5481481481,
        "MMLU_astronomy":0.6578947368,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.6943396226,
        "MMLU_college_biology":0.6527777778,
        "MMLU_college_chemistry":0.49,
        "MMLU_college_computer_science":0.52,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.6416184971,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.76,
        "MMLU_conceptual_physics":0.5617021277,
        "MMLU_econometrics":0.4385964912,
        "MMLU_electrical_engineering":0.5448275862,
        "MMLU_elementary_mathematics":0.3888888889,
        "MMLU_formal_logic":0.4206349206,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.7451612903,
        "MMLU_high_school_chemistry":0.4581280788,
        "MMLU_high_school_computer_science":0.65,
        "MMLU_high_school_european_history":0.7333333333,
        "MMLU_high_school_geography":0.7525252525,
        "MMLU_high_school_government_and_politics":0.8808290155,
        "MMLU_high_school_macroeconomics":0.6256410256,
        "MMLU_high_school_mathematics":0.3444444444,
        "MMLU_high_school_microeconomics":0.6428571429,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.823853211,
        "MMLU_high_school_statistics":0.4953703704,
        "MMLU_high_school_us_history":0.8039215686,
        "MMLU_high_school_world_history":0.8016877637,
        "MMLU_human_aging":0.6771300448,
        "MMLU_human_sexuality":0.7328244275,
        "MMLU_international_law":0.7768595041,
        "MMLU_jurisprudence":0.7407407407,
        "MMLU_logical_fallacies":0.7055214724,
        "MMLU_machine_learning":0.4464285714,
        "MMLU_management":0.7961165049,
        "MMLU_marketing":0.8632478632,
        "MMLU_medical_genetics":0.7,
        "MMLU_miscellaneous":0.7956577267,
        "MMLU_moral_disputes":0.6965317919,
        "MMLU_moral_scenarios":0.4134078212,
        "MMLU_nutrition":0.6503267974,
        "MMLU_philosophy":0.6977491961,
        "MMLU_prehistory":0.6481481481,
        "MMLU_professional_accounting":0.4680851064,
        "MMLU_professional_law":0.4511082138,
        "MMLU_professional_medicine":0.5588235294,
        "MMLU_professional_psychology":0.6192810458,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6489795918,
        "MMLU_sociology":0.7910447761,
        "MMLU_us_foreign_policy":0.8,
        "MMLU_virology":0.4759036145,
        "MMLU_world_religions":0.8011695906
    },
    {
        "Model":"Mistral-7B-OpenOrca-Guanaco-accu16",
        "URL":"https:\/\/huggingface.co\/caisarl76\/Mistral-7B-OpenOrca-Guanaco-accu16",
        "full_model_name":"caisarl76\/Mistral-7B-OpenOrca-Guanaco-accu16",
        "Parameters":7.0,
        "MMLU_average":0.6129237969,
        "arc:challenge|25":0.5631399317,
        "hellaswag|10":0.6345349532,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.5703703704,
        "MMLU_astronomy":0.6776315789,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.6716981132,
        "MMLU_college_biology":0.7013888889,
        "MMLU_college_chemistry":0.46,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.6011560694,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.78,
        "MMLU_conceptual_physics":0.5574468085,
        "MMLU_econometrics":0.4210526316,
        "MMLU_electrical_engineering":0.5448275862,
        "MMLU_elementary_mathematics":0.3915343915,
        "MMLU_formal_logic":0.4047619048,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.7483870968,
        "MMLU_high_school_chemistry":0.5172413793,
        "MMLU_high_school_computer_science":0.64,
        "MMLU_high_school_european_history":0.7454545455,
        "MMLU_high_school_geography":0.7777777778,
        "MMLU_high_school_government_and_politics":0.8549222798,
        "MMLU_high_school_macroeconomics":0.6102564103,
        "MMLU_high_school_mathematics":0.3259259259,
        "MMLU_high_school_microeconomics":0.6680672269,
        "MMLU_high_school_physics":0.3774834437,
        "MMLU_high_school_psychology":0.8183486239,
        "MMLU_high_school_statistics":0.4907407407,
        "MMLU_high_school_us_history":0.7990196078,
        "MMLU_high_school_world_history":0.7805907173,
        "MMLU_human_aging":0.6995515695,
        "MMLU_human_sexuality":0.7557251908,
        "MMLU_international_law":0.7520661157,
        "MMLU_jurisprudence":0.7592592593,
        "MMLU_logical_fallacies":0.7055214724,
        "MMLU_machine_learning":0.4285714286,
        "MMLU_management":0.786407767,
        "MMLU_marketing":0.858974359,
        "MMLU_medical_genetics":0.71,
        "MMLU_miscellaneous":0.7841634738,
        "MMLU_moral_disputes":0.6647398844,
        "MMLU_moral_scenarios":0.2659217877,
        "MMLU_nutrition":0.7058823529,
        "MMLU_philosophy":0.7041800643,
        "MMLU_prehistory":0.6635802469,
        "MMLU_professional_accounting":0.4468085106,
        "MMLU_professional_law":0.4511082138,
        "MMLU_professional_medicine":0.5808823529,
        "MMLU_professional_psychology":0.6323529412,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.7102040816,
        "MMLU_sociology":0.8407960199,
        "MMLU_us_foreign_policy":0.85,
        "MMLU_virology":0.5180722892,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"zephyr-7b-beta",
        "URL":"https:\/\/huggingface.co\/HuggingFaceH4\/zephyr-7b-beta",
        "full_model_name":"HuggingFaceH4\/zephyr-7b-beta",
        "Parameters":7.0,
        "MMLU_average":0.6106911136,
        "arc:challenge|25":0.590443686,
        "hellaswag|10":0.6491734714,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.6074074074,
        "MMLU_astronomy":0.6052631579,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.6603773585,
        "MMLU_college_biology":0.7013888889,
        "MMLU_college_chemistry":0.48,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.6416184971,
        "MMLU_college_physics":0.4215686275,
        "MMLU_computer_security":0.73,
        "MMLU_conceptual_physics":0.5191489362,
        "MMLU_econometrics":0.4210526316,
        "MMLU_electrical_engineering":0.5379310345,
        "MMLU_elementary_mathematics":0.3677248677,
        "MMLU_formal_logic":0.4444444444,
        "MMLU_global_facts":0.4,
        "MMLU_high_school_biology":0.7483870968,
        "MMLU_high_school_chemistry":0.5172413793,
        "MMLU_high_school_computer_science":0.67,
        "MMLU_high_school_european_history":0.7575757576,
        "MMLU_high_school_geography":0.7323232323,
        "MMLU_high_school_government_and_politics":0.8238341969,
        "MMLU_high_school_macroeconomics":0.6282051282,
        "MMLU_high_school_mathematics":0.3444444444,
        "MMLU_high_school_microeconomics":0.6764705882,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.8091743119,
        "MMLU_high_school_statistics":0.5462962963,
        "MMLU_high_school_us_history":0.7794117647,
        "MMLU_high_school_world_history":0.7341772152,
        "MMLU_human_aging":0.6367713004,
        "MMLU_human_sexuality":0.6717557252,
        "MMLU_international_law":0.7107438017,
        "MMLU_jurisprudence":0.7777777778,
        "MMLU_logical_fallacies":0.7055214724,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.8803418803,
        "MMLU_medical_genetics":0.71,
        "MMLU_miscellaneous":0.7777777778,
        "MMLU_moral_disputes":0.6705202312,
        "MMLU_moral_scenarios":0.3418994413,
        "MMLU_nutrition":0.6830065359,
        "MMLU_philosophy":0.6688102894,
        "MMLU_prehistory":0.6666666667,
        "MMLU_professional_accounting":0.4929078014,
        "MMLU_professional_law":0.4276401565,
        "MMLU_professional_medicine":0.6470588235,
        "MMLU_professional_psychology":0.6192810458,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.6816326531,
        "MMLU_sociology":0.8009950249,
        "MMLU_us_foreign_policy":0.78,
        "MMLU_virology":0.5180722892,
        "MMLU_world_religions":0.8070175439
    },
    {
        "Model":"robin-65b-v2-delta",
        "URL":"https:\/\/huggingface.co\/OptimalScale\/robin-65b-v2-delta",
        "full_model_name":"OptimalScale\/robin-65b-v2-delta",
        "Parameters":65.0,
        "MMLU_average":0.608221153,
        "arc:challenge|25":0.5819112628,
        "hellaswag|10":0.6137223661,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.562962963,
        "MMLU_astronomy":0.6776315789,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.6264150943,
        "MMLU_college_biology":0.6597222222,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.5895953757,
        "MMLU_college_physics":0.3823529412,
        "MMLU_computer_security":0.79,
        "MMLU_conceptual_physics":0.570212766,
        "MMLU_econometrics":0.3771929825,
        "MMLU_electrical_engineering":0.5103448276,
        "MMLU_elementary_mathematics":0.3835978836,
        "MMLU_formal_logic":0.3333333333,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.7322580645,
        "MMLU_high_school_chemistry":0.3793103448,
        "MMLU_high_school_computer_science":0.64,
        "MMLU_high_school_european_history":0.7696969697,
        "MMLU_high_school_geography":0.7777777778,
        "MMLU_high_school_government_and_politics":0.8808290155,
        "MMLU_high_school_macroeconomics":0.6102564103,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.6260504202,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.8036697248,
        "MMLU_high_school_statistics":0.4953703704,
        "MMLU_high_school_us_history":0.8088235294,
        "MMLU_high_school_world_history":0.8185654008,
        "MMLU_human_aging":0.66367713,
        "MMLU_human_sexuality":0.6870229008,
        "MMLU_international_law":0.7851239669,
        "MMLU_jurisprudence":0.7037037037,
        "MMLU_logical_fallacies":0.7423312883,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.8675213675,
        "MMLU_medical_genetics":0.66,
        "MMLU_miscellaneous":0.7994891443,
        "MMLU_moral_disputes":0.6907514451,
        "MMLU_moral_scenarios":0.3441340782,
        "MMLU_nutrition":0.6405228758,
        "MMLU_philosophy":0.7106109325,
        "MMLU_prehistory":0.7314814815,
        "MMLU_professional_accounting":0.4716312057,
        "MMLU_professional_law":0.4654498044,
        "MMLU_professional_medicine":0.5808823529,
        "MMLU_professional_psychology":0.6666666667,
        "MMLU_public_relations":0.7,
        "MMLU_security_studies":0.6979591837,
        "MMLU_sociology":0.855721393,
        "MMLU_us_foreign_policy":0.87,
        "MMLU_virology":0.5301204819,
        "MMLU_world_religions":0.8128654971
    },
    {
        "Model":"Ferret-7B",
        "URL":"https:\/\/huggingface.co\/euclaise\/Ferret-7B",
        "full_model_name":"euclaise\/Ferret-7B",
        "Parameters":7.0,
        "MMLU_average":0.6081853573,
        "arc:challenge|25":0.5836177474,
        "hellaswag|10":0.628062139,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.6296296296,
        "MMLU_astronomy":0.6644736842,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.6905660377,
        "MMLU_college_biology":0.6527777778,
        "MMLU_college_chemistry":0.47,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5895953757,
        "MMLU_college_physics":0.3431372549,
        "MMLU_computer_security":0.75,
        "MMLU_conceptual_physics":0.5872340426,
        "MMLU_econometrics":0.4473684211,
        "MMLU_electrical_engineering":0.5655172414,
        "MMLU_elementary_mathematics":0.3994708995,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.6806451613,
        "MMLU_high_school_chemistry":0.4729064039,
        "MMLU_high_school_computer_science":0.64,
        "MMLU_high_school_european_history":0.7393939394,
        "MMLU_high_school_geography":0.7777777778,
        "MMLU_high_school_government_and_politics":0.8186528497,
        "MMLU_high_school_macroeconomics":0.6128205128,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.6848739496,
        "MMLU_high_school_physics":0.357615894,
        "MMLU_high_school_psychology":0.8128440367,
        "MMLU_high_school_statistics":0.4537037037,
        "MMLU_high_school_us_history":0.7990196078,
        "MMLU_high_school_world_history":0.776371308,
        "MMLU_human_aging":0.6860986547,
        "MMLU_human_sexuality":0.7404580153,
        "MMLU_international_law":0.7024793388,
        "MMLU_jurisprudence":0.7407407407,
        "MMLU_logical_fallacies":0.7239263804,
        "MMLU_machine_learning":0.4196428571,
        "MMLU_management":0.7475728155,
        "MMLU_marketing":0.8205128205,
        "MMLU_medical_genetics":0.68,
        "MMLU_miscellaneous":0.8033205619,
        "MMLU_moral_disputes":0.6416184971,
        "MMLU_moral_scenarios":0.2972067039,
        "MMLU_nutrition":0.6928104575,
        "MMLU_philosophy":0.6688102894,
        "MMLU_prehistory":0.6913580247,
        "MMLU_professional_accounting":0.4822695035,
        "MMLU_professional_law":0.4211212516,
        "MMLU_professional_medicine":0.6323529412,
        "MMLU_professional_psychology":0.6470588235,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.6816326531,
        "MMLU_sociology":0.7910447761,
        "MMLU_us_foreign_policy":0.81,
        "MMLU_virology":0.5240963855,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"SciPhi-Self-RAG-Mistral-7B-32k",
        "URL":"https:\/\/huggingface.co\/SciPhi\/SciPhi-Self-RAG-Mistral-7B-32k",
        "full_model_name":"SciPhi\/SciPhi-Self-RAG-Mistral-7B-32k",
        "Parameters":7.0,
        "MMLU_average":0.6081312382,
        "arc:challenge|25":0.5341296928,
        "hellaswag|10":0.6132244573,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.6513157895,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.6264150943,
        "MMLU_college_biology":0.7083333333,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.6242774566,
        "MMLU_college_physics":0.4411764706,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.5063829787,
        "MMLU_econometrics":0.3859649123,
        "MMLU_electrical_engineering":0.5310344828,
        "MMLU_elementary_mathematics":0.4206349206,
        "MMLU_formal_logic":0.3888888889,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.7258064516,
        "MMLU_high_school_chemistry":0.4729064039,
        "MMLU_high_school_computer_science":0.62,
        "MMLU_high_school_european_history":0.7757575758,
        "MMLU_high_school_geography":0.7323232323,
        "MMLU_high_school_government_and_politics":0.8393782383,
        "MMLU_high_school_macroeconomics":0.6128205128,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.6680672269,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.8128440367,
        "MMLU_high_school_statistics":0.4675925926,
        "MMLU_high_school_us_history":0.7794117647,
        "MMLU_high_school_world_history":0.7637130802,
        "MMLU_human_aging":0.6950672646,
        "MMLU_human_sexuality":0.7251908397,
        "MMLU_international_law":0.8016528926,
        "MMLU_jurisprudence":0.7777777778,
        "MMLU_logical_fallacies":0.736196319,
        "MMLU_machine_learning":0.4464285714,
        "MMLU_management":0.786407767,
        "MMLU_marketing":0.8461538462,
        "MMLU_medical_genetics":0.65,
        "MMLU_miscellaneous":0.7994891443,
        "MMLU_moral_disputes":0.6936416185,
        "MMLU_moral_scenarios":0.3709497207,
        "MMLU_nutrition":0.6895424837,
        "MMLU_philosophy":0.6752411576,
        "MMLU_prehistory":0.6728395062,
        "MMLU_professional_accounting":0.4503546099,
        "MMLU_professional_law":0.4589308996,
        "MMLU_professional_medicine":0.6507352941,
        "MMLU_professional_psychology":0.6013071895,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.693877551,
        "MMLU_sociology":0.7960199005,
        "MMLU_us_foreign_policy":0.81,
        "MMLU_virology":0.5361445783,
        "MMLU_world_religions":0.8011695906
    },
    {
        "Model":"Synatra-7B-v0.3-RP",
        "URL":"https:\/\/huggingface.co\/maywell\/Synatra-7B-v0.3-RP",
        "full_model_name":"maywell\/Synatra-7B-v0.3-RP",
        "Parameters":7.0,
        "MMLU_average":0.6079991757,
        "arc:challenge|25":0.593003413,
        "hellaswag|10":0.6338378809,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.562962963,
        "MMLU_astronomy":0.6578947368,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.641509434,
        "MMLU_college_biology":0.6597222222,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.51,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5664739884,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.77,
        "MMLU_conceptual_physics":0.5106382979,
        "MMLU_econometrics":0.4561403509,
        "MMLU_electrical_engineering":0.524137931,
        "MMLU_elementary_mathematics":0.4497354497,
        "MMLU_formal_logic":0.4444444444,
        "MMLU_global_facts":0.27,
        "MMLU_high_school_biology":0.735483871,
        "MMLU_high_school_chemistry":0.4630541872,
        "MMLU_high_school_computer_science":0.63,
        "MMLU_high_school_european_history":0.7636363636,
        "MMLU_high_school_geography":0.803030303,
        "MMLU_high_school_government_and_politics":0.8497409326,
        "MMLU_high_school_macroeconomics":0.6,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.6344537815,
        "MMLU_high_school_physics":0.3841059603,
        "MMLU_high_school_psychology":0.7944954128,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.8088235294,
        "MMLU_high_school_world_history":0.7679324895,
        "MMLU_human_aging":0.6771300448,
        "MMLU_human_sexuality":0.7099236641,
        "MMLU_international_law":0.8099173554,
        "MMLU_jurisprudence":0.7685185185,
        "MMLU_logical_fallacies":0.7300613497,
        "MMLU_machine_learning":0.5178571429,
        "MMLU_management":0.7669902913,
        "MMLU_marketing":0.8461538462,
        "MMLU_medical_genetics":0.72,
        "MMLU_miscellaneous":0.8173690932,
        "MMLU_moral_disputes":0.6647398844,
        "MMLU_moral_scenarios":0.2346368715,
        "MMLU_nutrition":0.6797385621,
        "MMLU_philosophy":0.691318328,
        "MMLU_prehistory":0.712962963,
        "MMLU_professional_accounting":0.4645390071,
        "MMLU_professional_law":0.4550195567,
        "MMLU_professional_medicine":0.6213235294,
        "MMLU_professional_psychology":0.6339869281,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.6775510204,
        "MMLU_sociology":0.8258706468,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.8070175439
    },
    {
        "Model":"Mistralic-7B-1",
        "URL":"https:\/\/huggingface.co\/SkunkworksAI\/Mistralic-7B-1",
        "full_model_name":"SkunkworksAI\/Mistralic-7B-1",
        "Parameters":7.0,
        "MMLU_average":0.6079577489,
        "arc:challenge|25":0.5716723549,
        "hellaswag|10":0.6345349532,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.5851851852,
        "MMLU_astronomy":0.6776315789,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.6754716981,
        "MMLU_college_biology":0.6666666667,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.51,
        "MMLU_college_mathematics":0.39,
        "MMLU_college_medicine":0.5780346821,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.78,
        "MMLU_conceptual_physics":0.4936170213,
        "MMLU_econometrics":0.4035087719,
        "MMLU_electrical_engineering":0.5517241379,
        "MMLU_elementary_mathematics":0.3915343915,
        "MMLU_formal_logic":0.4523809524,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.735483871,
        "MMLU_high_school_chemistry":0.4236453202,
        "MMLU_high_school_computer_science":0.69,
        "MMLU_high_school_european_history":0.7636363636,
        "MMLU_high_school_geography":0.7727272727,
        "MMLU_high_school_government_and_politics":0.8601036269,
        "MMLU_high_school_macroeconomics":0.6102564103,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.6176470588,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.7871559633,
        "MMLU_high_school_statistics":0.4212962963,
        "MMLU_high_school_us_history":0.8088235294,
        "MMLU_high_school_world_history":0.7805907173,
        "MMLU_human_aging":0.6950672646,
        "MMLU_human_sexuality":0.7175572519,
        "MMLU_international_law":0.7768595041,
        "MMLU_jurisprudence":0.7314814815,
        "MMLU_logical_fallacies":0.7484662577,
        "MMLU_machine_learning":0.4642857143,
        "MMLU_management":0.7766990291,
        "MMLU_marketing":0.8717948718,
        "MMLU_medical_genetics":0.72,
        "MMLU_miscellaneous":0.7931034483,
        "MMLU_moral_disputes":0.6531791908,
        "MMLU_moral_scenarios":0.3396648045,
        "MMLU_nutrition":0.6666666667,
        "MMLU_philosophy":0.6977491961,
        "MMLU_prehistory":0.7067901235,
        "MMLU_professional_accounting":0.4290780142,
        "MMLU_professional_law":0.4491525424,
        "MMLU_professional_medicine":0.5808823529,
        "MMLU_professional_psychology":0.6209150327,
        "MMLU_public_relations":0.6272727273,
        "MMLU_security_studies":0.6653061224,
        "MMLU_sociology":0.8308457711,
        "MMLU_us_foreign_policy":0.81,
        "MMLU_virology":0.4879518072,
        "MMLU_world_religions":0.8187134503
    },
    {
        "Model":"llama2_7b_zh",
        "URL":"https:\/\/huggingface.co\/itsliupeng\/llama2_7b_zh",
        "full_model_name":"itsliupeng\/llama2_7b_zh",
        "Parameters":7.0,
        "MMLU_average":0.606873097,
        "arc:challenge|25":0.4795221843,
        "hellaswag|10":0.5608444533,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.5703703704,
        "MMLU_astronomy":0.6907894737,
        "MMLU_business_ethics":0.64,
        "MMLU_clinical_knowledge":0.6566037736,
        "MMLU_college_biology":0.7013888889,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.5,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.6011560694,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.72,
        "MMLU_conceptual_physics":0.5489361702,
        "MMLU_econometrics":0.3421052632,
        "MMLU_electrical_engineering":0.5724137931,
        "MMLU_elementary_mathematics":0.3888888889,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.7064516129,
        "MMLU_high_school_chemistry":0.4926108374,
        "MMLU_high_school_computer_science":0.62,
        "MMLU_high_school_european_history":0.7393939394,
        "MMLU_high_school_geography":0.7727272727,
        "MMLU_high_school_government_and_politics":0.8393782383,
        "MMLU_high_school_macroeconomics":0.5871794872,
        "MMLU_high_school_mathematics":0.3111111111,
        "MMLU_high_school_microeconomics":0.6764705882,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.8091743119,
        "MMLU_high_school_statistics":0.5185185185,
        "MMLU_high_school_us_history":0.7303921569,
        "MMLU_high_school_world_history":0.7383966245,
        "MMLU_human_aging":0.6591928251,
        "MMLU_human_sexuality":0.6870229008,
        "MMLU_international_law":0.7685950413,
        "MMLU_jurisprudence":0.7685185185,
        "MMLU_logical_fallacies":0.7116564417,
        "MMLU_machine_learning":0.4017857143,
        "MMLU_management":0.7766990291,
        "MMLU_marketing":0.8461538462,
        "MMLU_medical_genetics":0.74,
        "MMLU_miscellaneous":0.7726692209,
        "MMLU_moral_disputes":0.6820809249,
        "MMLU_moral_scenarios":0.3217877095,
        "MMLU_nutrition":0.6633986928,
        "MMLU_philosophy":0.6848874598,
        "MMLU_prehistory":0.6543209877,
        "MMLU_professional_accounting":0.4645390071,
        "MMLU_professional_law":0.4550195567,
        "MMLU_professional_medicine":0.6286764706,
        "MMLU_professional_psychology":0.6160130719,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.7224489796,
        "MMLU_sociology":0.8009950249,
        "MMLU_us_foreign_policy":0.79,
        "MMLU_virology":0.5180722892,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"openinstruct-mistral-7b",
        "URL":"https:\/\/huggingface.co\/monology\/openinstruct-mistral-7b",
        "full_model_name":"monology\/openinstruct-mistral-7b",
        "Parameters":7.0,
        "MMLU_average":0.6055089637,
        "arc:challenge|25":0.5622866894,
        "hellaswag|10":0.6253734316,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.6296296296,
        "MMLU_astronomy":0.6578947368,
        "MMLU_business_ethics":0.58,
        "MMLU_clinical_knowledge":0.679245283,
        "MMLU_college_biology":0.6875,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.5953757225,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.8,
        "MMLU_conceptual_physics":0.5234042553,
        "MMLU_econometrics":0.4473684211,
        "MMLU_electrical_engineering":0.5172413793,
        "MMLU_elementary_mathematics":0.328042328,
        "MMLU_formal_logic":0.3968253968,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.7387096774,
        "MMLU_high_school_chemistry":0.4482758621,
        "MMLU_high_school_computer_science":0.62,
        "MMLU_high_school_european_history":0.7393939394,
        "MMLU_high_school_geography":0.7626262626,
        "MMLU_high_school_government_and_politics":0.8704663212,
        "MMLU_high_school_macroeconomics":0.6282051282,
        "MMLU_high_school_mathematics":0.2888888889,
        "MMLU_high_school_microeconomics":0.6428571429,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.8091743119,
        "MMLU_high_school_statistics":0.4259259259,
        "MMLU_high_school_us_history":0.8088235294,
        "MMLU_high_school_world_history":0.7510548523,
        "MMLU_human_aging":0.6860986547,
        "MMLU_human_sexuality":0.6946564885,
        "MMLU_international_law":0.7685950413,
        "MMLU_jurisprudence":0.75,
        "MMLU_logical_fallacies":0.736196319,
        "MMLU_machine_learning":0.4196428571,
        "MMLU_management":0.7961165049,
        "MMLU_marketing":0.8333333333,
        "MMLU_medical_genetics":0.67,
        "MMLU_miscellaneous":0.7931034483,
        "MMLU_moral_disputes":0.6445086705,
        "MMLU_moral_scenarios":0.3061452514,
        "MMLU_nutrition":0.6960784314,
        "MMLU_philosophy":0.6881028939,
        "MMLU_prehistory":0.7222222222,
        "MMLU_professional_accounting":0.4468085106,
        "MMLU_professional_law":0.4374185137,
        "MMLU_professional_medicine":0.6397058824,
        "MMLU_professional_psychology":0.6454248366,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.6530612245,
        "MMLU_sociology":0.8208955224,
        "MMLU_us_foreign_policy":0.81,
        "MMLU_virology":0.4819277108,
        "MMLU_world_religions":0.7777777778
    },
    {
        "Model":"llama-33B-instructed",
        "URL":"https:\/\/huggingface.co\/Secbone\/llama-33B-instructed",
        "full_model_name":"Secbone\/llama-33B-instructed",
        "Parameters":33.0,
        "MMLU_average":0.605044866,
        "arc:challenge|25":0.6220136519,
        "hellaswag|10":0.680043816,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.6513157895,
        "MMLU_business_ethics":0.61,
        "MMLU_clinical_knowledge":0.6113207547,
        "MMLU_college_biology":0.625,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.4,
        "MMLU_college_medicine":0.5433526012,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.5106382979,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.4896551724,
        "MMLU_elementary_mathematics":0.3756613757,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.7129032258,
        "MMLU_high_school_chemistry":0.4285714286,
        "MMLU_high_school_computer_science":0.61,
        "MMLU_high_school_european_history":0.7333333333,
        "MMLU_high_school_geography":0.7727272727,
        "MMLU_high_school_government_and_politics":0.8860103627,
        "MMLU_high_school_macroeconomics":0.6051282051,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.6470588235,
        "MMLU_high_school_physics":0.3708609272,
        "MMLU_high_school_psychology":0.8128440367,
        "MMLU_high_school_statistics":0.462962963,
        "MMLU_high_school_us_history":0.8088235294,
        "MMLU_high_school_world_history":0.8270042194,
        "MMLU_human_aging":0.6771300448,
        "MMLU_human_sexuality":0.7251908397,
        "MMLU_international_law":0.8016528926,
        "MMLU_jurisprudence":0.7314814815,
        "MMLU_logical_fallacies":0.736196319,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.7475728155,
        "MMLU_marketing":0.8504273504,
        "MMLU_medical_genetics":0.64,
        "MMLU_miscellaneous":0.7956577267,
        "MMLU_moral_disputes":0.7052023121,
        "MMLU_moral_scenarios":0.4569832402,
        "MMLU_nutrition":0.6503267974,
        "MMLU_philosophy":0.7041800643,
        "MMLU_prehistory":0.712962963,
        "MMLU_professional_accounting":0.4680851064,
        "MMLU_professional_law":0.501303781,
        "MMLU_professional_medicine":0.5845588235,
        "MMLU_professional_psychology":0.6503267974,
        "MMLU_public_relations":0.7272727273,
        "MMLU_security_studies":0.7183673469,
        "MMLU_sociology":0.8109452736,
        "MMLU_us_foreign_policy":0.86,
        "MMLU_virology":0.5180722892,
        "MMLU_world_religions":0.8187134503
    },
    {
        "Model":"MistralInstructLongish",
        "URL":"https:\/\/huggingface.co\/KnutJaegersberg\/MistralInstructLongish",
        "full_model_name":"KnutJaegersberg\/MistralInstructLongish",
        "Parameters":null,
        "MMLU_average":0.6048552055,
        "arc:challenge|25":0.5537542662,
        "hellaswag|10":0.6246763593,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.6,
        "MMLU_astronomy":0.625,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.6867924528,
        "MMLU_college_biology":0.6527777778,
        "MMLU_college_chemistry":0.48,
        "MMLU_college_computer_science":0.52,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.6011560694,
        "MMLU_college_physics":0.3921568627,
        "MMLU_computer_security":0.79,
        "MMLU_conceptual_physics":0.5617021277,
        "MMLU_econometrics":0.4649122807,
        "MMLU_electrical_engineering":0.5517241379,
        "MMLU_elementary_mathematics":0.3915343915,
        "MMLU_formal_logic":0.3412698413,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.6967741935,
        "MMLU_high_school_chemistry":0.4532019704,
        "MMLU_high_school_computer_science":0.68,
        "MMLU_high_school_european_history":0.703030303,
        "MMLU_high_school_geography":0.7424242424,
        "MMLU_high_school_government_and_politics":0.8393782383,
        "MMLU_high_school_macroeconomics":0.6076923077,
        "MMLU_high_school_mathematics":0.3407407407,
        "MMLU_high_school_microeconomics":0.5882352941,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.7853211009,
        "MMLU_high_school_statistics":0.4537037037,
        "MMLU_high_school_us_history":0.75,
        "MMLU_high_school_world_history":0.7637130802,
        "MMLU_human_aging":0.6502242152,
        "MMLU_human_sexuality":0.7175572519,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.7314814815,
        "MMLU_logical_fallacies":0.7607361963,
        "MMLU_machine_learning":0.4821428571,
        "MMLU_management":0.786407767,
        "MMLU_marketing":0.8376068376,
        "MMLU_medical_genetics":0.75,
        "MMLU_miscellaneous":0.7943805875,
        "MMLU_moral_disputes":0.6300578035,
        "MMLU_moral_scenarios":0.2581005587,
        "MMLU_nutrition":0.6960784314,
        "MMLU_philosophy":0.652733119,
        "MMLU_prehistory":0.6666666667,
        "MMLU_professional_accounting":0.4219858156,
        "MMLU_professional_law":0.4087353325,
        "MMLU_professional_medicine":0.5919117647,
        "MMLU_professional_psychology":0.6258169935,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.6326530612,
        "MMLU_sociology":0.8208955224,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.5,
        "MMLU_world_religions":0.8362573099
    },
    {
        "Model":"airoboros-33b-gpt4-1.2",
        "URL":"https:\/\/huggingface.co\/jondurbin\/airoboros-33b-gpt4-1.2",
        "full_model_name":"jondurbin\/airoboros-33b-gpt4-1.2",
        "Parameters":33.0,
        "MMLU_average":0.6034817311,
        "arc:challenge|25":0.6322525597,
        "hellaswag|10":0.6643098984,
        "MMLU_abstract_algebra":0.38,
        "MMLU_anatomy":0.5555555556,
        "MMLU_astronomy":0.6513157895,
        "MMLU_business_ethics":0.6,
        "MMLU_clinical_knowledge":0.6075471698,
        "MMLU_college_biology":0.5972222222,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.549132948,
        "MMLU_college_physics":0.3529411765,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.4893617021,
        "MMLU_econometrics":0.3596491228,
        "MMLU_electrical_engineering":0.4896551724,
        "MMLU_elementary_mathematics":0.4047619048,
        "MMLU_formal_logic":0.4126984127,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.7258064516,
        "MMLU_high_school_chemistry":0.4384236453,
        "MMLU_high_school_computer_science":0.6,
        "MMLU_high_school_european_history":0.7090909091,
        "MMLU_high_school_geography":0.7424242424,
        "MMLU_high_school_government_and_politics":0.829015544,
        "MMLU_high_school_macroeconomics":0.5897435897,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.6176470588,
        "MMLU_high_school_physics":0.3642384106,
        "MMLU_high_school_psychology":0.7908256881,
        "MMLU_high_school_statistics":0.4537037037,
        "MMLU_high_school_us_history":0.7941176471,
        "MMLU_high_school_world_history":0.8143459916,
        "MMLU_human_aging":0.6502242152,
        "MMLU_human_sexuality":0.6564885496,
        "MMLU_international_law":0.7851239669,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.717791411,
        "MMLU_machine_learning":0.4464285714,
        "MMLU_management":0.7572815534,
        "MMLU_marketing":0.8547008547,
        "MMLU_medical_genetics":0.7,
        "MMLU_miscellaneous":0.7892720307,
        "MMLU_moral_disputes":0.7023121387,
        "MMLU_moral_scenarios":0.4268156425,
        "MMLU_nutrition":0.660130719,
        "MMLU_philosophy":0.6848874598,
        "MMLU_prehistory":0.700617284,
        "MMLU_professional_accounting":0.475177305,
        "MMLU_professional_law":0.462190352,
        "MMLU_professional_medicine":0.6102941176,
        "MMLU_professional_psychology":0.6241830065,
        "MMLU_public_relations":0.7090909091,
        "MMLU_security_studies":0.706122449,
        "MMLU_sociology":0.8258706468,
        "MMLU_us_foreign_policy":0.85,
        "MMLU_virology":0.5120481928,
        "MMLU_world_religions":0.8362573099
    },
    {
        "Model":"mistral-7b-v0.1-layla-v1",
        "URL":"https:\/\/huggingface.co\/l3utterfly\/mistral-7b-v0.1-layla-v1",
        "full_model_name":"l3utterfly\/mistral-7b-v0.1-layla-v1",
        "Parameters":7.0,
        "MMLU_average":0.6030958308,
        "arc:challenge|25":0.5639931741,
        "hellaswag|10":0.6378211512,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.6074074074,
        "MMLU_astronomy":0.6842105263,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.6716981132,
        "MMLU_college_biology":0.6736111111,
        "MMLU_college_chemistry":0.52,
        "MMLU_college_computer_science":0.51,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.6184971098,
        "MMLU_college_physics":0.3823529412,
        "MMLU_computer_security":0.73,
        "MMLU_conceptual_physics":0.5234042553,
        "MMLU_econometrics":0.3771929825,
        "MMLU_electrical_engineering":0.5379310345,
        "MMLU_elementary_mathematics":0.4100529101,
        "MMLU_formal_logic":0.4444444444,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.6967741935,
        "MMLU_high_school_chemistry":0.4334975369,
        "MMLU_high_school_computer_science":0.59,
        "MMLU_high_school_european_history":0.6909090909,
        "MMLU_high_school_geography":0.7525252525,
        "MMLU_high_school_government_and_politics":0.8238341969,
        "MMLU_high_school_macroeconomics":0.5794871795,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.6344537815,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.7926605505,
        "MMLU_high_school_statistics":0.4074074074,
        "MMLU_high_school_us_history":0.7450980392,
        "MMLU_high_school_world_history":0.7510548523,
        "MMLU_human_aging":0.668161435,
        "MMLU_human_sexuality":0.7175572519,
        "MMLU_international_law":0.7685950413,
        "MMLU_jurisprudence":0.75,
        "MMLU_logical_fallacies":0.754601227,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.7669902913,
        "MMLU_marketing":0.8504273504,
        "MMLU_medical_genetics":0.68,
        "MMLU_miscellaneous":0.7662835249,
        "MMLU_moral_disputes":0.6820809249,
        "MMLU_moral_scenarios":0.4201117318,
        "MMLU_nutrition":0.6928104575,
        "MMLU_philosophy":0.6784565916,
        "MMLU_prehistory":0.6666666667,
        "MMLU_professional_accounting":0.4361702128,
        "MMLU_professional_law":0.4406779661,
        "MMLU_professional_medicine":0.6102941176,
        "MMLU_professional_psychology":0.6045751634,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.6775510204,
        "MMLU_sociology":0.8407960199,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.5240963855,
        "MMLU_world_religions":0.7894736842
    },
    {
        "Model":"neu-sai-it1",
        "URL":"https:\/\/huggingface.co\/CoruNethron\/neu-sai-it1",
        "full_model_name":"CoruNethron\/neu-sai-it1",
        "Parameters":null,
        "MMLU_average":0.6017126829,
        "arc:challenge|25":0.5708191126,
        "hellaswag|10":0.6184027086,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.5851851852,
        "MMLU_astronomy":0.6381578947,
        "MMLU_business_ethics":0.6,
        "MMLU_clinical_knowledge":0.6452830189,
        "MMLU_college_biology":0.6944444444,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.6242774566,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.75,
        "MMLU_conceptual_physics":0.5021276596,
        "MMLU_econometrics":0.4298245614,
        "MMLU_electrical_engineering":0.5379310345,
        "MMLU_elementary_mathematics":0.3835978836,
        "MMLU_formal_logic":0.4206349206,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.7064516129,
        "MMLU_high_school_chemistry":0.4630541872,
        "MMLU_high_school_computer_science":0.62,
        "MMLU_high_school_european_history":0.7151515152,
        "MMLU_high_school_geography":0.7424242424,
        "MMLU_high_school_government_and_politics":0.8393782383,
        "MMLU_high_school_macroeconomics":0.5923076923,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.6386554622,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.8073394495,
        "MMLU_high_school_statistics":0.3935185185,
        "MMLU_high_school_us_history":0.8137254902,
        "MMLU_high_school_world_history":0.776371308,
        "MMLU_human_aging":0.6547085202,
        "MMLU_human_sexuality":0.7328244275,
        "MMLU_international_law":0.7520661157,
        "MMLU_jurisprudence":0.7592592593,
        "MMLU_logical_fallacies":0.7239263804,
        "MMLU_machine_learning":0.4910714286,
        "MMLU_management":0.7766990291,
        "MMLU_marketing":0.858974359,
        "MMLU_medical_genetics":0.77,
        "MMLU_miscellaneous":0.8020434227,
        "MMLU_moral_disputes":0.6242774566,
        "MMLU_moral_scenarios":0.3162011173,
        "MMLU_nutrition":0.6797385621,
        "MMLU_philosophy":0.6848874598,
        "MMLU_prehistory":0.6635802469,
        "MMLU_professional_accounting":0.4219858156,
        "MMLU_professional_law":0.4380704042,
        "MMLU_professional_medicine":0.5735294118,
        "MMLU_professional_psychology":0.6290849673,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6693877551,
        "MMLU_sociology":0.8507462687,
        "MMLU_us_foreign_policy":0.8,
        "MMLU_virology":0.4819277108,
        "MMLU_world_religions":0.8245614035
    },
    {
        "Model":"llama2_7b_mmlu",
        "URL":"https:\/\/huggingface.co\/itsliupeng\/llama2_7b_mmlu",
        "full_model_name":"itsliupeng\/llama2_7b_mmlu",
        "Parameters":7.0,
        "MMLU_average":0.6003934187,
        "arc:challenge|25":0.5179180887,
        "hellaswag|10":0.5918143796,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.5855263158,
        "MMLU_business_ethics":0.61,
        "MMLU_clinical_knowledge":0.6490566038,
        "MMLU_college_biology":0.6527777778,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.6358381503,
        "MMLU_college_physics":0.362745098,
        "MMLU_computer_security":0.77,
        "MMLU_conceptual_physics":0.5191489362,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.5793103448,
        "MMLU_elementary_mathematics":0.3888888889,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.6903225806,
        "MMLU_high_school_chemistry":0.4679802956,
        "MMLU_high_school_computer_science":0.59,
        "MMLU_high_school_european_history":0.7575757576,
        "MMLU_high_school_geography":0.7424242424,
        "MMLU_high_school_government_and_politics":0.8808290155,
        "MMLU_high_school_macroeconomics":0.6,
        "MMLU_high_school_mathematics":0.3111111111,
        "MMLU_high_school_microeconomics":0.6260504202,
        "MMLU_high_school_physics":0.357615894,
        "MMLU_high_school_psychology":0.7853211009,
        "MMLU_high_school_statistics":0.4166666667,
        "MMLU_high_school_us_history":0.7303921569,
        "MMLU_high_school_world_history":0.805907173,
        "MMLU_human_aging":0.668161435,
        "MMLU_human_sexuality":0.7022900763,
        "MMLU_international_law":0.8016528926,
        "MMLU_jurisprudence":0.75,
        "MMLU_logical_fallacies":0.7423312883,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.8058252427,
        "MMLU_marketing":0.8290598291,
        "MMLU_medical_genetics":0.71,
        "MMLU_miscellaneous":0.7765006386,
        "MMLU_moral_disputes":0.6965317919,
        "MMLU_moral_scenarios":0.3407821229,
        "MMLU_nutrition":0.6895424837,
        "MMLU_philosophy":0.7170418006,
        "MMLU_prehistory":0.6759259259,
        "MMLU_professional_accounting":0.4716312057,
        "MMLU_professional_law":0.4328552803,
        "MMLU_professional_medicine":0.5808823529,
        "MMLU_professional_psychology":0.6209150327,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.7102040816,
        "MMLU_sociology":0.7960199005,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.4638554217,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"Llama-chat-AY-13B",
        "URL":"https:\/\/huggingface.co\/posicube\/Llama-chat-AY-13B",
        "full_model_name":"posicube\/Llama-chat-AY-13B",
        "Parameters":13.0,
        "MMLU_average":0.6001343723,
        "arc:challenge|25":0.5981228669,
        "hellaswag|10":0.6362278431,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.5259259259,
        "MMLU_astronomy":0.5986842105,
        "MMLU_business_ethics":0.58,
        "MMLU_clinical_knowledge":0.641509434,
        "MMLU_college_biology":0.6666666667,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.5,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5895953757,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.4936170213,
        "MMLU_econometrics":0.3596491228,
        "MMLU_electrical_engineering":0.5931034483,
        "MMLU_elementary_mathematics":0.3465608466,
        "MMLU_formal_logic":0.373015873,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.6806451613,
        "MMLU_high_school_chemistry":0.4876847291,
        "MMLU_high_school_computer_science":0.61,
        "MMLU_high_school_european_history":0.7212121212,
        "MMLU_high_school_geography":0.7727272727,
        "MMLU_high_school_government_and_politics":0.8704663212,
        "MMLU_high_school_macroeconomics":0.6128205128,
        "MMLU_high_school_mathematics":0.3148148148,
        "MMLU_high_school_microeconomics":0.6092436975,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7889908257,
        "MMLU_high_school_statistics":0.4351851852,
        "MMLU_high_school_us_history":0.8333333333,
        "MMLU_high_school_world_history":0.7721518987,
        "MMLU_human_aging":0.6816143498,
        "MMLU_human_sexuality":0.6870229008,
        "MMLU_international_law":0.7603305785,
        "MMLU_jurisprudence":0.7962962963,
        "MMLU_logical_fallacies":0.6993865031,
        "MMLU_machine_learning":0.4196428571,
        "MMLU_management":0.7475728155,
        "MMLU_marketing":0.8418803419,
        "MMLU_medical_genetics":0.61,
        "MMLU_miscellaneous":0.7905491699,
        "MMLU_moral_disputes":0.661849711,
        "MMLU_moral_scenarios":0.4893854749,
        "MMLU_nutrition":0.6732026144,
        "MMLU_philosophy":0.6720257235,
        "MMLU_prehistory":0.6882716049,
        "MMLU_professional_accounting":0.4787234043,
        "MMLU_professional_law":0.4517601043,
        "MMLU_professional_medicine":0.5955882353,
        "MMLU_professional_psychology":0.5980392157,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.6775510204,
        "MMLU_sociology":0.776119403,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4939759036,
        "MMLU_world_religions":0.8070175439
    },
    {
        "Model":"Pwen-14B-Chat-20_30",
        "URL":"https:\/\/huggingface.co\/JosephusCheung\/Pwen-14B-Chat-20_30",
        "full_model_name":"JosephusCheung\/Pwen-14B-Chat-20_30",
        "Parameters":14.0,
        "MMLU_average":0.6000929732,
        "arc:challenge|25":0.5290102389,
        "hellaswag|10":0.6119298944,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.562962963,
        "MMLU_astronomy":0.6973684211,
        "MMLU_business_ethics":0.68,
        "MMLU_clinical_knowledge":0.6528301887,
        "MMLU_college_biology":0.7083333333,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.53,
        "MMLU_college_mathematics":0.41,
        "MMLU_college_medicine":0.6184971098,
        "MMLU_college_physics":0.4117647059,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.4382978723,
        "MMLU_econometrics":0.4473684211,
        "MMLU_electrical_engineering":0.5586206897,
        "MMLU_elementary_mathematics":0.4947089947,
        "MMLU_formal_logic":0.3968253968,
        "MMLU_global_facts":0.41,
        "MMLU_high_school_biology":0.7032258065,
        "MMLU_high_school_chemistry":0.5418719212,
        "MMLU_high_school_computer_science":0.64,
        "MMLU_high_school_european_history":0.4909090909,
        "MMLU_high_school_geography":0.797979798,
        "MMLU_high_school_government_and_politics":0.792746114,
        "MMLU_high_school_macroeconomics":0.6076923077,
        "MMLU_high_school_mathematics":0.362962963,
        "MMLU_high_school_microeconomics":0.6680672269,
        "MMLU_high_school_physics":0.4238410596,
        "MMLU_high_school_psychology":0.7834862385,
        "MMLU_high_school_statistics":0.5046296296,
        "MMLU_high_school_us_history":0.6176470588,
        "MMLU_high_school_world_history":0.7637130802,
        "MMLU_human_aging":0.5874439462,
        "MMLU_human_sexuality":0.6946564885,
        "MMLU_international_law":0.7768595041,
        "MMLU_jurisprudence":0.6388888889,
        "MMLU_logical_fallacies":0.6748466258,
        "MMLU_machine_learning":0.4017857143,
        "MMLU_management":0.786407767,
        "MMLU_marketing":0.8076923077,
        "MMLU_medical_genetics":0.69,
        "MMLU_miscellaneous":0.7496807152,
        "MMLU_moral_disputes":0.6271676301,
        "MMLU_moral_scenarios":0.382122905,
        "MMLU_nutrition":0.6928104575,
        "MMLU_philosophy":0.652733119,
        "MMLU_prehistory":0.6388888889,
        "MMLU_professional_accounting":0.4609929078,
        "MMLU_professional_law":0.4237288136,
        "MMLU_professional_medicine":0.6286764706,
        "MMLU_professional_psychology":0.6062091503,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.6857142857,
        "MMLU_sociology":0.7810945274,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.783625731
    },
    {
        "Model":"T1B",
        "URL":"https:\/\/huggingface.co\/AA051610\/T1B",
        "full_model_name":"AA051610\/T1B",
        "Parameters":1.0,
        "MMLU_average":0.6000929732,
        "arc:challenge|25":0.5290102389,
        "hellaswag|10":0.6119298944,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.562962963,
        "MMLU_astronomy":0.6973684211,
        "MMLU_business_ethics":0.68,
        "MMLU_clinical_knowledge":0.6528301887,
        "MMLU_college_biology":0.7083333333,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.53,
        "MMLU_college_mathematics":0.41,
        "MMLU_college_medicine":0.6184971098,
        "MMLU_college_physics":0.4117647059,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.4382978723,
        "MMLU_econometrics":0.4473684211,
        "MMLU_electrical_engineering":0.5586206897,
        "MMLU_elementary_mathematics":0.4947089947,
        "MMLU_formal_logic":0.3968253968,
        "MMLU_global_facts":0.41,
        "MMLU_high_school_biology":0.7032258065,
        "MMLU_high_school_chemistry":0.5418719212,
        "MMLU_high_school_computer_science":0.64,
        "MMLU_high_school_european_history":0.4909090909,
        "MMLU_high_school_geography":0.797979798,
        "MMLU_high_school_government_and_politics":0.792746114,
        "MMLU_high_school_macroeconomics":0.6076923077,
        "MMLU_high_school_mathematics":0.362962963,
        "MMLU_high_school_microeconomics":0.6680672269,
        "MMLU_high_school_physics":0.4238410596,
        "MMLU_high_school_psychology":0.7834862385,
        "MMLU_high_school_statistics":0.5046296296,
        "MMLU_high_school_us_history":0.6176470588,
        "MMLU_high_school_world_history":0.7637130802,
        "MMLU_human_aging":0.5874439462,
        "MMLU_human_sexuality":0.6946564885,
        "MMLU_international_law":0.7768595041,
        "MMLU_jurisprudence":0.6388888889,
        "MMLU_logical_fallacies":0.6748466258,
        "MMLU_machine_learning":0.4017857143,
        "MMLU_management":0.786407767,
        "MMLU_marketing":0.8076923077,
        "MMLU_medical_genetics":0.69,
        "MMLU_miscellaneous":0.7496807152,
        "MMLU_moral_disputes":0.6271676301,
        "MMLU_moral_scenarios":0.382122905,
        "MMLU_nutrition":0.6928104575,
        "MMLU_philosophy":0.652733119,
        "MMLU_prehistory":0.6388888889,
        "MMLU_professional_accounting":0.4609929078,
        "MMLU_professional_law":0.4237288136,
        "MMLU_professional_medicine":0.6286764706,
        "MMLU_professional_psychology":0.6062091503,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.6857142857,
        "MMLU_sociology":0.7810945274,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.783625731
    },
    {
        "Model":"sitebunny-13b",
        "URL":"https:\/\/huggingface.co\/42MARU\/sitebunny-13b",
        "full_model_name":"42MARU\/sitebunny-13b",
        "Parameters":13.0,
        "MMLU_average":0.5990765563,
        "arc:challenge|25":0.6040955631,
        "hellaswag|10":0.6422027485,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.5185185185,
        "MMLU_astronomy":0.5657894737,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.5962264151,
        "MMLU_college_biology":0.6944444444,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.54,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.5895953757,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.4808510638,
        "MMLU_econometrics":0.3684210526,
        "MMLU_electrical_engineering":0.5655172414,
        "MMLU_elementary_mathematics":0.335978836,
        "MMLU_formal_logic":0.3888888889,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.664516129,
        "MMLU_high_school_chemistry":0.4827586207,
        "MMLU_high_school_computer_science":0.57,
        "MMLU_high_school_european_history":0.7393939394,
        "MMLU_high_school_geography":0.7828282828,
        "MMLU_high_school_government_and_politics":0.8652849741,
        "MMLU_high_school_macroeconomics":0.6205128205,
        "MMLU_high_school_mathematics":0.3481481481,
        "MMLU_high_school_microeconomics":0.5924369748,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.7908256881,
        "MMLU_high_school_statistics":0.4398148148,
        "MMLU_high_school_us_history":0.8431372549,
        "MMLU_high_school_world_history":0.7721518987,
        "MMLU_human_aging":0.6950672646,
        "MMLU_human_sexuality":0.679389313,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.8148148148,
        "MMLU_logical_fallacies":0.6932515337,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.8461538462,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7943805875,
        "MMLU_moral_disputes":0.6647398844,
        "MMLU_moral_scenarios":0.5251396648,
        "MMLU_nutrition":0.660130719,
        "MMLU_philosophy":0.6816720257,
        "MMLU_prehistory":0.6913580247,
        "MMLU_professional_accounting":0.4858156028,
        "MMLU_professional_law":0.4491525424,
        "MMLU_professional_medicine":0.5919117647,
        "MMLU_professional_psychology":0.5947712418,
        "MMLU_public_relations":0.6727272727,
        "MMLU_security_studies":0.6775510204,
        "MMLU_sociology":0.776119403,
        "MMLU_us_foreign_policy":0.85,
        "MMLU_virology":0.5060240964,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"sheep-duck-llama-2-13b",
        "URL":"https:\/\/huggingface.co\/Riiid\/sheep-duck-llama-2-13b",
        "full_model_name":"Riiid\/sheep-duck-llama-2-13b",
        "Parameters":13.0,
        "MMLU_average":0.5988749336,
        "arc:challenge|25":0.6075085324,
        "hellaswag|10":0.6503684525,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.5986842105,
        "MMLU_business_ethics":0.58,
        "MMLU_clinical_knowledge":0.6,
        "MMLU_college_biology":0.7083333333,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.49,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.5722543353,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.4936170213,
        "MMLU_econometrics":0.3421052632,
        "MMLU_electrical_engineering":0.5586206897,
        "MMLU_elementary_mathematics":0.335978836,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.6838709677,
        "MMLU_high_school_chemistry":0.4581280788,
        "MMLU_high_school_computer_science":0.59,
        "MMLU_high_school_european_history":0.7515151515,
        "MMLU_high_school_geography":0.7575757576,
        "MMLU_high_school_government_and_politics":0.8704663212,
        "MMLU_high_school_macroeconomics":0.6205128205,
        "MMLU_high_school_mathematics":0.3259259259,
        "MMLU_high_school_microeconomics":0.6176470588,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.8,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.8333333333,
        "MMLU_high_school_world_history":0.7805907173,
        "MMLU_human_aging":0.6995515695,
        "MMLU_human_sexuality":0.679389313,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.8055555556,
        "MMLU_logical_fallacies":0.6993865031,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.8376068376,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7841634738,
        "MMLU_moral_disputes":0.676300578,
        "MMLU_moral_scenarios":0.5173184358,
        "MMLU_nutrition":0.6633986928,
        "MMLU_philosophy":0.6752411576,
        "MMLU_prehistory":0.6851851852,
        "MMLU_professional_accounting":0.4539007092,
        "MMLU_professional_law":0.4537157757,
        "MMLU_professional_medicine":0.6102941176,
        "MMLU_professional_psychology":0.6045751634,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.693877551,
        "MMLU_sociology":0.7810945274,
        "MMLU_us_foreign_policy":0.85,
        "MMLU_virology":0.5,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"Mistral-7B-openplatypus-1k",
        "URL":"https:\/\/huggingface.co\/mncai\/Mistral-7B-openplatypus-1k",
        "full_model_name":"mncai\/Mistral-7B-openplatypus-1k",
        "Parameters":7.0,
        "MMLU_average":0.5984043877,
        "arc:challenge|25":0.5674061433,
        "hellaswag|10":0.6487751444,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.5185185185,
        "MMLU_astronomy":0.6578947368,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.641509434,
        "MMLU_college_biology":0.6736111111,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.51,
        "MMLU_college_mathematics":0.42,
        "MMLU_college_medicine":0.5838150289,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.73,
        "MMLU_conceptual_physics":0.4978723404,
        "MMLU_econometrics":0.4736842105,
        "MMLU_electrical_engineering":0.5310344828,
        "MMLU_elementary_mathematics":0.4206349206,
        "MMLU_formal_logic":0.4523809524,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.7161290323,
        "MMLU_high_school_chemistry":0.5024630542,
        "MMLU_high_school_computer_science":0.62,
        "MMLU_high_school_european_history":0.6848484848,
        "MMLU_high_school_geography":0.7121212121,
        "MMLU_high_school_government_and_politics":0.8082901554,
        "MMLU_high_school_macroeconomics":0.5846153846,
        "MMLU_high_school_mathematics":0.3074074074,
        "MMLU_high_school_microeconomics":0.5966386555,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.776146789,
        "MMLU_high_school_statistics":0.3657407407,
        "MMLU_high_school_us_history":0.7843137255,
        "MMLU_high_school_world_history":0.7426160338,
        "MMLU_human_aging":0.6457399103,
        "MMLU_human_sexuality":0.6946564885,
        "MMLU_international_law":0.7685950413,
        "MMLU_jurisprudence":0.712962963,
        "MMLU_logical_fallacies":0.754601227,
        "MMLU_machine_learning":0.4375,
        "MMLU_management":0.8349514563,
        "MMLU_marketing":0.8333333333,
        "MMLU_medical_genetics":0.67,
        "MMLU_miscellaneous":0.779054917,
        "MMLU_moral_disputes":0.6734104046,
        "MMLU_moral_scenarios":0.3709497207,
        "MMLU_nutrition":0.6732026144,
        "MMLU_philosophy":0.691318328,
        "MMLU_prehistory":0.6697530864,
        "MMLU_professional_accounting":0.4255319149,
        "MMLU_professional_law":0.4556714472,
        "MMLU_professional_medicine":0.6580882353,
        "MMLU_professional_psychology":0.6160130719,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.6612244898,
        "MMLU_sociology":0.815920398,
        "MMLU_us_foreign_policy":0.81,
        "MMLU_virology":0.5722891566,
        "MMLU_world_religions":0.783625731
    },
    {
        "Model":"2x-LoRA-Assemble-13B",
        "URL":"https:\/\/huggingface.co\/PulsarAI\/2x-LoRA-Assemble-13B",
        "full_model_name":"PulsarAI\/2x-LoRA-Assemble-13B",
        "Parameters":13.0,
        "MMLU_average":0.5982004785,
        "arc:challenge|25":0.6040955631,
        "hellaswag|10":0.635829516,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.5185185185,
        "MMLU_astronomy":0.6184210526,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.6188679245,
        "MMLU_college_biology":0.6597222222,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.54,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.6011560694,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4978723404,
        "MMLU_econometrics":0.3596491228,
        "MMLU_electrical_engineering":0.5724137931,
        "MMLU_elementary_mathematics":0.3492063492,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.6677419355,
        "MMLU_high_school_chemistry":0.4876847291,
        "MMLU_high_school_computer_science":0.62,
        "MMLU_high_school_european_history":0.7272727273,
        "MMLU_high_school_geography":0.7777777778,
        "MMLU_high_school_government_and_politics":0.8808290155,
        "MMLU_high_school_macroeconomics":0.6205128205,
        "MMLU_high_school_mathematics":0.3222222222,
        "MMLU_high_school_microeconomics":0.5966386555,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.7944954128,
        "MMLU_high_school_statistics":0.4259259259,
        "MMLU_high_school_us_history":0.8333333333,
        "MMLU_high_school_world_history":0.7848101266,
        "MMLU_human_aging":0.6860986547,
        "MMLU_human_sexuality":0.679389313,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.7685185185,
        "MMLU_logical_fallacies":0.6993865031,
        "MMLU_machine_learning":0.4017857143,
        "MMLU_management":0.7572815534,
        "MMLU_marketing":0.8461538462,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.7956577267,
        "MMLU_moral_disputes":0.661849711,
        "MMLU_moral_scenarios":0.4804469274,
        "MMLU_nutrition":0.6699346405,
        "MMLU_philosophy":0.6881028939,
        "MMLU_prehistory":0.7067901235,
        "MMLU_professional_accounting":0.4680851064,
        "MMLU_professional_law":0.4608865711,
        "MMLU_professional_medicine":0.6102941176,
        "MMLU_professional_psychology":0.589869281,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.6775510204,
        "MMLU_sociology":0.7611940299,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4879518072,
        "MMLU_world_religions":0.8011695906
    },
    {
        "Model":"ShiningValiantXS",
        "URL":"https:\/\/huggingface.co\/ValiantLabs\/ShiningValiantXS",
        "full_model_name":"ValiantLabs\/ShiningValiantXS",
        "Parameters":null,
        "MMLU_average":0.5981493543,
        "arc:challenge|25":0.6032423208,
        "hellaswag|10":0.6360286795,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.5259259259,
        "MMLU_astronomy":0.6184210526,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.6226415094,
        "MMLU_college_biology":0.6597222222,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.55,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.5953757225,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4936170213,
        "MMLU_econometrics":0.3596491228,
        "MMLU_electrical_engineering":0.5724137931,
        "MMLU_elementary_mathematics":0.3412698413,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.6709677419,
        "MMLU_high_school_chemistry":0.4827586207,
        "MMLU_high_school_computer_science":0.62,
        "MMLU_high_school_european_history":0.7212121212,
        "MMLU_high_school_geography":0.7777777778,
        "MMLU_high_school_government_and_politics":0.8808290155,
        "MMLU_high_school_macroeconomics":0.6153846154,
        "MMLU_high_school_mathematics":0.3222222222,
        "MMLU_high_school_microeconomics":0.5966386555,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.7889908257,
        "MMLU_high_school_statistics":0.4305555556,
        "MMLU_high_school_us_history":0.8333333333,
        "MMLU_high_school_world_history":0.7848101266,
        "MMLU_human_aging":0.6860986547,
        "MMLU_human_sexuality":0.6870229008,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.7685185185,
        "MMLU_logical_fallacies":0.6932515337,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.7572815534,
        "MMLU_marketing":0.8504273504,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7956577267,
        "MMLU_moral_disputes":0.6560693642,
        "MMLU_moral_scenarios":0.4826815642,
        "MMLU_nutrition":0.6699346405,
        "MMLU_philosophy":0.6848874598,
        "MMLU_prehistory":0.7191358025,
        "MMLU_professional_accounting":0.475177305,
        "MMLU_professional_law":0.4582790091,
        "MMLU_professional_medicine":0.6102941176,
        "MMLU_professional_psychology":0.591503268,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.6734693878,
        "MMLU_sociology":0.7562189055,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4939759036,
        "MMLU_world_religions":0.8011695906
    },
    {
        "Model":"DaringFortitude",
        "URL":"https:\/\/huggingface.co\/sequelbox\/DaringFortitude",
        "full_model_name":"sequelbox\/DaringFortitude",
        "Parameters":null,
        "MMLU_average":0.5981493543,
        "arc:challenge|25":0.6032423208,
        "hellaswag|10":0.6360286795,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.5259259259,
        "MMLU_astronomy":0.6184210526,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.6226415094,
        "MMLU_college_biology":0.6597222222,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.55,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.5953757225,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4936170213,
        "MMLU_econometrics":0.3596491228,
        "MMLU_electrical_engineering":0.5724137931,
        "MMLU_elementary_mathematics":0.3412698413,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.6709677419,
        "MMLU_high_school_chemistry":0.4827586207,
        "MMLU_high_school_computer_science":0.62,
        "MMLU_high_school_european_history":0.7212121212,
        "MMLU_high_school_geography":0.7777777778,
        "MMLU_high_school_government_and_politics":0.8808290155,
        "MMLU_high_school_macroeconomics":0.6153846154,
        "MMLU_high_school_mathematics":0.3222222222,
        "MMLU_high_school_microeconomics":0.5966386555,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.7889908257,
        "MMLU_high_school_statistics":0.4305555556,
        "MMLU_high_school_us_history":0.8333333333,
        "MMLU_high_school_world_history":0.7848101266,
        "MMLU_human_aging":0.6860986547,
        "MMLU_human_sexuality":0.6870229008,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.7685185185,
        "MMLU_logical_fallacies":0.6932515337,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.7572815534,
        "MMLU_marketing":0.8504273504,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7956577267,
        "MMLU_moral_disputes":0.6560693642,
        "MMLU_moral_scenarios":0.4826815642,
        "MMLU_nutrition":0.6699346405,
        "MMLU_philosophy":0.6848874598,
        "MMLU_prehistory":0.7191358025,
        "MMLU_professional_accounting":0.475177305,
        "MMLU_professional_law":0.4582790091,
        "MMLU_professional_medicine":0.6102941176,
        "MMLU_professional_psychology":0.591503268,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.6734693878,
        "MMLU_sociology":0.7562189055,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4939759036,
        "MMLU_world_religions":0.8011695906
    },
    {
        "Model":"airochronos-33B",
        "URL":"https:\/\/huggingface.co\/Henk717\/airochronos-33B",
        "full_model_name":"Henk717\/airochronos-33B",
        "Parameters":33.0,
        "MMLU_average":0.5979359769,
        "arc:challenge|25":0.6186006826,
        "hellaswag|10":0.6603266282,
        "MMLU_abstract_algebra":0.42,
        "MMLU_anatomy":0.562962963,
        "MMLU_astronomy":0.6513157895,
        "MMLU_business_ethics":0.62,
        "MMLU_clinical_knowledge":0.6150943396,
        "MMLU_college_biology":0.6597222222,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.5780346821,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.72,
        "MMLU_conceptual_physics":0.5404255319,
        "MMLU_econometrics":0.3596491228,
        "MMLU_electrical_engineering":0.4827586207,
        "MMLU_elementary_mathematics":0.3862433862,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.4,
        "MMLU_high_school_biology":0.7096774194,
        "MMLU_high_school_chemistry":0.4285714286,
        "MMLU_high_school_computer_science":0.64,
        "MMLU_high_school_european_history":0.6848484848,
        "MMLU_high_school_geography":0.7626262626,
        "MMLU_high_school_government_and_politics":0.829015544,
        "MMLU_high_school_macroeconomics":0.5948717949,
        "MMLU_high_school_mathematics":0.3185185185,
        "MMLU_high_school_microeconomics":0.6386554622,
        "MMLU_high_school_physics":0.3841059603,
        "MMLU_high_school_psychology":0.7798165138,
        "MMLU_high_school_statistics":0.4768518519,
        "MMLU_high_school_us_history":0.7745098039,
        "MMLU_high_school_world_history":0.7932489451,
        "MMLU_human_aging":0.6591928251,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.7768595041,
        "MMLU_jurisprudence":0.75,
        "MMLU_logical_fallacies":0.736196319,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.7669902913,
        "MMLU_marketing":0.858974359,
        "MMLU_medical_genetics":0.68,
        "MMLU_miscellaneous":0.7765006386,
        "MMLU_moral_disputes":0.6705202312,
        "MMLU_moral_scenarios":0.3586592179,
        "MMLU_nutrition":0.6535947712,
        "MMLU_philosophy":0.6752411576,
        "MMLU_prehistory":0.6790123457,
        "MMLU_professional_accounting":0.4680851064,
        "MMLU_professional_law":0.4511082138,
        "MMLU_professional_medicine":0.5772058824,
        "MMLU_professional_psychology":0.637254902,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.6571428571,
        "MMLU_sociology":0.7960199005,
        "MMLU_us_foreign_policy":0.85,
        "MMLU_virology":0.5,
        "MMLU_world_religions":0.783625731
    },
    {
        "Model":"mistral-7b_open_platypus",
        "URL":"https:\/\/huggingface.co\/lgaalves\/mistral-7b_open_platypus",
        "full_model_name":"lgaalves\/mistral-7b_open_platypus",
        "Parameters":7.0,
        "MMLU_average":0.5976246069,
        "arc:challenge|25":0.5332764505,
        "hellaswag|10":0.6120294762,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.5703703704,
        "MMLU_astronomy":0.6381578947,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.6566037736,
        "MMLU_college_biology":0.6458333333,
        "MMLU_college_chemistry":0.46,
        "MMLU_college_computer_science":0.51,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.5895953757,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.77,
        "MMLU_conceptual_physics":0.4978723404,
        "MMLU_econometrics":0.4649122807,
        "MMLU_electrical_engineering":0.5103448276,
        "MMLU_elementary_mathematics":0.4232804233,
        "MMLU_formal_logic":0.373015873,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.6806451613,
        "MMLU_high_school_chemistry":0.4679802956,
        "MMLU_high_school_computer_science":0.64,
        "MMLU_high_school_european_history":0.7393939394,
        "MMLU_high_school_geography":0.7272727273,
        "MMLU_high_school_government_and_politics":0.8497409326,
        "MMLU_high_school_macroeconomics":0.5512820513,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.5672268908,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7724770642,
        "MMLU_high_school_statistics":0.3981481481,
        "MMLU_high_school_us_history":0.7696078431,
        "MMLU_high_school_world_history":0.7848101266,
        "MMLU_human_aging":0.7264573991,
        "MMLU_human_sexuality":0.6564885496,
        "MMLU_international_law":0.8429752066,
        "MMLU_jurisprudence":0.7037037037,
        "MMLU_logical_fallacies":0.7668711656,
        "MMLU_machine_learning":0.4642857143,
        "MMLU_management":0.6990291262,
        "MMLU_marketing":0.8333333333,
        "MMLU_medical_genetics":0.69,
        "MMLU_miscellaneous":0.7956577267,
        "MMLU_moral_disputes":0.6936416185,
        "MMLU_moral_scenarios":0.3765363128,
        "MMLU_nutrition":0.6470588235,
        "MMLU_philosophy":0.7041800643,
        "MMLU_prehistory":0.712962963,
        "MMLU_professional_accounting":0.4858156028,
        "MMLU_professional_law":0.44589309,
        "MMLU_professional_medicine":0.5919117647,
        "MMLU_professional_psychology":0.6209150327,
        "MMLU_public_relations":0.6272727273,
        "MMLU_security_studies":0.6244897959,
        "MMLU_sociology":0.7512437811,
        "MMLU_us_foreign_policy":0.79,
        "MMLU_virology":0.4879518072,
        "MMLU_world_religions":0.7777777778
    },
    {
        "Model":"Karen_TheEditor_V2_STRICT_Mistral_7B",
        "URL":"https:\/\/huggingface.co\/FPHam\/Karen_TheEditor_V2_STRICT_Mistral_7B",
        "full_model_name":"FPHam\/Karen_TheEditor_V2_STRICT_Mistral_7B",
        "Parameters":7.0,
        "MMLU_average":0.5955929356,
        "arc:challenge|25":0.5699658703,
        "hellaswag|10":0.6336387174,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.5925925926,
        "MMLU_astronomy":0.625,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.6226415094,
        "MMLU_college_biology":0.6805555556,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.58,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5549132948,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.75,
        "MMLU_conceptual_physics":0.5021276596,
        "MMLU_econometrics":0.3684210526,
        "MMLU_electrical_engineering":0.5172413793,
        "MMLU_elementary_mathematics":0.3994708995,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.6967741935,
        "MMLU_high_school_chemistry":0.4679802956,
        "MMLU_high_school_computer_science":0.66,
        "MMLU_high_school_european_history":0.7696969697,
        "MMLU_high_school_geography":0.7828282828,
        "MMLU_high_school_government_and_politics":0.8393782383,
        "MMLU_high_school_macroeconomics":0.5615384615,
        "MMLU_high_school_mathematics":0.3777777778,
        "MMLU_high_school_microeconomics":0.6008403361,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.7706422018,
        "MMLU_high_school_statistics":0.4305555556,
        "MMLU_high_school_us_history":0.7794117647,
        "MMLU_high_school_world_history":0.776371308,
        "MMLU_human_aging":0.668161435,
        "MMLU_human_sexuality":0.7022900763,
        "MMLU_international_law":0.7851239669,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.6748466258,
        "MMLU_machine_learning":0.4821428571,
        "MMLU_management":0.786407767,
        "MMLU_marketing":0.8675213675,
        "MMLU_medical_genetics":0.69,
        "MMLU_miscellaneous":0.7943805875,
        "MMLU_moral_disputes":0.6445086705,
        "MMLU_moral_scenarios":0.2502793296,
        "MMLU_nutrition":0.6339869281,
        "MMLU_philosophy":0.652733119,
        "MMLU_prehistory":0.6574074074,
        "MMLU_professional_accounting":0.4290780142,
        "MMLU_professional_law":0.4328552803,
        "MMLU_professional_medicine":0.5477941176,
        "MMLU_professional_psychology":0.6062091503,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.6775510204,
        "MMLU_sociology":0.8407960199,
        "MMLU_us_foreign_policy":0.79,
        "MMLU_virology":0.5120481928,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"llama-2-13B-ensemble-v5",
        "URL":"https:\/\/huggingface.co\/yeontaek\/llama-2-13B-ensemble-v5",
        "full_model_name":"yeontaek\/llama-2-13B-ensemble-v5",
        "Parameters":13.0,
        "MMLU_average":0.5949099167,
        "arc:challenge|25":0.5844709898,
        "hellaswag|10":0.6290579566,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.5185185185,
        "MMLU_astronomy":0.6315789474,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.6264150943,
        "MMLU_college_biology":0.6666666667,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5722543353,
        "MMLU_college_physics":0.362745098,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.4936170213,
        "MMLU_econometrics":0.3333333333,
        "MMLU_electrical_engineering":0.524137931,
        "MMLU_elementary_mathematics":0.3624338624,
        "MMLU_formal_logic":0.3968253968,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.6806451613,
        "MMLU_high_school_chemistry":0.4729064039,
        "MMLU_high_school_computer_science":0.6,
        "MMLU_high_school_european_history":0.7212121212,
        "MMLU_high_school_geography":0.7676767677,
        "MMLU_high_school_government_and_politics":0.8756476684,
        "MMLU_high_school_macroeconomics":0.6076923077,
        "MMLU_high_school_mathematics":0.3333333333,
        "MMLU_high_school_microeconomics":0.6344537815,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.8036697248,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.8382352941,
        "MMLU_high_school_world_history":0.7679324895,
        "MMLU_human_aging":0.6367713004,
        "MMLU_human_sexuality":0.6717557252,
        "MMLU_international_law":0.7190082645,
        "MMLU_jurisprudence":0.7407407407,
        "MMLU_logical_fallacies":0.7239263804,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.8504273504,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7879948914,
        "MMLU_moral_disputes":0.6416184971,
        "MMLU_moral_scenarios":0.4849162011,
        "MMLU_nutrition":0.6405228758,
        "MMLU_philosophy":0.7009646302,
        "MMLU_prehistory":0.7222222222,
        "MMLU_professional_accounting":0.4929078014,
        "MMLU_professional_law":0.4654498044,
        "MMLU_professional_medicine":0.5992647059,
        "MMLU_professional_psychology":0.6029411765,
        "MMLU_public_relations":0.6818181818,
        "MMLU_security_studies":0.6530612245,
        "MMLU_sociology":0.7711442786,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4819277108,
        "MMLU_world_religions":0.8011695906
    },
    {
        "Model":"speechless-llama2-hermes-orca-platypus-13b",
        "URL":"https:\/\/huggingface.co\/uukuguy\/speechless-llama2-hermes-orca-platypus-13b",
        "full_model_name":"uukuguy\/speechless-llama2-hermes-orca-platypus-13b",
        "Parameters":13.0,
        "MMLU_average":0.5938641096,
        "arc:challenge|25":0.5802047782,
        "hellaswag|10":0.6351324437,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.625,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.6150943396,
        "MMLU_college_biology":0.6944444444,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.5,
        "MMLU_college_mathematics":0.42,
        "MMLU_college_medicine":0.5433526012,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.4765957447,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.5310344828,
        "MMLU_elementary_mathematics":0.3756613757,
        "MMLU_formal_logic":0.4285714286,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.6548387097,
        "MMLU_high_school_chemistry":0.4384236453,
        "MMLU_high_school_computer_science":0.59,
        "MMLU_high_school_european_history":0.7272727273,
        "MMLU_high_school_geography":0.7676767677,
        "MMLU_high_school_government_and_politics":0.8601036269,
        "MMLU_high_school_macroeconomics":0.6051282051,
        "MMLU_high_school_mathematics":0.3185185185,
        "MMLU_high_school_microeconomics":0.6218487395,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.8018348624,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.8382352941,
        "MMLU_high_school_world_history":0.7679324895,
        "MMLU_human_aging":0.7040358744,
        "MMLU_human_sexuality":0.641221374,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.7685185185,
        "MMLU_logical_fallacies":0.7300613497,
        "MMLU_machine_learning":0.4017857143,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.8205128205,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.7726692209,
        "MMLU_moral_disputes":0.6734104046,
        "MMLU_moral_scenarios":0.4826815642,
        "MMLU_nutrition":0.614379085,
        "MMLU_philosophy":0.6720257235,
        "MMLU_prehistory":0.700617284,
        "MMLU_professional_accounting":0.4645390071,
        "MMLU_professional_law":0.4706649283,
        "MMLU_professional_medicine":0.6066176471,
        "MMLU_professional_psychology":0.5947712418,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.6612244898,
        "MMLU_sociology":0.7263681592,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4939759036,
        "MMLU_world_religions":0.8245614035
    },
    {
        "Model":"llama-30b-instruct",
        "URL":"https:\/\/huggingface.co\/upstage\/llama-30b-instruct",
        "full_model_name":"upstage\/llama-30b-instruct",
        "Parameters":30.0,
        "MMLU_average":0.5936780557,
        "arc:challenge|25":0.6049488055,
        "hellaswag|10":0.672276439,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.6381578947,
        "MMLU_business_ethics":0.61,
        "MMLU_clinical_knowledge":0.5924528302,
        "MMLU_college_biology":0.6111111111,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.43,
        "MMLU_college_medicine":0.450867052,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.67,
        "MMLU_conceptual_physics":0.4893617021,
        "MMLU_econometrics":0.3947368421,
        "MMLU_electrical_engineering":0.4827586207,
        "MMLU_elementary_mathematics":0.3650793651,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.7096774194,
        "MMLU_high_school_chemistry":0.4334975369,
        "MMLU_high_school_computer_science":0.63,
        "MMLU_high_school_european_history":0.7515151515,
        "MMLU_high_school_geography":0.7828282828,
        "MMLU_high_school_government_and_politics":0.8756476684,
        "MMLU_high_school_macroeconomics":0.5666666667,
        "MMLU_high_school_mathematics":0.2888888889,
        "MMLU_high_school_microeconomics":0.5966386555,
        "MMLU_high_school_physics":0.3973509934,
        "MMLU_high_school_psychology":0.7871559633,
        "MMLU_high_school_statistics":0.4259259259,
        "MMLU_high_school_us_history":0.818627451,
        "MMLU_high_school_world_history":0.8270042194,
        "MMLU_human_aging":0.6591928251,
        "MMLU_human_sexuality":0.6946564885,
        "MMLU_international_law":0.7685950413,
        "MMLU_jurisprudence":0.7314814815,
        "MMLU_logical_fallacies":0.7116564417,
        "MMLU_machine_learning":0.4464285714,
        "MMLU_management":0.7766990291,
        "MMLU_marketing":0.858974359,
        "MMLU_medical_genetics":0.66,
        "MMLU_miscellaneous":0.7841634738,
        "MMLU_moral_disputes":0.6647398844,
        "MMLU_moral_scenarios":0.4547486034,
        "MMLU_nutrition":0.6176470588,
        "MMLU_philosophy":0.691318328,
        "MMLU_prehistory":0.7191358025,
        "MMLU_professional_accounting":0.4219858156,
        "MMLU_professional_law":0.4439374185,
        "MMLU_professional_medicine":0.5477941176,
        "MMLU_professional_psychology":0.6405228758,
        "MMLU_public_relations":0.6727272727,
        "MMLU_security_studies":0.6653061224,
        "MMLU_sociology":0.8059701493,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.4939759036,
        "MMLU_world_religions":0.7719298246
    },
    {
        "Model":"Llama2-chat-AYB-13B",
        "URL":"https:\/\/huggingface.co\/posicube\/Llama2-chat-AYB-13B",
        "full_model_name":"posicube\/Llama2-chat-AYB-13B",
        "Parameters":13.0,
        "MMLU_average":0.5933661173,
        "arc:challenge|25":0.6100682594,
        "hellaswag|10":0.6524596694,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.5333333333,
        "MMLU_astronomy":0.6052631579,
        "MMLU_business_ethics":0.58,
        "MMLU_clinical_knowledge":0.6075471698,
        "MMLU_college_biology":0.6666666667,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5895953757,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4893617021,
        "MMLU_econometrics":0.3421052632,
        "MMLU_electrical_engineering":0.5724137931,
        "MMLU_elementary_mathematics":0.3386243386,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.6903225806,
        "MMLU_high_school_chemistry":0.4827586207,
        "MMLU_high_school_computer_science":0.59,
        "MMLU_high_school_european_history":0.7333333333,
        "MMLU_high_school_geography":0.7474747475,
        "MMLU_high_school_government_and_politics":0.8445595855,
        "MMLU_high_school_macroeconomics":0.6128205128,
        "MMLU_high_school_mathematics":0.3333333333,
        "MMLU_high_school_microeconomics":0.6092436975,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.8018348624,
        "MMLU_high_school_statistics":0.4305555556,
        "MMLU_high_school_us_history":0.8333333333,
        "MMLU_high_school_world_history":0.776371308,
        "MMLU_human_aging":0.6816143498,
        "MMLU_human_sexuality":0.679389313,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.7777777778,
        "MMLU_logical_fallacies":0.7055214724,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.8205128205,
        "MMLU_medical_genetics":0.62,
        "MMLU_miscellaneous":0.779054917,
        "MMLU_moral_disputes":0.6502890173,
        "MMLU_moral_scenarios":0.4770949721,
        "MMLU_nutrition":0.6535947712,
        "MMLU_philosophy":0.6688102894,
        "MMLU_prehistory":0.6820987654,
        "MMLU_professional_accounting":0.475177305,
        "MMLU_professional_law":0.4550195567,
        "MMLU_professional_medicine":0.6029411765,
        "MMLU_professional_psychology":0.5996732026,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.6612244898,
        "MMLU_sociology":0.7810945274,
        "MMLU_us_foreign_policy":0.85,
        "MMLU_virology":0.4939759036,
        "MMLU_world_religions":0.7894736842
    },
    {
        "Model":"speechless-llama2-luban-orca-platypus-13b",
        "URL":"https:\/\/huggingface.co\/uukuguy\/speechless-llama2-luban-orca-platypus-13b",
        "full_model_name":"uukuguy\/speechless-llama2-luban-orca-platypus-13b",
        "Parameters":13.0,
        "MMLU_average":0.5923198995,
        "arc:challenge|25":0.5947098976,
        "hellaswag|10":0.6261700856,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.5333333333,
        "MMLU_astronomy":0.6118421053,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.6037735849,
        "MMLU_college_biology":0.6458333333,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.52,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.5953757225,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.5063829787,
        "MMLU_econometrics":0.3333333333,
        "MMLU_electrical_engineering":0.5517241379,
        "MMLU_elementary_mathematics":0.3412698413,
        "MMLU_formal_logic":0.373015873,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.6741935484,
        "MMLU_high_school_chemistry":0.4778325123,
        "MMLU_high_school_computer_science":0.58,
        "MMLU_high_school_european_history":0.7454545455,
        "MMLU_high_school_geography":0.7575757576,
        "MMLU_high_school_government_and_politics":0.8652849741,
        "MMLU_high_school_macroeconomics":0.6205128205,
        "MMLU_high_school_mathematics":0.3259259259,
        "MMLU_high_school_microeconomics":0.6218487395,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.8,
        "MMLU_high_school_statistics":0.4166666667,
        "MMLU_high_school_us_history":0.8480392157,
        "MMLU_high_school_world_history":0.7848101266,
        "MMLU_human_aging":0.668161435,
        "MMLU_human_sexuality":0.7022900763,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.7685185185,
        "MMLU_logical_fallacies":0.6871165644,
        "MMLU_machine_learning":0.4107142857,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.8290598291,
        "MMLU_medical_genetics":0.63,
        "MMLU_miscellaneous":0.7905491699,
        "MMLU_moral_disputes":0.6445086705,
        "MMLU_moral_scenarios":0.4391061453,
        "MMLU_nutrition":0.6732026144,
        "MMLU_philosophy":0.6816720257,
        "MMLU_prehistory":0.6913580247,
        "MMLU_professional_accounting":0.4787234043,
        "MMLU_professional_law":0.4602346806,
        "MMLU_professional_medicine":0.6029411765,
        "MMLU_professional_psychology":0.5735294118,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.6653061224,
        "MMLU_sociology":0.7711442786,
        "MMLU_us_foreign_policy":0.79,
        "MMLU_virology":0.4879518072,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"vicuna-33b-v1.3",
        "URL":"https:\/\/huggingface.co\/lmsys\/vicuna-33b-v1.3",
        "full_model_name":"lmsys\/vicuna-33b-v1.3",
        "Parameters":33.0,
        "MMLU_average":0.592164721,
        "arc:challenge|25":0.5989761092,
        "hellaswag|10":0.6315475005,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.5037037037,
        "MMLU_astronomy":0.6513157895,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.641509434,
        "MMLU_college_biology":0.625,
        "MMLU_college_chemistry":0.36,
        "MMLU_college_computer_science":0.51,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.5028901734,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4723404255,
        "MMLU_econometrics":0.350877193,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.3756613757,
        "MMLU_formal_logic":0.373015873,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.6806451613,
        "MMLU_high_school_chemistry":0.4137931034,
        "MMLU_high_school_computer_science":0.63,
        "MMLU_high_school_european_history":0.7333333333,
        "MMLU_high_school_geography":0.7626262626,
        "MMLU_high_school_government_and_politics":0.8497409326,
        "MMLU_high_school_macroeconomics":0.5820512821,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.5924369748,
        "MMLU_high_school_physics":0.3708609272,
        "MMLU_high_school_psychology":0.7816513761,
        "MMLU_high_school_statistics":0.412037037,
        "MMLU_high_school_us_history":0.7892156863,
        "MMLU_high_school_world_history":0.8270042194,
        "MMLU_human_aging":0.6547085202,
        "MMLU_human_sexuality":0.7404580153,
        "MMLU_international_law":0.7520661157,
        "MMLU_jurisprudence":0.712962963,
        "MMLU_logical_fallacies":0.717791411,
        "MMLU_machine_learning":0.4642857143,
        "MMLU_management":0.7766990291,
        "MMLU_marketing":0.8760683761,
        "MMLU_medical_genetics":0.65,
        "MMLU_miscellaneous":0.7739463602,
        "MMLU_moral_disputes":0.6849710983,
        "MMLU_moral_scenarios":0.4357541899,
        "MMLU_nutrition":0.6339869281,
        "MMLU_philosophy":0.6816720257,
        "MMLU_prehistory":0.6851851852,
        "MMLU_professional_accounting":0.4255319149,
        "MMLU_professional_law":0.4752281617,
        "MMLU_professional_medicine":0.5772058824,
        "MMLU_professional_psychology":0.6470588235,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6897959184,
        "MMLU_sociology":0.815920398,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.5,
        "MMLU_world_religions":0.7660818713
    },
    {
        "Model":"MLewd-ReMM-L2-Chat-20B",
        "URL":"https:\/\/huggingface.co\/Undi95\/MLewd-ReMM-L2-Chat-20B",
        "full_model_name":"Undi95\/MLewd-ReMM-L2-Chat-20B",
        "Parameters":20.0,
        "MMLU_average":0.5913353415,
        "arc:challenge|25":0.5964163823,
        "hellaswag|10":0.6690898227,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.5037037037,
        "MMLU_astronomy":0.5723684211,
        "MMLU_business_ethics":0.63,
        "MMLU_clinical_knowledge":0.6226415094,
        "MMLU_college_biology":0.6458333333,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.5895953757,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4893617021,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.5310344828,
        "MMLU_elementary_mathematics":0.3465608466,
        "MMLU_formal_logic":0.3968253968,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.7064516129,
        "MMLU_high_school_chemistry":0.4926108374,
        "MMLU_high_school_computer_science":0.61,
        "MMLU_high_school_european_history":0.7696969697,
        "MMLU_high_school_geography":0.7626262626,
        "MMLU_high_school_government_and_politics":0.8393782383,
        "MMLU_high_school_macroeconomics":0.5948717949,
        "MMLU_high_school_mathematics":0.337037037,
        "MMLU_high_school_microeconomics":0.6386554622,
        "MMLU_high_school_physics":0.3841059603,
        "MMLU_high_school_psychology":0.7596330275,
        "MMLU_high_school_statistics":0.462962963,
        "MMLU_high_school_us_history":0.8039215686,
        "MMLU_high_school_world_history":0.7637130802,
        "MMLU_human_aging":0.6995515695,
        "MMLU_human_sexuality":0.7022900763,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.7592592593,
        "MMLU_logical_fallacies":0.7055214724,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.8461538462,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.7586206897,
        "MMLU_moral_disputes":0.6705202312,
        "MMLU_moral_scenarios":0.5083798883,
        "MMLU_nutrition":0.6437908497,
        "MMLU_philosophy":0.6784565916,
        "MMLU_prehistory":0.6728395062,
        "MMLU_professional_accounting":0.4609929078,
        "MMLU_professional_law":0.4628422425,
        "MMLU_professional_medicine":0.5955882353,
        "MMLU_professional_psychology":0.6176470588,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6857142857,
        "MMLU_sociology":0.7462686567,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.4638554217,
        "MMLU_world_religions":0.7543859649
    },
    {
        "Model":"hippogriff-30b-chat",
        "URL":"https:\/\/huggingface.co\/openaccess-ai-collective\/hippogriff-30b-chat",
        "full_model_name":"openaccess-ai-collective\/hippogriff-30b-chat",
        "Parameters":30.0,
        "MMLU_average":0.5908822385,
        "arc:challenge|25":0.6313993174,
        "hellaswag|10":0.6594303924,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.5259259259,
        "MMLU_astronomy":0.6315789474,
        "MMLU_business_ethics":0.61,
        "MMLU_clinical_knowledge":0.6037735849,
        "MMLU_college_biology":0.6527777778,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.5,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.4971098266,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4808510638,
        "MMLU_econometrics":0.3859649123,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.3544973545,
        "MMLU_formal_logic":0.3412698413,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.7129032258,
        "MMLU_high_school_chemistry":0.3891625616,
        "MMLU_high_school_computer_science":0.61,
        "MMLU_high_school_european_history":0.7333333333,
        "MMLU_high_school_geography":0.7575757576,
        "MMLU_high_school_government_and_politics":0.8445595855,
        "MMLU_high_school_macroeconomics":0.5692307692,
        "MMLU_high_school_mathematics":0.2888888889,
        "MMLU_high_school_microeconomics":0.5966386555,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.7871559633,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.8039215686,
        "MMLU_high_school_world_history":0.835443038,
        "MMLU_human_aging":0.66367713,
        "MMLU_human_sexuality":0.6717557252,
        "MMLU_international_law":0.7603305785,
        "MMLU_jurisprudence":0.7037037037,
        "MMLU_logical_fallacies":0.7484662577,
        "MMLU_machine_learning":0.4464285714,
        "MMLU_management":0.7766990291,
        "MMLU_marketing":0.8760683761,
        "MMLU_medical_genetics":0.66,
        "MMLU_miscellaneous":0.7803320562,
        "MMLU_moral_disputes":0.6849710983,
        "MMLU_moral_scenarios":0.382122905,
        "MMLU_nutrition":0.6045751634,
        "MMLU_philosophy":0.6816720257,
        "MMLU_prehistory":0.6666666667,
        "MMLU_professional_accounting":0.4432624113,
        "MMLU_professional_law":0.4491525424,
        "MMLU_professional_medicine":0.5808823529,
        "MMLU_professional_psychology":0.6307189542,
        "MMLU_public_relations":0.7,
        "MMLU_security_studies":0.6530612245,
        "MMLU_sociology":0.7960199005,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.5060240964,
        "MMLU_world_religions":0.8187134503
    },
    {
        "Model":"Platypus2xOpenOrca-13B-IA3-ensemble",
        "URL":"https:\/\/huggingface.co\/yeontaek\/Platypus2xOpenOrca-13B-IA3-ensemble",
        "full_model_name":"yeontaek\/Platypus2xOpenOrca-13B-IA3-ensemble",
        "Parameters":13.0,
        "MMLU_average":0.5906643064,
        "arc:challenge|25":0.5844709898,
        "hellaswag|10":0.6185022904,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.625,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.6150943396,
        "MMLU_college_biology":0.6805555556,
        "MMLU_college_chemistry":0.46,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.5664739884,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.74,
        "MMLU_conceptual_physics":0.4893617021,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.524137931,
        "MMLU_elementary_mathematics":0.3465608466,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.41,
        "MMLU_high_school_biology":0.7064516129,
        "MMLU_high_school_chemistry":0.4384236453,
        "MMLU_high_school_computer_science":0.61,
        "MMLU_high_school_european_history":0.7333333333,
        "MMLU_high_school_geography":0.7777777778,
        "MMLU_high_school_government_and_politics":0.8186528497,
        "MMLU_high_school_macroeconomics":0.5871794872,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.6386554622,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.776146789,
        "MMLU_high_school_statistics":0.4953703704,
        "MMLU_high_school_us_history":0.8382352941,
        "MMLU_high_school_world_history":0.7679324895,
        "MMLU_human_aging":0.6726457399,
        "MMLU_human_sexuality":0.6946564885,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.8333333333,
        "MMLU_logical_fallacies":0.6809815951,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.7766990291,
        "MMLU_marketing":0.8461538462,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.754789272,
        "MMLU_moral_disputes":0.6473988439,
        "MMLU_moral_scenarios":0.4491620112,
        "MMLU_nutrition":0.6209150327,
        "MMLU_philosophy":0.6430868167,
        "MMLU_prehistory":0.6728395062,
        "MMLU_professional_accounting":0.4716312057,
        "MMLU_professional_law":0.4517601043,
        "MMLU_professional_medicine":0.5661764706,
        "MMLU_professional_psychology":0.6029411765,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6857142857,
        "MMLU_sociology":0.7661691542,
        "MMLU_us_foreign_policy":0.81,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.8011695906
    },
    {
        "Model":"WizardLM-30B-V1.0",
        "URL":"https:\/\/huggingface.co\/LLMs\/WizardLM-30B-V1.0",
        "full_model_name":"LLMs\/WizardLM-30B-V1.0",
        "Parameters":30.0,
        "MMLU_average":0.5905118082,
        "arc:challenge|25":0.6032423208,
        "hellaswag|10":0.6334395539,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.5481481481,
        "MMLU_astronomy":0.6447368421,
        "MMLU_business_ethics":0.63,
        "MMLU_clinical_knowledge":0.6,
        "MMLU_college_biology":0.6666666667,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.5433526012,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4936170213,
        "MMLU_econometrics":0.3947368421,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.3756613757,
        "MMLU_formal_logic":0.373015873,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.6935483871,
        "MMLU_high_school_chemistry":0.4285714286,
        "MMLU_high_school_computer_science":0.62,
        "MMLU_high_school_european_history":0.7272727273,
        "MMLU_high_school_geography":0.7727272727,
        "MMLU_high_school_government_and_politics":0.8445595855,
        "MMLU_high_school_macroeconomics":0.5897435897,
        "MMLU_high_school_mathematics":0.3074074074,
        "MMLU_high_school_microeconomics":0.5714285714,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.7908256881,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.7941176471,
        "MMLU_high_school_world_history":0.805907173,
        "MMLU_human_aging":0.6591928251,
        "MMLU_human_sexuality":0.679389313,
        "MMLU_international_law":0.7603305785,
        "MMLU_jurisprudence":0.7037037037,
        "MMLU_logical_fallacies":0.736196319,
        "MMLU_machine_learning":0.4017857143,
        "MMLU_management":0.7475728155,
        "MMLU_marketing":0.8632478632,
        "MMLU_medical_genetics":0.65,
        "MMLU_miscellaneous":0.7701149425,
        "MMLU_moral_disputes":0.6589595376,
        "MMLU_moral_scenarios":0.3776536313,
        "MMLU_nutrition":0.660130719,
        "MMLU_philosophy":0.6816720257,
        "MMLU_prehistory":0.6944444444,
        "MMLU_professional_accounting":0.4716312057,
        "MMLU_professional_law":0.4452411995,
        "MMLU_professional_medicine":0.6029411765,
        "MMLU_professional_psychology":0.6437908497,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.6693877551,
        "MMLU_sociology":0.7960199005,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4759036145,
        "MMLU_world_religions":0.783625731
    },
    {
        "Model":"WizardLM-30B-fp16",
        "URL":"https:\/\/huggingface.co\/TheBloke\/WizardLM-30B-fp16",
        "full_model_name":"TheBloke\/WizardLM-30B-fp16",
        "Parameters":30.0,
        "MMLU_average":0.5902867714,
        "arc:challenge|25":0.6023890785,
        "hellaswag|10":0.6337382991,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.5481481481,
        "MMLU_astronomy":0.6447368421,
        "MMLU_business_ethics":0.63,
        "MMLU_clinical_knowledge":0.6,
        "MMLU_college_biology":0.6666666667,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.5433526012,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4936170213,
        "MMLU_econometrics":0.3947368421,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.3783068783,
        "MMLU_formal_logic":0.373015873,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.6935483871,
        "MMLU_high_school_chemistry":0.4285714286,
        "MMLU_high_school_computer_science":0.62,
        "MMLU_high_school_european_history":0.7272727273,
        "MMLU_high_school_geography":0.7727272727,
        "MMLU_high_school_government_and_politics":0.8445595855,
        "MMLU_high_school_macroeconomics":0.5897435897,
        "MMLU_high_school_mathematics":0.3074074074,
        "MMLU_high_school_microeconomics":0.5714285714,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.7926605505,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.7941176471,
        "MMLU_high_school_world_history":0.805907173,
        "MMLU_human_aging":0.6591928251,
        "MMLU_human_sexuality":0.6870229008,
        "MMLU_international_law":0.7603305785,
        "MMLU_jurisprudence":0.7037037037,
        "MMLU_logical_fallacies":0.736196319,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.7475728155,
        "MMLU_marketing":0.8632478632,
        "MMLU_medical_genetics":0.65,
        "MMLU_miscellaneous":0.7701149425,
        "MMLU_moral_disputes":0.6589595376,
        "MMLU_moral_scenarios":0.3776536313,
        "MMLU_nutrition":0.6568627451,
        "MMLU_philosophy":0.6816720257,
        "MMLU_prehistory":0.6944444444,
        "MMLU_professional_accounting":0.4716312057,
        "MMLU_professional_law":0.4452411995,
        "MMLU_professional_medicine":0.6029411765,
        "MMLU_professional_psychology":0.6437908497,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.6693877551,
        "MMLU_sociology":0.7960199005,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4759036145,
        "MMLU_world_religions":0.7777777778
    },
    {
        "Model":"webMistral-7B",
        "URL":"https:\/\/huggingface.co\/KnutJaegersberg\/webMistral-7B",
        "full_model_name":"KnutJaegersberg\/webMistral-7B",
        "Parameters":7.0,
        "MMLU_average":0.5899642835,
        "arc:challenge|25":0.5383959044,
        "hellaswag|10":0.6154152559,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.6,
        "MMLU_astronomy":0.6381578947,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.641509434,
        "MMLU_college_biology":0.6666666667,
        "MMLU_college_chemistry":0.46,
        "MMLU_college_computer_science":0.55,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.6184971098,
        "MMLU_college_physics":0.4411764706,
        "MMLU_computer_security":0.73,
        "MMLU_conceptual_physics":0.5106382979,
        "MMLU_econometrics":0.4385964912,
        "MMLU_electrical_engineering":0.5172413793,
        "MMLU_elementary_mathematics":0.3756613757,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.6967741935,
        "MMLU_high_school_chemistry":0.4679802956,
        "MMLU_high_school_computer_science":0.57,
        "MMLU_high_school_european_history":0.696969697,
        "MMLU_high_school_geography":0.7878787879,
        "MMLU_high_school_government_and_politics":0.829015544,
        "MMLU_high_school_macroeconomics":0.5846153846,
        "MMLU_high_school_mathematics":0.3222222222,
        "MMLU_high_school_microeconomics":0.6260504202,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.7596330275,
        "MMLU_high_school_statistics":0.4212962963,
        "MMLU_high_school_us_history":0.7549019608,
        "MMLU_high_school_world_history":0.7046413502,
        "MMLU_human_aging":0.6726457399,
        "MMLU_human_sexuality":0.6946564885,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.6666666667,
        "MMLU_logical_fallacies":0.7300613497,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.8461538462,
        "MMLU_medical_genetics":0.68,
        "MMLU_miscellaneous":0.7828863346,
        "MMLU_moral_disputes":0.6358381503,
        "MMLU_moral_scenarios":0.2737430168,
        "MMLU_nutrition":0.660130719,
        "MMLU_philosophy":0.6430868167,
        "MMLU_prehistory":0.6543209877,
        "MMLU_professional_accounting":0.4574468085,
        "MMLU_professional_law":0.4211212516,
        "MMLU_professional_medicine":0.5845588235,
        "MMLU_professional_psychology":0.5849673203,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6367346939,
        "MMLU_sociology":0.8208955224,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.5240963855,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"scarlett-33b",
        "URL":"https:\/\/huggingface.co\/ajibawa-2023\/scarlett-33b",
        "full_model_name":"ajibawa-2023\/scarlett-33b",
        "Parameters":33.0,
        "MMLU_average":0.5897991268,
        "arc:challenge|25":0.6501706485,
        "hellaswag|10":0.677155945,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.5259259259,
        "MMLU_astronomy":0.6513157895,
        "MMLU_business_ethics":0.58,
        "MMLU_clinical_knowledge":0.6075471698,
        "MMLU_college_biology":0.6319444444,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.49,
        "MMLU_college_mathematics":0.4,
        "MMLU_college_medicine":0.5317919075,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.67,
        "MMLU_conceptual_physics":0.514893617,
        "MMLU_econometrics":0.350877193,
        "MMLU_electrical_engineering":0.5103448276,
        "MMLU_elementary_mathematics":0.3571428571,
        "MMLU_formal_logic":0.3412698413,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.6967741935,
        "MMLU_high_school_chemistry":0.4187192118,
        "MMLU_high_school_computer_science":0.64,
        "MMLU_high_school_european_history":0.7272727273,
        "MMLU_high_school_geography":0.7222222222,
        "MMLU_high_school_government_and_politics":0.8341968912,
        "MMLU_high_school_macroeconomics":0.5923076923,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.5924369748,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.7724770642,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.7745098039,
        "MMLU_high_school_world_history":0.805907173,
        "MMLU_human_aging":0.6412556054,
        "MMLU_human_sexuality":0.6488549618,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.6851851852,
        "MMLU_logical_fallacies":0.6993865031,
        "MMLU_machine_learning":0.4642857143,
        "MMLU_management":0.7669902913,
        "MMLU_marketing":0.8418803419,
        "MMLU_medical_genetics":0.66,
        "MMLU_miscellaneous":0.7803320562,
        "MMLU_moral_disputes":0.676300578,
        "MMLU_moral_scenarios":0.4402234637,
        "MMLU_nutrition":0.6209150327,
        "MMLU_philosophy":0.6720257235,
        "MMLU_prehistory":0.6574074074,
        "MMLU_professional_accounting":0.4432624113,
        "MMLU_professional_law":0.4641460235,
        "MMLU_professional_medicine":0.5955882353,
        "MMLU_professional_psychology":0.6078431373,
        "MMLU_public_relations":0.6727272727,
        "MMLU_security_studies":0.6448979592,
        "MMLU_sociology":0.8059701493,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.5180722892,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"grendel",
        "URL":"https:\/\/huggingface.co\/openaccess-ai-collective\/grendel",
        "full_model_name":"openaccess-ai-collective\/grendel",
        "Parameters":null,
        "MMLU_average":0.5897704395,
        "arc:challenge|25":0.5597269625,
        "hellaswag|10":0.615813583,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.5703703704,
        "MMLU_astronomy":0.6118421053,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.6339622642,
        "MMLU_college_biology":0.6875,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.5,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.6069364162,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.74,
        "MMLU_conceptual_physics":0.5021276596,
        "MMLU_econometrics":0.4035087719,
        "MMLU_electrical_engineering":0.4965517241,
        "MMLU_elementary_mathematics":0.3994708995,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.4,
        "MMLU_high_school_biology":0.6903225806,
        "MMLU_high_school_chemistry":0.4975369458,
        "MMLU_high_school_computer_science":0.62,
        "MMLU_high_school_european_history":0.7454545455,
        "MMLU_high_school_geography":0.7575757576,
        "MMLU_high_school_government_and_politics":0.8238341969,
        "MMLU_high_school_macroeconomics":0.5435897436,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.6176470588,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.7853211009,
        "MMLU_high_school_statistics":0.3888888889,
        "MMLU_high_school_us_history":0.7401960784,
        "MMLU_high_school_world_history":0.7848101266,
        "MMLU_human_aging":0.6816143498,
        "MMLU_human_sexuality":0.7022900763,
        "MMLU_international_law":0.7107438017,
        "MMLU_jurisprudence":0.7407407407,
        "MMLU_logical_fallacies":0.6748466258,
        "MMLU_machine_learning":0.5089285714,
        "MMLU_management":0.7572815534,
        "MMLU_marketing":0.8376068376,
        "MMLU_medical_genetics":0.66,
        "MMLU_miscellaneous":0.7816091954,
        "MMLU_moral_disputes":0.6965317919,
        "MMLU_moral_scenarios":0.2189944134,
        "MMLU_nutrition":0.6666666667,
        "MMLU_philosophy":0.6559485531,
        "MMLU_prehistory":0.6481481481,
        "MMLU_professional_accounting":0.4255319149,
        "MMLU_professional_law":0.4198174707,
        "MMLU_professional_medicine":0.5808823529,
        "MMLU_professional_psychology":0.5996732026,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.6653061224,
        "MMLU_sociology":0.815920398,
        "MMLU_us_foreign_policy":0.81,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.7602339181
    },
    {
        "Model":"openbuddy-falcon-180b-v12-preview0",
        "URL":"https:\/\/huggingface.co\/OpenBuddy\/openbuddy-falcon-180b-v12-preview0",
        "full_model_name":"OpenBuddy\/openbuddy-falcon-180b-v12-preview0",
        "Parameters":180.0,
        "MMLU_average":0.5897129752,
        "arc:challenge|25":0.5989761092,
        "hellaswag|10":0.6565425214,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.6315789474,
        "MMLU_business_ethics":0.65,
        "MMLU_clinical_knowledge":0.6377358491,
        "MMLU_college_biology":0.6875,
        "MMLU_college_chemistry":0.45,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.549132948,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.76,
        "MMLU_conceptual_physics":0.5234042553,
        "MMLU_econometrics":0.350877193,
        "MMLU_electrical_engineering":0.4896551724,
        "MMLU_elementary_mathematics":0.4206349206,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.46,
        "MMLU_high_school_biology":0.7032258065,
        "MMLU_high_school_chemistry":0.4532019704,
        "MMLU_high_school_computer_science":0.63,
        "MMLU_high_school_european_history":0.7212121212,
        "MMLU_high_school_geography":0.7626262626,
        "MMLU_high_school_government_and_politics":0.8497409326,
        "MMLU_high_school_macroeconomics":0.5769230769,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.6344537815,
        "MMLU_high_school_physics":0.3642384106,
        "MMLU_high_school_psychology":0.7871559633,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.7156862745,
        "MMLU_high_school_world_history":0.7341772152,
        "MMLU_human_aging":0.6905829596,
        "MMLU_human_sexuality":0.7099236641,
        "MMLU_international_law":0.7438016529,
        "MMLU_jurisprudence":0.6574074074,
        "MMLU_logical_fallacies":0.6748466258,
        "MMLU_machine_learning":0.4642857143,
        "MMLU_management":0.7475728155,
        "MMLU_marketing":0.8846153846,
        "MMLU_medical_genetics":0.64,
        "MMLU_miscellaneous":0.7586206897,
        "MMLU_moral_disputes":0.6560693642,
        "MMLU_moral_scenarios":0.4279329609,
        "MMLU_nutrition":0.6241830065,
        "MMLU_philosophy":0.6816720257,
        "MMLU_prehistory":0.5987654321,
        "MMLU_professional_accounting":0.4184397163,
        "MMLU_professional_law":0.4341590613,
        "MMLU_professional_medicine":0.5992647059,
        "MMLU_professional_psychology":0.5800653595,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.6448979592,
        "MMLU_sociology":0.8059701493,
        "MMLU_us_foreign_policy":0.77,
        "MMLU_virology":0.5361445783,
        "MMLU_world_religions":0.7777777778
    },
    {
        "Model":"NATE-7b",
        "URL":"https:\/\/huggingface.co\/Delcos\/NATE-7b",
        "full_model_name":"Delcos\/NATE-7b",
        "Parameters":7.0,
        "MMLU_average":0.5891129061,
        "arc:challenge|25":0.5784982935,
        "hellaswag|10":0.6209918343,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.5789473684,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.641509434,
        "MMLU_college_biology":0.6180555556,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.51,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.5606936416,
        "MMLU_college_physics":0.3431372549,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.5063829787,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.5310344828,
        "MMLU_elementary_mathematics":0.3624338624,
        "MMLU_formal_logic":0.4047619048,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.7032258065,
        "MMLU_high_school_chemistry":0.4679802956,
        "MMLU_high_school_computer_science":0.57,
        "MMLU_high_school_european_history":0.703030303,
        "MMLU_high_school_geography":0.7676767677,
        "MMLU_high_school_government_and_politics":0.8549222798,
        "MMLU_high_school_macroeconomics":0.6358974359,
        "MMLU_high_school_mathematics":0.3222222222,
        "MMLU_high_school_microeconomics":0.6008403361,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.7944954128,
        "MMLU_high_school_statistics":0.4259259259,
        "MMLU_high_school_us_history":0.8235294118,
        "MMLU_high_school_world_history":0.7637130802,
        "MMLU_human_aging":0.7040358744,
        "MMLU_human_sexuality":0.641221374,
        "MMLU_international_law":0.7024793388,
        "MMLU_jurisprudence":0.787037037,
        "MMLU_logical_fallacies":0.6871165644,
        "MMLU_machine_learning":0.4107142857,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.8290598291,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7752234994,
        "MMLU_moral_disputes":0.6589595376,
        "MMLU_moral_scenarios":0.4379888268,
        "MMLU_nutrition":0.6535947712,
        "MMLU_philosophy":0.6559485531,
        "MMLU_prehistory":0.6697530864,
        "MMLU_professional_accounting":0.4574468085,
        "MMLU_professional_law":0.44589309,
        "MMLU_professional_medicine":0.5661764706,
        "MMLU_professional_psychology":0.591503268,
        "MMLU_public_relations":0.6727272727,
        "MMLU_security_studies":0.6612244898,
        "MMLU_sociology":0.7810945274,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.5060240964,
        "MMLU_world_religions":0.7719298246
    },
    {
        "Model":"Athena-tmp",
        "URL":"https:\/\/huggingface.co\/IkariDev\/Athena-tmp",
        "full_model_name":"IkariDev\/Athena-tmp",
        "Parameters":null,
        "MMLU_average":0.5886853623,
        "arc:challenge|25":0.5674061433,
        "hellaswag|10":0.6218880701,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.5185185185,
        "MMLU_astronomy":0.6381578947,
        "MMLU_business_ethics":0.62,
        "MMLU_clinical_knowledge":0.6075471698,
        "MMLU_college_biology":0.7083333333,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5433526012,
        "MMLU_college_physics":0.3823529412,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.5021276596,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.5103448276,
        "MMLU_elementary_mathematics":0.3518518519,
        "MMLU_formal_logic":0.4047619048,
        "MMLU_global_facts":0.41,
        "MMLU_high_school_biology":0.6451612903,
        "MMLU_high_school_chemistry":0.4581280788,
        "MMLU_high_school_computer_science":0.62,
        "MMLU_high_school_european_history":0.7636363636,
        "MMLU_high_school_geography":0.7727272727,
        "MMLU_high_school_government_and_politics":0.8497409326,
        "MMLU_high_school_macroeconomics":0.6205128205,
        "MMLU_high_school_mathematics":0.3481481481,
        "MMLU_high_school_microeconomics":0.6134453782,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.7963302752,
        "MMLU_high_school_statistics":0.4212962963,
        "MMLU_high_school_us_history":0.8137254902,
        "MMLU_high_school_world_history":0.7890295359,
        "MMLU_human_aging":0.6547085202,
        "MMLU_human_sexuality":0.6564885496,
        "MMLU_international_law":0.7190082645,
        "MMLU_jurisprudence":0.7592592593,
        "MMLU_logical_fallacies":0.7116564417,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.8205128205,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.779054917,
        "MMLU_moral_disputes":0.6445086705,
        "MMLU_moral_scenarios":0.4469273743,
        "MMLU_nutrition":0.6307189542,
        "MMLU_philosophy":0.691318328,
        "MMLU_prehistory":0.6944444444,
        "MMLU_professional_accounting":0.4503546099,
        "MMLU_professional_law":0.4628422425,
        "MMLU_professional_medicine":0.5698529412,
        "MMLU_professional_psychology":0.5882352941,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.6530612245,
        "MMLU_sociology":0.7213930348,
        "MMLU_us_foreign_policy":0.81,
        "MMLU_virology":0.4638554217,
        "MMLU_world_religions":0.8011695906
    },
    {
        "Model":"Platypus2xOpenOrca-13B-IA3",
        "URL":"https:\/\/huggingface.co\/yeontaek\/Platypus2xOpenOrca-13B-IA3",
        "full_model_name":"yeontaek\/Platypus2xOpenOrca-13B-IA3",
        "Parameters":13.0,
        "MMLU_average":0.5884353751,
        "arc:challenge|25":0.5870307167,
        "hellaswag|10":0.6146186019,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.6118421053,
        "MMLU_business_ethics":0.58,
        "MMLU_clinical_knowledge":0.6188679245,
        "MMLU_college_biology":0.6666666667,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.5722543353,
        "MMLU_college_physics":0.3529411765,
        "MMLU_computer_security":0.72,
        "MMLU_conceptual_physics":0.4978723404,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.5310344828,
        "MMLU_elementary_mathematics":0.3465608466,
        "MMLU_formal_logic":0.3888888889,
        "MMLU_global_facts":0.43,
        "MMLU_high_school_biology":0.7064516129,
        "MMLU_high_school_chemistry":0.4433497537,
        "MMLU_high_school_computer_science":0.6,
        "MMLU_high_school_european_history":0.7272727273,
        "MMLU_high_school_geography":0.7727272727,
        "MMLU_high_school_government_and_politics":0.8186528497,
        "MMLU_high_school_macroeconomics":0.5692307692,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.6302521008,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.7743119266,
        "MMLU_high_school_statistics":0.4907407407,
        "MMLU_high_school_us_history":0.7990196078,
        "MMLU_high_school_world_history":0.7805907173,
        "MMLU_human_aging":0.668161435,
        "MMLU_human_sexuality":0.7022900763,
        "MMLU_international_law":0.7438016529,
        "MMLU_jurisprudence":0.8240740741,
        "MMLU_logical_fallacies":0.6748466258,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.7669902913,
        "MMLU_marketing":0.8418803419,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.7573435504,
        "MMLU_moral_disputes":0.6387283237,
        "MMLU_moral_scenarios":0.4703910615,
        "MMLU_nutrition":0.6241830065,
        "MMLU_philosophy":0.652733119,
        "MMLU_prehistory":0.6697530864,
        "MMLU_professional_accounting":0.4716312057,
        "MMLU_professional_law":0.44589309,
        "MMLU_professional_medicine":0.5698529412,
        "MMLU_professional_psychology":0.5866013072,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6857142857,
        "MMLU_sociology":0.7562189055,
        "MMLU_us_foreign_policy":0.81,
        "MMLU_virology":0.4759036145,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"BrainDerp",
        "URL":"https:\/\/huggingface.co\/Sao10K\/BrainDerp",
        "full_model_name":"Sao10K\/BrainDerp",
        "Parameters":null,
        "MMLU_average":0.5881294347,
        "arc:challenge|25":0.5810580205,
        "hellaswag|10":0.6215893248,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.5855263158,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.6339622642,
        "MMLU_college_biology":0.6388888889,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.51,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.5606936416,
        "MMLU_college_physics":0.362745098,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4978723404,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.5310344828,
        "MMLU_elementary_mathematics":0.3597883598,
        "MMLU_formal_logic":0.3968253968,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.7,
        "MMLU_high_school_chemistry":0.4532019704,
        "MMLU_high_school_computer_science":0.56,
        "MMLU_high_school_european_history":0.6909090909,
        "MMLU_high_school_geography":0.7525252525,
        "MMLU_high_school_government_and_politics":0.8497409326,
        "MMLU_high_school_macroeconomics":0.6333333333,
        "MMLU_high_school_mathematics":0.3296296296,
        "MMLU_high_school_microeconomics":0.5966386555,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.7871559633,
        "MMLU_high_school_statistics":0.412037037,
        "MMLU_high_school_us_history":0.8284313725,
        "MMLU_high_school_world_history":0.7679324895,
        "MMLU_human_aging":0.7040358744,
        "MMLU_human_sexuality":0.6335877863,
        "MMLU_international_law":0.694214876,
        "MMLU_jurisprudence":0.7962962963,
        "MMLU_logical_fallacies":0.6687116564,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.8205128205,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7803320562,
        "MMLU_moral_disputes":0.6502890173,
        "MMLU_moral_scenarios":0.4346368715,
        "MMLU_nutrition":0.6666666667,
        "MMLU_philosophy":0.6495176849,
        "MMLU_prehistory":0.6697530864,
        "MMLU_professional_accounting":0.4680851064,
        "MMLU_professional_law":0.4465449804,
        "MMLU_professional_medicine":0.5661764706,
        "MMLU_professional_psychology":0.5849673203,
        "MMLU_public_relations":0.6909090909,
        "MMLU_security_studies":0.6693877551,
        "MMLU_sociology":0.7910447761,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.5120481928,
        "MMLU_world_religions":0.7894736842
    },
    {
        "Model":"samantha-1.1-llama-33b",
        "URL":"https:\/\/huggingface.co\/ehartford\/samantha-1.1-llama-33b",
        "full_model_name":"ehartford\/samantha-1.1-llama-33b",
        "Parameters":33.0,
        "MMLU_average":0.5879280918,
        "arc:challenge|25":0.6501706485,
        "hellaswag|10":0.676359291,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.5333333333,
        "MMLU_astronomy":0.6315789474,
        "MMLU_business_ethics":0.58,
        "MMLU_clinical_knowledge":0.6037735849,
        "MMLU_college_biology":0.6111111111,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.5549132948,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.514893617,
        "MMLU_econometrics":0.3421052632,
        "MMLU_electrical_engineering":0.524137931,
        "MMLU_elementary_mathematics":0.3518518519,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.7,
        "MMLU_high_school_chemistry":0.4384236453,
        "MMLU_high_school_computer_science":0.64,
        "MMLU_high_school_european_history":0.7272727273,
        "MMLU_high_school_geography":0.7121212121,
        "MMLU_high_school_government_and_politics":0.8238341969,
        "MMLU_high_school_macroeconomics":0.5769230769,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.5672268908,
        "MMLU_high_school_physics":0.3642384106,
        "MMLU_high_school_psychology":0.7688073394,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.7647058824,
        "MMLU_high_school_world_history":0.8270042194,
        "MMLU_human_aging":0.66367713,
        "MMLU_human_sexuality":0.6488549618,
        "MMLU_international_law":0.7190082645,
        "MMLU_jurisprudence":0.7037037037,
        "MMLU_logical_fallacies":0.717791411,
        "MMLU_machine_learning":0.4553571429,
        "MMLU_management":0.7766990291,
        "MMLU_marketing":0.8504273504,
        "MMLU_medical_genetics":0.64,
        "MMLU_miscellaneous":0.7816091954,
        "MMLU_moral_disputes":0.6647398844,
        "MMLU_moral_scenarios":0.4480446927,
        "MMLU_nutrition":0.6274509804,
        "MMLU_philosophy":0.6623794212,
        "MMLU_prehistory":0.6635802469,
        "MMLU_professional_accounting":0.4609929078,
        "MMLU_professional_law":0.4569752282,
        "MMLU_professional_medicine":0.5919117647,
        "MMLU_professional_psychology":0.612745098,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.6285714286,
        "MMLU_sociology":0.8208955224,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.5240963855,
        "MMLU_world_religions":0.7719298246
    },
    {
        "Model":"Platypus2xOpenOrca-13B-LoRa",
        "URL":"https:\/\/huggingface.co\/yeontaek\/Platypus2xOpenOrca-13B-LoRa",
        "full_model_name":"yeontaek\/Platypus2xOpenOrca-13B-LoRa",
        "Parameters":13.0,
        "MMLU_average":0.5876648673,
        "arc:challenge|25":0.5597269625,
        "hellaswag|10":0.6144194384,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5921052632,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.6075471698,
        "MMLU_college_biology":0.7152777778,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.5433526012,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.5063829787,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.5379310345,
        "MMLU_elementary_mathematics":0.3492063492,
        "MMLU_formal_logic":0.3968253968,
        "MMLU_global_facts":0.4,
        "MMLU_high_school_biology":0.7064516129,
        "MMLU_high_school_chemistry":0.4532019704,
        "MMLU_high_school_computer_science":0.62,
        "MMLU_high_school_european_history":0.7454545455,
        "MMLU_high_school_geography":0.7424242424,
        "MMLU_high_school_government_and_politics":0.8238341969,
        "MMLU_high_school_macroeconomics":0.6025641026,
        "MMLU_high_school_mathematics":0.337037037,
        "MMLU_high_school_microeconomics":0.6470588235,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.7871559633,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.8431372549,
        "MMLU_high_school_world_history":0.7890295359,
        "MMLU_human_aging":0.6367713004,
        "MMLU_human_sexuality":0.6564885496,
        "MMLU_international_law":0.6611570248,
        "MMLU_jurisprudence":0.7685185185,
        "MMLU_logical_fallacies":0.7116564417,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.7475728155,
        "MMLU_marketing":0.8333333333,
        "MMLU_medical_genetics":0.63,
        "MMLU_miscellaneous":0.7662835249,
        "MMLU_moral_disputes":0.6329479769,
        "MMLU_moral_scenarios":0.4402234637,
        "MMLU_nutrition":0.5849673203,
        "MMLU_philosophy":0.6655948553,
        "MMLU_prehistory":0.6975308642,
        "MMLU_professional_accounting":0.4822695035,
        "MMLU_professional_law":0.4687092568,
        "MMLU_professional_medicine":0.5661764706,
        "MMLU_professional_psychology":0.6094771242,
        "MMLU_public_relations":0.7090909091,
        "MMLU_security_studies":0.6530612245,
        "MMLU_sociology":0.7711442786,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4879518072,
        "MMLU_world_religions":0.7660818713
    },
    {
        "Model":"MLewd-L2-Chat-13B",
        "URL":"https:\/\/huggingface.co\/Undi95\/MLewd-L2-Chat-13B",
        "full_model_name":"Undi95\/MLewd-L2-Chat-13B",
        "Parameters":13.0,
        "MMLU_average":0.5875102312,
        "arc:challenge|25":0.5998293515,
        "hellaswag|10":0.6452897829,
        "MMLU_abstract_algebra":0.41,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.5789473684,
        "MMLU_business_ethics":0.6,
        "MMLU_clinical_knowledge":0.6339622642,
        "MMLU_college_biology":0.6666666667,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5549132948,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.72,
        "MMLU_conceptual_physics":0.4936170213,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.5172413793,
        "MMLU_elementary_mathematics":0.3492063492,
        "MMLU_formal_logic":0.4523809524,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.6935483871,
        "MMLU_high_school_chemistry":0.4581280788,
        "MMLU_high_school_computer_science":0.6,
        "MMLU_high_school_european_history":0.7212121212,
        "MMLU_high_school_geography":0.7474747475,
        "MMLU_high_school_government_and_politics":0.8549222798,
        "MMLU_high_school_macroeconomics":0.5794871795,
        "MMLU_high_school_mathematics":0.3407407407,
        "MMLU_high_school_microeconomics":0.6008403361,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.7743119266,
        "MMLU_high_school_statistics":0.4166666667,
        "MMLU_high_school_us_history":0.7990196078,
        "MMLU_high_school_world_history":0.7637130802,
        "MMLU_human_aging":0.6905829596,
        "MMLU_human_sexuality":0.6717557252,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.7962962963,
        "MMLU_logical_fallacies":0.6871165644,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.8247863248,
        "MMLU_medical_genetics":0.67,
        "MMLU_miscellaneous":0.779054917,
        "MMLU_moral_disputes":0.6416184971,
        "MMLU_moral_scenarios":0.5027932961,
        "MMLU_nutrition":0.6470588235,
        "MMLU_philosophy":0.6366559486,
        "MMLU_prehistory":0.6604938272,
        "MMLU_professional_accounting":0.4503546099,
        "MMLU_professional_law":0.4374185137,
        "MMLU_professional_medicine":0.5698529412,
        "MMLU_professional_psychology":0.591503268,
        "MMLU_public_relations":0.6272727273,
        "MMLU_security_studies":0.6653061224,
        "MMLU_sociology":0.7661691542,
        "MMLU_us_foreign_policy":0.86,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"Luban-Marcoroni-13B-v2",
        "URL":"https:\/\/huggingface.co\/Weyaxi\/Luban-Marcoroni-13B-v2",
        "full_model_name":"Weyaxi\/Luban-Marcoroni-13B-v2",
        "Parameters":13.0,
        "MMLU_average":0.5872480467,
        "arc:challenge|25":0.611774744,
        "hellaswag|10":0.6271659032,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.5333333333,
        "MMLU_astronomy":0.625,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.6150943396,
        "MMLU_college_biology":0.6527777778,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.5838150289,
        "MMLU_college_physics":0.3529411765,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.514893617,
        "MMLU_econometrics":0.3333333333,
        "MMLU_electrical_engineering":0.5793103448,
        "MMLU_elementary_mathematics":0.3650793651,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.7,
        "MMLU_high_school_chemistry":0.4482758621,
        "MMLU_high_school_computer_science":0.6,
        "MMLU_high_school_european_history":0.7272727273,
        "MMLU_high_school_geography":0.7575757576,
        "MMLU_high_school_government_and_politics":0.829015544,
        "MMLU_high_school_macroeconomics":0.5666666667,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.5840336134,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.7596330275,
        "MMLU_high_school_statistics":0.3935185185,
        "MMLU_high_school_us_history":0.8039215686,
        "MMLU_high_school_world_history":0.7637130802,
        "MMLU_human_aging":0.6860986547,
        "MMLU_human_sexuality":0.679389313,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.7685185185,
        "MMLU_logical_fallacies":0.6871165644,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.8461538462,
        "MMLU_medical_genetics":0.62,
        "MMLU_miscellaneous":0.779054917,
        "MMLU_moral_disputes":0.6242774566,
        "MMLU_moral_scenarios":0.4212290503,
        "MMLU_nutrition":0.6535947712,
        "MMLU_philosophy":0.6559485531,
        "MMLU_prehistory":0.6666666667,
        "MMLU_professional_accounting":0.4574468085,
        "MMLU_professional_law":0.4250325945,
        "MMLU_professional_medicine":0.5625,
        "MMLU_professional_psychology":0.5669934641,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.7020408163,
        "MMLU_sociology":0.7810945274,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4819277108,
        "MMLU_world_religions":0.8070175439
    },
    {
        "Model":"ChatAYT-Lora-Assamble-Marcoroni",
        "URL":"https:\/\/huggingface.co\/Weyaxi\/ChatAYT-Lora-Assamble-Marcoroni",
        "full_model_name":"Weyaxi\/ChatAYT-Lora-Assamble-Marcoroni",
        "Parameters":null,
        "MMLU_average":0.5871954137,
        "arc:challenge|25":0.6092150171,
        "hellaswag|10":0.6289583748,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.5259259259,
        "MMLU_astronomy":0.6118421053,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.6150943396,
        "MMLU_college_biology":0.6388888889,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.5,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5953757225,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.5276595745,
        "MMLU_econometrics":0.350877193,
        "MMLU_electrical_engineering":0.5724137931,
        "MMLU_elementary_mathematics":0.3571428571,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.7,
        "MMLU_high_school_chemistry":0.4729064039,
        "MMLU_high_school_computer_science":0.6,
        "MMLU_high_school_european_history":0.7212121212,
        "MMLU_high_school_geography":0.7575757576,
        "MMLU_high_school_government_and_politics":0.829015544,
        "MMLU_high_school_macroeconomics":0.5820512821,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.5840336134,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.7633027523,
        "MMLU_high_school_statistics":0.4027777778,
        "MMLU_high_school_us_history":0.8039215686,
        "MMLU_high_school_world_history":0.7679324895,
        "MMLU_human_aging":0.6816143498,
        "MMLU_human_sexuality":0.6946564885,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.787037037,
        "MMLU_logical_fallacies":0.6932515337,
        "MMLU_machine_learning":0.4196428571,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.8376068376,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7726692209,
        "MMLU_moral_disputes":0.6300578035,
        "MMLU_moral_scenarios":0.425698324,
        "MMLU_nutrition":0.637254902,
        "MMLU_philosophy":0.6559485531,
        "MMLU_prehistory":0.6666666667,
        "MMLU_professional_accounting":0.4645390071,
        "MMLU_professional_law":0.424380704,
        "MMLU_professional_medicine":0.5735294118,
        "MMLU_professional_psychology":0.5522875817,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.6979591837,
        "MMLU_sociology":0.7810945274,
        "MMLU_us_foreign_policy":0.79,
        "MMLU_virology":0.4759036145,
        "MMLU_world_religions":0.7660818713
    },
    {
        "Model":"Luban-Marcoroni-13B",
        "URL":"https:\/\/huggingface.co\/Weyaxi\/Luban-Marcoroni-13B",
        "full_model_name":"Weyaxi\/Luban-Marcoroni-13B",
        "Parameters":13.0,
        "MMLU_average":0.5870103494,
        "arc:challenge|25":0.6109215017,
        "hellaswag|10":0.6269667397,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.5333333333,
        "MMLU_astronomy":0.625,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.6150943396,
        "MMLU_college_biology":0.6527777778,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.5838150289,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.514893617,
        "MMLU_econometrics":0.3421052632,
        "MMLU_electrical_engineering":0.5793103448,
        "MMLU_elementary_mathematics":0.3650793651,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.7032258065,
        "MMLU_high_school_chemistry":0.4482758621,
        "MMLU_high_school_computer_science":0.6,
        "MMLU_high_school_european_history":0.7272727273,
        "MMLU_high_school_geography":0.7575757576,
        "MMLU_high_school_government_and_politics":0.829015544,
        "MMLU_high_school_macroeconomics":0.5666666667,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.5798319328,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.7577981651,
        "MMLU_high_school_statistics":0.3935185185,
        "MMLU_high_school_us_history":0.8039215686,
        "MMLU_high_school_world_history":0.7594936709,
        "MMLU_human_aging":0.6860986547,
        "MMLU_human_sexuality":0.679389313,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.7685185185,
        "MMLU_logical_fallacies":0.6871165644,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.8461538462,
        "MMLU_medical_genetics":0.62,
        "MMLU_miscellaneous":0.779054917,
        "MMLU_moral_disputes":0.6242774566,
        "MMLU_moral_scenarios":0.4212290503,
        "MMLU_nutrition":0.6535947712,
        "MMLU_philosophy":0.6559485531,
        "MMLU_prehistory":0.6666666667,
        "MMLU_professional_accounting":0.4503546099,
        "MMLU_professional_law":0.4263363755,
        "MMLU_professional_medicine":0.5625,
        "MMLU_professional_psychology":0.5669934641,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.706122449,
        "MMLU_sociology":0.7810945274,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4879518072,
        "MMLU_world_religions":0.8070175439
    },
    {
        "Model":"Chat-AYB-Nova-13B",
        "URL":"https:\/\/huggingface.co\/PulsarAI\/Chat-AYB-Nova-13B",
        "full_model_name":"PulsarAI\/Chat-AYB-Nova-13B",
        "Parameters":13.0,
        "MMLU_average":0.585815044,
        "arc:challenge|25":0.5836177474,
        "hellaswag|10":0.6440948018,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.5789473684,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.6150943396,
        "MMLU_college_biology":0.6527777778,
        "MMLU_college_chemistry":0.5,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.5606936416,
        "MMLU_college_physics":0.4215686275,
        "MMLU_computer_security":0.67,
        "MMLU_conceptual_physics":0.4936170213,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.3333333333,
        "MMLU_formal_logic":0.4523809524,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.6774193548,
        "MMLU_high_school_chemistry":0.4729064039,
        "MMLU_high_school_computer_science":0.6,
        "MMLU_high_school_european_history":0.7151515152,
        "MMLU_high_school_geography":0.7525252525,
        "MMLU_high_school_government_and_politics":0.829015544,
        "MMLU_high_school_macroeconomics":0.6076923077,
        "MMLU_high_school_mathematics":0.3111111111,
        "MMLU_high_school_microeconomics":0.6386554622,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.7926605505,
        "MMLU_high_school_statistics":0.5462962963,
        "MMLU_high_school_us_history":0.8039215686,
        "MMLU_high_school_world_history":0.776371308,
        "MMLU_human_aging":0.6816143498,
        "MMLU_human_sexuality":0.6488549618,
        "MMLU_international_law":0.694214876,
        "MMLU_jurisprudence":0.7777777778,
        "MMLU_logical_fallacies":0.7239263804,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.7766990291,
        "MMLU_marketing":0.8247863248,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7624521073,
        "MMLU_moral_disputes":0.6416184971,
        "MMLU_moral_scenarios":0.382122905,
        "MMLU_nutrition":0.614379085,
        "MMLU_philosophy":0.6655948553,
        "MMLU_prehistory":0.6820987654,
        "MMLU_professional_accounting":0.475177305,
        "MMLU_professional_law":0.4341590613,
        "MMLU_professional_medicine":0.5772058824,
        "MMLU_professional_psychology":0.5735294118,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.6244897959,
        "MMLU_sociology":0.7512437811,
        "MMLU_us_foreign_policy":0.85,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.7660818713
    },
    {
        "Model":"OpenOrca-Platypus2-13B-GPTQ",
        "URL":"https:\/\/huggingface.co\/TheBloke\/OpenOrca-Platypus2-13B-GPTQ",
        "full_model_name":"TheBloke\/OpenOrca-Platypus2-13B-GPTQ",
        "Parameters":13.0,
        "MMLU_average":0.5855659082,
        "arc:challenge|25":0.5759385666,
        "hellaswag|10":0.6208922525,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.6052631579,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.641509434,
        "MMLU_college_biology":0.6666666667,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.5606936416,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.4893617021,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.4827586207,
        "MMLU_elementary_mathematics":0.3597883598,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.46,
        "MMLU_high_school_biology":0.6774193548,
        "MMLU_high_school_chemistry":0.4334975369,
        "MMLU_high_school_computer_science":0.64,
        "MMLU_high_school_european_history":0.7575757576,
        "MMLU_high_school_geography":0.7575757576,
        "MMLU_high_school_government_and_politics":0.8445595855,
        "MMLU_high_school_macroeconomics":0.6205128205,
        "MMLU_high_school_mathematics":0.3481481481,
        "MMLU_high_school_microeconomics":0.6050420168,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.8110091743,
        "MMLU_high_school_statistics":0.4398148148,
        "MMLU_high_school_us_history":0.8284313725,
        "MMLU_high_school_world_history":0.7721518987,
        "MMLU_human_aging":0.6367713004,
        "MMLU_human_sexuality":0.6717557252,
        "MMLU_international_law":0.694214876,
        "MMLU_jurisprudence":0.75,
        "MMLU_logical_fallacies":0.7055214724,
        "MMLU_machine_learning":0.4107142857,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.8333333333,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.7828863346,
        "MMLU_moral_disputes":0.6445086705,
        "MMLU_moral_scenarios":0.4793296089,
        "MMLU_nutrition":0.637254902,
        "MMLU_philosophy":0.6945337621,
        "MMLU_prehistory":0.7037037037,
        "MMLU_professional_accounting":0.4539007092,
        "MMLU_professional_law":0.4491525424,
        "MMLU_professional_medicine":0.5514705882,
        "MMLU_professional_psychology":0.5767973856,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6408163265,
        "MMLU_sociology":0.7213930348,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4939759036,
        "MMLU_world_religions":0.7777777778
    },
    {
        "Model":"internlm-20b-chat",
        "URL":"https:\/\/huggingface.co\/internlm\/internlm-20b-chat",
        "full_model_name":"internlm\/internlm-20b-chat",
        "Parameters":20.0,
        "MMLU_average":0.5853445677,
        "arc:challenge|25":0.5051194539,
        "hellaswag|10":0.582254531,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.5407407407,
        "MMLU_astronomy":0.5789473684,
        "MMLU_business_ethics":0.63,
        "MMLU_clinical_knowledge":0.6,
        "MMLU_college_biology":0.625,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.51,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.5260115607,
        "MMLU_college_physics":0.4215686275,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.4510638298,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.5586206897,
        "MMLU_elementary_mathematics":0.3783068783,
        "MMLU_formal_logic":0.3888888889,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.6612903226,
        "MMLU_high_school_chemistry":0.4334975369,
        "MMLU_high_school_computer_science":0.66,
        "MMLU_high_school_european_history":0.7636363636,
        "MMLU_high_school_geography":0.7525252525,
        "MMLU_high_school_government_and_politics":0.8549222798,
        "MMLU_high_school_macroeconomics":0.5692307692,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.5630252101,
        "MMLU_high_school_physics":0.3642384106,
        "MMLU_high_school_psychology":0.7908256881,
        "MMLU_high_school_statistics":0.4537037037,
        "MMLU_high_school_us_history":0.7598039216,
        "MMLU_high_school_world_history":0.7890295359,
        "MMLU_human_aging":0.6547085202,
        "MMLU_human_sexuality":0.6641221374,
        "MMLU_international_law":0.7438016529,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.6871165644,
        "MMLU_machine_learning":0.4107142857,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.8504273504,
        "MMLU_medical_genetics":0.61,
        "MMLU_miscellaneous":0.7560664112,
        "MMLU_moral_disputes":0.6473988439,
        "MMLU_moral_scenarios":0.3988826816,
        "MMLU_nutrition":0.6437908497,
        "MMLU_philosophy":0.6334405145,
        "MMLU_prehistory":0.6172839506,
        "MMLU_professional_accounting":0.4290780142,
        "MMLU_professional_law":0.4374185137,
        "MMLU_professional_medicine":0.5551470588,
        "MMLU_professional_psychology":0.5964052288,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.6897959184,
        "MMLU_sociology":0.7810945274,
        "MMLU_us_foreign_policy":0.79,
        "MMLU_virology":0.4879518072,
        "MMLU_world_religions":0.7719298246
    },
    {
        "Model":"speechless-llama2-13b",
        "URL":"https:\/\/huggingface.co\/uukuguy\/speechless-llama2-13b",
        "full_model_name":"uukuguy\/speechless-llama2-13b",
        "Parameters":13.0,
        "MMLU_average":0.5851806228,
        "arc:challenge|25":0.5784982935,
        "hellaswag|10":0.6214897431,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.5481481481,
        "MMLU_astronomy":0.5657894737,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.6301886792,
        "MMLU_college_biology":0.625,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.5,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.549132948,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.4936170213,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.5448275862,
        "MMLU_elementary_mathematics":0.3650793651,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.6870967742,
        "MMLU_high_school_chemistry":0.4926108374,
        "MMLU_high_school_computer_science":0.63,
        "MMLU_high_school_european_history":0.703030303,
        "MMLU_high_school_geography":0.7626262626,
        "MMLU_high_school_government_and_politics":0.8445595855,
        "MMLU_high_school_macroeconomics":0.6256410256,
        "MMLU_high_school_mathematics":0.3111111111,
        "MMLU_high_school_microeconomics":0.6008403361,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.7908256881,
        "MMLU_high_school_statistics":0.4027777778,
        "MMLU_high_school_us_history":0.7990196078,
        "MMLU_high_school_world_history":0.776371308,
        "MMLU_human_aging":0.6905829596,
        "MMLU_human_sexuality":0.641221374,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.7685185185,
        "MMLU_logical_fallacies":0.6809815951,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.7572815534,
        "MMLU_marketing":0.8376068376,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.7637292465,
        "MMLU_moral_disputes":0.6473988439,
        "MMLU_moral_scenarios":0.4156424581,
        "MMLU_nutrition":0.6568627451,
        "MMLU_philosophy":0.652733119,
        "MMLU_prehistory":0.6450617284,
        "MMLU_professional_accounting":0.4539007092,
        "MMLU_professional_law":0.4367666232,
        "MMLU_professional_medicine":0.5808823529,
        "MMLU_professional_psychology":0.5669934641,
        "MMLU_public_relations":0.7,
        "MMLU_security_studies":0.6775510204,
        "MMLU_sociology":0.7562189055,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4879518072,
        "MMLU_world_religions":0.7777777778
    },
    {
        "Model":"llama-30b",
        "URL":"https:\/\/huggingface.co\/huggingface\/llama-30b",
        "full_model_name":"huggingface\/llama-30b",
        "Parameters":30.0,
        "MMLU_average":0.5846957725,
        "arc:challenge|25":0.5810580205,
        "hellaswag|10":0.6447918741,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.6118421053,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.5773584906,
        "MMLU_college_biology":0.5833333333,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.5433526012,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.514893617,
        "MMLU_econometrics":0.3596491228,
        "MMLU_electrical_engineering":0.5310344828,
        "MMLU_elementary_mathematics":0.373015873,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.6838709677,
        "MMLU_high_school_chemistry":0.4088669951,
        "MMLU_high_school_computer_science":0.6,
        "MMLU_high_school_european_history":0.7333333333,
        "MMLU_high_school_geography":0.7121212121,
        "MMLU_high_school_government_and_politics":0.8341968912,
        "MMLU_high_school_macroeconomics":0.5717948718,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.5756302521,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7633027523,
        "MMLU_high_school_statistics":0.4861111111,
        "MMLU_high_school_us_history":0.7843137255,
        "MMLU_high_school_world_history":0.7974683544,
        "MMLU_human_aging":0.6771300448,
        "MMLU_human_sexuality":0.6564885496,
        "MMLU_international_law":0.7520661157,
        "MMLU_jurisprudence":0.712962963,
        "MMLU_logical_fallacies":0.6809815951,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.7766990291,
        "MMLU_marketing":0.8461538462,
        "MMLU_medical_genetics":0.66,
        "MMLU_miscellaneous":0.7816091954,
        "MMLU_moral_disputes":0.6705202312,
        "MMLU_moral_scenarios":0.3899441341,
        "MMLU_nutrition":0.6241830065,
        "MMLU_philosophy":0.6655948553,
        "MMLU_prehistory":0.6697530864,
        "MMLU_professional_accounting":0.4361702128,
        "MMLU_professional_law":0.4654498044,
        "MMLU_professional_medicine":0.5551470588,
        "MMLU_professional_psychology":0.6405228758,
        "MMLU_public_relations":0.7,
        "MMLU_security_studies":0.6693877551,
        "MMLU_sociology":0.7910447761,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4939759036,
        "MMLU_world_religions":0.8011695906
    },
    {
        "Model":"llama-30B-hf-openassitant",
        "URL":"https:\/\/huggingface.co\/Yhyu13\/llama-30B-hf-openassitant",
        "full_model_name":"Yhyu13\/llama-30B-hf-openassitant",
        "Parameters":30.0,
        "MMLU_average":0.5846957725,
        "arc:challenge|25":0.5810580205,
        "hellaswag|10":0.6447918741,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.6118421053,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.5773584906,
        "MMLU_college_biology":0.5833333333,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.5433526012,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.514893617,
        "MMLU_econometrics":0.3596491228,
        "MMLU_electrical_engineering":0.5310344828,
        "MMLU_elementary_mathematics":0.373015873,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.6838709677,
        "MMLU_high_school_chemistry":0.4088669951,
        "MMLU_high_school_computer_science":0.6,
        "MMLU_high_school_european_history":0.7333333333,
        "MMLU_high_school_geography":0.7121212121,
        "MMLU_high_school_government_and_politics":0.8341968912,
        "MMLU_high_school_macroeconomics":0.5717948718,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.5756302521,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7633027523,
        "MMLU_high_school_statistics":0.4861111111,
        "MMLU_high_school_us_history":0.7843137255,
        "MMLU_high_school_world_history":0.7974683544,
        "MMLU_human_aging":0.6771300448,
        "MMLU_human_sexuality":0.6564885496,
        "MMLU_international_law":0.7520661157,
        "MMLU_jurisprudence":0.712962963,
        "MMLU_logical_fallacies":0.6809815951,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.7766990291,
        "MMLU_marketing":0.8461538462,
        "MMLU_medical_genetics":0.66,
        "MMLU_miscellaneous":0.7816091954,
        "MMLU_moral_disputes":0.6705202312,
        "MMLU_moral_scenarios":0.3899441341,
        "MMLU_nutrition":0.6241830065,
        "MMLU_philosophy":0.6655948553,
        "MMLU_prehistory":0.6697530864,
        "MMLU_professional_accounting":0.4361702128,
        "MMLU_professional_law":0.4654498044,
        "MMLU_professional_medicine":0.5551470588,
        "MMLU_professional_psychology":0.6405228758,
        "MMLU_public_relations":0.7,
        "MMLU_security_studies":0.6693877551,
        "MMLU_sociology":0.7910447761,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4939759036,
        "MMLU_world_religions":0.8011695906
    },
    {
        "Model":"Chronorctypus-Limarobormes-13b",
        "URL":"https:\/\/huggingface.co\/chargoddard\/Chronorctypus-Limarobormes-13b",
        "full_model_name":"chargoddard\/Chronorctypus-Limarobormes-13b",
        "Parameters":13.0,
        "MMLU_average":0.5844813681,
        "arc:challenge|25":0.5708191126,
        "hellaswag|10":0.6296554471,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.6315789474,
        "MMLU_business_ethics":0.58,
        "MMLU_clinical_knowledge":0.6377358491,
        "MMLU_college_biology":0.5972222222,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.55,
        "MMLU_college_mathematics":0.39,
        "MMLU_college_medicine":0.5260115607,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.514893617,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.4827586207,
        "MMLU_elementary_mathematics":0.328042328,
        "MMLU_formal_logic":0.4206349206,
        "MMLU_global_facts":0.4,
        "MMLU_high_school_biology":0.6322580645,
        "MMLU_high_school_chemistry":0.4433497537,
        "MMLU_high_school_computer_science":0.53,
        "MMLU_high_school_european_history":0.7151515152,
        "MMLU_high_school_geography":0.7070707071,
        "MMLU_high_school_government_and_politics":0.8393782383,
        "MMLU_high_school_macroeconomics":0.5846153846,
        "MMLU_high_school_mathematics":0.3555555556,
        "MMLU_high_school_microeconomics":0.5840336134,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.7834862385,
        "MMLU_high_school_statistics":0.412037037,
        "MMLU_high_school_us_history":0.8088235294,
        "MMLU_high_school_world_history":0.7848101266,
        "MMLU_human_aging":0.7130044843,
        "MMLU_human_sexuality":0.5954198473,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.7777777778,
        "MMLU_logical_fallacies":0.6809815951,
        "MMLU_machine_learning":0.4285714286,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.8162393162,
        "MMLU_medical_genetics":0.62,
        "MMLU_miscellaneous":0.7905491699,
        "MMLU_moral_disputes":0.6676300578,
        "MMLU_moral_scenarios":0.5016759777,
        "MMLU_nutrition":0.6176470588,
        "MMLU_philosophy":0.6495176849,
        "MMLU_prehistory":0.6697530864,
        "MMLU_professional_accounting":0.4468085106,
        "MMLU_professional_law":0.4706649283,
        "MMLU_professional_medicine":0.5845588235,
        "MMLU_professional_psychology":0.5947712418,
        "MMLU_public_relations":0.6909090909,
        "MMLU_security_studies":0.6326530612,
        "MMLU_sociology":0.7263681592,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.5120481928,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"llama-30b",
        "URL":"https:\/\/huggingface.co\/huggyllama\/llama-30b",
        "full_model_name":"huggyllama\/llama-30b",
        "Parameters":30.0,
        "MMLU_average":0.5844661284,
        "arc:challenge|25":0.5810580205,
        "hellaswag|10":0.6447918741,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.6118421053,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.5773584906,
        "MMLU_college_biology":0.5833333333,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.5433526012,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.514893617,
        "MMLU_econometrics":0.3596491228,
        "MMLU_electrical_engineering":0.524137931,
        "MMLU_elementary_mathematics":0.373015873,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.6838709677,
        "MMLU_high_school_chemistry":0.4088669951,
        "MMLU_high_school_computer_science":0.6,
        "MMLU_high_school_european_history":0.7333333333,
        "MMLU_high_school_geography":0.7121212121,
        "MMLU_high_school_government_and_politics":0.8341968912,
        "MMLU_high_school_macroeconomics":0.5641025641,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.5756302521,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7633027523,
        "MMLU_high_school_statistics":0.4861111111,
        "MMLU_high_school_us_history":0.7843137255,
        "MMLU_high_school_world_history":0.7974683544,
        "MMLU_human_aging":0.6771300448,
        "MMLU_human_sexuality":0.6564885496,
        "MMLU_international_law":0.7603305785,
        "MMLU_jurisprudence":0.712962963,
        "MMLU_logical_fallacies":0.6748466258,
        "MMLU_machine_learning":0.4017857143,
        "MMLU_management":0.7766990291,
        "MMLU_marketing":0.8461538462,
        "MMLU_medical_genetics":0.66,
        "MMLU_miscellaneous":0.7816091954,
        "MMLU_moral_disputes":0.6705202312,
        "MMLU_moral_scenarios":0.3865921788,
        "MMLU_nutrition":0.6241830065,
        "MMLU_philosophy":0.6655948553,
        "MMLU_prehistory":0.6666666667,
        "MMLU_professional_accounting":0.4361702128,
        "MMLU_professional_law":0.464797914,
        "MMLU_professional_medicine":0.5588235294,
        "MMLU_professional_psychology":0.6405228758,
        "MMLU_public_relations":0.7,
        "MMLU_security_studies":0.6653061224,
        "MMLU_sociology":0.7910447761,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4939759036,
        "MMLU_world_religions":0.8011695906
    },
    {
        "Model":"speechless-orca-platypus-coig-lite-2k-0.6e-13b",
        "URL":"https:\/\/huggingface.co\/uukuguy\/speechless-orca-platypus-coig-lite-2k-0.6e-13b",
        "full_model_name":"uukuguy\/speechless-orca-platypus-coig-lite-2k-0.6e-13b",
        "Parameters":13.0,
        "MMLU_average":0.5834241147,
        "arc:challenge|25":0.5546075085,
        "hellaswag|10":0.605158335,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.6381578947,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.6264150943,
        "MMLU_college_biology":0.6458333333,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.5,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5317919075,
        "MMLU_college_physics":0.3431372549,
        "MMLU_computer_security":0.72,
        "MMLU_conceptual_physics":0.4638297872,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.5517241379,
        "MMLU_elementary_mathematics":0.3597883598,
        "MMLU_formal_logic":0.4047619048,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.6580645161,
        "MMLU_high_school_chemistry":0.4581280788,
        "MMLU_high_school_computer_science":0.63,
        "MMLU_high_school_european_history":0.7393939394,
        "MMLU_high_school_geography":0.7575757576,
        "MMLU_high_school_government_and_politics":0.829015544,
        "MMLU_high_school_macroeconomics":0.6102564103,
        "MMLU_high_school_mathematics":0.3148148148,
        "MMLU_high_school_microeconomics":0.5882352941,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.7889908257,
        "MMLU_high_school_statistics":0.4907407407,
        "MMLU_high_school_us_history":0.8235294118,
        "MMLU_high_school_world_history":0.7721518987,
        "MMLU_human_aging":0.6591928251,
        "MMLU_human_sexuality":0.6259541985,
        "MMLU_international_law":0.7024793388,
        "MMLU_jurisprudence":0.7592592593,
        "MMLU_logical_fallacies":0.6625766871,
        "MMLU_machine_learning":0.4107142857,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.8034188034,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.7777777778,
        "MMLU_moral_disputes":0.6705202312,
        "MMLU_moral_scenarios":0.4491620112,
        "MMLU_nutrition":0.6209150327,
        "MMLU_philosophy":0.6881028939,
        "MMLU_prehistory":0.6944444444,
        "MMLU_professional_accounting":0.4503546099,
        "MMLU_professional_law":0.4563233377,
        "MMLU_professional_medicine":0.5367647059,
        "MMLU_professional_psychology":0.589869281,
        "MMLU_public_relations":0.6272727273,
        "MMLU_security_studies":0.6816326531,
        "MMLU_sociology":0.7611940299,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.7660818713
    },
    {
        "Model":"vigogne-33b-instruct",
        "URL":"https:\/\/huggingface.co\/bofenghuang\/vigogne-33b-instruct",
        "full_model_name":"bofenghuang\/vigogne-33b-instruct",
        "Parameters":33.0,
        "MMLU_average":0.5831569049,
        "arc:challenge|25":0.6211604096,
        "hellaswag|10":0.6549492133,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.5259259259,
        "MMLU_astronomy":0.5986842105,
        "MMLU_business_ethics":0.61,
        "MMLU_clinical_knowledge":0.5962264151,
        "MMLU_college_biology":0.6388888889,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.5144508671,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.4893617021,
        "MMLU_econometrics":0.3333333333,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.380952381,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.6838709677,
        "MMLU_high_school_chemistry":0.4334975369,
        "MMLU_high_school_computer_science":0.58,
        "MMLU_high_school_european_history":0.7212121212,
        "MMLU_high_school_geography":0.7323232323,
        "MMLU_high_school_government_and_politics":0.8186528497,
        "MMLU_high_school_macroeconomics":0.5846153846,
        "MMLU_high_school_mathematics":0.2925925926,
        "MMLU_high_school_microeconomics":0.6008403361,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.7706422018,
        "MMLU_high_school_statistics":0.4351851852,
        "MMLU_high_school_us_history":0.8039215686,
        "MMLU_high_school_world_history":0.8016877637,
        "MMLU_human_aging":0.668161435,
        "MMLU_human_sexuality":0.6870229008,
        "MMLU_international_law":0.7768595041,
        "MMLU_jurisprudence":0.712962963,
        "MMLU_logical_fallacies":0.6748466258,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.7572815534,
        "MMLU_marketing":0.858974359,
        "MMLU_medical_genetics":0.67,
        "MMLU_miscellaneous":0.7713920817,
        "MMLU_moral_disputes":0.6502890173,
        "MMLU_moral_scenarios":0.3765363128,
        "MMLU_nutrition":0.6176470588,
        "MMLU_philosophy":0.6752411576,
        "MMLU_prehistory":0.6481481481,
        "MMLU_professional_accounting":0.4397163121,
        "MMLU_professional_law":0.443285528,
        "MMLU_professional_medicine":0.5698529412,
        "MMLU_professional_psychology":0.6094771242,
        "MMLU_public_relations":0.6909090909,
        "MMLU_security_studies":0.6448979592,
        "MMLU_sociology":0.8059701493,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4819277108,
        "MMLU_world_religions":0.7660818713
    },
    {
        "Model":"PuddleJumper-13b-V2",
        "URL":"https:\/\/huggingface.co\/totally-not-an-llm\/PuddleJumper-13b-V2",
        "full_model_name":"totally-not-an-llm\/PuddleJumper-13b-V2",
        "Parameters":13.0,
        "MMLU_average":0.5830028749,
        "arc:challenge|25":0.5443686007,
        "hellaswag|10":0.6052579167,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.6118421053,
        "MMLU_business_ethics":0.6,
        "MMLU_clinical_knowledge":0.6037735849,
        "MMLU_college_biology":0.6527777778,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5838150289,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.67,
        "MMLU_conceptual_physics":0.5191489362,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.5172413793,
        "MMLU_elementary_mathematics":0.3465608466,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.6483870968,
        "MMLU_high_school_chemistry":0.4581280788,
        "MMLU_high_school_computer_science":0.57,
        "MMLU_high_school_european_history":0.7333333333,
        "MMLU_high_school_geography":0.7424242424,
        "MMLU_high_school_government_and_politics":0.8445595855,
        "MMLU_high_school_macroeconomics":0.6051282051,
        "MMLU_high_school_mathematics":0.3444444444,
        "MMLU_high_school_microeconomics":0.6134453782,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.7981651376,
        "MMLU_high_school_statistics":0.4444444444,
        "MMLU_high_school_us_history":0.8284313725,
        "MMLU_high_school_world_history":0.7805907173,
        "MMLU_human_aging":0.6457399103,
        "MMLU_human_sexuality":0.6870229008,
        "MMLU_international_law":0.7438016529,
        "MMLU_jurisprudence":0.7314814815,
        "MMLU_logical_fallacies":0.6993865031,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.7087378641,
        "MMLU_marketing":0.8162393162,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7701149425,
        "MMLU_moral_disputes":0.6473988439,
        "MMLU_moral_scenarios":0.4938547486,
        "MMLU_nutrition":0.6241830065,
        "MMLU_philosophy":0.6945337621,
        "MMLU_prehistory":0.7037037037,
        "MMLU_professional_accounting":0.4716312057,
        "MMLU_professional_law":0.4517601043,
        "MMLU_professional_medicine":0.5588235294,
        "MMLU_professional_psychology":0.5767973856,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.6448979592,
        "MMLU_sociology":0.7611940299,
        "MMLU_us_foreign_policy":0.79,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"based-30b",
        "URL":"https:\/\/huggingface.co\/ehartford\/based-30b",
        "full_model_name":"ehartford\/based-30b",
        "Parameters":30.0,
        "MMLU_average":0.5828458248,
        "arc:challenge|25":0.6126279863,
        "hellaswag|10":0.6648078072,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.5259259259,
        "MMLU_astronomy":0.6184210526,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.5735849057,
        "MMLU_college_biology":0.5763888889,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.4,
        "MMLU_college_medicine":0.5375722543,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.72,
        "MMLU_conceptual_physics":0.5234042553,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.3597883598,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.6838709677,
        "MMLU_high_school_chemistry":0.4088669951,
        "MMLU_high_school_computer_science":0.63,
        "MMLU_high_school_european_history":0.7515151515,
        "MMLU_high_school_geography":0.7171717172,
        "MMLU_high_school_government_and_politics":0.8134715026,
        "MMLU_high_school_macroeconomics":0.5769230769,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.5588235294,
        "MMLU_high_school_physics":0.3708609272,
        "MMLU_high_school_psychology":0.7651376147,
        "MMLU_high_school_statistics":0.4444444444,
        "MMLU_high_school_us_history":0.7745098039,
        "MMLU_high_school_world_history":0.8016877637,
        "MMLU_human_aging":0.66367713,
        "MMLU_human_sexuality":0.6335877863,
        "MMLU_international_law":0.7520661157,
        "MMLU_jurisprudence":0.6481481481,
        "MMLU_logical_fallacies":0.6932515337,
        "MMLU_machine_learning":0.4285714286,
        "MMLU_management":0.786407767,
        "MMLU_marketing":0.8376068376,
        "MMLU_medical_genetics":0.69,
        "MMLU_miscellaneous":0.7713920817,
        "MMLU_moral_disputes":0.6387283237,
        "MMLU_moral_scenarios":0.417877095,
        "MMLU_nutrition":0.614379085,
        "MMLU_philosophy":0.6720257235,
        "MMLU_prehistory":0.6574074074,
        "MMLU_professional_accounting":0.4361702128,
        "MMLU_professional_law":0.4413298566,
        "MMLU_professional_medicine":0.5551470588,
        "MMLU_professional_psychology":0.6258169935,
        "MMLU_public_relations":0.6909090909,
        "MMLU_security_studies":0.6326530612,
        "MMLU_sociology":0.7860696517,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4879518072,
        "MMLU_world_religions":0.8070175439
    },
    {
        "Model":"2x-LoRA-Assemble-Platypus2-13B",
        "URL":"https:\/\/huggingface.co\/PulsarAI\/2x-LoRA-Assemble-Platypus2-13B",
        "full_model_name":"PulsarAI\/2x-LoRA-Assemble-Platypus2-13B",
        "Parameters":13.0,
        "MMLU_average":0.5825083371,
        "arc:challenge|25":0.5742320819,
        "hellaswag|10":0.6287592113,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.6118421053,
        "MMLU_business_ethics":0.58,
        "MMLU_clinical_knowledge":0.5811320755,
        "MMLU_college_biology":0.6319444444,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.49,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.5664739884,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.67,
        "MMLU_conceptual_physics":0.4765957447,
        "MMLU_econometrics":0.350877193,
        "MMLU_electrical_engineering":0.5310344828,
        "MMLU_elementary_mathematics":0.3465608466,
        "MMLU_formal_logic":0.4365079365,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.6258064516,
        "MMLU_high_school_chemistry":0.39408867,
        "MMLU_high_school_computer_science":0.59,
        "MMLU_high_school_european_history":0.7393939394,
        "MMLU_high_school_geography":0.7424242424,
        "MMLU_high_school_government_and_politics":0.8704663212,
        "MMLU_high_school_macroeconomics":0.6179487179,
        "MMLU_high_school_mathematics":0.3333333333,
        "MMLU_high_school_microeconomics":0.5714285714,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.7816513761,
        "MMLU_high_school_statistics":0.4398148148,
        "MMLU_high_school_us_history":0.818627451,
        "MMLU_high_school_world_history":0.7552742616,
        "MMLU_human_aging":0.6771300448,
        "MMLU_human_sexuality":0.6564885496,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.7407407407,
        "MMLU_logical_fallacies":0.6932515337,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.8333333333,
        "MMLU_medical_genetics":0.61,
        "MMLU_miscellaneous":0.7828863346,
        "MMLU_moral_disputes":0.6387283237,
        "MMLU_moral_scenarios":0.4815642458,
        "MMLU_nutrition":0.637254902,
        "MMLU_philosophy":0.6591639871,
        "MMLU_prehistory":0.6944444444,
        "MMLU_professional_accounting":0.4503546099,
        "MMLU_professional_law":0.4524119948,
        "MMLU_professional_medicine":0.5992647059,
        "MMLU_professional_psychology":0.5931372549,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6204081633,
        "MMLU_sociology":0.6965174129,
        "MMLU_us_foreign_policy":0.8,
        "MMLU_virology":0.4819277108,
        "MMLU_world_religions":0.8128654971
    },
    {
        "Model":"Wizard-Vicuna-30B-Uncensored-fp16",
        "URL":"https:\/\/huggingface.co\/TheBloke\/Wizard-Vicuna-30B-Uncensored-fp16",
        "full_model_name":"TheBloke\/Wizard-Vicuna-30B-Uncensored-fp16",
        "Parameters":30.0,
        "MMLU_average":0.5823676231,
        "arc:challenge|25":0.5989761092,
        "hellaswag|10":0.6386178052,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.5333333333,
        "MMLU_astronomy":0.5986842105,
        "MMLU_business_ethics":0.6,
        "MMLU_clinical_knowledge":0.6150943396,
        "MMLU_college_biology":0.6041666667,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.39,
        "MMLU_college_medicine":0.5317919075,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.65,
        "MMLU_conceptual_physics":0.5021276596,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.5103448276,
        "MMLU_elementary_mathematics":0.3439153439,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.4,
        "MMLU_high_school_biology":0.6516129032,
        "MMLU_high_school_chemistry":0.3990147783,
        "MMLU_high_school_computer_science":0.61,
        "MMLU_high_school_european_history":0.7212121212,
        "MMLU_high_school_geography":0.7626262626,
        "MMLU_high_school_government_and_politics":0.8082901554,
        "MMLU_high_school_macroeconomics":0.5717948718,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.6134453782,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.7816513761,
        "MMLU_high_school_statistics":0.4444444444,
        "MMLU_high_school_us_history":0.7598039216,
        "MMLU_high_school_world_history":0.7890295359,
        "MMLU_human_aging":0.6412556054,
        "MMLU_human_sexuality":0.679389313,
        "MMLU_international_law":0.7520661157,
        "MMLU_jurisprudence":0.6944444444,
        "MMLU_logical_fallacies":0.6993865031,
        "MMLU_machine_learning":0.4821428571,
        "MMLU_management":0.786407767,
        "MMLU_marketing":0.8504273504,
        "MMLU_medical_genetics":0.64,
        "MMLU_miscellaneous":0.7637292465,
        "MMLU_moral_disputes":0.6589595376,
        "MMLU_moral_scenarios":0.4145251397,
        "MMLU_nutrition":0.6307189542,
        "MMLU_philosophy":0.6881028939,
        "MMLU_prehistory":0.6697530864,
        "MMLU_professional_accounting":0.4574468085,
        "MMLU_professional_law":0.4576271186,
        "MMLU_professional_medicine":0.5698529412,
        "MMLU_professional_psychology":0.6045751634,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.6,
        "MMLU_sociology":0.7860696517,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.5180722892,
        "MMLU_world_religions":0.783625731
    },
    {
        "Model":"openbuddy-llama-30b-v7.1-bf16",
        "URL":"https:\/\/huggingface.co\/OpenBuddyEA\/openbuddy-llama-30b-v7.1-bf16",
        "full_model_name":"OpenBuddyEA\/openbuddy-llama-30b-v7.1-bf16",
        "Parameters":30.0,
        "MMLU_average":0.5818394921,
        "arc:challenge|25":0.5844709898,
        "hellaswag|10":0.6191993627,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.5185185185,
        "MMLU_astronomy":0.6315789474,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.558490566,
        "MMLU_college_biology":0.5763888889,
        "MMLU_college_chemistry":0.33,
        "MMLU_college_computer_science":0.51,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.5317919075,
        "MMLU_college_physics":0.3431372549,
        "MMLU_computer_security":0.76,
        "MMLU_conceptual_physics":0.4936170213,
        "MMLU_econometrics":0.3684210526,
        "MMLU_electrical_engineering":0.4896551724,
        "MMLU_elementary_mathematics":0.3571428571,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.6774193548,
        "MMLU_high_school_chemistry":0.3645320197,
        "MMLU_high_school_computer_science":0.56,
        "MMLU_high_school_european_history":0.7090909091,
        "MMLU_high_school_geography":0.7474747475,
        "MMLU_high_school_government_and_politics":0.829015544,
        "MMLU_high_school_macroeconomics":0.5743589744,
        "MMLU_high_school_mathematics":0.3222222222,
        "MMLU_high_school_microeconomics":0.5588235294,
        "MMLU_high_school_physics":0.3841059603,
        "MMLU_high_school_psychology":0.7944954128,
        "MMLU_high_school_statistics":0.4074074074,
        "MMLU_high_school_us_history":0.7843137255,
        "MMLU_high_school_world_history":0.7805907173,
        "MMLU_human_aging":0.6816143498,
        "MMLU_human_sexuality":0.641221374,
        "MMLU_international_law":0.7190082645,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.7239263804,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.7475728155,
        "MMLU_marketing":0.8675213675,
        "MMLU_medical_genetics":0.65,
        "MMLU_miscellaneous":0.7777777778,
        "MMLU_moral_disputes":0.6473988439,
        "MMLU_moral_scenarios":0.4055865922,
        "MMLU_nutrition":0.6470588235,
        "MMLU_philosophy":0.6495176849,
        "MMLU_prehistory":0.6666666667,
        "MMLU_professional_accounting":0.4219858156,
        "MMLU_professional_law":0.4315514993,
        "MMLU_professional_medicine":0.5808823529,
        "MMLU_professional_psychology":0.6176470588,
        "MMLU_public_relations":0.7,
        "MMLU_security_studies":0.6693877551,
        "MMLU_sociology":0.7412935323,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.5060240964,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"magpie-13b",
        "URL":"https:\/\/huggingface.co\/boomerchan\/magpie-13b",
        "full_model_name":"boomerchan\/magpie-13b",
        "Parameters":13.0,
        "MMLU_average":0.5814712698,
        "arc:challenge|25":0.5955631399,
        "hellaswag|10":0.6403106951,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5723684211,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.6075471698,
        "MMLU_college_biology":0.6319444444,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.5780346821,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.4680851064,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.4965517241,
        "MMLU_elementary_mathematics":0.3306878307,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.6580645161,
        "MMLU_high_school_chemistry":0.4532019704,
        "MMLU_high_school_computer_science":0.57,
        "MMLU_high_school_european_history":0.6848484848,
        "MMLU_high_school_geography":0.7474747475,
        "MMLU_high_school_government_and_politics":0.829015544,
        "MMLU_high_school_macroeconomics":0.5358974359,
        "MMLU_high_school_mathematics":0.3259259259,
        "MMLU_high_school_microeconomics":0.6134453782,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.7743119266,
        "MMLU_high_school_statistics":0.4537037037,
        "MMLU_high_school_us_history":0.7843137255,
        "MMLU_high_school_world_history":0.7552742616,
        "MMLU_human_aging":0.6726457399,
        "MMLU_human_sexuality":0.6488549618,
        "MMLU_international_law":0.7438016529,
        "MMLU_jurisprudence":0.7962962963,
        "MMLU_logical_fallacies":0.6993865031,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.7669902913,
        "MMLU_marketing":0.8034188034,
        "MMLU_medical_genetics":0.64,
        "MMLU_miscellaneous":0.7752234994,
        "MMLU_moral_disputes":0.6560693642,
        "MMLU_moral_scenarios":0.4670391061,
        "MMLU_nutrition":0.6437908497,
        "MMLU_philosophy":0.6495176849,
        "MMLU_prehistory":0.6604938272,
        "MMLU_professional_accounting":0.4468085106,
        "MMLU_professional_law":0.4413298566,
        "MMLU_professional_medicine":0.625,
        "MMLU_professional_psychology":0.591503268,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.6489795918,
        "MMLU_sociology":0.7810945274,
        "MMLU_us_foreign_policy":0.86,
        "MMLU_virology":0.4879518072,
        "MMLU_world_religions":0.783625731
    },
    {
        "Model":"Unholy-v1-12L-13B",
        "URL":"https:\/\/huggingface.co\/Undi95\/Unholy-v1-12L-13B",
        "full_model_name":"Undi95\/Unholy-v1-12L-13B",
        "Parameters":13.0,
        "MMLU_average":0.5808136499,
        "arc:challenge|25":0.614334471,
        "hellaswag|10":0.639514041,
        "MMLU_abstract_algebra":0.38,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5855263158,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.6264150943,
        "MMLU_college_biology":0.6597222222,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.5,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.549132948,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.4893617021,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.5448275862,
        "MMLU_elementary_mathematics":0.3174603175,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.6709677419,
        "MMLU_high_school_chemistry":0.4581280788,
        "MMLU_high_school_computer_science":0.61,
        "MMLU_high_school_european_history":0.703030303,
        "MMLU_high_school_geography":0.7222222222,
        "MMLU_high_school_government_and_politics":0.8341968912,
        "MMLU_high_school_macroeconomics":0.5564102564,
        "MMLU_high_school_mathematics":0.337037037,
        "MMLU_high_school_microeconomics":0.5966386555,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.7559633028,
        "MMLU_high_school_statistics":0.3981481481,
        "MMLU_high_school_us_history":0.7990196078,
        "MMLU_high_school_world_history":0.7721518987,
        "MMLU_human_aging":0.6950672646,
        "MMLU_human_sexuality":0.6259541985,
        "MMLU_international_law":0.7603305785,
        "MMLU_jurisprudence":0.787037037,
        "MMLU_logical_fallacies":0.6871165644,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.8076923077,
        "MMLU_medical_genetics":0.65,
        "MMLU_miscellaneous":0.7701149425,
        "MMLU_moral_disputes":0.6473988439,
        "MMLU_moral_scenarios":0.4815642458,
        "MMLU_nutrition":0.6339869281,
        "MMLU_philosophy":0.6398713826,
        "MMLU_prehistory":0.6759259259,
        "MMLU_professional_accounting":0.4397163121,
        "MMLU_professional_law":0.4465449804,
        "MMLU_professional_medicine":0.5588235294,
        "MMLU_professional_psychology":0.5800653595,
        "MMLU_public_relations":0.6727272727,
        "MMLU_security_studies":0.6775510204,
        "MMLU_sociology":0.7810945274,
        "MMLU_us_foreign_policy":0.85,
        "MMLU_virology":0.4759036145,
        "MMLU_world_religions":0.7660818713
    },
    {
        "Model":"test_merge_p_ov1_w0.66_w0.5_n1",
        "URL":"https:\/\/huggingface.co\/xxyyy123\/test_merge_p_ov1_w0.66_w0.5_n1",
        "full_model_name":"xxyyy123\/test_merge_p_ov1_w0.66_w0.5_n1",
        "Parameters":null,
        "MMLU_average":0.5804702514,
        "arc:challenge|25":0.6015358362,
        "hellaswag|10":0.6246763593,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.5526315789,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.6075471698,
        "MMLU_college_biology":0.6597222222,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.5722543353,
        "MMLU_college_physics":0.3431372549,
        "MMLU_computer_security":0.67,
        "MMLU_conceptual_physics":0.5106382979,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.5448275862,
        "MMLU_elementary_mathematics":0.3439153439,
        "MMLU_formal_logic":0.373015873,
        "MMLU_global_facts":0.44,
        "MMLU_high_school_biology":0.7,
        "MMLU_high_school_chemistry":0.4137931034,
        "MMLU_high_school_computer_science":0.56,
        "MMLU_high_school_european_history":0.7515151515,
        "MMLU_high_school_geography":0.7626262626,
        "MMLU_high_school_government_and_politics":0.8186528497,
        "MMLU_high_school_macroeconomics":0.6128205128,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.5840336134,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.7853211009,
        "MMLU_high_school_statistics":0.4212962963,
        "MMLU_high_school_us_history":0.8088235294,
        "MMLU_high_school_world_history":0.7552742616,
        "MMLU_human_aging":0.6995515695,
        "MMLU_human_sexuality":0.6717557252,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.8148148148,
        "MMLU_logical_fallacies":0.6503067485,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.7475728155,
        "MMLU_marketing":0.8504273504,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.7713920817,
        "MMLU_moral_disputes":0.6271676301,
        "MMLU_moral_scenarios":0.3843575419,
        "MMLU_nutrition":0.614379085,
        "MMLU_philosophy":0.6591639871,
        "MMLU_prehistory":0.6975308642,
        "MMLU_professional_accounting":0.4432624113,
        "MMLU_professional_law":0.4361147327,
        "MMLU_professional_medicine":0.5808823529,
        "MMLU_professional_psychology":0.5882352941,
        "MMLU_public_relations":0.6272727273,
        "MMLU_security_studies":0.6653061224,
        "MMLU_sociology":0.7860696517,
        "MMLU_us_foreign_policy":0.76,
        "MMLU_virology":0.4819277108,
        "MMLU_world_religions":0.7894736842
    },
    {
        "Model":"Luban-Platypus2-13B-QLora-0.80-epoch",
        "URL":"https:\/\/huggingface.co\/TFLai\/Luban-Platypus2-13B-QLora-0.80-epoch",
        "full_model_name":"TFLai\/Luban-Platypus2-13B-QLora-0.80-epoch",
        "Parameters":13.0,
        "MMLU_average":0.5803487031,
        "arc:challenge|25":0.5759385666,
        "hellaswag|10":0.6199960167,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.6184210526,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.6075471698,
        "MMLU_college_biology":0.6111111111,
        "MMLU_college_chemistry":0.36,
        "MMLU_college_computer_science":0.52,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.5433526012,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.485106383,
        "MMLU_econometrics":0.3421052632,
        "MMLU_electrical_engineering":0.5793103448,
        "MMLU_elementary_mathematics":0.3492063492,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.6516129032,
        "MMLU_high_school_chemistry":0.4630541872,
        "MMLU_high_school_computer_science":0.62,
        "MMLU_high_school_european_history":0.7575757576,
        "MMLU_high_school_geography":0.7323232323,
        "MMLU_high_school_government_and_politics":0.8341968912,
        "MMLU_high_school_macroeconomics":0.6102564103,
        "MMLU_high_school_mathematics":0.3444444444,
        "MMLU_high_school_microeconomics":0.5672268908,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.7798165138,
        "MMLU_high_school_statistics":0.4074074074,
        "MMLU_high_school_us_history":0.8039215686,
        "MMLU_high_school_world_history":0.7637130802,
        "MMLU_human_aging":0.6547085202,
        "MMLU_human_sexuality":0.6564885496,
        "MMLU_international_law":0.7438016529,
        "MMLU_jurisprudence":0.7685185185,
        "MMLU_logical_fallacies":0.6625766871,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.8418803419,
        "MMLU_medical_genetics":0.61,
        "MMLU_miscellaneous":0.7765006386,
        "MMLU_moral_disputes":0.612716763,
        "MMLU_moral_scenarios":0.4335195531,
        "MMLU_nutrition":0.6568627451,
        "MMLU_philosophy":0.6463022508,
        "MMLU_prehistory":0.6913580247,
        "MMLU_professional_accounting":0.4645390071,
        "MMLU_professional_law":0.4393741851,
        "MMLU_professional_medicine":0.5588235294,
        "MMLU_professional_psychology":0.5931372549,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.6653061224,
        "MMLU_sociology":0.7412935323,
        "MMLU_us_foreign_policy":0.8,
        "MMLU_virology":0.4759036145,
        "MMLU_world_religions":0.8070175439
    },
    {
        "Model":"llama2-13b-megacode2_min100",
        "URL":"https:\/\/huggingface.co\/andreaskoepf\/llama2-13b-megacode2_min100",
        "full_model_name":"andreaskoepf\/llama2-13b-megacode2_min100",
        "Parameters":13.0,
        "MMLU_average":0.5791638768,
        "arc:challenge|25":0.5588737201,
        "hellaswag|10":0.6141206931,
        "MMLU_abstract_algebra":0.41,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.5657894737,
        "MMLU_business_ethics":0.64,
        "MMLU_clinical_knowledge":0.6075471698,
        "MMLU_college_biology":0.5972222222,
        "MMLU_college_chemistry":0.46,
        "MMLU_college_computer_science":0.5,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.5202312139,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.4553191489,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.5448275862,
        "MMLU_elementary_mathematics":0.3253968254,
        "MMLU_formal_logic":0.4126984127,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.7193548387,
        "MMLU_high_school_chemistry":0.4482758621,
        "MMLU_high_school_computer_science":0.52,
        "MMLU_high_school_european_history":0.7212121212,
        "MMLU_high_school_geography":0.7575757576,
        "MMLU_high_school_government_and_politics":0.8082901554,
        "MMLU_high_school_macroeconomics":0.5641025641,
        "MMLU_high_school_mathematics":0.3148148148,
        "MMLU_high_school_microeconomics":0.5462184874,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.7577981651,
        "MMLU_high_school_statistics":0.462962963,
        "MMLU_high_school_us_history":0.7598039216,
        "MMLU_high_school_world_history":0.7721518987,
        "MMLU_human_aging":0.6502242152,
        "MMLU_human_sexuality":0.6564885496,
        "MMLU_international_law":0.7851239669,
        "MMLU_jurisprudence":0.7777777778,
        "MMLU_logical_fallacies":0.6503067485,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.7766990291,
        "MMLU_marketing":0.8162393162,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.7765006386,
        "MMLU_moral_disputes":0.6300578035,
        "MMLU_moral_scenarios":0.4737430168,
        "MMLU_nutrition":0.6274509804,
        "MMLU_philosophy":0.6109324759,
        "MMLU_prehistory":0.6512345679,
        "MMLU_professional_accounting":0.4361702128,
        "MMLU_professional_law":0.4152542373,
        "MMLU_professional_medicine":0.5588235294,
        "MMLU_professional_psychology":0.5555555556,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6816326531,
        "MMLU_sociology":0.7512437811,
        "MMLU_us_foreign_policy":0.85,
        "MMLU_virology":0.4879518072,
        "MMLU_world_religions":0.8128654971
    },
    {
        "Model":"LosslessMegaCoder-llama2-13b-mini",
        "URL":"https:\/\/huggingface.co\/rombodawg\/LosslessMegaCoder-llama2-13b-mini",
        "full_model_name":"rombodawg\/LosslessMegaCoder-llama2-13b-mini",
        "Parameters":13.0,
        "MMLU_average":0.5791638768,
        "arc:challenge|25":0.5588737201,
        "hellaswag|10":0.6141206931,
        "MMLU_abstract_algebra":0.41,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.5657894737,
        "MMLU_business_ethics":0.64,
        "MMLU_clinical_knowledge":0.6075471698,
        "MMLU_college_biology":0.5972222222,
        "MMLU_college_chemistry":0.46,
        "MMLU_college_computer_science":0.5,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.5202312139,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.4553191489,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.5448275862,
        "MMLU_elementary_mathematics":0.3253968254,
        "MMLU_formal_logic":0.4126984127,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.7193548387,
        "MMLU_high_school_chemistry":0.4482758621,
        "MMLU_high_school_computer_science":0.52,
        "MMLU_high_school_european_history":0.7212121212,
        "MMLU_high_school_geography":0.7575757576,
        "MMLU_high_school_government_and_politics":0.8082901554,
        "MMLU_high_school_macroeconomics":0.5641025641,
        "MMLU_high_school_mathematics":0.3148148148,
        "MMLU_high_school_microeconomics":0.5462184874,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.7577981651,
        "MMLU_high_school_statistics":0.462962963,
        "MMLU_high_school_us_history":0.7598039216,
        "MMLU_high_school_world_history":0.7721518987,
        "MMLU_human_aging":0.6502242152,
        "MMLU_human_sexuality":0.6564885496,
        "MMLU_international_law":0.7851239669,
        "MMLU_jurisprudence":0.7777777778,
        "MMLU_logical_fallacies":0.6503067485,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.7766990291,
        "MMLU_marketing":0.8162393162,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.7765006386,
        "MMLU_moral_disputes":0.6300578035,
        "MMLU_moral_scenarios":0.4737430168,
        "MMLU_nutrition":0.6274509804,
        "MMLU_philosophy":0.6109324759,
        "MMLU_prehistory":0.6512345679,
        "MMLU_professional_accounting":0.4361702128,
        "MMLU_professional_law":0.4152542373,
        "MMLU_professional_medicine":0.5588235294,
        "MMLU_professional_psychology":0.5555555556,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6816326531,
        "MMLU_sociology":0.7512437811,
        "MMLU_us_foreign_policy":0.85,
        "MMLU_virology":0.4879518072,
        "MMLU_world_religions":0.8128654971
    },
    {
        "Model":"Platypus2xOpenOrca-13B-IA3-v2",
        "URL":"https:\/\/huggingface.co\/yeontaek\/Platypus2xOpenOrca-13B-IA3-v2",
        "full_model_name":"yeontaek\/Platypus2xOpenOrca-13B-IA3-v2",
        "Parameters":13.0,
        "MMLU_average":0.5790621324,
        "arc:challenge|25":0.5836177474,
        "hellaswag|10":0.6140211113,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.6578947368,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.6037735849,
        "MMLU_college_biology":0.6597222222,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.4971098266,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.73,
        "MMLU_conceptual_physics":0.4723404255,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.5103448276,
        "MMLU_elementary_mathematics":0.3333333333,
        "MMLU_formal_logic":0.4126984127,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.7096774194,
        "MMLU_high_school_chemistry":0.4236453202,
        "MMLU_high_school_computer_science":0.6,
        "MMLU_high_school_european_history":0.7333333333,
        "MMLU_high_school_geography":0.7474747475,
        "MMLU_high_school_government_and_politics":0.829015544,
        "MMLU_high_school_macroeconomics":0.5692307692,
        "MMLU_high_school_mathematics":0.2925925926,
        "MMLU_high_school_microeconomics":0.6554621849,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.7688073394,
        "MMLU_high_school_statistics":0.4583333333,
        "MMLU_high_school_us_history":0.8137254902,
        "MMLU_high_school_world_history":0.746835443,
        "MMLU_human_aging":0.6457399103,
        "MMLU_human_sexuality":0.6870229008,
        "MMLU_international_law":0.7190082645,
        "MMLU_jurisprudence":0.787037037,
        "MMLU_logical_fallacies":0.6809815951,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.7475728155,
        "MMLU_marketing":0.8205128205,
        "MMLU_medical_genetics":0.53,
        "MMLU_miscellaneous":0.7611749681,
        "MMLU_moral_disputes":0.6387283237,
        "MMLU_moral_scenarios":0.4379888268,
        "MMLU_nutrition":0.614379085,
        "MMLU_philosophy":0.6398713826,
        "MMLU_prehistory":0.6604938272,
        "MMLU_professional_accounting":0.4680851064,
        "MMLU_professional_law":0.4530638853,
        "MMLU_professional_medicine":0.5477941176,
        "MMLU_professional_psychology":0.5866013072,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6734693878,
        "MMLU_sociology":0.7512437811,
        "MMLU_us_foreign_policy":0.76,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.783625731
    },
    {
        "Model":"oasst-rlhf-2-llama-30b-7k-steps-hf",
        "URL":"https:\/\/huggingface.co\/Yhyu13\/oasst-rlhf-2-llama-30b-7k-steps-hf",
        "full_model_name":"Yhyu13\/oasst-rlhf-2-llama-30b-7k-steps-hf",
        "Parameters":30.0,
        "MMLU_average":0.5788737418,
        "arc:challenge|25":0.5887372014,
        "hellaswag|10":0.6411073491,
        "MMLU_abstract_algebra":0.41,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5855263158,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.6113207547,
        "MMLU_college_biology":0.6111111111,
        "MMLU_college_chemistry":0.51,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.4,
        "MMLU_college_medicine":0.5549132948,
        "MMLU_college_physics":0.4117647059,
        "MMLU_computer_security":0.59,
        "MMLU_conceptual_physics":0.4638297872,
        "MMLU_econometrics":0.3947368421,
        "MMLU_electrical_engineering":0.4413793103,
        "MMLU_elementary_mathematics":0.3148148148,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.4,
        "MMLU_high_school_biology":0.6612903226,
        "MMLU_high_school_chemistry":0.4088669951,
        "MMLU_high_school_computer_science":0.61,
        "MMLU_high_school_european_history":0.703030303,
        "MMLU_high_school_geography":0.7171717172,
        "MMLU_high_school_government_and_politics":0.8082901554,
        "MMLU_high_school_macroeconomics":0.5743589744,
        "MMLU_high_school_mathematics":0.3259259259,
        "MMLU_high_school_microeconomics":0.5924369748,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.7743119266,
        "MMLU_high_school_statistics":0.4953703704,
        "MMLU_high_school_us_history":0.7745098039,
        "MMLU_high_school_world_history":0.776371308,
        "MMLU_human_aging":0.6726457399,
        "MMLU_human_sexuality":0.6564885496,
        "MMLU_international_law":0.7768595041,
        "MMLU_jurisprudence":0.6666666667,
        "MMLU_logical_fallacies":0.6687116564,
        "MMLU_machine_learning":0.4107142857,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.8675213675,
        "MMLU_medical_genetics":0.63,
        "MMLU_miscellaneous":0.7394636015,
        "MMLU_moral_disputes":0.6473988439,
        "MMLU_moral_scenarios":0.4011173184,
        "MMLU_nutrition":0.6013071895,
        "MMLU_philosophy":0.6463022508,
        "MMLU_prehistory":0.6327160494,
        "MMLU_professional_accounting":0.4219858156,
        "MMLU_professional_law":0.4328552803,
        "MMLU_professional_medicine":0.6102941176,
        "MMLU_professional_psychology":0.5882352941,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.6163265306,
        "MMLU_sociology":0.7860696517,
        "MMLU_us_foreign_policy":0.78,
        "MMLU_virology":0.5120481928,
        "MMLU_world_religions":0.783625731
    },
    {
        "Model":"MegaMix-S1-13B",
        "URL":"https:\/\/huggingface.co\/gradientputri\/MegaMix-S1-13B",
        "full_model_name":"gradientputri\/MegaMix-S1-13B",
        "Parameters":13.0,
        "MMLU_average":0.5788444555,
        "arc:challenge|25":0.587883959,
        "hellaswag|10":0.6412069309,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.5855263158,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.6339622642,
        "MMLU_college_biology":0.6388888889,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5433526012,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.4595744681,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.3412698413,
        "MMLU_formal_logic":0.3888888889,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.6774193548,
        "MMLU_high_school_chemistry":0.4679802956,
        "MMLU_high_school_computer_science":0.58,
        "MMLU_high_school_european_history":0.6666666667,
        "MMLU_high_school_geography":0.7323232323,
        "MMLU_high_school_government_and_politics":0.8445595855,
        "MMLU_high_school_macroeconomics":0.541025641,
        "MMLU_high_school_mathematics":0.3111111111,
        "MMLU_high_school_microeconomics":0.5966386555,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.7724770642,
        "MMLU_high_school_statistics":0.4537037037,
        "MMLU_high_school_us_history":0.7892156863,
        "MMLU_high_school_world_history":0.7890295359,
        "MMLU_human_aging":0.6995515695,
        "MMLU_human_sexuality":0.641221374,
        "MMLU_international_law":0.7438016529,
        "MMLU_jurisprudence":0.7685185185,
        "MMLU_logical_fallacies":0.6932515337,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.7669902913,
        "MMLU_marketing":0.8205128205,
        "MMLU_medical_genetics":0.62,
        "MMLU_miscellaneous":0.7726692209,
        "MMLU_moral_disputes":0.6502890173,
        "MMLU_moral_scenarios":0.4893854749,
        "MMLU_nutrition":0.6339869281,
        "MMLU_philosophy":0.6591639871,
        "MMLU_prehistory":0.6450617284,
        "MMLU_professional_accounting":0.4361702128,
        "MMLU_professional_law":0.4335071708,
        "MMLU_professional_medicine":0.5845588235,
        "MMLU_professional_psychology":0.589869281,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.6530612245,
        "MMLU_sociology":0.7810945274,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.7777777778
    },
    {
        "Model":"OpenOrcaxOpenChat-Preview2-13B-GPTQ",
        "URL":"https:\/\/huggingface.co\/TheBloke\/OpenOrcaxOpenChat-Preview2-13B-GPTQ",
        "full_model_name":"TheBloke\/OpenOrcaxOpenChat-Preview2-13B-GPTQ",
        "Parameters":13.0,
        "MMLU_average":0.5784878883,
        "arc:challenge|25":0.5912969283,
        "hellaswag|10":0.6223859789,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5789473684,
        "MMLU_business_ethics":0.61,
        "MMLU_clinical_knowledge":0.6113207547,
        "MMLU_college_biology":0.625,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.5722543353,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.4595744681,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.5103448276,
        "MMLU_elementary_mathematics":0.3333333333,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.4,
        "MMLU_high_school_biology":0.6967741935,
        "MMLU_high_school_chemistry":0.4532019704,
        "MMLU_high_school_computer_science":0.57,
        "MMLU_high_school_european_history":0.7151515152,
        "MMLU_high_school_geography":0.7575757576,
        "MMLU_high_school_government_and_politics":0.8082901554,
        "MMLU_high_school_macroeconomics":0.5641025641,
        "MMLU_high_school_mathematics":0.3185185185,
        "MMLU_high_school_microeconomics":0.6176470588,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7559633028,
        "MMLU_high_school_statistics":0.4351851852,
        "MMLU_high_school_us_history":0.7696078431,
        "MMLU_high_school_world_history":0.776371308,
        "MMLU_human_aging":0.66367713,
        "MMLU_human_sexuality":0.7022900763,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.7962962963,
        "MMLU_logical_fallacies":0.6809815951,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.7475728155,
        "MMLU_marketing":0.8418803419,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7637292465,
        "MMLU_moral_disputes":0.6473988439,
        "MMLU_moral_scenarios":0.4737430168,
        "MMLU_nutrition":0.6470588235,
        "MMLU_philosophy":0.6495176849,
        "MMLU_prehistory":0.6759259259,
        "MMLU_professional_accounting":0.4680851064,
        "MMLU_professional_law":0.4348109518,
        "MMLU_professional_medicine":0.5257352941,
        "MMLU_professional_psychology":0.5931372549,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.693877551,
        "MMLU_sociology":0.7512437811,
        "MMLU_us_foreign_policy":0.79,
        "MMLU_virology":0.4518072289,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"Platypus2xOpenOrcaxGuanaco-13B-LoRa",
        "URL":"https:\/\/huggingface.co\/yeontaek\/Platypus2xOpenOrcaxGuanaco-13B-LoRa",
        "full_model_name":"yeontaek\/Platypus2xOpenOrcaxGuanaco-13B-LoRa",
        "Parameters":13.0,
        "MMLU_average":0.5783808631,
        "arc:challenge|25":0.5699658703,
        "hellaswag|10":0.6065524796,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.5037037037,
        "MMLU_astronomy":0.5921052632,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.6301886792,
        "MMLU_college_biology":0.6458333333,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.549132948,
        "MMLU_college_physics":0.3823529412,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.4893617021,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.524137931,
        "MMLU_elementary_mathematics":0.3677248677,
        "MMLU_formal_logic":0.3968253968,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.6967741935,
        "MMLU_high_school_chemistry":0.4285714286,
        "MMLU_high_school_computer_science":0.58,
        "MMLU_high_school_european_history":0.7212121212,
        "MMLU_high_school_geography":0.7626262626,
        "MMLU_high_school_government_and_politics":0.8134715026,
        "MMLU_high_school_macroeconomics":0.5743589744,
        "MMLU_high_school_mathematics":0.3259259259,
        "MMLU_high_school_microeconomics":0.6344537815,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7779816514,
        "MMLU_high_school_statistics":0.4398148148,
        "MMLU_high_school_us_history":0.8137254902,
        "MMLU_high_school_world_history":0.7848101266,
        "MMLU_human_aging":0.6188340807,
        "MMLU_human_sexuality":0.6641221374,
        "MMLU_international_law":0.6859504132,
        "MMLU_jurisprudence":0.7407407407,
        "MMLU_logical_fallacies":0.6871165644,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.7475728155,
        "MMLU_marketing":0.8034188034,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7701149425,
        "MMLU_moral_disputes":0.6387283237,
        "MMLU_moral_scenarios":0.4223463687,
        "MMLU_nutrition":0.5751633987,
        "MMLU_philosophy":0.6463022508,
        "MMLU_prehistory":0.6728395062,
        "MMLU_professional_accounting":0.4787234043,
        "MMLU_professional_law":0.464797914,
        "MMLU_professional_medicine":0.5514705882,
        "MMLU_professional_psychology":0.5735294118,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.6367346939,
        "MMLU_sociology":0.7562189055,
        "MMLU_us_foreign_policy":0.78,
        "MMLU_virology":0.4518072289,
        "MMLU_world_religions":0.7894736842
    },
    {
        "Model":"Vicuzard-30B-Uncensored-instruct-PL-lora_unload",
        "URL":"https:\/\/huggingface.co\/Aspik101\/Vicuzard-30B-Uncensored-instruct-PL-lora_unload",
        "full_model_name":"Aspik101\/Vicuzard-30B-Uncensored-instruct-PL-lora_unload",
        "Parameters":30.0,
        "MMLU_average":0.578154757,
        "arc:challenge|25":0.5964163823,
        "hellaswag|10":0.6429994025,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.5037037037,
        "MMLU_astronomy":0.5789473684,
        "MMLU_business_ethics":0.62,
        "MMLU_clinical_knowledge":0.6188679245,
        "MMLU_college_biology":0.5833333333,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.5,
        "MMLU_college_mathematics":0.4,
        "MMLU_college_medicine":0.5260115607,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.514893617,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.4965517241,
        "MMLU_elementary_mathematics":0.328042328,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.4,
        "MMLU_high_school_biology":0.6516129032,
        "MMLU_high_school_chemistry":0.3842364532,
        "MMLU_high_school_computer_science":0.61,
        "MMLU_high_school_european_history":0.7151515152,
        "MMLU_high_school_geography":0.7525252525,
        "MMLU_high_school_government_and_politics":0.8031088083,
        "MMLU_high_school_macroeconomics":0.5512820513,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.6218487395,
        "MMLU_high_school_physics":0.3708609272,
        "MMLU_high_school_psychology":0.7651376147,
        "MMLU_high_school_statistics":0.4305555556,
        "MMLU_high_school_us_history":0.7794117647,
        "MMLU_high_school_world_history":0.8185654008,
        "MMLU_human_aging":0.6860986547,
        "MMLU_human_sexuality":0.6335877863,
        "MMLU_international_law":0.7520661157,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.7055214724,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.8058252427,
        "MMLU_marketing":0.8632478632,
        "MMLU_medical_genetics":0.61,
        "MMLU_miscellaneous":0.7586206897,
        "MMLU_moral_disputes":0.6416184971,
        "MMLU_moral_scenarios":0.4111731844,
        "MMLU_nutrition":0.6013071895,
        "MMLU_philosophy":0.6816720257,
        "MMLU_prehistory":0.6635802469,
        "MMLU_professional_accounting":0.4539007092,
        "MMLU_professional_law":0.4491525424,
        "MMLU_professional_medicine":0.5477941176,
        "MMLU_professional_psychology":0.6029411765,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.5959183673,
        "MMLU_sociology":0.7711442786,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.7719298246
    },
    {
        "Model":"falcon-40b-openassistant-peft",
        "URL":"https:\/\/huggingface.co\/dfurman\/falcon-40b-openassistant-peft",
        "full_model_name":"dfurman\/falcon-40b-openassistant-peft",
        "Parameters":40.0,
        "MMLU_average":0.5777490654,
        "arc:challenge|25":0.5887372014,
        "hellaswag|10":0.661621191,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.5481481481,
        "MMLU_astronomy":0.5855263158,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.5924528302,
        "MMLU_college_biology":0.6666666667,
        "MMLU_college_chemistry":0.46,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.5433526012,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.64,
        "MMLU_conceptual_physics":0.4510638298,
        "MMLU_econometrics":0.3333333333,
        "MMLU_electrical_engineering":0.5310344828,
        "MMLU_elementary_mathematics":0.335978836,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.6806451613,
        "MMLU_high_school_chemistry":0.4482758621,
        "MMLU_high_school_computer_science":0.61,
        "MMLU_high_school_european_history":0.696969697,
        "MMLU_high_school_geography":0.7474747475,
        "MMLU_high_school_government_and_politics":0.7720207254,
        "MMLU_high_school_macroeconomics":0.5461538462,
        "MMLU_high_school_mathematics":0.3111111111,
        "MMLU_high_school_microeconomics":0.5630252101,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.776146789,
        "MMLU_high_school_statistics":0.4814814815,
        "MMLU_high_school_us_history":0.7254901961,
        "MMLU_high_school_world_history":0.6962025316,
        "MMLU_human_aging":0.7264573991,
        "MMLU_human_sexuality":0.7480916031,
        "MMLU_international_law":0.6859504132,
        "MMLU_jurisprudence":0.712962963,
        "MMLU_logical_fallacies":0.6871165644,
        "MMLU_machine_learning":0.2946428571,
        "MMLU_management":0.786407767,
        "MMLU_marketing":0.811965812,
        "MMLU_medical_genetics":0.67,
        "MMLU_miscellaneous":0.7573435504,
        "MMLU_moral_disputes":0.6502890173,
        "MMLU_moral_scenarios":0.3296089385,
        "MMLU_nutrition":0.6666666667,
        "MMLU_philosophy":0.6752411576,
        "MMLU_prehistory":0.6635802469,
        "MMLU_professional_accounting":0.4574468085,
        "MMLU_professional_law":0.4426336375,
        "MMLU_professional_medicine":0.6139705882,
        "MMLU_professional_psychology":0.5833333333,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.6775510204,
        "MMLU_sociology":0.8109452736,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.4759036145,
        "MMLU_world_religions":0.8362573099
    },
    {
        "Model":"OpenOrca-Nebula-7B",
        "URL":"https:\/\/huggingface.co\/Weyaxi\/OpenOrca-Nebula-7B",
        "full_model_name":"Weyaxi\/OpenOrca-Nebula-7B",
        "Parameters":7.0,
        "MMLU_average":0.5776959564,
        "arc:challenge|25":0.5529010239,
        "hellaswag|10":0.6283608843,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.5777777778,
        "MMLU_astronomy":0.5986842105,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.5773584906,
        "MMLU_college_biology":0.6597222222,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.612716763,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.4595744681,
        "MMLU_econometrics":0.3947368421,
        "MMLU_electrical_engineering":0.5310344828,
        "MMLU_elementary_mathematics":0.417989418,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.7129032258,
        "MMLU_high_school_chemistry":0.4729064039,
        "MMLU_high_school_computer_science":0.6,
        "MMLU_high_school_european_history":0.7151515152,
        "MMLU_high_school_geography":0.7626262626,
        "MMLU_high_school_government_and_politics":0.7823834197,
        "MMLU_high_school_macroeconomics":0.5769230769,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.5630252101,
        "MMLU_high_school_physics":0.3774834437,
        "MMLU_high_school_psychology":0.7798165138,
        "MMLU_high_school_statistics":0.3981481481,
        "MMLU_high_school_us_history":0.7450980392,
        "MMLU_high_school_world_history":0.7848101266,
        "MMLU_human_aging":0.6412556054,
        "MMLU_human_sexuality":0.7022900763,
        "MMLU_international_law":0.7190082645,
        "MMLU_jurisprudence":0.6481481481,
        "MMLU_logical_fallacies":0.6687116564,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.786407767,
        "MMLU_marketing":0.8162393162,
        "MMLU_medical_genetics":0.72,
        "MMLU_miscellaneous":0.7739463602,
        "MMLU_moral_disputes":0.6156069364,
        "MMLU_moral_scenarios":0.3553072626,
        "MMLU_nutrition":0.6307189542,
        "MMLU_philosophy":0.6463022508,
        "MMLU_prehistory":0.6759259259,
        "MMLU_professional_accounting":0.4290780142,
        "MMLU_professional_law":0.4302477184,
        "MMLU_professional_medicine":0.5220588235,
        "MMLU_professional_psychology":0.5980392157,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.5265306122,
        "MMLU_sociology":0.7711442786,
        "MMLU_us_foreign_policy":0.79,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.7602339181
    },
    {
        "Model":"GPT4-X-Alpasta-30b",
        "URL":"https:\/\/huggingface.co\/MetaIX\/GPT4-X-Alpasta-30b",
        "full_model_name":"MetaIX\/GPT4-X-Alpasta-30b",
        "Parameters":30.0,
        "MMLU_average":0.5771037992,
        "arc:challenge|25":0.6015358362,
        "hellaswag|10":0.6376219877,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5855263158,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.5924528302,
        "MMLU_college_biology":0.6458333333,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.51,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.5086705202,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.58,
        "MMLU_conceptual_physics":0.4936170213,
        "MMLU_econometrics":0.3421052632,
        "MMLU_electrical_engineering":0.4827586207,
        "MMLU_elementary_mathematics":0.3677248677,
        "MMLU_formal_logic":0.3015873016,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.6677419355,
        "MMLU_high_school_chemistry":0.4088669951,
        "MMLU_high_school_computer_science":0.64,
        "MMLU_high_school_european_history":0.703030303,
        "MMLU_high_school_geography":0.7272727273,
        "MMLU_high_school_government_and_politics":0.8186528497,
        "MMLU_high_school_macroeconomics":0.5512820513,
        "MMLU_high_school_mathematics":0.2925925926,
        "MMLU_high_school_microeconomics":0.6302521008,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.776146789,
        "MMLU_high_school_statistics":0.4675925926,
        "MMLU_high_school_us_history":0.7794117647,
        "MMLU_high_school_world_history":0.7890295359,
        "MMLU_human_aging":0.6771300448,
        "MMLU_human_sexuality":0.6641221374,
        "MMLU_international_law":0.7520661157,
        "MMLU_jurisprudence":0.6574074074,
        "MMLU_logical_fallacies":0.6625766871,
        "MMLU_machine_learning":0.4107142857,
        "MMLU_management":0.7572815534,
        "MMLU_marketing":0.8547008547,
        "MMLU_medical_genetics":0.64,
        "MMLU_miscellaneous":0.7573435504,
        "MMLU_moral_disputes":0.6445086705,
        "MMLU_moral_scenarios":0.4279329609,
        "MMLU_nutrition":0.6013071895,
        "MMLU_philosophy":0.6591639871,
        "MMLU_prehistory":0.6419753086,
        "MMLU_professional_accounting":0.4326241135,
        "MMLU_professional_law":0.4263363755,
        "MMLU_professional_medicine":0.5808823529,
        "MMLU_professional_psychology":0.6013071895,
        "MMLU_public_relations":0.5818181818,
        "MMLU_security_studies":0.6408163265,
        "MMLU_sociology":0.7562189055,
        "MMLU_us_foreign_policy":0.85,
        "MMLU_virology":0.5421686747,
        "MMLU_world_religions":0.7894736842
    },
    {
        "Model":"Llama-2-13b-orca-v1",
        "URL":"https:\/\/huggingface.co\/circulus\/Llama-2-13b-orca-v1",
        "full_model_name":"circulus\/Llama-2-13b-orca-v1",
        "Parameters":13.0,
        "MMLU_average":0.5770884693,
        "arc:challenge|25":0.5981228669,
        "hellaswag|10":0.6236805417,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.5263157895,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.6150943396,
        "MMLU_college_biology":0.6180555556,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.5375722543,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.74,
        "MMLU_conceptual_physics":0.4680851064,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.5448275862,
        "MMLU_elementary_mathematics":0.3412698413,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.6709677419,
        "MMLU_high_school_chemistry":0.4334975369,
        "MMLU_high_school_computer_science":0.6,
        "MMLU_high_school_european_history":0.703030303,
        "MMLU_high_school_geography":0.7474747475,
        "MMLU_high_school_government_and_politics":0.8238341969,
        "MMLU_high_school_macroeconomics":0.5743589744,
        "MMLU_high_school_mathematics":0.2925925926,
        "MMLU_high_school_microeconomics":0.5966386555,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.7596330275,
        "MMLU_high_school_statistics":0.4398148148,
        "MMLU_high_school_us_history":0.75,
        "MMLU_high_school_world_history":0.7510548523,
        "MMLU_human_aging":0.6726457399,
        "MMLU_human_sexuality":0.679389313,
        "MMLU_international_law":0.7685950413,
        "MMLU_jurisprudence":0.75,
        "MMLU_logical_fallacies":0.7055214724,
        "MMLU_machine_learning":0.4464285714,
        "MMLU_management":0.7669902913,
        "MMLU_marketing":0.8290598291,
        "MMLU_medical_genetics":0.67,
        "MMLU_miscellaneous":0.7739463602,
        "MMLU_moral_disputes":0.6445086705,
        "MMLU_moral_scenarios":0.4357541899,
        "MMLU_nutrition":0.6339869281,
        "MMLU_philosophy":0.6559485531,
        "MMLU_prehistory":0.6327160494,
        "MMLU_professional_accounting":0.4326241135,
        "MMLU_professional_law":0.4152542373,
        "MMLU_professional_medicine":0.5367647059,
        "MMLU_professional_psychology":0.5833333333,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6448979592,
        "MMLU_sociology":0.7263681592,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.7777777778
    },
    {
        "Model":"StableBeluga-13B",
        "URL":"https:\/\/huggingface.co\/stabilityai\/StableBeluga-13B",
        "full_model_name":"stabilityai\/StableBeluga-13B",
        "Parameters":13.0,
        "MMLU_average":0.5770884693,
        "arc:challenge|25":0.5981228669,
        "hellaswag|10":0.6236805417,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.5263157895,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.6150943396,
        "MMLU_college_biology":0.6180555556,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.5375722543,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.74,
        "MMLU_conceptual_physics":0.4680851064,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.5448275862,
        "MMLU_elementary_mathematics":0.3412698413,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.6709677419,
        "MMLU_high_school_chemistry":0.4334975369,
        "MMLU_high_school_computer_science":0.6,
        "MMLU_high_school_european_history":0.703030303,
        "MMLU_high_school_geography":0.7474747475,
        "MMLU_high_school_government_and_politics":0.8238341969,
        "MMLU_high_school_macroeconomics":0.5743589744,
        "MMLU_high_school_mathematics":0.2925925926,
        "MMLU_high_school_microeconomics":0.5966386555,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.7596330275,
        "MMLU_high_school_statistics":0.4398148148,
        "MMLU_high_school_us_history":0.75,
        "MMLU_high_school_world_history":0.7510548523,
        "MMLU_human_aging":0.6726457399,
        "MMLU_human_sexuality":0.679389313,
        "MMLU_international_law":0.7685950413,
        "MMLU_jurisprudence":0.75,
        "MMLU_logical_fallacies":0.7055214724,
        "MMLU_machine_learning":0.4464285714,
        "MMLU_management":0.7669902913,
        "MMLU_marketing":0.8290598291,
        "MMLU_medical_genetics":0.67,
        "MMLU_miscellaneous":0.7739463602,
        "MMLU_moral_disputes":0.6445086705,
        "MMLU_moral_scenarios":0.4357541899,
        "MMLU_nutrition":0.6339869281,
        "MMLU_philosophy":0.6559485531,
        "MMLU_prehistory":0.6327160494,
        "MMLU_professional_accounting":0.4326241135,
        "MMLU_professional_law":0.4152542373,
        "MMLU_professional_medicine":0.5367647059,
        "MMLU_professional_psychology":0.5833333333,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6448979592,
        "MMLU_sociology":0.7263681592,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.7777777778
    },
    {
        "Model":"llama-2-13B-ensemble-v3",
        "URL":"https:\/\/huggingface.co\/yeontaek\/llama-2-13B-ensemble-v3",
        "full_model_name":"yeontaek\/llama-2-13B-ensemble-v3",
        "Parameters":13.0,
        "MMLU_average":0.5767460206,
        "arc:challenge|25":0.5981228669,
        "hellaswag|10":0.6245767775,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.5263157895,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.6113207547,
        "MMLU_college_biology":0.6180555556,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.5317919075,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.75,
        "MMLU_conceptual_physics":0.4638297872,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.5448275862,
        "MMLU_elementary_mathematics":0.3386243386,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.6677419355,
        "MMLU_high_school_chemistry":0.4334975369,
        "MMLU_high_school_computer_science":0.6,
        "MMLU_high_school_european_history":0.703030303,
        "MMLU_high_school_geography":0.7474747475,
        "MMLU_high_school_government_and_politics":0.8238341969,
        "MMLU_high_school_macroeconomics":0.5717948718,
        "MMLU_high_school_mathematics":0.2888888889,
        "MMLU_high_school_microeconomics":0.5966386555,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.7651376147,
        "MMLU_high_school_statistics":0.4444444444,
        "MMLU_high_school_us_history":0.7549019608,
        "MMLU_high_school_world_history":0.7552742616,
        "MMLU_human_aging":0.6726457399,
        "MMLU_human_sexuality":0.679389313,
        "MMLU_international_law":0.7685950413,
        "MMLU_jurisprudence":0.75,
        "MMLU_logical_fallacies":0.7055214724,
        "MMLU_machine_learning":0.4464285714,
        "MMLU_management":0.7669902913,
        "MMLU_marketing":0.8290598291,
        "MMLU_medical_genetics":0.67,
        "MMLU_miscellaneous":0.7739463602,
        "MMLU_moral_disputes":0.6445086705,
        "MMLU_moral_scenarios":0.4324022346,
        "MMLU_nutrition":0.637254902,
        "MMLU_philosophy":0.6495176849,
        "MMLU_prehistory":0.6327160494,
        "MMLU_professional_accounting":0.4290780142,
        "MMLU_professional_law":0.4139504563,
        "MMLU_professional_medicine":0.5294117647,
        "MMLU_professional_psychology":0.5833333333,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.6448979592,
        "MMLU_sociology":0.7263681592,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4638554217,
        "MMLU_world_religions":0.7777777778
    },
    {
        "Model":"mistral_7b_norobots",
        "URL":"https:\/\/huggingface.co\/qblocks\/mistral_7b_norobots",
        "full_model_name":"qblocks\/mistral_7b_norobots",
        "Parameters":7.0,
        "MMLU_average":0.5766207833,
        "arc:challenge|25":0.5460750853,
        "hellaswag|10":0.6029675363,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.5481481481,
        "MMLU_astronomy":0.5723684211,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.6452830189,
        "MMLU_college_biology":0.6597222222,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.56,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5317919075,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.78,
        "MMLU_conceptual_physics":0.4978723404,
        "MMLU_econometrics":0.3859649123,
        "MMLU_electrical_engineering":0.5586206897,
        "MMLU_elementary_mathematics":0.3915343915,
        "MMLU_formal_logic":0.3412698413,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.7,
        "MMLU_high_school_chemistry":0.4433497537,
        "MMLU_high_school_computer_science":0.63,
        "MMLU_high_school_european_history":0.703030303,
        "MMLU_high_school_geography":0.7878787879,
        "MMLU_high_school_government_and_politics":0.792746114,
        "MMLU_high_school_macroeconomics":0.5461538462,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.5840336134,
        "MMLU_high_school_physics":0.3841059603,
        "MMLU_high_school_psychology":0.7302752294,
        "MMLU_high_school_statistics":0.4074074074,
        "MMLU_high_school_us_history":0.7450980392,
        "MMLU_high_school_world_history":0.7172995781,
        "MMLU_human_aging":0.6502242152,
        "MMLU_human_sexuality":0.679389313,
        "MMLU_international_law":0.7190082645,
        "MMLU_jurisprudence":0.6388888889,
        "MMLU_logical_fallacies":0.6809815951,
        "MMLU_machine_learning":0.4464285714,
        "MMLU_management":0.7572815534,
        "MMLU_marketing":0.8247863248,
        "MMLU_medical_genetics":0.66,
        "MMLU_miscellaneous":0.7624521073,
        "MMLU_moral_disputes":0.6184971098,
        "MMLU_moral_scenarios":0.243575419,
        "MMLU_nutrition":0.6274509804,
        "MMLU_philosophy":0.6720257235,
        "MMLU_prehistory":0.6450617284,
        "MMLU_professional_accounting":0.4468085106,
        "MMLU_professional_law":0.3989569752,
        "MMLU_professional_medicine":0.5772058824,
        "MMLU_professional_psychology":0.5964052288,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.6204081633,
        "MMLU_sociology":0.7810945274,
        "MMLU_us_foreign_policy":0.74,
        "MMLU_virology":0.4759036145,
        "MMLU_world_religions":0.783625731
    },
    {
        "Model":"llama-2-13B-ensemble-v1",
        "URL":"https:\/\/huggingface.co\/yeontaek\/llama-2-13B-ensemble-v1",
        "full_model_name":"yeontaek\/llama-2-13B-ensemble-v1",
        "Parameters":13.0,
        "MMLU_average":0.5759081756,
        "arc:challenge|25":0.5998293515,
        "hellaswag|10":0.6264688309,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.5394736842,
        "MMLU_business_ethics":0.58,
        "MMLU_clinical_knowledge":0.6075471698,
        "MMLU_college_biology":0.6319444444,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.549132948,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.75,
        "MMLU_conceptual_physics":0.4638297872,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.5448275862,
        "MMLU_elementary_mathematics":0.3306878307,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.6580645161,
        "MMLU_high_school_chemistry":0.4285714286,
        "MMLU_high_school_computer_science":0.59,
        "MMLU_high_school_european_history":0.7090909091,
        "MMLU_high_school_geography":0.7424242424,
        "MMLU_high_school_government_and_politics":0.8238341969,
        "MMLU_high_school_macroeconomics":0.5692307692,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.6092436975,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.7651376147,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.7598039216,
        "MMLU_high_school_world_history":0.7594936709,
        "MMLU_human_aging":0.6726457399,
        "MMLU_human_sexuality":0.679389313,
        "MMLU_international_law":0.7768595041,
        "MMLU_jurisprudence":0.75,
        "MMLU_logical_fallacies":0.7055214724,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.7669902913,
        "MMLU_marketing":0.8290598291,
        "MMLU_medical_genetics":0.67,
        "MMLU_miscellaneous":0.7765006386,
        "MMLU_moral_disputes":0.6531791908,
        "MMLU_moral_scenarios":0.4324022346,
        "MMLU_nutrition":0.6307189542,
        "MMLU_philosophy":0.652733119,
        "MMLU_prehistory":0.6265432099,
        "MMLU_professional_accounting":0.4219858156,
        "MMLU_professional_law":0.4178617992,
        "MMLU_professional_medicine":0.5330882353,
        "MMLU_professional_psychology":0.5849673203,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6367346939,
        "MMLU_sociology":0.7263681592,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.783625731
    },
    {
        "Model":"alpaca-cleaned-llama-30b-bf16",
        "URL":"https:\/\/huggingface.co\/dsvv-cair\/alpaca-cleaned-llama-30b-bf16",
        "full_model_name":"dsvv-cair\/alpaca-cleaned-llama-30b-bf16",
        "Parameters":30.0,
        "MMLU_average":0.5752229572,
        "arc:challenge|25":0.5802047782,
        "hellaswag|10":0.6389165505,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.6513157895,
        "MMLU_business_ethics":0.61,
        "MMLU_clinical_knowledge":0.5962264151,
        "MMLU_college_biology":0.6041666667,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.4855491329,
        "MMLU_college_physics":0.3529411765,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.4553191489,
        "MMLU_econometrics":0.3684210526,
        "MMLU_electrical_engineering":0.5103448276,
        "MMLU_elementary_mathematics":0.3703703704,
        "MMLU_formal_logic":0.3412698413,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.7161290323,
        "MMLU_high_school_chemistry":0.4384236453,
        "MMLU_high_school_computer_science":0.53,
        "MMLU_high_school_european_history":0.6909090909,
        "MMLU_high_school_geography":0.7373737374,
        "MMLU_high_school_government_and_politics":0.8082901554,
        "MMLU_high_school_macroeconomics":0.558974359,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.5546218487,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7541284404,
        "MMLU_high_school_statistics":0.462962963,
        "MMLU_high_school_us_history":0.7892156863,
        "MMLU_high_school_world_history":0.7848101266,
        "MMLU_human_aging":0.6278026906,
        "MMLU_human_sexuality":0.641221374,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.6809815951,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.8247863248,
        "MMLU_medical_genetics":0.63,
        "MMLU_miscellaneous":0.7522349936,
        "MMLU_moral_disputes":0.6647398844,
        "MMLU_moral_scenarios":0.3944134078,
        "MMLU_nutrition":0.6078431373,
        "MMLU_philosophy":0.6784565916,
        "MMLU_prehistory":0.6851851852,
        "MMLU_professional_accounting":0.4609929078,
        "MMLU_professional_law":0.4295958279,
        "MMLU_professional_medicine":0.5661764706,
        "MMLU_professional_psychology":0.5866013072,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.6040816327,
        "MMLU_sociology":0.7512437811,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.5180722892,
        "MMLU_world_religions":0.7719298246
    },
    {
        "Model":"OpenOrcaxOpenChat-Preview2-13B",
        "URL":"https:\/\/huggingface.co\/Open-Orca\/OpenOrcaxOpenChat-Preview2-13B",
        "full_model_name":"Open-Orca\/OpenOrcaxOpenChat-Preview2-13B",
        "Parameters":13.0,
        "MMLU_average":0.575144336,
        "arc:challenge|25":0.5964163823,
        "hellaswag|10":0.620294762,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.5723684211,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.641509434,
        "MMLU_college_biology":0.6458333333,
        "MMLU_college_chemistry":0.45,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.5606936416,
        "MMLU_college_physics":0.3431372549,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.485106383,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.524137931,
        "MMLU_elementary_mathematics":0.3412698413,
        "MMLU_formal_logic":0.3888888889,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.7064516129,
        "MMLU_high_school_chemistry":0.4827586207,
        "MMLU_high_school_computer_science":0.54,
        "MMLU_high_school_european_history":0.7212121212,
        "MMLU_high_school_geography":0.7323232323,
        "MMLU_high_school_government_and_politics":0.7979274611,
        "MMLU_high_school_macroeconomics":0.5564102564,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.6344537815,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.7614678899,
        "MMLU_high_school_statistics":0.4166666667,
        "MMLU_high_school_us_history":0.7745098039,
        "MMLU_high_school_world_history":0.7510548523,
        "MMLU_human_aging":0.66367713,
        "MMLU_human_sexuality":0.679389313,
        "MMLU_international_law":0.7190082645,
        "MMLU_jurisprudence":0.7592592593,
        "MMLU_logical_fallacies":0.6564417178,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.7572815534,
        "MMLU_marketing":0.8162393162,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.7752234994,
        "MMLU_moral_disputes":0.6416184971,
        "MMLU_moral_scenarios":0.4592178771,
        "MMLU_nutrition":0.6274509804,
        "MMLU_philosophy":0.6495176849,
        "MMLU_prehistory":0.6512345679,
        "MMLU_professional_accounting":0.4219858156,
        "MMLU_professional_law":0.425684485,
        "MMLU_professional_medicine":0.5698529412,
        "MMLU_professional_psychology":0.5849673203,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.6897959184,
        "MMLU_sociology":0.7611940299,
        "MMLU_us_foreign_policy":0.81,
        "MMLU_virology":0.5,
        "MMLU_world_religions":0.7660818713
    },
    {
        "Model":"duplicitous-mammal-13b",
        "URL":"https:\/\/huggingface.co\/chargoddard\/duplicitous-mammal-13b",
        "full_model_name":"chargoddard\/duplicitous-mammal-13b",
        "Parameters":13.0,
        "MMLU_average":0.5749851064,
        "arc:challenge|25":0.593003413,
        "hellaswag|10":0.6456881099,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.5263157895,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.6037735849,
        "MMLU_college_biology":0.6388888889,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.53,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.5202312139,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.4680851064,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.5379310345,
        "MMLU_elementary_mathematics":0.3518518519,
        "MMLU_formal_logic":0.4126984127,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.6451612903,
        "MMLU_high_school_chemistry":0.4532019704,
        "MMLU_high_school_computer_science":0.56,
        "MMLU_high_school_european_history":0.703030303,
        "MMLU_high_school_geography":0.702020202,
        "MMLU_high_school_government_and_politics":0.8445595855,
        "MMLU_high_school_macroeconomics":0.5564102564,
        "MMLU_high_school_mathematics":0.3111111111,
        "MMLU_high_school_microeconomics":0.5588235294,
        "MMLU_high_school_physics":0.3841059603,
        "MMLU_high_school_psychology":0.7633027523,
        "MMLU_high_school_statistics":0.4351851852,
        "MMLU_high_school_us_history":0.7941176471,
        "MMLU_high_school_world_history":0.7721518987,
        "MMLU_human_aging":0.6995515695,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.75,
        "MMLU_logical_fallacies":0.6503067485,
        "MMLU_machine_learning":0.4375,
        "MMLU_management":0.6893203883,
        "MMLU_marketing":0.8290598291,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.7803320562,
        "MMLU_moral_disputes":0.6502890173,
        "MMLU_moral_scenarios":0.4815642458,
        "MMLU_nutrition":0.6241830065,
        "MMLU_philosophy":0.6463022508,
        "MMLU_prehistory":0.6512345679,
        "MMLU_professional_accounting":0.414893617,
        "MMLU_professional_law":0.4465449804,
        "MMLU_professional_medicine":0.5404411765,
        "MMLU_professional_psychology":0.591503268,
        "MMLU_public_relations":0.6909090909,
        "MMLU_security_studies":0.6612244898,
        "MMLU_sociology":0.7313432836,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"manticore-30b-chat-pyg-alpha",
        "URL":"https:\/\/huggingface.co\/openaccess-ai-collective\/manticore-30b-chat-pyg-alpha",
        "full_model_name":"openaccess-ai-collective\/manticore-30b-chat-pyg-alpha",
        "Parameters":30.0,
        "MMLU_average":0.5749406488,
        "arc:challenge|25":0.614334471,
        "hellaswag|10":0.6500697072,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.6118421053,
        "MMLU_business_ethics":0.6,
        "MMLU_clinical_knowledge":0.5811320755,
        "MMLU_college_biology":0.5972222222,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.4855491329,
        "MMLU_college_physics":0.362745098,
        "MMLU_computer_security":0.64,
        "MMLU_conceptual_physics":0.4468085106,
        "MMLU_econometrics":0.3333333333,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.3333333333,
        "MMLU_formal_logic":0.4047619048,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.6967741935,
        "MMLU_high_school_chemistry":0.3694581281,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.696969697,
        "MMLU_high_school_geography":0.7424242424,
        "MMLU_high_school_government_and_politics":0.829015544,
        "MMLU_high_school_macroeconomics":0.5948717949,
        "MMLU_high_school_mathematics":0.2925925926,
        "MMLU_high_school_microeconomics":0.5840336134,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7633027523,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.7647058824,
        "MMLU_high_school_world_history":0.7890295359,
        "MMLU_human_aging":0.6591928251,
        "MMLU_human_sexuality":0.5877862595,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.6851851852,
        "MMLU_logical_fallacies":0.6871165644,
        "MMLU_machine_learning":0.4642857143,
        "MMLU_management":0.7475728155,
        "MMLU_marketing":0.8547008547,
        "MMLU_medical_genetics":0.64,
        "MMLU_miscellaneous":0.7713920817,
        "MMLU_moral_disputes":0.6271676301,
        "MMLU_moral_scenarios":0.4346368715,
        "MMLU_nutrition":0.614379085,
        "MMLU_philosophy":0.6881028939,
        "MMLU_prehistory":0.6419753086,
        "MMLU_professional_accounting":0.414893617,
        "MMLU_professional_law":0.4354628422,
        "MMLU_professional_medicine":0.5882352941,
        "MMLU_professional_psychology":0.6209150327,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.6612244898,
        "MMLU_sociology":0.7960199005,
        "MMLU_us_foreign_policy":0.81,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.8011695906
    },
    {
        "Model":"viwaai-30b_v4",
        "URL":"https:\/\/huggingface.co\/kajdun\/viwaai-30b_v4",
        "full_model_name":"kajdun\/viwaai-30b_v4",
        "Parameters":30.0,
        "MMLU_average":0.5746519095,
        "arc:challenge|25":0.6160409556,
        "hellaswag|10":0.6479784903,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.6184210526,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.5735849057,
        "MMLU_college_biology":0.5972222222,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.5028901734,
        "MMLU_college_physics":0.3431372549,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.4595744681,
        "MMLU_econometrics":0.3333333333,
        "MMLU_electrical_engineering":0.4551724138,
        "MMLU_elementary_mathematics":0.3253968254,
        "MMLU_formal_logic":0.3968253968,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.7,
        "MMLU_high_school_chemistry":0.3743842365,
        "MMLU_high_school_computer_science":0.57,
        "MMLU_high_school_european_history":0.7333333333,
        "MMLU_high_school_geography":0.7474747475,
        "MMLU_high_school_government_and_politics":0.8186528497,
        "MMLU_high_school_macroeconomics":0.5820512821,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.6050420168,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.7651376147,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.75,
        "MMLU_high_school_world_history":0.7974683544,
        "MMLU_human_aging":0.6591928251,
        "MMLU_human_sexuality":0.5954198473,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.6944444444,
        "MMLU_logical_fallacies":0.6809815951,
        "MMLU_machine_learning":0.4553571429,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.8675213675,
        "MMLU_medical_genetics":0.67,
        "MMLU_miscellaneous":0.7662835249,
        "MMLU_moral_disputes":0.6271676301,
        "MMLU_moral_scenarios":0.4223463687,
        "MMLU_nutrition":0.5947712418,
        "MMLU_philosophy":0.6848874598,
        "MMLU_prehistory":0.6450617284,
        "MMLU_professional_accounting":0.4113475177,
        "MMLU_professional_law":0.4400260756,
        "MMLU_professional_medicine":0.5772058824,
        "MMLU_professional_psychology":0.6192810458,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.6693877551,
        "MMLU_sociology":0.7810945274,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.783625731
    },
    {
        "Model":"ennodata-raw-pankajmathur-13b-peft",
        "URL":"https:\/\/huggingface.co\/Enno-Ai\/ennodata-raw-pankajmathur-13b-peft",
        "full_model_name":"Enno-Ai\/ennodata-raw-pankajmathur-13b-peft",
        "Parameters":13.0,
        "MMLU_average":0.574444993,
        "arc:challenge|25":0.5827645051,
        "hellaswag|10":0.6224855606,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.5037037037,
        "MMLU_astronomy":0.5526315789,
        "MMLU_business_ethics":0.58,
        "MMLU_clinical_knowledge":0.6037735849,
        "MMLU_college_biology":0.625,
        "MMLU_college_chemistry":0.36,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.5317919075,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.4936170213,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.4965517241,
        "MMLU_elementary_mathematics":0.2962962963,
        "MMLU_formal_logic":0.4523809524,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.6225806452,
        "MMLU_high_school_chemistry":0.4482758621,
        "MMLU_high_school_computer_science":0.61,
        "MMLU_high_school_european_history":0.7090909091,
        "MMLU_high_school_geography":0.7525252525,
        "MMLU_high_school_government_and_politics":0.829015544,
        "MMLU_high_school_macroeconomics":0.6025641026,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.6050420168,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.7871559633,
        "MMLU_high_school_statistics":0.4953703704,
        "MMLU_high_school_us_history":0.7843137255,
        "MMLU_high_school_world_history":0.7890295359,
        "MMLU_human_aging":0.6547085202,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.694214876,
        "MMLU_jurisprudence":0.6944444444,
        "MMLU_logical_fallacies":0.7116564417,
        "MMLU_machine_learning":0.4107142857,
        "MMLU_management":0.6990291262,
        "MMLU_marketing":0.8247863248,
        "MMLU_medical_genetics":0.64,
        "MMLU_miscellaneous":0.7701149425,
        "MMLU_moral_disputes":0.6416184971,
        "MMLU_moral_scenarios":0.4681564246,
        "MMLU_nutrition":0.6111111111,
        "MMLU_philosophy":0.6591639871,
        "MMLU_prehistory":0.6666666667,
        "MMLU_professional_accounting":0.4219858156,
        "MMLU_professional_law":0.4478487614,
        "MMLU_professional_medicine":0.5625,
        "MMLU_professional_psychology":0.5980392157,
        "MMLU_public_relations":0.6909090909,
        "MMLU_security_studies":0.5836734694,
        "MMLU_sociology":0.6368159204,
        "MMLU_us_foreign_policy":0.81,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.783625731
    },
    {
        "Model":"llama-2-13B-ensemble-v6",
        "URL":"https:\/\/huggingface.co\/yeontaek\/llama-2-13B-ensemble-v6",
        "full_model_name":"yeontaek\/llama-2-13B-ensemble-v6",
        "Parameters":13.0,
        "MMLU_average":0.5738328353,
        "arc:challenge|25":0.5034129693,
        "hellaswag|10":0.6101374228,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.5259259259,
        "MMLU_astronomy":0.5855263158,
        "MMLU_business_ethics":0.6,
        "MMLU_clinical_knowledge":0.6301886792,
        "MMLU_college_biology":0.6666666667,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.5202312139,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4765957447,
        "MMLU_econometrics":0.3421052632,
        "MMLU_electrical_engineering":0.4965517241,
        "MMLU_elementary_mathematics":0.3544973545,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.6838709677,
        "MMLU_high_school_chemistry":0.4187192118,
        "MMLU_high_school_computer_science":0.56,
        "MMLU_high_school_european_history":0.7212121212,
        "MMLU_high_school_geography":0.7626262626,
        "MMLU_high_school_government_and_politics":0.8393782383,
        "MMLU_high_school_macroeconomics":0.5846153846,
        "MMLU_high_school_mathematics":0.3407407407,
        "MMLU_high_school_microeconomics":0.5546218487,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.7743119266,
        "MMLU_high_school_statistics":0.4074074074,
        "MMLU_high_school_us_history":0.8284313725,
        "MMLU_high_school_world_history":0.776371308,
        "MMLU_human_aging":0.6278026906,
        "MMLU_human_sexuality":0.6641221374,
        "MMLU_international_law":0.694214876,
        "MMLU_jurisprudence":0.7592592593,
        "MMLU_logical_fallacies":0.7239263804,
        "MMLU_machine_learning":0.4107142857,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.8162393162,
        "MMLU_medical_genetics":0.53,
        "MMLU_miscellaneous":0.7675606641,
        "MMLU_moral_disputes":0.6358381503,
        "MMLU_moral_scenarios":0.3810055866,
        "MMLU_nutrition":0.6078431373,
        "MMLU_philosophy":0.6688102894,
        "MMLU_prehistory":0.6820987654,
        "MMLU_professional_accounting":0.4539007092,
        "MMLU_professional_law":0.4413298566,
        "MMLU_professional_medicine":0.5441176471,
        "MMLU_professional_psychology":0.5816993464,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.6408163265,
        "MMLU_sociology":0.6965174129,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4397590361,
        "MMLU_world_religions":0.8070175439
    },
    {
        "Model":"airoboros-33b-2.1",
        "URL":"https:\/\/huggingface.co\/jondurbin\/airoboros-33b-2.1",
        "full_model_name":"jondurbin\/airoboros-33b-2.1",
        "Parameters":33.0,
        "MMLU_average":0.5737181085,
        "arc:challenge|25":0.6254266212,
        "hellaswag|10":0.6523600876,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5855263158,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.5924528302,
        "MMLU_college_biology":0.5902777778,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.4913294798,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4382978723,
        "MMLU_econometrics":0.3859649123,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.3201058201,
        "MMLU_formal_logic":0.4206349206,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.6870967742,
        "MMLU_high_school_chemistry":0.4187192118,
        "MMLU_high_school_computer_science":0.58,
        "MMLU_high_school_european_history":0.6848484848,
        "MMLU_high_school_geography":0.7424242424,
        "MMLU_high_school_government_and_politics":0.8134715026,
        "MMLU_high_school_macroeconomics":0.5538461538,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.5798319328,
        "MMLU_high_school_physics":0.357615894,
        "MMLU_high_school_psychology":0.7706422018,
        "MMLU_high_school_statistics":0.4305555556,
        "MMLU_high_school_us_history":0.7352941176,
        "MMLU_high_school_world_history":0.7890295359,
        "MMLU_human_aging":0.6233183857,
        "MMLU_human_sexuality":0.6259541985,
        "MMLU_international_law":0.8181818182,
        "MMLU_jurisprudence":0.6851851852,
        "MMLU_logical_fallacies":0.6809815951,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.8290598291,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.7522349936,
        "MMLU_moral_disputes":0.676300578,
        "MMLU_moral_scenarios":0.3765363128,
        "MMLU_nutrition":0.5980392157,
        "MMLU_philosophy":0.6752411576,
        "MMLU_prehistory":0.6512345679,
        "MMLU_professional_accounting":0.4255319149,
        "MMLU_professional_law":0.4439374185,
        "MMLU_professional_medicine":0.5588235294,
        "MMLU_professional_psychology":0.612745098,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.6530612245,
        "MMLU_sociology":0.7810945274,
        "MMLU_us_foreign_policy":0.81,
        "MMLU_virology":0.5060240964,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"Zephyrus-L1-33B",
        "URL":"https:\/\/huggingface.co\/Sao10K\/Zephyrus-L1-33B",
        "full_model_name":"Sao10K\/Zephyrus-L1-33B",
        "Parameters":33.0,
        "MMLU_average":0.5736881519,
        "arc:challenge|25":0.6126279863,
        "hellaswag|10":0.646086437,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.5259259259,
        "MMLU_astronomy":0.6184210526,
        "MMLU_business_ethics":0.58,
        "MMLU_clinical_knowledge":0.5509433962,
        "MMLU_college_biology":0.7083333333,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.5144508671,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.67,
        "MMLU_conceptual_physics":0.5404255319,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.3465608466,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.6516129032,
        "MMLU_high_school_chemistry":0.4532019704,
        "MMLU_high_school_computer_science":0.62,
        "MMLU_high_school_european_history":0.696969697,
        "MMLU_high_school_geography":0.7373737374,
        "MMLU_high_school_government_and_politics":0.792746114,
        "MMLU_high_school_macroeconomics":0.5717948718,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.6092436975,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.7706422018,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.7794117647,
        "MMLU_high_school_world_history":0.7721518987,
        "MMLU_human_aging":0.6278026906,
        "MMLU_human_sexuality":0.6030534351,
        "MMLU_international_law":0.7603305785,
        "MMLU_jurisprudence":0.6666666667,
        "MMLU_logical_fallacies":0.6871165644,
        "MMLU_machine_learning":0.4107142857,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.8333333333,
        "MMLU_medical_genetics":0.62,
        "MMLU_miscellaneous":0.7662835249,
        "MMLU_moral_disputes":0.6473988439,
        "MMLU_moral_scenarios":0.3832402235,
        "MMLU_nutrition":0.6176470588,
        "MMLU_philosophy":0.6655948553,
        "MMLU_prehistory":0.6327160494,
        "MMLU_professional_accounting":0.4361702128,
        "MMLU_professional_law":0.4400260756,
        "MMLU_professional_medicine":0.5735294118,
        "MMLU_professional_psychology":0.5947712418,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.5551020408,
        "MMLU_sociology":0.7313432836,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4939759036,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"MXLewd-L2-20B",
        "URL":"https:\/\/huggingface.co\/Undi95\/MXLewd-L2-20B",
        "full_model_name":"Undi95\/MXLewd-L2-20B",
        "Parameters":20.0,
        "MMLU_average":0.5736331156,
        "arc:challenge|25":0.5938566553,
        "hellaswag|10":0.6700856403,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.5460526316,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.6339622642,
        "MMLU_college_biology":0.6736111111,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.4,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.5317919075,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.4765957447,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.524137931,
        "MMLU_elementary_mathematics":0.3492063492,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.6677419355,
        "MMLU_high_school_chemistry":0.4827586207,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.6848484848,
        "MMLU_high_school_geography":0.7222222222,
        "MMLU_high_school_government_and_politics":0.7979274611,
        "MMLU_high_school_macroeconomics":0.5641025641,
        "MMLU_high_school_mathematics":0.3407407407,
        "MMLU_high_school_microeconomics":0.6134453782,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.7449541284,
        "MMLU_high_school_statistics":0.5,
        "MMLU_high_school_us_history":0.7647058824,
        "MMLU_high_school_world_history":0.7637130802,
        "MMLU_human_aging":0.6860986547,
        "MMLU_human_sexuality":0.6259541985,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.6809815951,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.7087378641,
        "MMLU_marketing":0.8418803419,
        "MMLU_medical_genetics":0.63,
        "MMLU_miscellaneous":0.7726692209,
        "MMLU_moral_disputes":0.6647398844,
        "MMLU_moral_scenarios":0.4100558659,
        "MMLU_nutrition":0.6307189542,
        "MMLU_philosophy":0.6720257235,
        "MMLU_prehistory":0.6944444444,
        "MMLU_professional_accounting":0.4468085106,
        "MMLU_professional_law":0.4308996089,
        "MMLU_professional_medicine":0.5992647059,
        "MMLU_professional_psychology":0.5996732026,
        "MMLU_public_relations":0.5454545455,
        "MMLU_security_studies":0.6367346939,
        "MMLU_sociology":0.7661691542,
        "MMLU_us_foreign_policy":0.86,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.7485380117
    },
    {
        "Model":"model_007_13b_v2",
        "URL":"https:\/\/huggingface.co\/psmathur\/model_007_13b_v2",
        "full_model_name":"psmathur\/model_007_13b_v2",
        "Parameters":13.0,
        "MMLU_average":0.5731579597,
        "arc:challenge|25":0.5802047782,
        "hellaswag|10":0.6255725951,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.5657894737,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.5886792453,
        "MMLU_college_biology":0.6458333333,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.5144508671,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.62,
        "MMLU_conceptual_physics":0.4638297872,
        "MMLU_econometrics":0.3333333333,
        "MMLU_electrical_engineering":0.4827586207,
        "MMLU_elementary_mathematics":0.3121693122,
        "MMLU_formal_logic":0.4444444444,
        "MMLU_global_facts":0.42,
        "MMLU_high_school_biology":0.6322580645,
        "MMLU_high_school_chemistry":0.4236453202,
        "MMLU_high_school_computer_science":0.57,
        "MMLU_high_school_european_history":0.7090909091,
        "MMLU_high_school_geography":0.7373737374,
        "MMLU_high_school_government_and_politics":0.829015544,
        "MMLU_high_school_macroeconomics":0.5897435897,
        "MMLU_high_school_mathematics":0.3407407407,
        "MMLU_high_school_microeconomics":0.5798319328,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.7816513761,
        "MMLU_high_school_statistics":0.4861111111,
        "MMLU_high_school_us_history":0.7892156863,
        "MMLU_high_school_world_history":0.776371308,
        "MMLU_human_aging":0.668161435,
        "MMLU_human_sexuality":0.6183206107,
        "MMLU_international_law":0.6776859504,
        "MMLU_jurisprudence":0.7407407407,
        "MMLU_logical_fallacies":0.6932515337,
        "MMLU_machine_learning":0.4017857143,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.8247863248,
        "MMLU_medical_genetics":0.64,
        "MMLU_miscellaneous":0.7765006386,
        "MMLU_moral_disputes":0.6329479769,
        "MMLU_moral_scenarios":0.4815642458,
        "MMLU_nutrition":0.614379085,
        "MMLU_philosophy":0.6655948553,
        "MMLU_prehistory":0.6913580247,
        "MMLU_professional_accounting":0.4539007092,
        "MMLU_professional_law":0.462190352,
        "MMLU_professional_medicine":0.5808823529,
        "MMLU_professional_psychology":0.612745098,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.6,
        "MMLU_sociology":0.5572139303,
        "MMLU_us_foreign_policy":0.78,
        "MMLU_virology":0.4638554217,
        "MMLU_world_religions":0.7894736842
    },
    {
        "Model":"test-help-steer-filtered-orig",
        "URL":"https:\/\/huggingface.co\/Weyaxi\/test-help-steer-filtered-orig",
        "full_model_name":"Weyaxi\/test-help-steer-filtered-orig",
        "Parameters":null,
        "MMLU_average":0.5723883106,
        "arc:challenge|25":0.5392491468,
        "hellaswag|10":0.6126269667,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.5259259259,
        "MMLU_astronomy":0.6381578947,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.6566037736,
        "MMLU_college_biology":0.6597222222,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.4,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5780346821,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4936170213,
        "MMLU_econometrics":0.4122807018,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.3492063492,
        "MMLU_formal_logic":0.3888888889,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.6967741935,
        "MMLU_high_school_chemistry":0.4482758621,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.696969697,
        "MMLU_high_school_geography":0.7222222222,
        "MMLU_high_school_government_and_politics":0.792746114,
        "MMLU_high_school_macroeconomics":0.5487179487,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.6260504202,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.7596330275,
        "MMLU_high_school_statistics":0.4768518519,
        "MMLU_high_school_us_history":0.7303921569,
        "MMLU_high_school_world_history":0.7088607595,
        "MMLU_human_aging":0.6502242152,
        "MMLU_human_sexuality":0.641221374,
        "MMLU_international_law":0.7520661157,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.6993865031,
        "MMLU_machine_learning":0.4821428571,
        "MMLU_management":0.7669902913,
        "MMLU_marketing":0.7905982906,
        "MMLU_medical_genetics":0.68,
        "MMLU_miscellaneous":0.7803320562,
        "MMLU_moral_disputes":0.612716763,
        "MMLU_moral_scenarios":0.3094972067,
        "MMLU_nutrition":0.6078431373,
        "MMLU_philosophy":0.691318328,
        "MMLU_prehistory":0.6882716049,
        "MMLU_professional_accounting":0.4397163121,
        "MMLU_professional_law":0.4165580183,
        "MMLU_professional_medicine":0.5698529412,
        "MMLU_professional_psychology":0.6078431373,
        "MMLU_public_relations":0.5727272727,
        "MMLU_security_studies":0.5387755102,
        "MMLU_sociology":0.7711442786,
        "MMLU_us_foreign_policy":0.81,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.8187134503
    },
    {
        "Model":"Llama-2-13b-FINETUNE4",
        "URL":"https:\/\/huggingface.co\/wei123602\/Llama-2-13b-FINETUNE4",
        "full_model_name":"wei123602\/Llama-2-13b-FINETUNE4",
        "Parameters":13.0,
        "MMLU_average":0.5720748904,
        "arc:challenge|25":0.5401023891,
        "hellaswag|10":0.6098386776,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.6052631579,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.6,
        "MMLU_college_biology":0.5486111111,
        "MMLU_college_chemistry":0.47,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.5780346821,
        "MMLU_college_physics":0.4117647059,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.4255319149,
        "MMLU_econometrics":0.350877193,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.335978836,
        "MMLU_formal_logic":0.3968253968,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.6322580645,
        "MMLU_high_school_chemistry":0.4334975369,
        "MMLU_high_school_computer_science":0.61,
        "MMLU_high_school_european_history":0.7272727273,
        "MMLU_high_school_geography":0.7070707071,
        "MMLU_high_school_government_and_politics":0.7875647668,
        "MMLU_high_school_macroeconomics":0.6,
        "MMLU_high_school_mathematics":0.3555555556,
        "MMLU_high_school_microeconomics":0.6134453782,
        "MMLU_high_school_physics":0.3708609272,
        "MMLU_high_school_psychology":0.776146789,
        "MMLU_high_school_statistics":0.462962963,
        "MMLU_high_school_us_history":0.7843137255,
        "MMLU_high_school_world_history":0.7890295359,
        "MMLU_human_aging":0.6502242152,
        "MMLU_human_sexuality":0.5801526718,
        "MMLU_international_law":0.7685950413,
        "MMLU_jurisprudence":0.7685185185,
        "MMLU_logical_fallacies":0.7116564417,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.8034188034,
        "MMLU_medical_genetics":0.63,
        "MMLU_miscellaneous":0.7765006386,
        "MMLU_moral_disputes":0.5895953757,
        "MMLU_moral_scenarios":0.4480446927,
        "MMLU_nutrition":0.5751633987,
        "MMLU_philosophy":0.6591639871,
        "MMLU_prehistory":0.6574074074,
        "MMLU_professional_accounting":0.4964539007,
        "MMLU_professional_law":0.4413298566,
        "MMLU_professional_medicine":0.5772058824,
        "MMLU_professional_psychology":0.5833333333,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.5469387755,
        "MMLU_sociology":0.7213930348,
        "MMLU_us_foreign_policy":0.78,
        "MMLU_virology":0.4096385542,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"Platypus2-13B-LoRa-v2",
        "URL":"https:\/\/huggingface.co\/yeontaek\/Platypus2-13B-LoRa-v2",
        "full_model_name":"yeontaek\/Platypus2-13B-LoRa-v2",
        "Parameters":13.0,
        "MMLU_average":0.571460784,
        "arc:challenge|25":0.5563139932,
        "hellaswag|10":0.6179047998,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.5986842105,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.6150943396,
        "MMLU_college_biology":0.6111111111,
        "MMLU_college_chemistry":0.49,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.5722543353,
        "MMLU_college_physics":0.3431372549,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.429787234,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.4482758621,
        "MMLU_elementary_mathematics":0.3386243386,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.6709677419,
        "MMLU_high_school_chemistry":0.4581280788,
        "MMLU_high_school_computer_science":0.59,
        "MMLU_high_school_european_history":0.7090909091,
        "MMLU_high_school_geography":0.7424242424,
        "MMLU_high_school_government_and_politics":0.8186528497,
        "MMLU_high_school_macroeconomics":0.5307692308,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.6218487395,
        "MMLU_high_school_physics":0.3642384106,
        "MMLU_high_school_psychology":0.7688073394,
        "MMLU_high_school_statistics":0.4907407407,
        "MMLU_high_school_us_history":0.7549019608,
        "MMLU_high_school_world_history":0.7637130802,
        "MMLU_human_aging":0.6591928251,
        "MMLU_human_sexuality":0.6335877863,
        "MMLU_international_law":0.7107438017,
        "MMLU_jurisprudence":0.7407407407,
        "MMLU_logical_fallacies":0.6687116564,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.811965812,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.7611749681,
        "MMLU_moral_disputes":0.6705202312,
        "MMLU_moral_scenarios":0.4167597765,
        "MMLU_nutrition":0.6241830065,
        "MMLU_philosophy":0.6623794212,
        "MMLU_prehistory":0.6543209877,
        "MMLU_professional_accounting":0.4609929078,
        "MMLU_professional_law":0.4602346806,
        "MMLU_professional_medicine":0.5698529412,
        "MMLU_professional_psychology":0.5996732026,
        "MMLU_public_relations":0.6727272727,
        "MMLU_security_studies":0.5959183673,
        "MMLU_sociology":0.7462686567,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4397590361,
        "MMLU_world_religions":0.7777777778
    },
    {
        "Model":"Stable-Platypus2-13B-QLoRA-0.80-epoch",
        "URL":"https:\/\/huggingface.co\/TFLai\/Stable-Platypus2-13B-QLoRA-0.80-epoch",
        "full_model_name":"TFLai\/Stable-Platypus2-13B-QLoRA-0.80-epoch",
        "Parameters":13.0,
        "MMLU_average":0.570914041,
        "arc:challenge|25":0.587883959,
        "hellaswag|10":0.6265684127,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5986842105,
        "MMLU_business_ethics":0.58,
        "MMLU_clinical_knowledge":0.5811320755,
        "MMLU_college_biology":0.6111111111,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.41,
        "MMLU_college_medicine":0.5549132948,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4808510638,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.5448275862,
        "MMLU_elementary_mathematics":0.3333333333,
        "MMLU_formal_logic":0.4047619048,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.6387096774,
        "MMLU_high_school_chemistry":0.39408867,
        "MMLU_high_school_computer_science":0.57,
        "MMLU_high_school_european_history":0.7333333333,
        "MMLU_high_school_geography":0.7474747475,
        "MMLU_high_school_government_and_politics":0.8341968912,
        "MMLU_high_school_macroeconomics":0.5564102564,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.5840336134,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.7834862385,
        "MMLU_high_school_statistics":0.4305555556,
        "MMLU_high_school_us_history":0.7794117647,
        "MMLU_high_school_world_history":0.7510548523,
        "MMLU_human_aging":0.6367713004,
        "MMLU_human_sexuality":0.6488549618,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.6944444444,
        "MMLU_logical_fallacies":0.6625766871,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.7572815534,
        "MMLU_marketing":0.8504273504,
        "MMLU_medical_genetics":0.65,
        "MMLU_miscellaneous":0.7816091954,
        "MMLU_moral_disputes":0.6502890173,
        "MMLU_moral_scenarios":0.4122905028,
        "MMLU_nutrition":0.6013071895,
        "MMLU_philosophy":0.6559485531,
        "MMLU_prehistory":0.6512345679,
        "MMLU_professional_accounting":0.4468085106,
        "MMLU_professional_law":0.426988266,
        "MMLU_professional_medicine":0.5735294118,
        "MMLU_professional_psychology":0.5800653595,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.6244897959,
        "MMLU_sociology":0.7114427861,
        "MMLU_us_foreign_policy":0.79,
        "MMLU_virology":0.4819277108,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"AppleSauce-L2-13b",
        "URL":"https:\/\/huggingface.co\/sauce1337\/AppleSauce-L2-13b",
        "full_model_name":"sauce1337\/AppleSauce-L2-13b",
        "Parameters":13.0,
        "MMLU_average":0.5706573362,
        "arc:challenge|25":0.593003413,
        "hellaswag|10":0.6426010755,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.5037037037,
        "MMLU_astronomy":0.5460526316,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.6188679245,
        "MMLU_college_biology":0.6041666667,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.49,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.549132948,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.74,
        "MMLU_conceptual_physics":0.4510638298,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.5103448276,
        "MMLU_elementary_mathematics":0.3174603175,
        "MMLU_formal_logic":0.4126984127,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.6612903226,
        "MMLU_high_school_chemistry":0.4630541872,
        "MMLU_high_school_computer_science":0.58,
        "MMLU_high_school_european_history":0.6909090909,
        "MMLU_high_school_geography":0.7070707071,
        "MMLU_high_school_government_and_politics":0.8393782383,
        "MMLU_high_school_macroeconomics":0.5461538462,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.5630252101,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.7596330275,
        "MMLU_high_school_statistics":0.4074074074,
        "MMLU_high_school_us_history":0.7745098039,
        "MMLU_high_school_world_history":0.7510548523,
        "MMLU_human_aging":0.6771300448,
        "MMLU_human_sexuality":0.6259541985,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.6932515337,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.7905982906,
        "MMLU_medical_genetics":0.61,
        "MMLU_miscellaneous":0.7573435504,
        "MMLU_moral_disputes":0.6473988439,
        "MMLU_moral_scenarios":0.4324022346,
        "MMLU_nutrition":0.6307189542,
        "MMLU_philosophy":0.652733119,
        "MMLU_prehistory":0.6512345679,
        "MMLU_professional_accounting":0.4255319149,
        "MMLU_professional_law":0.4282920469,
        "MMLU_professional_medicine":0.5808823529,
        "MMLU_professional_psychology":0.5784313725,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6326530612,
        "MMLU_sociology":0.7562189055,
        "MMLU_us_foreign_policy":0.87,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.7543859649
    },
    {
        "Model":"llama2-13b-orca-8k-3319",
        "URL":"https:\/\/huggingface.co\/OpenAssistant\/llama2-13b-orca-8k-3319",
        "full_model_name":"OpenAssistant\/llama2-13b-orca-8k-3319",
        "Parameters":13.0,
        "MMLU_average":0.5705831686,
        "arc:challenge|25":0.5588737201,
        "hellaswag|10":0.6136227843,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.625,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.6113207547,
        "MMLU_college_biology":0.625,
        "MMLU_college_chemistry":0.45,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.5028901734,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.5379310345,
        "MMLU_elementary_mathematics":0.3333333333,
        "MMLU_formal_logic":0.3015873016,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.6838709677,
        "MMLU_high_school_chemistry":0.4975369458,
        "MMLU_high_school_computer_science":0.61,
        "MMLU_high_school_european_history":0.7212121212,
        "MMLU_high_school_geography":0.7272727273,
        "MMLU_high_school_government_and_politics":0.7875647668,
        "MMLU_high_school_macroeconomics":0.5717948718,
        "MMLU_high_school_mathematics":0.3148148148,
        "MMLU_high_school_microeconomics":0.5630252101,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.7559633028,
        "MMLU_high_school_statistics":0.3935185185,
        "MMLU_high_school_us_history":0.7941176471,
        "MMLU_high_school_world_history":0.776371308,
        "MMLU_human_aging":0.6412556054,
        "MMLU_human_sexuality":0.6488549618,
        "MMLU_international_law":0.7851239669,
        "MMLU_jurisprudence":0.7314814815,
        "MMLU_logical_fallacies":0.6687116564,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.8034188034,
        "MMLU_medical_genetics":0.61,
        "MMLU_miscellaneous":0.7713920817,
        "MMLU_moral_disputes":0.661849711,
        "MMLU_moral_scenarios":0.3787709497,
        "MMLU_nutrition":0.6209150327,
        "MMLU_philosophy":0.6430868167,
        "MMLU_prehistory":0.6666666667,
        "MMLU_professional_accounting":0.4326241135,
        "MMLU_professional_law":0.4178617992,
        "MMLU_professional_medicine":0.5183823529,
        "MMLU_professional_psychology":0.5800653595,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6816326531,
        "MMLU_sociology":0.7512437811,
        "MMLU_us_foreign_policy":0.8,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.7719298246
    },
    {
        "Model":"OpenOrca-Platypus2-13B-thera-1250",
        "URL":"https:\/\/huggingface.co\/gaodrew\/OpenOrca-Platypus2-13B-thera-1250",
        "full_model_name":"gaodrew\/OpenOrca-Platypus2-13B-thera-1250",
        "Parameters":13.0,
        "MMLU_average":0.5703995605,
        "arc:challenge|25":0.5554607509,
        "hellaswag|10":0.6116311492,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5460526316,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.5962264151,
        "MMLU_college_biology":0.6180555556,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.49,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.5549132948,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4808510638,
        "MMLU_econometrics":0.3421052632,
        "MMLU_electrical_engineering":0.5172413793,
        "MMLU_elementary_mathematics":0.3148148148,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.6741935484,
        "MMLU_high_school_chemistry":0.4088669951,
        "MMLU_high_school_computer_science":0.58,
        "MMLU_high_school_european_history":0.696969697,
        "MMLU_high_school_geography":0.6919191919,
        "MMLU_high_school_government_and_politics":0.829015544,
        "MMLU_high_school_macroeconomics":0.5666666667,
        "MMLU_high_school_mathematics":0.3555555556,
        "MMLU_high_school_microeconomics":0.5924369748,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.7871559633,
        "MMLU_high_school_statistics":0.462962963,
        "MMLU_high_school_us_history":0.75,
        "MMLU_high_school_world_history":0.7721518987,
        "MMLU_human_aging":0.66367713,
        "MMLU_human_sexuality":0.5954198473,
        "MMLU_international_law":0.7024793388,
        "MMLU_jurisprudence":0.7037037037,
        "MMLU_logical_fallacies":0.6809815951,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.7475728155,
        "MMLU_marketing":0.811965812,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.7701149425,
        "MMLU_moral_disputes":0.6445086705,
        "MMLU_moral_scenarios":0.4458100559,
        "MMLU_nutrition":0.6078431373,
        "MMLU_philosophy":0.6784565916,
        "MMLU_prehistory":0.6574074074,
        "MMLU_professional_accounting":0.4503546099,
        "MMLU_professional_law":0.4569752282,
        "MMLU_professional_medicine":0.5919117647,
        "MMLU_professional_psychology":0.5571895425,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.6653061224,
        "MMLU_sociology":0.7562189055,
        "MMLU_us_foreign_policy":0.78,
        "MMLU_virology":0.4518072289,
        "MMLU_world_religions":0.7660818713
    },
    {
        "Model":"CollectiveCognition-v1.1-Nebula-7B",
        "URL":"https:\/\/huggingface.co\/PulsarAI\/CollectiveCognition-v1.1-Nebula-7B",
        "full_model_name":"PulsarAI\/CollectiveCognition-v1.1-Nebula-7B",
        "Parameters":7.0,
        "MMLU_average":0.5702619553,
        "arc:challenge|25":0.5324232082,
        "hellaswag|10":0.63095001,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.5555555556,
        "MMLU_astronomy":0.5986842105,
        "MMLU_business_ethics":0.43,
        "MMLU_clinical_knowledge":0.6188679245,
        "MMLU_college_biology":0.6319444444,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.5433526012,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.4936170213,
        "MMLU_econometrics":0.4473684211,
        "MMLU_electrical_engineering":0.5448275862,
        "MMLU_elementary_mathematics":0.3915343915,
        "MMLU_formal_logic":0.380952381,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.6483870968,
        "MMLU_high_school_chemistry":0.4532019704,
        "MMLU_high_school_computer_science":0.57,
        "MMLU_high_school_european_history":0.7333333333,
        "MMLU_high_school_geography":0.7676767677,
        "MMLU_high_school_government_and_politics":0.8238341969,
        "MMLU_high_school_macroeconomics":0.5615384615,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.5588235294,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7614678899,
        "MMLU_high_school_statistics":0.3935185185,
        "MMLU_high_school_us_history":0.7205882353,
        "MMLU_high_school_world_history":0.7299578059,
        "MMLU_human_aging":0.668161435,
        "MMLU_human_sexuality":0.6564885496,
        "MMLU_international_law":0.7603305785,
        "MMLU_jurisprudence":0.6944444444,
        "MMLU_logical_fallacies":0.6871165644,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.811965812,
        "MMLU_medical_genetics":0.66,
        "MMLU_miscellaneous":0.7713920817,
        "MMLU_moral_disputes":0.6184971098,
        "MMLU_moral_scenarios":0.2279329609,
        "MMLU_nutrition":0.6405228758,
        "MMLU_philosophy":0.6334405145,
        "MMLU_prehistory":0.6820987654,
        "MMLU_professional_accounting":0.4290780142,
        "MMLU_professional_law":0.4315514993,
        "MMLU_professional_medicine":0.5257352941,
        "MMLU_professional_psychology":0.6078431373,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.5755102041,
        "MMLU_sociology":0.7512437811,
        "MMLU_us_foreign_policy":0.8,
        "MMLU_virology":0.5180722892,
        "MMLU_world_religions":0.783625731
    },
    {
        "Model":"huginnv1.2",
        "URL":"https:\/\/huggingface.co\/The-Face-Of-Goonery\/huginnv1.2",
        "full_model_name":"The-Face-Of-Goonery\/huginnv1.2",
        "Parameters":null,
        "MMLU_average":0.5701561556,
        "arc:challenge|25":0.5784982935,
        "hellaswag|10":0.6463851822,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.5037037037,
        "MMLU_astronomy":0.5789473684,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.6075471698,
        "MMLU_college_biology":0.5972222222,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5260115607,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4553191489,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.3465608466,
        "MMLU_formal_logic":0.380952381,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.6612903226,
        "MMLU_high_school_chemistry":0.4876847291,
        "MMLU_high_school_computer_science":0.57,
        "MMLU_high_school_european_history":0.6787878788,
        "MMLU_high_school_geography":0.7222222222,
        "MMLU_high_school_government_and_politics":0.8134715026,
        "MMLU_high_school_macroeconomics":0.5461538462,
        "MMLU_high_school_mathematics":0.3222222222,
        "MMLU_high_school_microeconomics":0.6134453782,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7449541284,
        "MMLU_high_school_statistics":0.4305555556,
        "MMLU_high_school_us_history":0.7745098039,
        "MMLU_high_school_world_history":0.7426160338,
        "MMLU_human_aging":0.6860986547,
        "MMLU_human_sexuality":0.6641221374,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.7407407407,
        "MMLU_logical_fallacies":0.6871165644,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.7991452991,
        "MMLU_medical_genetics":0.65,
        "MMLU_miscellaneous":0.7624521073,
        "MMLU_moral_disputes":0.6300578035,
        "MMLU_moral_scenarios":0.4603351955,
        "MMLU_nutrition":0.6078431373,
        "MMLU_philosophy":0.6655948553,
        "MMLU_prehistory":0.6327160494,
        "MMLU_professional_accounting":0.4609929078,
        "MMLU_professional_law":0.4282920469,
        "MMLU_professional_medicine":0.5367647059,
        "MMLU_professional_psychology":0.5653594771,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.6489795918,
        "MMLU_sociology":0.7213930348,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.7719298246
    },
    {
        "Model":"llama-op-v4",
        "URL":"https:\/\/huggingface.co\/anhnv125\/llama-op-v4",
        "full_model_name":"anhnv125\/llama-op-v4",
        "Parameters":null,
        "MMLU_average":0.5701096174,
        "arc:challenge|25":0.5648464164,
        "hellaswag|10":0.5943039235,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4296296296,
        "MMLU_astronomy":0.5657894737,
        "MMLU_business_ethics":0.62,
        "MMLU_clinical_knowledge":0.558490566,
        "MMLU_college_biology":0.6180555556,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.5086705202,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.65,
        "MMLU_conceptual_physics":0.4425531915,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4896551724,
        "MMLU_elementary_mathematics":0.3518518519,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.43,
        "MMLU_high_school_biology":0.6612903226,
        "MMLU_high_school_chemistry":0.4285714286,
        "MMLU_high_school_computer_science":0.58,
        "MMLU_high_school_european_history":0.7272727273,
        "MMLU_high_school_geography":0.7373737374,
        "MMLU_high_school_government_and_politics":0.8445595855,
        "MMLU_high_school_macroeconomics":0.6,
        "MMLU_high_school_mathematics":0.3185185185,
        "MMLU_high_school_microeconomics":0.5798319328,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.7779816514,
        "MMLU_high_school_statistics":0.5,
        "MMLU_high_school_us_history":0.7892156863,
        "MMLU_high_school_world_history":0.776371308,
        "MMLU_human_aging":0.6412556054,
        "MMLU_human_sexuality":0.6183206107,
        "MMLU_international_law":0.7024793388,
        "MMLU_jurisprudence":0.7592592593,
        "MMLU_logical_fallacies":0.6932515337,
        "MMLU_machine_learning":0.4196428571,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.7735042735,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.7420178799,
        "MMLU_moral_disputes":0.6242774566,
        "MMLU_moral_scenarios":0.4837988827,
        "MMLU_nutrition":0.6176470588,
        "MMLU_philosophy":0.6334405145,
        "MMLU_prehistory":0.6111111111,
        "MMLU_professional_accounting":0.4574468085,
        "MMLU_professional_law":0.4380704042,
        "MMLU_professional_medicine":0.5588235294,
        "MMLU_professional_psychology":0.5604575163,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.6571428571,
        "MMLU_sociology":0.7412935323,
        "MMLU_us_foreign_policy":0.79,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.7777777778
    },
    {
        "Model":"MLewd-Chat-v2-13B",
        "URL":"https:\/\/huggingface.co\/Undi95\/MLewd-Chat-v2-13B",
        "full_model_name":"Undi95\/MLewd-Chat-v2-13B",
        "Parameters":13.0,
        "MMLU_average":0.5700037175,
        "arc:challenge|25":0.5921501706,
        "hellaswag|10":0.6437960566,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.5185185185,
        "MMLU_astronomy":0.5460526316,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.5773584906,
        "MMLU_college_biology":0.6180555556,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.49,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.5260115607,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.4978723404,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.5172413793,
        "MMLU_elementary_mathematics":0.3201058201,
        "MMLU_formal_logic":0.3968253968,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.6516129032,
        "MMLU_high_school_chemistry":0.4334975369,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.703030303,
        "MMLU_high_school_geography":0.7272727273,
        "MMLU_high_school_government_and_politics":0.829015544,
        "MMLU_high_school_macroeconomics":0.5487179487,
        "MMLU_high_school_mathematics":0.3222222222,
        "MMLU_high_school_microeconomics":0.5882352941,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.7577981651,
        "MMLU_high_school_statistics":0.3935185185,
        "MMLU_high_school_us_history":0.7843137255,
        "MMLU_high_school_world_history":0.7721518987,
        "MMLU_human_aging":0.6905829596,
        "MMLU_human_sexuality":0.6870229008,
        "MMLU_international_law":0.7107438017,
        "MMLU_jurisprudence":0.7407407407,
        "MMLU_logical_fallacies":0.6748466258,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.6893203883,
        "MMLU_marketing":0.8205128205,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.7675606641,
        "MMLU_moral_disputes":0.6184971098,
        "MMLU_moral_scenarios":0.4268156425,
        "MMLU_nutrition":0.6437908497,
        "MMLU_philosophy":0.6463022508,
        "MMLU_prehistory":0.6358024691,
        "MMLU_professional_accounting":0.4326241135,
        "MMLU_professional_law":0.4322033898,
        "MMLU_professional_medicine":0.5183823529,
        "MMLU_professional_psychology":0.5833333333,
        "MMLU_public_relations":0.6727272727,
        "MMLU_security_studies":0.6530612245,
        "MMLU_sociology":0.7313432836,
        "MMLU_us_foreign_policy":0.86,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"Nebula-7B",
        "URL":"https:\/\/huggingface.co\/PulsarAI\/Nebula-7B",
        "full_model_name":"PulsarAI\/Nebula-7B",
        "Parameters":7.0,
        "MMLU_average":0.5699849011,
        "arc:challenge|25":0.5418088737,
        "hellaswag|10":0.6342362079,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.5555555556,
        "MMLU_astronomy":0.5986842105,
        "MMLU_business_ethics":0.43,
        "MMLU_clinical_knowledge":0.5773584906,
        "MMLU_college_biology":0.6944444444,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.5202312139,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.5063829787,
        "MMLU_econometrics":0.4649122807,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.380952381,
        "MMLU_formal_logic":0.4047619048,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.6516129032,
        "MMLU_high_school_chemistry":0.4778325123,
        "MMLU_high_school_computer_science":0.56,
        "MMLU_high_school_european_history":0.696969697,
        "MMLU_high_school_geography":0.7525252525,
        "MMLU_high_school_government_and_politics":0.8082901554,
        "MMLU_high_school_macroeconomics":0.5333333333,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.5588235294,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7743119266,
        "MMLU_high_school_statistics":0.3935185185,
        "MMLU_high_school_us_history":0.7156862745,
        "MMLU_high_school_world_history":0.7510548523,
        "MMLU_human_aging":0.668161435,
        "MMLU_human_sexuality":0.679389313,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.6851851852,
        "MMLU_logical_fallacies":0.6932515337,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.6990291262,
        "MMLU_marketing":0.8205128205,
        "MMLU_medical_genetics":0.66,
        "MMLU_miscellaneous":0.8007662835,
        "MMLU_moral_disputes":0.6242774566,
        "MMLU_moral_scenarios":0.2659217877,
        "MMLU_nutrition":0.6274509804,
        "MMLU_philosophy":0.6623794212,
        "MMLU_prehistory":0.7067901235,
        "MMLU_professional_accounting":0.4468085106,
        "MMLU_professional_law":0.4406779661,
        "MMLU_professional_medicine":0.5441176471,
        "MMLU_professional_psychology":0.5980392157,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.5142857143,
        "MMLU_sociology":0.7661691542,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4819277108,
        "MMLU_world_religions":0.7660818713
    },
    {
        "Model":"StableBeluga-13B-instruct-PL-lora_unload",
        "URL":"https:\/\/huggingface.co\/Aspik101\/StableBeluga-13B-instruct-PL-lora_unload",
        "full_model_name":"Aspik101\/StableBeluga-13B-instruct-PL-lora_unload",
        "Parameters":13.0,
        "MMLU_average":0.569856085,
        "arc:challenge|25":0.590443686,
        "hellaswag|10":0.6184027086,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.4934210526,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.6,
        "MMLU_college_biology":0.6111111111,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.5086705202,
        "MMLU_college_physics":0.3431372549,
        "MMLU_computer_security":0.73,
        "MMLU_conceptual_physics":0.4510638298,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.5448275862,
        "MMLU_elementary_mathematics":0.3253968254,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.6612903226,
        "MMLU_high_school_chemistry":0.4137931034,
        "MMLU_high_school_computer_science":0.58,
        "MMLU_high_school_european_history":0.7090909091,
        "MMLU_high_school_geography":0.7373737374,
        "MMLU_high_school_government_and_politics":0.792746114,
        "MMLU_high_school_macroeconomics":0.5358974359,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.5966386555,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7596330275,
        "MMLU_high_school_statistics":0.4305555556,
        "MMLU_high_school_us_history":0.7647058824,
        "MMLU_high_school_world_history":0.7510548523,
        "MMLU_human_aging":0.6771300448,
        "MMLU_human_sexuality":0.6641221374,
        "MMLU_international_law":0.7520661157,
        "MMLU_jurisprudence":0.7314814815,
        "MMLU_logical_fallacies":0.7055214724,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.7572815534,
        "MMLU_marketing":0.8034188034,
        "MMLU_medical_genetics":0.68,
        "MMLU_miscellaneous":0.7701149425,
        "MMLU_moral_disputes":0.6531791908,
        "MMLU_moral_scenarios":0.4167597765,
        "MMLU_nutrition":0.6339869281,
        "MMLU_philosophy":0.6302250804,
        "MMLU_prehistory":0.6481481481,
        "MMLU_professional_accounting":0.4219858156,
        "MMLU_professional_law":0.4093872229,
        "MMLU_professional_medicine":0.5183823529,
        "MMLU_professional_psychology":0.5816993464,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.6285714286,
        "MMLU_sociology":0.7462686567,
        "MMLU_us_foreign_policy":0.79,
        "MMLU_virology":0.4879518072,
        "MMLU_world_religions":0.7660818713
    },
    {
        "Model":"Emerhyst-20B",
        "URL":"https:\/\/huggingface.co\/Undi95\/Emerhyst-20B",
        "full_model_name":"Undi95\/Emerhyst-20B",
        "Parameters":20.0,
        "MMLU_average":0.5697927045,
        "arc:challenge|25":0.5895904437,
        "hellaswag|10":0.6613224457,
        "MMLU_abstract_algebra":0.38,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.5394736842,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.6,
        "MMLU_college_biology":0.6458333333,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.5317919075,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4808510638,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.5517241379,
        "MMLU_elementary_mathematics":0.3386243386,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.6580645161,
        "MMLU_high_school_chemistry":0.4482758621,
        "MMLU_high_school_computer_science":0.53,
        "MMLU_high_school_european_history":0.6909090909,
        "MMLU_high_school_geography":0.7222222222,
        "MMLU_high_school_government_and_politics":0.8134715026,
        "MMLU_high_school_macroeconomics":0.5717948718,
        "MMLU_high_school_mathematics":0.3333333333,
        "MMLU_high_school_microeconomics":0.6176470588,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.7339449541,
        "MMLU_high_school_statistics":0.4583333333,
        "MMLU_high_school_us_history":0.7745098039,
        "MMLU_high_school_world_history":0.7721518987,
        "MMLU_human_aging":0.6726457399,
        "MMLU_human_sexuality":0.641221374,
        "MMLU_international_law":0.7603305785,
        "MMLU_jurisprudence":0.712962963,
        "MMLU_logical_fallacies":0.7055214724,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.6601941748,
        "MMLU_marketing":0.8290598291,
        "MMLU_medical_genetics":0.62,
        "MMLU_miscellaneous":0.7471264368,
        "MMLU_moral_disputes":0.6473988439,
        "MMLU_moral_scenarios":0.4078212291,
        "MMLU_nutrition":0.6339869281,
        "MMLU_philosophy":0.6688102894,
        "MMLU_prehistory":0.6759259259,
        "MMLU_professional_accounting":0.4361702128,
        "MMLU_professional_law":0.4530638853,
        "MMLU_professional_medicine":0.5551470588,
        "MMLU_professional_psychology":0.6062091503,
        "MMLU_public_relations":0.5727272727,
        "MMLU_security_studies":0.6489795918,
        "MMLU_sociology":0.736318408,
        "MMLU_us_foreign_policy":0.85,
        "MMLU_virology":0.3975903614,
        "MMLU_world_religions":0.7485380117
    },
    {
        "Model":"Ensemble5-Platypus2-13B-QLora-0.80-epoch",
        "URL":"https:\/\/huggingface.co\/TFLai\/Ensemble5-Platypus2-13B-QLora-0.80-epoch",
        "full_model_name":"TFLai\/Ensemble5-Platypus2-13B-QLora-0.80-epoch",
        "Parameters":13.0,
        "MMLU_average":0.5693713409,
        "arc:challenge|25":0.5674061433,
        "hellaswag|10":0.6262696674,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.5037037037,
        "MMLU_astronomy":0.5986842105,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.5811320755,
        "MMLU_college_biology":0.6527777778,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.5549132948,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.4553191489,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.4896551724,
        "MMLU_elementary_mathematics":0.3650793651,
        "MMLU_formal_logic":0.3968253968,
        "MMLU_global_facts":0.4,
        "MMLU_high_school_biology":0.6161290323,
        "MMLU_high_school_chemistry":0.4187192118,
        "MMLU_high_school_computer_science":0.58,
        "MMLU_high_school_european_history":0.7151515152,
        "MMLU_high_school_geography":0.7474747475,
        "MMLU_high_school_government_and_politics":0.8497409326,
        "MMLU_high_school_macroeconomics":0.5948717949,
        "MMLU_high_school_mathematics":0.362962963,
        "MMLU_high_school_microeconomics":0.5714285714,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7963302752,
        "MMLU_high_school_statistics":0.4398148148,
        "MMLU_high_school_us_history":0.8333333333,
        "MMLU_high_school_world_history":0.7594936709,
        "MMLU_human_aging":0.6502242152,
        "MMLU_human_sexuality":0.6030534351,
        "MMLU_international_law":0.7107438017,
        "MMLU_jurisprudence":0.6851851852,
        "MMLU_logical_fallacies":0.7055214724,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.8162393162,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7892720307,
        "MMLU_moral_disputes":0.6184971098,
        "MMLU_moral_scenarios":0.4804469274,
        "MMLU_nutrition":0.5849673203,
        "MMLU_philosophy":0.652733119,
        "MMLU_prehistory":0.6882716049,
        "MMLU_professional_accounting":0.4609929078,
        "MMLU_professional_law":0.4654498044,
        "MMLU_professional_medicine":0.5735294118,
        "MMLU_professional_psychology":0.5964052288,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.5755102041,
        "MMLU_sociology":0.671641791,
        "MMLU_us_foreign_policy":0.75,
        "MMLU_virology":0.4819277108,
        "MMLU_world_religions":0.7894736842
    },
    {
        "Model":"dolphin-2.0-mistral-7b",
        "URL":"https:\/\/huggingface.co\/ehartford\/dolphin-2.0-mistral-7b",
        "full_model_name":"ehartford\/dolphin-2.0-mistral-7b",
        "Parameters":7.0,
        "MMLU_average":0.5689889111,
        "arc:challenge|25":0.5520477816,
        "hellaswag|10":0.6082453694,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.6184210526,
        "MMLU_business_ethics":0.6,
        "MMLU_clinical_knowledge":0.6339622642,
        "MMLU_college_biology":0.6458333333,
        "MMLU_college_chemistry":0.45,
        "MMLU_college_computer_science":0.49,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.5780346821,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.4936170213,
        "MMLU_econometrics":0.3684210526,
        "MMLU_electrical_engineering":0.5724137931,
        "MMLU_elementary_mathematics":0.373015873,
        "MMLU_formal_logic":0.3888888889,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.6161290323,
        "MMLU_high_school_chemistry":0.4137931034,
        "MMLU_high_school_computer_science":0.6,
        "MMLU_high_school_european_history":0.7151515152,
        "MMLU_high_school_geography":0.6818181818,
        "MMLU_high_school_government_and_politics":0.7979274611,
        "MMLU_high_school_macroeconomics":0.5435897436,
        "MMLU_high_school_mathematics":0.337037037,
        "MMLU_high_school_microeconomics":0.5672268908,
        "MMLU_high_school_physics":0.3708609272,
        "MMLU_high_school_psychology":0.7357798165,
        "MMLU_high_school_statistics":0.4212962963,
        "MMLU_high_school_us_history":0.7303921569,
        "MMLU_high_school_world_history":0.7257383966,
        "MMLU_human_aging":0.6188340807,
        "MMLU_human_sexuality":0.679389313,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.6666666667,
        "MMLU_logical_fallacies":0.6625766871,
        "MMLU_machine_learning":0.4196428571,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.811965812,
        "MMLU_medical_genetics":0.66,
        "MMLU_miscellaneous":0.754789272,
        "MMLU_moral_disputes":0.6329479769,
        "MMLU_moral_scenarios":0.3463687151,
        "MMLU_nutrition":0.591503268,
        "MMLU_philosophy":0.61414791,
        "MMLU_prehistory":0.5956790123,
        "MMLU_professional_accounting":0.414893617,
        "MMLU_professional_law":0.4087353325,
        "MMLU_professional_medicine":0.5992647059,
        "MMLU_professional_psychology":0.5588235294,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.7020408163,
        "MMLU_sociology":0.7114427861,
        "MMLU_us_foreign_policy":0.79,
        "MMLU_virology":0.5180722892,
        "MMLU_world_religions":0.730994152
    },
    {
        "Model":"llava-v1.5-13b-hf",
        "URL":"https:\/\/huggingface.co\/Community-LM\/llava-v1.5-13b-hf",
        "full_model_name":"Community-LM\/llava-v1.5-13b-hf",
        "Parameters":13.0,
        "MMLU_average":0.5688676037,
        "arc:challenge|25":0.5324232082,
        "hellaswag|10":0.6011750647,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.5855263158,
        "MMLU_business_ethics":0.67,
        "MMLU_clinical_knowledge":0.6037735849,
        "MMLU_college_biology":0.6041666667,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.5375722543,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.4468085106,
        "MMLU_econometrics":0.3333333333,
        "MMLU_electrical_engineering":0.5103448276,
        "MMLU_elementary_mathematics":0.328042328,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.7129032258,
        "MMLU_high_school_chemistry":0.4285714286,
        "MMLU_high_school_computer_science":0.58,
        "MMLU_high_school_european_history":0.7151515152,
        "MMLU_high_school_geography":0.7424242424,
        "MMLU_high_school_government_and_politics":0.8393782383,
        "MMLU_high_school_macroeconomics":0.5384615385,
        "MMLU_high_school_mathematics":0.3148148148,
        "MMLU_high_school_microeconomics":0.5672268908,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.7577981651,
        "MMLU_high_school_statistics":0.4259259259,
        "MMLU_high_school_us_history":0.7549019608,
        "MMLU_high_school_world_history":0.7805907173,
        "MMLU_human_aging":0.668161435,
        "MMLU_human_sexuality":0.641221374,
        "MMLU_international_law":0.7190082645,
        "MMLU_jurisprudence":0.7314814815,
        "MMLU_logical_fallacies":0.6319018405,
        "MMLU_machine_learning":0.4196428571,
        "MMLU_management":0.7572815534,
        "MMLU_marketing":0.8461538462,
        "MMLU_medical_genetics":0.64,
        "MMLU_miscellaneous":0.7739463602,
        "MMLU_moral_disputes":0.6271676301,
        "MMLU_moral_scenarios":0.3240223464,
        "MMLU_nutrition":0.6078431373,
        "MMLU_philosophy":0.6237942122,
        "MMLU_prehistory":0.6975308642,
        "MMLU_professional_accounting":0.4078014184,
        "MMLU_professional_law":0.4159061278,
        "MMLU_professional_medicine":0.5477941176,
        "MMLU_professional_psychology":0.5784313725,
        "MMLU_public_relations":0.6,
        "MMLU_security_studies":0.6530612245,
        "MMLU_sociology":0.7611940299,
        "MMLU_us_foreign_policy":0.81,
        "MMLU_virology":0.5060240964,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"dulia-13b-8k-alpha",
        "URL":"https:\/\/huggingface.co\/duliadotio\/dulia-13b-8k-alpha",
        "full_model_name":"duliadotio\/dulia-13b-8k-alpha",
        "Parameters":13.0,
        "MMLU_average":0.5686904791,
        "arc:challenge|25":0.5571672355,
        "hellaswag|10":0.6138219478,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.625,
        "MMLU_business_ethics":0.58,
        "MMLU_clinical_knowledge":0.5962264151,
        "MMLU_college_biology":0.6180555556,
        "MMLU_college_chemistry":0.45,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.4913294798,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.429787234,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.5379310345,
        "MMLU_elementary_mathematics":0.335978836,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.6838709677,
        "MMLU_high_school_chemistry":0.4778325123,
        "MMLU_high_school_computer_science":0.62,
        "MMLU_high_school_european_history":0.7151515152,
        "MMLU_high_school_geography":0.7272727273,
        "MMLU_high_school_government_and_politics":0.7772020725,
        "MMLU_high_school_macroeconomics":0.5615384615,
        "MMLU_high_school_mathematics":0.3185185185,
        "MMLU_high_school_microeconomics":0.5756302521,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.7504587156,
        "MMLU_high_school_statistics":0.3796296296,
        "MMLU_high_school_us_history":0.7892156863,
        "MMLU_high_school_world_history":0.776371308,
        "MMLU_human_aging":0.6457399103,
        "MMLU_human_sexuality":0.641221374,
        "MMLU_international_law":0.8016528926,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.6687116564,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.7087378641,
        "MMLU_marketing":0.8076923077,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.7713920817,
        "MMLU_moral_disputes":0.6647398844,
        "MMLU_moral_scenarios":0.3765363128,
        "MMLU_nutrition":0.6274509804,
        "MMLU_philosophy":0.6463022508,
        "MMLU_prehistory":0.6697530864,
        "MMLU_professional_accounting":0.4397163121,
        "MMLU_professional_law":0.4172099087,
        "MMLU_professional_medicine":0.5183823529,
        "MMLU_professional_psychology":0.5800653595,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.693877551,
        "MMLU_sociology":0.7562189055,
        "MMLU_us_foreign_policy":0.8,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.7777777778
    },
    {
        "Model":"llama2-platypus-llama2-chat-13B-hf",
        "URL":"https:\/\/huggingface.co\/chickencaesar\/llama2-platypus-llama2-chat-13B-hf",
        "full_model_name":"chickencaesar\/llama2-platypus-llama2-chat-13B-hf",
        "Parameters":13.0,
        "MMLU_average":0.5686148082,
        "arc:challenge|25":0.5674061433,
        "hellaswag|10":0.6227843059,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.5131578947,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.5924528302,
        "MMLU_college_biology":0.5972222222,
        "MMLU_college_chemistry":0.48,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.5722543353,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.4468085106,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.3544973545,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.6548387097,
        "MMLU_high_school_chemistry":0.4532019704,
        "MMLU_high_school_computer_science":0.53,
        "MMLU_high_school_european_history":0.6848484848,
        "MMLU_high_school_geography":0.7070707071,
        "MMLU_high_school_government_and_politics":0.8082901554,
        "MMLU_high_school_macroeconomics":0.5333333333,
        "MMLU_high_school_mathematics":0.3333333333,
        "MMLU_high_school_microeconomics":0.5966386555,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.776146789,
        "MMLU_high_school_statistics":0.4444444444,
        "MMLU_high_school_us_history":0.7745098039,
        "MMLU_high_school_world_history":0.7679324895,
        "MMLU_human_aging":0.66367713,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.7592592593,
        "MMLU_logical_fallacies":0.6809815951,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.7475728155,
        "MMLU_marketing":0.7735042735,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7662835249,
        "MMLU_moral_disputes":0.6271676301,
        "MMLU_moral_scenarios":0.3698324022,
        "MMLU_nutrition":0.637254902,
        "MMLU_philosophy":0.6752411576,
        "MMLU_prehistory":0.6512345679,
        "MMLU_professional_accounting":0.4361702128,
        "MMLU_professional_law":0.4641460235,
        "MMLU_professional_medicine":0.6102941176,
        "MMLU_professional_psychology":0.5882352941,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.6326530612,
        "MMLU_sociology":0.7263681592,
        "MMLU_us_foreign_policy":0.8,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.7894736842
    },
    {
        "Model":"EnsembleV5-Nova-13B",
        "URL":"https:\/\/huggingface.co\/PulsarAI\/EnsembleV5-Nova-13B",
        "full_model_name":"PulsarAI\/EnsembleV5-Nova-13B",
        "Parameters":13.0,
        "MMLU_average":0.5678841966,
        "arc:challenge|25":0.5784982935,
        "hellaswag|10":0.6217884883,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5789473684,
        "MMLU_business_ethics":0.58,
        "MMLU_clinical_knowledge":0.6037735849,
        "MMLU_college_biology":0.6527777778,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5433526012,
        "MMLU_college_physics":0.3823529412,
        "MMLU_computer_security":0.63,
        "MMLU_conceptual_physics":0.4638297872,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4413793103,
        "MMLU_elementary_mathematics":0.3783068783,
        "MMLU_formal_logic":0.4047619048,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.6580645161,
        "MMLU_high_school_chemistry":0.4334975369,
        "MMLU_high_school_computer_science":0.63,
        "MMLU_high_school_european_history":0.7212121212,
        "MMLU_high_school_geography":0.7626262626,
        "MMLU_high_school_government_and_politics":0.8393782383,
        "MMLU_high_school_macroeconomics":0.6153846154,
        "MMLU_high_school_mathematics":0.3296296296,
        "MMLU_high_school_microeconomics":0.6302521008,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.8,
        "MMLU_high_school_statistics":0.5138888889,
        "MMLU_high_school_us_history":0.8137254902,
        "MMLU_high_school_world_history":0.776371308,
        "MMLU_human_aging":0.6278026906,
        "MMLU_human_sexuality":0.6335877863,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.6203703704,
        "MMLU_logical_fallacies":0.6809815951,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.7572815534,
        "MMLU_marketing":0.7991452991,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.7586206897,
        "MMLU_moral_disputes":0.6271676301,
        "MMLU_moral_scenarios":0.3843575419,
        "MMLU_nutrition":0.6045751634,
        "MMLU_philosophy":0.6463022508,
        "MMLU_prehistory":0.6419753086,
        "MMLU_professional_accounting":0.4432624113,
        "MMLU_professional_law":0.4439374185,
        "MMLU_professional_medicine":0.5919117647,
        "MMLU_professional_psychology":0.5751633987,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.5714285714,
        "MMLU_sociology":0.6766169154,
        "MMLU_us_foreign_policy":0.75,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.7660818713
    },
    {
        "Model":"speechless-orca-platypus-coig-lite-4k-0.6e-13b",
        "URL":"https:\/\/huggingface.co\/uukuguy\/speechless-orca-platypus-coig-lite-4k-0.6e-13b",
        "full_model_name":"uukuguy\/speechless-orca-platypus-coig-lite-4k-0.6e-13b",
        "Parameters":13.0,
        "MMLU_average":0.5677294721,
        "arc:challenge|25":0.5341296928,
        "hellaswag|10":0.5964947222,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.6052631579,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.6339622642,
        "MMLU_college_biology":0.6458333333,
        "MMLU_college_chemistry":0.52,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.5664739884,
        "MMLU_college_physics":0.4117647059,
        "MMLU_computer_security":0.67,
        "MMLU_conceptual_physics":0.4680851064,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.3253968254,
        "MMLU_formal_logic":0.380952381,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.6483870968,
        "MMLU_high_school_chemistry":0.4729064039,
        "MMLU_high_school_computer_science":0.56,
        "MMLU_high_school_european_history":0.7090909091,
        "MMLU_high_school_geography":0.7171717172,
        "MMLU_high_school_government_and_politics":0.8134715026,
        "MMLU_high_school_macroeconomics":0.6076923077,
        "MMLU_high_school_mathematics":0.3074074074,
        "MMLU_high_school_microeconomics":0.6050420168,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.7559633028,
        "MMLU_high_school_statistics":0.5277777778,
        "MMLU_high_school_us_history":0.8137254902,
        "MMLU_high_school_world_history":0.7848101266,
        "MMLU_human_aging":0.6322869955,
        "MMLU_human_sexuality":0.6488549618,
        "MMLU_international_law":0.6776859504,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.6564417178,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.7087378641,
        "MMLU_marketing":0.7606837607,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.7279693487,
        "MMLU_moral_disputes":0.6473988439,
        "MMLU_moral_scenarios":0.4916201117,
        "MMLU_nutrition":0.5849673203,
        "MMLU_philosophy":0.6173633441,
        "MMLU_prehistory":0.6388888889,
        "MMLU_professional_accounting":0.4219858156,
        "MMLU_professional_law":0.443285528,
        "MMLU_professional_medicine":0.5698529412,
        "MMLU_professional_psychology":0.5473856209,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.6693877551,
        "MMLU_sociology":0.736318408,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.730994152
    },
    {
        "Model":"openchat_v3.1",
        "URL":"https:\/\/huggingface.co\/openchat\/openchat_v3.1",
        "full_model_name":"openchat\/openchat_v3.1",
        "Parameters":null,
        "MMLU_average":0.5675547173,
        "arc:challenge|25":0.5656996587,
        "hellaswag|10":0.6285600478,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.5460526316,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.6264150943,
        "MMLU_college_biology":0.6111111111,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.5375722543,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.72,
        "MMLU_conceptual_physics":0.4255319149,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.328042328,
        "MMLU_formal_logic":0.380952381,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.6870967742,
        "MMLU_high_school_chemistry":0.4679802956,
        "MMLU_high_school_computer_science":0.6,
        "MMLU_high_school_european_history":0.6666666667,
        "MMLU_high_school_geography":0.7121212121,
        "MMLU_high_school_government_and_politics":0.8031088083,
        "MMLU_high_school_macroeconomics":0.5435897436,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.5840336134,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.7541284404,
        "MMLU_high_school_statistics":0.4907407407,
        "MMLU_high_school_us_history":0.7352941176,
        "MMLU_high_school_world_history":0.7215189873,
        "MMLU_human_aging":0.6457399103,
        "MMLU_human_sexuality":0.6488549618,
        "MMLU_international_law":0.7190082645,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.6748466258,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.7087378641,
        "MMLU_marketing":0.8034188034,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.7611749681,
        "MMLU_moral_disputes":0.6502890173,
        "MMLU_moral_scenarios":0.4603351955,
        "MMLU_nutrition":0.6176470588,
        "MMLU_philosophy":0.652733119,
        "MMLU_prehistory":0.6388888889,
        "MMLU_professional_accounting":0.4184397163,
        "MMLU_professional_law":0.4230769231,
        "MMLU_professional_medicine":0.5919117647,
        "MMLU_professional_psychology":0.5604575163,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.6489795918,
        "MMLU_sociology":0.7462686567,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.7602339181
    },
    {
        "Model":"qCammel-13",
        "URL":"https:\/\/huggingface.co\/augtoma\/qCammel-13",
        "full_model_name":"augtoma\/qCammel-13",
        "Parameters":null,
        "MMLU_average":0.56727354,
        "arc:challenge|25":0.5656996587,
        "hellaswag|10":0.6380203147,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.5555555556,
        "MMLU_astronomy":0.6118421053,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.6,
        "MMLU_college_biology":0.5763888889,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.5,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.5375722543,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.4085106383,
        "MMLU_econometrics":0.3421052632,
        "MMLU_electrical_engineering":0.524137931,
        "MMLU_elementary_mathematics":0.3518518519,
        "MMLU_formal_logic":0.373015873,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.6935483871,
        "MMLU_high_school_chemistry":0.4334975369,
        "MMLU_high_school_computer_science":0.61,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.7474747475,
        "MMLU_high_school_government_and_politics":0.8082901554,
        "MMLU_high_school_macroeconomics":0.5871794872,
        "MMLU_high_school_mathematics":0.337037037,
        "MMLU_high_school_microeconomics":0.5588235294,
        "MMLU_high_school_physics":0.3774834437,
        "MMLU_high_school_psychology":0.7743119266,
        "MMLU_high_school_statistics":0.4351851852,
        "MMLU_high_school_us_history":0.75,
        "MMLU_high_school_world_history":0.7341772152,
        "MMLU_human_aging":0.6412556054,
        "MMLU_human_sexuality":0.5801526718,
        "MMLU_international_law":0.7107438017,
        "MMLU_jurisprudence":0.712962963,
        "MMLU_logical_fallacies":0.6625766871,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.7905982906,
        "MMLU_medical_genetics":0.53,
        "MMLU_miscellaneous":0.754789272,
        "MMLU_moral_disputes":0.6387283237,
        "MMLU_moral_scenarios":0.4424581006,
        "MMLU_nutrition":0.6013071895,
        "MMLU_philosophy":0.6430868167,
        "MMLU_prehistory":0.6358024691,
        "MMLU_professional_accounting":0.4255319149,
        "MMLU_professional_law":0.4315514993,
        "MMLU_professional_medicine":0.5992647059,
        "MMLU_professional_psychology":0.5571895425,
        "MMLU_public_relations":0.6818181818,
        "MMLU_security_studies":0.6040816327,
        "MMLU_sociology":0.7562189055,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.7777777778
    },
    {
        "Model":"openchat_v3.2",
        "URL":"https:\/\/huggingface.co\/openchat\/openchat_v3.2",
        "full_model_name":"openchat\/openchat_v3.2",
        "Parameters":null,
        "MMLU_average":0.566826847,
        "arc:challenge|25":0.5588737201,
        "hellaswag|10":0.6248755228,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5657894737,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.6339622642,
        "MMLU_college_biology":0.5972222222,
        "MMLU_college_chemistry":0.48,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5549132948,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.73,
        "MMLU_conceptual_physics":0.429787234,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.3306878307,
        "MMLU_formal_logic":0.4047619048,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.6774193548,
        "MMLU_high_school_chemistry":0.4532019704,
        "MMLU_high_school_computer_science":0.58,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.7171717172,
        "MMLU_high_school_government_and_politics":0.7979274611,
        "MMLU_high_school_macroeconomics":0.5435897436,
        "MMLU_high_school_mathematics":0.2925925926,
        "MMLU_high_school_microeconomics":0.5630252101,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.7486238532,
        "MMLU_high_school_statistics":0.4861111111,
        "MMLU_high_school_us_history":0.7450980392,
        "MMLU_high_school_world_history":0.7341772152,
        "MMLU_human_aging":0.6457399103,
        "MMLU_human_sexuality":0.6564885496,
        "MMLU_international_law":0.7107438017,
        "MMLU_jurisprudence":0.7037037037,
        "MMLU_logical_fallacies":0.6687116564,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.8162393162,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.7471264368,
        "MMLU_moral_disputes":0.6329479769,
        "MMLU_moral_scenarios":0.4480446927,
        "MMLU_nutrition":0.6470588235,
        "MMLU_philosophy":0.6430868167,
        "MMLU_prehistory":0.6265432099,
        "MMLU_professional_accounting":0.4007092199,
        "MMLU_professional_law":0.426988266,
        "MMLU_professional_medicine":0.5992647059,
        "MMLU_professional_psychology":0.568627451,
        "MMLU_public_relations":0.6272727273,
        "MMLU_security_studies":0.6530612245,
        "MMLU_sociology":0.7512437811,
        "MMLU_us_foreign_policy":0.81,
        "MMLU_virology":0.4759036145,
        "MMLU_world_religions":0.7719298246
    },
    {
        "Model":"openbuddy-mistral-7b-v13.1",
        "URL":"https:\/\/huggingface.co\/OpenBuddy\/openbuddy-mistral-7b-v13.1",
        "full_model_name":"OpenBuddy\/openbuddy-mistral-7b-v13.1",
        "Parameters":7.0,
        "MMLU_average":0.566807112,
        "arc:challenge|25":0.4914675768,
        "hellaswag|10":0.5593507269,
        "MMLU_abstract_algebra":0.38,
        "MMLU_anatomy":0.5185185185,
        "MMLU_astronomy":0.5789473684,
        "MMLU_business_ethics":0.6,
        "MMLU_clinical_knowledge":0.6226415094,
        "MMLU_college_biology":0.6180555556,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.5260115607,
        "MMLU_college_physics":0.3529411765,
        "MMLU_computer_security":0.75,
        "MMLU_conceptual_physics":0.4978723404,
        "MMLU_econometrics":0.4035087719,
        "MMLU_electrical_engineering":0.4827586207,
        "MMLU_elementary_mathematics":0.3544973545,
        "MMLU_formal_logic":0.3888888889,
        "MMLU_global_facts":0.29,
        "MMLU_high_school_biology":0.6516129032,
        "MMLU_high_school_chemistry":0.4334975369,
        "MMLU_high_school_computer_science":0.65,
        "MMLU_high_school_european_history":0.7151515152,
        "MMLU_high_school_geography":0.702020202,
        "MMLU_high_school_government_and_politics":0.8186528497,
        "MMLU_high_school_macroeconomics":0.5333333333,
        "MMLU_high_school_mathematics":0.3259259259,
        "MMLU_high_school_microeconomics":0.5756302521,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.7431192661,
        "MMLU_high_school_statistics":0.3564814815,
        "MMLU_high_school_us_history":0.7058823529,
        "MMLU_high_school_world_history":0.7172995781,
        "MMLU_human_aging":0.6322869955,
        "MMLU_human_sexuality":0.6564885496,
        "MMLU_international_law":0.7603305785,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.6687116564,
        "MMLU_machine_learning":0.4464285714,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.858974359,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.7471264368,
        "MMLU_moral_disputes":0.6329479769,
        "MMLU_moral_scenarios":0.251396648,
        "MMLU_nutrition":0.6535947712,
        "MMLU_philosophy":0.6398713826,
        "MMLU_prehistory":0.6358024691,
        "MMLU_professional_accounting":0.4184397163,
        "MMLU_professional_law":0.4165580183,
        "MMLU_professional_medicine":0.4889705882,
        "MMLU_professional_psychology":0.5490196078,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.6163265306,
        "MMLU_sociology":0.7462686567,
        "MMLU_us_foreign_policy":0.79,
        "MMLU_virology":0.4759036145,
        "MMLU_world_religions":0.7660818713
    },
    {
        "Model":"llama-2-13b-Beluga-QLoRA",
        "URL":"https:\/\/huggingface.co\/yeontaek\/llama-2-13b-Beluga-QLoRA",
        "full_model_name":"yeontaek\/llama-2-13b-Beluga-QLoRA",
        "Parameters":13.0,
        "MMLU_average":0.5666941928,
        "arc:challenge|25":0.5460750853,
        "hellaswag|10":0.6101374228,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.5723684211,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.5962264151,
        "MMLU_college_biology":0.6041666667,
        "MMLU_college_chemistry":0.49,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.5375722543,
        "MMLU_college_physics":0.3921568627,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.429787234,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.3518518519,
        "MMLU_formal_logic":0.380952381,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.6483870968,
        "MMLU_high_school_chemistry":0.4876847291,
        "MMLU_high_school_computer_science":0.57,
        "MMLU_high_school_european_history":0.7090909091,
        "MMLU_high_school_geography":0.7272727273,
        "MMLU_high_school_government_and_politics":0.8134715026,
        "MMLU_high_school_macroeconomics":0.5256410256,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.6386554622,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7651376147,
        "MMLU_high_school_statistics":0.5277777778,
        "MMLU_high_school_us_history":0.7450980392,
        "MMLU_high_school_world_history":0.7510548523,
        "MMLU_human_aging":0.6322869955,
        "MMLU_human_sexuality":0.6641221374,
        "MMLU_international_law":0.7851239669,
        "MMLU_jurisprudence":0.6944444444,
        "MMLU_logical_fallacies":0.7116564417,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.7572815534,
        "MMLU_marketing":0.8076923077,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7777777778,
        "MMLU_moral_disputes":0.6589595376,
        "MMLU_moral_scenarios":0.4279329609,
        "MMLU_nutrition":0.6274509804,
        "MMLU_philosophy":0.6752411576,
        "MMLU_prehistory":0.6450617284,
        "MMLU_professional_accounting":0.4113475177,
        "MMLU_professional_law":0.4224250326,
        "MMLU_professional_medicine":0.5514705882,
        "MMLU_professional_psychology":0.5816993464,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6367346939,
        "MMLU_sociology":0.5970149254,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4397590361,
        "MMLU_world_religions":0.7660818713
    },
    {
        "Model":"Llama-2-13b-FINETUNE4_TEST2",
        "URL":"https:\/\/huggingface.co\/wei123602\/Llama-2-13b-FINETUNE4_TEST2",
        "full_model_name":"wei123602\/Llama-2-13b-FINETUNE4_TEST2",
        "Parameters":13.0,
        "MMLU_average":0.566115619,
        "arc:challenge|25":0.5401023891,
        "hellaswag|10":0.6101374228,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.5037037037,
        "MMLU_astronomy":0.5789473684,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.5962264151,
        "MMLU_college_biology":0.5902777778,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.51,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.4855491329,
        "MMLU_college_physics":0.3921568627,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.4425531915,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.5172413793,
        "MMLU_elementary_mathematics":0.3465608466,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.6419354839,
        "MMLU_high_school_chemistry":0.4088669951,
        "MMLU_high_school_computer_science":0.6,
        "MMLU_high_school_european_history":0.7151515152,
        "MMLU_high_school_geography":0.7121212121,
        "MMLU_high_school_government_and_politics":0.8445595855,
        "MMLU_high_school_macroeconomics":0.5564102564,
        "MMLU_high_school_mathematics":0.3444444444,
        "MMLU_high_school_microeconomics":0.6050420168,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.7816513761,
        "MMLU_high_school_statistics":0.462962963,
        "MMLU_high_school_us_history":0.7401960784,
        "MMLU_high_school_world_history":0.7805907173,
        "MMLU_human_aging":0.6547085202,
        "MMLU_human_sexuality":0.641221374,
        "MMLU_international_law":0.7438016529,
        "MMLU_jurisprudence":0.7592592593,
        "MMLU_logical_fallacies":0.6932515337,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.7863247863,
        "MMLU_medical_genetics":0.66,
        "MMLU_miscellaneous":0.779054917,
        "MMLU_moral_disputes":0.6242774566,
        "MMLU_moral_scenarios":0.3139664804,
        "MMLU_nutrition":0.5947712418,
        "MMLU_philosophy":0.6559485531,
        "MMLU_prehistory":0.6481481481,
        "MMLU_professional_accounting":0.4574468085,
        "MMLU_professional_law":0.4478487614,
        "MMLU_professional_medicine":0.5551470588,
        "MMLU_professional_psychology":0.5751633987,
        "MMLU_public_relations":0.6272727273,
        "MMLU_security_studies":0.5346938776,
        "MMLU_sociology":0.736318408,
        "MMLU_us_foreign_policy":0.77,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.7602339181
    },
    {
        "Model":"llama-30b-supercot",
        "URL":"https:\/\/huggingface.co\/ausboss\/llama-30b-supercot",
        "full_model_name":"ausboss\/llama-30b-supercot",
        "Parameters":30.0,
        "MMLU_average":0.5656010499,
        "arc:challenge|25":0.6100682594,
        "hellaswag|10":0.6498705437,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.5407407407,
        "MMLU_astronomy":0.6184210526,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.5773584906,
        "MMLU_college_biology":0.5902777778,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.5375722543,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.67,
        "MMLU_conceptual_physics":0.5063829787,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.4482758621,
        "MMLU_elementary_mathematics":0.3624338624,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.6709677419,
        "MMLU_high_school_chemistry":0.3793103448,
        "MMLU_high_school_computer_science":0.59,
        "MMLU_high_school_european_history":0.7333333333,
        "MMLU_high_school_geography":0.7828282828,
        "MMLU_high_school_government_and_politics":0.8082901554,
        "MMLU_high_school_macroeconomics":0.5512820513,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.5504201681,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.7486238532,
        "MMLU_high_school_statistics":0.462962963,
        "MMLU_high_school_us_history":0.7598039216,
        "MMLU_high_school_world_history":0.7932489451,
        "MMLU_human_aging":0.6322869955,
        "MMLU_human_sexuality":0.6335877863,
        "MMLU_international_law":0.7603305785,
        "MMLU_jurisprudence":0.6944444444,
        "MMLU_logical_fallacies":0.7055214724,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.7766990291,
        "MMLU_marketing":0.8205128205,
        "MMLU_medical_genetics":0.67,
        "MMLU_miscellaneous":0.7650063857,
        "MMLU_moral_disputes":0.6300578035,
        "MMLU_moral_scenarios":0.2927374302,
        "MMLU_nutrition":0.5751633987,
        "MMLU_philosophy":0.6623794212,
        "MMLU_prehistory":0.6327160494,
        "MMLU_professional_accounting":0.4361702128,
        "MMLU_professional_law":0.424380704,
        "MMLU_professional_medicine":0.5477941176,
        "MMLU_professional_psychology":0.5964052288,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.5755102041,
        "MMLU_sociology":0.7263681592,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.4939759036,
        "MMLU_world_religions":0.783625731
    },
    {
        "Model":"Xwin-LM-13B-V0.1",
        "URL":"https:\/\/huggingface.co\/Xwin-LM\/Xwin-LM-13B-V0.1",
        "full_model_name":"Xwin-LM\/Xwin-LM-13B-V0.1",
        "Parameters":13.0,
        "MMLU_average":0.5652857968,
        "arc:challenge|25":0.587883959,
        "hellaswag|10":0.6381198964,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.5037037037,
        "MMLU_astronomy":0.5394736842,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.6264150943,
        "MMLU_college_biology":0.5972222222,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.49,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.5722543353,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.74,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.524137931,
        "MMLU_elementary_mathematics":0.3227513228,
        "MMLU_formal_logic":0.3412698413,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.6774193548,
        "MMLU_high_school_chemistry":0.4827586207,
        "MMLU_high_school_computer_science":0.56,
        "MMLU_high_school_european_history":0.6484848485,
        "MMLU_high_school_geography":0.702020202,
        "MMLU_high_school_government_and_politics":0.8031088083,
        "MMLU_high_school_macroeconomics":0.5256410256,
        "MMLU_high_school_mathematics":0.2925925926,
        "MMLU_high_school_microeconomics":0.5756302521,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.752293578,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.75,
        "MMLU_high_school_world_history":0.7510548523,
        "MMLU_human_aging":0.6547085202,
        "MMLU_human_sexuality":0.641221374,
        "MMLU_international_law":0.7438016529,
        "MMLU_jurisprudence":0.7407407407,
        "MMLU_logical_fallacies":0.6809815951,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.7669902913,
        "MMLU_marketing":0.8034188034,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.7765006386,
        "MMLU_moral_disputes":0.6473988439,
        "MMLU_moral_scenarios":0.4044692737,
        "MMLU_nutrition":0.660130719,
        "MMLU_philosophy":0.6495176849,
        "MMLU_prehistory":0.6728395062,
        "MMLU_professional_accounting":0.4255319149,
        "MMLU_professional_law":0.4308996089,
        "MMLU_professional_medicine":0.5919117647,
        "MMLU_professional_psychology":0.5849673203,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.6775510204,
        "MMLU_sociology":0.7462686567,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.7543859649
    },
    {
        "Model":"orca_mini_v3_13b",
        "URL":"https:\/\/huggingface.co\/psmathur\/orca_mini_v3_13b",
        "full_model_name":"psmathur\/orca_mini_v3_13b",
        "Parameters":13.0,
        "MMLU_average":0.5651926585,
        "arc:challenge|25":0.6023890785,
        "hellaswag|10":0.6250746863,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.5526315789,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.6150943396,
        "MMLU_college_biology":0.5763888889,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.5260115607,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.74,
        "MMLU_conceptual_physics":0.485106383,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.5310344828,
        "MMLU_elementary_mathematics":0.3068783069,
        "MMLU_formal_logic":0.3412698413,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.635483871,
        "MMLU_high_school_chemistry":0.4433497537,
        "MMLU_high_school_computer_science":0.59,
        "MMLU_high_school_european_history":0.6909090909,
        "MMLU_high_school_geography":0.7373737374,
        "MMLU_high_school_government_and_politics":0.8082901554,
        "MMLU_high_school_macroeconomics":0.5666666667,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.6050420168,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.7541284404,
        "MMLU_high_school_statistics":0.4907407407,
        "MMLU_high_school_us_history":0.7549019608,
        "MMLU_high_school_world_history":0.7426160338,
        "MMLU_human_aging":0.66367713,
        "MMLU_human_sexuality":0.641221374,
        "MMLU_international_law":0.7438016529,
        "MMLU_jurisprudence":0.7314814815,
        "MMLU_logical_fallacies":0.6932515337,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.7669902913,
        "MMLU_marketing":0.7991452991,
        "MMLU_medical_genetics":0.64,
        "MMLU_miscellaneous":0.7637292465,
        "MMLU_moral_disputes":0.6271676301,
        "MMLU_moral_scenarios":0.3664804469,
        "MMLU_nutrition":0.6111111111,
        "MMLU_philosophy":0.652733119,
        "MMLU_prehistory":0.6388888889,
        "MMLU_professional_accounting":0.4219858156,
        "MMLU_professional_law":0.426988266,
        "MMLU_professional_medicine":0.5367647059,
        "MMLU_professional_psychology":0.5751633987,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.6448979592,
        "MMLU_sociology":0.6169154229,
        "MMLU_us_foreign_policy":0.85,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.7719298246
    },
    {
        "Model":"firefly-llama2-13b-v1.2",
        "URL":"https:\/\/huggingface.co\/YeungNLP\/firefly-llama2-13b-v1.2",
        "full_model_name":"YeungNLP\/firefly-llama2-13b-v1.2",
        "Parameters":13.0,
        "MMLU_average":0.5651428768,
        "arc:challenge|25":0.5716723549,
        "hellaswag|10":0.6093407688,
        "MMLU_abstract_algebra":0.38,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.5526315789,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.6188679245,
        "MMLU_college_biology":0.5625,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.49,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.5202312139,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.72,
        "MMLU_conceptual_physics":0.3957446809,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.524137931,
        "MMLU_elementary_mathematics":0.3439153439,
        "MMLU_formal_logic":0.3333333333,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.6838709677,
        "MMLU_high_school_chemistry":0.4729064039,
        "MMLU_high_school_computer_science":0.57,
        "MMLU_high_school_european_history":0.6909090909,
        "MMLU_high_school_geography":0.7171717172,
        "MMLU_high_school_government_and_politics":0.8082901554,
        "MMLU_high_school_macroeconomics":0.5153846154,
        "MMLU_high_school_mathematics":0.3481481481,
        "MMLU_high_school_microeconomics":0.5756302521,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.7467889908,
        "MMLU_high_school_statistics":0.4212962963,
        "MMLU_high_school_us_history":0.7745098039,
        "MMLU_high_school_world_history":0.7341772152,
        "MMLU_human_aging":0.6502242152,
        "MMLU_human_sexuality":0.6259541985,
        "MMLU_international_law":0.7685950413,
        "MMLU_jurisprudence":0.75,
        "MMLU_logical_fallacies":0.6871165644,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.7905982906,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.7522349936,
        "MMLU_moral_disputes":0.6416184971,
        "MMLU_moral_scenarios":0.4402234637,
        "MMLU_nutrition":0.5980392157,
        "MMLU_philosophy":0.6430868167,
        "MMLU_prehistory":0.6358024691,
        "MMLU_professional_accounting":0.4042553191,
        "MMLU_professional_law":0.4106910039,
        "MMLU_professional_medicine":0.5367647059,
        "MMLU_professional_psychology":0.5588235294,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6204081633,
        "MMLU_sociology":0.736318408,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4819277108,
        "MMLU_world_religions":0.7543859649
    },
    {
        "Model":"llama-2-13B-ensemble-v4",
        "URL":"https:\/\/huggingface.co\/yeontaek\/llama-2-13B-ensemble-v4",
        "full_model_name":"yeontaek\/llama-2-13B-ensemble-v4",
        "Parameters":13.0,
        "MMLU_average":0.5647586461,
        "arc:challenge|25":0.6015358362,
        "hellaswag|10":0.624775941,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.5526315789,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.6113207547,
        "MMLU_college_biology":0.5694444444,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.5260115607,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.74,
        "MMLU_conceptual_physics":0.485106383,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.524137931,
        "MMLU_elementary_mathematics":0.3042328042,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.6322580645,
        "MMLU_high_school_chemistry":0.4433497537,
        "MMLU_high_school_computer_science":0.59,
        "MMLU_high_school_european_history":0.6909090909,
        "MMLU_high_school_geography":0.7373737374,
        "MMLU_high_school_government_and_politics":0.8082901554,
        "MMLU_high_school_macroeconomics":0.5641025641,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.6050420168,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.7559633028,
        "MMLU_high_school_statistics":0.4907407407,
        "MMLU_high_school_us_history":0.75,
        "MMLU_high_school_world_history":0.7341772152,
        "MMLU_human_aging":0.668161435,
        "MMLU_human_sexuality":0.6488549618,
        "MMLU_international_law":0.7438016529,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.6871165644,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.7669902913,
        "MMLU_marketing":0.7991452991,
        "MMLU_medical_genetics":0.63,
        "MMLU_miscellaneous":0.7650063857,
        "MMLU_moral_disputes":0.6242774566,
        "MMLU_moral_scenarios":0.3664804469,
        "MMLU_nutrition":0.6045751634,
        "MMLU_philosophy":0.652733119,
        "MMLU_prehistory":0.6419753086,
        "MMLU_professional_accounting":0.4184397163,
        "MMLU_professional_law":0.4263363755,
        "MMLU_professional_medicine":0.5330882353,
        "MMLU_professional_psychology":0.5767973856,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.6489795918,
        "MMLU_sociology":0.6268656716,
        "MMLU_us_foreign_policy":0.85,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.7719298246
    },
    {
        "Model":"llama2-13b-instructmining-40k-sharegpt",
        "URL":"https:\/\/huggingface.co\/yihan6324\/llama2-13b-instructmining-40k-sharegpt",
        "full_model_name":"yihan6324\/llama2-13b-instructmining-40k-sharegpt",
        "Parameters":13.0,
        "MMLU_average":0.5647519866,
        "arc:challenge|25":0.5656996587,
        "hellaswag|10":0.6328420633,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.625,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.6150943396,
        "MMLU_college_biology":0.6111111111,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.5895953757,
        "MMLU_college_physics":0.3431372549,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.4595744681,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.2936507937,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.6709677419,
        "MMLU_high_school_chemistry":0.4532019704,
        "MMLU_high_school_computer_science":0.56,
        "MMLU_high_school_european_history":0.6787878788,
        "MMLU_high_school_geography":0.7171717172,
        "MMLU_high_school_government_and_politics":0.8134715026,
        "MMLU_high_school_macroeconomics":0.5333333333,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.5378151261,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.7669724771,
        "MMLU_high_school_statistics":0.4305555556,
        "MMLU_high_school_us_history":0.7647058824,
        "MMLU_high_school_world_history":0.7510548523,
        "MMLU_human_aging":0.6726457399,
        "MMLU_human_sexuality":0.6183206107,
        "MMLU_international_law":0.7190082645,
        "MMLU_jurisprudence":0.712962963,
        "MMLU_logical_fallacies":0.6625766871,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.8418803419,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.7509578544,
        "MMLU_moral_disputes":0.6300578035,
        "MMLU_moral_scenarios":0.4122905028,
        "MMLU_nutrition":0.6013071895,
        "MMLU_philosophy":0.6205787781,
        "MMLU_prehistory":0.6265432099,
        "MMLU_professional_accounting":0.4219858156,
        "MMLU_professional_law":0.425684485,
        "MMLU_professional_medicine":0.5735294118,
        "MMLU_professional_psychology":0.5784313725,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.6285714286,
        "MMLU_sociology":0.776119403,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4939759036,
        "MMLU_world_religions":0.7719298246
    },
    {
        "Model":"Wizard-Vicuna-30B-Uncensored-GPTQ",
        "URL":"https:\/\/huggingface.co\/TheBloke\/Wizard-Vicuna-30B-Uncensored-GPTQ",
        "full_model_name":"TheBloke\/Wizard-Vicuna-30B-Uncensored-GPTQ",
        "Parameters":30.0,
        "MMLU_average":0.5645849913,
        "arc:challenge|25":0.5836177474,
        "hellaswag|10":0.6248755228,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.5328947368,
        "MMLU_business_ethics":0.6,
        "MMLU_clinical_knowledge":0.5849056604,
        "MMLU_college_biology":0.5833333333,
        "MMLU_college_chemistry":0.36,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.5144508671,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.4425531915,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.3253968254,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.47,
        "MMLU_high_school_biology":0.6677419355,
        "MMLU_high_school_chemistry":0.4088669951,
        "MMLU_high_school_computer_science":0.57,
        "MMLU_high_school_european_history":0.6909090909,
        "MMLU_high_school_geography":0.7222222222,
        "MMLU_high_school_government_and_politics":0.7823834197,
        "MMLU_high_school_macroeconomics":0.5666666667,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.5798319328,
        "MMLU_high_school_physics":0.3708609272,
        "MMLU_high_school_psychology":0.7376146789,
        "MMLU_high_school_statistics":0.4212962963,
        "MMLU_high_school_us_history":0.7549019608,
        "MMLU_high_school_world_history":0.7637130802,
        "MMLU_human_aging":0.6457399103,
        "MMLU_human_sexuality":0.641221374,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.6851851852,
        "MMLU_logical_fallacies":0.6932515337,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.7669902913,
        "MMLU_marketing":0.8547008547,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.7471264368,
        "MMLU_moral_disputes":0.6300578035,
        "MMLU_moral_scenarios":0.4234636872,
        "MMLU_nutrition":0.591503268,
        "MMLU_philosophy":0.7041800643,
        "MMLU_prehistory":0.6265432099,
        "MMLU_professional_accounting":0.4361702128,
        "MMLU_professional_law":0.4361147327,
        "MMLU_professional_medicine":0.5551470588,
        "MMLU_professional_psychology":0.5784313725,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.612244898,
        "MMLU_sociology":0.776119403,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.4879518072,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"Stheno-Inverted-L2-13B",
        "URL":"https:\/\/huggingface.co\/Sao10K\/Stheno-Inverted-L2-13B",
        "full_model_name":"Sao10K\/Stheno-Inverted-L2-13B",
        "Parameters":13.0,
        "MMLU_average":0.5645347973,
        "arc:challenge|25":0.5605802048,
        "hellaswag|10":0.639514041,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.5592105263,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.641509434,
        "MMLU_college_biology":0.6111111111,
        "MMLU_college_chemistry":0.45,
        "MMLU_college_computer_science":0.49,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.5144508671,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4382978723,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.5517241379,
        "MMLU_elementary_mathematics":0.3333333333,
        "MMLU_formal_logic":0.380952381,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.6419354839,
        "MMLU_high_school_chemistry":0.4285714286,
        "MMLU_high_school_computer_science":0.58,
        "MMLU_high_school_european_history":0.6848484848,
        "MMLU_high_school_geography":0.6818181818,
        "MMLU_high_school_government_and_politics":0.8186528497,
        "MMLU_high_school_macroeconomics":0.5230769231,
        "MMLU_high_school_mathematics":0.3185185185,
        "MMLU_high_school_microeconomics":0.5420168067,
        "MMLU_high_school_physics":0.3708609272,
        "MMLU_high_school_psychology":0.7321100917,
        "MMLU_high_school_statistics":0.3842592593,
        "MMLU_high_school_us_history":0.7549019608,
        "MMLU_high_school_world_history":0.7383966245,
        "MMLU_human_aging":0.6726457399,
        "MMLU_human_sexuality":0.641221374,
        "MMLU_international_law":0.7603305785,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.6809815951,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.7905982906,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.7624521073,
        "MMLU_moral_disputes":0.6329479769,
        "MMLU_moral_scenarios":0.4491620112,
        "MMLU_nutrition":0.6078431373,
        "MMLU_philosophy":0.6302250804,
        "MMLU_prehistory":0.6172839506,
        "MMLU_professional_accounting":0.4007092199,
        "MMLU_professional_law":0.4178617992,
        "MMLU_professional_medicine":0.5183823529,
        "MMLU_professional_psychology":0.5604575163,
        "MMLU_public_relations":0.6272727273,
        "MMLU_security_studies":0.6040816327,
        "MMLU_sociology":0.7562189055,
        "MMLU_us_foreign_policy":0.85,
        "MMLU_virology":0.4819277108,
        "MMLU_world_religions":0.7660818713
    },
    {
        "Model":"OrcaMini-Platypus2-13B-QLoRA-0.80-epoch",
        "URL":"https:\/\/huggingface.co\/TFLai\/OrcaMini-Platypus2-13B-QLoRA-0.80-epoch",
        "full_model_name":"TFLai\/OrcaMini-Platypus2-13B-QLoRA-0.80-epoch",
        "Parameters":13.0,
        "MMLU_average":0.5641608673,
        "arc:challenge|25":0.5810580205,
        "hellaswag|10":0.6268671579,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5921052632,
        "MMLU_business_ethics":0.58,
        "MMLU_clinical_knowledge":0.5962264151,
        "MMLU_college_biology":0.625,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.5549132948,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.4808510638,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.5172413793,
        "MMLU_elementary_mathematics":0.3095238095,
        "MMLU_formal_logic":0.3333333333,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.6129032258,
        "MMLU_high_school_chemistry":0.4384236453,
        "MMLU_high_school_computer_science":0.58,
        "MMLU_high_school_european_history":0.7393939394,
        "MMLU_high_school_geography":0.7323232323,
        "MMLU_high_school_government_and_politics":0.8393782383,
        "MMLU_high_school_macroeconomics":0.5538461538,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.5840336134,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.7816513761,
        "MMLU_high_school_statistics":0.4675925926,
        "MMLU_high_school_us_history":0.7794117647,
        "MMLU_high_school_world_history":0.7637130802,
        "MMLU_human_aging":0.6457399103,
        "MMLU_human_sexuality":0.6335877863,
        "MMLU_international_law":0.7190082645,
        "MMLU_jurisprudence":0.7037037037,
        "MMLU_logical_fallacies":0.6687116564,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.7475728155,
        "MMLU_marketing":0.8461538462,
        "MMLU_medical_genetics":0.63,
        "MMLU_miscellaneous":0.7701149425,
        "MMLU_moral_disputes":0.6184971098,
        "MMLU_moral_scenarios":0.3474860335,
        "MMLU_nutrition":0.5816993464,
        "MMLU_philosophy":0.6398713826,
        "MMLU_prehistory":0.6481481481,
        "MMLU_professional_accounting":0.4361702128,
        "MMLU_professional_law":0.4367666232,
        "MMLU_professional_medicine":0.5625,
        "MMLU_professional_psychology":0.5718954248,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.6163265306,
        "MMLU_sociology":0.6069651741,
        "MMLU_us_foreign_policy":0.8,
        "MMLU_virology":0.4518072289,
        "MMLU_world_religions":0.7894736842
    },
    {
        "Model":"Stheno-1.1-L2-13B",
        "URL":"https:\/\/huggingface.co\/Sao10K\/Stheno-1.1-L2-13B",
        "full_model_name":"Sao10K\/Stheno-1.1-L2-13B",
        "Parameters":13.0,
        "MMLU_average":0.5639089846,
        "arc:challenge|25":0.5708191126,
        "hellaswag|10":0.6461860187,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5526315789,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.6150943396,
        "MMLU_college_biology":0.6180555556,
        "MMLU_college_chemistry":0.46,
        "MMLU_college_computer_science":0.49,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.5260115607,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.4468085106,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.5448275862,
        "MMLU_elementary_mathematics":0.328042328,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.6709677419,
        "MMLU_high_school_chemistry":0.4679802956,
        "MMLU_high_school_computer_science":0.61,
        "MMLU_high_school_european_history":0.6848484848,
        "MMLU_high_school_geography":0.696969697,
        "MMLU_high_school_government_and_politics":0.8186528497,
        "MMLU_high_school_macroeconomics":0.5358974359,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.5798319328,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.7357798165,
        "MMLU_high_school_statistics":0.3888888889,
        "MMLU_high_school_us_history":0.7598039216,
        "MMLU_high_school_world_history":0.7426160338,
        "MMLU_human_aging":0.6591928251,
        "MMLU_human_sexuality":0.641221374,
        "MMLU_international_law":0.7107438017,
        "MMLU_jurisprudence":0.7314814815,
        "MMLU_logical_fallacies":0.6503067485,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.8162393162,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7432950192,
        "MMLU_moral_disputes":0.6156069364,
        "MMLU_moral_scenarios":0.4659217877,
        "MMLU_nutrition":0.6176470588,
        "MMLU_philosophy":0.6109324759,
        "MMLU_prehistory":0.5956790123,
        "MMLU_professional_accounting":0.4397163121,
        "MMLU_professional_law":0.4276401565,
        "MMLU_professional_medicine":0.5220588235,
        "MMLU_professional_psychology":0.5718954248,
        "MMLU_public_relations":0.6272727273,
        "MMLU_security_studies":0.6326530612,
        "MMLU_sociology":0.7412935323,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.4638554217,
        "MMLU_world_religions":0.7660818713
    },
    {
        "Model":"Llama-2-13b-FINETUNE4_TEST3",
        "URL":"https:\/\/huggingface.co\/wei123602\/Llama-2-13b-FINETUNE4_TEST3",
        "full_model_name":"wei123602\/Llama-2-13b-FINETUNE4_TEST3",
        "Parameters":13.0,
        "MMLU_average":0.5637418962,
        "arc:challenge|25":0.5358361775,
        "hellaswag|10":0.6083449512,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.5185185185,
        "MMLU_astronomy":0.5986842105,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.6452830189,
        "MMLU_college_biology":0.5694444444,
        "MMLU_college_chemistry":0.45,
        "MMLU_college_computer_science":0.4,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.5549132948,
        "MMLU_college_physics":0.3431372549,
        "MMLU_computer_security":0.64,
        "MMLU_conceptual_physics":0.4170212766,
        "MMLU_econometrics":0.3771929825,
        "MMLU_electrical_engineering":0.4344827586,
        "MMLU_elementary_mathematics":0.3544973545,
        "MMLU_formal_logic":0.4126984127,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.5774193548,
        "MMLU_high_school_chemistry":0.4630541872,
        "MMLU_high_school_computer_science":0.62,
        "MMLU_high_school_european_history":0.7454545455,
        "MMLU_high_school_geography":0.7121212121,
        "MMLU_high_school_government_and_politics":0.8238341969,
        "MMLU_high_school_macroeconomics":0.5794871795,
        "MMLU_high_school_mathematics":0.3185185185,
        "MMLU_high_school_microeconomics":0.6428571429,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.7724770642,
        "MMLU_high_school_statistics":0.5046296296,
        "MMLU_high_school_us_history":0.75,
        "MMLU_high_school_world_history":0.7890295359,
        "MMLU_human_aging":0.6322869955,
        "MMLU_human_sexuality":0.6030534351,
        "MMLU_international_law":0.6859504132,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.6441717791,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.7669902913,
        "MMLU_marketing":0.7991452991,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.7713920817,
        "MMLU_moral_disputes":0.6271676301,
        "MMLU_moral_scenarios":0.3162011173,
        "MMLU_nutrition":0.6078431373,
        "MMLU_philosophy":0.6463022508,
        "MMLU_prehistory":0.6450617284,
        "MMLU_professional_accounting":0.5,
        "MMLU_professional_law":0.462190352,
        "MMLU_professional_medicine":0.5661764706,
        "MMLU_professional_psychology":0.5947712418,
        "MMLU_public_relations":0.6272727273,
        "MMLU_security_studies":0.5959183673,
        "MMLU_sociology":0.6069651741,
        "MMLU_us_foreign_policy":0.74,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.7543859649
    },
    {
        "Model":"Asimov-7B-v1",
        "URL":"https:\/\/huggingface.co\/prithivida\/Asimov-7B-v1",
        "full_model_name":"prithivida\/Asimov-7B-v1",
        "Parameters":7.0,
        "MMLU_average":0.5634822734,
        "arc:challenge|25":0.5460750853,
        "hellaswag|10":0.6097390958,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5526315789,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.6188679245,
        "MMLU_college_biology":0.5833333333,
        "MMLU_college_chemistry":0.47,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.41,
        "MMLU_college_medicine":0.5664739884,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.4595744681,
        "MMLU_econometrics":0.3333333333,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.3862433862,
        "MMLU_formal_logic":0.4126984127,
        "MMLU_global_facts":0.28,
        "MMLU_high_school_biology":0.6741935484,
        "MMLU_high_school_chemistry":0.4088669951,
        "MMLU_high_school_computer_science":0.49,
        "MMLU_high_school_european_history":0.7333333333,
        "MMLU_high_school_geography":0.7323232323,
        "MMLU_high_school_government_and_politics":0.829015544,
        "MMLU_high_school_macroeconomics":0.5487179487,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.6050420168,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.7431192661,
        "MMLU_high_school_statistics":0.4907407407,
        "MMLU_high_school_us_history":0.7058823529,
        "MMLU_high_school_world_history":0.7383966245,
        "MMLU_human_aging":0.6143497758,
        "MMLU_human_sexuality":0.7480916031,
        "MMLU_international_law":0.7603305785,
        "MMLU_jurisprudence":0.6574074074,
        "MMLU_logical_fallacies":0.6748466258,
        "MMLU_machine_learning":0.4553571429,
        "MMLU_management":0.6990291262,
        "MMLU_marketing":0.811965812,
        "MMLU_medical_genetics":0.65,
        "MMLU_miscellaneous":0.7075351213,
        "MMLU_moral_disputes":0.5924855491,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.6274509804,
        "MMLU_philosophy":0.6270096463,
        "MMLU_prehistory":0.5833333333,
        "MMLU_professional_accounting":0.4042553191,
        "MMLU_professional_law":0.3852672751,
        "MMLU_professional_medicine":0.5698529412,
        "MMLU_professional_psychology":0.568627451,
        "MMLU_public_relations":0.5363636364,
        "MMLU_security_studies":0.6571428571,
        "MMLU_sociology":0.7711442786,
        "MMLU_us_foreign_policy":0.76,
        "MMLU_virology":0.4518072289,
        "MMLU_world_religions":0.7368421053
    },
    {
        "Model":"OpenHermes-13B",
        "URL":"https:\/\/huggingface.co\/teknium\/OpenHermes-13B",
        "full_model_name":"teknium\/OpenHermes-13B",
        "Parameters":13.0,
        "MMLU_average":0.563472778,
        "arc:challenge|25":0.5648464164,
        "hellaswag|10":0.6246763593,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5723684211,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.5735849057,
        "MMLU_college_biology":0.5972222222,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.43,
        "MMLU_college_medicine":0.5375722543,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.4765957447,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.3597883598,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.6387096774,
        "MMLU_high_school_chemistry":0.4827586207,
        "MMLU_high_school_computer_science":0.53,
        "MMLU_high_school_european_history":0.6424242424,
        "MMLU_high_school_geography":0.6818181818,
        "MMLU_high_school_government_and_politics":0.7875647668,
        "MMLU_high_school_macroeconomics":0.541025641,
        "MMLU_high_school_mathematics":0.3074074074,
        "MMLU_high_school_microeconomics":0.5924369748,
        "MMLU_high_school_physics":0.357615894,
        "MMLU_high_school_psychology":0.7357798165,
        "MMLU_high_school_statistics":0.5,
        "MMLU_high_school_us_history":0.7696078431,
        "MMLU_high_school_world_history":0.7341772152,
        "MMLU_human_aging":0.66367713,
        "MMLU_human_sexuality":0.641221374,
        "MMLU_international_law":0.7603305785,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.6748466258,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.7087378641,
        "MMLU_marketing":0.8247863248,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7496807152,
        "MMLU_moral_disputes":0.6473988439,
        "MMLU_moral_scenarios":0.451396648,
        "MMLU_nutrition":0.6274509804,
        "MMLU_philosophy":0.6077170418,
        "MMLU_prehistory":0.6419753086,
        "MMLU_professional_accounting":0.4539007092,
        "MMLU_professional_law":0.4322033898,
        "MMLU_professional_medicine":0.5330882353,
        "MMLU_professional_psychology":0.5473856209,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.6489795918,
        "MMLU_sociology":0.7263681592,
        "MMLU_us_foreign_policy":0.8,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.7719298246
    },
    {
        "Model":"T1C",
        "URL":"https:\/\/huggingface.co\/AA051610\/T1C",
        "full_model_name":"AA051610\/T1C",
        "Parameters":null,
        "MMLU_average":0.563397183,
        "arc:challenge|25":0.4709897611,
        "hellaswag|10":0.5382393945,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.5037037037,
        "MMLU_astronomy":0.5789473684,
        "MMLU_business_ethics":0.65,
        "MMLU_clinical_knowledge":0.641509434,
        "MMLU_college_biology":0.6111111111,
        "MMLU_college_chemistry":0.45,
        "MMLU_college_computer_science":0.5,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.6184971098,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.5021276596,
        "MMLU_econometrics":0.4473684211,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.3412698413,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.6774193548,
        "MMLU_high_school_chemistry":0.4187192118,
        "MMLU_high_school_computer_science":0.6,
        "MMLU_high_school_european_history":0.2606060606,
        "MMLU_high_school_geography":0.7373737374,
        "MMLU_high_school_government_and_politics":0.7979274611,
        "MMLU_high_school_macroeconomics":0.5128205128,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.5462184874,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7541284404,
        "MMLU_high_school_statistics":0.412037037,
        "MMLU_high_school_us_history":0.6029411765,
        "MMLU_high_school_world_history":0.7637130802,
        "MMLU_human_aging":0.6905829596,
        "MMLU_human_sexuality":0.6488549618,
        "MMLU_international_law":0.7520661157,
        "MMLU_jurisprudence":0.6944444444,
        "MMLU_logical_fallacies":0.6441717791,
        "MMLU_machine_learning":0.4464285714,
        "MMLU_management":0.6990291262,
        "MMLU_marketing":0.8290598291,
        "MMLU_medical_genetics":0.69,
        "MMLU_miscellaneous":0.7624521073,
        "MMLU_moral_disputes":0.6069364162,
        "MMLU_moral_scenarios":0.3027932961,
        "MMLU_nutrition":0.6339869281,
        "MMLU_philosophy":0.6559485531,
        "MMLU_prehistory":0.6080246914,
        "MMLU_professional_accounting":0.4113475177,
        "MMLU_professional_law":0.4517601043,
        "MMLU_professional_medicine":0.5183823529,
        "MMLU_professional_psychology":0.5718954248,
        "MMLU_public_relations":0.5818181818,
        "MMLU_security_studies":0.6326530612,
        "MMLU_sociology":0.7810945274,
        "MMLU_us_foreign_policy":0.78,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.783625731
    },
    {
        "Model":"openbuddy-mistral-7b-v13",
        "URL":"https:\/\/huggingface.co\/OpenBuddy\/openbuddy-mistral-7b-v13",
        "full_model_name":"OpenBuddy\/openbuddy-mistral-7b-v13",
        "Parameters":7.0,
        "MMLU_average":0.5633588541,
        "arc:challenge|25":0.4914675768,
        "hellaswag|10":0.5572595101,
        "MMLU_abstract_algebra":0.39,
        "MMLU_anatomy":0.5037037037,
        "MMLU_astronomy":0.5526315789,
        "MMLU_business_ethics":0.61,
        "MMLU_clinical_knowledge":0.6150943396,
        "MMLU_college_biology":0.6180555556,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.4,
        "MMLU_college_mathematics":0.44,
        "MMLU_college_medicine":0.5028901734,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.75,
        "MMLU_conceptual_physics":0.4723404255,
        "MMLU_econometrics":0.4122807018,
        "MMLU_electrical_engineering":0.524137931,
        "MMLU_elementary_mathematics":0.3544973545,
        "MMLU_formal_logic":0.4047619048,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.635483871,
        "MMLU_high_school_chemistry":0.4433497537,
        "MMLU_high_school_computer_science":0.63,
        "MMLU_high_school_european_history":0.7272727273,
        "MMLU_high_school_geography":0.7272727273,
        "MMLU_high_school_government_and_politics":0.8031088083,
        "MMLU_high_school_macroeconomics":0.5102564103,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.5420168067,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.7596330275,
        "MMLU_high_school_statistics":0.3472222222,
        "MMLU_high_school_us_history":0.7058823529,
        "MMLU_high_school_world_history":0.7299578059,
        "MMLU_human_aging":0.6726457399,
        "MMLU_human_sexuality":0.6183206107,
        "MMLU_international_law":0.7520661157,
        "MMLU_jurisprudence":0.6759259259,
        "MMLU_logical_fallacies":0.6503067485,
        "MMLU_machine_learning":0.4821428571,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.8675213675,
        "MMLU_medical_genetics":0.64,
        "MMLU_miscellaneous":0.7509578544,
        "MMLU_moral_disputes":0.5895953757,
        "MMLU_moral_scenarios":0.2391061453,
        "MMLU_nutrition":0.6078431373,
        "MMLU_philosophy":0.6398713826,
        "MMLU_prehistory":0.5987654321,
        "MMLU_professional_accounting":0.4078014184,
        "MMLU_professional_law":0.426988266,
        "MMLU_professional_medicine":0.5183823529,
        "MMLU_professional_psychology":0.5669934641,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.5918367347,
        "MMLU_sociology":0.7064676617,
        "MMLU_us_foreign_policy":0.77,
        "MMLU_virology":0.4939759036,
        "MMLU_world_religions":0.7719298246
    },
    {
        "Model":"Stheno-L2-13B",
        "URL":"https:\/\/huggingface.co\/Sao10K\/Stheno-L2-13B",
        "full_model_name":"Sao10K\/Stheno-L2-13B",
        "Parameters":13.0,
        "MMLU_average":0.5633416571,
        "arc:challenge|25":0.5742320819,
        "hellaswag|10":0.6434973113,
        "MMLU_abstract_algebra":0.38,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.5855263158,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.6075471698,
        "MMLU_college_biology":0.625,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.4971098266,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.4340425532,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.5379310345,
        "MMLU_elementary_mathematics":0.335978836,
        "MMLU_formal_logic":0.380952381,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.6677419355,
        "MMLU_high_school_chemistry":0.4532019704,
        "MMLU_high_school_computer_science":0.59,
        "MMLU_high_school_european_history":0.6909090909,
        "MMLU_high_school_geography":0.696969697,
        "MMLU_high_school_government_and_politics":0.8186528497,
        "MMLU_high_school_macroeconomics":0.5205128205,
        "MMLU_high_school_mathematics":0.3074074074,
        "MMLU_high_school_microeconomics":0.5630252101,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.7247706422,
        "MMLU_high_school_statistics":0.3611111111,
        "MMLU_high_school_us_history":0.7549019608,
        "MMLU_high_school_world_history":0.7341772152,
        "MMLU_human_aging":0.6726457399,
        "MMLU_human_sexuality":0.6335877863,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.7407407407,
        "MMLU_logical_fallacies":0.6687116564,
        "MMLU_machine_learning":0.4285714286,
        "MMLU_management":0.7087378641,
        "MMLU_marketing":0.8205128205,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.748403576,
        "MMLU_moral_disputes":0.6156069364,
        "MMLU_moral_scenarios":0.4335195531,
        "MMLU_nutrition":0.6176470588,
        "MMLU_philosophy":0.6077170418,
        "MMLU_prehistory":0.5956790123,
        "MMLU_professional_accounting":0.4184397163,
        "MMLU_professional_law":0.4295958279,
        "MMLU_professional_medicine":0.5073529412,
        "MMLU_professional_psychology":0.5653594771,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.6204081633,
        "MMLU_sociology":0.7313432836,
        "MMLU_us_foreign_policy":0.8,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.7719298246
    },
    {
        "Model":"llama-2-13b-OpenOrca_20w",
        "URL":"https:\/\/huggingface.co\/CHIH-HUNG\/llama-2-13b-OpenOrca_20w",
        "full_model_name":"CHIH-HUNG\/llama-2-13b-OpenOrca_20w",
        "Parameters":13.0,
        "MMLU_average":0.5629734206,
        "arc:challenge|25":0.5588737201,
        "hellaswag|10":0.6183031269,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.5723684211,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.6377358491,
        "MMLU_college_biology":0.5694444444,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.5144508671,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.74,
        "MMLU_conceptual_physics":0.4425531915,
        "MMLU_econometrics":0.2192982456,
        "MMLU_electrical_engineering":0.5310344828,
        "MMLU_elementary_mathematics":0.3492063492,
        "MMLU_formal_logic":0.4126984127,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.6516129032,
        "MMLU_high_school_chemistry":0.4679802956,
        "MMLU_high_school_computer_science":0.58,
        "MMLU_high_school_european_history":0.6787878788,
        "MMLU_high_school_geography":0.7121212121,
        "MMLU_high_school_government_and_politics":0.8341968912,
        "MMLU_high_school_macroeconomics":0.5025641026,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.5756302521,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.7339449541,
        "MMLU_high_school_statistics":0.462962963,
        "MMLU_high_school_us_history":0.7598039216,
        "MMLU_high_school_world_history":0.7552742616,
        "MMLU_human_aging":0.6457399103,
        "MMLU_human_sexuality":0.641221374,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.6944444444,
        "MMLU_logical_fallacies":0.6809815951,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.8076923077,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.7662835249,
        "MMLU_moral_disputes":0.6647398844,
        "MMLU_moral_scenarios":0.4480446927,
        "MMLU_nutrition":0.637254902,
        "MMLU_philosophy":0.6495176849,
        "MMLU_prehistory":0.6327160494,
        "MMLU_professional_accounting":0.4184397163,
        "MMLU_professional_law":0.4074315515,
        "MMLU_professional_medicine":0.5367647059,
        "MMLU_professional_psychology":0.5473856209,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.6489795918,
        "MMLU_sociology":0.7512437811,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4036144578,
        "MMLU_world_religions":0.783625731
    },
    {
        "Model":"llama-2-13b-dolphin_5w",
        "URL":"https:\/\/huggingface.co\/CHIH-HUNG\/llama-2-13b-dolphin_5w",
        "full_model_name":"CHIH-HUNG\/llama-2-13b-dolphin_5w",
        "Parameters":13.0,
        "MMLU_average":0.5622853731,
        "arc:challenge|25":0.5656996587,
        "hellaswag|10":0.6195976897,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.5333333333,
        "MMLU_astronomy":0.5657894737,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.6264150943,
        "MMLU_college_biology":0.5625,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.49,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.5260115607,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.4127659574,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.5310344828,
        "MMLU_elementary_mathematics":0.3412698413,
        "MMLU_formal_logic":0.373015873,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.6580645161,
        "MMLU_high_school_chemistry":0.4581280788,
        "MMLU_high_school_computer_science":0.58,
        "MMLU_high_school_european_history":0.703030303,
        "MMLU_high_school_geography":0.702020202,
        "MMLU_high_school_government_and_politics":0.8186528497,
        "MMLU_high_school_macroeconomics":0.5025641026,
        "MMLU_high_school_mathematics":0.3074074074,
        "MMLU_high_school_microeconomics":0.5756302521,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7431192661,
        "MMLU_high_school_statistics":0.4768518519,
        "MMLU_high_school_us_history":0.75,
        "MMLU_high_school_world_history":0.7637130802,
        "MMLU_human_aging":0.6771300448,
        "MMLU_human_sexuality":0.641221374,
        "MMLU_international_law":0.7190082645,
        "MMLU_jurisprudence":0.75,
        "MMLU_logical_fallacies":0.6871165644,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.7572815534,
        "MMLU_marketing":0.7820512821,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.7535121328,
        "MMLU_moral_disputes":0.661849711,
        "MMLU_moral_scenarios":0.3597765363,
        "MMLU_nutrition":0.6241830065,
        "MMLU_philosophy":0.6591639871,
        "MMLU_prehistory":0.6450617284,
        "MMLU_professional_accounting":0.4219858156,
        "MMLU_professional_law":0.4211212516,
        "MMLU_professional_medicine":0.5073529412,
        "MMLU_professional_psychology":0.545751634,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.6571428571,
        "MMLU_sociology":0.7512437811,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.4337349398,
        "MMLU_world_religions":0.7602339181
    },
    {
        "Model":"ReMM-v2.2-L2-13B",
        "URL":"https:\/\/huggingface.co\/Undi95\/ReMM-v2.2-L2-13B",
        "full_model_name":"Undi95\/ReMM-v2.2-L2-13B",
        "Parameters":13.0,
        "MMLU_average":0.5621694276,
        "arc:challenge|25":0.5836177474,
        "hellaswag|10":0.6488747262,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.5328947368,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.5924528302,
        "MMLU_college_biology":0.6041666667,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.5317919075,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.4723404255,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.5172413793,
        "MMLU_elementary_mathematics":0.3439153439,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.6483870968,
        "MMLU_high_school_chemistry":0.4384236453,
        "MMLU_high_school_computer_science":0.56,
        "MMLU_high_school_european_history":0.6666666667,
        "MMLU_high_school_geography":0.702020202,
        "MMLU_high_school_government_and_politics":0.7979274611,
        "MMLU_high_school_macroeconomics":0.5256410256,
        "MMLU_high_school_mathematics":0.3259259259,
        "MMLU_high_school_microeconomics":0.5966386555,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.7302752294,
        "MMLU_high_school_statistics":0.412037037,
        "MMLU_high_school_us_history":0.7647058824,
        "MMLU_high_school_world_history":0.7637130802,
        "MMLU_human_aging":0.6771300448,
        "MMLU_human_sexuality":0.6183206107,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.75,
        "MMLU_logical_fallacies":0.6993865031,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.7087378641,
        "MMLU_marketing":0.7948717949,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.7611749681,
        "MMLU_moral_disputes":0.6358381503,
        "MMLU_moral_scenarios":0.4536312849,
        "MMLU_nutrition":0.614379085,
        "MMLU_philosophy":0.6430868167,
        "MMLU_prehistory":0.6203703704,
        "MMLU_professional_accounting":0.4255319149,
        "MMLU_professional_law":0.4250325945,
        "MMLU_professional_medicine":0.5183823529,
        "MMLU_professional_psychology":0.5816993464,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.6244897959,
        "MMLU_sociology":0.6865671642,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4638554217,
        "MMLU_world_religions":0.7602339181
    },
    {
        "Model":"llama-2-13b-FINETUNE1_17w-r4",
        "URL":"https:\/\/huggingface.co\/CHIH-HUNG\/llama-2-13b-FINETUNE1_17w-r4",
        "full_model_name":"CHIH-HUNG\/llama-2-13b-FINETUNE1_17w-r4",
        "Parameters":13.0,
        "MMLU_average":0.561849437,
        "arc:challenge|25":0.5247440273,
        "hellaswag|10":0.6145190201,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.5333333333,
        "MMLU_astronomy":0.5986842105,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.6150943396,
        "MMLU_college_biology":0.5763888889,
        "MMLU_college_chemistry":0.47,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.5144508671,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.429787234,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.3465608466,
        "MMLU_formal_logic":0.4126984127,
        "MMLU_global_facts":0.28,
        "MMLU_high_school_biology":0.6709677419,
        "MMLU_high_school_chemistry":0.4532019704,
        "MMLU_high_school_computer_science":0.59,
        "MMLU_high_school_european_history":0.6666666667,
        "MMLU_high_school_geography":0.7121212121,
        "MMLU_high_school_government_and_politics":0.8238341969,
        "MMLU_high_school_macroeconomics":0.5282051282,
        "MMLU_high_school_mathematics":0.3185185185,
        "MMLU_high_school_microeconomics":0.5756302521,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.7614678899,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.7450980392,
        "MMLU_high_school_world_history":0.7510548523,
        "MMLU_human_aging":0.668161435,
        "MMLU_human_sexuality":0.6183206107,
        "MMLU_international_law":0.7603305785,
        "MMLU_jurisprudence":0.7407407407,
        "MMLU_logical_fallacies":0.6380368098,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.7475728155,
        "MMLU_marketing":0.8418803419,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.7777777778,
        "MMLU_moral_disputes":0.6387283237,
        "MMLU_moral_scenarios":0.3061452514,
        "MMLU_nutrition":0.6209150327,
        "MMLU_philosophy":0.6302250804,
        "MMLU_prehistory":0.6604938272,
        "MMLU_professional_accounting":0.4219858156,
        "MMLU_professional_law":0.4191655802,
        "MMLU_professional_medicine":0.5441176471,
        "MMLU_professional_psychology":0.5522875817,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.5836734694,
        "MMLU_sociology":0.7213930348,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4759036145,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"llama-2-13b-FINETUNE1_17w-r16",
        "URL":"https:\/\/huggingface.co\/CHIH-HUNG\/llama-2-13b-FINETUNE1_17w-r16",
        "full_model_name":"CHIH-HUNG\/llama-2-13b-FINETUNE1_17w-r16",
        "Parameters":13.0,
        "MMLU_average":0.5615527343,
        "arc:challenge|25":0.5307167235,
        "hellaswag|10":0.6154152559,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.6184210526,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.6188679245,
        "MMLU_college_biology":0.5694444444,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.39,
        "MMLU_college_medicine":0.5086705202,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4595744681,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.5103448276,
        "MMLU_elementary_mathematics":0.3439153439,
        "MMLU_formal_logic":0.4206349206,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.6451612903,
        "MMLU_high_school_chemistry":0.4433497537,
        "MMLU_high_school_computer_science":0.56,
        "MMLU_high_school_european_history":0.6606060606,
        "MMLU_high_school_geography":0.7070707071,
        "MMLU_high_school_government_and_politics":0.8082901554,
        "MMLU_high_school_macroeconomics":0.5179487179,
        "MMLU_high_school_mathematics":0.3259259259,
        "MMLU_high_school_microeconomics":0.5504201681,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.7633027523,
        "MMLU_high_school_statistics":0.462962963,
        "MMLU_high_school_us_history":0.75,
        "MMLU_high_school_world_history":0.7426160338,
        "MMLU_human_aging":0.6591928251,
        "MMLU_human_sexuality":0.5954198473,
        "MMLU_international_law":0.7520661157,
        "MMLU_jurisprudence":0.7592592593,
        "MMLU_logical_fallacies":0.6564417178,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.811965812,
        "MMLU_medical_genetics":0.62,
        "MMLU_miscellaneous":0.7752234994,
        "MMLU_moral_disputes":0.6358381503,
        "MMLU_moral_scenarios":0.3865921788,
        "MMLU_nutrition":0.6339869281,
        "MMLU_philosophy":0.6302250804,
        "MMLU_prehistory":0.6543209877,
        "MMLU_professional_accounting":0.4219858156,
        "MMLU_professional_law":0.408083442,
        "MMLU_professional_medicine":0.5404411765,
        "MMLU_professional_psychology":0.5375816993,
        "MMLU_public_relations":0.6727272727,
        "MMLU_security_studies":0.6244897959,
        "MMLU_sociology":0.7164179104,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.7777777778
    },
    {
        "Model":"CalliopeDS-v2-L2-13B",
        "URL":"https:\/\/huggingface.co\/Doctor-Shotgun\/CalliopeDS-v2-L2-13B",
        "full_model_name":"Doctor-Shotgun\/CalliopeDS-v2-L2-13B",
        "Parameters":13.0,
        "MMLU_average":0.5613892471,
        "arc:challenge|25":0.5981228669,
        "hellaswag|10":0.646883091,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.5328947368,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.6037735849,
        "MMLU_college_biology":0.6111111111,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.51,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.5086705202,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.429787234,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4827586207,
        "MMLU_elementary_mathematics":0.3439153439,
        "MMLU_formal_logic":0.373015873,
        "MMLU_global_facts":0.4,
        "MMLU_high_school_biology":0.6548387097,
        "MMLU_high_school_chemistry":0.4285714286,
        "MMLU_high_school_computer_science":0.57,
        "MMLU_high_school_european_history":0.6606060606,
        "MMLU_high_school_geography":0.7070707071,
        "MMLU_high_school_government_and_politics":0.8134715026,
        "MMLU_high_school_macroeconomics":0.5128205128,
        "MMLU_high_school_mathematics":0.3222222222,
        "MMLU_high_school_microeconomics":0.5630252101,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.7541284404,
        "MMLU_high_school_statistics":0.4166666667,
        "MMLU_high_school_us_history":0.7647058824,
        "MMLU_high_school_world_history":0.7383966245,
        "MMLU_human_aging":0.6726457399,
        "MMLU_human_sexuality":0.6259541985,
        "MMLU_international_law":0.7520661157,
        "MMLU_jurisprudence":0.7314814815,
        "MMLU_logical_fallacies":0.6625766871,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.6893203883,
        "MMLU_marketing":0.7905982906,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.7624521073,
        "MMLU_moral_disputes":0.6271676301,
        "MMLU_moral_scenarios":0.461452514,
        "MMLU_nutrition":0.6176470588,
        "MMLU_philosophy":0.6077170418,
        "MMLU_prehistory":0.6265432099,
        "MMLU_professional_accounting":0.4184397163,
        "MMLU_professional_law":0.4113428944,
        "MMLU_professional_medicine":0.5698529412,
        "MMLU_professional_psychology":0.5473856209,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.6163265306,
        "MMLU_sociology":0.7462686567,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4397590361,
        "MMLU_world_religions":0.783625731
    },
    {
        "Model":"nash-vicuna-13b-v1dot5-ep2-w-rag-w-simple",
        "URL":"https:\/\/huggingface.co\/luffycodes\/nash-vicuna-13b-v1dot5-ep2-w-rag-w-simple",
        "full_model_name":"luffycodes\/nash-vicuna-13b-v1dot5-ep2-w-rag-w-simple",
        "Parameters":13.0,
        "MMLU_average":0.5612422186,
        "arc:challenge|25":0.5537542662,
        "hellaswag|10":0.6131248755,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5526315789,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.6075471698,
        "MMLU_college_biology":0.5902777778,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.49,
        "MMLU_college_mathematics":0.4,
        "MMLU_college_medicine":0.5317919075,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.4382978723,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.3386243386,
        "MMLU_formal_logic":0.373015873,
        "MMLU_global_facts":0.4,
        "MMLU_high_school_biology":0.6709677419,
        "MMLU_high_school_chemistry":0.4433497537,
        "MMLU_high_school_computer_science":0.51,
        "MMLU_high_school_european_history":0.6666666667,
        "MMLU_high_school_geography":0.6868686869,
        "MMLU_high_school_government_and_politics":0.8134715026,
        "MMLU_high_school_macroeconomics":0.5615384615,
        "MMLU_high_school_mathematics":0.3222222222,
        "MMLU_high_school_microeconomics":0.5798319328,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.7486238532,
        "MMLU_high_school_statistics":0.4398148148,
        "MMLU_high_school_us_history":0.7401960784,
        "MMLU_high_school_world_history":0.7805907173,
        "MMLU_human_aging":0.6591928251,
        "MMLU_human_sexuality":0.6717557252,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.712962963,
        "MMLU_logical_fallacies":0.7055214724,
        "MMLU_machine_learning":0.4017857143,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.8376068376,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.7522349936,
        "MMLU_moral_disputes":0.5924855491,
        "MMLU_moral_scenarios":0.3653631285,
        "MMLU_nutrition":0.6241830065,
        "MMLU_philosophy":0.6366559486,
        "MMLU_prehistory":0.6141975309,
        "MMLU_professional_accounting":0.414893617,
        "MMLU_professional_law":0.4308996089,
        "MMLU_professional_medicine":0.4963235294,
        "MMLU_professional_psychology":0.5555555556,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.6244897959,
        "MMLU_sociology":0.7711442786,
        "MMLU_us_foreign_policy":0.76,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.7894736842
    },
    {
        "Model":"llama-2-13b-Open_Platypus_and_ccp_2.6w",
        "URL":"https:\/\/huggingface.co\/CHIH-HUNG\/llama-2-13b-Open_Platypus_and_ccp_2.6w",
        "full_model_name":"CHIH-HUNG\/llama-2-13b-Open_Platypus_and_ccp_2.6w",
        "Parameters":13.0,
        "MMLU_average":0.5612116554,
        "arc:challenge|25":0.5486348123,
        "hellaswag|10":0.6192989444,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.5197368421,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.6264150943,
        "MMLU_college_biology":0.6111111111,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.5260115607,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.72,
        "MMLU_conceptual_physics":0.429787234,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.5103448276,
        "MMLU_elementary_mathematics":0.328042328,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.6741935484,
        "MMLU_high_school_chemistry":0.4926108374,
        "MMLU_high_school_computer_science":0.58,
        "MMLU_high_school_european_history":0.6666666667,
        "MMLU_high_school_geography":0.6767676768,
        "MMLU_high_school_government_and_politics":0.8341968912,
        "MMLU_high_school_macroeconomics":0.5,
        "MMLU_high_school_mathematics":0.3185185185,
        "MMLU_high_school_microeconomics":0.5672268908,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.7633027523,
        "MMLU_high_school_statistics":0.4398148148,
        "MMLU_high_school_us_history":0.7549019608,
        "MMLU_high_school_world_history":0.7383966245,
        "MMLU_human_aging":0.6412556054,
        "MMLU_human_sexuality":0.641221374,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.712962963,
        "MMLU_logical_fallacies":0.6871165644,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.7572815534,
        "MMLU_marketing":0.7991452991,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.7535121328,
        "MMLU_moral_disputes":0.6387283237,
        "MMLU_moral_scenarios":0.4581005587,
        "MMLU_nutrition":0.6209150327,
        "MMLU_philosophy":0.6430868167,
        "MMLU_prehistory":0.6419753086,
        "MMLU_professional_accounting":0.4219858156,
        "MMLU_professional_law":0.4276401565,
        "MMLU_professional_medicine":0.5698529412,
        "MMLU_professional_psychology":0.5522875817,
        "MMLU_public_relations":0.6272727273,
        "MMLU_security_studies":0.6489795918,
        "MMLU_sociology":0.7412935323,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4638554217,
        "MMLU_world_religions":0.7777777778
    },
    {
        "Model":"Synthia-13B",
        "URL":"https:\/\/huggingface.co\/migtissera\/Synthia-13B",
        "full_model_name":"migtissera\/Synthia-13B",
        "Parameters":13.0,
        "MMLU_average":0.5610860589,
        "arc:challenge|25":0.5546075085,
        "hellaswag|10":0.6237801235,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.5657894737,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.5849056604,
        "MMLU_college_biology":0.5972222222,
        "MMLU_college_chemistry":0.36,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.41,
        "MMLU_college_medicine":0.5086705202,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4127659574,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.5448275862,
        "MMLU_elementary_mathematics":0.3412698413,
        "MMLU_formal_logic":0.3968253968,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.6548387097,
        "MMLU_high_school_chemistry":0.4039408867,
        "MMLU_high_school_computer_science":0.57,
        "MMLU_high_school_european_history":0.6909090909,
        "MMLU_high_school_geography":0.702020202,
        "MMLU_high_school_government_and_politics":0.7979274611,
        "MMLU_high_school_macroeconomics":0.5076923077,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.5630252101,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7651376147,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.7696078431,
        "MMLU_high_school_world_history":0.7679324895,
        "MMLU_human_aging":0.6547085202,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.7438016529,
        "MMLU_jurisprudence":0.7314814815,
        "MMLU_logical_fallacies":0.7055214724,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.811965812,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.7624521073,
        "MMLU_moral_disputes":0.6156069364,
        "MMLU_moral_scenarios":0.4759776536,
        "MMLU_nutrition":0.5882352941,
        "MMLU_philosophy":0.61414791,
        "MMLU_prehistory":0.6604938272,
        "MMLU_professional_accounting":0.4113475177,
        "MMLU_professional_law":0.4002607562,
        "MMLU_professional_medicine":0.5330882353,
        "MMLU_professional_psychology":0.5784313725,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6244897959,
        "MMLU_sociology":0.7213930348,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4337349398,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"llama-2-13b-FINETUNE3_3.3w-r16-gate_up_down",
        "URL":"https:\/\/huggingface.co\/CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r16-gate_up_down",
        "full_model_name":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r16-gate_up_down",
        "Parameters":13.0,
        "MMLU_average":0.5607979376,
        "arc:challenge|25":0.542662116,
        "hellaswag|10":0.6103365863,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5657894737,
        "MMLU_business_ethics":0.49,
        "MMLU_clinical_knowledge":0.6075471698,
        "MMLU_college_biology":0.5902777778,
        "MMLU_college_chemistry":0.46,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.5086705202,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.74,
        "MMLU_conceptual_physics":0.4127659574,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.3703703704,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.6451612903,
        "MMLU_high_school_chemistry":0.4532019704,
        "MMLU_high_school_computer_science":0.56,
        "MMLU_high_school_european_history":0.6787878788,
        "MMLU_high_school_geography":0.7222222222,
        "MMLU_high_school_government_and_politics":0.7772020725,
        "MMLU_high_school_macroeconomics":0.5307692308,
        "MMLU_high_school_mathematics":0.3407407407,
        "MMLU_high_school_microeconomics":0.5966386555,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.7724770642,
        "MMLU_high_school_statistics":0.5046296296,
        "MMLU_high_school_us_history":0.7745098039,
        "MMLU_high_school_world_history":0.7637130802,
        "MMLU_human_aging":0.66367713,
        "MMLU_human_sexuality":0.572519084,
        "MMLU_international_law":0.7438016529,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.6687116564,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.7905982906,
        "MMLU_medical_genetics":0.62,
        "MMLU_miscellaneous":0.7611749681,
        "MMLU_moral_disputes":0.6589595376,
        "MMLU_moral_scenarios":0.3508379888,
        "MMLU_nutrition":0.6111111111,
        "MMLU_philosophy":0.6398713826,
        "MMLU_prehistory":0.6450617284,
        "MMLU_professional_accounting":0.4503546099,
        "MMLU_professional_law":0.4211212516,
        "MMLU_professional_medicine":0.5882352941,
        "MMLU_professional_psychology":0.5702614379,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.5755102041,
        "MMLU_sociology":0.7313432836,
        "MMLU_us_foreign_policy":0.75,
        "MMLU_virology":0.4518072289,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"llama-2-13b-Open-Platypus_2.5w",
        "URL":"https:\/\/huggingface.co\/CHIH-HUNG\/llama-2-13b-Open-Platypus_2.5w",
        "full_model_name":"CHIH-HUNG\/llama-2-13b-Open-Platypus_2.5w",
        "Parameters":13.0,
        "MMLU_average":0.5606240764,
        "arc:challenge|25":0.5460750853,
        "hellaswag|10":0.6169089823,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.5526315789,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.6150943396,
        "MMLU_college_biology":0.625,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.5202312139,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.4085106383,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.524137931,
        "MMLU_elementary_mathematics":0.3306878307,
        "MMLU_formal_logic":0.3333333333,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.6774193548,
        "MMLU_high_school_chemistry":0.4778325123,
        "MMLU_high_school_computer_science":0.6,
        "MMLU_high_school_european_history":0.6848484848,
        "MMLU_high_school_geography":0.7171717172,
        "MMLU_high_school_government_and_politics":0.8031088083,
        "MMLU_high_school_macroeconomics":0.5230769231,
        "MMLU_high_school_mathematics":0.3259259259,
        "MMLU_high_school_microeconomics":0.5504201681,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.7596330275,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.7647058824,
        "MMLU_high_school_world_history":0.776371308,
        "MMLU_human_aging":0.6233183857,
        "MMLU_human_sexuality":0.6183206107,
        "MMLU_international_law":0.7190082645,
        "MMLU_jurisprudence":0.75,
        "MMLU_logical_fallacies":0.6809815951,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.7475728155,
        "MMLU_marketing":0.7863247863,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.7611749681,
        "MMLU_moral_disputes":0.6531791908,
        "MMLU_moral_scenarios":0.4815642458,
        "MMLU_nutrition":0.6176470588,
        "MMLU_philosophy":0.6881028939,
        "MMLU_prehistory":0.6543209877,
        "MMLU_professional_accounting":0.4397163121,
        "MMLU_professional_law":0.4178617992,
        "MMLU_professional_medicine":0.5294117647,
        "MMLU_professional_psychology":0.5604575163,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.6081632653,
        "MMLU_sociology":0.7114427861,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4397590361,
        "MMLU_world_religions":0.7660818713
    },
    {
        "Model":"xDAN_13b_l2_lora",
        "URL":"https:\/\/huggingface.co\/xDAN-AI\/xDAN_13b_l2_lora",
        "full_model_name":"xDAN-AI\/xDAN_13b_l2_lora",
        "Parameters":13.0,
        "MMLU_average":0.5603251818,
        "arc:challenge|25":0.569112628,
        "hellaswag|10":0.6207926708,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.5723684211,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.6226415094,
        "MMLU_college_biology":0.6180555556,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.4913294798,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.4255319149,
        "MMLU_econometrics":0.350877193,
        "MMLU_electrical_engineering":0.5379310345,
        "MMLU_elementary_mathematics":0.3068783069,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.6612903226,
        "MMLU_high_school_chemistry":0.4679802956,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.6787878788,
        "MMLU_high_school_geography":0.7171717172,
        "MMLU_high_school_government_and_politics":0.792746114,
        "MMLU_high_school_macroeconomics":0.4974358974,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.5840336134,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.7559633028,
        "MMLU_high_school_statistics":0.4212962963,
        "MMLU_high_school_us_history":0.7745098039,
        "MMLU_high_school_world_history":0.746835443,
        "MMLU_human_aging":0.6726457399,
        "MMLU_human_sexuality":0.6030534351,
        "MMLU_international_law":0.7190082645,
        "MMLU_jurisprudence":0.7314814815,
        "MMLU_logical_fallacies":0.717791411,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.7905982906,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.7675606641,
        "MMLU_moral_disputes":0.6473988439,
        "MMLU_moral_scenarios":0.4469273743,
        "MMLU_nutrition":0.6274509804,
        "MMLU_philosophy":0.6302250804,
        "MMLU_prehistory":0.6234567901,
        "MMLU_professional_accounting":0.4184397163,
        "MMLU_professional_law":0.4035202086,
        "MMLU_professional_medicine":0.4926470588,
        "MMLU_professional_psychology":0.5522875817,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.6571428571,
        "MMLU_sociology":0.7412935323,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.4518072289,
        "MMLU_world_religions":0.7660818713
    },
    {
        "Model":"13B-Legerdemain-L2",
        "URL":"https:\/\/huggingface.co\/CalderaAI\/13B-Legerdemain-L2",
        "full_model_name":"CalderaAI\/13B-Legerdemain-L2",
        "Parameters":13.0,
        "MMLU_average":0.5600295831,
        "arc:challenge|25":0.5733788396,
        "hellaswag|10":0.635431189,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.5259259259,
        "MMLU_astronomy":0.5723684211,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.6377358491,
        "MMLU_college_biology":0.625,
        "MMLU_college_chemistry":0.47,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.5375722543,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4255319149,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.3227513228,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.6709677419,
        "MMLU_high_school_chemistry":0.4926108374,
        "MMLU_high_school_computer_science":0.59,
        "MMLU_high_school_european_history":0.6545454545,
        "MMLU_high_school_geography":0.7272727273,
        "MMLU_high_school_government_and_politics":0.8031088083,
        "MMLU_high_school_macroeconomics":0.5435897436,
        "MMLU_high_school_mathematics":0.3148148148,
        "MMLU_high_school_microeconomics":0.5924369748,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7321100917,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.7598039216,
        "MMLU_high_school_world_history":0.7299578059,
        "MMLU_human_aging":0.6547085202,
        "MMLU_human_sexuality":0.6183206107,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.7777777778,
        "MMLU_logical_fallacies":0.6871165644,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.7991452991,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.7471264368,
        "MMLU_moral_disputes":0.6329479769,
        "MMLU_moral_scenarios":0.2703910615,
        "MMLU_nutrition":0.6274509804,
        "MMLU_philosophy":0.6302250804,
        "MMLU_prehistory":0.6049382716,
        "MMLU_professional_accounting":0.4290780142,
        "MMLU_professional_law":0.424380704,
        "MMLU_professional_medicine":0.5220588235,
        "MMLU_professional_psychology":0.5326797386,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.6367346939,
        "MMLU_sociology":0.7661691542,
        "MMLU_us_foreign_policy":0.87,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.7777777778
    },
    {
        "Model":"llama2-13b-sharegpt4-test",
        "URL":"https:\/\/huggingface.co\/lu-vae\/llama2-13b-sharegpt4-test",
        "full_model_name":"lu-vae\/llama2-13b-sharegpt4-test",
        "Parameters":13.0,
        "MMLU_average":0.559887822,
        "arc:challenge|25":0.54778157,
        "hellaswag|10":0.6249751046,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.5328947368,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.6,
        "MMLU_college_biology":0.6180555556,
        "MMLU_college_chemistry":0.45,
        "MMLU_college_computer_science":0.49,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.5260115607,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.4638297872,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.5103448276,
        "MMLU_elementary_mathematics":0.3174603175,
        "MMLU_formal_logic":0.3412698413,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.6741935484,
        "MMLU_high_school_chemistry":0.4088669951,
        "MMLU_high_school_computer_science":0.5,
        "MMLU_high_school_european_history":0.6848484848,
        "MMLU_high_school_geography":0.6868686869,
        "MMLU_high_school_government_and_politics":0.792746114,
        "MMLU_high_school_macroeconomics":0.5179487179,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.5798319328,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.7376146789,
        "MMLU_high_school_statistics":0.4212962963,
        "MMLU_high_school_us_history":0.7598039216,
        "MMLU_high_school_world_history":0.7594936709,
        "MMLU_human_aging":0.6591928251,
        "MMLU_human_sexuality":0.6641221374,
        "MMLU_international_law":0.7520661157,
        "MMLU_jurisprudence":0.6574074074,
        "MMLU_logical_fallacies":0.6687116564,
        "MMLU_machine_learning":0.4464285714,
        "MMLU_management":0.6893203883,
        "MMLU_marketing":0.8247863248,
        "MMLU_medical_genetics":0.62,
        "MMLU_miscellaneous":0.7292464879,
        "MMLU_moral_disputes":0.6387283237,
        "MMLU_moral_scenarios":0.4011173184,
        "MMLU_nutrition":0.614379085,
        "MMLU_philosophy":0.6237942122,
        "MMLU_prehistory":0.5925925926,
        "MMLU_professional_accounting":0.4503546099,
        "MMLU_professional_law":0.4178617992,
        "MMLU_professional_medicine":0.5514705882,
        "MMLU_professional_psychology":0.5800653595,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6081632653,
        "MMLU_sociology":0.7562189055,
        "MMLU_us_foreign_policy":0.81,
        "MMLU_virology":0.4819277108,
        "MMLU_world_religions":0.7719298246
    },
    {
        "Model":"llama-2-13b-code-alpaca",
        "URL":"https:\/\/huggingface.co\/layoric\/llama-2-13b-code-alpaca",
        "full_model_name":"layoric\/llama-2-13b-code-alpaca",
        "Parameters":13.0,
        "MMLU_average":0.559262181,
        "arc:challenge|25":0.5682593857,
        "hellaswag|10":0.616610237,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.5197368421,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.6264150943,
        "MMLU_college_biology":0.5694444444,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.5375722543,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.72,
        "MMLU_conceptual_physics":0.429787234,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4965517241,
        "MMLU_elementary_mathematics":0.3174603175,
        "MMLU_formal_logic":0.373015873,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.6806451613,
        "MMLU_high_school_chemistry":0.4581280788,
        "MMLU_high_school_computer_science":0.59,
        "MMLU_high_school_european_history":0.6666666667,
        "MMLU_high_school_geography":0.6868686869,
        "MMLU_high_school_government_and_politics":0.8238341969,
        "MMLU_high_school_macroeconomics":0.5025641026,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.5798319328,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.752293578,
        "MMLU_high_school_statistics":0.4259259259,
        "MMLU_high_school_us_history":0.7696078431,
        "MMLU_high_school_world_history":0.7383966245,
        "MMLU_human_aging":0.6457399103,
        "MMLU_human_sexuality":0.6488549618,
        "MMLU_international_law":0.7190082645,
        "MMLU_jurisprudence":0.75,
        "MMLU_logical_fallacies":0.6809815951,
        "MMLU_machine_learning":0.2946428571,
        "MMLU_management":0.7669902913,
        "MMLU_marketing":0.7948717949,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.7586206897,
        "MMLU_moral_disputes":0.6416184971,
        "MMLU_moral_scenarios":0.3675977654,
        "MMLU_nutrition":0.6307189542,
        "MMLU_philosophy":0.6623794212,
        "MMLU_prehistory":0.6388888889,
        "MMLU_professional_accounting":0.4042553191,
        "MMLU_professional_law":0.425684485,
        "MMLU_professional_medicine":0.5404411765,
        "MMLU_professional_psychology":0.5604575163,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.6408163265,
        "MMLU_sociology":0.7661691542,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.7660818713
    },
    {
        "Model":"vigogne-2-13b-instruct",
        "URL":"https:\/\/huggingface.co\/bofenghuang\/vigogne-2-13b-instruct",
        "full_model_name":"bofenghuang\/vigogne-2-13b-instruct",
        "Parameters":13.0,
        "MMLU_average":0.5592035759,
        "arc:challenge|25":0.5767918089,
        "hellaswag|10":0.6310495917,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.5592105263,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.6,
        "MMLU_college_biology":0.5902777778,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.5028901734,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.73,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.4827586207,
        "MMLU_elementary_mathematics":0.3412698413,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.6677419355,
        "MMLU_high_school_chemistry":0.4482758621,
        "MMLU_high_school_computer_science":0.54,
        "MMLU_high_school_european_history":0.6787878788,
        "MMLU_high_school_geography":0.6767676768,
        "MMLU_high_school_government_and_politics":0.7875647668,
        "MMLU_high_school_macroeconomics":0.5230769231,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.5840336134,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.7394495413,
        "MMLU_high_school_statistics":0.4305555556,
        "MMLU_high_school_us_history":0.7598039216,
        "MMLU_high_school_world_history":0.7383966245,
        "MMLU_human_aging":0.6547085202,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.6944444444,
        "MMLU_logical_fallacies":0.6993865031,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.8290598291,
        "MMLU_medical_genetics":0.61,
        "MMLU_miscellaneous":0.7675606641,
        "MMLU_moral_disputes":0.6445086705,
        "MMLU_moral_scenarios":0.4268156425,
        "MMLU_nutrition":0.6274509804,
        "MMLU_philosophy":0.6366559486,
        "MMLU_prehistory":0.6296296296,
        "MMLU_professional_accounting":0.4219858156,
        "MMLU_professional_law":0.4295958279,
        "MMLU_professional_medicine":0.5220588235,
        "MMLU_professional_psychology":0.5571895425,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.6204081633,
        "MMLU_sociology":0.7412935323,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.7719298246
    },
    {
        "Model":"Amethyst-13B",
        "URL":"https:\/\/huggingface.co\/Undi95\/Amethyst-13B",
        "full_model_name":"Undi95\/Amethyst-13B",
        "Parameters":13.0,
        "MMLU_average":0.559101502,
        "arc:challenge|25":0.5921501706,
        "hellaswag|10":0.638717387,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.5259259259,
        "MMLU_astronomy":0.5460526316,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.5698113208,
        "MMLU_college_biology":0.5763888889,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.5433526012,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.4765957447,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.4965517241,
        "MMLU_elementary_mathematics":0.3253968254,
        "MMLU_formal_logic":0.3888888889,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.6548387097,
        "MMLU_high_school_chemistry":0.4137931034,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.7070707071,
        "MMLU_high_school_government_and_politics":0.7979274611,
        "MMLU_high_school_macroeconomics":0.5256410256,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.5924369748,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.7357798165,
        "MMLU_high_school_statistics":0.4074074074,
        "MMLU_high_school_us_history":0.75,
        "MMLU_high_school_world_history":0.7552742616,
        "MMLU_human_aging":0.6905829596,
        "MMLU_human_sexuality":0.6488549618,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.6871165644,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.6893203883,
        "MMLU_marketing":0.7905982906,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.754789272,
        "MMLU_moral_disputes":0.6358381503,
        "MMLU_moral_scenarios":0.4324022346,
        "MMLU_nutrition":0.6307189542,
        "MMLU_philosophy":0.6398713826,
        "MMLU_prehistory":0.6358024691,
        "MMLU_professional_accounting":0.4219858156,
        "MMLU_professional_law":0.4224250326,
        "MMLU_professional_medicine":0.5294117647,
        "MMLU_professional_psychology":0.5800653595,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6285714286,
        "MMLU_sociology":0.7064676617,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.4819277108,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"llama-2-13b-FINETUNE4_3.8w-r16-q_k_v_o_gate_up_down",
        "URL":"https:\/\/huggingface.co\/CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r16-q_k_v_o_gate_up_down",
        "full_model_name":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r16-q_k_v_o_gate_up_down",
        "Parameters":13.0,
        "MMLU_average":0.5589640394,
        "arc:challenge|25":0.5255972696,
        "hellaswag|10":0.6081457877,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.5037037037,
        "MMLU_astronomy":0.5723684211,
        "MMLU_business_ethics":0.48,
        "MMLU_clinical_knowledge":0.5886792453,
        "MMLU_college_biology":0.5833333333,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5202312139,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4723404255,
        "MMLU_econometrics":0.3421052632,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.3571428571,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.6258064516,
        "MMLU_high_school_chemistry":0.4482758621,
        "MMLU_high_school_computer_science":0.51,
        "MMLU_high_school_european_history":0.6545454545,
        "MMLU_high_school_geography":0.7323232323,
        "MMLU_high_school_government_and_politics":0.8186528497,
        "MMLU_high_school_macroeconomics":0.5384615385,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.6134453782,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.7798165138,
        "MMLU_high_school_statistics":0.5138888889,
        "MMLU_high_school_us_history":0.7794117647,
        "MMLU_high_school_world_history":0.7552742616,
        "MMLU_human_aging":0.6233183857,
        "MMLU_human_sexuality":0.679389313,
        "MMLU_international_law":0.7438016529,
        "MMLU_jurisprudence":0.6944444444,
        "MMLU_logical_fallacies":0.6564417178,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.7669902913,
        "MMLU_marketing":0.8205128205,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7535121328,
        "MMLU_moral_disputes":0.6300578035,
        "MMLU_moral_scenarios":0.4558659218,
        "MMLU_nutrition":0.5980392157,
        "MMLU_philosophy":0.6237942122,
        "MMLU_prehistory":0.6111111111,
        "MMLU_professional_accounting":0.4397163121,
        "MMLU_professional_law":0.4289439374,
        "MMLU_professional_medicine":0.5955882353,
        "MMLU_professional_psychology":0.5555555556,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.5918367347,
        "MMLU_sociology":0.6965174129,
        "MMLU_us_foreign_policy":0.78,
        "MMLU_virology":0.3915662651,
        "MMLU_world_religions":0.783625731
    },
    {
        "Model":"llama-2-13b-FINETUNE1_17w-gate_up_down_proj",
        "URL":"https:\/\/huggingface.co\/CHIH-HUNG\/llama-2-13b-FINETUNE1_17w-gate_up_down_proj",
        "full_model_name":"CHIH-HUNG\/llama-2-13b-FINETUNE1_17w-gate_up_down_proj",
        "Parameters":13.0,
        "MMLU_average":0.5589032782,
        "arc:challenge|25":0.5290102389,
        "hellaswag|10":0.6146186019,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.5986842105,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.6113207547,
        "MMLU_college_biology":0.5833333333,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.5028901734,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.73,
        "MMLU_conceptual_physics":0.4127659574,
        "MMLU_econometrics":0.3421052632,
        "MMLU_electrical_engineering":0.5172413793,
        "MMLU_elementary_mathematics":0.3624338624,
        "MMLU_formal_logic":0.4206349206,
        "MMLU_global_facts":0.28,
        "MMLU_high_school_biology":0.6516129032,
        "MMLU_high_school_chemistry":0.4532019704,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.7171717172,
        "MMLU_high_school_government_and_politics":0.8186528497,
        "MMLU_high_school_macroeconomics":0.5076923077,
        "MMLU_high_school_mathematics":0.337037037,
        "MMLU_high_school_microeconomics":0.5546218487,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.7559633028,
        "MMLU_high_school_statistics":0.4675925926,
        "MMLU_high_school_us_history":0.7401960784,
        "MMLU_high_school_world_history":0.7552742616,
        "MMLU_human_aging":0.6547085202,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.7685950413,
        "MMLU_jurisprudence":0.75,
        "MMLU_logical_fallacies":0.6503067485,
        "MMLU_machine_learning":0.2410714286,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.8247863248,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.7752234994,
        "MMLU_moral_disputes":0.6473988439,
        "MMLU_moral_scenarios":0.2759776536,
        "MMLU_nutrition":0.5980392157,
        "MMLU_philosophy":0.6334405145,
        "MMLU_prehistory":0.6543209877,
        "MMLU_professional_accounting":0.4326241135,
        "MMLU_professional_law":0.4159061278,
        "MMLU_professional_medicine":0.5294117647,
        "MMLU_professional_psychology":0.5653594771,
        "MMLU_public_relations":0.6727272727,
        "MMLU_security_studies":0.6163265306,
        "MMLU_sociology":0.7064676617,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.7777777778
    },
    {
        "Model":"llama-2-13b-FINETUNE4_3.8w-r16-q_k_v_o",
        "URL":"https:\/\/huggingface.co\/CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r16-q_k_v_o",
        "full_model_name":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r16-q_k_v_o",
        "Parameters":13.0,
        "MMLU_average":0.5587107352,
        "arc:challenge|25":0.5179180887,
        "hellaswag|10":0.6127265485,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.5921052632,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.5924528302,
        "MMLU_college_biology":0.5763888889,
        "MMLU_college_chemistry":0.5,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.5144508671,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.65,
        "MMLU_conceptual_physics":0.4595744681,
        "MMLU_econometrics":0.3596491228,
        "MMLU_electrical_engineering":0.4482758621,
        "MMLU_elementary_mathematics":0.3386243386,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.6709677419,
        "MMLU_high_school_chemistry":0.4334975369,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.6424242424,
        "MMLU_high_school_geography":0.702020202,
        "MMLU_high_school_government_and_politics":0.8186528497,
        "MMLU_high_school_macroeconomics":0.5564102564,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.5714285714,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.7688073394,
        "MMLU_high_school_statistics":0.4768518519,
        "MMLU_high_school_us_history":0.7892156863,
        "MMLU_high_school_world_history":0.7679324895,
        "MMLU_human_aging":0.6233183857,
        "MMLU_human_sexuality":0.5954198473,
        "MMLU_international_law":0.7603305785,
        "MMLU_jurisprudence":0.712962963,
        "MMLU_logical_fallacies":0.6748466258,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.7572815534,
        "MMLU_marketing":0.8162393162,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.7675606641,
        "MMLU_moral_disputes":0.6473988439,
        "MMLU_moral_scenarios":0.3776536313,
        "MMLU_nutrition":0.5849673203,
        "MMLU_philosophy":0.6559485531,
        "MMLU_prehistory":0.6358024691,
        "MMLU_professional_accounting":0.4397163121,
        "MMLU_professional_law":0.4230769231,
        "MMLU_professional_medicine":0.6286764706,
        "MMLU_professional_psychology":0.5669934641,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6,
        "MMLU_sociology":0.7064676617,
        "MMLU_us_foreign_policy":0.75,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.8011695906
    },
    {
        "Model":"chronos-33b",
        "URL":"https:\/\/huggingface.co\/elinas\/chronos-33b",
        "full_model_name":"elinas\/chronos-33b",
        "Parameters":33.0,
        "MMLU_average":0.5586763996,
        "arc:challenge|25":0.5895904437,
        "hellaswag|10":0.6340370444,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.5921052632,
        "MMLU_business_ethics":0.6,
        "MMLU_clinical_knowledge":0.5962264151,
        "MMLU_college_biology":0.6597222222,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.5433526012,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.485106383,
        "MMLU_econometrics":0.3596491228,
        "MMLU_electrical_engineering":0.4827586207,
        "MMLU_elementary_mathematics":0.3253968254,
        "MMLU_formal_logic":0.3412698413,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.6419354839,
        "MMLU_high_school_chemistry":0.3891625616,
        "MMLU_high_school_computer_science":0.62,
        "MMLU_high_school_european_history":0.6909090909,
        "MMLU_high_school_geography":0.6262626263,
        "MMLU_high_school_government_and_politics":0.8238341969,
        "MMLU_high_school_macroeconomics":0.5128205128,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.5084033613,
        "MMLU_high_school_physics":0.3708609272,
        "MMLU_high_school_psychology":0.7467889908,
        "MMLU_high_school_statistics":0.4583333333,
        "MMLU_high_school_us_history":0.7254901961,
        "MMLU_high_school_world_history":0.7637130802,
        "MMLU_human_aging":0.6457399103,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.7190082645,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.6993865031,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.7948717949,
        "MMLU_medical_genetics":0.63,
        "MMLU_miscellaneous":0.7496807152,
        "MMLU_moral_disputes":0.6184971098,
        "MMLU_moral_scenarios":0.2916201117,
        "MMLU_nutrition":0.6013071895,
        "MMLU_philosophy":0.6334405145,
        "MMLU_prehistory":0.6327160494,
        "MMLU_professional_accounting":0.4361702128,
        "MMLU_professional_law":0.4387222947,
        "MMLU_professional_medicine":0.4926470588,
        "MMLU_professional_psychology":0.5718954248,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6204081633,
        "MMLU_sociology":0.736318408,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.4819277108,
        "MMLU_world_religions":0.7660818713
    },
    {
        "Model":"minotaur-llama2-13b-qlora",
        "URL":"https:\/\/huggingface.co\/ehartford\/minotaur-llama2-13b-qlora",
        "full_model_name":"ehartford\/minotaur-llama2-13b-qlora",
        "Parameters":13.0,
        "MMLU_average":0.5586639382,
        "arc:challenge|25":0.5674061433,
        "hellaswag|10":0.6226847242,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.6052631579,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.6150943396,
        "MMLU_college_biology":0.5902777778,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.5317919075,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.4468085106,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.5172413793,
        "MMLU_elementary_mathematics":0.291005291,
        "MMLU_formal_logic":0.380952381,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.6516129032,
        "MMLU_high_school_chemistry":0.3842364532,
        "MMLU_high_school_computer_science":0.58,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.7323232323,
        "MMLU_high_school_government_and_politics":0.8082901554,
        "MMLU_high_school_macroeconomics":0.5230769231,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.5588235294,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.7504587156,
        "MMLU_high_school_statistics":0.4166666667,
        "MMLU_high_school_us_history":0.7696078431,
        "MMLU_high_school_world_history":0.7299578059,
        "MMLU_human_aging":0.6278026906,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.7024793388,
        "MMLU_jurisprudence":0.712962963,
        "MMLU_logical_fallacies":0.6196319018,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.7475728155,
        "MMLU_marketing":0.8205128205,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7573435504,
        "MMLU_moral_disputes":0.6156069364,
        "MMLU_moral_scenarios":0.4212290503,
        "MMLU_nutrition":0.6274509804,
        "MMLU_philosophy":0.5884244373,
        "MMLU_prehistory":0.6234567901,
        "MMLU_professional_accounting":0.4361702128,
        "MMLU_professional_law":0.4015645372,
        "MMLU_professional_medicine":0.5514705882,
        "MMLU_professional_psychology":0.5310457516,
        "MMLU_public_relations":0.6272727273,
        "MMLU_security_studies":0.6612244898,
        "MMLU_sociology":0.7562189055,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.8070175439
    },
    {
        "Model":"Kimiko-13B-fp16",
        "URL":"https:\/\/huggingface.co\/TheBloke\/Kimiko-13B-fp16",
        "full_model_name":"TheBloke\/Kimiko-13B-fp16",
        "Parameters":13.0,
        "MMLU_average":0.5584575577,
        "arc:challenge|25":0.5529010239,
        "hellaswag|10":0.6173073093,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.5657894737,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.6301886792,
        "MMLU_college_biology":0.5833333333,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.5202312139,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4085106383,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.5172413793,
        "MMLU_elementary_mathematics":0.3148148148,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.6677419355,
        "MMLU_high_school_chemistry":0.4236453202,
        "MMLU_high_school_computer_science":0.58,
        "MMLU_high_school_european_history":0.6606060606,
        "MMLU_high_school_geography":0.6919191919,
        "MMLU_high_school_government_and_politics":0.829015544,
        "MMLU_high_school_macroeconomics":0.5205128205,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.5798319328,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7596330275,
        "MMLU_high_school_statistics":0.4861111111,
        "MMLU_high_school_us_history":0.7450980392,
        "MMLU_high_school_world_history":0.7341772152,
        "MMLU_human_aging":0.6322869955,
        "MMLU_human_sexuality":0.6717557252,
        "MMLU_international_law":0.694214876,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.6932515337,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.7961165049,
        "MMLU_marketing":0.7863247863,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.7432950192,
        "MMLU_moral_disputes":0.6445086705,
        "MMLU_moral_scenarios":0.3396648045,
        "MMLU_nutrition":0.6437908497,
        "MMLU_philosophy":0.6334405145,
        "MMLU_prehistory":0.6327160494,
        "MMLU_professional_accounting":0.4078014184,
        "MMLU_professional_law":0.4263363755,
        "MMLU_professional_medicine":0.5404411765,
        "MMLU_professional_psychology":0.5490196078,
        "MMLU_public_relations":0.6272727273,
        "MMLU_security_studies":0.6448979592,
        "MMLU_sociology":0.7512437811,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4638554217,
        "MMLU_world_religions":0.7719298246
    },
    {
        "Model":"Llama-2-13b-chat-dutch",
        "URL":"https:\/\/huggingface.co\/BramVanroy\/Llama-2-13b-chat-dutch",
        "full_model_name":"BramVanroy\/Llama-2-13b-chat-dutch",
        "Parameters":13.0,
        "MMLU_average":0.5582168489,
        "arc:challenge|25":0.5375426621,
        "hellaswag|10":0.6097390958,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.5657894737,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.6075471698,
        "MMLU_college_biology":0.5416666667,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.549132948,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.72,
        "MMLU_conceptual_physics":0.4553191489,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.524137931,
        "MMLU_elementary_mathematics":0.3253968254,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.6677419355,
        "MMLU_high_school_chemistry":0.4630541872,
        "MMLU_high_school_computer_science":0.58,
        "MMLU_high_school_european_history":0.6545454545,
        "MMLU_high_school_geography":0.696969697,
        "MMLU_high_school_government_and_politics":0.7979274611,
        "MMLU_high_school_macroeconomics":0.5358974359,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.5714285714,
        "MMLU_high_school_physics":0.3642384106,
        "MMLU_high_school_psychology":0.7467889908,
        "MMLU_high_school_statistics":0.4351851852,
        "MMLU_high_school_us_history":0.7598039216,
        "MMLU_high_school_world_history":0.7172995781,
        "MMLU_human_aging":0.6278026906,
        "MMLU_human_sexuality":0.6259541985,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.7592592593,
        "MMLU_logical_fallacies":0.7116564417,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.7863247863,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.748403576,
        "MMLU_moral_disputes":0.6329479769,
        "MMLU_moral_scenarios":0.3553072626,
        "MMLU_nutrition":0.6111111111,
        "MMLU_philosophy":0.652733119,
        "MMLU_prehistory":0.6543209877,
        "MMLU_professional_accounting":0.4007092199,
        "MMLU_professional_law":0.4139504563,
        "MMLU_professional_medicine":0.5220588235,
        "MMLU_professional_psychology":0.5571895425,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.5959183673,
        "MMLU_sociology":0.7512437811,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.7719298246
    },
    {
        "Model":"chronolima-airo-grad-l2-13B",
        "URL":"https:\/\/huggingface.co\/kingbri\/chronolima-airo-grad-l2-13B",
        "full_model_name":"kingbri\/chronolima-airo-grad-l2-13B",
        "Parameters":13.0,
        "MMLU_average":0.5580485142,
        "arc:challenge|25":0.5639931741,
        "hellaswag|10":0.6368253336,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.5197368421,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.5698113208,
        "MMLU_college_biology":0.5625,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.4,
        "MMLU_college_medicine":0.5028901734,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4468085106,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.3148148148,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.6419354839,
        "MMLU_high_school_chemistry":0.4088669951,
        "MMLU_high_school_computer_science":0.53,
        "MMLU_high_school_european_history":0.6545454545,
        "MMLU_high_school_geography":0.6919191919,
        "MMLU_high_school_government_and_politics":0.8134715026,
        "MMLU_high_school_macroeconomics":0.5205128205,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.5714285714,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.728440367,
        "MMLU_high_school_statistics":0.4305555556,
        "MMLU_high_school_us_history":0.75,
        "MMLU_high_school_world_history":0.7552742616,
        "MMLU_human_aging":0.6771300448,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.6944444444,
        "MMLU_logical_fallacies":0.6748466258,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.7087378641,
        "MMLU_marketing":0.8205128205,
        "MMLU_medical_genetics":0.63,
        "MMLU_miscellaneous":0.7535121328,
        "MMLU_moral_disputes":0.6502890173,
        "MMLU_moral_scenarios":0.4167597765,
        "MMLU_nutrition":0.5947712418,
        "MMLU_philosophy":0.6430868167,
        "MMLU_prehistory":0.6203703704,
        "MMLU_professional_accounting":0.4184397163,
        "MMLU_professional_law":0.4354628422,
        "MMLU_professional_medicine":0.5257352941,
        "MMLU_professional_psychology":0.5718954248,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.6081632653,
        "MMLU_sociology":0.7313432836,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4759036145,
        "MMLU_world_religions":0.7602339181
    },
    {
        "Model":"airolima-chronos-grad-l2-13B",
        "URL":"https:\/\/huggingface.co\/kingbri\/airolima-chronos-grad-l2-13B",
        "full_model_name":"kingbri\/airolima-chronos-grad-l2-13B",
        "Parameters":13.0,
        "MMLU_average":0.557776619,
        "arc:challenge|25":0.5622866894,
        "hellaswag|10":0.6367257518,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.5197368421,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.5735849057,
        "MMLU_college_biology":0.5486111111,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.4,
        "MMLU_college_medicine":0.5086705202,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4468085106,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.5172413793,
        "MMLU_elementary_mathematics":0.3227513228,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.6419354839,
        "MMLU_high_school_chemistry":0.4137931034,
        "MMLU_high_school_computer_science":0.53,
        "MMLU_high_school_european_history":0.6545454545,
        "MMLU_high_school_geography":0.6919191919,
        "MMLU_high_school_government_and_politics":0.8082901554,
        "MMLU_high_school_macroeconomics":0.5282051282,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.5672268908,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7321100917,
        "MMLU_high_school_statistics":0.4166666667,
        "MMLU_high_school_us_history":0.7450980392,
        "MMLU_high_school_world_history":0.7510548523,
        "MMLU_human_aging":0.6726457399,
        "MMLU_human_sexuality":0.6183206107,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.7037037037,
        "MMLU_logical_fallacies":0.6687116564,
        "MMLU_machine_learning":0.4107142857,
        "MMLU_management":0.7087378641,
        "MMLU_marketing":0.8162393162,
        "MMLU_medical_genetics":0.63,
        "MMLU_miscellaneous":0.7509578544,
        "MMLU_moral_disputes":0.6387283237,
        "MMLU_moral_scenarios":0.4145251397,
        "MMLU_nutrition":0.5980392157,
        "MMLU_philosophy":0.6398713826,
        "MMLU_prehistory":0.6234567901,
        "MMLU_professional_accounting":0.414893617,
        "MMLU_professional_law":0.4335071708,
        "MMLU_professional_medicine":0.5183823529,
        "MMLU_professional_psychology":0.5702614379,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.6163265306,
        "MMLU_sociology":0.7213930348,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4759036145,
        "MMLU_world_religions":0.7602339181
    },
    {
        "Model":"Stheno-Inverted-1.2-L2-13B",
        "URL":"https:\/\/huggingface.co\/Sao10K\/Stheno-Inverted-1.2-L2-13B",
        "full_model_name":"Sao10K\/Stheno-Inverted-1.2-L2-13B",
        "Parameters":13.0,
        "MMLU_average":0.5577368542,
        "arc:challenge|25":0.5682593857,
        "hellaswag|10":0.6417048397,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.5592105263,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.6264150943,
        "MMLU_college_biology":0.6111111111,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.4913294798,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.5379310345,
        "MMLU_elementary_mathematics":0.3174603175,
        "MMLU_formal_logic":0.3412698413,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.6580645161,
        "MMLU_high_school_chemistry":0.4433497537,
        "MMLU_high_school_computer_science":0.57,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.702020202,
        "MMLU_high_school_government_and_politics":0.8134715026,
        "MMLU_high_school_macroeconomics":0.5230769231,
        "MMLU_high_school_mathematics":0.3074074074,
        "MMLU_high_school_microeconomics":0.5420168067,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.7394495413,
        "MMLU_high_school_statistics":0.3657407407,
        "MMLU_high_school_us_history":0.7549019608,
        "MMLU_high_school_world_history":0.7426160338,
        "MMLU_human_aging":0.668161435,
        "MMLU_human_sexuality":0.6488549618,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.7314814815,
        "MMLU_logical_fallacies":0.6871165644,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.7991452991,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.7509578544,
        "MMLU_moral_disputes":0.612716763,
        "MMLU_moral_scenarios":0.4067039106,
        "MMLU_nutrition":0.614379085,
        "MMLU_philosophy":0.6334405145,
        "MMLU_prehistory":0.6080246914,
        "MMLU_professional_accounting":0.4042553191,
        "MMLU_professional_law":0.4093872229,
        "MMLU_professional_medicine":0.5367647059,
        "MMLU_professional_psychology":0.5604575163,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.6408163265,
        "MMLU_sociology":0.7213930348,
        "MMLU_us_foreign_policy":0.85,
        "MMLU_virology":0.4638554217,
        "MMLU_world_religions":0.7602339181
    },
    {
        "Model":"llama-2-13b-guanaco-peft",
        "URL":"https:\/\/huggingface.co\/dfurman\/llama-2-13b-guanaco-peft",
        "full_model_name":"dfurman\/llama-2-13b-guanaco-peft",
        "Parameters":13.0,
        "MMLU_average":0.5575505384,
        "arc:challenge|25":0.5605802048,
        "hellaswag|10":0.617008564,
        "MMLU_abstract_algebra":0.38,
        "MMLU_anatomy":0.4518518519,
        "MMLU_astronomy":0.5197368421,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.6075471698,
        "MMLU_college_biology":0.5902777778,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4913294798,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.5103448276,
        "MMLU_elementary_mathematics":0.3518518519,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.6774193548,
        "MMLU_high_school_chemistry":0.4334975369,
        "MMLU_high_school_computer_science":0.59,
        "MMLU_high_school_european_history":0.6666666667,
        "MMLU_high_school_geography":0.7070707071,
        "MMLU_high_school_government_and_politics":0.8341968912,
        "MMLU_high_school_macroeconomics":0.5102564103,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.5588235294,
        "MMLU_high_school_physics":0.3642384106,
        "MMLU_high_school_psychology":0.7596330275,
        "MMLU_high_school_statistics":0.4212962963,
        "MMLU_high_school_us_history":0.7647058824,
        "MMLU_high_school_world_history":0.7510548523,
        "MMLU_human_aging":0.6367713004,
        "MMLU_human_sexuality":0.6335877863,
        "MMLU_international_law":0.7190082645,
        "MMLU_jurisprudence":0.712962963,
        "MMLU_logical_fallacies":0.6687116564,
        "MMLU_machine_learning":0.2321428571,
        "MMLU_management":0.7669902913,
        "MMLU_marketing":0.7948717949,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.7509578544,
        "MMLU_moral_disputes":0.6387283237,
        "MMLU_moral_scenarios":0.4067039106,
        "MMLU_nutrition":0.637254902,
        "MMLU_philosophy":0.6366559486,
        "MMLU_prehistory":0.6327160494,
        "MMLU_professional_accounting":0.4078014184,
        "MMLU_professional_law":0.4172099087,
        "MMLU_professional_medicine":0.5441176471,
        "MMLU_professional_psychology":0.5359477124,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.6326530612,
        "MMLU_sociology":0.7512437811,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.7660818713
    },
    {
        "Model":"Uncensored-Jordan-13B",
        "URL":"https:\/\/huggingface.co\/ajibawa-2023\/Uncensored-Jordan-13B",
        "full_model_name":"ajibawa-2023\/Uncensored-Jordan-13B",
        "Parameters":13.0,
        "MMLU_average":0.5574625492,
        "arc:challenge|25":0.5401023891,
        "hellaswag|10":0.6352320255,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.5657894737,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.6075471698,
        "MMLU_college_biology":0.5833333333,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.5317919075,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.485106383,
        "MMLU_econometrics":0.2192982456,
        "MMLU_electrical_engineering":0.5448275862,
        "MMLU_elementary_mathematics":0.3174603175,
        "MMLU_formal_logic":0.373015873,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.6516129032,
        "MMLU_high_school_chemistry":0.4137931034,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.6919191919,
        "MMLU_high_school_government_and_politics":0.8031088083,
        "MMLU_high_school_macroeconomics":0.4897435897,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.5588235294,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.7394495413,
        "MMLU_high_school_statistics":0.3703703704,
        "MMLU_high_school_us_history":0.7794117647,
        "MMLU_high_school_world_history":0.7299578059,
        "MMLU_human_aging":0.668161435,
        "MMLU_human_sexuality":0.6259541985,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.6388888889,
        "MMLU_logical_fallacies":0.6687116564,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.6699029126,
        "MMLU_marketing":0.8376068376,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7573435504,
        "MMLU_moral_disputes":0.6300578035,
        "MMLU_moral_scenarios":0.4189944134,
        "MMLU_nutrition":0.6307189542,
        "MMLU_philosophy":0.6366559486,
        "MMLU_prehistory":0.6419753086,
        "MMLU_professional_accounting":0.3936170213,
        "MMLU_professional_law":0.4165580183,
        "MMLU_professional_medicine":0.5147058824,
        "MMLU_professional_psychology":0.5784313725,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.612244898,
        "MMLU_sociology":0.7263681592,
        "MMLU_us_foreign_policy":0.8,
        "MMLU_virology":0.4939759036,
        "MMLU_world_religions":0.7719298246
    },
    {
        "Model":"MythoMix-Platypus2-13B-QLoRA-0.80-epoch",
        "URL":"https:\/\/huggingface.co\/TFLai\/MythoMix-Platypus2-13B-QLoRA-0.80-epoch",
        "full_model_name":"TFLai\/MythoMix-Platypus2-13B-QLoRA-0.80-epoch",
        "Parameters":13.0,
        "MMLU_average":0.5574493264,
        "arc:challenge|25":0.566552901,
        "hellaswag|10":0.6332403904,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.5185185185,
        "MMLU_astronomy":0.5394736842,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.5547169811,
        "MMLU_college_biology":0.625,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.549132948,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.67,
        "MMLU_conceptual_physics":0.429787234,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4896551724,
        "MMLU_elementary_mathematics":0.2962962963,
        "MMLU_formal_logic":0.3888888889,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.5967741935,
        "MMLU_high_school_chemistry":0.3743842365,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.7272727273,
        "MMLU_high_school_geography":0.7121212121,
        "MMLU_high_school_government_and_politics":0.8341968912,
        "MMLU_high_school_macroeconomics":0.5384615385,
        "MMLU_high_school_mathematics":0.3407407407,
        "MMLU_high_school_microeconomics":0.5714285714,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.7559633028,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.7647058824,
        "MMLU_high_school_world_history":0.7299578059,
        "MMLU_human_aging":0.6457399103,
        "MMLU_human_sexuality":0.6335877863,
        "MMLU_international_law":0.7520661157,
        "MMLU_jurisprudence":0.7037037037,
        "MMLU_logical_fallacies":0.6625766871,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.6893203883,
        "MMLU_marketing":0.8376068376,
        "MMLU_medical_genetics":0.64,
        "MMLU_miscellaneous":0.7675606641,
        "MMLU_moral_disputes":0.6011560694,
        "MMLU_moral_scenarios":0.4491620112,
        "MMLU_nutrition":0.591503268,
        "MMLU_philosophy":0.6045016077,
        "MMLU_prehistory":0.6296296296,
        "MMLU_professional_accounting":0.4219858156,
        "MMLU_professional_law":0.4393741851,
        "MMLU_professional_medicine":0.5330882353,
        "MMLU_professional_psychology":0.5964052288,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.612244898,
        "MMLU_sociology":0.7213930348,
        "MMLU_us_foreign_policy":0.78,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.7777777778
    },
    {
        "Model":"Huginn-19b-prototype",
        "URL":"https:\/\/huggingface.co\/The-Face-Of-Goonery\/Huginn-19b-prototype",
        "full_model_name":"The-Face-Of-Goonery\/Huginn-19b-prototype",
        "Parameters":19.0,
        "MMLU_average":0.5573026621,
        "arc:challenge|25":0.5529010239,
        "hellaswag|10":0.6199960167,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.5394736842,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.6339622642,
        "MMLU_college_biology":0.6041666667,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.5202312139,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4170212766,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.5448275862,
        "MMLU_elementary_mathematics":0.3412698413,
        "MMLU_formal_logic":0.3333333333,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.6451612903,
        "MMLU_high_school_chemistry":0.4827586207,
        "MMLU_high_school_computer_science":0.56,
        "MMLU_high_school_european_history":0.6848484848,
        "MMLU_high_school_geography":0.702020202,
        "MMLU_high_school_government_and_politics":0.7979274611,
        "MMLU_high_school_macroeconomics":0.4846153846,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.5546218487,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.7394495413,
        "MMLU_high_school_statistics":0.3842592593,
        "MMLU_high_school_us_history":0.7549019608,
        "MMLU_high_school_world_history":0.7426160338,
        "MMLU_human_aging":0.6816143498,
        "MMLU_human_sexuality":0.6488549618,
        "MMLU_international_law":0.7603305785,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.7055214724,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.7087378641,
        "MMLU_marketing":0.8247863248,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.7662835249,
        "MMLU_moral_disputes":0.6445086705,
        "MMLU_moral_scenarios":0.3418994413,
        "MMLU_nutrition":0.6339869281,
        "MMLU_philosophy":0.6334405145,
        "MMLU_prehistory":0.6358024691,
        "MMLU_professional_accounting":0.4007092199,
        "MMLU_professional_law":0.4054758801,
        "MMLU_professional_medicine":0.5147058824,
        "MMLU_professional_psychology":0.5653594771,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.6367346939,
        "MMLU_sociology":0.7412935323,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.7660818713
    },
    {
        "Model":"NewHope_HF_not_official",
        "URL":"https:\/\/huggingface.co\/WhoTookMyAmogusNickname\/NewHope_HF_not_official",
        "full_model_name":"WhoTookMyAmogusNickname\/NewHope_HF_not_official",
        "Parameters":null,
        "MMLU_average":0.557254627,
        "arc:challenge|25":0.5759385666,
        "hellaswag|10":0.6371240789,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.437037037,
        "MMLU_astronomy":0.5526315789,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.5924528302,
        "MMLU_college_biology":0.5625,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.5260115607,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.67,
        "MMLU_conceptual_physics":0.4638297872,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.3227513228,
        "MMLU_formal_logic":0.373015873,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.7064516129,
        "MMLU_high_school_chemistry":0.4088669951,
        "MMLU_high_school_computer_science":0.54,
        "MMLU_high_school_european_history":0.6606060606,
        "MMLU_high_school_geography":0.702020202,
        "MMLU_high_school_government_and_politics":0.8341968912,
        "MMLU_high_school_macroeconomics":0.5307692308,
        "MMLU_high_school_mathematics":0.3148148148,
        "MMLU_high_school_microeconomics":0.6176470588,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.7247706422,
        "MMLU_high_school_statistics":0.462962963,
        "MMLU_high_school_us_history":0.7696078431,
        "MMLU_high_school_world_history":0.7341772152,
        "MMLU_human_aging":0.6591928251,
        "MMLU_human_sexuality":0.6183206107,
        "MMLU_international_law":0.7107438017,
        "MMLU_jurisprudence":0.6944444444,
        "MMLU_logical_fallacies":0.6441717791,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.7766990291,
        "MMLU_marketing":0.7991452991,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.7522349936,
        "MMLU_moral_disputes":0.6358381503,
        "MMLU_moral_scenarios":0.3776536313,
        "MMLU_nutrition":0.6405228758,
        "MMLU_philosophy":0.6430868167,
        "MMLU_prehistory":0.6327160494,
        "MMLU_professional_accounting":0.4290780142,
        "MMLU_professional_law":0.4380704042,
        "MMLU_professional_medicine":0.5625,
        "MMLU_professional_psychology":0.5441176471,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.6326530612,
        "MMLU_sociology":0.7412935323,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4819277108,
        "MMLU_world_religions":0.783625731
    },
    {
        "Model":"NewHope",
        "URL":"https:\/\/huggingface.co\/SLAM-group\/NewHope",
        "full_model_name":"SLAM-group\/NewHope",
        "Parameters":null,
        "MMLU_average":0.5571905933,
        "arc:challenge|25":0.5767918089,
        "hellaswag|10":0.6366261701,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.437037037,
        "MMLU_astronomy":0.5328947368,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.5811320755,
        "MMLU_college_biology":0.5763888889,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.5260115607,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.67,
        "MMLU_conceptual_physics":0.4595744681,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.3148148148,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.6870967742,
        "MMLU_high_school_chemistry":0.3990147783,
        "MMLU_high_school_computer_science":0.54,
        "MMLU_high_school_european_history":0.6606060606,
        "MMLU_high_school_geography":0.7070707071,
        "MMLU_high_school_government_and_politics":0.8341968912,
        "MMLU_high_school_macroeconomics":0.5256410256,
        "MMLU_high_school_mathematics":0.3259259259,
        "MMLU_high_school_microeconomics":0.6134453782,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.7211009174,
        "MMLU_high_school_statistics":0.4583333333,
        "MMLU_high_school_us_history":0.7745098039,
        "MMLU_high_school_world_history":0.7426160338,
        "MMLU_human_aging":0.6547085202,
        "MMLU_human_sexuality":0.6259541985,
        "MMLU_international_law":0.7107438017,
        "MMLU_jurisprudence":0.6944444444,
        "MMLU_logical_fallacies":0.6380368098,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.7766990291,
        "MMLU_marketing":0.7991452991,
        "MMLU_medical_genetics":0.53,
        "MMLU_miscellaneous":0.7496807152,
        "MMLU_moral_disputes":0.6358381503,
        "MMLU_moral_scenarios":0.3810055866,
        "MMLU_nutrition":0.6405228758,
        "MMLU_philosophy":0.6366559486,
        "MMLU_prehistory":0.6388888889,
        "MMLU_professional_accounting":0.4255319149,
        "MMLU_professional_law":0.44589309,
        "MMLU_professional_medicine":0.5588235294,
        "MMLU_professional_psychology":0.5473856209,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.6285714286,
        "MMLU_sociology":0.736318408,
        "MMLU_us_foreign_policy":0.85,
        "MMLU_virology":0.4939759036,
        "MMLU_world_religions":0.783625731
    },
    {
        "Model":"openbuddy-llama2-34b-v11.1-bf16",
        "URL":"https:\/\/huggingface.co\/oPenBuddy\/openbuddy-llama2-34b-v11.1-bf16",
        "full_model_name":"oPenBuddy\/openbuddy-llama2-34b-v11.1-bf16",
        "Parameters":34.0,
        "MMLU_average":0.5571082206,
        "arc:challenge|25":0.4641638225,
        "hellaswag|10":0.5283808006,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.4074074074,
        "MMLU_astronomy":0.5855263158,
        "MMLU_business_ethics":0.63,
        "MMLU_clinical_knowledge":0.5132075472,
        "MMLU_college_biology":0.5347222222,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.5,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.4739884393,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.72,
        "MMLU_conceptual_physics":0.4553191489,
        "MMLU_econometrics":0.3684210526,
        "MMLU_electrical_engineering":0.4896551724,
        "MMLU_elementary_mathematics":0.3783068783,
        "MMLU_formal_logic":0.4603174603,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.6258064516,
        "MMLU_high_school_chemistry":0.4088669951,
        "MMLU_high_school_computer_science":0.7,
        "MMLU_high_school_european_history":0.7151515152,
        "MMLU_high_school_geography":0.7323232323,
        "MMLU_high_school_government_and_politics":0.7979274611,
        "MMLU_high_school_macroeconomics":0.5538461538,
        "MMLU_high_school_mathematics":0.3444444444,
        "MMLU_high_school_microeconomics":0.5588235294,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7412844037,
        "MMLU_high_school_statistics":0.5185185185,
        "MMLU_high_school_us_history":0.7009803922,
        "MMLU_high_school_world_history":0.7594936709,
        "MMLU_human_aging":0.6143497758,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.7520661157,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.6993865031,
        "MMLU_machine_learning":0.4375,
        "MMLU_management":0.6699029126,
        "MMLU_marketing":0.8333333333,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.7100893997,
        "MMLU_moral_disputes":0.612716763,
        "MMLU_moral_scenarios":0.2860335196,
        "MMLU_nutrition":0.5784313725,
        "MMLU_philosophy":0.6173633441,
        "MMLU_prehistory":0.5679012346,
        "MMLU_professional_accounting":0.3865248227,
        "MMLU_professional_law":0.3996088657,
        "MMLU_professional_medicine":0.4595588235,
        "MMLU_professional_psychology":0.5081699346,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.6612244898,
        "MMLU_sociology":0.7213930348,
        "MMLU_us_foreign_policy":0.72,
        "MMLU_virology":0.4096385542,
        "MMLU_world_religions":0.7368421053
    },
    {
        "Model":"MythoLogic-L2-13b",
        "URL":"https:\/\/huggingface.co\/Gryphe\/MythoLogic-L2-13b",
        "full_model_name":"Gryphe\/MythoLogic-L2-13b",
        "Parameters":13.0,
        "MMLU_average":0.557044543,
        "arc:challenge|25":0.5750853242,
        "hellaswag|10":0.6408086039,
        "MMLU_abstract_algebra":0.38,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.5394736842,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.5962264151,
        "MMLU_college_biology":0.5833333333,
        "MMLU_college_chemistry":0.36,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5202312139,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.4340425532,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.4896551724,
        "MMLU_elementary_mathematics":0.328042328,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.6387096774,
        "MMLU_high_school_chemistry":0.4137931034,
        "MMLU_high_school_computer_science":0.54,
        "MMLU_high_school_european_history":0.6606060606,
        "MMLU_high_school_geography":0.6818181818,
        "MMLU_high_school_government_and_politics":0.7875647668,
        "MMLU_high_school_macroeconomics":0.5358974359,
        "MMLU_high_school_mathematics":0.2925925926,
        "MMLU_high_school_microeconomics":0.5588235294,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.7321100917,
        "MMLU_high_school_statistics":0.4166666667,
        "MMLU_high_school_us_history":0.7549019608,
        "MMLU_high_school_world_history":0.7426160338,
        "MMLU_human_aging":0.6816143498,
        "MMLU_human_sexuality":0.5954198473,
        "MMLU_international_law":0.7438016529,
        "MMLU_jurisprudence":0.6944444444,
        "MMLU_logical_fallacies":0.6748466258,
        "MMLU_machine_learning":0.4017857143,
        "MMLU_management":0.7087378641,
        "MMLU_marketing":0.8247863248,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7586206897,
        "MMLU_moral_disputes":0.6271676301,
        "MMLU_moral_scenarios":0.4793296089,
        "MMLU_nutrition":0.5882352941,
        "MMLU_philosophy":0.6334405145,
        "MMLU_prehistory":0.6141975309,
        "MMLU_professional_accounting":0.4184397163,
        "MMLU_professional_law":0.4302477184,
        "MMLU_professional_medicine":0.5147058824,
        "MMLU_professional_psychology":0.5735294118,
        "MMLU_public_relations":0.6727272727,
        "MMLU_security_studies":0.6163265306,
        "MMLU_sociology":0.7263681592,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.4518072289,
        "MMLU_world_religions":0.783625731
    },
    {
        "Model":"Emerald-13B",
        "URL":"https:\/\/huggingface.co\/Undi95\/Emerald-13B",
        "full_model_name":"Undi95\/Emerald-13B",
        "Parameters":13.0,
        "MMLU_average":0.5569560786,
        "arc:challenge|25":0.5853242321,
        "hellaswag|10":0.6385182235,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.5259259259,
        "MMLU_astronomy":0.5394736842,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.5660377358,
        "MMLU_college_biology":0.5902777778,
        "MMLU_college_chemistry":0.32,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.39,
        "MMLU_college_medicine":0.5260115607,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.4680851064,
        "MMLU_econometrics":0.3333333333,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.3333333333,
        "MMLU_formal_logic":0.380952381,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.6322580645,
        "MMLU_high_school_chemistry":0.4334975369,
        "MMLU_high_school_computer_science":0.52,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.7070707071,
        "MMLU_high_school_government_and_politics":0.7720207254,
        "MMLU_high_school_macroeconomics":0.5128205128,
        "MMLU_high_school_mathematics":0.3185185185,
        "MMLU_high_school_microeconomics":0.5840336134,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.7376146789,
        "MMLU_high_school_statistics":0.3796296296,
        "MMLU_high_school_us_history":0.7745098039,
        "MMLU_high_school_world_history":0.7721518987,
        "MMLU_human_aging":0.6816143498,
        "MMLU_human_sexuality":0.6488549618,
        "MMLU_international_law":0.7520661157,
        "MMLU_jurisprudence":0.712962963,
        "MMLU_logical_fallacies":0.6871165644,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.7820512821,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.754789272,
        "MMLU_moral_disputes":0.6358381503,
        "MMLU_moral_scenarios":0.4122905028,
        "MMLU_nutrition":0.6241830065,
        "MMLU_philosophy":0.6398713826,
        "MMLU_prehistory":0.6296296296,
        "MMLU_professional_accounting":0.4113475177,
        "MMLU_professional_law":0.4263363755,
        "MMLU_professional_medicine":0.5183823529,
        "MMLU_professional_psychology":0.5702614379,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.5918367347,
        "MMLU_sociology":0.7064676617,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4638554217,
        "MMLU_world_religions":0.7894736842
    },
    {
        "Model":"llama-2-13b-FINETUNE4_3.8w-r16-gate_up_down-test1",
        "URL":"https:\/\/huggingface.co\/CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r16-gate_up_down-test1",
        "full_model_name":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r16-gate_up_down-test1",
        "Parameters":13.0,
        "MMLU_average":0.5562885487,
        "arc:challenge|25":0.5255972696,
        "hellaswag|10":0.6141206931,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.5259259259,
        "MMLU_astronomy":0.5592105263,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.6226415094,
        "MMLU_college_biology":0.6041666667,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.4797687861,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.4127659574,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4551724138,
        "MMLU_elementary_mathematics":0.3518518519,
        "MMLU_formal_logic":0.3412698413,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.6387096774,
        "MMLU_high_school_chemistry":0.4827586207,
        "MMLU_high_school_computer_science":0.53,
        "MMLU_high_school_european_history":0.7090909091,
        "MMLU_high_school_geography":0.702020202,
        "MMLU_high_school_government_and_politics":0.7668393782,
        "MMLU_high_school_macroeconomics":0.5230769231,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.5840336134,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.7614678899,
        "MMLU_high_school_statistics":0.4305555556,
        "MMLU_high_school_us_history":0.7450980392,
        "MMLU_high_school_world_history":0.7594936709,
        "MMLU_human_aging":0.6591928251,
        "MMLU_human_sexuality":0.5801526718,
        "MMLU_international_law":0.7768595041,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.6809815951,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.7948717949,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.7675606641,
        "MMLU_moral_disputes":0.6387283237,
        "MMLU_moral_scenarios":0.4703910615,
        "MMLU_nutrition":0.5849673203,
        "MMLU_philosophy":0.6334405145,
        "MMLU_prehistory":0.6327160494,
        "MMLU_professional_accounting":0.4397163121,
        "MMLU_professional_law":0.424380704,
        "MMLU_professional_medicine":0.5514705882,
        "MMLU_professional_psychology":0.5735294118,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.5959183673,
        "MMLU_sociology":0.7164179104,
        "MMLU_us_foreign_policy":0.75,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.7719298246
    },
    {
        "Model":"Platypus2-13B-QLoRA-0.80-epoch",
        "URL":"https:\/\/huggingface.co\/TFLai\/Platypus2-13B-QLoRA-0.80-epoch",
        "full_model_name":"TFLai\/Platypus2-13B-QLoRA-0.80-epoch",
        "Parameters":13.0,
        "MMLU_average":0.5562706921,
        "arc:challenge|25":0.5443686007,
        "hellaswag|10":0.6076478789,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.5592105263,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.5849056604,
        "MMLU_college_biology":0.6319444444,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.49,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.5086705202,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.67,
        "MMLU_conceptual_physics":0.4680851064,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.3333333333,
        "MMLU_formal_logic":0.3968253968,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.664516129,
        "MMLU_high_school_chemistry":0.4285714286,
        "MMLU_high_school_computer_science":0.56,
        "MMLU_high_school_european_history":0.7151515152,
        "MMLU_high_school_geography":0.7323232323,
        "MMLU_high_school_government_and_politics":0.8082901554,
        "MMLU_high_school_macroeconomics":0.5025641026,
        "MMLU_high_school_mathematics":0.3185185185,
        "MMLU_high_school_microeconomics":0.5378151261,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7688073394,
        "MMLU_high_school_statistics":0.4675925926,
        "MMLU_high_school_us_history":0.7156862745,
        "MMLU_high_school_world_history":0.7383966245,
        "MMLU_human_aging":0.6547085202,
        "MMLU_human_sexuality":0.6259541985,
        "MMLU_international_law":0.7438016529,
        "MMLU_jurisprudence":0.6666666667,
        "MMLU_logical_fallacies":0.6380368098,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.7766990291,
        "MMLU_marketing":0.8205128205,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7688378033,
        "MMLU_moral_disputes":0.6184971098,
        "MMLU_moral_scenarios":0.2893854749,
        "MMLU_nutrition":0.591503268,
        "MMLU_philosophy":0.6173633441,
        "MMLU_prehistory":0.6296296296,
        "MMLU_professional_accounting":0.4219858156,
        "MMLU_professional_law":0.4302477184,
        "MMLU_professional_medicine":0.5882352941,
        "MMLU_professional_psychology":0.5964052288,
        "MMLU_public_relations":0.6727272727,
        "MMLU_security_studies":0.587755102,
        "MMLU_sociology":0.7263681592,
        "MMLU_us_foreign_policy":0.77,
        "MMLU_virology":0.4397590361,
        "MMLU_world_religions":0.7719298246
    },
    {
        "Model":"Redmond-Puffin-13B-instruct-PL-lora_unload",
        "URL":"https:\/\/huggingface.co\/Aspik101\/Redmond-Puffin-13B-instruct-PL-lora_unload",
        "full_model_name":"Aspik101\/Redmond-Puffin-13B-instruct-PL-lora_unload",
        "Parameters":13.0,
        "MMLU_average":0.5560984705,
        "arc:challenge|25":0.5639931741,
        "hellaswag|10":0.6173073093,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.5328947368,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.6075471698,
        "MMLU_college_biology":0.5972222222,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5086705202,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.72,
        "MMLU_conceptual_physics":0.4425531915,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.5172413793,
        "MMLU_elementary_mathematics":0.3412698413,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.6580645161,
        "MMLU_high_school_chemistry":0.4482758621,
        "MMLU_high_school_computer_science":0.57,
        "MMLU_high_school_european_history":0.6545454545,
        "MMLU_high_school_geography":0.6818181818,
        "MMLU_high_school_government_and_politics":0.8186528497,
        "MMLU_high_school_macroeconomics":0.5025641026,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.5546218487,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.7412844037,
        "MMLU_high_school_statistics":0.462962963,
        "MMLU_high_school_us_history":0.7549019608,
        "MMLU_high_school_world_history":0.7383966245,
        "MMLU_human_aging":0.6502242152,
        "MMLU_human_sexuality":0.641221374,
        "MMLU_international_law":0.7107438017,
        "MMLU_jurisprudence":0.712962963,
        "MMLU_logical_fallacies":0.6871165644,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.786407767,
        "MMLU_marketing":0.7905982906,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.748403576,
        "MMLU_moral_disputes":0.6647398844,
        "MMLU_moral_scenarios":0.3832402235,
        "MMLU_nutrition":0.6176470588,
        "MMLU_philosophy":0.6430868167,
        "MMLU_prehistory":0.6265432099,
        "MMLU_professional_accounting":0.4219858156,
        "MMLU_professional_law":0.425684485,
        "MMLU_professional_medicine":0.5294117647,
        "MMLU_professional_psychology":0.5408496732,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.612244898,
        "MMLU_sociology":0.7462686567,
        "MMLU_us_foreign_policy":0.86,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.7660818713
    },
    {
        "Model":"llama2_13b_instructed_version2",
        "URL":"https:\/\/huggingface.co\/Expert68\/llama2_13b_instructed_version2",
        "full_model_name":"Expert68\/llama2_13b_instructed_version2",
        "Parameters":13.0,
        "MMLU_average":0.5560606804,
        "arc:challenge|25":0.5631399317,
        "hellaswag|10":0.6412069309,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.5460526316,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.5924528302,
        "MMLU_college_biology":0.625,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.5144508671,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.62,
        "MMLU_conceptual_physics":0.4510638298,
        "MMLU_econometrics":0.3333333333,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.3227513228,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.6483870968,
        "MMLU_high_school_chemistry":0.4384236453,
        "MMLU_high_school_computer_science":0.53,
        "MMLU_high_school_european_history":0.696969697,
        "MMLU_high_school_geography":0.6717171717,
        "MMLU_high_school_government_and_politics":0.8238341969,
        "MMLU_high_school_macroeconomics":0.5153846154,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.5924369748,
        "MMLU_high_school_physics":0.3708609272,
        "MMLU_high_school_psychology":0.728440367,
        "MMLU_high_school_statistics":0.4259259259,
        "MMLU_high_school_us_history":0.7745098039,
        "MMLU_high_school_world_history":0.7383966245,
        "MMLU_human_aging":0.6771300448,
        "MMLU_human_sexuality":0.5801526718,
        "MMLU_international_law":0.7107438017,
        "MMLU_jurisprudence":0.6944444444,
        "MMLU_logical_fallacies":0.6687116564,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.8205128205,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.7637292465,
        "MMLU_moral_disputes":0.6242774566,
        "MMLU_moral_scenarios":0.4011173184,
        "MMLU_nutrition":0.591503268,
        "MMLU_philosophy":0.652733119,
        "MMLU_prehistory":0.6604938272,
        "MMLU_professional_accounting":0.4361702128,
        "MMLU_professional_law":0.4380704042,
        "MMLU_professional_medicine":0.5330882353,
        "MMLU_professional_psychology":0.5751633987,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6,
        "MMLU_sociology":0.7164179104,
        "MMLU_us_foreign_policy":0.79,
        "MMLU_virology":0.4518072289,
        "MMLU_world_religions":0.7660818713
    },
    {
        "Model":"Athena-v1",
        "URL":"https:\/\/huggingface.co\/IkariDev\/Athena-v1",
        "full_model_name":"IkariDev\/Athena-v1",
        "Parameters":null,
        "MMLU_average":0.5560519847,
        "arc:challenge|25":0.5605802048,
        "hellaswag|10":0.6315475005,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.5263157895,
        "MMLU_business_ethics":0.58,
        "MMLU_clinical_knowledge":0.5962264151,
        "MMLU_college_biology":0.5694444444,
        "MMLU_college_chemistry":0.36,
        "MMLU_college_computer_science":0.51,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.5317919075,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.4085106383,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.5310344828,
        "MMLU_elementary_mathematics":0.3227513228,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.4,
        "MMLU_high_school_biology":0.6580645161,
        "MMLU_high_school_chemistry":0.4532019704,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.6606060606,
        "MMLU_high_school_geography":0.702020202,
        "MMLU_high_school_government_and_politics":0.8186528497,
        "MMLU_high_school_macroeconomics":0.5102564103,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.5336134454,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.7247706422,
        "MMLU_high_school_statistics":0.4212962963,
        "MMLU_high_school_us_history":0.7598039216,
        "MMLU_high_school_world_history":0.7299578059,
        "MMLU_human_aging":0.668161435,
        "MMLU_human_sexuality":0.6259541985,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.6809815951,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.7777777778,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.7420178799,
        "MMLU_moral_disputes":0.6213872832,
        "MMLU_moral_scenarios":0.3206703911,
        "MMLU_nutrition":0.6078431373,
        "MMLU_philosophy":0.6430868167,
        "MMLU_prehistory":0.6574074074,
        "MMLU_professional_accounting":0.4042553191,
        "MMLU_professional_law":0.3924380704,
        "MMLU_professional_medicine":0.5257352941,
        "MMLU_professional_psychology":0.5408496732,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.6408163265,
        "MMLU_sociology":0.7512437811,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4939759036,
        "MMLU_world_religions":0.7251461988
    },
    {
        "Model":"llama-2-13b-alpaca-test",
        "URL":"https:\/\/huggingface.co\/CHIH-HUNG\/llama-2-13b-alpaca-test",
        "full_model_name":"CHIH-HUNG\/llama-2-13b-alpaca-test",
        "Parameters":13.0,
        "MMLU_average":0.5559241122,
        "arc:challenge|25":0.5571672355,
        "hellaswag|10":0.6117307309,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.5657894737,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.6150943396,
        "MMLU_college_biology":0.5625,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.5260115607,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.4340425532,
        "MMLU_econometrics":0.3421052632,
        "MMLU_electrical_engineering":0.5172413793,
        "MMLU_elementary_mathematics":0.3306878307,
        "MMLU_formal_logic":0.3412698413,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.6580645161,
        "MMLU_high_school_chemistry":0.4630541872,
        "MMLU_high_school_computer_science":0.61,
        "MMLU_high_school_european_history":0.6666666667,
        "MMLU_high_school_geography":0.702020202,
        "MMLU_high_school_government_and_politics":0.8238341969,
        "MMLU_high_school_macroeconomics":0.5153846154,
        "MMLU_high_school_mathematics":0.2888888889,
        "MMLU_high_school_microeconomics":0.5714285714,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.7376146789,
        "MMLU_high_school_statistics":0.4166666667,
        "MMLU_high_school_us_history":0.7352941176,
        "MMLU_high_school_world_history":0.7299578059,
        "MMLU_human_aging":0.6547085202,
        "MMLU_human_sexuality":0.6259541985,
        "MMLU_international_law":0.7438016529,
        "MMLU_jurisprudence":0.7407407407,
        "MMLU_logical_fallacies":0.6932515337,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.7669902913,
        "MMLU_marketing":0.8076923077,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.7611749681,
        "MMLU_moral_disputes":0.6445086705,
        "MMLU_moral_scenarios":0.2726256983,
        "MMLU_nutrition":0.6045751634,
        "MMLU_philosophy":0.6463022508,
        "MMLU_prehistory":0.6203703704,
        "MMLU_professional_accounting":0.4078014184,
        "MMLU_professional_law":0.4178617992,
        "MMLU_professional_medicine":0.4889705882,
        "MMLU_professional_psychology":0.5653594771,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.612244898,
        "MMLU_sociology":0.7512437811,
        "MMLU_us_foreign_policy":0.79,
        "MMLU_virology":0.4397590361,
        "MMLU_world_religions":0.7602339181
    },
    {
        "Model":"llama-2-13b-FINETUNE3_3.3w-r16-q_k_v_o",
        "URL":"https:\/\/huggingface.co\/CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r16-q_k_v_o",
        "full_model_name":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r16-q_k_v_o",
        "Parameters":13.0,
        "MMLU_average":0.5558055222,
        "arc:challenge|25":0.5435153584,
        "hellaswag|10":0.6028679546,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.5526315789,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.5886792453,
        "MMLU_college_biology":0.5902777778,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.5028901734,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.73,
        "MMLU_conceptual_physics":0.4595744681,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.4344827586,
        "MMLU_elementary_mathematics":0.335978836,
        "MMLU_formal_logic":0.3333333333,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.6709677419,
        "MMLU_high_school_chemistry":0.4482758621,
        "MMLU_high_school_computer_science":0.52,
        "MMLU_high_school_european_history":0.6848484848,
        "MMLU_high_school_geography":0.7121212121,
        "MMLU_high_school_government_and_politics":0.8031088083,
        "MMLU_high_school_macroeconomics":0.4923076923,
        "MMLU_high_school_mathematics":0.3148148148,
        "MMLU_high_school_microeconomics":0.5672268908,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.752293578,
        "MMLU_high_school_statistics":0.5231481481,
        "MMLU_high_school_us_history":0.7745098039,
        "MMLU_high_school_world_history":0.7426160338,
        "MMLU_human_aging":0.6726457399,
        "MMLU_human_sexuality":0.5801526718,
        "MMLU_international_law":0.7685950413,
        "MMLU_jurisprudence":0.6851851852,
        "MMLU_logical_fallacies":0.6319018405,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.7961165049,
        "MMLU_marketing":0.8333333333,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7713920817,
        "MMLU_moral_disputes":0.6531791908,
        "MMLU_moral_scenarios":0.3162011173,
        "MMLU_nutrition":0.5980392157,
        "MMLU_philosophy":0.6752411576,
        "MMLU_prehistory":0.6419753086,
        "MMLU_professional_accounting":0.4042553191,
        "MMLU_professional_law":0.4132985658,
        "MMLU_professional_medicine":0.5588235294,
        "MMLU_professional_psychology":0.5751633987,
        "MMLU_public_relations":0.5818181818,
        "MMLU_security_studies":0.6163265306,
        "MMLU_sociology":0.7462686567,
        "MMLU_us_foreign_policy":0.76,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.7660818713
    },
    {
        "Model":"Libra-19B",
        "URL":"https:\/\/huggingface.co\/Envoid\/Libra-19B",
        "full_model_name":"Envoid\/Libra-19B",
        "Parameters":19.0,
        "MMLU_average":0.5556872825,
        "arc:challenge|25":0.566552901,
        "hellaswag|10":0.6181039634,
        "MMLU_abstract_algebra":0.4,
        "MMLU_anatomy":0.5333333333,
        "MMLU_astronomy":0.5460526316,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.5962264151,
        "MMLU_college_biology":0.5763888889,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.5260115607,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.4382978723,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4965517241,
        "MMLU_elementary_mathematics":0.3174603175,
        "MMLU_formal_logic":0.3412698413,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.6677419355,
        "MMLU_high_school_chemistry":0.3842364532,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.6848484848,
        "MMLU_high_school_geography":0.6919191919,
        "MMLU_high_school_government_and_politics":0.792746114,
        "MMLU_high_school_macroeconomics":0.5153846154,
        "MMLU_high_school_mathematics":0.3185185185,
        "MMLU_high_school_microeconomics":0.5378151261,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.7229357798,
        "MMLU_high_school_statistics":0.4398148148,
        "MMLU_high_school_us_history":0.7745098039,
        "MMLU_high_school_world_history":0.7215189873,
        "MMLU_human_aging":0.6771300448,
        "MMLU_human_sexuality":0.6488549618,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.7592592593,
        "MMLU_logical_fallacies":0.6564417178,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.8076923077,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.7471264368,
        "MMLU_moral_disputes":0.6300578035,
        "MMLU_moral_scenarios":0.4055865922,
        "MMLU_nutrition":0.6045751634,
        "MMLU_philosophy":0.6302250804,
        "MMLU_prehistory":0.6141975309,
        "MMLU_professional_accounting":0.4042553191,
        "MMLU_professional_law":0.4211212516,
        "MMLU_professional_medicine":0.5551470588,
        "MMLU_professional_psychology":0.5702614379,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.6244897959,
        "MMLU_sociology":0.7263681592,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4759036145,
        "MMLU_world_religions":0.7602339181
    },
    {
        "Model":"Synatra-11B-Testbench",
        "URL":"https:\/\/huggingface.co\/maywell\/Synatra-11B-Testbench",
        "full_model_name":"maywell\/Synatra-11B-Testbench",
        "Parameters":11.0,
        "MMLU_average":0.5555706992,
        "arc:challenge|25":0.5273037543,
        "hellaswag|10":0.5855407289,
        "MMLU_abstract_algebra":0.38,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5526315789,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.6113207547,
        "MMLU_college_biology":0.6041666667,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.51,
        "MMLU_college_mathematics":0.4,
        "MMLU_college_medicine":0.5895953757,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.74,
        "MMLU_conceptual_physics":0.514893617,
        "MMLU_econometrics":0.3771929825,
        "MMLU_electrical_engineering":0.5379310345,
        "MMLU_elementary_mathematics":0.3465608466,
        "MMLU_formal_logic":0.3333333333,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.6483870968,
        "MMLU_high_school_chemistry":0.4088669951,
        "MMLU_high_school_computer_science":0.63,
        "MMLU_high_school_european_history":0.6787878788,
        "MMLU_high_school_geography":0.7070707071,
        "MMLU_high_school_government_and_politics":0.7357512953,
        "MMLU_high_school_macroeconomics":0.5333333333,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.5168067227,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.7376146789,
        "MMLU_high_school_statistics":0.4398148148,
        "MMLU_high_school_us_history":0.7058823529,
        "MMLU_high_school_world_history":0.6919831224,
        "MMLU_human_aging":0.6143497758,
        "MMLU_human_sexuality":0.6641221374,
        "MMLU_international_law":0.7024793388,
        "MMLU_jurisprudence":0.6759259259,
        "MMLU_logical_fallacies":0.6196319018,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.6893203883,
        "MMLU_marketing":0.8205128205,
        "MMLU_medical_genetics":0.64,
        "MMLU_miscellaneous":0.748403576,
        "MMLU_moral_disputes":0.6358381503,
        "MMLU_moral_scenarios":0.3273743017,
        "MMLU_nutrition":0.5588235294,
        "MMLU_philosophy":0.6430868167,
        "MMLU_prehistory":0.5925925926,
        "MMLU_professional_accounting":0.3936170213,
        "MMLU_professional_law":0.3885267275,
        "MMLU_professional_medicine":0.5,
        "MMLU_professional_psychology":0.5326797386,
        "MMLU_public_relations":0.6,
        "MMLU_security_studies":0.6285714286,
        "MMLU_sociology":0.7313432836,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4337349398,
        "MMLU_world_religions":0.7134502924
    },
    {
        "Model":"zephyr_7b_norobots",
        "URL":"https:\/\/huggingface.co\/qblocks\/zephyr_7b_norobots",
        "full_model_name":"qblocks\/zephyr_7b_norobots",
        "Parameters":7.0,
        "MMLU_average":0.5552251892,
        "arc:challenge|25":0.5179180887,
        "hellaswag|10":0.5922127066,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.5986842105,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.6226415094,
        "MMLU_college_biology":0.6111111111,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.49,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.5375722543,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.4425531915,
        "MMLU_econometrics":0.3684210526,
        "MMLU_electrical_engineering":0.4965517241,
        "MMLU_elementary_mathematics":0.3624338624,
        "MMLU_formal_logic":0.4126984127,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.6451612903,
        "MMLU_high_school_chemistry":0.4482758621,
        "MMLU_high_school_computer_science":0.59,
        "MMLU_high_school_european_history":0.6848484848,
        "MMLU_high_school_geography":0.7727272727,
        "MMLU_high_school_government_and_politics":0.7512953368,
        "MMLU_high_school_macroeconomics":0.5487179487,
        "MMLU_high_school_mathematics":0.3296296296,
        "MMLU_high_school_microeconomics":0.5756302521,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.7394495413,
        "MMLU_high_school_statistics":0.4027777778,
        "MMLU_high_school_us_history":0.6911764706,
        "MMLU_high_school_world_history":0.6371308017,
        "MMLU_human_aging":0.5919282511,
        "MMLU_human_sexuality":0.6870229008,
        "MMLU_international_law":0.6776859504,
        "MMLU_jurisprudence":0.6666666667,
        "MMLU_logical_fallacies":0.6564417178,
        "MMLU_machine_learning":0.4732142857,
        "MMLU_management":0.6990291262,
        "MMLU_marketing":0.7863247863,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7062579821,
        "MMLU_moral_disputes":0.6069364162,
        "MMLU_moral_scenarios":0.2983240223,
        "MMLU_nutrition":0.6078431373,
        "MMLU_philosophy":0.6495176849,
        "MMLU_prehistory":0.6172839506,
        "MMLU_professional_accounting":0.4113475177,
        "MMLU_professional_law":0.3813559322,
        "MMLU_professional_medicine":0.5992647059,
        "MMLU_professional_psychology":0.5588235294,
        "MMLU_public_relations":0.5454545455,
        "MMLU_security_studies":0.6367346939,
        "MMLU_sociology":0.7462686567,
        "MMLU_us_foreign_policy":0.77,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.7777777778
    },
    {
        "Model":"llama2-13b-orca-v2-8k-3166",
        "URL":"https:\/\/huggingface.co\/OpenAssistant\/llama2-13b-orca-v2-8k-3166",
        "full_model_name":"OpenAssistant\/llama2-13b-orca-v2-8k-3166",
        "Parameters":13.0,
        "MMLU_average":0.5549613167,
        "arc:challenge|25":0.5366894198,
        "hellaswag|10":0.5955984864,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.6184210526,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.5849056604,
        "MMLU_college_biology":0.5555555556,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.4797687861,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.4127659574,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.524137931,
        "MMLU_elementary_mathematics":0.3333333333,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.6387096774,
        "MMLU_high_school_chemistry":0.4384236453,
        "MMLU_high_school_computer_science":0.53,
        "MMLU_high_school_european_history":0.7090909091,
        "MMLU_high_school_geography":0.702020202,
        "MMLU_high_school_government_and_politics":0.792746114,
        "MMLU_high_school_macroeconomics":0.5153846154,
        "MMLU_high_school_mathematics":0.3555555556,
        "MMLU_high_school_microeconomics":0.5546218487,
        "MMLU_high_school_physics":0.357615894,
        "MMLU_high_school_psychology":0.7339449541,
        "MMLU_high_school_statistics":0.3842592593,
        "MMLU_high_school_us_history":0.7794117647,
        "MMLU_high_school_world_history":0.7510548523,
        "MMLU_human_aging":0.6502242152,
        "MMLU_human_sexuality":0.6335877863,
        "MMLU_international_law":0.7685950413,
        "MMLU_jurisprudence":0.6574074074,
        "MMLU_logical_fallacies":0.6134969325,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.6990291262,
        "MMLU_marketing":0.8076923077,
        "MMLU_medical_genetics":0.64,
        "MMLU_miscellaneous":0.754789272,
        "MMLU_moral_disputes":0.6358381503,
        "MMLU_moral_scenarios":0.3251396648,
        "MMLU_nutrition":0.6339869281,
        "MMLU_philosophy":0.6109324759,
        "MMLU_prehistory":0.5956790123,
        "MMLU_professional_accounting":0.3936170213,
        "MMLU_professional_law":0.4263363755,
        "MMLU_professional_medicine":0.4963235294,
        "MMLU_professional_psychology":0.545751634,
        "MMLU_public_relations":0.6272727273,
        "MMLU_security_studies":0.693877551,
        "MMLU_sociology":0.7462686567,
        "MMLU_us_foreign_policy":0.8,
        "MMLU_virology":0.4096385542,
        "MMLU_world_religions":0.783625731
    },
    {
        "Model":"llama-2-13b-guanaco-fp16",
        "URL":"https:\/\/huggingface.co\/CoolWP\/llama-2-13b-guanaco-fp16",
        "full_model_name":"CoolWP\/llama-2-13b-guanaco-fp16",
        "Parameters":13.0,
        "MMLU_average":0.554748379,
        "arc:challenge|25":0.5529010239,
        "hellaswag|10":0.6151165107,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.5263157895,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.6037735849,
        "MMLU_college_biology":0.5763888889,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.5144508671,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.72,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.5379310345,
        "MMLU_elementary_mathematics":0.3333333333,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.664516129,
        "MMLU_high_school_chemistry":0.4581280788,
        "MMLU_high_school_computer_science":0.59,
        "MMLU_high_school_european_history":0.6666666667,
        "MMLU_high_school_geography":0.6919191919,
        "MMLU_high_school_government_and_politics":0.8031088083,
        "MMLU_high_school_macroeconomics":0.5102564103,
        "MMLU_high_school_mathematics":0.2925925926,
        "MMLU_high_school_microeconomics":0.5714285714,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.7486238532,
        "MMLU_high_school_statistics":0.4444444444,
        "MMLU_high_school_us_history":0.7598039216,
        "MMLU_high_school_world_history":0.7299578059,
        "MMLU_human_aging":0.6547085202,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.712962963,
        "MMLU_logical_fallacies":0.6932515337,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.7669902913,
        "MMLU_marketing":0.7991452991,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.7586206897,
        "MMLU_moral_disputes":0.6329479769,
        "MMLU_moral_scenarios":0.3553072626,
        "MMLU_nutrition":0.6111111111,
        "MMLU_philosophy":0.6205787781,
        "MMLU_prehistory":0.6358024691,
        "MMLU_professional_accounting":0.3971631206,
        "MMLU_professional_law":0.4185136897,
        "MMLU_professional_medicine":0.5294117647,
        "MMLU_professional_psychology":0.5408496732,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.6081632653,
        "MMLU_sociology":0.7412935323,
        "MMLU_us_foreign_policy":0.81,
        "MMLU_virology":0.4337349398,
        "MMLU_world_religions":0.7602339181
    },
    {
        "Model":"Llama-2-13B-Instruct-v0.2",
        "URL":"https:\/\/huggingface.co\/dfurman\/Llama-2-13B-Instruct-v0.2",
        "full_model_name":"dfurman\/Llama-2-13B-Instruct-v0.2",
        "Parameters":13.0,
        "MMLU_average":0.5545772685,
        "arc:challenge|25":0.5639931741,
        "hellaswag|10":0.6139215296,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.5185185185,
        "MMLU_astronomy":0.5460526316,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.6037735849,
        "MMLU_college_biology":0.5555555556,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.51,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.5028901734,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4510638298,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.5103448276,
        "MMLU_elementary_mathematics":0.328042328,
        "MMLU_formal_logic":0.3333333333,
        "MMLU_global_facts":0.4,
        "MMLU_high_school_biology":0.6548387097,
        "MMLU_high_school_chemistry":0.4482758621,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.6818181818,
        "MMLU_high_school_government_and_politics":0.7875647668,
        "MMLU_high_school_macroeconomics":0.5230769231,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.5882352941,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.7357798165,
        "MMLU_high_school_statistics":0.375,
        "MMLU_high_school_us_history":0.7892156863,
        "MMLU_high_school_world_history":0.746835443,
        "MMLU_human_aging":0.6547085202,
        "MMLU_human_sexuality":0.6259541985,
        "MMLU_international_law":0.7603305785,
        "MMLU_jurisprudence":0.6851851852,
        "MMLU_logical_fallacies":0.6319018405,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.7087378641,
        "MMLU_marketing":0.8076923077,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.7522349936,
        "MMLU_moral_disputes":0.6242774566,
        "MMLU_moral_scenarios":0.3664804469,
        "MMLU_nutrition":0.5784313725,
        "MMLU_philosophy":0.6302250804,
        "MMLU_prehistory":0.6265432099,
        "MMLU_professional_accounting":0.3936170213,
        "MMLU_professional_law":0.4204693611,
        "MMLU_professional_medicine":0.5220588235,
        "MMLU_professional_psychology":0.5506535948,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.6,
        "MMLU_sociology":0.7313432836,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.7777777778
    },
    {
        "Model":"llama2-13b-Chinese-chat",
        "URL":"https:\/\/huggingface.co\/shareAI\/llama2-13b-Chinese-chat",
        "full_model_name":"shareAI\/llama2-13b-Chinese-chat",
        "Parameters":13.0,
        "MMLU_average":0.5545052056,
        "arc:challenge|25":0.5605802048,
        "hellaswag|10":0.6160127465,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.5526315789,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.6264150943,
        "MMLU_college_biology":0.5833333333,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.49,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.5202312139,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.73,
        "MMLU_conceptual_physics":0.4042553191,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.3201058201,
        "MMLU_formal_logic":0.3015873016,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.6451612903,
        "MMLU_high_school_chemistry":0.4630541872,
        "MMLU_high_school_computer_science":0.56,
        "MMLU_high_school_european_history":0.6666666667,
        "MMLU_high_school_geography":0.6818181818,
        "MMLU_high_school_government_and_politics":0.8082901554,
        "MMLU_high_school_macroeconomics":0.4820512821,
        "MMLU_high_school_mathematics":0.3074074074,
        "MMLU_high_school_microeconomics":0.5588235294,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.7467889908,
        "MMLU_high_school_statistics":0.4074074074,
        "MMLU_high_school_us_history":0.7745098039,
        "MMLU_high_school_world_history":0.7299578059,
        "MMLU_human_aging":0.6591928251,
        "MMLU_human_sexuality":0.5954198473,
        "MMLU_international_law":0.7520661157,
        "MMLU_jurisprudence":0.712962963,
        "MMLU_logical_fallacies":0.6503067485,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.7948717949,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.7471264368,
        "MMLU_moral_disputes":0.6329479769,
        "MMLU_moral_scenarios":0.3932960894,
        "MMLU_nutrition":0.6241830065,
        "MMLU_philosophy":0.6463022508,
        "MMLU_prehistory":0.6358024691,
        "MMLU_professional_accounting":0.4255319149,
        "MMLU_professional_law":0.4132985658,
        "MMLU_professional_medicine":0.5036764706,
        "MMLU_professional_psychology":0.5522875817,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.6408163265,
        "MMLU_sociology":0.7313432836,
        "MMLU_us_foreign_policy":0.86,
        "MMLU_virology":0.4638554217,
        "MMLU_world_religions":0.7543859649
    },
    {
        "Model":"falcon-40b-instruct",
        "URL":"https:\/\/huggingface.co\/tiiuae\/falcon-40b-instruct",
        "full_model_name":"tiiuae\/falcon-40b-instruct",
        "Parameters":40.0,
        "MMLU_average":0.5544786128,
        "arc:challenge|25":0.5827645051,
        "hellaswag|10":0.6461860187,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.5407407407,
        "MMLU_astronomy":0.5526315789,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.5924528302,
        "MMLU_college_biology":0.6111111111,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.5317919075,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.59,
        "MMLU_conceptual_physics":0.4340425532,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.4965517241,
        "MMLU_elementary_mathematics":0.3333333333,
        "MMLU_formal_logic":0.3333333333,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.6516129032,
        "MMLU_high_school_chemistry":0.4236453202,
        "MMLU_high_school_computer_science":0.53,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.7424242424,
        "MMLU_high_school_government_and_politics":0.7875647668,
        "MMLU_high_school_macroeconomics":0.5512820513,
        "MMLU_high_school_mathematics":0.3333333333,
        "MMLU_high_school_microeconomics":0.5714285714,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7541284404,
        "MMLU_high_school_statistics":0.4305555556,
        "MMLU_high_school_us_history":0.7107843137,
        "MMLU_high_school_world_history":0.6793248945,
        "MMLU_human_aging":0.6591928251,
        "MMLU_human_sexuality":0.6717557252,
        "MMLU_international_law":0.694214876,
        "MMLU_jurisprudence":0.6481481481,
        "MMLU_logical_fallacies":0.6687116564,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.7572815534,
        "MMLU_marketing":0.8205128205,
        "MMLU_medical_genetics":0.64,
        "MMLU_miscellaneous":0.7420178799,
        "MMLU_moral_disputes":0.612716763,
        "MMLU_moral_scenarios":0.3206703911,
        "MMLU_nutrition":0.6274509804,
        "MMLU_philosophy":0.6398713826,
        "MMLU_prehistory":0.6203703704,
        "MMLU_professional_accounting":0.390070922,
        "MMLU_professional_law":0.4022164276,
        "MMLU_professional_medicine":0.5808823529,
        "MMLU_professional_psychology":0.5196078431,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.6326530612,
        "MMLU_sociology":0.7661691542,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4939759036,
        "MMLU_world_religions":0.7777777778
    },
    {
        "Model":"airoboros-c34b-2.2.1",
        "URL":"https:\/\/huggingface.co\/jondurbin\/airoboros-c34b-2.2.1",
        "full_model_name":"jondurbin\/airoboros-c34b-2.2.1",
        "Parameters":34.0,
        "MMLU_average":0.5543126632,
        "arc:challenge|25":0.5179180887,
        "hellaswag|10":0.5763792073,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.5723684211,
        "MMLU_business_ethics":0.6,
        "MMLU_clinical_knowledge":0.5358490566,
        "MMLU_college_biology":0.5694444444,
        "MMLU_college_chemistry":0.46,
        "MMLU_college_computer_science":0.54,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.4624277457,
        "MMLU_college_physics":0.3431372549,
        "MMLU_computer_security":0.72,
        "MMLU_conceptual_physics":0.485106383,
        "MMLU_econometrics":0.4385964912,
        "MMLU_electrical_engineering":0.5724137931,
        "MMLU_elementary_mathematics":0.4047619048,
        "MMLU_formal_logic":0.4365079365,
        "MMLU_global_facts":0.29,
        "MMLU_high_school_biology":0.6322580645,
        "MMLU_high_school_chemistry":0.4187192118,
        "MMLU_high_school_computer_science":0.74,
        "MMLU_high_school_european_history":0.6909090909,
        "MMLU_high_school_geography":0.7121212121,
        "MMLU_high_school_government_and_politics":0.7564766839,
        "MMLU_high_school_macroeconomics":0.5128205128,
        "MMLU_high_school_mathematics":0.3444444444,
        "MMLU_high_school_microeconomics":0.5756302521,
        "MMLU_high_school_physics":0.3907284768,
        "MMLU_high_school_psychology":0.6935779817,
        "MMLU_high_school_statistics":0.4537037037,
        "MMLU_high_school_us_history":0.7254901961,
        "MMLU_high_school_world_history":0.7426160338,
        "MMLU_human_aging":0.5560538117,
        "MMLU_human_sexuality":0.5190839695,
        "MMLU_international_law":0.7024793388,
        "MMLU_jurisprudence":0.712962963,
        "MMLU_logical_fallacies":0.6993865031,
        "MMLU_machine_learning":0.4553571429,
        "MMLU_management":0.6990291262,
        "MMLU_marketing":0.7863247863,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.6845466156,
        "MMLU_moral_disputes":0.6069364162,
        "MMLU_moral_scenarios":0.3229050279,
        "MMLU_nutrition":0.5849673203,
        "MMLU_philosophy":0.6012861736,
        "MMLU_prehistory":0.5987654321,
        "MMLU_professional_accounting":0.390070922,
        "MMLU_professional_law":0.387874837,
        "MMLU_professional_medicine":0.4742647059,
        "MMLU_professional_psychology":0.4934640523,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.6285714286,
        "MMLU_sociology":0.7462686567,
        "MMLU_us_foreign_policy":0.72,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.701754386
    },
    {
        "Model":"ReMM-Mistral-13B",
        "URL":"https:\/\/huggingface.co\/Undi95\/ReMM-Mistral-13B",
        "full_model_name":"Undi95\/ReMM-Mistral-13B",
        "Parameters":13.0,
        "MMLU_average":0.5543049447,
        "arc:challenge|25":0.5895904437,
        "hellaswag|10":0.6428998208,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.5328947368,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.5660377358,
        "MMLU_college_biology":0.5972222222,
        "MMLU_college_chemistry":0.33,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.5202312139,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4936170213,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.4965517241,
        "MMLU_elementary_mathematics":0.3439153439,
        "MMLU_formal_logic":0.3015873016,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.6419354839,
        "MMLU_high_school_chemistry":0.4137931034,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.6919191919,
        "MMLU_high_school_government_and_politics":0.7772020725,
        "MMLU_high_school_macroeconomics":0.5128205128,
        "MMLU_high_school_mathematics":0.3296296296,
        "MMLU_high_school_microeconomics":0.5924369748,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.7229357798,
        "MMLU_high_school_statistics":0.3703703704,
        "MMLU_high_school_us_history":0.7598039216,
        "MMLU_high_school_world_history":0.7552742616,
        "MMLU_human_aging":0.6860986547,
        "MMLU_human_sexuality":0.6488549618,
        "MMLU_international_law":0.7107438017,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.6748466258,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.7948717949,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.7611749681,
        "MMLU_moral_disputes":0.6242774566,
        "MMLU_moral_scenarios":0.3932960894,
        "MMLU_nutrition":0.6176470588,
        "MMLU_philosophy":0.6366559486,
        "MMLU_prehistory":0.6172839506,
        "MMLU_professional_accounting":0.4219858156,
        "MMLU_professional_law":0.4224250326,
        "MMLU_professional_medicine":0.5183823529,
        "MMLU_professional_psychology":0.5669934641,
        "MMLU_public_relations":0.6909090909,
        "MMLU_security_studies":0.6204081633,
        "MMLU_sociology":0.6666666667,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.7602339181
    },
    {
        "Model":"Athena-Platypus2-13B-QLora-0.80-epoch",
        "URL":"https:\/\/huggingface.co\/TFLai\/Athena-Platypus2-13B-QLora-0.80-epoch",
        "full_model_name":"TFLai\/Athena-Platypus2-13B-QLora-0.80-epoch",
        "Parameters":13.0,
        "MMLU_average":0.5542928634,
        "arc:challenge|25":0.5332764505,
        "hellaswag|10":0.6037641904,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.5037037037,
        "MMLU_astronomy":0.6052631579,
        "MMLU_business_ethics":0.6,
        "MMLU_clinical_knowledge":0.5509433962,
        "MMLU_college_biology":0.6388888889,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.4913294798,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.64,
        "MMLU_conceptual_physics":0.4425531915,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.3518518519,
        "MMLU_formal_logic":0.380952381,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.5774193548,
        "MMLU_high_school_chemistry":0.3891625616,
        "MMLU_high_school_computer_science":0.59,
        "MMLU_high_school_european_history":0.7212121212,
        "MMLU_high_school_geography":0.7575757576,
        "MMLU_high_school_government_and_politics":0.8393782383,
        "MMLU_high_school_macroeconomics":0.6102564103,
        "MMLU_high_school_mathematics":0.3407407407,
        "MMLU_high_school_microeconomics":0.512605042,
        "MMLU_high_school_physics":0.357615894,
        "MMLU_high_school_psychology":0.7651376147,
        "MMLU_high_school_statistics":0.4166666667,
        "MMLU_high_school_us_history":0.7941176471,
        "MMLU_high_school_world_history":0.7679324895,
        "MMLU_human_aging":0.6412556054,
        "MMLU_human_sexuality":0.5648854962,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.7037037037,
        "MMLU_logical_fallacies":0.6625766871,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.7905982906,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.7688378033,
        "MMLU_moral_disputes":0.6098265896,
        "MMLU_moral_scenarios":0.4592178771,
        "MMLU_nutrition":0.5751633987,
        "MMLU_philosophy":0.61414791,
        "MMLU_prehistory":0.6604938272,
        "MMLU_professional_accounting":0.4574468085,
        "MMLU_professional_law":0.4537157757,
        "MMLU_professional_medicine":0.5735294118,
        "MMLU_professional_psychology":0.5882352941,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.5795918367,
        "MMLU_sociology":0.4527363184,
        "MMLU_us_foreign_policy":0.79,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"Redmond-Puffin-13B",
        "URL":"https:\/\/huggingface.co\/NousResearch\/Redmond-Puffin-13B",
        "full_model_name":"NousResearch\/Redmond-Puffin-13B",
        "Parameters":13.0,
        "MMLU_average":0.5535891025,
        "arc:challenge|25":0.5674061433,
        "hellaswag|10":0.627265485,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.5460526316,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.5849056604,
        "MMLU_college_biology":0.5902777778,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.5260115607,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.4255319149,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.3148148148,
        "MMLU_formal_logic":0.373015873,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.6741935484,
        "MMLU_high_school_chemistry":0.3842364532,
        "MMLU_high_school_computer_science":0.57,
        "MMLU_high_school_european_history":0.6848484848,
        "MMLU_high_school_geography":0.6717171717,
        "MMLU_high_school_government_and_politics":0.829015544,
        "MMLU_high_school_macroeconomics":0.4974358974,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.5462184874,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.7614678899,
        "MMLU_high_school_statistics":0.4259259259,
        "MMLU_high_school_us_history":0.7647058824,
        "MMLU_high_school_world_history":0.7552742616,
        "MMLU_human_aging":0.6591928251,
        "MMLU_human_sexuality":0.6335877863,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.6851851852,
        "MMLU_logical_fallacies":0.6687116564,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.7572815534,
        "MMLU_marketing":0.7948717949,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.7535121328,
        "MMLU_moral_disputes":0.6300578035,
        "MMLU_moral_scenarios":0.2837988827,
        "MMLU_nutrition":0.6470588235,
        "MMLU_philosophy":0.6591639871,
        "MMLU_prehistory":0.6574074074,
        "MMLU_professional_accounting":0.3936170213,
        "MMLU_professional_law":0.425684485,
        "MMLU_professional_medicine":0.5404411765,
        "MMLU_professional_psychology":0.5555555556,
        "MMLU_public_relations":0.6,
        "MMLU_security_studies":0.6489795918,
        "MMLU_sociology":0.7462686567,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4819277108,
        "MMLU_world_religions":0.7660818713
    },
    {
        "Model":"llama-2-13b-FINETUNE2_TEST_2.2w",
        "URL":"https:\/\/huggingface.co\/CHIH-HUNG\/llama-2-13b-FINETUNE2_TEST_2.2w",
        "full_model_name":"CHIH-HUNG\/llama-2-13b-FINETUNE2_TEST_2.2w",
        "Parameters":13.0,
        "MMLU_average":0.553549502,
        "arc:challenge|25":0.5298634812,
        "hellaswag|10":0.6190001992,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5657894737,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.6264150943,
        "MMLU_college_biology":0.5902777778,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.39,
        "MMLU_college_medicine":0.4913294798,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.429787234,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.3201058201,
        "MMLU_formal_logic":0.3333333333,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.6290322581,
        "MMLU_high_school_chemistry":0.4334975369,
        "MMLU_high_school_computer_science":0.57,
        "MMLU_high_school_european_history":0.6666666667,
        "MMLU_high_school_geography":0.7070707071,
        "MMLU_high_school_government_and_politics":0.8031088083,
        "MMLU_high_school_macroeconomics":0.5076923077,
        "MMLU_high_school_mathematics":0.3148148148,
        "MMLU_high_school_microeconomics":0.5504201681,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.7376146789,
        "MMLU_high_school_statistics":0.412037037,
        "MMLU_high_school_us_history":0.7401960784,
        "MMLU_high_school_world_history":0.7510548523,
        "MMLU_human_aging":0.668161435,
        "MMLU_human_sexuality":0.6183206107,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.712962963,
        "MMLU_logical_fallacies":0.6748466258,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.7475728155,
        "MMLU_marketing":0.8034188034,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.7560664112,
        "MMLU_moral_disputes":0.6589595376,
        "MMLU_moral_scenarios":0.4089385475,
        "MMLU_nutrition":0.5718954248,
        "MMLU_philosophy":0.6334405145,
        "MMLU_prehistory":0.6388888889,
        "MMLU_professional_accounting":0.4113475177,
        "MMLU_professional_law":0.4276401565,
        "MMLU_professional_medicine":0.5625,
        "MMLU_professional_psychology":0.5718954248,
        "MMLU_public_relations":0.6272727273,
        "MMLU_security_studies":0.612244898,
        "MMLU_sociology":0.7313432836,
        "MMLU_us_foreign_policy":0.79,
        "MMLU_virology":0.4397590361,
        "MMLU_world_religions":0.7953216374
    },
    {
        "Model":"MythoMax-L2-13b",
        "URL":"https:\/\/huggingface.co\/Gryphe\/MythoMax-L2-13b",
        "full_model_name":"Gryphe\/MythoMax-L2-13b",
        "Parameters":13.0,
        "MMLU_average":0.5533243347,
        "arc:challenge|25":0.5827645051,
        "hellaswag|10":0.6423023302,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.5259259259,
        "MMLU_astronomy":0.5197368421,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.5471698113,
        "MMLU_college_biology":0.5902777778,
        "MMLU_college_chemistry":0.33,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5202312139,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4765957447,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.3095238095,
        "MMLU_formal_logic":0.3333333333,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.635483871,
        "MMLU_high_school_chemistry":0.4088669951,
        "MMLU_high_school_computer_science":0.56,
        "MMLU_high_school_european_history":0.6848484848,
        "MMLU_high_school_geography":0.6868686869,
        "MMLU_high_school_government_and_politics":0.7720207254,
        "MMLU_high_school_macroeconomics":0.5128205128,
        "MMLU_high_school_mathematics":0.3259259259,
        "MMLU_high_school_microeconomics":0.5756302521,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.719266055,
        "MMLU_high_school_statistics":0.3842592593,
        "MMLU_high_school_us_history":0.7647058824,
        "MMLU_high_school_world_history":0.7594936709,
        "MMLU_human_aging":0.6860986547,
        "MMLU_human_sexuality":0.6259541985,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.6748466258,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.7991452991,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7688378033,
        "MMLU_moral_disputes":0.6242774566,
        "MMLU_moral_scenarios":0.4391061453,
        "MMLU_nutrition":0.591503268,
        "MMLU_philosophy":0.6430868167,
        "MMLU_prehistory":0.6265432099,
        "MMLU_professional_accounting":0.4007092199,
        "MMLU_professional_law":0.426988266,
        "MMLU_professional_medicine":0.5036764706,
        "MMLU_professional_psychology":0.5751633987,
        "MMLU_public_relations":0.6727272727,
        "MMLU_security_studies":0.6204081633,
        "MMLU_sociology":0.6915422886,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4638554217,
        "MMLU_world_religions":0.7777777778
    },
    {
        "Model":"Huginn-13b-v1.2",
        "URL":"https:\/\/huggingface.co\/The-Face-Of-Goonery\/Huginn-13b-v1.2",
        "full_model_name":"The-Face-Of-Goonery\/Huginn-13b-v1.2",
        "Parameters":13.0,
        "MMLU_average":0.5533243347,
        "arc:challenge|25":0.5827645051,
        "hellaswag|10":0.6423023302,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.5259259259,
        "MMLU_astronomy":0.5197368421,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.5471698113,
        "MMLU_college_biology":0.5902777778,
        "MMLU_college_chemistry":0.33,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5202312139,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4765957447,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.3095238095,
        "MMLU_formal_logic":0.3333333333,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.635483871,
        "MMLU_high_school_chemistry":0.4088669951,
        "MMLU_high_school_computer_science":0.56,
        "MMLU_high_school_european_history":0.6848484848,
        "MMLU_high_school_geography":0.6868686869,
        "MMLU_high_school_government_and_politics":0.7720207254,
        "MMLU_high_school_macroeconomics":0.5128205128,
        "MMLU_high_school_mathematics":0.3259259259,
        "MMLU_high_school_microeconomics":0.5756302521,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.719266055,
        "MMLU_high_school_statistics":0.3842592593,
        "MMLU_high_school_us_history":0.7647058824,
        "MMLU_high_school_world_history":0.7594936709,
        "MMLU_human_aging":0.6860986547,
        "MMLU_human_sexuality":0.6259541985,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.6748466258,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.7991452991,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7688378033,
        "MMLU_moral_disputes":0.6242774566,
        "MMLU_moral_scenarios":0.4391061453,
        "MMLU_nutrition":0.591503268,
        "MMLU_philosophy":0.6430868167,
        "MMLU_prehistory":0.6265432099,
        "MMLU_professional_accounting":0.4007092199,
        "MMLU_professional_law":0.426988266,
        "MMLU_professional_medicine":0.5036764706,
        "MMLU_professional_psychology":0.5751633987,
        "MMLU_public_relations":0.6727272727,
        "MMLU_security_studies":0.6204081633,
        "MMLU_sociology":0.6915422886,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4638554217,
        "MMLU_world_religions":0.7777777778
    },
    {
        "Model":"llama-2-13b-FINETUNE4_3.8w-r4-q_k_v_o_gate_up_down",
        "URL":"https:\/\/huggingface.co\/CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r4-q_k_v_o_gate_up_down",
        "full_model_name":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r4-q_k_v_o_gate_up_down",
        "Parameters":13.0,
        "MMLU_average":0.5529796872,
        "arc:challenge|25":0.5187713311,
        "hellaswag|10":0.6044612627,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.5657894737,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.5962264151,
        "MMLU_college_biology":0.6041666667,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.4624277457,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.72,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4551724138,
        "MMLU_elementary_mathematics":0.3068783069,
        "MMLU_formal_logic":0.373015873,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.6741935484,
        "MMLU_high_school_chemistry":0.4532019704,
        "MMLU_high_school_computer_science":0.5,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.6919191919,
        "MMLU_high_school_government_and_politics":0.8031088083,
        "MMLU_high_school_macroeconomics":0.5102564103,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.5882352941,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.7724770642,
        "MMLU_high_school_statistics":0.4444444444,
        "MMLU_high_school_us_history":0.7549019608,
        "MMLU_high_school_world_history":0.7426160338,
        "MMLU_human_aging":0.668161435,
        "MMLU_human_sexuality":0.641221374,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.7314814815,
        "MMLU_logical_fallacies":0.6687116564,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.8247863248,
        "MMLU_medical_genetics":0.63,
        "MMLU_miscellaneous":0.754789272,
        "MMLU_moral_disputes":0.6589595376,
        "MMLU_moral_scenarios":0.3351955307,
        "MMLU_nutrition":0.5882352941,
        "MMLU_philosophy":0.6398713826,
        "MMLU_prehistory":0.6141975309,
        "MMLU_professional_accounting":0.4574468085,
        "MMLU_professional_law":0.4374185137,
        "MMLU_professional_medicine":0.5477941176,
        "MMLU_professional_psychology":0.5539215686,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.5183673469,
        "MMLU_sociology":0.7064676617,
        "MMLU_us_foreign_policy":0.71,
        "MMLU_virology":0.4337349398,
        "MMLU_world_religions":0.8011695906
    },
    {
        "Model":"TekniumAiroboros-Nebula-7B",
        "URL":"https:\/\/huggingface.co\/Weyaxi\/TekniumAiroboros-Nebula-7B",
        "full_model_name":"Weyaxi\/TekniumAiroboros-Nebula-7B",
        "Parameters":7.0,
        "MMLU_average":0.5525070839,
        "arc:challenge|25":0.5264505119,
        "hellaswag|10":0.6211909978,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.5259259259,
        "MMLU_astronomy":0.5131578947,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.6075471698,
        "MMLU_college_biology":0.6527777778,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.34,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.5202312139,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.65,
        "MMLU_conceptual_physics":0.514893617,
        "MMLU_econometrics":0.4473684211,
        "MMLU_electrical_engineering":0.4896551724,
        "MMLU_elementary_mathematics":0.3677248677,
        "MMLU_formal_logic":0.4047619048,
        "MMLU_global_facts":0.27,
        "MMLU_high_school_biology":0.6548387097,
        "MMLU_high_school_chemistry":0.3793103448,
        "MMLU_high_school_computer_science":0.53,
        "MMLU_high_school_european_history":0.7212121212,
        "MMLU_high_school_geography":0.7272727273,
        "MMLU_high_school_government_and_politics":0.7823834197,
        "MMLU_high_school_macroeconomics":0.5333333333,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.5588235294,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.7688073394,
        "MMLU_high_school_statistics":0.337962963,
        "MMLU_high_school_us_history":0.7401960784,
        "MMLU_high_school_world_history":0.7088607595,
        "MMLU_human_aging":0.6367713004,
        "MMLU_human_sexuality":0.6259541985,
        "MMLU_international_law":0.7190082645,
        "MMLU_jurisprudence":0.6759259259,
        "MMLU_logical_fallacies":0.6809815951,
        "MMLU_machine_learning":0.4375,
        "MMLU_management":0.7087378641,
        "MMLU_marketing":0.858974359,
        "MMLU_medical_genetics":0.63,
        "MMLU_miscellaneous":0.7560664112,
        "MMLU_moral_disputes":0.5867052023,
        "MMLU_moral_scenarios":0.3340782123,
        "MMLU_nutrition":0.6045751634,
        "MMLU_philosophy":0.6237942122,
        "MMLU_prehistory":0.6388888889,
        "MMLU_professional_accounting":0.3936170213,
        "MMLU_professional_law":0.4341590613,
        "MMLU_professional_medicine":0.4926470588,
        "MMLU_professional_psychology":0.5653594771,
        "MMLU_public_relations":0.6272727273,
        "MMLU_security_studies":0.5183673469,
        "MMLU_sociology":0.7661691542,
        "MMLU_us_foreign_policy":0.75,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.7485380117
    },
    {
        "Model":"llama-2-13b-platypus-vicuna-wizard",
        "URL":"https:\/\/huggingface.co\/pe-nlp\/llama-2-13b-platypus-vicuna-wizard",
        "full_model_name":"pe-nlp\/llama-2-13b-platypus-vicuna-wizard",
        "Parameters":13.0,
        "MMLU_average":0.5521066997,
        "arc:challenge|25":0.5648464164,
        "hellaswag|10":0.6149173471,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.5185185185,
        "MMLU_astronomy":0.5855263158,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.6,
        "MMLU_college_biology":0.5694444444,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.5317919075,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.370212766,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.5103448276,
        "MMLU_elementary_mathematics":0.3439153439,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.6483870968,
        "MMLU_high_school_chemistry":0.4729064039,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.6787878788,
        "MMLU_high_school_geography":0.7171717172,
        "MMLU_high_school_government_and_politics":0.792746114,
        "MMLU_high_school_macroeconomics":0.5564102564,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.5546218487,
        "MMLU_high_school_physics":0.3708609272,
        "MMLU_high_school_psychology":0.7266055046,
        "MMLU_high_school_statistics":0.4861111111,
        "MMLU_high_school_us_history":0.7450980392,
        "MMLU_high_school_world_history":0.7046413502,
        "MMLU_human_aging":0.6591928251,
        "MMLU_human_sexuality":0.6335877863,
        "MMLU_international_law":0.6694214876,
        "MMLU_jurisprudence":0.75,
        "MMLU_logical_fallacies":0.6687116564,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.7572815534,
        "MMLU_marketing":0.7735042735,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.7445721584,
        "MMLU_moral_disputes":0.6502890173,
        "MMLU_moral_scenarios":0.2502793296,
        "MMLU_nutrition":0.6274509804,
        "MMLU_philosophy":0.6398713826,
        "MMLU_prehistory":0.5956790123,
        "MMLU_professional_accounting":0.4255319149,
        "MMLU_professional_law":0.4165580183,
        "MMLU_professional_medicine":0.5330882353,
        "MMLU_professional_psychology":0.5392156863,
        "MMLU_public_relations":0.6,
        "MMLU_security_studies":0.6081632653,
        "MMLU_sociology":0.736318408,
        "MMLU_us_foreign_policy":0.81,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.730994152
    },
    {
        "Model":"trurl-2-13b-academic",
        "URL":"https:\/\/huggingface.co\/Voicelab\/trurl-2-13b-academic",
        "full_model_name":"Voicelab\/trurl-2-13b-academic",
        "Parameters":13.0,
        "MMLU_average":0.5519982807,
        "arc:challenge|25":0.5392491468,
        "hellaswag|10":0.5957976499,
        "MMLU_abstract_algebra":0.39,
        "MMLU_anatomy":0.5185185185,
        "MMLU_astronomy":0.5657894737,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.5698113208,
        "MMLU_college_biology":0.5972222222,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.4624277457,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.4127659574,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.4896551724,
        "MMLU_elementary_mathematics":0.3544973545,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.28,
        "MMLU_high_school_biology":0.6419354839,
        "MMLU_high_school_chemistry":0.4433497537,
        "MMLU_high_school_computer_science":0.56,
        "MMLU_high_school_european_history":0.6787878788,
        "MMLU_high_school_geography":0.6666666667,
        "MMLU_high_school_government_and_politics":0.8341968912,
        "MMLU_high_school_macroeconomics":0.5307692308,
        "MMLU_high_school_mathematics":0.3222222222,
        "MMLU_high_school_microeconomics":0.5588235294,
        "MMLU_high_school_physics":0.3642384106,
        "MMLU_high_school_psychology":0.7412844037,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.7107843137,
        "MMLU_high_school_world_history":0.746835443,
        "MMLU_human_aging":0.6233183857,
        "MMLU_human_sexuality":0.6641221374,
        "MMLU_international_law":0.7190082645,
        "MMLU_jurisprudence":0.6851851852,
        "MMLU_logical_fallacies":0.6748466258,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.7863247863,
        "MMLU_medical_genetics":0.62,
        "MMLU_miscellaneous":0.7190293742,
        "MMLU_moral_disputes":0.6098265896,
        "MMLU_moral_scenarios":0.3765363128,
        "MMLU_nutrition":0.6078431373,
        "MMLU_philosophy":0.5819935691,
        "MMLU_prehistory":0.5956790123,
        "MMLU_professional_accounting":0.4007092199,
        "MMLU_professional_law":0.4002607562,
        "MMLU_professional_medicine":0.4779411765,
        "MMLU_professional_psychology":0.5441176471,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.6448979592,
        "MMLU_sociology":0.7810945274,
        "MMLU_us_foreign_policy":0.8,
        "MMLU_virology":0.4819277108,
        "MMLU_world_religions":0.730994152
    },
    {
        "Model":"airophin-13b-pntk-16k-fp16",
        "URL":"https:\/\/huggingface.co\/bhenrym14\/airophin-13b-pntk-16k-fp16",
        "full_model_name":"bhenrym14\/airophin-13b-pntk-16k-fp16",
        "Parameters":13.0,
        "MMLU_average":0.551889915,
        "arc:challenge|25":0.5742320819,
        "hellaswag|10":0.6348336985,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.6118421053,
        "MMLU_business_ethics":0.5,
        "MMLU_clinical_knowledge":0.5962264151,
        "MMLU_college_biology":0.5972222222,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.4855491329,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.4382978723,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.5379310345,
        "MMLU_elementary_mathematics":0.3306878307,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.6451612903,
        "MMLU_high_school_chemistry":0.4088669951,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.6515151515,
        "MMLU_high_school_government_and_politics":0.8031088083,
        "MMLU_high_school_macroeconomics":0.4923076923,
        "MMLU_high_school_mathematics":0.3074074074,
        "MMLU_high_school_microeconomics":0.5336134454,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.7174311927,
        "MMLU_high_school_statistics":0.3657407407,
        "MMLU_high_school_us_history":0.7549019608,
        "MMLU_high_school_world_history":0.7130801688,
        "MMLU_human_aging":0.6502242152,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.6944444444,
        "MMLU_logical_fallacies":0.6748466258,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.8076923077,
        "MMLU_medical_genetics":0.62,
        "MMLU_miscellaneous":0.7496807152,
        "MMLU_moral_disputes":0.6300578035,
        "MMLU_moral_scenarios":0.4189944134,
        "MMLU_nutrition":0.591503268,
        "MMLU_philosophy":0.6334405145,
        "MMLU_prehistory":0.6543209877,
        "MMLU_professional_accounting":0.414893617,
        "MMLU_professional_law":0.4139504563,
        "MMLU_professional_medicine":0.5,
        "MMLU_professional_psychology":0.5604575163,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.6163265306,
        "MMLU_sociology":0.7213930348,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4879518072,
        "MMLU_world_religions":0.7660818713
    },
    {
        "Model":"PuddleJumper-Platypus2-13B-QLoRA-0.80-epoch",
        "URL":"https:\/\/huggingface.co\/TFLai\/PuddleJumper-Platypus2-13B-QLoRA-0.80-epoch",
        "full_model_name":"TFLai\/PuddleJumper-Platypus2-13B-QLoRA-0.80-epoch",
        "Parameters":13.0,
        "MMLU_average":0.5514599987,
        "arc:challenge|25":0.5119453925,
        "hellaswag|10":0.5905198168,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.5037037037,
        "MMLU_astronomy":0.6118421053,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.5509433962,
        "MMLU_college_biology":0.6111111111,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.37,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.4797687861,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.67,
        "MMLU_conceptual_physics":0.4255319149,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.4413793103,
        "MMLU_elementary_mathematics":0.3597883598,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.5774193548,
        "MMLU_high_school_chemistry":0.3842364532,
        "MMLU_high_school_computer_science":0.61,
        "MMLU_high_school_european_history":0.7090909091,
        "MMLU_high_school_geography":0.7525252525,
        "MMLU_high_school_government_and_politics":0.8186528497,
        "MMLU_high_school_macroeconomics":0.6128205128,
        "MMLU_high_school_mathematics":0.3259259259,
        "MMLU_high_school_microeconomics":0.512605042,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.7706422018,
        "MMLU_high_school_statistics":0.4074074074,
        "MMLU_high_school_us_history":0.7745098039,
        "MMLU_high_school_world_history":0.7637130802,
        "MMLU_human_aging":0.6367713004,
        "MMLU_human_sexuality":0.572519084,
        "MMLU_international_law":0.7024793388,
        "MMLU_jurisprudence":0.6388888889,
        "MMLU_logical_fallacies":0.6503067485,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.7087378641,
        "MMLU_marketing":0.7863247863,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.7650063857,
        "MMLU_moral_disputes":0.598265896,
        "MMLU_moral_scenarios":0.4368715084,
        "MMLU_nutrition":0.568627451,
        "MMLU_philosophy":0.6173633441,
        "MMLU_prehistory":0.6450617284,
        "MMLU_professional_accounting":0.475177305,
        "MMLU_professional_law":0.4439374185,
        "MMLU_professional_medicine":0.5661764706,
        "MMLU_professional_psychology":0.5833333333,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.5795918367,
        "MMLU_sociology":0.5621890547,
        "MMLU_us_foreign_policy":0.79,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.783625731
    },
    {
        "Model":"WizardLM-13B-V1.2-PL-lora_unload",
        "URL":"https:\/\/huggingface.co\/Lajonbot\/WizardLM-13B-V1.2-PL-lora_unload",
        "full_model_name":"Lajonbot\/WizardLM-13B-V1.2-PL-lora_unload",
        "Parameters":13.0,
        "MMLU_average":0.5514585248,
        "arc:challenge|25":0.5358361775,
        "hellaswag|10":0.6145190201,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.5789473684,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.5886792453,
        "MMLU_college_biology":0.5277777778,
        "MMLU_college_chemistry":0.45,
        "MMLU_college_computer_science":0.51,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.4855491329,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.5379310345,
        "MMLU_elementary_mathematics":0.3571428571,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.664516129,
        "MMLU_high_school_chemistry":0.4532019704,
        "MMLU_high_school_computer_science":0.59,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.6919191919,
        "MMLU_high_school_government_and_politics":0.7720207254,
        "MMLU_high_school_macroeconomics":0.5128205128,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.5336134454,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.728440367,
        "MMLU_high_school_statistics":0.4305555556,
        "MMLU_high_school_us_history":0.75,
        "MMLU_high_school_world_history":0.7130801688,
        "MMLU_human_aging":0.6367713004,
        "MMLU_human_sexuality":0.6335877863,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.7407407407,
        "MMLU_logical_fallacies":0.6809815951,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.6990291262,
        "MMLU_marketing":0.811965812,
        "MMLU_medical_genetics":0.65,
        "MMLU_miscellaneous":0.7228607918,
        "MMLU_moral_disputes":0.6445086705,
        "MMLU_moral_scenarios":0.2413407821,
        "MMLU_nutrition":0.6078431373,
        "MMLU_philosophy":0.6077170418,
        "MMLU_prehistory":0.5956790123,
        "MMLU_professional_accounting":0.4219858156,
        "MMLU_professional_law":0.4022164276,
        "MMLU_professional_medicine":0.5625,
        "MMLU_professional_psychology":0.5163398693,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.6775510204,
        "MMLU_sociology":0.7263681592,
        "MMLU_us_foreign_policy":0.79,
        "MMLU_virology":0.4638554217,
        "MMLU_world_religions":0.7485380117
    },
    {
        "Model":"Nous-Hermes-13B-Code",
        "URL":"https:\/\/huggingface.co\/Undi95\/Nous-Hermes-13B-Code",
        "full_model_name":"Undi95\/Nous-Hermes-13B-Code",
        "Parameters":13.0,
        "MMLU_average":0.5513166759,
        "arc:challenge|25":0.5793515358,
        "hellaswag|10":0.6333399721,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.5037037037,
        "MMLU_astronomy":0.5394736842,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.5773584906,
        "MMLU_college_biology":0.6388888889,
        "MMLU_college_chemistry":0.46,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.5144508671,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.65,
        "MMLU_conceptual_physics":0.4638297872,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.335978836,
        "MMLU_formal_logic":0.380952381,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.6290322581,
        "MMLU_high_school_chemistry":0.4334975369,
        "MMLU_high_school_computer_science":0.58,
        "MMLU_high_school_european_history":0.6242424242,
        "MMLU_high_school_geography":0.6868686869,
        "MMLU_high_school_government_and_politics":0.7616580311,
        "MMLU_high_school_macroeconomics":0.4846153846,
        "MMLU_high_school_mathematics":0.3222222222,
        "MMLU_high_school_microeconomics":0.5588235294,
        "MMLU_high_school_physics":0.4105960265,
        "MMLU_high_school_psychology":0.7302752294,
        "MMLU_high_school_statistics":0.4768518519,
        "MMLU_high_school_us_history":0.7647058824,
        "MMLU_high_school_world_history":0.7215189873,
        "MMLU_human_aging":0.6098654709,
        "MMLU_human_sexuality":0.5801526718,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.7037037037,
        "MMLU_logical_fallacies":0.6625766871,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.7606837607,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.7675606641,
        "MMLU_moral_disputes":0.5867052023,
        "MMLU_moral_scenarios":0.3765363128,
        "MMLU_nutrition":0.5784313725,
        "MMLU_philosophy":0.6173633441,
        "MMLU_prehistory":0.6327160494,
        "MMLU_professional_accounting":0.4042553191,
        "MMLU_professional_law":0.4152542373,
        "MMLU_professional_medicine":0.5330882353,
        "MMLU_professional_psychology":0.5441176471,
        "MMLU_public_relations":0.5818181818,
        "MMLU_security_studies":0.5836734694,
        "MMLU_sociology":0.7512437811,
        "MMLU_us_foreign_policy":0.8,
        "MMLU_virology":0.4337349398,
        "MMLU_world_religions":0.7719298246
    },
    {
        "Model":"chinese-alpaca-2-13b",
        "URL":"https:\/\/huggingface.co\/ziqingyang\/chinese-alpaca-2-13b",
        "full_model_name":"ziqingyang\/chinese-alpaca-2-13b",
        "Parameters":13.0,
        "MMLU_average":0.5510175229,
        "arc:challenge|25":0.5418088737,
        "hellaswag|10":0.5968930492,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.5855263158,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.5886792453,
        "MMLU_college_biology":0.5625,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.5,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.5260115607,
        "MMLU_college_physics":0.3529411765,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.3787234043,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.5448275862,
        "MMLU_elementary_mathematics":0.3227513228,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.664516129,
        "MMLU_high_school_chemistry":0.4482758621,
        "MMLU_high_school_computer_science":0.62,
        "MMLU_high_school_european_history":0.6606060606,
        "MMLU_high_school_geography":0.702020202,
        "MMLU_high_school_government_and_politics":0.7979274611,
        "MMLU_high_school_macroeconomics":0.5128205128,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.5798319328,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.7541284404,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.7549019608,
        "MMLU_high_school_world_history":0.7088607595,
        "MMLU_human_aging":0.6278026906,
        "MMLU_human_sexuality":0.6030534351,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.7407407407,
        "MMLU_logical_fallacies":0.6196319018,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.6990291262,
        "MMLU_marketing":0.8076923077,
        "MMLU_medical_genetics":0.53,
        "MMLU_miscellaneous":0.7522349936,
        "MMLU_moral_disputes":0.6242774566,
        "MMLU_moral_scenarios":0.3865921788,
        "MMLU_nutrition":0.5947712418,
        "MMLU_philosophy":0.6366559486,
        "MMLU_prehistory":0.6018518519,
        "MMLU_professional_accounting":0.4042553191,
        "MMLU_professional_law":0.4230769231,
        "MMLU_professional_medicine":0.4779411765,
        "MMLU_professional_psychology":0.522875817,
        "MMLU_public_relations":0.6,
        "MMLU_security_studies":0.5959183673,
        "MMLU_sociology":0.6865671642,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4638554217,
        "MMLU_world_religions":0.7543859649
    },
    {
        "Model":"MLewd-v2.4-13B",
        "URL":"https:\/\/huggingface.co\/Undi95\/MLewd-v2.4-13B",
        "full_model_name":"Undi95\/MLewd-v2.4-13B",
        "Parameters":13.0,
        "MMLU_average":0.5509813549,
        "arc:challenge|25":0.5870307167,
        "hellaswag|10":0.6404102768,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.5333333333,
        "MMLU_astronomy":0.5263157895,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.558490566,
        "MMLU_college_biology":0.5763888889,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5202312139,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.4808510638,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.5172413793,
        "MMLU_elementary_mathematics":0.3042328042,
        "MMLU_formal_logic":0.373015873,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.6290322581,
        "MMLU_high_school_chemistry":0.3891625616,
        "MMLU_high_school_computer_science":0.5,
        "MMLU_high_school_european_history":0.6848484848,
        "MMLU_high_school_geography":0.6868686869,
        "MMLU_high_school_government_and_politics":0.7823834197,
        "MMLU_high_school_macroeconomics":0.5256410256,
        "MMLU_high_school_mathematics":0.3185185185,
        "MMLU_high_school_microeconomics":0.5672268908,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.7321100917,
        "MMLU_high_school_statistics":0.3472222222,
        "MMLU_high_school_us_history":0.7647058824,
        "MMLU_high_school_world_history":0.7426160338,
        "MMLU_human_aging":0.6816143498,
        "MMLU_human_sexuality":0.6488549618,
        "MMLU_international_law":0.7438016529,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.6687116564,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.6699029126,
        "MMLU_marketing":0.7991452991,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.7573435504,
        "MMLU_moral_disputes":0.612716763,
        "MMLU_moral_scenarios":0.3765363128,
        "MMLU_nutrition":0.6045751634,
        "MMLU_philosophy":0.6334405145,
        "MMLU_prehistory":0.6419753086,
        "MMLU_professional_accounting":0.4326241135,
        "MMLU_professional_law":0.4165580183,
        "MMLU_professional_medicine":0.5073529412,
        "MMLU_professional_psychology":0.568627451,
        "MMLU_public_relations":0.6727272727,
        "MMLU_security_studies":0.6163265306,
        "MMLU_sociology":0.6965174129,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.7777777778
    },
    {
        "Model":"chronos-hermes-13b-v2",
        "URL":"https:\/\/huggingface.co\/Austism\/chronos-hermes-13b-v2",
        "full_model_name":"Austism\/chronos-hermes-13b-v2",
        "Parameters":13.0,
        "MMLU_average":0.550516144,
        "arc:challenge|25":0.5639931741,
        "hellaswag|10":0.635032862,
        "MMLU_abstract_algebra":0.38,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.5065789474,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.558490566,
        "MMLU_college_biology":0.5486111111,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.5202312139,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.67,
        "MMLU_conceptual_physics":0.4170212766,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.335978836,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.6258064516,
        "MMLU_high_school_chemistry":0.4088669951,
        "MMLU_high_school_computer_science":0.53,
        "MMLU_high_school_european_history":0.6545454545,
        "MMLU_high_school_geography":0.6767676768,
        "MMLU_high_school_government_and_politics":0.7668393782,
        "MMLU_high_school_macroeconomics":0.5153846154,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.5378151261,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.7027522936,
        "MMLU_high_school_statistics":0.4074074074,
        "MMLU_high_school_us_history":0.7450980392,
        "MMLU_high_school_world_history":0.7341772152,
        "MMLU_human_aging":0.6726457399,
        "MMLU_human_sexuality":0.5801526718,
        "MMLU_international_law":0.7438016529,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.6687116564,
        "MMLU_machine_learning":0.4375,
        "MMLU_management":0.6893203883,
        "MMLU_marketing":0.8162393162,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7496807152,
        "MMLU_moral_disputes":0.6329479769,
        "MMLU_moral_scenarios":0.4413407821,
        "MMLU_nutrition":0.5718954248,
        "MMLU_philosophy":0.6398713826,
        "MMLU_prehistory":0.6234567901,
        "MMLU_professional_accounting":0.414893617,
        "MMLU_professional_law":0.4289439374,
        "MMLU_professional_medicine":0.5147058824,
        "MMLU_professional_psychology":0.5620915033,
        "MMLU_public_relations":0.6909090909,
        "MMLU_security_studies":0.612244898,
        "MMLU_sociology":0.7213930348,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.4518072289,
        "MMLU_world_religions":0.7719298246
    },
    {
        "Model":"Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16_merged",
        "URL":"https:\/\/huggingface.co\/dhmeltzer\/Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16_merged",
        "full_model_name":"dhmeltzer\/Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16_merged",
        "Parameters":13.0,
        "MMLU_average":0.5501644278,
        "arc:challenge|25":0.5358361775,
        "hellaswag|10":0.609241187,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.5131578947,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.6150943396,
        "MMLU_college_biology":0.5625,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.5144508671,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.67,
        "MMLU_conceptual_physics":0.4468085106,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.5310344828,
        "MMLU_elementary_mathematics":0.3227513228,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.664516129,
        "MMLU_high_school_chemistry":0.4187192118,
        "MMLU_high_school_computer_science":0.56,
        "MMLU_high_school_european_history":0.6545454545,
        "MMLU_high_school_geography":0.6565656566,
        "MMLU_high_school_government_and_politics":0.8082901554,
        "MMLU_high_school_macroeconomics":0.5307692308,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.5672268908,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7614678899,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.7450980392,
        "MMLU_high_school_world_history":0.7088607595,
        "MMLU_human_aging":0.6412556054,
        "MMLU_human_sexuality":0.6488549618,
        "MMLU_international_law":0.6859504132,
        "MMLU_jurisprudence":0.6666666667,
        "MMLU_logical_fallacies":0.6748466258,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.7991452991,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.7445721584,
        "MMLU_moral_disputes":0.6300578035,
        "MMLU_moral_scenarios":0.2893854749,
        "MMLU_nutrition":0.5784313725,
        "MMLU_philosophy":0.6559485531,
        "MMLU_prehistory":0.6450617284,
        "MMLU_professional_accounting":0.4113475177,
        "MMLU_professional_law":0.4159061278,
        "MMLU_professional_medicine":0.5477941176,
        "MMLU_professional_psychology":0.5424836601,
        "MMLU_public_relations":0.6818181818,
        "MMLU_security_studies":0.6285714286,
        "MMLU_sociology":0.7313432836,
        "MMLU_us_foreign_policy":0.81,
        "MMLU_virology":0.4638554217,
        "MMLU_world_religions":0.783625731
    },
    {
        "Model":"Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16_merged",
        "URL":"https:\/\/huggingface.co\/dhmeltzer\/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16_merged",
        "full_model_name":"dhmeltzer\/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16_merged",
        "Parameters":13.0,
        "MMLU_average":0.5500154272,
        "arc:challenge|25":0.5418088737,
        "hellaswag|10":0.6116311492,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.5131578947,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.6113207547,
        "MMLU_college_biology":0.5208333333,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.5028901734,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.4468085106,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.5448275862,
        "MMLU_elementary_mathematics":0.3174603175,
        "MMLU_formal_logic":0.4047619048,
        "MMLU_global_facts":0.29,
        "MMLU_high_school_biology":0.6677419355,
        "MMLU_high_school_chemistry":0.4285714286,
        "MMLU_high_school_computer_science":0.59,
        "MMLU_high_school_european_history":0.6545454545,
        "MMLU_high_school_geography":0.6616161616,
        "MMLU_high_school_government_and_politics":0.8186528497,
        "MMLU_high_school_macroeconomics":0.5076923077,
        "MMLU_high_school_mathematics":0.2888888889,
        "MMLU_high_school_microeconomics":0.5462184874,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.7559633028,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.7254901961,
        "MMLU_high_school_world_history":0.7215189873,
        "MMLU_human_aging":0.6367713004,
        "MMLU_human_sexuality":0.6564885496,
        "MMLU_international_law":0.7107438017,
        "MMLU_jurisprudence":0.6851851852,
        "MMLU_logical_fallacies":0.6564417178,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.811965812,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.7458492976,
        "MMLU_moral_disputes":0.6416184971,
        "MMLU_moral_scenarios":0.3474860335,
        "MMLU_nutrition":0.5849673203,
        "MMLU_philosophy":0.6366559486,
        "MMLU_prehistory":0.6327160494,
        "MMLU_professional_accounting":0.4078014184,
        "MMLU_professional_law":0.4113428944,
        "MMLU_professional_medicine":0.5073529412,
        "MMLU_professional_psychology":0.5424836601,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.6448979592,
        "MMLU_sociology":0.7263681592,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.7777777778
    },
    {
        "Model":"WizardMath-13B-V1.0",
        "URL":"https:\/\/huggingface.co\/WizardLM\/WizardMath-13B-V1.0",
        "full_model_name":"WizardLM\/WizardMath-13B-V1.0",
        "Parameters":13.0,
        "MMLU_average":0.5498654149,
        "arc:challenge|25":0.5580204778,
        "hellaswag|10":0.6286596296,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.5407407407,
        "MMLU_astronomy":0.5789473684,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.5849056604,
        "MMLU_college_biology":0.5972222222,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.5202312139,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.4468085106,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.4896551724,
        "MMLU_elementary_mathematics":0.3492063492,
        "MMLU_formal_logic":0.3015873016,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.6419354839,
        "MMLU_high_school_chemistry":0.4187192118,
        "MMLU_high_school_computer_science":0.56,
        "MMLU_high_school_european_history":0.6666666667,
        "MMLU_high_school_geography":0.7222222222,
        "MMLU_high_school_government_and_politics":0.8393782383,
        "MMLU_high_school_macroeconomics":0.5179487179,
        "MMLU_high_school_mathematics":0.3148148148,
        "MMLU_high_school_microeconomics":0.5546218487,
        "MMLU_high_school_physics":0.3774834437,
        "MMLU_high_school_psychology":0.7376146789,
        "MMLU_high_school_statistics":0.4444444444,
        "MMLU_high_school_us_history":0.7647058824,
        "MMLU_high_school_world_history":0.7679324895,
        "MMLU_human_aging":0.6098654709,
        "MMLU_human_sexuality":0.641221374,
        "MMLU_international_law":0.6611570248,
        "MMLU_jurisprudence":0.6759259259,
        "MMLU_logical_fallacies":0.6196319018,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.7905982906,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.7509578544,
        "MMLU_moral_disputes":0.612716763,
        "MMLU_moral_scenarios":0.3474860335,
        "MMLU_nutrition":0.6176470588,
        "MMLU_philosophy":0.6270096463,
        "MMLU_prehistory":0.6296296296,
        "MMLU_professional_accounting":0.3829787234,
        "MMLU_professional_law":0.4250325945,
        "MMLU_professional_medicine":0.5588235294,
        "MMLU_professional_psychology":0.5539215686,
        "MMLU_public_relations":0.5636363636,
        "MMLU_security_studies":0.612244898,
        "MMLU_sociology":0.6517412935,
        "MMLU_us_foreign_policy":0.79,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.7894736842
    },
    {
        "Model":"LLaMA2-13B-Tiefighter",
        "URL":"https:\/\/huggingface.co\/KoboldAI\/LLaMA2-13B-Tiefighter",
        "full_model_name":"KoboldAI\/LLaMA2-13B-Tiefighter",
        "Parameters":13.0,
        "MMLU_average":0.5497839985,
        "arc:challenge|25":0.5682593857,
        "hellaswag|10":0.6500697072,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.5789473684,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.6,
        "MMLU_college_biology":0.5972222222,
        "MMLU_college_chemistry":0.33,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.5317919075,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.65,
        "MMLU_conceptual_physics":0.4680851064,
        "MMLU_econometrics":0.3596491228,
        "MMLU_electrical_engineering":0.5310344828,
        "MMLU_elementary_mathematics":0.3095238095,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.6387096774,
        "MMLU_high_school_chemistry":0.4137931034,
        "MMLU_high_school_computer_science":0.53,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.702020202,
        "MMLU_high_school_government_and_politics":0.7772020725,
        "MMLU_high_school_macroeconomics":0.5051282051,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.5462184874,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.7211009174,
        "MMLU_high_school_statistics":0.3796296296,
        "MMLU_high_school_us_history":0.75,
        "MMLU_high_school_world_history":0.746835443,
        "MMLU_human_aging":0.66367713,
        "MMLU_human_sexuality":0.6183206107,
        "MMLU_international_law":0.7603305785,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.6564417178,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.8162393162,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.7458492976,
        "MMLU_moral_disputes":0.6213872832,
        "MMLU_moral_scenarios":0.3363128492,
        "MMLU_nutrition":0.6176470588,
        "MMLU_philosophy":0.61414791,
        "MMLU_prehistory":0.6080246914,
        "MMLU_professional_accounting":0.4007092199,
        "MMLU_professional_law":0.4335071708,
        "MMLU_professional_medicine":0.5257352941,
        "MMLU_professional_psychology":0.5604575163,
        "MMLU_public_relations":0.6,
        "MMLU_security_studies":0.6367346939,
        "MMLU_sociology":0.7263681592,
        "MMLU_us_foreign_policy":0.85,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.783625731
    },
    {
        "Model":"wizard-limarp",
        "URL":"https:\/\/huggingface.co\/khoantap\/wizard-limarp",
        "full_model_name":"khoantap\/wizard-limarp",
        "Parameters":null,
        "MMLU_average":0.5496069637,
        "arc:challenge|25":0.545221843,
        "hellaswag|10":0.6244771958,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.5037037037,
        "MMLU_astronomy":0.5394736842,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.6075471698,
        "MMLU_college_biology":0.5625,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.5086705202,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.63,
        "MMLU_conceptual_physics":0.4340425532,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.3544973545,
        "MMLU_formal_logic":0.4047619048,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.6387096774,
        "MMLU_high_school_chemistry":0.4581280788,
        "MMLU_high_school_computer_science":0.61,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.702020202,
        "MMLU_high_school_government_and_politics":0.792746114,
        "MMLU_high_school_macroeconomics":0.5205128205,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.5840336134,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.728440367,
        "MMLU_high_school_statistics":0.3888888889,
        "MMLU_high_school_us_history":0.75,
        "MMLU_high_school_world_history":0.7257383966,
        "MMLU_human_aging":0.6502242152,
        "MMLU_human_sexuality":0.6183206107,
        "MMLU_international_law":0.7768595041,
        "MMLU_jurisprudence":0.7407407407,
        "MMLU_logical_fallacies":0.6380368098,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.6699029126,
        "MMLU_marketing":0.811965812,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7279693487,
        "MMLU_moral_disputes":0.5924855491,
        "MMLU_moral_scenarios":0.3072625698,
        "MMLU_nutrition":0.6013071895,
        "MMLU_philosophy":0.6045016077,
        "MMLU_prehistory":0.5771604938,
        "MMLU_professional_accounting":0.4184397163,
        "MMLU_professional_law":0.4146023468,
        "MMLU_professional_medicine":0.5551470588,
        "MMLU_professional_psychology":0.5359477124,
        "MMLU_public_relations":0.6909090909,
        "MMLU_security_studies":0.6693877551,
        "MMLU_sociology":0.7014925373,
        "MMLU_us_foreign_policy":0.79,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.7543859649
    },
    {
        "Model":"llama-2-13b-FINETUNE4_3.8w-r8-q_k_v_o",
        "URL":"https:\/\/huggingface.co\/CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r8-q_k_v_o",
        "full_model_name":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r8-q_k_v_o",
        "Parameters":13.0,
        "MMLU_average":0.5494970583,
        "arc:challenge|25":0.5315699659,
        "hellaswag|10":0.6103365863,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.5921052632,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.5886792453,
        "MMLU_college_biology":0.5555555556,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.5086705202,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.4553191489,
        "MMLU_econometrics":0.3596491228,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.3465608466,
        "MMLU_formal_logic":0.380952381,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.6258064516,
        "MMLU_high_school_chemistry":0.3743842365,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.6303030303,
        "MMLU_high_school_geography":0.7222222222,
        "MMLU_high_school_government_and_politics":0.792746114,
        "MMLU_high_school_macroeconomics":0.5128205128,
        "MMLU_high_school_mathematics":0.3185185185,
        "MMLU_high_school_microeconomics":0.5546218487,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.7559633028,
        "MMLU_high_school_statistics":0.4305555556,
        "MMLU_high_school_us_history":0.7303921569,
        "MMLU_high_school_world_history":0.7510548523,
        "MMLU_human_aging":0.6367713004,
        "MMLU_human_sexuality":0.5954198473,
        "MMLU_international_law":0.7603305785,
        "MMLU_jurisprudence":0.6944444444,
        "MMLU_logical_fallacies":0.6196319018,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.7475728155,
        "MMLU_marketing":0.7991452991,
        "MMLU_medical_genetics":0.68,
        "MMLU_miscellaneous":0.7509578544,
        "MMLU_moral_disputes":0.6445086705,
        "MMLU_moral_scenarios":0.4212290503,
        "MMLU_nutrition":0.5980392157,
        "MMLU_philosophy":0.6495176849,
        "MMLU_prehistory":0.6234567901,
        "MMLU_professional_accounting":0.4290780142,
        "MMLU_professional_law":0.4152542373,
        "MMLU_professional_medicine":0.5514705882,
        "MMLU_professional_psychology":0.5669934641,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.5183673469,
        "MMLU_sociology":0.6766169154,
        "MMLU_us_foreign_policy":0.74,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.8011695906
    },
    {
        "Model":"platypus2-22b-relora",
        "URL":"https:\/\/huggingface.co\/chargoddard\/platypus2-22b-relora",
        "full_model_name":"chargoddard\/platypus2-22b-relora",
        "Parameters":22.0,
        "MMLU_average":0.5494275192,
        "arc:challenge|25":0.5315699659,
        "hellaswag|10":0.6156144194,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.5197368421,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.5735849057,
        "MMLU_college_biology":0.5902777778,
        "MMLU_college_chemistry":0.36,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.5086705202,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.73,
        "MMLU_conceptual_physics":0.4127659574,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.5379310345,
        "MMLU_elementary_mathematics":0.3201058201,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.6419354839,
        "MMLU_high_school_chemistry":0.4236453202,
        "MMLU_high_school_computer_science":0.62,
        "MMLU_high_school_european_history":0.696969697,
        "MMLU_high_school_geography":0.6818181818,
        "MMLU_high_school_government_and_politics":0.8186528497,
        "MMLU_high_school_macroeconomics":0.5153846154,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.525210084,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.7247706422,
        "MMLU_high_school_statistics":0.412037037,
        "MMLU_high_school_us_history":0.7254901961,
        "MMLU_high_school_world_history":0.7510548523,
        "MMLU_human_aging":0.6412556054,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.7438016529,
        "MMLU_jurisprudence":0.6666666667,
        "MMLU_logical_fallacies":0.6503067485,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.7820512821,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.748403576,
        "MMLU_moral_disputes":0.6271676301,
        "MMLU_moral_scenarios":0.3407821229,
        "MMLU_nutrition":0.6111111111,
        "MMLU_philosophy":0.6302250804,
        "MMLU_prehistory":0.6419753086,
        "MMLU_professional_accounting":0.4184397163,
        "MMLU_professional_law":0.4152542373,
        "MMLU_professional_medicine":0.4816176471,
        "MMLU_professional_psychology":0.568627451,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.6081632653,
        "MMLU_sociology":0.7114427861,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.7543859649
    },
    {
        "Model":"llama-2-13b-FINETUNE3_3.3w-r16-q_k_v_o_gate_up_down",
        "URL":"https:\/\/huggingface.co\/CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r16-q_k_v_o_gate_up_down",
        "full_model_name":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r16-q_k_v_o_gate_up_down",
        "Parameters":13.0,
        "MMLU_average":0.5493590668,
        "arc:challenge|25":0.5494880546,
        "hellaswag|10":0.6065524796,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.5592105263,
        "MMLU_business_ethics":0.49,
        "MMLU_clinical_knowledge":0.5924528302,
        "MMLU_college_biology":0.5763888889,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.5202312139,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.4551724138,
        "MMLU_elementary_mathematics":0.3174603175,
        "MMLU_formal_logic":0.3015873016,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.6806451613,
        "MMLU_high_school_chemistry":0.4581280788,
        "MMLU_high_school_computer_science":0.48,
        "MMLU_high_school_european_history":0.6787878788,
        "MMLU_high_school_geography":0.702020202,
        "MMLU_high_school_government_and_politics":0.8341968912,
        "MMLU_high_school_macroeconomics":0.5282051282,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.6260504202,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.7559633028,
        "MMLU_high_school_statistics":0.462962963,
        "MMLU_high_school_us_history":0.7598039216,
        "MMLU_high_school_world_history":0.7215189873,
        "MMLU_human_aging":0.6547085202,
        "MMLU_human_sexuality":0.5954198473,
        "MMLU_international_law":0.7520661157,
        "MMLU_jurisprudence":0.7314814815,
        "MMLU_logical_fallacies":0.6073619632,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.7475728155,
        "MMLU_marketing":0.8076923077,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.7611749681,
        "MMLU_moral_disputes":0.6502890173,
        "MMLU_moral_scenarios":0.2346368715,
        "MMLU_nutrition":0.6013071895,
        "MMLU_philosophy":0.6655948553,
        "MMLU_prehistory":0.6234567901,
        "MMLU_professional_accounting":0.4468085106,
        "MMLU_professional_law":0.4113428944,
        "MMLU_professional_medicine":0.5551470588,
        "MMLU_professional_psychology":0.5539215686,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.6163265306,
        "MMLU_sociology":0.7462686567,
        "MMLU_us_foreign_policy":0.79,
        "MMLU_virology":0.4759036145,
        "MMLU_world_religions":0.7719298246
    },
    {
        "Model":"llama-2-13b-FINETUNE5_4w-r8-gate_up_down",
        "URL":"https:\/\/huggingface.co\/CHIH-HUNG\/llama-2-13b-FINETUNE5_4w-r8-gate_up_down",
        "full_model_name":"CHIH-HUNG\/llama-2-13b-FINETUNE5_4w-r8-gate_up_down",
        "Parameters":13.0,
        "MMLU_average":0.5487902789,
        "arc:challenge|25":0.5281569966,
        "hellaswag|10":0.6113324039,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.5657894737,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.6,
        "MMLU_college_biology":0.6180555556,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.5028901734,
        "MMLU_college_physics":0.3431372549,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4255319149,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.4551724138,
        "MMLU_elementary_mathematics":0.3306878307,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.6548387097,
        "MMLU_high_school_chemistry":0.4236453202,
        "MMLU_high_school_computer_science":0.51,
        "MMLU_high_school_european_history":0.6787878788,
        "MMLU_high_school_geography":0.7070707071,
        "MMLU_high_school_government_and_politics":0.8031088083,
        "MMLU_high_school_macroeconomics":0.5025641026,
        "MMLU_high_school_mathematics":0.3222222222,
        "MMLU_high_school_microeconomics":0.5882352941,
        "MMLU_high_school_physics":0.3708609272,
        "MMLU_high_school_psychology":0.7559633028,
        "MMLU_high_school_statistics":0.4768518519,
        "MMLU_high_school_us_history":0.7794117647,
        "MMLU_high_school_world_history":0.7257383966,
        "MMLU_human_aging":0.6233183857,
        "MMLU_human_sexuality":0.5954198473,
        "MMLU_international_law":0.7520661157,
        "MMLU_jurisprudence":0.6851851852,
        "MMLU_logical_fallacies":0.6687116564,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.7475728155,
        "MMLU_marketing":0.7991452991,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.754789272,
        "MMLU_moral_disputes":0.6213872832,
        "MMLU_moral_scenarios":0.287150838,
        "MMLU_nutrition":0.5718954248,
        "MMLU_philosophy":0.6495176849,
        "MMLU_prehistory":0.6296296296,
        "MMLU_professional_accounting":0.4468085106,
        "MMLU_professional_law":0.4335071708,
        "MMLU_professional_medicine":0.5330882353,
        "MMLU_professional_psychology":0.5751633987,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.5387755102,
        "MMLU_sociology":0.7064676617,
        "MMLU_us_foreign_policy":0.77,
        "MMLU_virology":0.4096385542,
        "MMLU_world_religions":0.7602339181
    },
    {
        "Model":"airoboros-l2-13b-2.1",
        "URL":"https:\/\/huggingface.co\/jondurbin\/airoboros-l2-13b-2.1",
        "full_model_name":"jondurbin\/airoboros-l2-13b-2.1",
        "Parameters":13.0,
        "MMLU_average":0.5482828979,
        "arc:challenge|25":0.54778157,
        "hellaswag|10":0.6242780323,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.5197368421,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.5924528302,
        "MMLU_college_biology":0.5625,
        "MMLU_college_chemistry":0.36,
        "MMLU_college_computer_science":0.53,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5086705202,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.3957446809,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.5310344828,
        "MMLU_elementary_mathematics":0.2962962963,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.6387096774,
        "MMLU_high_school_chemistry":0.4679802956,
        "MMLU_high_school_computer_science":0.56,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.6818181818,
        "MMLU_high_school_government_and_politics":0.7720207254,
        "MMLU_high_school_macroeconomics":0.5,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.5084033613,
        "MMLU_high_school_physics":0.3708609272,
        "MMLU_high_school_psychology":0.7155963303,
        "MMLU_high_school_statistics":0.4212962963,
        "MMLU_high_school_us_history":0.7598039216,
        "MMLU_high_school_world_history":0.7088607595,
        "MMLU_human_aging":0.6457399103,
        "MMLU_human_sexuality":0.6183206107,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.712962963,
        "MMLU_logical_fallacies":0.6871165644,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.7521367521,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.7445721584,
        "MMLU_moral_disputes":0.6184971098,
        "MMLU_moral_scenarios":0.3497206704,
        "MMLU_nutrition":0.5882352941,
        "MMLU_philosophy":0.6237942122,
        "MMLU_prehistory":0.6543209877,
        "MMLU_professional_accounting":0.414893617,
        "MMLU_professional_law":0.3930899609,
        "MMLU_professional_medicine":0.5220588235,
        "MMLU_professional_psychology":0.5408496732,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6163265306,
        "MMLU_sociology":0.7412935323,
        "MMLU_us_foreign_policy":0.78,
        "MMLU_virology":0.4759036145,
        "MMLU_world_religions":0.7251461988
    },
    {
        "Model":"Llama-2-13B-GPTQ",
        "URL":"https:\/\/huggingface.co\/TheBloke\/Llama-2-13B-GPTQ",
        "full_model_name":"TheBloke\/Llama-2-13B-GPTQ",
        "Parameters":13.0,
        "MMLU_average":0.5480940122,
        "arc:challenge|25":0.5383959044,
        "hellaswag|10":0.6082453694,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.5328947368,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.6226415094,
        "MMLU_college_biology":0.6041666667,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.5202312139,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.350877193,
        "MMLU_electrical_engineering":0.5310344828,
        "MMLU_elementary_mathematics":0.3148148148,
        "MMLU_formal_logic":0.3015873016,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.6322580645,
        "MMLU_high_school_chemistry":0.4334975369,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.6303030303,
        "MMLU_high_school_geography":0.6666666667,
        "MMLU_high_school_government_and_politics":0.7875647668,
        "MMLU_high_school_macroeconomics":0.5102564103,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.5378151261,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.7357798165,
        "MMLU_high_school_statistics":0.4537037037,
        "MMLU_high_school_us_history":0.7254901961,
        "MMLU_high_school_world_history":0.7004219409,
        "MMLU_human_aging":0.6547085202,
        "MMLU_human_sexuality":0.6259541985,
        "MMLU_international_law":0.7107438017,
        "MMLU_jurisprudence":0.6759259259,
        "MMLU_logical_fallacies":0.6625766871,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.7475728155,
        "MMLU_marketing":0.8034188034,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.7573435504,
        "MMLU_moral_disputes":0.6156069364,
        "MMLU_moral_scenarios":0.3608938547,
        "MMLU_nutrition":0.5947712418,
        "MMLU_philosophy":0.6237942122,
        "MMLU_prehistory":0.6358024691,
        "MMLU_professional_accounting":0.3936170213,
        "MMLU_professional_law":0.425684485,
        "MMLU_professional_medicine":0.5294117647,
        "MMLU_professional_psychology":0.5653594771,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.6285714286,
        "MMLU_sociology":0.7064676617,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4638554217,
        "MMLU_world_religions":0.7485380117
    },
    {
        "Model":"Chat-Stheno-L2-13B",
        "URL":"https:\/\/huggingface.co\/Sao10K\/Chat-Stheno-L2-13B",
        "full_model_name":"Sao10K\/Chat-Stheno-L2-13B",
        "Parameters":13.0,
        "MMLU_average":0.5479756678,
        "arc:challenge|25":0.5392491468,
        "hellaswag|10":0.6128261303,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.5592105263,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.5811320755,
        "MMLU_college_biology":0.5694444444,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.4566473988,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.4042553191,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.5103448276,
        "MMLU_elementary_mathematics":0.3439153439,
        "MMLU_formal_logic":0.3015873016,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.635483871,
        "MMLU_high_school_chemistry":0.4532019704,
        "MMLU_high_school_computer_science":0.57,
        "MMLU_high_school_european_history":0.6848484848,
        "MMLU_high_school_geography":0.702020202,
        "MMLU_high_school_government_and_politics":0.792746114,
        "MMLU_high_school_macroeconomics":0.4948717949,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.525210084,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7412844037,
        "MMLU_high_school_statistics":0.4074074074,
        "MMLU_high_school_us_history":0.7450980392,
        "MMLU_high_school_world_history":0.7299578059,
        "MMLU_human_aging":0.6457399103,
        "MMLU_human_sexuality":0.641221374,
        "MMLU_international_law":0.7851239669,
        "MMLU_jurisprudence":0.7037037037,
        "MMLU_logical_fallacies":0.6564417178,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.7863247863,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.7509578544,
        "MMLU_moral_disputes":0.612716763,
        "MMLU_moral_scenarios":0.294972067,
        "MMLU_nutrition":0.6045751634,
        "MMLU_philosophy":0.5980707395,
        "MMLU_prehistory":0.6080246914,
        "MMLU_professional_accounting":0.390070922,
        "MMLU_professional_law":0.3872229465,
        "MMLU_professional_medicine":0.5110294118,
        "MMLU_professional_psychology":0.5441176471,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.6326530612,
        "MMLU_sociology":0.7462686567,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4879518072,
        "MMLU_world_religions":0.7251461988
    },
    {
        "Model":"Python-Code-13B",
        "URL":"https:\/\/huggingface.co\/ajibawa-2023\/Python-Code-13B",
        "full_model_name":"ajibawa-2023\/Python-Code-13B",
        "Parameters":13.0,
        "MMLU_average":0.5477825515,
        "arc:challenge|25":0.5469283276,
        "hellaswag|10":0.6303525194,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4518518519,
        "MMLU_astronomy":0.5131578947,
        "MMLU_business_ethics":0.5,
        "MMLU_clinical_knowledge":0.6037735849,
        "MMLU_college_biology":0.5833333333,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.5260115607,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.4255319149,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.3253968254,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.6290322581,
        "MMLU_high_school_chemistry":0.4285714286,
        "MMLU_high_school_computer_science":0.53,
        "MMLU_high_school_european_history":0.6484848485,
        "MMLU_high_school_geography":0.6818181818,
        "MMLU_high_school_government_and_politics":0.8031088083,
        "MMLU_high_school_macroeconomics":0.5102564103,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.5420168067,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.7321100917,
        "MMLU_high_school_statistics":0.4074074074,
        "MMLU_high_school_us_history":0.7549019608,
        "MMLU_high_school_world_history":0.7257383966,
        "MMLU_human_aging":0.6547085202,
        "MMLU_human_sexuality":0.6259541985,
        "MMLU_international_law":0.7190082645,
        "MMLU_jurisprudence":0.6944444444,
        "MMLU_logical_fallacies":0.6564417178,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.7475728155,
        "MMLU_marketing":0.7948717949,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.7458492976,
        "MMLU_moral_disputes":0.6242774566,
        "MMLU_moral_scenarios":0.3765363128,
        "MMLU_nutrition":0.6339869281,
        "MMLU_philosophy":0.6237942122,
        "MMLU_prehistory":0.6049382716,
        "MMLU_professional_accounting":0.390070922,
        "MMLU_professional_law":0.4185136897,
        "MMLU_professional_medicine":0.5257352941,
        "MMLU_professional_psychology":0.5718954248,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.6326530612,
        "MMLU_sociology":0.7611940299,
        "MMLU_us_foreign_policy":0.86,
        "MMLU_virology":0.4819277108,
        "MMLU_world_religions":0.7777777778
    },
    {
        "Model":"robin-33B-v2-fp16",
        "URL":"https:\/\/huggingface.co\/TheBloke\/robin-33B-v2-fp16",
        "full_model_name":"TheBloke\/robin-33B-v2-fp16",
        "Parameters":33.0,
        "MMLU_average":0.5471043158,
        "arc:challenge|25":0.5947098976,
        "hellaswag|10":0.6331408086,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.5037037037,
        "MMLU_astronomy":0.5657894737,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.520754717,
        "MMLU_college_biology":0.5555555556,
        "MMLU_college_chemistry":0.36,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.4855491329,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.65,
        "MMLU_conceptual_physics":0.4510638298,
        "MMLU_econometrics":0.3684210526,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.328042328,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.6161290323,
        "MMLU_high_school_chemistry":0.3743842365,
        "MMLU_high_school_computer_science":0.6,
        "MMLU_high_school_european_history":0.7333333333,
        "MMLU_high_school_geography":0.7222222222,
        "MMLU_high_school_government_and_politics":0.7720207254,
        "MMLU_high_school_macroeconomics":0.5025641026,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.5630252101,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.7339449541,
        "MMLU_high_school_statistics":0.4212962963,
        "MMLU_high_school_us_history":0.7598039216,
        "MMLU_high_school_world_history":0.776371308,
        "MMLU_human_aging":0.6188340807,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.7024793388,
        "MMLU_jurisprudence":0.6666666667,
        "MMLU_logical_fallacies":0.6932515337,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.6310679612,
        "MMLU_marketing":0.8418803419,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.7305236271,
        "MMLU_moral_disputes":0.5953757225,
        "MMLU_moral_scenarios":0.2603351955,
        "MMLU_nutrition":0.5882352941,
        "MMLU_philosophy":0.6302250804,
        "MMLU_prehistory":0.6080246914,
        "MMLU_professional_accounting":0.4361702128,
        "MMLU_professional_law":0.4126466754,
        "MMLU_professional_medicine":0.5404411765,
        "MMLU_professional_psychology":0.5571895425,
        "MMLU_public_relations":0.6,
        "MMLU_security_studies":0.5714285714,
        "MMLU_sociology":0.7661691542,
        "MMLU_us_foreign_policy":0.8,
        "MMLU_virology":0.4819277108,
        "MMLU_world_religions":0.7602339181
    },
    {
        "Model":"EverythingLM-13b-V3-peft",
        "URL":"https:\/\/huggingface.co\/totally-not-an-llm\/EverythingLM-13b-V3-peft",
        "full_model_name":"totally-not-an-llm\/EverythingLM-13b-V3-peft",
        "Parameters":13.0,
        "MMLU_average":0.547010405,
        "arc:challenge|25":0.5494880546,
        "hellaswag|10":0.605954989,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.5197368421,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.5886792453,
        "MMLU_college_biology":0.6319444444,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.4797687861,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.4638297872,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.4827586207,
        "MMLU_elementary_mathematics":0.3201058201,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.6161290323,
        "MMLU_high_school_chemistry":0.4039408867,
        "MMLU_high_school_computer_science":0.54,
        "MMLU_high_school_european_history":0.6606060606,
        "MMLU_high_school_geography":0.7070707071,
        "MMLU_high_school_government_and_politics":0.7409326425,
        "MMLU_high_school_macroeconomics":0.4820512821,
        "MMLU_high_school_mathematics":0.3222222222,
        "MMLU_high_school_microeconomics":0.5714285714,
        "MMLU_high_school_physics":0.4304635762,
        "MMLU_high_school_psychology":0.7137614679,
        "MMLU_high_school_statistics":0.412037037,
        "MMLU_high_school_us_history":0.7254901961,
        "MMLU_high_school_world_history":0.7215189873,
        "MMLU_human_aging":0.6367713004,
        "MMLU_human_sexuality":0.5572519084,
        "MMLU_international_law":0.7190082645,
        "MMLU_jurisprudence":0.6759259259,
        "MMLU_logical_fallacies":0.6441717791,
        "MMLU_machine_learning":0.4464285714,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.7948717949,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.7330779055,
        "MMLU_moral_disputes":0.6271676301,
        "MMLU_moral_scenarios":0.4245810056,
        "MMLU_nutrition":0.5816993464,
        "MMLU_philosophy":0.6173633441,
        "MMLU_prehistory":0.6111111111,
        "MMLU_professional_accounting":0.4042553191,
        "MMLU_professional_law":0.3872229465,
        "MMLU_professional_medicine":0.5147058824,
        "MMLU_professional_psychology":0.5375816993,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.587755102,
        "MMLU_sociology":0.736318408,
        "MMLU_us_foreign_policy":0.8,
        "MMLU_virology":0.4337349398,
        "MMLU_world_religions":0.7485380117
    },
    {
        "Model":"llama-2-13b-vicuna-wizard",
        "URL":"https:\/\/huggingface.co\/pe-nlp\/llama-2-13b-vicuna-wizard",
        "full_model_name":"pe-nlp\/llama-2-13b-vicuna-wizard",
        "Parameters":13.0,
        "MMLU_average":0.5468153856,
        "arc:challenge|25":0.5435153584,
        "hellaswag|10":0.6154152559,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.5259259259,
        "MMLU_astronomy":0.5526315789,
        "MMLU_business_ethics":0.58,
        "MMLU_clinical_knowledge":0.5962264151,
        "MMLU_college_biology":0.5902777778,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.22,
        "MMLU_college_medicine":0.5202312139,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.5310344828,
        "MMLU_elementary_mathematics":0.3095238095,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.6612903226,
        "MMLU_high_school_chemistry":0.4975369458,
        "MMLU_high_school_computer_science":0.56,
        "MMLU_high_school_european_history":0.6424242424,
        "MMLU_high_school_geography":0.6868686869,
        "MMLU_high_school_government_and_politics":0.8134715026,
        "MMLU_high_school_macroeconomics":0.5,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.5462184874,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.7229357798,
        "MMLU_high_school_statistics":0.4351851852,
        "MMLU_high_school_us_history":0.7205882353,
        "MMLU_high_school_world_history":0.7341772152,
        "MMLU_human_aging":0.6457399103,
        "MMLU_human_sexuality":0.6259541985,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.7314814815,
        "MMLU_logical_fallacies":0.6748466258,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.7475728155,
        "MMLU_marketing":0.8034188034,
        "MMLU_medical_genetics":0.53,
        "MMLU_miscellaneous":0.7573435504,
        "MMLU_moral_disputes":0.6531791908,
        "MMLU_moral_scenarios":0.2648044693,
        "MMLU_nutrition":0.6078431373,
        "MMLU_philosophy":0.6463022508,
        "MMLU_prehistory":0.6296296296,
        "MMLU_professional_accounting":0.4255319149,
        "MMLU_professional_law":0.4119947849,
        "MMLU_professional_medicine":0.4816176471,
        "MMLU_professional_psychology":0.5473856209,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.6367346939,
        "MMLU_sociology":0.736318408,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.7426900585
    },
    {
        "Model":"llama2-13b-ft-mc4_nl_cleaned_tiny",
        "URL":"https:\/\/huggingface.co\/BramVanroy\/llama2-13b-ft-mc4_nl_cleaned_tiny",
        "full_model_name":"BramVanroy\/llama2-13b-ft-mc4_nl_cleaned_tiny",
        "Parameters":13.0,
        "MMLU_average":0.5466770764,
        "arc:challenge|25":0.5469283276,
        "hellaswag|10":0.612925712,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.5394736842,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.6037735849,
        "MMLU_college_biology":0.5486111111,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4913294798,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.4510638298,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.3121693122,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.6677419355,
        "MMLU_high_school_chemistry":0.4137931034,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.6363636364,
        "MMLU_high_school_geography":0.6666666667,
        "MMLU_high_school_government_and_politics":0.7875647668,
        "MMLU_high_school_macroeconomics":0.5076923077,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.5630252101,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.7559633028,
        "MMLU_high_school_statistics":0.4537037037,
        "MMLU_high_school_us_history":0.7401960784,
        "MMLU_high_school_world_history":0.7341772152,
        "MMLU_human_aging":0.6547085202,
        "MMLU_human_sexuality":0.6030534351,
        "MMLU_international_law":0.7107438017,
        "MMLU_jurisprudence":0.7314814815,
        "MMLU_logical_fallacies":0.6687116564,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.7991452991,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.7420178799,
        "MMLU_moral_disputes":0.6416184971,
        "MMLU_moral_scenarios":0.3474860335,
        "MMLU_nutrition":0.6274509804,
        "MMLU_philosophy":0.6655948553,
        "MMLU_prehistory":0.6388888889,
        "MMLU_professional_accounting":0.390070922,
        "MMLU_professional_law":0.4119947849,
        "MMLU_professional_medicine":0.5367647059,
        "MMLU_professional_psychology":0.545751634,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.6204081633,
        "MMLU_sociology":0.7313432836,
        "MMLU_us_foreign_policy":0.81,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.7777777778
    },
    {
        "Model":"llama2-22b-blocktriangular",
        "URL":"https:\/\/huggingface.co\/chargoddard\/llama2-22b-blocktriangular",
        "full_model_name":"chargoddard\/llama2-22b-blocktriangular",
        "Parameters":22.0,
        "MMLU_average":0.5464036023,
        "arc:challenge|25":0.5494880546,
        "hellaswag|10":0.6181039634,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.5328947368,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.5886792453,
        "MMLU_college_biology":0.5763888889,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.5144508671,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.5103448276,
        "MMLU_elementary_mathematics":0.3386243386,
        "MMLU_formal_logic":0.2698412698,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.6580645161,
        "MMLU_high_school_chemistry":0.4433497537,
        "MMLU_high_school_computer_science":0.56,
        "MMLU_high_school_european_history":0.6545454545,
        "MMLU_high_school_geography":0.6616161616,
        "MMLU_high_school_government_and_politics":0.792746114,
        "MMLU_high_school_macroeconomics":0.4948717949,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.5630252101,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.7357798165,
        "MMLU_high_school_statistics":0.4259259259,
        "MMLU_high_school_us_history":0.7450980392,
        "MMLU_high_school_world_history":0.746835443,
        "MMLU_human_aging":0.6367713004,
        "MMLU_human_sexuality":0.5801526718,
        "MMLU_international_law":0.7024793388,
        "MMLU_jurisprudence":0.7314814815,
        "MMLU_logical_fallacies":0.6687116564,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.7948717949,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.7445721584,
        "MMLU_moral_disputes":0.6416184971,
        "MMLU_moral_scenarios":0.3184357542,
        "MMLU_nutrition":0.6241830065,
        "MMLU_philosophy":0.6334405145,
        "MMLU_prehistory":0.6419753086,
        "MMLU_professional_accounting":0.4007092199,
        "MMLU_professional_law":0.4178617992,
        "MMLU_professional_medicine":0.4779411765,
        "MMLU_professional_psychology":0.5424836601,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.6081632653,
        "MMLU_sociology":0.7213930348,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.7719298246
    },
    {
        "Model":"Llama-2-13b-chat-hf",
        "URL":"https:\/\/huggingface.co\/meta-llama\/Llama-2-13b-chat-hf",
        "full_model_name":"meta-llama\/Llama-2-13b-chat-hf",
        "Parameters":13.0,
        "MMLU_average":0.5463627088,
        "arc:challenge|25":0.5563139932,
        "hellaswag|10":0.6293567019,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.5460526316,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.5849056604,
        "MMLU_college_biology":0.5833333333,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.4624277457,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.4,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.3412698413,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.6419354839,
        "MMLU_high_school_chemistry":0.4630541872,
        "MMLU_high_school_computer_science":0.59,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.702020202,
        "MMLU_high_school_government_and_politics":0.7875647668,
        "MMLU_high_school_macroeconomics":0.4923076923,
        "MMLU_high_school_mathematics":0.3111111111,
        "MMLU_high_school_microeconomics":0.5294117647,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.7321100917,
        "MMLU_high_school_statistics":0.3888888889,
        "MMLU_high_school_us_history":0.75,
        "MMLU_high_school_world_history":0.7172995781,
        "MMLU_human_aging":0.6457399103,
        "MMLU_human_sexuality":0.6335877863,
        "MMLU_international_law":0.7685950413,
        "MMLU_jurisprudence":0.6944444444,
        "MMLU_logical_fallacies":0.6503067485,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.7863247863,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.7471264368,
        "MMLU_moral_disputes":0.612716763,
        "MMLU_moral_scenarios":0.305027933,
        "MMLU_nutrition":0.5947712418,
        "MMLU_philosophy":0.5884244373,
        "MMLU_prehistory":0.6111111111,
        "MMLU_professional_accounting":0.3865248227,
        "MMLU_professional_law":0.3911342894,
        "MMLU_professional_medicine":0.5,
        "MMLU_professional_psychology":0.5424836601,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.6408163265,
        "MMLU_sociology":0.7512437811,
        "MMLU_us_foreign_policy":0.81,
        "MMLU_virology":0.4819277108,
        "MMLU_world_religions":0.730994152
    },
    {
        "Model":"CodeUp-Llama-2-13b-chat-hf",
        "URL":"https:\/\/huggingface.co\/deepse\/CodeUp-Llama-2-13b-chat-hf",
        "full_model_name":"deepse\/CodeUp-Llama-2-13b-chat-hf",
        "Parameters":13.0,
        "MMLU_average":0.5462619093,
        "arc:challenge|25":0.5580204778,
        "hellaswag|10":0.6292571201,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.5460526316,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.5849056604,
        "MMLU_college_biology":0.5833333333,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.4682080925,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.4,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.3412698413,
        "MMLU_formal_logic":0.3015873016,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.6419354839,
        "MMLU_high_school_chemistry":0.4581280788,
        "MMLU_high_school_computer_science":0.59,
        "MMLU_high_school_european_history":0.6787878788,
        "MMLU_high_school_geography":0.702020202,
        "MMLU_high_school_government_and_politics":0.7875647668,
        "MMLU_high_school_macroeconomics":0.4948717949,
        "MMLU_high_school_mathematics":0.3074074074,
        "MMLU_high_school_microeconomics":0.5294117647,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.7321100917,
        "MMLU_high_school_statistics":0.3888888889,
        "MMLU_high_school_us_history":0.7401960784,
        "MMLU_high_school_world_history":0.7172995781,
        "MMLU_human_aging":0.6457399103,
        "MMLU_human_sexuality":0.6335877863,
        "MMLU_international_law":0.7685950413,
        "MMLU_jurisprudence":0.6944444444,
        "MMLU_logical_fallacies":0.6503067485,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.7863247863,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.7471264368,
        "MMLU_moral_disputes":0.612716763,
        "MMLU_moral_scenarios":0.3094972067,
        "MMLU_nutrition":0.5947712418,
        "MMLU_philosophy":0.5884244373,
        "MMLU_prehistory":0.6111111111,
        "MMLU_professional_accounting":0.390070922,
        "MMLU_professional_law":0.3917861799,
        "MMLU_professional_medicine":0.5,
        "MMLU_professional_psychology":0.5441176471,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.6367346939,
        "MMLU_sociology":0.7512437811,
        "MMLU_us_foreign_policy":0.81,
        "MMLU_virology":0.4819277108,
        "MMLU_world_religions":0.730994152
    },
    {
        "Model":"Morningstar-13b-hf",
        "URL":"https:\/\/huggingface.co\/NewstaR\/Morningstar-13b-hf",
        "full_model_name":"NewstaR\/Morningstar-13b-hf",
        "Parameters":13.0,
        "MMLU_average":0.5462619093,
        "arc:challenge|25":0.5580204778,
        "hellaswag|10":0.6292571201,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.5460526316,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.5849056604,
        "MMLU_college_biology":0.5833333333,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.4682080925,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.4,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.3412698413,
        "MMLU_formal_logic":0.3015873016,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.6419354839,
        "MMLU_high_school_chemistry":0.4581280788,
        "MMLU_high_school_computer_science":0.59,
        "MMLU_high_school_european_history":0.6787878788,
        "MMLU_high_school_geography":0.702020202,
        "MMLU_high_school_government_and_politics":0.7875647668,
        "MMLU_high_school_macroeconomics":0.4948717949,
        "MMLU_high_school_mathematics":0.3074074074,
        "MMLU_high_school_microeconomics":0.5294117647,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.7321100917,
        "MMLU_high_school_statistics":0.3888888889,
        "MMLU_high_school_us_history":0.7401960784,
        "MMLU_high_school_world_history":0.7172995781,
        "MMLU_human_aging":0.6457399103,
        "MMLU_human_sexuality":0.6335877863,
        "MMLU_international_law":0.7685950413,
        "MMLU_jurisprudence":0.6944444444,
        "MMLU_logical_fallacies":0.6503067485,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.7863247863,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.7471264368,
        "MMLU_moral_disputes":0.612716763,
        "MMLU_moral_scenarios":0.3094972067,
        "MMLU_nutrition":0.5947712418,
        "MMLU_philosophy":0.5884244373,
        "MMLU_prehistory":0.6111111111,
        "MMLU_professional_accounting":0.390070922,
        "MMLU_professional_law":0.3917861799,
        "MMLU_professional_medicine":0.5,
        "MMLU_professional_psychology":0.5441176471,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.6367346939,
        "MMLU_sociology":0.7512437811,
        "MMLU_us_foreign_policy":0.81,
        "MMLU_virology":0.4819277108,
        "MMLU_world_religions":0.730994152
    },
    {
        "Model":"Airboros2.1-Platypus2-13B-QLora-0.80-epoch",
        "URL":"https:\/\/huggingface.co\/TFLai\/Airboros2.1-Platypus2-13B-QLora-0.80-epoch",
        "full_model_name":"TFLai\/Airboros2.1-Platypus2-13B-QLora-0.80-epoch",
        "Parameters":13.0,
        "MMLU_average":0.5462359548,
        "arc:challenge|25":0.5529010239,
        "hellaswag|10":0.6234813782,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5657894737,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.5924528302,
        "MMLU_college_biology":0.6041666667,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.5433526012,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.3787234043,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.4551724138,
        "MMLU_elementary_mathematics":0.3068783069,
        "MMLU_formal_logic":0.380952381,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.6129032258,
        "MMLU_high_school_chemistry":0.4088669951,
        "MMLU_high_school_computer_science":0.54,
        "MMLU_high_school_european_history":0.7212121212,
        "MMLU_high_school_geography":0.7171717172,
        "MMLU_high_school_government_and_politics":0.8031088083,
        "MMLU_high_school_macroeconomics":0.4743589744,
        "MMLU_high_school_mathematics":0.2925925926,
        "MMLU_high_school_microeconomics":0.474789916,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.7577981651,
        "MMLU_high_school_statistics":0.4212962963,
        "MMLU_high_school_us_history":0.7647058824,
        "MMLU_high_school_world_history":0.7215189873,
        "MMLU_human_aging":0.6322869955,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.694214876,
        "MMLU_jurisprudence":0.6574074074,
        "MMLU_logical_fallacies":0.6564417178,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.7863247863,
        "MMLU_medical_genetics":0.64,
        "MMLU_miscellaneous":0.7586206897,
        "MMLU_moral_disputes":0.598265896,
        "MMLU_moral_scenarios":0.2826815642,
        "MMLU_nutrition":0.545751634,
        "MMLU_philosophy":0.6237942122,
        "MMLU_prehistory":0.6388888889,
        "MMLU_professional_accounting":0.4255319149,
        "MMLU_professional_law":0.4315514993,
        "MMLU_professional_medicine":0.5588235294,
        "MMLU_professional_psychology":0.591503268,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.587755102,
        "MMLU_sociology":0.6766169154,
        "MMLU_us_foreign_policy":0.76,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.7719298246
    },
    {
        "Model":"Llama-2-13b-hf_Open-Platypus-8bit-att",
        "URL":"https:\/\/huggingface.co\/NekoPunchBBB\/Llama-2-13b-hf_Open-Platypus-8bit-att",
        "full_model_name":"NekoPunchBBB\/Llama-2-13b-hf_Open-Platypus-8bit-att",
        "Parameters":13.0,
        "MMLU_average":0.5455983639,
        "arc:challenge|25":0.5554607509,
        "hellaswag|10":0.6132244573,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.5185185185,
        "MMLU_astronomy":0.5197368421,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.5962264151,
        "MMLU_college_biology":0.5763888889,
        "MMLU_college_chemistry":0.36,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5086705202,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.429787234,
        "MMLU_econometrics":0.350877193,
        "MMLU_electrical_engineering":0.524137931,
        "MMLU_elementary_mathematics":0.3201058201,
        "MMLU_formal_logic":0.2777777778,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.6774193548,
        "MMLU_high_school_chemistry":0.4187192118,
        "MMLU_high_school_computer_science":0.58,
        "MMLU_high_school_european_history":0.6303030303,
        "MMLU_high_school_geography":0.6919191919,
        "MMLU_high_school_government_and_politics":0.8031088083,
        "MMLU_high_school_macroeconomics":0.5102564103,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.5462184874,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.752293578,
        "MMLU_high_school_statistics":0.4166666667,
        "MMLU_high_school_us_history":0.7549019608,
        "MMLU_high_school_world_history":0.7299578059,
        "MMLU_human_aging":0.668161435,
        "MMLU_human_sexuality":0.5648854962,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.7037037037,
        "MMLU_logical_fallacies":0.6809815951,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.7475728155,
        "MMLU_marketing":0.7991452991,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.7522349936,
        "MMLU_moral_disputes":0.6358381503,
        "MMLU_moral_scenarios":0.3329608939,
        "MMLU_nutrition":0.6078431373,
        "MMLU_philosophy":0.6463022508,
        "MMLU_prehistory":0.6327160494,
        "MMLU_professional_accounting":0.4007092199,
        "MMLU_professional_law":0.4185136897,
        "MMLU_professional_medicine":0.4779411765,
        "MMLU_professional_psychology":0.5522875817,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.5795918367,
        "MMLU_sociology":0.7313432836,
        "MMLU_us_foreign_policy":0.8,
        "MMLU_virology":0.4337349398,
        "MMLU_world_religions":0.7368421053
    },
    {
        "Model":"llama2-22B-GPLATTY",
        "URL":"https:\/\/huggingface.co\/grimpep\/llama2-22B-GPLATTY",
        "full_model_name":"grimpep\/llama2-22B-GPLATTY",
        "Parameters":22.0,
        "MMLU_average":0.545472582,
        "arc:challenge|25":0.5605802048,
        "hellaswag|10":0.6290579566,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.5777777778,
        "MMLU_astronomy":0.5723684211,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.5924528302,
        "MMLU_college_biology":0.6319444444,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.4855491329,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.4425531915,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.3306878307,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.6258064516,
        "MMLU_high_school_chemistry":0.4433497537,
        "MMLU_high_school_computer_science":0.48,
        "MMLU_high_school_european_history":0.6484848485,
        "MMLU_high_school_geography":0.6818181818,
        "MMLU_high_school_government_and_politics":0.792746114,
        "MMLU_high_school_macroeconomics":0.5307692308,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.5588235294,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.728440367,
        "MMLU_high_school_statistics":0.412037037,
        "MMLU_high_school_us_history":0.7696078431,
        "MMLU_high_school_world_history":0.7383966245,
        "MMLU_human_aging":0.66367713,
        "MMLU_human_sexuality":0.6030534351,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.712962963,
        "MMLU_logical_fallacies":0.6748466258,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.764957265,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.7598978289,
        "MMLU_moral_disputes":0.6213872832,
        "MMLU_moral_scenarios":0.356424581,
        "MMLU_nutrition":0.6176470588,
        "MMLU_philosophy":0.6334405145,
        "MMLU_prehistory":0.6327160494,
        "MMLU_professional_accounting":0.414893617,
        "MMLU_professional_law":0.3996088657,
        "MMLU_professional_medicine":0.5,
        "MMLU_professional_psychology":0.5522875817,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.6448979592,
        "MMLU_sociology":0.6069651741,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.7719298246
    },
    {
        "Model":"llama2-22b-wizard_vicuna",
        "URL":"https:\/\/huggingface.co\/grimpep\/llama2-22b-wizard_vicuna",
        "full_model_name":"grimpep\/llama2-22b-wizard_vicuna",
        "Parameters":22.0,
        "MMLU_average":0.545472582,
        "arc:challenge|25":0.5605802048,
        "hellaswag|10":0.6290579566,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.5777777778,
        "MMLU_astronomy":0.5723684211,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.5924528302,
        "MMLU_college_biology":0.6319444444,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.4855491329,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.4425531915,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.3306878307,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.6258064516,
        "MMLU_high_school_chemistry":0.4433497537,
        "MMLU_high_school_computer_science":0.48,
        "MMLU_high_school_european_history":0.6484848485,
        "MMLU_high_school_geography":0.6818181818,
        "MMLU_high_school_government_and_politics":0.792746114,
        "MMLU_high_school_macroeconomics":0.5307692308,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.5588235294,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.728440367,
        "MMLU_high_school_statistics":0.412037037,
        "MMLU_high_school_us_history":0.7696078431,
        "MMLU_high_school_world_history":0.7383966245,
        "MMLU_human_aging":0.66367713,
        "MMLU_human_sexuality":0.6030534351,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.712962963,
        "MMLU_logical_fallacies":0.6748466258,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.764957265,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.7598978289,
        "MMLU_moral_disputes":0.6213872832,
        "MMLU_moral_scenarios":0.356424581,
        "MMLU_nutrition":0.6176470588,
        "MMLU_philosophy":0.6334405145,
        "MMLU_prehistory":0.6327160494,
        "MMLU_professional_accounting":0.414893617,
        "MMLU_professional_law":0.3996088657,
        "MMLU_professional_medicine":0.5,
        "MMLU_professional_psychology":0.5522875817,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.6448979592,
        "MMLU_sociology":0.6069651741,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.7719298246
    },
    {
        "Model":"Mistral-Trismegistus-7B",
        "URL":"https:\/\/huggingface.co\/teknium\/Mistral-Trismegistus-7B",
        "full_model_name":"teknium\/Mistral-Trismegistus-7B",
        "Parameters":7.0,
        "MMLU_average":0.544890283,
        "arc:challenge|25":0.5051194539,
        "hellaswag|10":0.5929097789,
        "MMLU_abstract_algebra":0.21,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5263157895,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.6,
        "MMLU_college_biology":0.6111111111,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.5722543353,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.4595744681,
        "MMLU_econometrics":0.4210526316,
        "MMLU_electrical_engineering":0.5517241379,
        "MMLU_elementary_mathematics":0.3624338624,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.6967741935,
        "MMLU_high_school_chemistry":0.4581280788,
        "MMLU_high_school_computer_science":0.62,
        "MMLU_high_school_european_history":0.6909090909,
        "MMLU_high_school_geography":0.7171717172,
        "MMLU_high_school_government_and_politics":0.7409326425,
        "MMLU_high_school_macroeconomics":0.5076923077,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.5294117647,
        "MMLU_high_school_physics":0.357615894,
        "MMLU_high_school_psychology":0.6935779817,
        "MMLU_high_school_statistics":0.412037037,
        "MMLU_high_school_us_history":0.6715686275,
        "MMLU_high_school_world_history":0.7215189873,
        "MMLU_human_aging":0.600896861,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.7190082645,
        "MMLU_jurisprudence":0.6018518519,
        "MMLU_logical_fallacies":0.6134969325,
        "MMLU_machine_learning":0.4642857143,
        "MMLU_management":0.7087378641,
        "MMLU_marketing":0.8034188034,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7203065134,
        "MMLU_moral_disputes":0.6242774566,
        "MMLU_moral_scenarios":0.2592178771,
        "MMLU_nutrition":0.5947712418,
        "MMLU_philosophy":0.6109324759,
        "MMLU_prehistory":0.5864197531,
        "MMLU_professional_accounting":0.4078014184,
        "MMLU_professional_law":0.4009126467,
        "MMLU_professional_medicine":0.5441176471,
        "MMLU_professional_psychology":0.5179738562,
        "MMLU_public_relations":0.5454545455,
        "MMLU_security_studies":0.5755102041,
        "MMLU_sociology":0.7313432836,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.7543859649
    },
    {
        "Model":"llama2-13b-FINETUNE3_TEST",
        "URL":"https:\/\/huggingface.co\/wei123602\/llama2-13b-FINETUNE3_TEST",
        "full_model_name":"wei123602\/llama2-13b-FINETUNE3_TEST",
        "Parameters":13.0,
        "MMLU_average":0.544849719,
        "arc:challenge|25":0.4795221843,
        "hellaswag|10":0.5889265087,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.5526315789,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.5962264151,
        "MMLU_college_biology":0.5625,
        "MMLU_college_chemistry":0.49,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5433526012,
        "MMLU_college_physics":0.362745098,
        "MMLU_computer_security":0.62,
        "MMLU_conceptual_physics":0.4765957447,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.3931034483,
        "MMLU_elementary_mathematics":0.3201058201,
        "MMLU_formal_logic":0.4523809524,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.6225806452,
        "MMLU_high_school_chemistry":0.4679802956,
        "MMLU_high_school_computer_science":0.57,
        "MMLU_high_school_european_history":0.6666666667,
        "MMLU_high_school_geography":0.6767676768,
        "MMLU_high_school_government_and_politics":0.8238341969,
        "MMLU_high_school_macroeconomics":0.5692307692,
        "MMLU_high_school_mathematics":0.2888888889,
        "MMLU_high_school_microeconomics":0.6008403361,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.7889908257,
        "MMLU_high_school_statistics":0.462962963,
        "MMLU_high_school_us_history":0.7598039216,
        "MMLU_high_school_world_history":0.7510548523,
        "MMLU_human_aging":0.5650224215,
        "MMLU_human_sexuality":0.5801526718,
        "MMLU_international_law":0.6446280992,
        "MMLU_jurisprudence":0.6944444444,
        "MMLU_logical_fallacies":0.6257668712,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.6893203883,
        "MMLU_marketing":0.8247863248,
        "MMLU_medical_genetics":0.61,
        "MMLU_miscellaneous":0.7190293742,
        "MMLU_moral_disputes":0.5722543353,
        "MMLU_moral_scenarios":0.312849162,
        "MMLU_nutrition":0.5816993464,
        "MMLU_philosophy":0.6270096463,
        "MMLU_prehistory":0.6111111111,
        "MMLU_professional_accounting":0.3723404255,
        "MMLU_professional_law":0.4511082138,
        "MMLU_professional_medicine":0.5845588235,
        "MMLU_professional_psychology":0.5522875817,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.5428571429,
        "MMLU_sociology":0.671641791,
        "MMLU_us_foreign_policy":0.72,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.7368421053
    },
    {
        "Model":"Llama-2-13b-chat-german",
        "URL":"https:\/\/huggingface.co\/jphme\/Llama-2-13b-chat-german",
        "full_model_name":"jphme\/Llama-2-13b-chat-german",
        "Parameters":13.0,
        "MMLU_average":0.544500933,
        "arc:challenge|25":0.5349829352,
        "hellaswag|10":0.61621191,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.5723684211,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.5811320755,
        "MMLU_college_biology":0.5833333333,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.4682080925,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.3829787234,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.5586206897,
        "MMLU_elementary_mathematics":0.3386243386,
        "MMLU_formal_logic":0.3015873016,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.6064516129,
        "MMLU_high_school_chemistry":0.4433497537,
        "MMLU_high_school_computer_science":0.58,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.6818181818,
        "MMLU_high_school_government_and_politics":0.792746114,
        "MMLU_high_school_macroeconomics":0.4820512821,
        "MMLU_high_school_mathematics":0.2925925926,
        "MMLU_high_school_microeconomics":0.5168067227,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7412844037,
        "MMLU_high_school_statistics":0.3981481481,
        "MMLU_high_school_us_history":0.7549019608,
        "MMLU_high_school_world_history":0.7426160338,
        "MMLU_human_aging":0.6322869955,
        "MMLU_human_sexuality":0.6335877863,
        "MMLU_international_law":0.7685950413,
        "MMLU_jurisprudence":0.7037037037,
        "MMLU_logical_fallacies":0.6625766871,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.7777777778,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.7458492976,
        "MMLU_moral_disputes":0.6069364162,
        "MMLU_moral_scenarios":0.2893854749,
        "MMLU_nutrition":0.614379085,
        "MMLU_philosophy":0.6045016077,
        "MMLU_prehistory":0.6141975309,
        "MMLU_professional_accounting":0.3971631206,
        "MMLU_professional_law":0.3911342894,
        "MMLU_professional_medicine":0.4963235294,
        "MMLU_professional_psychology":0.5310457516,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.6448979592,
        "MMLU_sociology":0.7263681592,
        "MMLU_us_foreign_policy":0.79,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.7485380117
    },
    {
        "Model":"llama-2-13b-FINETUNE4_3.8w-r8-q_k_v_o_gate_up_down",
        "URL":"https:\/\/huggingface.co\/CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r8-q_k_v_o_gate_up_down",
        "full_model_name":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r8-q_k_v_o_gate_up_down",
        "Parameters":13.0,
        "MMLU_average":0.5441931997,
        "arc:challenge|25":0.5162116041,
        "hellaswag|10":0.6066520613,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.4222222222,
        "MMLU_astronomy":0.5394736842,
        "MMLU_business_ethics":0.5,
        "MMLU_clinical_knowledge":0.5849056604,
        "MMLU_college_biology":0.5625,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.5144508671,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.73,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.3859649123,
        "MMLU_electrical_engineering":0.4344827586,
        "MMLU_elementary_mathematics":0.3227513228,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.6,
        "MMLU_high_school_chemistry":0.4088669951,
        "MMLU_high_school_computer_science":0.56,
        "MMLU_high_school_european_history":0.6848484848,
        "MMLU_high_school_geography":0.7070707071,
        "MMLU_high_school_government_and_politics":0.7979274611,
        "MMLU_high_school_macroeconomics":0.5,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.5882352941,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.7412844037,
        "MMLU_high_school_statistics":0.4583333333,
        "MMLU_high_school_us_history":0.7696078431,
        "MMLU_high_school_world_history":0.7890295359,
        "MMLU_human_aging":0.6771300448,
        "MMLU_human_sexuality":0.6030534351,
        "MMLU_international_law":0.7438016529,
        "MMLU_jurisprudence":0.6666666667,
        "MMLU_logical_fallacies":0.6564417178,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.7087378641,
        "MMLU_marketing":0.7820512821,
        "MMLU_medical_genetics":0.68,
        "MMLU_miscellaneous":0.7560664112,
        "MMLU_moral_disputes":0.6589595376,
        "MMLU_moral_scenarios":0.2659217877,
        "MMLU_nutrition":0.5653594771,
        "MMLU_philosophy":0.6302250804,
        "MMLU_prehistory":0.6141975309,
        "MMLU_professional_accounting":0.4219858156,
        "MMLU_professional_law":0.4217731421,
        "MMLU_professional_medicine":0.5294117647,
        "MMLU_professional_psychology":0.5506535948,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.5020408163,
        "MMLU_sociology":0.6567164179,
        "MMLU_us_foreign_policy":0.76,
        "MMLU_virology":0.4337349398,
        "MMLU_world_religions":0.7660818713
    },
    {
        "Model":"iubaris-13b-v3",
        "URL":"https:\/\/huggingface.co\/kajdun\/iubaris-13b-v3",
        "full_model_name":"kajdun\/iubaris-13b-v3",
        "Parameters":13.0,
        "MMLU_average":0.5441852351,
        "arc:challenge|25":0.5563139932,
        "hellaswag|10":0.6252738498,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.5855263158,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.6037735849,
        "MMLU_college_biology":0.5555555556,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.5028901734,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.64,
        "MMLU_conceptual_physics":0.4127659574,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.3412698413,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.6451612903,
        "MMLU_high_school_chemistry":0.4433497537,
        "MMLU_high_school_computer_science":0.62,
        "MMLU_high_school_european_history":0.6545454545,
        "MMLU_high_school_geography":0.6868686869,
        "MMLU_high_school_government_and_politics":0.7875647668,
        "MMLU_high_school_macroeconomics":0.5205128205,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.5588235294,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.7247706422,
        "MMLU_high_school_statistics":0.3611111111,
        "MMLU_high_school_us_history":0.75,
        "MMLU_high_school_world_history":0.7257383966,
        "MMLU_human_aging":0.6412556054,
        "MMLU_human_sexuality":0.6030534351,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.7222222222,
        "MMLU_logical_fallacies":0.6503067485,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.8076923077,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.7318007663,
        "MMLU_moral_disputes":0.6040462428,
        "MMLU_moral_scenarios":0.2737430168,
        "MMLU_nutrition":0.6111111111,
        "MMLU_philosophy":0.5884244373,
        "MMLU_prehistory":0.6080246914,
        "MMLU_professional_accounting":0.4184397163,
        "MMLU_professional_law":0.4002607562,
        "MMLU_professional_medicine":0.5404411765,
        "MMLU_professional_psychology":0.5375816993,
        "MMLU_public_relations":0.6818181818,
        "MMLU_security_studies":0.6693877551,
        "MMLU_sociology":0.671641791,
        "MMLU_us_foreign_policy":0.76,
        "MMLU_virology":0.4638554217,
        "MMLU_world_religions":0.7192982456
    },
    {
        "Model":"Colossal-LLaMA-2-7b-base",
        "URL":"https:\/\/huggingface.co\/hpcai-tech\/Colossal-LLaMA-2-7b-base",
        "full_model_name":"hpcai-tech\/Colossal-LLaMA-2-7b-base",
        "Parameters":7.0,
        "MMLU_average":0.5440494798,
        "arc:challenge|25":0.4863481229,
        "hellaswag|10":0.526986656,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.4934210526,
        "MMLU_business_ethics":0.59,
        "MMLU_clinical_knowledge":0.5849056604,
        "MMLU_college_biology":0.5486111111,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.4971098266,
        "MMLU_college_physics":0.3823529412,
        "MMLU_computer_security":0.64,
        "MMLU_conceptual_physics":0.4638297872,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.5517241379,
        "MMLU_elementary_mathematics":0.335978836,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.6451612903,
        "MMLU_high_school_chemistry":0.4482758621,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.703030303,
        "MMLU_high_school_geography":0.702020202,
        "MMLU_high_school_government_and_politics":0.7720207254,
        "MMLU_high_school_macroeconomics":0.5230769231,
        "MMLU_high_school_mathematics":0.2888888889,
        "MMLU_high_school_microeconomics":0.5882352941,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.7504587156,
        "MMLU_high_school_statistics":0.4027777778,
        "MMLU_high_school_us_history":0.7303921569,
        "MMLU_high_school_world_history":0.7805907173,
        "MMLU_human_aging":0.6053811659,
        "MMLU_human_sexuality":0.6564885496,
        "MMLU_international_law":0.7190082645,
        "MMLU_jurisprudence":0.5925925926,
        "MMLU_logical_fallacies":0.5950920245,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.7905982906,
        "MMLU_medical_genetics":0.66,
        "MMLU_miscellaneous":0.7151979566,
        "MMLU_moral_disputes":0.5780346821,
        "MMLU_moral_scenarios":0.2748603352,
        "MMLU_nutrition":0.5882352941,
        "MMLU_philosophy":0.61414791,
        "MMLU_prehistory":0.6049382716,
        "MMLU_professional_accounting":0.4113475177,
        "MMLU_professional_law":0.4139504563,
        "MMLU_professional_medicine":0.4779411765,
        "MMLU_professional_psychology":0.5375816993,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.6530612245,
        "MMLU_sociology":0.7562189055,
        "MMLU_us_foreign_policy":0.75,
        "MMLU_virology":0.4397590361,
        "MMLU_world_religions":0.730994152
    },
    {
        "Model":"llama13B-quant8-testv1-openorca-customdataset",
        "URL":"https:\/\/huggingface.co\/IGeniusDev\/llama13B-quant8-testv1-openorca-customdataset",
        "full_model_name":"IGeniusDev\/llama13B-quant8-testv1-openorca-customdataset",
        "Parameters":13.0,
        "MMLU_average":0.543336775,
        "arc:challenge|25":0.5605802048,
        "hellaswag|10":0.6236805417,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.5259259259,
        "MMLU_astronomy":0.5065789474,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.5735849057,
        "MMLU_college_biology":0.5902777778,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.5028901734,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.4127659574,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.5172413793,
        "MMLU_elementary_mathematics":0.3306878307,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.4,
        "MMLU_high_school_biology":0.6258064516,
        "MMLU_high_school_chemistry":0.4236453202,
        "MMLU_high_school_computer_science":0.49,
        "MMLU_high_school_european_history":0.6606060606,
        "MMLU_high_school_geography":0.6868686869,
        "MMLU_high_school_government_and_politics":0.7461139896,
        "MMLU_high_school_macroeconomics":0.5153846154,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.5504201681,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.7321100917,
        "MMLU_high_school_statistics":0.3703703704,
        "MMLU_high_school_us_history":0.7696078431,
        "MMLU_high_school_world_history":0.7215189873,
        "MMLU_human_aging":0.6322869955,
        "MMLU_human_sexuality":0.5954198473,
        "MMLU_international_law":0.7438016529,
        "MMLU_jurisprudence":0.7037037037,
        "MMLU_logical_fallacies":0.6687116564,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.7669902913,
        "MMLU_marketing":0.8034188034,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.7471264368,
        "MMLU_moral_disputes":0.6271676301,
        "MMLU_moral_scenarios":0.3072625698,
        "MMLU_nutrition":0.6209150327,
        "MMLU_philosophy":0.6334405145,
        "MMLU_prehistory":0.6172839506,
        "MMLU_professional_accounting":0.3794326241,
        "MMLU_professional_law":0.4048239896,
        "MMLU_professional_medicine":0.5147058824,
        "MMLU_professional_psychology":0.5408496732,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.5959183673,
        "MMLU_sociology":0.7114427861,
        "MMLU_us_foreign_policy":0.74,
        "MMLU_virology":0.4397590361,
        "MMLU_world_religions":0.7543859649
    },
    {
        "Model":"llama-2-13b-FINETUNE3_3.3w-r4-q_k_v_o_gate_up_down",
        "URL":"https:\/\/huggingface.co\/CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r4-q_k_v_o_gate_up_down",
        "full_model_name":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r4-q_k_v_o_gate_up_down",
        "Parameters":13.0,
        "MMLU_average":0.5431818974,
        "arc:challenge|25":0.5324232082,
        "hellaswag|10":0.6043616809,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.5328947368,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.5924528302,
        "MMLU_college_biology":0.6041666667,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.5144508671,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.72,
        "MMLU_conceptual_physics":0.4382978723,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.3333333333,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.6322580645,
        "MMLU_high_school_chemistry":0.4679802956,
        "MMLU_high_school_computer_science":0.53,
        "MMLU_high_school_european_history":0.6787878788,
        "MMLU_high_school_geography":0.696969697,
        "MMLU_high_school_government_and_politics":0.7616580311,
        "MMLU_high_school_macroeconomics":0.4846153846,
        "MMLU_high_school_mathematics":0.362962963,
        "MMLU_high_school_microeconomics":0.6008403361,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.719266055,
        "MMLU_high_school_statistics":0.4166666667,
        "MMLU_high_school_us_history":0.7450980392,
        "MMLU_high_school_world_history":0.7172995781,
        "MMLU_human_aging":0.6412556054,
        "MMLU_human_sexuality":0.5419847328,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.75,
        "MMLU_logical_fallacies":0.5950920245,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.7564102564,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.7611749681,
        "MMLU_moral_disputes":0.6098265896,
        "MMLU_moral_scenarios":0.3027932961,
        "MMLU_nutrition":0.6013071895,
        "MMLU_philosophy":0.6398713826,
        "MMLU_prehistory":0.5864197531,
        "MMLU_professional_accounting":0.4184397163,
        "MMLU_professional_law":0.4198174707,
        "MMLU_professional_medicine":0.5220588235,
        "MMLU_professional_psychology":0.5490196078,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.5632653061,
        "MMLU_sociology":0.6965174129,
        "MMLU_us_foreign_policy":0.75,
        "MMLU_virology":0.4337349398,
        "MMLU_world_religions":0.783625731
    },
    {
        "Model":"Llama-2-13b-ft-instruct-es",
        "URL":"https:\/\/huggingface.co\/clibrain\/Llama-2-13b-ft-instruct-es",
        "full_model_name":"clibrain\/Llama-2-13b-ft-instruct-es",
        "Parameters":13.0,
        "MMLU_average":0.5431099108,
        "arc:challenge|25":0.5580204778,
        "hellaswag|10":0.6141206931,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.5245283019,
        "MMLU_college_biology":0.5486111111,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.5,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4797687861,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.4425531915,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.5172413793,
        "MMLU_elementary_mathematics":0.3412698413,
        "MMLU_formal_logic":0.380952381,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.6419354839,
        "MMLU_high_school_chemistry":0.4334975369,
        "MMLU_high_school_computer_science":0.63,
        "MMLU_high_school_european_history":0.6606060606,
        "MMLU_high_school_geography":0.6767676768,
        "MMLU_high_school_government_and_politics":0.7668393782,
        "MMLU_high_school_macroeconomics":0.5153846154,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.5546218487,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.7064220183,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.7401960784,
        "MMLU_high_school_world_history":0.7257383966,
        "MMLU_human_aging":0.6547085202,
        "MMLU_human_sexuality":0.5572519084,
        "MMLU_international_law":0.6694214876,
        "MMLU_jurisprudence":0.6666666667,
        "MMLU_logical_fallacies":0.6073619632,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.7521367521,
        "MMLU_medical_genetics":0.63,
        "MMLU_miscellaneous":0.7266922095,
        "MMLU_moral_disputes":0.598265896,
        "MMLU_moral_scenarios":0.3642458101,
        "MMLU_nutrition":0.5947712418,
        "MMLU_philosophy":0.6270096463,
        "MMLU_prehistory":0.5956790123,
        "MMLU_professional_accounting":0.3723404255,
        "MMLU_professional_law":0.4113428944,
        "MMLU_professional_medicine":0.5220588235,
        "MMLU_professional_psychology":0.5441176471,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.6,
        "MMLU_sociology":0.7910447761,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.730994152
    },
    {
        "Model":"llama-2-13b-FINETUNE1_17w",
        "URL":"https:\/\/huggingface.co\/CHIH-HUNG\/llama-2-13b-FINETUNE1_17w",
        "full_model_name":"CHIH-HUNG\/llama-2-13b-FINETUNE1_17w",
        "Parameters":13.0,
        "MMLU_average":0.5431064215,
        "arc:challenge|25":0.5520477816,
        "hellaswag|10":0.6043616809,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.5131578947,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.5811320755,
        "MMLU_college_biology":0.5555555556,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.49,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.4913294798,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.75,
        "MMLU_conceptual_physics":0.3787234043,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.4965517241,
        "MMLU_elementary_mathematics":0.328042328,
        "MMLU_formal_logic":0.3412698413,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.6129032258,
        "MMLU_high_school_chemistry":0.4236453202,
        "MMLU_high_school_computer_science":0.54,
        "MMLU_high_school_european_history":0.6848484848,
        "MMLU_high_school_geography":0.6717171717,
        "MMLU_high_school_government_and_politics":0.8186528497,
        "MMLU_high_school_macroeconomics":0.5435897436,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.5630252101,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.7577981651,
        "MMLU_high_school_statistics":0.4351851852,
        "MMLU_high_school_us_history":0.7352941176,
        "MMLU_high_school_world_history":0.7299578059,
        "MMLU_human_aging":0.6322869955,
        "MMLU_human_sexuality":0.5954198473,
        "MMLU_international_law":0.694214876,
        "MMLU_jurisprudence":0.6759259259,
        "MMLU_logical_fallacies":0.6625766871,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.8034188034,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7650063857,
        "MMLU_moral_disputes":0.6184971098,
        "MMLU_moral_scenarios":0.2659217877,
        "MMLU_nutrition":0.5947712418,
        "MMLU_philosophy":0.6237942122,
        "MMLU_prehistory":0.6419753086,
        "MMLU_professional_accounting":0.4078014184,
        "MMLU_professional_law":0.3950456323,
        "MMLU_professional_medicine":0.5073529412,
        "MMLU_professional_psychology":0.5490196078,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.5795918367,
        "MMLU_sociology":0.7462686567,
        "MMLU_us_foreign_policy":0.77,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.8187134503
    },
    {
        "Model":"Philosophy-Platypus2-13b",
        "URL":"https:\/\/huggingface.co\/radm\/Philosophy-Platypus2-13b",
        "full_model_name":"radm\/Philosophy-Platypus2-13b",
        "Parameters":13.0,
        "MMLU_average":0.5430431297,
        "arc:challenge|25":0.54778157,
        "hellaswag|10":0.5828520215,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.4222222222,
        "MMLU_astronomy":0.5986842105,
        "MMLU_business_ethics":0.49,
        "MMLU_clinical_knowledge":0.5924528302,
        "MMLU_college_biology":0.6388888889,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.4739884393,
        "MMLU_college_physics":0.362745098,
        "MMLU_computer_security":0.65,
        "MMLU_conceptual_physics":0.4340425532,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.3756613757,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.664516129,
        "MMLU_high_school_chemistry":0.4137931034,
        "MMLU_high_school_computer_science":0.49,
        "MMLU_high_school_european_history":0.6787878788,
        "MMLU_high_school_geography":0.7323232323,
        "MMLU_high_school_government_and_politics":0.7979274611,
        "MMLU_high_school_macroeconomics":0.6,
        "MMLU_high_school_mathematics":0.3444444444,
        "MMLU_high_school_microeconomics":0.5630252101,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.7688073394,
        "MMLU_high_school_statistics":0.4351851852,
        "MMLU_high_school_us_history":0.75,
        "MMLU_high_school_world_history":0.7215189873,
        "MMLU_human_aging":0.6053811659,
        "MMLU_human_sexuality":0.5954198473,
        "MMLU_international_law":0.6611570248,
        "MMLU_jurisprudence":0.6388888889,
        "MMLU_logical_fallacies":0.5950920245,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.6990291262,
        "MMLU_marketing":0.7008547009,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.7420178799,
        "MMLU_moral_disputes":0.598265896,
        "MMLU_moral_scenarios":0.3463687151,
        "MMLU_nutrition":0.5816993464,
        "MMLU_philosophy":0.6366559486,
        "MMLU_prehistory":0.6388888889,
        "MMLU_professional_accounting":0.4184397163,
        "MMLU_professional_law":0.4035202086,
        "MMLU_professional_medicine":0.5441176471,
        "MMLU_professional_psychology":0.5343137255,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.5959183673,
        "MMLU_sociology":0.7164179104,
        "MMLU_us_foreign_policy":0.78,
        "MMLU_virology":0.4638554217,
        "MMLU_world_religions":0.7602339181
    },
    {
        "Model":"genz-13b-v2",
        "URL":"https:\/\/huggingface.co\/budecosystem\/genz-13b-v2",
        "full_model_name":"budecosystem\/genz-13b-v2",
        "Parameters":13.0,
        "MMLU_average":0.5429976497,
        "arc:challenge|25":0.5324232082,
        "hellaswag|10":0.6072495519,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.5263157895,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.5924528302,
        "MMLU_college_biology":0.6041666667,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5028901734,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.6,
        "MMLU_conceptual_physics":0.4680851064,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.4827586207,
        "MMLU_elementary_mathematics":0.3201058201,
        "MMLU_formal_logic":0.3412698413,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.6,
        "MMLU_high_school_chemistry":0.4334975369,
        "MMLU_high_school_computer_science":0.48,
        "MMLU_high_school_european_history":0.6545454545,
        "MMLU_high_school_geography":0.6767676768,
        "MMLU_high_school_government_and_politics":0.7720207254,
        "MMLU_high_school_macroeconomics":0.4871794872,
        "MMLU_high_school_mathematics":0.3296296296,
        "MMLU_high_school_microeconomics":0.5462184874,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.7247706422,
        "MMLU_high_school_statistics":0.4814814815,
        "MMLU_high_school_us_history":0.7401960784,
        "MMLU_high_school_world_history":0.6877637131,
        "MMLU_human_aging":0.668161435,
        "MMLU_human_sexuality":0.6259541985,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.6851851852,
        "MMLU_logical_fallacies":0.7055214724,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.6990291262,
        "MMLU_marketing":0.8247863248,
        "MMLU_medical_genetics":0.51,
        "MMLU_miscellaneous":0.7164750958,
        "MMLU_moral_disputes":0.6184971098,
        "MMLU_moral_scenarios":0.3251396648,
        "MMLU_nutrition":0.5784313725,
        "MMLU_philosophy":0.5980707395,
        "MMLU_prehistory":0.6203703704,
        "MMLU_professional_accounting":0.4007092199,
        "MMLU_professional_law":0.4191655802,
        "MMLU_professional_medicine":0.5036764706,
        "MMLU_professional_psychology":0.5261437908,
        "MMLU_public_relations":0.6272727273,
        "MMLU_security_studies":0.6204081633,
        "MMLU_sociology":0.6865671642,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.7192982456
    },
    {
        "Model":"LLAMA-13B-test-finetuning",
        "URL":"https:\/\/huggingface.co\/iGenius-AI-Team\/LLAMA-13B-test-finetuning",
        "full_model_name":"iGenius-AI-Team\/LLAMA-13B-test-finetuning",
        "Parameters":13.0,
        "MMLU_average":0.5427386831,
        "arc:challenge|25":0.545221843,
        "hellaswag|10":0.6175064728,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.5526315789,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.6075471698,
        "MMLU_college_biology":0.5763888889,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.4855491329,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.4510638298,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.4896551724,
        "MMLU_elementary_mathematics":0.2989417989,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.28,
        "MMLU_high_school_biology":0.635483871,
        "MMLU_high_school_chemistry":0.4039408867,
        "MMLU_high_school_computer_science":0.58,
        "MMLU_high_school_european_history":0.6909090909,
        "MMLU_high_school_geography":0.6767676768,
        "MMLU_high_school_government_and_politics":0.7823834197,
        "MMLU_high_school_macroeconomics":0.4974358974,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.5588235294,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.752293578,
        "MMLU_high_school_statistics":0.4212962963,
        "MMLU_high_school_us_history":0.75,
        "MMLU_high_school_world_history":0.7383966245,
        "MMLU_human_aging":0.66367713,
        "MMLU_human_sexuality":0.5877862595,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.6851851852,
        "MMLU_logical_fallacies":0.6564417178,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.7777777778,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.7458492976,
        "MMLU_moral_disputes":0.6184971098,
        "MMLU_moral_scenarios":0.3195530726,
        "MMLU_nutrition":0.6274509804,
        "MMLU_philosophy":0.6463022508,
        "MMLU_prehistory":0.6358024691,
        "MMLU_professional_accounting":0.3971631206,
        "MMLU_professional_law":0.406779661,
        "MMLU_professional_medicine":0.4632352941,
        "MMLU_professional_psychology":0.5620915033,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.5755102041,
        "MMLU_sociology":0.7512437811,
        "MMLU_us_foreign_policy":0.78,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.7660818713
    },
    {
        "Model":"llama-2-13b-FINETUNE2_3w",
        "URL":"https:\/\/huggingface.co\/CHIH-HUNG\/llama-2-13b-FINETUNE2_3w",
        "full_model_name":"CHIH-HUNG\/llama-2-13b-FINETUNE2_3w",
        "Parameters":13.0,
        "MMLU_average":0.5425467489,
        "arc:challenge|25":0.542662116,
        "hellaswag|10":0.6145190201,
        "MMLU_abstract_algebra":0.39,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5723684211,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.6188679245,
        "MMLU_college_biology":0.5694444444,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.4971098266,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.429787234,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.3333333333,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.6258064516,
        "MMLU_high_school_chemistry":0.4236453202,
        "MMLU_high_school_computer_science":0.58,
        "MMLU_high_school_european_history":0.6666666667,
        "MMLU_high_school_geography":0.6818181818,
        "MMLU_high_school_government_and_politics":0.8082901554,
        "MMLU_high_school_macroeconomics":0.4974358974,
        "MMLU_high_school_mathematics":0.3222222222,
        "MMLU_high_school_microeconomics":0.5294117647,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.7247706422,
        "MMLU_high_school_statistics":0.3888888889,
        "MMLU_high_school_us_history":0.7352941176,
        "MMLU_high_school_world_history":0.7257383966,
        "MMLU_human_aging":0.5964125561,
        "MMLU_human_sexuality":0.5801526718,
        "MMLU_international_law":0.7190082645,
        "MMLU_jurisprudence":0.6851851852,
        "MMLU_logical_fallacies":0.6196319018,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.811965812,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.7765006386,
        "MMLU_moral_disputes":0.6242774566,
        "MMLU_moral_scenarios":0.2446927374,
        "MMLU_nutrition":0.5620915033,
        "MMLU_philosophy":0.6302250804,
        "MMLU_prehistory":0.6604938272,
        "MMLU_professional_accounting":0.414893617,
        "MMLU_professional_law":0.3956975228,
        "MMLU_professional_medicine":0.4926470588,
        "MMLU_professional_psychology":0.5408496732,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.5714285714,
        "MMLU_sociology":0.7263681592,
        "MMLU_us_foreign_policy":0.77,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.7602339181
    },
    {
        "Model":"Samantha-Nebula-7B",
        "URL":"https:\/\/huggingface.co\/Weyaxi\/Samantha-Nebula-7B",
        "full_model_name":"Weyaxi\/Samantha-Nebula-7B",
        "Parameters":7.0,
        "MMLU_average":0.5421190258,
        "arc:challenge|25":0.5264505119,
        "hellaswag|10":0.6335391356,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.5333333333,
        "MMLU_astronomy":0.5855263158,
        "MMLU_business_ethics":0.44,
        "MMLU_clinical_knowledge":0.5698113208,
        "MMLU_college_biology":0.5833333333,
        "MMLU_college_chemistry":0.36,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5260115607,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.64,
        "MMLU_conceptual_physics":0.5063829787,
        "MMLU_econometrics":0.3771929825,
        "MMLU_electrical_engineering":0.4482758621,
        "MMLU_elementary_mathematics":0.3703703704,
        "MMLU_formal_logic":0.4285714286,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.6258064516,
        "MMLU_high_school_chemistry":0.4433497537,
        "MMLU_high_school_computer_science":0.54,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.6818181818,
        "MMLU_high_school_government_and_politics":0.792746114,
        "MMLU_high_school_macroeconomics":0.5230769231,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.5336134454,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.7486238532,
        "MMLU_high_school_statistics":0.3611111111,
        "MMLU_high_school_us_history":0.6862745098,
        "MMLU_high_school_world_history":0.7046413502,
        "MMLU_human_aging":0.6143497758,
        "MMLU_human_sexuality":0.5572519084,
        "MMLU_international_law":0.6446280992,
        "MMLU_jurisprudence":0.6018518519,
        "MMLU_logical_fallacies":0.6380368098,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.6699029126,
        "MMLU_marketing":0.7991452991,
        "MMLU_medical_genetics":0.61,
        "MMLU_miscellaneous":0.7586206897,
        "MMLU_moral_disputes":0.6069364162,
        "MMLU_moral_scenarios":0.2335195531,
        "MMLU_nutrition":0.5947712418,
        "MMLU_philosophy":0.6688102894,
        "MMLU_prehistory":0.6512345679,
        "MMLU_professional_accounting":0.4290780142,
        "MMLU_professional_law":0.4146023468,
        "MMLU_professional_medicine":0.5257352941,
        "MMLU_professional_psychology":0.5784313725,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.5183673469,
        "MMLU_sociology":0.7313432836,
        "MMLU_us_foreign_policy":0.8,
        "MMLU_virology":0.5120481928,
        "MMLU_world_religions":0.7485380117
    },
    {
        "Model":"GodziLLa-30B",
        "URL":"https:\/\/huggingface.co\/MayaPH\/GodziLLa-30B",
        "full_model_name":"MayaPH\/GodziLLa-30B",
        "Parameters":30.0,
        "MMLU_average":0.5421152844,
        "arc:challenge|25":0.5955631399,
        "hellaswag|10":0.6142202748,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.5185185185,
        "MMLU_astronomy":0.6052631579,
        "MMLU_business_ethics":0.61,
        "MMLU_clinical_knowledge":0.558490566,
        "MMLU_college_biology":0.6388888889,
        "MMLU_college_chemistry":0.33,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.4855491329,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.4808510638,
        "MMLU_econometrics":0.350877193,
        "MMLU_electrical_engineering":0.4,
        "MMLU_elementary_mathematics":0.3703703704,
        "MMLU_formal_logic":0.3412698413,
        "MMLU_global_facts":0.4,
        "MMLU_high_school_biology":0.6451612903,
        "MMLU_high_school_chemistry":0.3990147783,
        "MMLU_high_school_computer_science":0.59,
        "MMLU_high_school_european_history":0.6909090909,
        "MMLU_high_school_geography":0.7121212121,
        "MMLU_high_school_government_and_politics":0.7823834197,
        "MMLU_high_school_macroeconomics":0.5051282051,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.5756302521,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.7064220183,
        "MMLU_high_school_statistics":0.4212962963,
        "MMLU_high_school_us_history":0.7450980392,
        "MMLU_high_school_world_history":0.7637130802,
        "MMLU_human_aging":0.6457399103,
        "MMLU_human_sexuality":0.5419847328,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.6851851852,
        "MMLU_logical_fallacies":0.6441717791,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.7087378641,
        "MMLU_marketing":0.7948717949,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.7471264368,
        "MMLU_moral_disputes":0.5867052023,
        "MMLU_moral_scenarios":0.2659217877,
        "MMLU_nutrition":0.5555555556,
        "MMLU_philosophy":0.6430868167,
        "MMLU_prehistory":0.5895061728,
        "MMLU_professional_accounting":0.4361702128,
        "MMLU_professional_law":0.4074315515,
        "MMLU_professional_medicine":0.5257352941,
        "MMLU_professional_psychology":0.5702614379,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.5102040816,
        "MMLU_sociology":0.6766169154,
        "MMLU_us_foreign_policy":0.76,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.730994152
    },
    {
        "Model":"CodeEngine",
        "URL":"https:\/\/huggingface.co\/Undi95\/CodeEngine",
        "full_model_name":"Undi95\/CodeEngine",
        "Parameters":null,
        "MMLU_average":0.5417882809,
        "arc:challenge|25":0.5494880546,
        "hellaswag|10":0.624377614,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.5037037037,
        "MMLU_astronomy":0.5131578947,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.5849056604,
        "MMLU_college_biology":0.5625,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.53,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5028901734,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.3957446809,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.5310344828,
        "MMLU_elementary_mathematics":0.3015873016,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.6322580645,
        "MMLU_high_school_chemistry":0.4679802956,
        "MMLU_high_school_computer_science":0.57,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.6717171717,
        "MMLU_high_school_government_and_politics":0.7564766839,
        "MMLU_high_school_macroeconomics":0.5,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.487394958,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7137614679,
        "MMLU_high_school_statistics":0.4212962963,
        "MMLU_high_school_us_history":0.75,
        "MMLU_high_school_world_history":0.7130801688,
        "MMLU_human_aging":0.6322869955,
        "MMLU_human_sexuality":0.6183206107,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.7037037037,
        "MMLU_logical_fallacies":0.6809815951,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.6601941748,
        "MMLU_marketing":0.7393162393,
        "MMLU_medical_genetics":0.53,
        "MMLU_miscellaneous":0.7318007663,
        "MMLU_moral_disputes":0.598265896,
        "MMLU_moral_scenarios":0.3519553073,
        "MMLU_nutrition":0.6013071895,
        "MMLU_philosophy":0.61414791,
        "MMLU_prehistory":0.6481481481,
        "MMLU_professional_accounting":0.3865248227,
        "MMLU_professional_law":0.3846153846,
        "MMLU_professional_medicine":0.4926470588,
        "MMLU_professional_psychology":0.5343137255,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6081632653,
        "MMLU_sociology":0.7412935323,
        "MMLU_us_foreign_policy":0.78,
        "MMLU_virology":0.4397590361,
        "MMLU_world_religions":0.730994152
    },
    {
        "Model":"llama2-13b-chinese-v1",
        "URL":"https:\/\/huggingface.co\/gywy\/llama2-13b-chinese-v1",
        "full_model_name":"gywy\/llama2-13b-chinese-v1",
        "Parameters":13.0,
        "MMLU_average":0.5417811181,
        "arc:challenge|25":0.5631399317,
        "hellaswag|10":0.5381398128,
        "MMLU_abstract_algebra":0.4,
        "MMLU_anatomy":0.5037037037,
        "MMLU_astronomy":0.5657894737,
        "MMLU_business_ethics":0.5,
        "MMLU_clinical_knowledge":0.5924528302,
        "MMLU_college_biology":0.5555555556,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.5086705202,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.3201058201,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.6612903226,
        "MMLU_high_school_chemistry":0.4334975369,
        "MMLU_high_school_computer_science":0.52,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.6868686869,
        "MMLU_high_school_government_and_politics":0.7772020725,
        "MMLU_high_school_macroeconomics":0.4846153846,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.6008403361,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.7009174312,
        "MMLU_high_school_statistics":0.4027777778,
        "MMLU_high_school_us_history":0.7401960784,
        "MMLU_high_school_world_history":0.7426160338,
        "MMLU_human_aging":0.66367713,
        "MMLU_human_sexuality":0.6335877863,
        "MMLU_international_law":0.7190082645,
        "MMLU_jurisprudence":0.712962963,
        "MMLU_logical_fallacies":0.6134969325,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.7087378641,
        "MMLU_marketing":0.7564102564,
        "MMLU_medical_genetics":0.48,
        "MMLU_miscellaneous":0.7254150702,
        "MMLU_moral_disputes":0.6098265896,
        "MMLU_moral_scenarios":0.3709497207,
        "MMLU_nutrition":0.5784313725,
        "MMLU_philosophy":0.6237942122,
        "MMLU_prehistory":0.5740740741,
        "MMLU_professional_accounting":0.4432624113,
        "MMLU_professional_law":0.3970013038,
        "MMLU_professional_medicine":0.5073529412,
        "MMLU_professional_psychology":0.5375816993,
        "MMLU_public_relations":0.6,
        "MMLU_security_studies":0.612244898,
        "MMLU_sociology":0.7263681592,
        "MMLU_us_foreign_policy":0.77,
        "MMLU_virology":0.4759036145,
        "MMLU_world_religions":0.7602339181
    },
    {
        "Model":"QuantumLM",
        "URL":"https:\/\/huggingface.co\/quantumaikr\/QuantumLM",
        "full_model_name":"quantumaikr\/QuantumLM",
        "Parameters":null,
        "MMLU_average":0.5416580221,
        "arc:challenge|25":0.5093856655,
        "hellaswag|10":0.5990838478,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5394736842,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.5622641509,
        "MMLU_college_biology":0.5347222222,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.5,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.4104046243,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.4085106383,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.5172413793,
        "MMLU_elementary_mathematics":0.3306878307,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.6322580645,
        "MMLU_high_school_chemistry":0.4039408867,
        "MMLU_high_school_computer_science":0.6,
        "MMLU_high_school_european_history":0.6424242424,
        "MMLU_high_school_geography":0.702020202,
        "MMLU_high_school_government_and_politics":0.7616580311,
        "MMLU_high_school_macroeconomics":0.4948717949,
        "MMLU_high_school_mathematics":0.3222222222,
        "MMLU_high_school_microeconomics":0.5294117647,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.7266055046,
        "MMLU_high_school_statistics":0.3935185185,
        "MMLU_high_school_us_history":0.7303921569,
        "MMLU_high_school_world_history":0.7046413502,
        "MMLU_human_aging":0.6457399103,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.7438016529,
        "MMLU_jurisprudence":0.6944444444,
        "MMLU_logical_fallacies":0.6380368098,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.7863247863,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7420178799,
        "MMLU_moral_disputes":0.612716763,
        "MMLU_moral_scenarios":0.3530726257,
        "MMLU_nutrition":0.5980392157,
        "MMLU_philosophy":0.61414791,
        "MMLU_prehistory":0.6018518519,
        "MMLU_professional_accounting":0.3581560284,
        "MMLU_professional_law":0.3722294654,
        "MMLU_professional_medicine":0.5036764706,
        "MMLU_professional_psychology":0.5130718954,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.6081632653,
        "MMLU_sociology":0.7114427861,
        "MMLU_us_foreign_policy":0.77,
        "MMLU_virology":0.5,
        "MMLU_world_religions":0.7368421053
    },
    {
        "Model":"Alpagasus-2-13B-QLoRA-pipeline",
        "URL":"https:\/\/huggingface.co\/StudentLLM\/Alpagasus-2-13B-QLoRA-pipeline",
        "full_model_name":"StudentLLM\/Alpagasus-2-13B-QLoRA-pipeline",
        "Parameters":13.0,
        "MMLU_average":0.5414434079,
        "arc:challenge|25":0.5401023891,
        "hellaswag|10":0.6054570803,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.5333333333,
        "MMLU_astronomy":0.5065789474,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.5849056604,
        "MMLU_college_biology":0.5763888889,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.5144508671,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.4553191489,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.4896551724,
        "MMLU_elementary_mathematics":0.335978836,
        "MMLU_formal_logic":0.3412698413,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.6483870968,
        "MMLU_high_school_chemistry":0.4581280788,
        "MMLU_high_school_computer_science":0.53,
        "MMLU_high_school_european_history":0.6424242424,
        "MMLU_high_school_geography":0.6767676768,
        "MMLU_high_school_government_and_politics":0.7772020725,
        "MMLU_high_school_macroeconomics":0.4717948718,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.5504201681,
        "MMLU_high_school_physics":0.3774834437,
        "MMLU_high_school_psychology":0.7229357798,
        "MMLU_high_school_statistics":0.4583333333,
        "MMLU_high_school_us_history":0.6960784314,
        "MMLU_high_school_world_history":0.7215189873,
        "MMLU_human_aging":0.6547085202,
        "MMLU_human_sexuality":0.6183206107,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.6851851852,
        "MMLU_logical_fallacies":0.6809815951,
        "MMLU_machine_learning":0.2410714286,
        "MMLU_management":0.7572815534,
        "MMLU_marketing":0.7905982906,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.7266922095,
        "MMLU_moral_disputes":0.6387283237,
        "MMLU_moral_scenarios":0.261452514,
        "MMLU_nutrition":0.5849673203,
        "MMLU_philosophy":0.6463022508,
        "MMLU_prehistory":0.6203703704,
        "MMLU_professional_accounting":0.3936170213,
        "MMLU_professional_law":0.4165580183,
        "MMLU_professional_medicine":0.5110294118,
        "MMLU_professional_psychology":0.545751634,
        "MMLU_public_relations":0.5727272727,
        "MMLU_security_studies":0.6163265306,
        "MMLU_sociology":0.7213930348,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.7368421053
    },
    {
        "Model":"ReMM-L2-13B",
        "URL":"https:\/\/huggingface.co\/Undi95\/ReMM-L2-13B",
        "full_model_name":"Undi95\/ReMM-L2-13B",
        "Parameters":13.0,
        "MMLU_average":0.5411462693,
        "arc:challenge|25":0.5716723549,
        "hellaswag|10":0.6375224059,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.5,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.6037735849,
        "MMLU_college_biology":0.5972222222,
        "MMLU_college_chemistry":0.36,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.5028901734,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.5103448276,
        "MMLU_elementary_mathematics":0.328042328,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.6258064516,
        "MMLU_high_school_chemistry":0.4088669951,
        "MMLU_high_school_computer_science":0.51,
        "MMLU_high_school_european_history":0.6666666667,
        "MMLU_high_school_geography":0.6767676768,
        "MMLU_high_school_government_and_politics":0.7875647668,
        "MMLU_high_school_macroeconomics":0.5153846154,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.525210084,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.7119266055,
        "MMLU_high_school_statistics":0.3888888889,
        "MMLU_high_school_us_history":0.7745098039,
        "MMLU_high_school_world_history":0.7130801688,
        "MMLU_human_aging":0.66367713,
        "MMLU_human_sexuality":0.6564885496,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.7314814815,
        "MMLU_logical_fallacies":0.6625766871,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.6504854369,
        "MMLU_marketing":0.764957265,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.7343550447,
        "MMLU_moral_disputes":0.6300578035,
        "MMLU_moral_scenarios":0.3016759777,
        "MMLU_nutrition":0.614379085,
        "MMLU_philosophy":0.6366559486,
        "MMLU_prehistory":0.6141975309,
        "MMLU_professional_accounting":0.4078014184,
        "MMLU_professional_law":0.4211212516,
        "MMLU_professional_medicine":0.5036764706,
        "MMLU_professional_psychology":0.5522875817,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6163265306,
        "MMLU_sociology":0.7014925373,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.4397590361,
        "MMLU_world_religions":0.730994152
    },
    {
        "Model":"ReMM-L2-13B-PIPPA",
        "URL":"https:\/\/huggingface.co\/Undi95\/ReMM-L2-13B-PIPPA",
        "full_model_name":"Undi95\/ReMM-L2-13B-PIPPA",
        "Parameters":13.0,
        "MMLU_average":0.5410079927,
        "arc:challenge|25":0.5725255973,
        "hellaswag|10":0.6374228241,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.4934210526,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.6075471698,
        "MMLU_college_biology":0.5972222222,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.5028901734,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.5103448276,
        "MMLU_elementary_mathematics":0.328042328,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.6258064516,
        "MMLU_high_school_chemistry":0.4088669951,
        "MMLU_high_school_computer_science":0.51,
        "MMLU_high_school_european_history":0.6666666667,
        "MMLU_high_school_geography":0.6818181818,
        "MMLU_high_school_government_and_politics":0.7875647668,
        "MMLU_high_school_macroeconomics":0.5179487179,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.525210084,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.7100917431,
        "MMLU_high_school_statistics":0.3888888889,
        "MMLU_high_school_us_history":0.7696078431,
        "MMLU_high_school_world_history":0.7130801688,
        "MMLU_human_aging":0.66367713,
        "MMLU_human_sexuality":0.6564885496,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.7314814815,
        "MMLU_logical_fallacies":0.6625766871,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.6504854369,
        "MMLU_marketing":0.7692307692,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.7356321839,
        "MMLU_moral_disputes":0.6329479769,
        "MMLU_moral_scenarios":0.3039106145,
        "MMLU_nutrition":0.614379085,
        "MMLU_philosophy":0.6366559486,
        "MMLU_prehistory":0.6141975309,
        "MMLU_professional_accounting":0.4042553191,
        "MMLU_professional_law":0.4211212516,
        "MMLU_professional_medicine":0.5073529412,
        "MMLU_professional_psychology":0.5539215686,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6204081633,
        "MMLU_sociology":0.7014925373,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.4397590361,
        "MMLU_world_religions":0.730994152
    },
    {
        "Model":"llama-2-13b-FINETUNE3_3.3w-r8-gate_up_down",
        "URL":"https:\/\/huggingface.co\/CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r8-gate_up_down",
        "full_model_name":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r8-gate_up_down",
        "Parameters":13.0,
        "MMLU_average":0.5396495793,
        "arc:challenge|25":0.5332764505,
        "hellaswag|10":0.6108344951,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5394736842,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.6075471698,
        "MMLU_college_biology":0.5625,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.5028901734,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.429787234,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.4482758621,
        "MMLU_elementary_mathematics":0.3333333333,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.28,
        "MMLU_high_school_biology":0.6419354839,
        "MMLU_high_school_chemistry":0.4285714286,
        "MMLU_high_school_computer_science":0.54,
        "MMLU_high_school_european_history":0.6909090909,
        "MMLU_high_school_geography":0.7121212121,
        "MMLU_high_school_government_and_politics":0.7564766839,
        "MMLU_high_school_macroeconomics":0.4820512821,
        "MMLU_high_school_mathematics":0.3407407407,
        "MMLU_high_school_microeconomics":0.5546218487,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.7211009174,
        "MMLU_high_school_statistics":0.4074074074,
        "MMLU_high_school_us_history":0.7598039216,
        "MMLU_high_school_world_history":0.7426160338,
        "MMLU_human_aging":0.6547085202,
        "MMLU_human_sexuality":0.5496183206,
        "MMLU_international_law":0.7685950413,
        "MMLU_jurisprudence":0.712962963,
        "MMLU_logical_fallacies":0.6748466258,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.7735042735,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.7509578544,
        "MMLU_moral_disputes":0.612716763,
        "MMLU_moral_scenarios":0.3329608939,
        "MMLU_nutrition":0.5718954248,
        "MMLU_philosophy":0.6398713826,
        "MMLU_prehistory":0.6358024691,
        "MMLU_professional_accounting":0.4432624113,
        "MMLU_professional_law":0.4100391134,
        "MMLU_professional_medicine":0.5257352941,
        "MMLU_professional_psychology":0.5620915033,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.5183673469,
        "MMLU_sociology":0.6815920398,
        "MMLU_us_foreign_policy":0.72,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.730994152
    },
    {
        "Model":"llama-2-13b-FINETUNE2_3w-q_k_v_o_proj",
        "URL":"https:\/\/huggingface.co\/CHIH-HUNG\/llama-2-13b-FINETUNE2_3w-q_k_v_o_proj",
        "full_model_name":"CHIH-HUNG\/llama-2-13b-FINETUNE2_3w-q_k_v_o_proj",
        "Parameters":13.0,
        "MMLU_average":0.5390465167,
        "arc:challenge|25":0.5460750853,
        "hellaswag|10":0.6153156742,
        "MMLU_abstract_algebra":0.38,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.5394736842,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.6,
        "MMLU_college_biology":0.5972222222,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.4797687861,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4170212766,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.4965517241,
        "MMLU_elementary_mathematics":0.3333333333,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.6,
        "MMLU_high_school_chemistry":0.4187192118,
        "MMLU_high_school_computer_science":0.59,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.6868686869,
        "MMLU_high_school_government_and_politics":0.7772020725,
        "MMLU_high_school_macroeconomics":0.4794871795,
        "MMLU_high_school_mathematics":0.3185185185,
        "MMLU_high_school_microeconomics":0.525210084,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7302752294,
        "MMLU_high_school_statistics":0.3935185185,
        "MMLU_high_school_us_history":0.7156862745,
        "MMLU_high_school_world_history":0.7383966245,
        "MMLU_human_aging":0.6188340807,
        "MMLU_human_sexuality":0.572519084,
        "MMLU_international_law":0.7107438017,
        "MMLU_jurisprudence":0.6388888889,
        "MMLU_logical_fallacies":0.6257668712,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.8247863248,
        "MMLU_medical_genetics":0.61,
        "MMLU_miscellaneous":0.7637292465,
        "MMLU_moral_disputes":0.612716763,
        "MMLU_moral_scenarios":0.2402234637,
        "MMLU_nutrition":0.5620915033,
        "MMLU_philosophy":0.6173633441,
        "MMLU_prehistory":0.6543209877,
        "MMLU_professional_accounting":0.4184397163,
        "MMLU_professional_law":0.3937418514,
        "MMLU_professional_medicine":0.4779411765,
        "MMLU_professional_psychology":0.5424836601,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.5673469388,
        "MMLU_sociology":0.6965174129,
        "MMLU_us_foreign_policy":0.78,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.7543859649
    },
    {
        "Model":"ANIMA-Phi-Neptune-Mistral-7B-v3",
        "URL":"https:\/\/huggingface.co\/Severian\/ANIMA-Phi-Neptune-Mistral-7B-v3",
        "full_model_name":"Severian\/ANIMA-Phi-Neptune-Mistral-7B-v3",
        "Parameters":7.0,
        "MMLU_average":0.538386209,
        "arc:challenge|25":0.5401023891,
        "hellaswag|10":0.5893248357,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.5263157895,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.6377358491,
        "MMLU_college_biology":0.5347222222,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.54,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.5722543353,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.65,
        "MMLU_conceptual_physics":0.4425531915,
        "MMLU_econometrics":0.3859649123,
        "MMLU_electrical_engineering":0.5448275862,
        "MMLU_elementary_mathematics":0.3439153439,
        "MMLU_formal_logic":0.3888888889,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.6161290323,
        "MMLU_high_school_chemistry":0.3743842365,
        "MMLU_high_school_computer_science":0.56,
        "MMLU_high_school_european_history":0.6545454545,
        "MMLU_high_school_geography":0.6565656566,
        "MMLU_high_school_government_and_politics":0.7357512953,
        "MMLU_high_school_macroeconomics":0.4769230769,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.4957983193,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.7229357798,
        "MMLU_high_school_statistics":0.3981481481,
        "MMLU_high_school_us_history":0.6519607843,
        "MMLU_high_school_world_history":0.7088607595,
        "MMLU_human_aging":0.6098654709,
        "MMLU_human_sexuality":0.6183206107,
        "MMLU_international_law":0.6446280992,
        "MMLU_jurisprudence":0.6111111111,
        "MMLU_logical_fallacies":0.6196319018,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.8290598291,
        "MMLU_medical_genetics":0.62,
        "MMLU_miscellaneous":0.7088122605,
        "MMLU_moral_disputes":0.5664739884,
        "MMLU_moral_scenarios":0.3497206704,
        "MMLU_nutrition":0.568627451,
        "MMLU_philosophy":0.6302250804,
        "MMLU_prehistory":0.6018518519,
        "MMLU_professional_accounting":0.3723404255,
        "MMLU_professional_law":0.3872229465,
        "MMLU_professional_medicine":0.4852941176,
        "MMLU_professional_psychology":0.5359477124,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.6530612245,
        "MMLU_sociology":0.7164179104,
        "MMLU_us_foreign_policy":0.79,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.7251461988
    },
    {
        "Model":"guanaco-33b-merged",
        "URL":"https:\/\/huggingface.co\/timdettmers\/guanaco-33b-merged",
        "full_model_name":"timdettmers\/guanaco-33b-merged",
        "Parameters":33.0,
        "MMLU_average":0.5378094701,
        "arc:challenge|25":0.5870307167,
        "hellaswag|10":0.6446922924,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5657894737,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.5622641509,
        "MMLU_college_biology":0.5625,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.5144508671,
        "MMLU_college_physics":0.3431372549,
        "MMLU_computer_security":0.59,
        "MMLU_conceptual_physics":0.4595744681,
        "MMLU_econometrics":0.3684210526,
        "MMLU_electrical_engineering":0.4413793103,
        "MMLU_elementary_mathematics":0.3121693122,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.6290322581,
        "MMLU_high_school_chemistry":0.3793103448,
        "MMLU_high_school_computer_science":0.54,
        "MMLU_high_school_european_history":0.7212121212,
        "MMLU_high_school_geography":0.7222222222,
        "MMLU_high_school_government_and_politics":0.7150259067,
        "MMLU_high_school_macroeconomics":0.4923076923,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.5294117647,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.7302752294,
        "MMLU_high_school_statistics":0.375,
        "MMLU_high_school_us_history":0.7794117647,
        "MMLU_high_school_world_history":0.7679324895,
        "MMLU_human_aging":0.5874439462,
        "MMLU_human_sexuality":0.641221374,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.6111111111,
        "MMLU_logical_fallacies":0.6380368098,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.6310679612,
        "MMLU_marketing":0.7863247863,
        "MMLU_medical_genetics":0.62,
        "MMLU_miscellaneous":0.6756066411,
        "MMLU_moral_disputes":0.549132948,
        "MMLU_moral_scenarios":0.2569832402,
        "MMLU_nutrition":0.5718954248,
        "MMLU_philosophy":0.6012861736,
        "MMLU_prehistory":0.6172839506,
        "MMLU_professional_accounting":0.4290780142,
        "MMLU_professional_law":0.424380704,
        "MMLU_professional_medicine":0.5808823529,
        "MMLU_professional_psychology":0.5212418301,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.5918367347,
        "MMLU_sociology":0.6915422886,
        "MMLU_us_foreign_policy":0.78,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.7251461988
    },
    {
        "Model":"llama-2-13b-FINETUNE5_4w-r4-q_k_v_o_gate_up_down",
        "URL":"https:\/\/huggingface.co\/CHIH-HUNG\/llama-2-13b-FINETUNE5_4w-r4-q_k_v_o_gate_up_down",
        "full_model_name":"CHIH-HUNG\/llama-2-13b-FINETUNE5_4w-r4-q_k_v_o_gate_up_down",
        "Parameters":13.0,
        "MMLU_average":0.5377036087,
        "arc:challenge|25":0.5153583618,
        "hellaswag|10":0.6060545708,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.5131578947,
        "MMLU_business_ethics":0.47,
        "MMLU_clinical_knowledge":0.5811320755,
        "MMLU_college_biology":0.5486111111,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.4913294798,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.63,
        "MMLU_conceptual_physics":0.4425531915,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.4413793103,
        "MMLU_elementary_mathematics":0.328042328,
        "MMLU_formal_logic":0.3333333333,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.6516129032,
        "MMLU_high_school_chemistry":0.4679802956,
        "MMLU_high_school_computer_science":0.48,
        "MMLU_high_school_european_history":0.696969697,
        "MMLU_high_school_geography":0.6717171717,
        "MMLU_high_school_government_and_politics":0.792746114,
        "MMLU_high_school_macroeconomics":0.4846153846,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.6008403361,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.7302752294,
        "MMLU_high_school_statistics":0.4212962963,
        "MMLU_high_school_us_history":0.7598039216,
        "MMLU_high_school_world_history":0.6708860759,
        "MMLU_human_aging":0.6457399103,
        "MMLU_human_sexuality":0.5648854962,
        "MMLU_international_law":0.7438016529,
        "MMLU_jurisprudence":0.7407407407,
        "MMLU_logical_fallacies":0.5582822086,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.7766990291,
        "MMLU_marketing":0.7735042735,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.7381864623,
        "MMLU_moral_disputes":0.6242774566,
        "MMLU_moral_scenarios":0.3340782123,
        "MMLU_nutrition":0.5849673203,
        "MMLU_philosophy":0.6430868167,
        "MMLU_prehistory":0.5802469136,
        "MMLU_professional_accounting":0.4397163121,
        "MMLU_professional_law":0.4217731421,
        "MMLU_professional_medicine":0.5625,
        "MMLU_professional_psychology":0.5506535948,
        "MMLU_public_relations":0.5545454545,
        "MMLU_security_studies":0.5102040816,
        "MMLU_sociology":0.6865671642,
        "MMLU_us_foreign_policy":0.78,
        "MMLU_virology":0.3855421687,
        "MMLU_world_religions":0.7426900585
    },
    {
        "Model":"airoboros-l2-13b-gpt4-m2.0",
        "URL":"https:\/\/huggingface.co\/jondurbin\/airoboros-l2-13b-gpt4-m2.0",
        "full_model_name":"jondurbin\/airoboros-l2-13b-gpt4-m2.0",
        "Parameters":13.0,
        "MMLU_average":0.5373386766,
        "arc:challenge|25":0.5469283276,
        "hellaswag|10":0.6118303127,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.5131578947,
        "MMLU_business_ethics":0.6,
        "MMLU_clinical_knowledge":0.5735849057,
        "MMLU_college_biology":0.5694444444,
        "MMLU_college_chemistry":0.36,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4739884393,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.4638297872,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.5310344828,
        "MMLU_elementary_mathematics":0.3412698413,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.6032258065,
        "MMLU_high_school_chemistry":0.4137931034,
        "MMLU_high_school_computer_science":0.56,
        "MMLU_high_school_european_history":0.6606060606,
        "MMLU_high_school_geography":0.6565656566,
        "MMLU_high_school_government_and_politics":0.7875647668,
        "MMLU_high_school_macroeconomics":0.4794871795,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.5168067227,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.7174311927,
        "MMLU_high_school_statistics":0.3935185185,
        "MMLU_high_school_us_history":0.7303921569,
        "MMLU_high_school_world_history":0.7172995781,
        "MMLU_human_aging":0.6233183857,
        "MMLU_human_sexuality":0.5801526718,
        "MMLU_international_law":0.7024793388,
        "MMLU_jurisprudence":0.6851851852,
        "MMLU_logical_fallacies":0.6625766871,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.7087378641,
        "MMLU_marketing":0.8076923077,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.7100893997,
        "MMLU_moral_disputes":0.5924855491,
        "MMLU_moral_scenarios":0.3396648045,
        "MMLU_nutrition":0.591503268,
        "MMLU_philosophy":0.6302250804,
        "MMLU_prehistory":0.6049382716,
        "MMLU_professional_accounting":0.4113475177,
        "MMLU_professional_law":0.3930899609,
        "MMLU_professional_medicine":0.4448529412,
        "MMLU_professional_psychology":0.5539215686,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.6244897959,
        "MMLU_sociology":0.7164179104,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.7426900585
    },
    {
        "Model":"llama-2-26b-trenchcoat-stack",
        "URL":"https:\/\/huggingface.co\/chargoddard\/llama-2-26b-trenchcoat-stack",
        "full_model_name":"chargoddard\/llama-2-26b-trenchcoat-stack",
        "Parameters":26.0,
        "MMLU_average":0.5372580535,
        "arc:challenge|25":0.5102389078,
        "hellaswag|10":0.5666201952,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.5037037037,
        "MMLU_astronomy":0.5263157895,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.6113207547,
        "MMLU_college_biology":0.5833333333,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.38,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.4739884393,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4510638298,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.4482758621,
        "MMLU_elementary_mathematics":0.3201058201,
        "MMLU_formal_logic":0.2777777778,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.664516129,
        "MMLU_high_school_chemistry":0.4039408867,
        "MMLU_high_school_computer_science":0.54,
        "MMLU_high_school_european_history":0.6303030303,
        "MMLU_high_school_geography":0.702020202,
        "MMLU_high_school_government_and_politics":0.792746114,
        "MMLU_high_school_macroeconomics":0.4897435897,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.5294117647,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.7357798165,
        "MMLU_high_school_statistics":0.4768518519,
        "MMLU_high_school_us_history":0.7254901961,
        "MMLU_high_school_world_history":0.6962025316,
        "MMLU_human_aging":0.6457399103,
        "MMLU_human_sexuality":0.6488549618,
        "MMLU_international_law":0.7190082645,
        "MMLU_jurisprudence":0.6944444444,
        "MMLU_logical_fallacies":0.6932515337,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.7521367521,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7407407407,
        "MMLU_moral_disputes":0.6445086705,
        "MMLU_moral_scenarios":0.251396648,
        "MMLU_nutrition":0.5947712418,
        "MMLU_philosophy":0.6270096463,
        "MMLU_prehistory":0.6080246914,
        "MMLU_professional_accounting":0.4219858156,
        "MMLU_professional_law":0.4106910039,
        "MMLU_professional_medicine":0.4301470588,
        "MMLU_professional_psychology":0.5163398693,
        "MMLU_public_relations":0.6,
        "MMLU_security_studies":0.612244898,
        "MMLU_sociology":0.7213930348,
        "MMLU_us_foreign_policy":0.81,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.7543859649
    },
    {
        "Model":"PuffedConvo13bLoraE4",
        "URL":"https:\/\/huggingface.co\/NobodyExistsOnTheInternet\/PuffedConvo13bLoraE4",
        "full_model_name":"NobodyExistsOnTheInternet\/PuffedConvo13bLoraE4",
        "Parameters":13.0,
        "MMLU_average":0.5371970286,
        "arc:challenge|25":0.5588737201,
        "hellaswag|10":0.6551483768,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4518518519,
        "MMLU_astronomy":0.5394736842,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.5622641509,
        "MMLU_college_biology":0.5416666667,
        "MMLU_college_chemistry":0.32,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.5260115607,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4340425532,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.2962962963,
        "MMLU_formal_logic":0.3412698413,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.6193548387,
        "MMLU_high_school_chemistry":0.354679803,
        "MMLU_high_school_computer_science":0.54,
        "MMLU_high_school_european_history":0.6909090909,
        "MMLU_high_school_geography":0.696969697,
        "MMLU_high_school_government_and_politics":0.7875647668,
        "MMLU_high_school_macroeconomics":0.4897435897,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.5420168067,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.7266055046,
        "MMLU_high_school_statistics":0.375,
        "MMLU_high_school_us_history":0.7156862745,
        "MMLU_high_school_world_history":0.746835443,
        "MMLU_human_aging":0.6547085202,
        "MMLU_human_sexuality":0.6564885496,
        "MMLU_international_law":0.7107438017,
        "MMLU_jurisprudence":0.6296296296,
        "MMLU_logical_fallacies":0.6625766871,
        "MMLU_machine_learning":0.4107142857,
        "MMLU_management":0.6893203883,
        "MMLU_marketing":0.7905982906,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.7509578544,
        "MMLU_moral_disputes":0.6011560694,
        "MMLU_moral_scenarios":0.2592178771,
        "MMLU_nutrition":0.5784313725,
        "MMLU_philosophy":0.6688102894,
        "MMLU_prehistory":0.6296296296,
        "MMLU_professional_accounting":0.3936170213,
        "MMLU_professional_law":0.4119947849,
        "MMLU_professional_medicine":0.4889705882,
        "MMLU_professional_psychology":0.5669934641,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.6163265306,
        "MMLU_sociology":0.7512437811,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.7660818713
    },
    {
        "Model":"Guanaco-13B-Uncensored",
        "URL":"https:\/\/huggingface.co\/Fredithefish\/Guanaco-13B-Uncensored",
        "full_model_name":"Fredithefish\/Guanaco-13B-Uncensored",
        "Parameters":13.0,
        "MMLU_average":0.5365469198,
        "arc:challenge|25":0.5622866894,
        "hellaswag|10":0.6208922525,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.5197368421,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.5811320755,
        "MMLU_college_biology":0.5347222222,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.4,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.5433526012,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.4425531915,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.5448275862,
        "MMLU_elementary_mathematics":0.291005291,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.6096774194,
        "MMLU_high_school_chemistry":0.4482758621,
        "MMLU_high_school_computer_science":0.53,
        "MMLU_high_school_european_history":0.6545454545,
        "MMLU_high_school_geography":0.6818181818,
        "MMLU_high_school_government_and_politics":0.7875647668,
        "MMLU_high_school_macroeconomics":0.5051282051,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.5462184874,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.7155963303,
        "MMLU_high_school_statistics":0.4259259259,
        "MMLU_high_school_us_history":0.7352941176,
        "MMLU_high_school_world_history":0.7046413502,
        "MMLU_human_aging":0.6053811659,
        "MMLU_human_sexuality":0.5801526718,
        "MMLU_international_law":0.694214876,
        "MMLU_jurisprudence":0.6759259259,
        "MMLU_logical_fallacies":0.6196319018,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.7435897436,
        "MMLU_medical_genetics":0.61,
        "MMLU_miscellaneous":0.7266922095,
        "MMLU_moral_disputes":0.6213872832,
        "MMLU_moral_scenarios":0.3374301676,
        "MMLU_nutrition":0.5980392157,
        "MMLU_philosophy":0.61414791,
        "MMLU_prehistory":0.6172839506,
        "MMLU_professional_accounting":0.4219858156,
        "MMLU_professional_law":0.406779661,
        "MMLU_professional_medicine":0.5477941176,
        "MMLU_professional_psychology":0.5392156863,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.587755102,
        "MMLU_sociology":0.6815920398,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.7602339181
    },
    {
        "Model":"llama2-22b-chat-wizard-uncensored",
        "URL":"https:\/\/huggingface.co\/nkpz\/llama2-22b-chat-wizard-uncensored",
        "full_model_name":"nkpz\/llama2-22b-chat-wizard-uncensored",
        "Parameters":22.0,
        "MMLU_average":0.5361583971,
        "arc:challenge|25":0.5273037543,
        "hellaswag|10":0.6118303127,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4444444444,
        "MMLU_astronomy":0.5592105263,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.5849056604,
        "MMLU_college_biology":0.6041666667,
        "MMLU_college_chemistry":0.36,
        "MMLU_college_computer_science":0.5,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.4913294798,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.64,
        "MMLU_conceptual_physics":0.4170212766,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.5103448276,
        "MMLU_elementary_mathematics":0.3412698413,
        "MMLU_formal_logic":0.3333333333,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.6161290323,
        "MMLU_high_school_chemistry":0.4334975369,
        "MMLU_high_school_computer_science":0.57,
        "MMLU_high_school_european_history":0.6424242424,
        "MMLU_high_school_geography":0.6464646465,
        "MMLU_high_school_government_and_politics":0.7720207254,
        "MMLU_high_school_macroeconomics":0.4641025641,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.525210084,
        "MMLU_high_school_physics":0.3841059603,
        "MMLU_high_school_psychology":0.7229357798,
        "MMLU_high_school_statistics":0.3611111111,
        "MMLU_high_school_us_history":0.7205882353,
        "MMLU_high_school_world_history":0.7046413502,
        "MMLU_human_aging":0.6233183857,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.7190082645,
        "MMLU_jurisprudence":0.75,
        "MMLU_logical_fallacies":0.6564417178,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.7521367521,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.7394636015,
        "MMLU_moral_disputes":0.6069364162,
        "MMLU_moral_scenarios":0.3206703911,
        "MMLU_nutrition":0.6078431373,
        "MMLU_philosophy":0.5916398714,
        "MMLU_prehistory":0.6049382716,
        "MMLU_professional_accounting":0.3829787234,
        "MMLU_professional_law":0.3820078227,
        "MMLU_professional_medicine":0.4411764706,
        "MMLU_professional_psychology":0.5,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.6612244898,
        "MMLU_sociology":0.7014925373,
        "MMLU_us_foreign_policy":0.77,
        "MMLU_virology":0.4638554217,
        "MMLU_world_religions":0.7485380117
    },
    {
        "Model":"tigerbot-13b-base",
        "URL":"https:\/\/huggingface.co\/TigerResearch\/tigerbot-13b-base",
        "full_model_name":"TigerResearch\/tigerbot-13b-base",
        "Parameters":13.0,
        "MMLU_average":0.5356551769,
        "arc:challenge|25":0.5017064846,
        "hellaswag|10":0.5725951006,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4296296296,
        "MMLU_astronomy":0.5592105263,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.5735849057,
        "MMLU_college_biology":0.5555555556,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.4624277457,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.67,
        "MMLU_conceptual_physics":0.3957446809,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4827586207,
        "MMLU_elementary_mathematics":0.328042328,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.6483870968,
        "MMLU_high_school_chemistry":0.4039408867,
        "MMLU_high_school_computer_science":0.59,
        "MMLU_high_school_european_history":0.6545454545,
        "MMLU_high_school_geography":0.6464646465,
        "MMLU_high_school_government_and_politics":0.7668393782,
        "MMLU_high_school_macroeconomics":0.4794871795,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.5462184874,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.7412844037,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.6862745098,
        "MMLU_high_school_world_history":0.7172995781,
        "MMLU_human_aging":0.6367713004,
        "MMLU_human_sexuality":0.6030534351,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.5833333333,
        "MMLU_logical_fallacies":0.6625766871,
        "MMLU_machine_learning":0.4285714286,
        "MMLU_management":0.6893203883,
        "MMLU_marketing":0.7777777778,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.7318007663,
        "MMLU_moral_disputes":0.5693641618,
        "MMLU_moral_scenarios":0.2324022346,
        "MMLU_nutrition":0.5980392157,
        "MMLU_philosophy":0.6623794212,
        "MMLU_prehistory":0.5648148148,
        "MMLU_professional_accounting":0.4007092199,
        "MMLU_professional_law":0.3917861799,
        "MMLU_professional_medicine":0.5183823529,
        "MMLU_professional_psychology":0.4869281046,
        "MMLU_public_relations":0.6909090909,
        "MMLU_security_studies":0.6244897959,
        "MMLU_sociology":0.7661691542,
        "MMLU_us_foreign_policy":0.74,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.7777777778
    },
    {
        "Model":"CreativityEngine",
        "URL":"https:\/\/huggingface.co\/Undi95\/CreativityEngine",
        "full_model_name":"Undi95\/CreativityEngine",
        "Parameters":null,
        "MMLU_average":0.5355265534,
        "arc:challenge|25":0.5563139932,
        "hellaswag|10":0.6253734316,
        "MMLU_abstract_algebra":0.39,
        "MMLU_anatomy":0.437037037,
        "MMLU_astronomy":0.5,
        "MMLU_business_ethics":0.5,
        "MMLU_clinical_knowledge":0.5547169811,
        "MMLU_college_biology":0.5416666667,
        "MMLU_college_chemistry":0.32,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.450867052,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.4255319149,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.5172413793,
        "MMLU_elementary_mathematics":0.3174603175,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.4,
        "MMLU_high_school_biology":0.5935483871,
        "MMLU_high_school_chemistry":0.4039408867,
        "MMLU_high_school_computer_science":0.51,
        "MMLU_high_school_european_history":0.6484848485,
        "MMLU_high_school_geography":0.6767676768,
        "MMLU_high_school_government_and_politics":0.7720207254,
        "MMLU_high_school_macroeconomics":0.5179487179,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.4831932773,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.7027522936,
        "MMLU_high_school_statistics":0.3703703704,
        "MMLU_high_school_us_history":0.7254901961,
        "MMLU_high_school_world_history":0.7257383966,
        "MMLU_human_aging":0.668161435,
        "MMLU_human_sexuality":0.6030534351,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.6574074074,
        "MMLU_logical_fallacies":0.6564417178,
        "MMLU_machine_learning":0.4017857143,
        "MMLU_management":0.640776699,
        "MMLU_marketing":0.8076923077,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7432950192,
        "MMLU_moral_disputes":0.6069364162,
        "MMLU_moral_scenarios":0.4357541899,
        "MMLU_nutrition":0.545751634,
        "MMLU_philosophy":0.6173633441,
        "MMLU_prehistory":0.6018518519,
        "MMLU_professional_accounting":0.4007092199,
        "MMLU_professional_law":0.4198174707,
        "MMLU_professional_medicine":0.5,
        "MMLU_professional_psychology":0.5408496732,
        "MMLU_public_relations":0.7,
        "MMLU_security_studies":0.5795918367,
        "MMLU_sociology":0.6865671642,
        "MMLU_us_foreign_policy":0.84,
        "MMLU_virology":0.4518072289,
        "MMLU_world_religions":0.7602339181
    },
    {
        "Model":"Qwen-LLaMAfied-7B-Chat",
        "URL":"https:\/\/huggingface.co\/JosephusCheung\/Qwen-LLaMAfied-7B-Chat",
        "full_model_name":"JosephusCheung\/Qwen-LLaMAfied-7B-Chat",
        "Parameters":7.0,
        "MMLU_average":0.5351728104,
        "arc:challenge|25":0.4650170648,
        "hellaswag|10":0.6408086039,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.5986842105,
        "MMLU_business_ethics":0.61,
        "MMLU_clinical_knowledge":0.6150943396,
        "MMLU_college_biology":0.5763888889,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.5,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.5260115607,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.4425531915,
        "MMLU_econometrics":0.3859649123,
        "MMLU_electrical_engineering":0.5172413793,
        "MMLU_elementary_mathematics":0.3518518519,
        "MMLU_formal_logic":0.373015873,
        "MMLU_global_facts":0.27,
        "MMLU_high_school_biology":0.6419354839,
        "MMLU_high_school_chemistry":0.4532019704,
        "MMLU_high_school_computer_science":0.58,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.696969697,
        "MMLU_high_school_government_and_politics":0.7875647668,
        "MMLU_high_school_macroeconomics":0.5128205128,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.5210084034,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.7339449541,
        "MMLU_high_school_statistics":0.3842592593,
        "MMLU_high_school_us_history":0.5931372549,
        "MMLU_high_school_world_history":0.7510548523,
        "MMLU_human_aging":0.600896861,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.7438016529,
        "MMLU_jurisprudence":0.6851851852,
        "MMLU_logical_fallacies":0.6196319018,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.640776699,
        "MMLU_marketing":0.7863247863,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.7330779055,
        "MMLU_moral_disputes":0.6156069364,
        "MMLU_moral_scenarios":0.2603351955,
        "MMLU_nutrition":0.6241830065,
        "MMLU_philosophy":0.6463022508,
        "MMLU_prehistory":0.6419753086,
        "MMLU_professional_accounting":0.3723404255,
        "MMLU_professional_law":0.4132985658,
        "MMLU_professional_medicine":0.5220588235,
        "MMLU_professional_psychology":0.5310457516,
        "MMLU_public_relations":0.6,
        "MMLU_security_studies":0.6448979592,
        "MMLU_sociology":0.7661691542,
        "MMLU_us_foreign_policy":0.78,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.6842105263
    },
    {
        "Model":"effi-13b",
        "URL":"https:\/\/huggingface.co\/aiplanet\/effi-13b",
        "full_model_name":"aiplanet\/effi-13b",
        "Parameters":13.0,
        "MMLU_average":0.5346937752,
        "arc:challenge|25":0.4906143345,
        "hellaswag|10":0.6058554073,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.5037037037,
        "MMLU_astronomy":0.5263157895,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.5509433962,
        "MMLU_college_biology":0.5416666667,
        "MMLU_college_chemistry":0.33,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.4566473988,
        "MMLU_college_physics":0.3529411765,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.429787234,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.524137931,
        "MMLU_elementary_mathematics":0.3253968254,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.6161290323,
        "MMLU_high_school_chemistry":0.4187192118,
        "MMLU_high_school_computer_science":0.57,
        "MMLU_high_school_european_history":0.6909090909,
        "MMLU_high_school_geography":0.702020202,
        "MMLU_high_school_government_and_politics":0.7772020725,
        "MMLU_high_school_macroeconomics":0.4974358974,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.5,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.728440367,
        "MMLU_high_school_statistics":0.3564814815,
        "MMLU_high_school_us_history":0.6862745098,
        "MMLU_high_school_world_history":0.7046413502,
        "MMLU_human_aging":0.6322869955,
        "MMLU_human_sexuality":0.5877862595,
        "MMLU_international_law":0.7520661157,
        "MMLU_jurisprudence":0.6759259259,
        "MMLU_logical_fallacies":0.6196319018,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.7991452991,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.7343550447,
        "MMLU_moral_disputes":0.5809248555,
        "MMLU_moral_scenarios":0.3262569832,
        "MMLU_nutrition":0.5849673203,
        "MMLU_philosophy":0.5980707395,
        "MMLU_prehistory":0.6141975309,
        "MMLU_professional_accounting":0.390070922,
        "MMLU_professional_law":0.3696219035,
        "MMLU_professional_medicine":0.4963235294,
        "MMLU_professional_psychology":0.5147058824,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.6163265306,
        "MMLU_sociology":0.7114427861,
        "MMLU_us_foreign_policy":0.78,
        "MMLU_virology":0.4819277108,
        "MMLU_world_religions":0.7602339181
    },
    {
        "Model":"speechless-codellama-dolphin-orca-platypus-34b",
        "URL":"https:\/\/huggingface.co\/uukuguy\/speechless-codellama-dolphin-orca-platypus-34b",
        "full_model_name":"uukuguy\/speechless-codellama-dolphin-orca-platypus-34b",
        "Parameters":34.0,
        "MMLU_average":0.5346703247,
        "arc:challenge|25":0.4957337884,
        "hellaswag|10":0.5479984067,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.437037037,
        "MMLU_astronomy":0.5328947368,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.4830188679,
        "MMLU_college_biology":0.5555555556,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.4335260116,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.4680851064,
        "MMLU_econometrics":0.4298245614,
        "MMLU_electrical_engineering":0.4965517241,
        "MMLU_elementary_mathematics":0.3835978836,
        "MMLU_formal_logic":0.4920634921,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.5806451613,
        "MMLU_high_school_chemistry":0.354679803,
        "MMLU_high_school_computer_science":0.63,
        "MMLU_high_school_european_history":0.7151515152,
        "MMLU_high_school_geography":0.6717171717,
        "MMLU_high_school_government_and_politics":0.7461139896,
        "MMLU_high_school_macroeconomics":0.4871794872,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.5294117647,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.7211009174,
        "MMLU_high_school_statistics":0.4027777778,
        "MMLU_high_school_us_history":0.75,
        "MMLU_high_school_world_history":0.7552742616,
        "MMLU_human_aging":0.6053811659,
        "MMLU_human_sexuality":0.5190839695,
        "MMLU_international_law":0.6694214876,
        "MMLU_jurisprudence":0.6759259259,
        "MMLU_logical_fallacies":0.7239263804,
        "MMLU_machine_learning":0.4375,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.7863247863,
        "MMLU_medical_genetics":0.48,
        "MMLU_miscellaneous":0.6666666667,
        "MMLU_moral_disputes":0.5578034682,
        "MMLU_moral_scenarios":0.2715083799,
        "MMLU_nutrition":0.545751634,
        "MMLU_philosophy":0.5916398714,
        "MMLU_prehistory":0.5987654321,
        "MMLU_professional_accounting":0.390070922,
        "MMLU_professional_law":0.4198174707,
        "MMLU_professional_medicine":0.4558823529,
        "MMLU_professional_psychology":0.5310457516,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.6612244898,
        "MMLU_sociology":0.7512437811,
        "MMLU_us_foreign_policy":0.74,
        "MMLU_virology":0.3975903614,
        "MMLU_world_religions":0.6374269006
    },
    {
        "Model":"speechless-codellama-34b-v1.0",
        "URL":"https:\/\/huggingface.co\/speechlessai\/speechless-codellama-34b-v1.0",
        "full_model_name":"speechlessai\/speechless-codellama-34b-v1.0",
        "Parameters":34.0,
        "MMLU_average":0.5346703247,
        "arc:challenge|25":0.4957337884,
        "hellaswag|10":0.5479984067,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.437037037,
        "MMLU_astronomy":0.5328947368,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.4830188679,
        "MMLU_college_biology":0.5555555556,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.4335260116,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.4680851064,
        "MMLU_econometrics":0.4298245614,
        "MMLU_electrical_engineering":0.4965517241,
        "MMLU_elementary_mathematics":0.3835978836,
        "MMLU_formal_logic":0.4920634921,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.5806451613,
        "MMLU_high_school_chemistry":0.354679803,
        "MMLU_high_school_computer_science":0.63,
        "MMLU_high_school_european_history":0.7151515152,
        "MMLU_high_school_geography":0.6717171717,
        "MMLU_high_school_government_and_politics":0.7461139896,
        "MMLU_high_school_macroeconomics":0.4871794872,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.5294117647,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.7211009174,
        "MMLU_high_school_statistics":0.4027777778,
        "MMLU_high_school_us_history":0.75,
        "MMLU_high_school_world_history":0.7552742616,
        "MMLU_human_aging":0.6053811659,
        "MMLU_human_sexuality":0.5190839695,
        "MMLU_international_law":0.6694214876,
        "MMLU_jurisprudence":0.6759259259,
        "MMLU_logical_fallacies":0.7239263804,
        "MMLU_machine_learning":0.4375,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.7863247863,
        "MMLU_medical_genetics":0.48,
        "MMLU_miscellaneous":0.6666666667,
        "MMLU_moral_disputes":0.5578034682,
        "MMLU_moral_scenarios":0.2715083799,
        "MMLU_nutrition":0.545751634,
        "MMLU_philosophy":0.5916398714,
        "MMLU_prehistory":0.5987654321,
        "MMLU_professional_accounting":0.390070922,
        "MMLU_professional_law":0.4198174707,
        "MMLU_professional_medicine":0.4558823529,
        "MMLU_professional_psychology":0.5310457516,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.6612244898,
        "MMLU_sociology":0.7512437811,
        "MMLU_us_foreign_policy":0.74,
        "MMLU_virology":0.3975903614,
        "MMLU_world_religions":0.6374269006
    },
    {
        "Model":"llama2-13b-math1.1",
        "URL":"https:\/\/huggingface.co\/FelixChao\/llama2-13b-math1.1",
        "full_model_name":"FelixChao\/llama2-13b-math1.1",
        "Parameters":13.0,
        "MMLU_average":0.5343392171,
        "arc:challenge|25":0.5153583618,
        "hellaswag|10":0.609639514,
        "MMLU_abstract_algebra":0.38,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5526315789,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.5698113208,
        "MMLU_college_biology":0.5347222222,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.4393063584,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.65,
        "MMLU_conceptual_physics":0.4127659574,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.524137931,
        "MMLU_elementary_mathematics":0.3306878307,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.6322580645,
        "MMLU_high_school_chemistry":0.4088669951,
        "MMLU_high_school_computer_science":0.57,
        "MMLU_high_school_european_history":0.6666666667,
        "MMLU_high_school_geography":0.6868686869,
        "MMLU_high_school_government_and_politics":0.725388601,
        "MMLU_high_school_macroeconomics":0.4820512821,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.525210084,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.7155963303,
        "MMLU_high_school_statistics":0.3425925926,
        "MMLU_high_school_us_history":0.7401960784,
        "MMLU_high_school_world_history":0.7383966245,
        "MMLU_human_aging":0.6726457399,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.7768595041,
        "MMLU_jurisprudence":0.7037037037,
        "MMLU_logical_fallacies":0.6441717791,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.6990291262,
        "MMLU_marketing":0.7777777778,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.7356321839,
        "MMLU_moral_disputes":0.6069364162,
        "MMLU_moral_scenarios":0.3039106145,
        "MMLU_nutrition":0.6045751634,
        "MMLU_philosophy":0.6109324759,
        "MMLU_prehistory":0.5956790123,
        "MMLU_professional_accounting":0.390070922,
        "MMLU_professional_law":0.3741851369,
        "MMLU_professional_medicine":0.4448529412,
        "MMLU_professional_psychology":0.5310457516,
        "MMLU_public_relations":0.6545454545,
        "MMLU_security_studies":0.6285714286,
        "MMLU_sociology":0.7462686567,
        "MMLU_us_foreign_policy":0.78,
        "MMLU_virology":0.4759036145,
        "MMLU_world_religions":0.7368421053
    },
    {
        "Model":"llama-2-13b-FINETUNE3_3.3w-r8-q_k_v_o_gate_up_down",
        "URL":"https:\/\/huggingface.co\/CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r8-q_k_v_o_gate_up_down",
        "full_model_name":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r8-q_k_v_o_gate_up_down",
        "Parameters":13.0,
        "MMLU_average":0.5342642261,
        "arc:challenge|25":0.5349829352,
        "hellaswag|10":0.6030671181,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.5657894737,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.5773584906,
        "MMLU_college_biology":0.5416666667,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.4,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.4624277457,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.71,
        "MMLU_conceptual_physics":0.4085106383,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4206896552,
        "MMLU_elementary_mathematics":0.3412698413,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.6322580645,
        "MMLU_high_school_chemistry":0.39408867,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.7151515152,
        "MMLU_high_school_geography":0.7171717172,
        "MMLU_high_school_government_and_politics":0.7979274611,
        "MMLU_high_school_macroeconomics":0.5128205128,
        "MMLU_high_school_mathematics":0.3222222222,
        "MMLU_high_school_microeconomics":0.5672268908,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7339449541,
        "MMLU_high_school_statistics":0.3981481481,
        "MMLU_high_school_us_history":0.7549019608,
        "MMLU_high_school_world_history":0.7341772152,
        "MMLU_human_aging":0.6188340807,
        "MMLU_human_sexuality":0.534351145,
        "MMLU_international_law":0.7190082645,
        "MMLU_jurisprudence":0.6666666667,
        "MMLU_logical_fallacies":0.6441717791,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.7777777778,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.7598978289,
        "MMLU_moral_disputes":0.5924855491,
        "MMLU_moral_scenarios":0.251396648,
        "MMLU_nutrition":0.5522875817,
        "MMLU_philosophy":0.6559485531,
        "MMLU_prehistory":0.6327160494,
        "MMLU_professional_accounting":0.4645390071,
        "MMLU_professional_law":0.4100391134,
        "MMLU_professional_medicine":0.4852941176,
        "MMLU_professional_psychology":0.5326797386,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.5142857143,
        "MMLU_sociology":0.6915422886,
        "MMLU_us_foreign_policy":0.72,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.7719298246
    },
    {
        "Model":"llama2-28B-Airo03",
        "URL":"https:\/\/huggingface.co\/grimpep\/llama2-28B-Airo03",
        "full_model_name":"grimpep\/llama2-28B-Airo03",
        "Parameters":28.0,
        "MMLU_average":0.5331106097,
        "arc:challenge|25":0.5563139932,
        "hellaswag|10":0.6195976897,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.5333333333,
        "MMLU_astronomy":0.5921052632,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.5735849057,
        "MMLU_college_biology":0.5694444444,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.4913294798,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.4382978723,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.4827586207,
        "MMLU_elementary_mathematics":0.3624338624,
        "MMLU_formal_logic":0.3333333333,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.6258064516,
        "MMLU_high_school_chemistry":0.4876847291,
        "MMLU_high_school_computer_science":0.5,
        "MMLU_high_school_european_history":0.6606060606,
        "MMLU_high_school_geography":0.6666666667,
        "MMLU_high_school_government_and_politics":0.7512953368,
        "MMLU_high_school_macroeconomics":0.5128205128,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.5378151261,
        "MMLU_high_school_physics":0.357615894,
        "MMLU_high_school_psychology":0.7119266055,
        "MMLU_high_school_statistics":0.375,
        "MMLU_high_school_us_history":0.7303921569,
        "MMLU_high_school_world_history":0.7383966245,
        "MMLU_human_aging":0.6322869955,
        "MMLU_human_sexuality":0.6183206107,
        "MMLU_international_law":0.7024793388,
        "MMLU_jurisprudence":0.6481481481,
        "MMLU_logical_fallacies":0.6625766871,
        "MMLU_machine_learning":0.2946428571,
        "MMLU_management":0.6504854369,
        "MMLU_marketing":0.7478632479,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.7445721584,
        "MMLU_moral_disputes":0.5924855491,
        "MMLU_moral_scenarios":0.3173184358,
        "MMLU_nutrition":0.5980392157,
        "MMLU_philosophy":0.6045016077,
        "MMLU_prehistory":0.6234567901,
        "MMLU_professional_accounting":0.4078014184,
        "MMLU_professional_law":0.3833116037,
        "MMLU_professional_medicine":0.4669117647,
        "MMLU_professional_psychology":0.5441176471,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.6244897959,
        "MMLU_sociology":0.5771144279,
        "MMLU_us_foreign_policy":0.81,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.7426900585
    },
    {
        "Model":"Synthia-34B-v1.2",
        "URL":"https:\/\/huggingface.co\/migtissera\/Synthia-34B-v1.2",
        "full_model_name":"migtissera\/Synthia-34B-v1.2",
        "Parameters":34.0,
        "MMLU_average":0.5319759678,
        "arc:challenge|25":0.5119453925,
        "hellaswag|10":0.5587532364,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.437037037,
        "MMLU_astronomy":0.5065789474,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.5433962264,
        "MMLU_college_biology":0.5694444444,
        "MMLU_college_chemistry":0.46,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.5028901734,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.72,
        "MMLU_conceptual_physics":0.4170212766,
        "MMLU_econometrics":0.3947368421,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.4021164021,
        "MMLU_formal_logic":0.4365079365,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.6225806452,
        "MMLU_high_school_chemistry":0.3842364532,
        "MMLU_high_school_computer_science":0.67,
        "MMLU_high_school_european_history":0.6606060606,
        "MMLU_high_school_geography":0.6666666667,
        "MMLU_high_school_government_and_politics":0.7305699482,
        "MMLU_high_school_macroeconomics":0.5128205128,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.5672268908,
        "MMLU_high_school_physics":0.3907284768,
        "MMLU_high_school_psychology":0.695412844,
        "MMLU_high_school_statistics":0.4027777778,
        "MMLU_high_school_us_history":0.7107843137,
        "MMLU_high_school_world_history":0.7172995781,
        "MMLU_human_aging":0.5426008969,
        "MMLU_human_sexuality":0.534351145,
        "MMLU_international_law":0.694214876,
        "MMLU_jurisprudence":0.6481481481,
        "MMLU_logical_fallacies":0.6380368098,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.6601941748,
        "MMLU_marketing":0.7692307692,
        "MMLU_medical_genetics":0.51,
        "MMLU_miscellaneous":0.6743295019,
        "MMLU_moral_disputes":0.5867052023,
        "MMLU_moral_scenarios":0.3597765363,
        "MMLU_nutrition":0.5294117647,
        "MMLU_philosophy":0.61414791,
        "MMLU_prehistory":0.5524691358,
        "MMLU_professional_accounting":0.3794326241,
        "MMLU_professional_law":0.3787483703,
        "MMLU_professional_medicine":0.4080882353,
        "MMLU_professional_psychology":0.4836601307,
        "MMLU_public_relations":0.5727272727,
        "MMLU_security_studies":0.6448979592,
        "MMLU_sociology":0.7313432836,
        "MMLU_us_foreign_policy":0.7,
        "MMLU_virology":0.4397590361,
        "MMLU_world_religions":0.6666666667
    },
    {
        "Model":"tulu-13B-fp16",
        "URL":"https:\/\/huggingface.co\/TheBloke\/tulu-13B-fp16",
        "full_model_name":"TheBloke\/tulu-13B-fp16",
        "Parameters":13.0,
        "MMLU_average":0.5319496177,
        "arc:challenge|25":0.5110921502,
        "hellaswag|10":0.6066520613,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.5263157895,
        "MMLU_business_ethics":0.46,
        "MMLU_clinical_knowledge":0.5547169811,
        "MMLU_college_biology":0.5972222222,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.4219653179,
        "MMLU_college_physics":0.1764705882,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.4553191489,
        "MMLU_econometrics":0.3771929825,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.3386243386,
        "MMLU_formal_logic":0.373015873,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.5967741935,
        "MMLU_high_school_chemistry":0.3596059113,
        "MMLU_high_school_computer_science":0.5,
        "MMLU_high_school_european_history":0.7212121212,
        "MMLU_high_school_geography":0.6818181818,
        "MMLU_high_school_government_and_politics":0.7512953368,
        "MMLU_high_school_macroeconomics":0.5205128205,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.5756302521,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.7211009174,
        "MMLU_high_school_statistics":0.3657407407,
        "MMLU_high_school_us_history":0.6911764706,
        "MMLU_high_school_world_history":0.7594936709,
        "MMLU_human_aging":0.5739910314,
        "MMLU_human_sexuality":0.6183206107,
        "MMLU_international_law":0.6859504132,
        "MMLU_jurisprudence":0.7037037037,
        "MMLU_logical_fallacies":0.6380368098,
        "MMLU_machine_learning":0.4464285714,
        "MMLU_management":0.6990291262,
        "MMLU_marketing":0.7692307692,
        "MMLU_medical_genetics":0.51,
        "MMLU_miscellaneous":0.7139208174,
        "MMLU_moral_disputes":0.5953757225,
        "MMLU_moral_scenarios":0.330726257,
        "MMLU_nutrition":0.568627451,
        "MMLU_philosophy":0.6205787781,
        "MMLU_prehistory":0.5740740741,
        "MMLU_professional_accounting":0.4290780142,
        "MMLU_professional_law":0.3996088657,
        "MMLU_professional_medicine":0.4926470588,
        "MMLU_professional_psychology":0.5212418301,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6081632653,
        "MMLU_sociology":0.7562189055,
        "MMLU_us_foreign_policy":0.77,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.7426900585
    },
    {
        "Model":"Airoboros-L2-13B-2.1-GPTQ",
        "URL":"https:\/\/huggingface.co\/TheBloke\/Airoboros-L2-13B-2.1-GPTQ",
        "full_model_name":"TheBloke\/Airoboros-L2-13B-2.1-GPTQ",
        "Parameters":13.0,
        "MMLU_average":0.5315824645,
        "arc:challenge|25":0.5401023891,
        "hellaswag|10":0.6171081458,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.5197368421,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.5698113208,
        "MMLU_college_biology":0.5763888889,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.5028901734,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.3957446809,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.5517241379,
        "MMLU_elementary_mathematics":0.3201058201,
        "MMLU_formal_logic":0.3015873016,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.6193548387,
        "MMLU_high_school_chemistry":0.4384236453,
        "MMLU_high_school_computer_science":0.54,
        "MMLU_high_school_european_history":0.6787878788,
        "MMLU_high_school_geography":0.6868686869,
        "MMLU_high_school_government_and_politics":0.7772020725,
        "MMLU_high_school_macroeconomics":0.441025641,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.4453781513,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.6880733945,
        "MMLU_high_school_statistics":0.4027777778,
        "MMLU_high_school_us_history":0.7352941176,
        "MMLU_high_school_world_history":0.6877637131,
        "MMLU_human_aging":0.6278026906,
        "MMLU_human_sexuality":0.6030534351,
        "MMLU_international_law":0.7438016529,
        "MMLU_jurisprudence":0.6851851852,
        "MMLU_logical_fallacies":0.6319018405,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.640776699,
        "MMLU_marketing":0.735042735,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.7305236271,
        "MMLU_moral_disputes":0.598265896,
        "MMLU_moral_scenarios":0.3217877095,
        "MMLU_nutrition":0.5718954248,
        "MMLU_philosophy":0.6205787781,
        "MMLU_prehistory":0.6419753086,
        "MMLU_professional_accounting":0.3865248227,
        "MMLU_professional_law":0.3872229465,
        "MMLU_professional_medicine":0.4852941176,
        "MMLU_professional_psychology":0.5392156863,
        "MMLU_public_relations":0.6272727273,
        "MMLU_security_studies":0.5795918367,
        "MMLU_sociology":0.6865671642,
        "MMLU_us_foreign_policy":0.78,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.7192982456
    },
    {
        "Model":"Llama2-13B-no_robots-alpaca-lora",
        "URL":"https:\/\/huggingface.co\/Undi95\/Llama2-13B-no_robots-alpaca-lora",
        "full_model_name":"Undi95\/Llama2-13B-no_robots-alpaca-lora",
        "Parameters":13.0,
        "MMLU_average":0.531053209,
        "arc:challenge|25":0.5418088737,
        "hellaswag|10":0.63095001,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.5328947368,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.5660377358,
        "MMLU_college_biology":0.5625,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.4971098266,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.4127659574,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.5379310345,
        "MMLU_elementary_mathematics":0.3492063492,
        "MMLU_formal_logic":0.2619047619,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.6258064516,
        "MMLU_high_school_chemistry":0.4630541872,
        "MMLU_high_school_computer_science":0.5,
        "MMLU_high_school_european_history":0.6242424242,
        "MMLU_high_school_geography":0.6464646465,
        "MMLU_high_school_government_and_politics":0.7564766839,
        "MMLU_high_school_macroeconomics":0.5205128205,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.5588235294,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.6844036697,
        "MMLU_high_school_statistics":0.4212962963,
        "MMLU_high_school_us_history":0.7205882353,
        "MMLU_high_school_world_history":0.7383966245,
        "MMLU_human_aging":0.6502242152,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.7190082645,
        "MMLU_jurisprudence":0.6111111111,
        "MMLU_logical_fallacies":0.6625766871,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.735042735,
        "MMLU_medical_genetics":0.62,
        "MMLU_miscellaneous":0.7266922095,
        "MMLU_moral_disputes":0.6156069364,
        "MMLU_moral_scenarios":0.269273743,
        "MMLU_nutrition":0.5555555556,
        "MMLU_philosophy":0.61414791,
        "MMLU_prehistory":0.6172839506,
        "MMLU_professional_accounting":0.4184397163,
        "MMLU_professional_law":0.4113428944,
        "MMLU_professional_medicine":0.4338235294,
        "MMLU_professional_psychology":0.5294117647,
        "MMLU_public_relations":0.5363636364,
        "MMLU_security_studies":0.6163265306,
        "MMLU_sociology":0.7412935323,
        "MMLU_us_foreign_policy":0.74,
        "MMLU_virology":0.4819277108,
        "MMLU_world_religions":0.7485380117
    },
    {
        "Model":"llama2-13b-math1.2",
        "URL":"https:\/\/huggingface.co\/FelixChao\/llama2-13b-math1.2",
        "full_model_name":"FelixChao\/llama2-13b-math1.2",
        "Parameters":13.0,
        "MMLU_average":0.53050049,
        "arc:challenge|25":0.5204778157,
        "hellaswag|10":0.609241187,
        "MMLU_abstract_algebra":0.41,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5394736842,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.5698113208,
        "MMLU_college_biology":0.5555555556,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.450867052,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.65,
        "MMLU_conceptual_physics":0.4127659574,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.5103448276,
        "MMLU_elementary_mathematics":0.3148148148,
        "MMLU_formal_logic":0.2619047619,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.6193548387,
        "MMLU_high_school_chemistry":0.4236453202,
        "MMLU_high_school_computer_science":0.56,
        "MMLU_high_school_european_history":0.6787878788,
        "MMLU_high_school_geography":0.6818181818,
        "MMLU_high_school_government_and_politics":0.7098445596,
        "MMLU_high_school_macroeconomics":0.4692307692,
        "MMLU_high_school_mathematics":0.2925925926,
        "MMLU_high_school_microeconomics":0.5168067227,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.7027522936,
        "MMLU_high_school_statistics":0.3194444444,
        "MMLU_high_school_us_history":0.7549019608,
        "MMLU_high_school_world_history":0.7341772152,
        "MMLU_human_aging":0.6502242152,
        "MMLU_human_sexuality":0.5877862595,
        "MMLU_international_law":0.7685950413,
        "MMLU_jurisprudence":0.7037037037,
        "MMLU_logical_fallacies":0.6441717791,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.6893203883,
        "MMLU_marketing":0.7735042735,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.7254150702,
        "MMLU_moral_disputes":0.612716763,
        "MMLU_moral_scenarios":0.3117318436,
        "MMLU_nutrition":0.5947712418,
        "MMLU_philosophy":0.5916398714,
        "MMLU_prehistory":0.5895061728,
        "MMLU_professional_accounting":0.3829787234,
        "MMLU_professional_law":0.3709256845,
        "MMLU_professional_medicine":0.4669117647,
        "MMLU_professional_psychology":0.5245098039,
        "MMLU_public_relations":0.6272727273,
        "MMLU_security_studies":0.6204081633,
        "MMLU_sociology":0.7164179104,
        "MMLU_us_foreign_policy":0.75,
        "MMLU_virology":0.4638554217,
        "MMLU_world_religions":0.7485380117
    },
    {
        "Model":"chinese-llama-2-13b",
        "URL":"https:\/\/huggingface.co\/ziqingyang\/chinese-llama-2-13b",
        "full_model_name":"ziqingyang\/chinese-llama-2-13b",
        "Parameters":13.0,
        "MMLU_average":0.5301430647,
        "arc:challenge|25":0.5324232082,
        "hellaswag|10":0.5930093607,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.5037037037,
        "MMLU_astronomy":0.5263157895,
        "MMLU_business_ethics":0.5,
        "MMLU_clinical_knowledge":0.5849056604,
        "MMLU_college_biology":0.5416666667,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.549132948,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.5103448276,
        "MMLU_elementary_mathematics":0.3068783069,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.6709677419,
        "MMLU_high_school_chemistry":0.4482758621,
        "MMLU_high_school_computer_science":0.54,
        "MMLU_high_school_european_history":0.6242424242,
        "MMLU_high_school_geography":0.6616161616,
        "MMLU_high_school_government_and_politics":0.7616580311,
        "MMLU_high_school_macroeconomics":0.5282051282,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.5462184874,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.7082568807,
        "MMLU_high_school_statistics":0.4768518519,
        "MMLU_high_school_us_history":0.7156862745,
        "MMLU_high_school_world_history":0.7004219409,
        "MMLU_human_aging":0.5964125561,
        "MMLU_human_sexuality":0.5801526718,
        "MMLU_international_law":0.7685950413,
        "MMLU_jurisprudence":0.6851851852,
        "MMLU_logical_fallacies":0.6380368098,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.6699029126,
        "MMLU_marketing":0.7606837607,
        "MMLU_medical_genetics":0.51,
        "MMLU_miscellaneous":0.6922094508,
        "MMLU_moral_disputes":0.5895953757,
        "MMLU_moral_scenarios":0.2670391061,
        "MMLU_nutrition":0.568627451,
        "MMLU_philosophy":0.6495176849,
        "MMLU_prehistory":0.6080246914,
        "MMLU_professional_accounting":0.4184397163,
        "MMLU_professional_law":0.3930899609,
        "MMLU_professional_medicine":0.4779411765,
        "MMLU_professional_psychology":0.5163398693,
        "MMLU_public_relations":0.5727272727,
        "MMLU_security_studies":0.5714285714,
        "MMLU_sociology":0.6766169154,
        "MMLU_us_foreign_policy":0.77,
        "MMLU_virology":0.4397590361,
        "MMLU_world_religions":0.7368421053
    },
    {
        "Model":"llama-2-13b-FINETUNE3_3.3w-r4-q_k_v_o",
        "URL":"https:\/\/huggingface.co\/CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r4-q_k_v_o",
        "full_model_name":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r4-q_k_v_o",
        "Parameters":13.0,
        "MMLU_average":0.5299602044,
        "arc:challenge|25":0.5460750853,
        "hellaswag|10":0.6028679546,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.4074074074,
        "MMLU_astronomy":0.5263157895,
        "MMLU_business_ethics":0.49,
        "MMLU_clinical_knowledge":0.5660377358,
        "MMLU_college_biology":0.5416666667,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.4682080925,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.4723404255,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.4137931034,
        "MMLU_elementary_mathematics":0.3201058201,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.6225806452,
        "MMLU_high_school_chemistry":0.4039408867,
        "MMLU_high_school_computer_science":0.48,
        "MMLU_high_school_european_history":0.696969697,
        "MMLU_high_school_geography":0.6717171717,
        "MMLU_high_school_government_and_politics":0.7875647668,
        "MMLU_high_school_macroeconomics":0.4769230769,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.5924369748,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.7155963303,
        "MMLU_high_school_statistics":0.462962963,
        "MMLU_high_school_us_history":0.7009803922,
        "MMLU_high_school_world_history":0.7215189873,
        "MMLU_human_aging":0.6457399103,
        "MMLU_human_sexuality":0.5419847328,
        "MMLU_international_law":0.7520661157,
        "MMLU_jurisprudence":0.6203703704,
        "MMLU_logical_fallacies":0.6319018405,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.7087378641,
        "MMLU_marketing":0.7991452991,
        "MMLU_medical_genetics":0.65,
        "MMLU_miscellaneous":0.7279693487,
        "MMLU_moral_disputes":0.5838150289,
        "MMLU_moral_scenarios":0.2905027933,
        "MMLU_nutrition":0.5816993464,
        "MMLU_philosophy":0.6173633441,
        "MMLU_prehistory":0.5833333333,
        "MMLU_professional_accounting":0.4042553191,
        "MMLU_professional_law":0.3917861799,
        "MMLU_professional_medicine":0.4963235294,
        "MMLU_professional_psychology":0.5555555556,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.5510204082,
        "MMLU_sociology":0.6766169154,
        "MMLU_us_foreign_policy":0.72,
        "MMLU_virology":0.4518072289,
        "MMLU_world_religions":0.7368421053
    },
    {
        "Model":"wizardLM-13B-1.0-fp16",
        "URL":"https:\/\/huggingface.co\/TheBloke\/wizardLM-13B-1.0-fp16",
        "full_model_name":"TheBloke\/wizardLM-13B-1.0-fp16",
        "Parameters":13.0,
        "MMLU_average":0.5290438267,
        "arc:challenge|25":0.5537542662,
        "hellaswag|10":0.6105357499,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.5394736842,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.5358490566,
        "MMLU_college_biology":0.5347222222,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.4682080925,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.67,
        "MMLU_conceptual_physics":0.4170212766,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.4275862069,
        "MMLU_elementary_mathematics":0.3174603175,
        "MMLU_formal_logic":0.3968253968,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.5967741935,
        "MMLU_high_school_chemistry":0.3793103448,
        "MMLU_high_school_computer_science":0.56,
        "MMLU_high_school_european_history":0.6787878788,
        "MMLU_high_school_geography":0.702020202,
        "MMLU_high_school_government_and_politics":0.7202072539,
        "MMLU_high_school_macroeconomics":0.4743589744,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.5084033613,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.6990825688,
        "MMLU_high_school_statistics":0.3611111111,
        "MMLU_high_school_us_history":0.6666666667,
        "MMLU_high_school_world_history":0.7088607595,
        "MMLU_human_aging":0.5874439462,
        "MMLU_human_sexuality":0.6335877863,
        "MMLU_international_law":0.6694214876,
        "MMLU_jurisprudence":0.6388888889,
        "MMLU_logical_fallacies":0.6380368098,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.7572815534,
        "MMLU_marketing":0.7777777778,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.724137931,
        "MMLU_moral_disputes":0.5809248555,
        "MMLU_moral_scenarios":0.2670391061,
        "MMLU_nutrition":0.591503268,
        "MMLU_philosophy":0.5852090032,
        "MMLU_prehistory":0.5679012346,
        "MMLU_professional_accounting":0.414893617,
        "MMLU_professional_law":0.4224250326,
        "MMLU_professional_medicine":0.5551470588,
        "MMLU_professional_psychology":0.5392156863,
        "MMLU_public_relations":0.6,
        "MMLU_security_studies":0.6408163265,
        "MMLU_sociology":0.7412935323,
        "MMLU_us_foreign_policy":0.8,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.7543859649
    },
    {
        "Model":"L2-MythoMax22b-instruct-Falseblock",
        "URL":"https:\/\/huggingface.co\/grimpep\/L2-MythoMax22b-instruct-Falseblock",
        "full_model_name":"grimpep\/L2-MythoMax22b-instruct-Falseblock",
        "Parameters":22.0,
        "MMLU_average":0.5289926018,
        "arc:challenge|25":0.5614334471,
        "hellaswag|10":0.6187014539,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.5131578947,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.5547169811,
        "MMLU_college_biology":0.5416666667,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.450867052,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.62,
        "MMLU_conceptual_physics":0.4340425532,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.3253968254,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.41,
        "MMLU_high_school_biology":0.6096774194,
        "MMLU_high_school_chemistry":0.4236453202,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.6565656566,
        "MMLU_high_school_government_and_politics":0.7720207254,
        "MMLU_high_school_macroeconomics":0.5025641026,
        "MMLU_high_school_mathematics":0.3111111111,
        "MMLU_high_school_microeconomics":0.5546218487,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.6788990826,
        "MMLU_high_school_statistics":0.3564814815,
        "MMLU_high_school_us_history":0.7058823529,
        "MMLU_high_school_world_history":0.7172995781,
        "MMLU_human_aging":0.6367713004,
        "MMLU_human_sexuality":0.6183206107,
        "MMLU_international_law":0.6859504132,
        "MMLU_jurisprudence":0.7037037037,
        "MMLU_logical_fallacies":0.6441717791,
        "MMLU_machine_learning":0.2946428571,
        "MMLU_management":0.6699029126,
        "MMLU_marketing":0.7905982906,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.6947637292,
        "MMLU_moral_disputes":0.6098265896,
        "MMLU_moral_scenarios":0.2413407821,
        "MMLU_nutrition":0.591503268,
        "MMLU_philosophy":0.6173633441,
        "MMLU_prehistory":0.6018518519,
        "MMLU_professional_accounting":0.4290780142,
        "MMLU_professional_law":0.4028683181,
        "MMLU_professional_medicine":0.5147058824,
        "MMLU_professional_psychology":0.5245098039,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6163265306,
        "MMLU_sociology":0.6567164179,
        "MMLU_us_foreign_policy":0.73,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.7368421053
    },
    {
        "Model":"mcq-hal-vicuna-13b-v1.5",
        "URL":"https:\/\/huggingface.co\/luffycodes\/mcq-hal-vicuna-13b-v1.5",
        "full_model_name":"luffycodes\/mcq-hal-vicuna-13b-v1.5",
        "Parameters":13.0,
        "MMLU_average":0.5289819155,
        "arc:challenge|25":0.5196245734,
        "hellaswag|10":0.6095399323,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4444444444,
        "MMLU_astronomy":0.5197368421,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.5509433962,
        "MMLU_college_biology":0.5347222222,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.4739884393,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.4085106383,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.3201058201,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.41,
        "MMLU_high_school_biology":0.5935483871,
        "MMLU_high_school_chemistry":0.4334975369,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.6848484848,
        "MMLU_high_school_geography":0.6565656566,
        "MMLU_high_school_government_and_politics":0.7202072539,
        "MMLU_high_school_macroeconomics":0.5179487179,
        "MMLU_high_school_mathematics":0.337037037,
        "MMLU_high_school_microeconomics":0.5546218487,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.6917431193,
        "MMLU_high_school_statistics":0.3564814815,
        "MMLU_high_school_us_history":0.7107843137,
        "MMLU_high_school_world_history":0.6919831224,
        "MMLU_human_aging":0.6457399103,
        "MMLU_human_sexuality":0.5877862595,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.6388888889,
        "MMLU_logical_fallacies":0.6441717791,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.7521367521,
        "MMLU_medical_genetics":0.5,
        "MMLU_miscellaneous":0.7458492976,
        "MMLU_moral_disputes":0.5231213873,
        "MMLU_moral_scenarios":0.2547486034,
        "MMLU_nutrition":0.6078431373,
        "MMLU_philosophy":0.6173633441,
        "MMLU_prehistory":0.5802469136,
        "MMLU_professional_accounting":0.3865248227,
        "MMLU_professional_law":0.4217731421,
        "MMLU_professional_medicine":0.4926470588,
        "MMLU_professional_psychology":0.5310457516,
        "MMLU_public_relations":0.6454545455,
        "MMLU_security_studies":0.6081632653,
        "MMLU_sociology":0.6915422886,
        "MMLU_us_foreign_policy":0.75,
        "MMLU_virology":0.4397590361,
        "MMLU_world_religions":0.7251461988
    },
    {
        "Model":"ANIMA-Phi-Neptune-Mistral-7B",
        "URL":"https:\/\/huggingface.co\/Severian\/ANIMA-Phi-Neptune-Mistral-7B",
        "full_model_name":"Severian\/ANIMA-Phi-Neptune-Mistral-7B",
        "Parameters":7.0,
        "MMLU_average":0.5289233986,
        "arc:challenge|25":0.5145051195,
        "hellaswag|10":0.5718980283,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.5394736842,
        "MMLU_business_ethics":0.49,
        "MMLU_clinical_knowledge":0.6037735849,
        "MMLU_college_biology":0.5277777778,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.5,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.549132948,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.65,
        "MMLU_conceptual_physics":0.4382978723,
        "MMLU_econometrics":0.3421052632,
        "MMLU_electrical_engineering":0.5448275862,
        "MMLU_elementary_mathematics":0.3597883598,
        "MMLU_formal_logic":0.3888888889,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.5870967742,
        "MMLU_high_school_chemistry":0.3694581281,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.6545454545,
        "MMLU_high_school_geography":0.6818181818,
        "MMLU_high_school_government_and_politics":0.725388601,
        "MMLU_high_school_macroeconomics":0.5,
        "MMLU_high_school_mathematics":0.3111111111,
        "MMLU_high_school_microeconomics":0.4831932773,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.7119266055,
        "MMLU_high_school_statistics":0.3703703704,
        "MMLU_high_school_us_history":0.7156862745,
        "MMLU_high_school_world_history":0.7046413502,
        "MMLU_human_aging":0.5874439462,
        "MMLU_human_sexuality":0.6030534351,
        "MMLU_international_law":0.6776859504,
        "MMLU_jurisprudence":0.6203703704,
        "MMLU_logical_fallacies":0.6196319018,
        "MMLU_machine_learning":0.4375,
        "MMLU_management":0.6504854369,
        "MMLU_marketing":0.8205128205,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.7062579821,
        "MMLU_moral_disputes":0.5606936416,
        "MMLU_moral_scenarios":0.3061452514,
        "MMLU_nutrition":0.545751634,
        "MMLU_philosophy":0.6012861736,
        "MMLU_prehistory":0.6049382716,
        "MMLU_professional_accounting":0.3758865248,
        "MMLU_professional_law":0.387874837,
        "MMLU_professional_medicine":0.4669117647,
        "MMLU_professional_psychology":0.5049019608,
        "MMLU_public_relations":0.6272727273,
        "MMLU_security_studies":0.6489795918,
        "MMLU_sociology":0.7313432836,
        "MMLU_us_foreign_policy":0.74,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.7192982456
    },
    {
        "Model":"StableBeluga-7B",
        "URL":"https:\/\/huggingface.co\/stabilityai\/StableBeluga-7B",
        "full_model_name":"stabilityai\/StableBeluga-7B",
        "Parameters":7.0,
        "MMLU_average":0.5271247199,
        "arc:challenge|25":0.5341296928,
        "hellaswag|10":0.5966938857,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.4539473684,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.6188679245,
        "MMLU_college_biology":0.5208333333,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.4450867052,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.4936170213,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.3174603175,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.5709677419,
        "MMLU_high_school_chemistry":0.3596059113,
        "MMLU_high_school_computer_science":0.43,
        "MMLU_high_school_european_history":0.7090909091,
        "MMLU_high_school_geography":0.6868686869,
        "MMLU_high_school_government_and_politics":0.7823834197,
        "MMLU_high_school_macroeconomics":0.5102564103,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.5378151261,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.728440367,
        "MMLU_high_school_statistics":0.4814814815,
        "MMLU_high_school_us_history":0.7009803922,
        "MMLU_high_school_world_history":0.7172995781,
        "MMLU_human_aging":0.6188340807,
        "MMLU_human_sexuality":0.641221374,
        "MMLU_international_law":0.7107438017,
        "MMLU_jurisprudence":0.6296296296,
        "MMLU_logical_fallacies":0.5950920245,
        "MMLU_machine_learning":0.4017857143,
        "MMLU_management":0.7475728155,
        "MMLU_marketing":0.7863247863,
        "MMLU_medical_genetics":0.63,
        "MMLU_miscellaneous":0.724137931,
        "MMLU_moral_disputes":0.5606936416,
        "MMLU_moral_scenarios":0.243575419,
        "MMLU_nutrition":0.5490196078,
        "MMLU_philosophy":0.5852090032,
        "MMLU_prehistory":0.5679012346,
        "MMLU_professional_accounting":0.390070922,
        "MMLU_professional_law":0.3924380704,
        "MMLU_professional_medicine":0.5294117647,
        "MMLU_professional_psychology":0.5245098039,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.6244897959,
        "MMLU_sociology":0.6766169154,
        "MMLU_us_foreign_policy":0.75,
        "MMLU_virology":0.3975903614,
        "MMLU_world_religions":0.7192982456
    },
    {
        "Model":"tableBeluga-7B-instruct-pl-lora_unload",
        "URL":"https:\/\/huggingface.co\/Lajonbot\/tableBeluga-7B-instruct-pl-lora_unload",
        "full_model_name":"Lajonbot\/tableBeluga-7B-instruct-pl-lora_unload",
        "Parameters":7.0,
        "MMLU_average":0.5269866297,
        "arc:challenge|25":0.5341296928,
        "hellaswag|10":0.5966938857,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.4539473684,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.6188679245,
        "MMLU_college_biology":0.5208333333,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.4450867052,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.4936170213,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.3174603175,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.5709677419,
        "MMLU_high_school_chemistry":0.3596059113,
        "MMLU_high_school_computer_science":0.43,
        "MMLU_high_school_european_history":0.696969697,
        "MMLU_high_school_geography":0.6868686869,
        "MMLU_high_school_government_and_politics":0.7823834197,
        "MMLU_high_school_macroeconomics":0.5102564103,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.5378151261,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.728440367,
        "MMLU_high_school_statistics":0.4814814815,
        "MMLU_high_school_us_history":0.7058823529,
        "MMLU_high_school_world_history":0.7172995781,
        "MMLU_human_aging":0.6188340807,
        "MMLU_human_sexuality":0.641221374,
        "MMLU_international_law":0.7107438017,
        "MMLU_jurisprudence":0.6296296296,
        "MMLU_logical_fallacies":0.5950920245,
        "MMLU_machine_learning":0.4017857143,
        "MMLU_management":0.7475728155,
        "MMLU_marketing":0.7863247863,
        "MMLU_medical_genetics":0.63,
        "MMLU_miscellaneous":0.724137931,
        "MMLU_moral_disputes":0.5606936416,
        "MMLU_moral_scenarios":0.243575419,
        "MMLU_nutrition":0.5490196078,
        "MMLU_philosophy":0.5852090032,
        "MMLU_prehistory":0.5679012346,
        "MMLU_professional_accounting":0.390070922,
        "MMLU_professional_law":0.3917861799,
        "MMLU_professional_medicine":0.5294117647,
        "MMLU_professional_psychology":0.5245098039,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.6244897959,
        "MMLU_sociology":0.6766169154,
        "MMLU_us_foreign_policy":0.75,
        "MMLU_virology":0.3975903614,
        "MMLU_world_religions":0.7192982456
    },
    {
        "Model":"firefly-llama2-13b-chat",
        "URL":"https:\/\/huggingface.co\/YeungNLP\/firefly-llama2-13b-chat",
        "full_model_name":"YeungNLP\/firefly-llama2-13b-chat",
        "Parameters":13.0,
        "MMLU_average":0.5255558808,
        "arc:challenge|25":0.5273037543,
        "hellaswag|10":0.590420235,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.4802631579,
        "MMLU_business_ethics":0.58,
        "MMLU_clinical_knowledge":0.5698113208,
        "MMLU_college_biology":0.5555555556,
        "MMLU_college_chemistry":0.32,
        "MMLU_college_computer_science":0.38,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.450867052,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.63,
        "MMLU_conceptual_physics":0.4042553191,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.5103448276,
        "MMLU_elementary_mathematics":0.3333333333,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.6161290323,
        "MMLU_high_school_chemistry":0.3694581281,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.6606060606,
        "MMLU_high_school_geography":0.6565656566,
        "MMLU_high_school_government_and_politics":0.725388601,
        "MMLU_high_school_macroeconomics":0.4820512821,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.4915966387,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.7009174312,
        "MMLU_high_school_statistics":0.3611111111,
        "MMLU_high_school_us_history":0.7205882353,
        "MMLU_high_school_world_history":0.7130801688,
        "MMLU_human_aging":0.6502242152,
        "MMLU_human_sexuality":0.6030534351,
        "MMLU_international_law":0.7438016529,
        "MMLU_jurisprudence":0.6574074074,
        "MMLU_logical_fallacies":0.6196319018,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.6504854369,
        "MMLU_marketing":0.7692307692,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.7088122605,
        "MMLU_moral_disputes":0.6069364162,
        "MMLU_moral_scenarios":0.3854748603,
        "MMLU_nutrition":0.5980392157,
        "MMLU_philosophy":0.5884244373,
        "MMLU_prehistory":0.5771604938,
        "MMLU_professional_accounting":0.3439716312,
        "MMLU_professional_law":0.3956975228,
        "MMLU_professional_medicine":0.4889705882,
        "MMLU_professional_psychology":0.5375816993,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.6163265306,
        "MMLU_sociology":0.6865671642,
        "MMLU_us_foreign_policy":0.73,
        "MMLU_virology":0.4759036145,
        "MMLU_world_religions":0.7134502924
    },
    {
        "Model":"Stheno-Mix-L2-20B",
        "URL":"https:\/\/huggingface.co\/Sao10K\/Stheno-Mix-L2-20B",
        "full_model_name":"Sao10K\/Stheno-Mix-L2-20B",
        "Parameters":20.0,
        "MMLU_average":0.5250678935,
        "arc:challenge|25":0.5460750853,
        "hellaswag|10":0.6040629357,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4518518519,
        "MMLU_astronomy":0.5,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.5547169811,
        "MMLU_college_biology":0.5347222222,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.4855491329,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.4170212766,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.5103448276,
        "MMLU_elementary_mathematics":0.2936507937,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.6129032258,
        "MMLU_high_school_chemistry":0.4433497537,
        "MMLU_high_school_computer_science":0.47,
        "MMLU_high_school_european_history":0.6606060606,
        "MMLU_high_school_geography":0.6767676768,
        "MMLU_high_school_government_and_politics":0.7668393782,
        "MMLU_high_school_macroeconomics":0.4743589744,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.4957983193,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.7119266055,
        "MMLU_high_school_statistics":0.337962963,
        "MMLU_high_school_us_history":0.7254901961,
        "MMLU_high_school_world_history":0.6835443038,
        "MMLU_human_aging":0.6367713004,
        "MMLU_human_sexuality":0.5954198473,
        "MMLU_international_law":0.7355371901,
        "MMLU_jurisprudence":0.6666666667,
        "MMLU_logical_fallacies":0.6441717791,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.6699029126,
        "MMLU_marketing":0.7435897436,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.724137931,
        "MMLU_moral_disputes":0.5838150289,
        "MMLU_moral_scenarios":0.2715083799,
        "MMLU_nutrition":0.5882352941,
        "MMLU_philosophy":0.6109324759,
        "MMLU_prehistory":0.6234567901,
        "MMLU_professional_accounting":0.3936170213,
        "MMLU_professional_law":0.3872229465,
        "MMLU_professional_medicine":0.5036764706,
        "MMLU_professional_psychology":0.5196078431,
        "MMLU_public_relations":0.6,
        "MMLU_security_studies":0.6163265306,
        "MMLU_sociology":0.6965174129,
        "MMLU_us_foreign_policy":0.79,
        "MMLU_virology":0.4518072289,
        "MMLU_world_religions":0.7192982456
    },
    {
        "Model":"llama2-22b-daydreamer-v3",
        "URL":"https:\/\/huggingface.co\/nkpz\/llama2-22b-daydreamer-v3",
        "full_model_name":"nkpz\/llama2-22b-daydreamer-v3",
        "Parameters":22.0,
        "MMLU_average":0.5249323431,
        "arc:challenge|25":0.5255972696,
        "hellaswag|10":0.601075483,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.5394736842,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.5849056604,
        "MMLU_college_biology":0.5347222222,
        "MMLU_college_chemistry":0.46,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.4566473988,
        "MMLU_college_physics":0.3431372549,
        "MMLU_computer_security":0.64,
        "MMLU_conceptual_physics":0.3234042553,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.3201058201,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.6419354839,
        "MMLU_high_school_chemistry":0.3694581281,
        "MMLU_high_school_computer_science":0.51,
        "MMLU_high_school_european_history":0.6848484848,
        "MMLU_high_school_geography":0.6666666667,
        "MMLU_high_school_government_and_politics":0.7616580311,
        "MMLU_high_school_macroeconomics":0.5153846154,
        "MMLU_high_school_mathematics":0.3111111111,
        "MMLU_high_school_microeconomics":0.4957983193,
        "MMLU_high_school_physics":0.3841059603,
        "MMLU_high_school_psychology":0.7247706422,
        "MMLU_high_school_statistics":0.4583333333,
        "MMLU_high_school_us_history":0.7450980392,
        "MMLU_high_school_world_history":0.7088607595,
        "MMLU_human_aging":0.5874439462,
        "MMLU_human_sexuality":0.6030534351,
        "MMLU_international_law":0.694214876,
        "MMLU_jurisprudence":0.6203703704,
        "MMLU_logical_fallacies":0.6564417178,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.6601941748,
        "MMLU_marketing":0.7478632479,
        "MMLU_medical_genetics":0.62,
        "MMLU_miscellaneous":0.7011494253,
        "MMLU_moral_disputes":0.5693641618,
        "MMLU_moral_scenarios":0.2536312849,
        "MMLU_nutrition":0.5261437908,
        "MMLU_philosophy":0.6012861736,
        "MMLU_prehistory":0.5709876543,
        "MMLU_professional_accounting":0.365248227,
        "MMLU_professional_law":0.4002607562,
        "MMLU_professional_medicine":0.5036764706,
        "MMLU_professional_psychology":0.5261437908,
        "MMLU_public_relations":0.5727272727,
        "MMLU_security_studies":0.4857142857,
        "MMLU_sociology":0.6965174129,
        "MMLU_us_foreign_policy":0.7,
        "MMLU_virology":0.4036144578,
        "MMLU_world_religions":0.7368421053
    },
    {
        "Model":"Asclepius-Llama2-13B",
        "URL":"https:\/\/huggingface.co\/starmpcc\/Asclepius-Llama2-13B",
        "full_model_name":"starmpcc\/Asclepius-Llama2-13B",
        "Parameters":13.0,
        "MMLU_average":0.5238050313,
        "arc:challenge|25":0.5324232082,
        "hellaswag|10":0.6115315674,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4444444444,
        "MMLU_astronomy":0.5197368421,
        "MMLU_business_ethics":0.49,
        "MMLU_clinical_knowledge":0.5962264151,
        "MMLU_college_biology":0.5763888889,
        "MMLU_college_chemistry":0.36,
        "MMLU_college_computer_science":0.49,
        "MMLU_college_mathematics":0.41,
        "MMLU_college_medicine":0.5260115607,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.429787234,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.4965517241,
        "MMLU_elementary_mathematics":0.3439153439,
        "MMLU_formal_logic":0.4285714286,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.6096774194,
        "MMLU_high_school_chemistry":0.3990147783,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.6545454545,
        "MMLU_high_school_geography":0.6868686869,
        "MMLU_high_school_government_and_politics":0.7668393782,
        "MMLU_high_school_macroeconomics":0.5307692308,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.525210084,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.7082568807,
        "MMLU_high_school_statistics":0.4305555556,
        "MMLU_high_school_us_history":0.7254901961,
        "MMLU_high_school_world_history":0.6835443038,
        "MMLU_human_aging":0.5605381166,
        "MMLU_human_sexuality":0.6183206107,
        "MMLU_international_law":0.5950413223,
        "MMLU_jurisprudence":0.6296296296,
        "MMLU_logical_fallacies":0.6196319018,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.735042735,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.7318007663,
        "MMLU_moral_disputes":0.5578034682,
        "MMLU_moral_scenarios":0.251396648,
        "MMLU_nutrition":0.5326797386,
        "MMLU_philosophy":0.5948553055,
        "MMLU_prehistory":0.5864197531,
        "MMLU_professional_accounting":0.365248227,
        "MMLU_professional_law":0.3761408083,
        "MMLU_professional_medicine":0.5110294118,
        "MMLU_professional_psychology":0.4934640523,
        "MMLU_public_relations":0.5636363636,
        "MMLU_security_studies":0.5469387755,
        "MMLU_sociology":0.736318408,
        "MMLU_us_foreign_policy":0.74,
        "MMLU_virology":0.4096385542,
        "MMLU_world_religions":0.7426900585
    },
    {
        "Model":"orca_mini_v3_7b",
        "URL":"https:\/\/huggingface.co\/pankajmathur\/orca_mini_v3_7b",
        "full_model_name":"pankajmathur\/orca_mini_v3_7b",
        "Parameters":7.0,
        "MMLU_average":0.5236900854,
        "arc:challenge|25":0.5298634812,
        "hellaswag|10":0.6064528978,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5065789474,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.6075471698,
        "MMLU_college_biology":0.5486111111,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.4,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.4566473988,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.57,
        "MMLU_conceptual_physics":0.4936170213,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.3121693122,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.5548387097,
        "MMLU_high_school_chemistry":0.3793103448,
        "MMLU_high_school_computer_science":0.46,
        "MMLU_high_school_european_history":0.7151515152,
        "MMLU_high_school_geography":0.696969697,
        "MMLU_high_school_government_and_politics":0.7720207254,
        "MMLU_high_school_macroeconomics":0.5128205128,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.5210084034,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.7211009174,
        "MMLU_high_school_statistics":0.4351851852,
        "MMLU_high_school_us_history":0.7009803922,
        "MMLU_high_school_world_history":0.746835443,
        "MMLU_human_aging":0.5874439462,
        "MMLU_human_sexuality":0.5954198473,
        "MMLU_international_law":0.694214876,
        "MMLU_jurisprudence":0.6388888889,
        "MMLU_logical_fallacies":0.5950920245,
        "MMLU_machine_learning":0.4285714286,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.7820512821,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7228607918,
        "MMLU_moral_disputes":0.5693641618,
        "MMLU_moral_scenarios":0.2726256983,
        "MMLU_nutrition":0.5555555556,
        "MMLU_philosophy":0.5948553055,
        "MMLU_prehistory":0.5802469136,
        "MMLU_professional_accounting":0.3936170213,
        "MMLU_professional_law":0.3950456323,
        "MMLU_professional_medicine":0.5257352941,
        "MMLU_professional_psychology":0.5,
        "MMLU_public_relations":0.5636363636,
        "MMLU_security_studies":0.6163265306,
        "MMLU_sociology":0.6517412935,
        "MMLU_us_foreign_policy":0.73,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.7134502924
    },
    {
        "Model":"orca_mini_v3_7b",
        "URL":"https:\/\/huggingface.co\/psmathur\/orca_mini_v3_7b",
        "full_model_name":"psmathur\/orca_mini_v3_7b",
        "Parameters":7.0,
        "MMLU_average":0.5236900854,
        "arc:challenge|25":0.5298634812,
        "hellaswag|10":0.6064528978,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5065789474,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.6075471698,
        "MMLU_college_biology":0.5486111111,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.4,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.4566473988,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.57,
        "MMLU_conceptual_physics":0.4936170213,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.3121693122,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.5548387097,
        "MMLU_high_school_chemistry":0.3793103448,
        "MMLU_high_school_computer_science":0.46,
        "MMLU_high_school_european_history":0.7151515152,
        "MMLU_high_school_geography":0.696969697,
        "MMLU_high_school_government_and_politics":0.7720207254,
        "MMLU_high_school_macroeconomics":0.5128205128,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.5210084034,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.7211009174,
        "MMLU_high_school_statistics":0.4351851852,
        "MMLU_high_school_us_history":0.7009803922,
        "MMLU_high_school_world_history":0.746835443,
        "MMLU_human_aging":0.5874439462,
        "MMLU_human_sexuality":0.5954198473,
        "MMLU_international_law":0.694214876,
        "MMLU_jurisprudence":0.6388888889,
        "MMLU_logical_fallacies":0.5950920245,
        "MMLU_machine_learning":0.4285714286,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.7820512821,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7228607918,
        "MMLU_moral_disputes":0.5693641618,
        "MMLU_moral_scenarios":0.2726256983,
        "MMLU_nutrition":0.5555555556,
        "MMLU_philosophy":0.5948553055,
        "MMLU_prehistory":0.5802469136,
        "MMLU_professional_accounting":0.3936170213,
        "MMLU_professional_law":0.3950456323,
        "MMLU_professional_medicine":0.5257352941,
        "MMLU_professional_psychology":0.5,
        "MMLU_public_relations":0.5636363636,
        "MMLU_security_studies":0.6163265306,
        "MMLU_sociology":0.6517412935,
        "MMLU_us_foreign_policy":0.73,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.7134502924
    },
    {
        "Model":"Huginn-13b-v4.5",
        "URL":"https:\/\/huggingface.co\/The-Face-Of-Goonery\/Huginn-13b-v4.5",
        "full_model_name":"The-Face-Of-Goonery\/Huginn-13b-v4.5",
        "Parameters":13.0,
        "MMLU_average":0.5232432088,
        "arc:challenge|25":0.5750853242,
        "hellaswag|10":0.6285600478,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.5328947368,
        "MMLU_business_ethics":0.46,
        "MMLU_clinical_knowledge":0.5773584906,
        "MMLU_college_biology":0.5208333333,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4855491329,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.67,
        "MMLU_conceptual_physics":0.4382978723,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.3095238095,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.6419354839,
        "MMLU_high_school_chemistry":0.3743842365,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.6606060606,
        "MMLU_high_school_geography":0.6767676768,
        "MMLU_high_school_government_and_politics":0.7564766839,
        "MMLU_high_school_macroeconomics":0.5051282051,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.5210084034,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.7100917431,
        "MMLU_high_school_statistics":0.4351851852,
        "MMLU_high_school_us_history":0.6960784314,
        "MMLU_high_school_world_history":0.6877637131,
        "MMLU_human_aging":0.6457399103,
        "MMLU_human_sexuality":0.6183206107,
        "MMLU_international_law":0.6859504132,
        "MMLU_jurisprudence":0.6574074074,
        "MMLU_logical_fallacies":0.5950920245,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.7572815534,
        "MMLU_marketing":0.7948717949,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.7075351213,
        "MMLU_moral_disputes":0.5953757225,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.5751633987,
        "MMLU_philosophy":0.5852090032,
        "MMLU_prehistory":0.5555555556,
        "MMLU_professional_accounting":0.3936170213,
        "MMLU_professional_law":0.3970013038,
        "MMLU_professional_medicine":0.5073529412,
        "MMLU_professional_psychology":0.4787581699,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.6244897959,
        "MMLU_sociology":0.7164179104,
        "MMLU_us_foreign_policy":0.75,
        "MMLU_virology":0.4096385542,
        "MMLU_world_religions":0.7134502924
    },
    {
        "Model":"Huginn-13b-V4",
        "URL":"https:\/\/huggingface.co\/The-Face-Of-Goonery\/Huginn-13b-V4",
        "full_model_name":"The-Face-Of-Goonery\/Huginn-13b-V4",
        "Parameters":13.0,
        "MMLU_average":0.5232432088,
        "arc:challenge|25":0.5750853242,
        "hellaswag|10":0.6285600478,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.5328947368,
        "MMLU_business_ethics":0.46,
        "MMLU_clinical_knowledge":0.5773584906,
        "MMLU_college_biology":0.5208333333,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4855491329,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.67,
        "MMLU_conceptual_physics":0.4382978723,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.3095238095,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.6419354839,
        "MMLU_high_school_chemistry":0.3743842365,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.6606060606,
        "MMLU_high_school_geography":0.6767676768,
        "MMLU_high_school_government_and_politics":0.7564766839,
        "MMLU_high_school_macroeconomics":0.5051282051,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.5210084034,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.7100917431,
        "MMLU_high_school_statistics":0.4351851852,
        "MMLU_high_school_us_history":0.6960784314,
        "MMLU_high_school_world_history":0.6877637131,
        "MMLU_human_aging":0.6457399103,
        "MMLU_human_sexuality":0.6183206107,
        "MMLU_international_law":0.6859504132,
        "MMLU_jurisprudence":0.6574074074,
        "MMLU_logical_fallacies":0.5950920245,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.7572815534,
        "MMLU_marketing":0.7948717949,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.7075351213,
        "MMLU_moral_disputes":0.5953757225,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.5751633987,
        "MMLU_philosophy":0.5852090032,
        "MMLU_prehistory":0.5555555556,
        "MMLU_professional_accounting":0.3936170213,
        "MMLU_professional_law":0.3970013038,
        "MMLU_professional_medicine":0.5073529412,
        "MMLU_professional_psychology":0.4787581699,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.6244897959,
        "MMLU_sociology":0.7164179104,
        "MMLU_us_foreign_policy":0.75,
        "MMLU_virology":0.4096385542,
        "MMLU_world_religions":0.7134502924
    },
    {
        "Model":"Yousei-22B",
        "URL":"https:\/\/huggingface.co\/Envoid\/Yousei-22B",
        "full_model_name":"Envoid\/Yousei-22B",
        "Parameters":22.0,
        "MMLU_average":0.5230953995,
        "arc:challenge|25":0.5127986348,
        "hellaswag|10":0.5825532762,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.4934210526,
        "MMLU_business_ethics":0.5,
        "MMLU_clinical_knowledge":0.5924528302,
        "MMLU_college_biology":0.5416666667,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4739884393,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.6,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.4413793103,
        "MMLU_elementary_mathematics":0.3095238095,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.6451612903,
        "MMLU_high_school_chemistry":0.3891625616,
        "MMLU_high_school_computer_science":0.47,
        "MMLU_high_school_european_history":0.6545454545,
        "MMLU_high_school_geography":0.6464646465,
        "MMLU_high_school_government_and_politics":0.7202072539,
        "MMLU_high_school_macroeconomics":0.4743589744,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.5336134454,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.7009174312,
        "MMLU_high_school_statistics":0.3472222222,
        "MMLU_high_school_us_history":0.7058823529,
        "MMLU_high_school_world_history":0.7088607595,
        "MMLU_human_aging":0.6143497758,
        "MMLU_human_sexuality":0.5572519084,
        "MMLU_international_law":0.6859504132,
        "MMLU_jurisprudence":0.6851851852,
        "MMLU_logical_fallacies":0.6380368098,
        "MMLU_machine_learning":0.4553571429,
        "MMLU_management":0.572815534,
        "MMLU_marketing":0.7991452991,
        "MMLU_medical_genetics":0.61,
        "MMLU_miscellaneous":0.7203065134,
        "MMLU_moral_disputes":0.5809248555,
        "MMLU_moral_scenarios":0.3720670391,
        "MMLU_nutrition":0.5588235294,
        "MMLU_philosophy":0.6077170418,
        "MMLU_prehistory":0.6358024691,
        "MMLU_professional_accounting":0.3617021277,
        "MMLU_professional_law":0.4074315515,
        "MMLU_professional_medicine":0.4852941176,
        "MMLU_professional_psychology":0.5016339869,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6612244898,
        "MMLU_sociology":0.6368159204,
        "MMLU_us_foreign_policy":0.79,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.7543859649
    },
    {
        "Model":"vicuna-13b-v1.3-PL-lora_unload",
        "URL":"https:\/\/huggingface.co\/Lajonbot\/vicuna-13b-v1.3-PL-lora_unload",
        "full_model_name":"Lajonbot\/vicuna-13b-v1.3-PL-lora_unload",
        "Parameters":13.0,
        "MMLU_average":0.5219507265,
        "arc:challenge|25":0.5290102389,
        "hellaswag|10":0.5996813384,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.4868421053,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.4905660377,
        "MMLU_college_biology":0.5763888889,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.3930635838,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.4042553191,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.4827586207,
        "MMLU_elementary_mathematics":0.291005291,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.5774193548,
        "MMLU_high_school_chemistry":0.4285714286,
        "MMLU_high_school_computer_science":0.53,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.6868686869,
        "MMLU_high_school_government_and_politics":0.7512953368,
        "MMLU_high_school_macroeconomics":0.4435897436,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.5,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.7119266055,
        "MMLU_high_school_statistics":0.3842592593,
        "MMLU_high_school_us_history":0.6911764706,
        "MMLU_high_school_world_history":0.6877637131,
        "MMLU_human_aging":0.5695067265,
        "MMLU_human_sexuality":0.6564885496,
        "MMLU_international_law":0.6776859504,
        "MMLU_jurisprudence":0.5833333333,
        "MMLU_logical_fallacies":0.6134969325,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.8034188034,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.717752235,
        "MMLU_moral_disputes":0.5664739884,
        "MMLU_moral_scenarios":0.2882681564,
        "MMLU_nutrition":0.6013071895,
        "MMLU_philosophy":0.5691318328,
        "MMLU_prehistory":0.5648148148,
        "MMLU_professional_accounting":0.3936170213,
        "MMLU_professional_law":0.4100391134,
        "MMLU_professional_medicine":0.5,
        "MMLU_professional_psychology":0.5130718954,
        "MMLU_public_relations":0.5818181818,
        "MMLU_security_studies":0.5714285714,
        "MMLU_sociology":0.7213930348,
        "MMLU_us_foreign_policy":0.75,
        "MMLU_virology":0.4879518072,
        "MMLU_world_religions":0.7894736842
    },
    {
        "Model":"ANIMA-Phi-Neptune-Mistral-7B-v1",
        "URL":"https:\/\/huggingface.co\/Severian\/ANIMA-Phi-Neptune-Mistral-7B-v1",
        "full_model_name":"Severian\/ANIMA-Phi-Neptune-Mistral-7B-v1",
        "Parameters":7.0,
        "MMLU_average":0.5218180554,
        "arc:challenge|25":0.5,
        "hellaswag|10":0.5657239594,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4074074074,
        "MMLU_astronomy":0.4868421053,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.5849056604,
        "MMLU_college_biology":0.5277777778,
        "MMLU_college_chemistry":0.33,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.23,
        "MMLU_college_medicine":0.5375722543,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4553191489,
        "MMLU_econometrics":0.3596491228,
        "MMLU_electrical_engineering":0.5586206897,
        "MMLU_elementary_mathematics":0.3571428571,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.5870967742,
        "MMLU_high_school_chemistry":0.3497536946,
        "MMLU_high_school_computer_science":0.54,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.6212121212,
        "MMLU_high_school_government_and_politics":0.7202072539,
        "MMLU_high_school_macroeconomics":0.4666666667,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.4705882353,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.7100917431,
        "MMLU_high_school_statistics":0.3796296296,
        "MMLU_high_school_us_history":0.6666666667,
        "MMLU_high_school_world_history":0.6751054852,
        "MMLU_human_aging":0.6098654709,
        "MMLU_human_sexuality":0.5877862595,
        "MMLU_international_law":0.6198347107,
        "MMLU_jurisprudence":0.6388888889,
        "MMLU_logical_fallacies":0.6073619632,
        "MMLU_machine_learning":0.4821428571,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.811965812,
        "MMLU_medical_genetics":0.62,
        "MMLU_miscellaneous":0.711366539,
        "MMLU_moral_disputes":0.5404624277,
        "MMLU_moral_scenarios":0.2737430168,
        "MMLU_nutrition":0.5620915033,
        "MMLU_philosophy":0.5980707395,
        "MMLU_prehistory":0.6141975309,
        "MMLU_professional_accounting":0.3617021277,
        "MMLU_professional_law":0.3872229465,
        "MMLU_professional_medicine":0.4852941176,
        "MMLU_professional_psychology":0.5098039216,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.6244897959,
        "MMLU_sociology":0.7014925373,
        "MMLU_us_foreign_policy":0.72,
        "MMLU_virology":0.4337349398,
        "MMLU_world_religions":0.701754386
    },
    {
        "Model":"ANIMA-Phi-Neptune-Mistral-LoRa",
        "URL":"https:\/\/huggingface.co\/Severian\/ANIMA-Phi-Neptune-Mistral-LoRa",
        "full_model_name":"Severian\/ANIMA-Phi-Neptune-Mistral-LoRa",
        "Parameters":null,
        "MMLU_average":0.5212501814,
        "arc:challenge|25":0.5,
        "hellaswag|10":0.5656243776,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4074074074,
        "MMLU_astronomy":0.4868421053,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.5849056604,
        "MMLU_college_biology":0.5277777778,
        "MMLU_college_chemistry":0.33,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.23,
        "MMLU_college_medicine":0.5375722543,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4553191489,
        "MMLU_econometrics":0.3596491228,
        "MMLU_electrical_engineering":0.5586206897,
        "MMLU_elementary_mathematics":0.3571428571,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.5838709677,
        "MMLU_high_school_chemistry":0.3448275862,
        "MMLU_high_school_computer_science":0.53,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.6212121212,
        "MMLU_high_school_government_and_politics":0.7202072539,
        "MMLU_high_school_macroeconomics":0.4641025641,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.4705882353,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.7100917431,
        "MMLU_high_school_statistics":0.3796296296,
        "MMLU_high_school_us_history":0.6666666667,
        "MMLU_high_school_world_history":0.6751054852,
        "MMLU_human_aging":0.6053811659,
        "MMLU_human_sexuality":0.5877862595,
        "MMLU_international_law":0.6198347107,
        "MMLU_jurisprudence":0.6388888889,
        "MMLU_logical_fallacies":0.6134969325,
        "MMLU_machine_learning":0.4821428571,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.811965812,
        "MMLU_medical_genetics":0.61,
        "MMLU_miscellaneous":0.7075351213,
        "MMLU_moral_disputes":0.5404624277,
        "MMLU_moral_scenarios":0.2759776536,
        "MMLU_nutrition":0.5620915033,
        "MMLU_philosophy":0.5980707395,
        "MMLU_prehistory":0.6172839506,
        "MMLU_professional_accounting":0.3617021277,
        "MMLU_professional_law":0.3885267275,
        "MMLU_professional_medicine":0.4779411765,
        "MMLU_professional_psychology":0.5114379085,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.6204081633,
        "MMLU_sociology":0.7014925373,
        "MMLU_us_foreign_policy":0.72,
        "MMLU_virology":0.4337349398,
        "MMLU_world_religions":0.701754386
    },
    {
        "Model":"Dans-MysteryModel-13b",
        "URL":"https:\/\/huggingface.co\/PocketDoc\/Dans-MysteryModel-13b",
        "full_model_name":"PocketDoc\/Dans-MysteryModel-13b",
        "Parameters":13.0,
        "MMLU_average":0.5205715466,
        "arc:challenge|25":0.5255972696,
        "hellaswag|10":0.5996813384,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.5,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.5924528302,
        "MMLU_college_biology":0.5138888889,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4855491329,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.62,
        "MMLU_conceptual_physics":0.4255319149,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.4551724138,
        "MMLU_elementary_mathematics":0.3121693122,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.6193548387,
        "MMLU_high_school_chemistry":0.4532019704,
        "MMLU_high_school_computer_science":0.54,
        "MMLU_high_school_european_history":0.6181818182,
        "MMLU_high_school_geography":0.6717171717,
        "MMLU_high_school_government_and_politics":0.7409326425,
        "MMLU_high_school_macroeconomics":0.4666666667,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.5,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.6678899083,
        "MMLU_high_school_statistics":0.3981481481,
        "MMLU_high_school_us_history":0.7205882353,
        "MMLU_high_school_world_history":0.6877637131,
        "MMLU_human_aging":0.6367713004,
        "MMLU_human_sexuality":0.641221374,
        "MMLU_international_law":0.6859504132,
        "MMLU_jurisprudence":0.6759259259,
        "MMLU_logical_fallacies":0.6012269939,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.6893203883,
        "MMLU_marketing":0.7435897436,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.6909323116,
        "MMLU_moral_disputes":0.6069364162,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.591503268,
        "MMLU_philosophy":0.5723472669,
        "MMLU_prehistory":0.5802469136,
        "MMLU_professional_accounting":0.4007092199,
        "MMLU_professional_law":0.4002607562,
        "MMLU_professional_medicine":0.4816176471,
        "MMLU_professional_psychology":0.5016339869,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.5918367347,
        "MMLU_sociology":0.7164179104,
        "MMLU_us_foreign_policy":0.78,
        "MMLU_virology":0.4036144578,
        "MMLU_world_religions":0.7134502924
    },
    {
        "Model":"Vicuna-13B-CoT",
        "URL":"https:\/\/huggingface.co\/kevinpro\/Vicuna-13B-CoT",
        "full_model_name":"kevinpro\/Vicuna-13B-CoT",
        "Parameters":13.0,
        "MMLU_average":0.5193614752,
        "arc:challenge|25":0.5196245734,
        "hellaswag|10":0.6007767377,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.5065789474,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.4981132075,
        "MMLU_college_biology":0.5972222222,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.4161849711,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.64,
        "MMLU_conceptual_physics":0.3957446809,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.3386243386,
        "MMLU_formal_logic":0.4047619048,
        "MMLU_global_facts":0.26,
        "MMLU_high_school_biology":0.5677419355,
        "MMLU_high_school_chemistry":0.4039408867,
        "MMLU_high_school_computer_science":0.54,
        "MMLU_high_school_european_history":0.6666666667,
        "MMLU_high_school_geography":0.6565656566,
        "MMLU_high_school_government_and_politics":0.6943005181,
        "MMLU_high_school_macroeconomics":0.4717948718,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.4495798319,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.6862385321,
        "MMLU_high_school_statistics":0.3703703704,
        "MMLU_high_school_us_history":0.6862745098,
        "MMLU_high_school_world_history":0.7088607595,
        "MMLU_human_aging":0.5874439462,
        "MMLU_human_sexuality":0.6717557252,
        "MMLU_international_law":0.6859504132,
        "MMLU_jurisprudence":0.6203703704,
        "MMLU_logical_fallacies":0.6380368098,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.6990291262,
        "MMLU_marketing":0.7521367521,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.6922094508,
        "MMLU_moral_disputes":0.5462427746,
        "MMLU_moral_scenarios":0.330726257,
        "MMLU_nutrition":0.5555555556,
        "MMLU_philosophy":0.5241157556,
        "MMLU_prehistory":0.5462962963,
        "MMLU_professional_accounting":0.3936170213,
        "MMLU_professional_law":0.4165580183,
        "MMLU_professional_medicine":0.4889705882,
        "MMLU_professional_psychology":0.5212418301,
        "MMLU_public_relations":0.5545454545,
        "MMLU_security_studies":0.6244897959,
        "MMLU_sociology":0.7711442786,
        "MMLU_us_foreign_policy":0.76,
        "MMLU_virology":0.4397590361,
        "MMLU_world_religions":0.7134502924
    },
    {
        "Model":"zarafusionex-1.2-l2-7b",
        "URL":"https:\/\/huggingface.co\/zarakiquemparte\/zarafusionex-1.2-l2-7b",
        "full_model_name":"zarakiquemparte\/zarafusionex-1.2-l2-7b",
        "Parameters":7.0,
        "MMLU_average":0.5193510987,
        "arc:challenge|25":0.5349829352,
        "hellaswag|10":0.597789285,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.5333333333,
        "MMLU_astronomy":0.4473684211,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.5811320755,
        "MMLU_college_biology":0.5625,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.4450867052,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.59,
        "MMLU_conceptual_physics":0.4680851064,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.4896551724,
        "MMLU_elementary_mathematics":0.3015873016,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.5741935484,
        "MMLU_high_school_chemistry":0.3743842365,
        "MMLU_high_school_computer_science":0.45,
        "MMLU_high_school_european_history":0.6484848485,
        "MMLU_high_school_geography":0.6666666667,
        "MMLU_high_school_government_and_politics":0.7616580311,
        "MMLU_high_school_macroeconomics":0.5025641026,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.4915966387,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.7119266055,
        "MMLU_high_school_statistics":0.4398148148,
        "MMLU_high_school_us_history":0.6960784314,
        "MMLU_high_school_world_history":0.7172995781,
        "MMLU_human_aging":0.6278026906,
        "MMLU_human_sexuality":0.6183206107,
        "MMLU_international_law":0.6611570248,
        "MMLU_jurisprudence":0.5740740741,
        "MMLU_logical_fallacies":0.5644171779,
        "MMLU_machine_learning":0.4017857143,
        "MMLU_management":0.7087378641,
        "MMLU_marketing":0.764957265,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.7100893997,
        "MMLU_moral_disputes":0.5549132948,
        "MMLU_moral_scenarios":0.2737430168,
        "MMLU_nutrition":0.5555555556,
        "MMLU_philosophy":0.5980707395,
        "MMLU_prehistory":0.5462962963,
        "MMLU_professional_accounting":0.3794326241,
        "MMLU_professional_law":0.3767926988,
        "MMLU_professional_medicine":0.5367647059,
        "MMLU_professional_psychology":0.5163398693,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.6081632653,
        "MMLU_sociology":0.6865671642,
        "MMLU_us_foreign_policy":0.76,
        "MMLU_virology":0.3975903614,
        "MMLU_world_religions":0.7368421053
    },
    {
        "Model":"vicuna-13b-delta-v1.1",
        "URL":"https:\/\/huggingface.co\/lmsys\/vicuna-13b-delta-v1.1",
        "full_model_name":"lmsys\/vicuna-13b-delta-v1.1",
        "Parameters":13.0,
        "MMLU_average":0.5190047112,
        "arc:challenge|25":0.5196245734,
        "hellaswag|10":0.6011750647,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.5065789474,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.4981132075,
        "MMLU_college_biology":0.5972222222,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.4161849711,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.64,
        "MMLU_conceptual_physics":0.3914893617,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.3386243386,
        "MMLU_formal_logic":0.4047619048,
        "MMLU_global_facts":0.26,
        "MMLU_high_school_biology":0.5677419355,
        "MMLU_high_school_chemistry":0.4039408867,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.6666666667,
        "MMLU_high_school_geography":0.6515151515,
        "MMLU_high_school_government_and_politics":0.6943005181,
        "MMLU_high_school_macroeconomics":0.4717948718,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.4453781513,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.6844036697,
        "MMLU_high_school_statistics":0.3657407407,
        "MMLU_high_school_us_history":0.6862745098,
        "MMLU_high_school_world_history":0.7130801688,
        "MMLU_human_aging":0.5829596413,
        "MMLU_human_sexuality":0.6717557252,
        "MMLU_international_law":0.6859504132,
        "MMLU_jurisprudence":0.6203703704,
        "MMLU_logical_fallacies":0.6441717791,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.6990291262,
        "MMLU_marketing":0.7521367521,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.6922094508,
        "MMLU_moral_disputes":0.5462427746,
        "MMLU_moral_scenarios":0.3318435754,
        "MMLU_nutrition":0.5522875817,
        "MMLU_philosophy":0.5241157556,
        "MMLU_prehistory":0.549382716,
        "MMLU_professional_accounting":0.390070922,
        "MMLU_professional_law":0.4185136897,
        "MMLU_professional_medicine":0.4926470588,
        "MMLU_professional_psychology":0.5179738562,
        "MMLU_public_relations":0.5454545455,
        "MMLU_security_studies":0.6244897959,
        "MMLU_sociology":0.7711442786,
        "MMLU_us_foreign_policy":0.76,
        "MMLU_virology":0.4397590361,
        "MMLU_world_religions":0.7134502924
    },
    {
        "Model":"Vicuna-13B-CoT-fp16",
        "URL":"https:\/\/huggingface.co\/TheBloke\/Vicuna-13B-CoT-fp16",
        "full_model_name":"TheBloke\/Vicuna-13B-CoT-fp16",
        "Parameters":13.0,
        "MMLU_average":0.5190047112,
        "arc:challenge|25":0.5196245734,
        "hellaswag|10":0.6011750647,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.5065789474,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.4981132075,
        "MMLU_college_biology":0.5972222222,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.4161849711,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.64,
        "MMLU_conceptual_physics":0.3914893617,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.3386243386,
        "MMLU_formal_logic":0.4047619048,
        "MMLU_global_facts":0.26,
        "MMLU_high_school_biology":0.5677419355,
        "MMLU_high_school_chemistry":0.4039408867,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.6666666667,
        "MMLU_high_school_geography":0.6515151515,
        "MMLU_high_school_government_and_politics":0.6943005181,
        "MMLU_high_school_macroeconomics":0.4717948718,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.4453781513,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.6844036697,
        "MMLU_high_school_statistics":0.3657407407,
        "MMLU_high_school_us_history":0.6862745098,
        "MMLU_high_school_world_history":0.7130801688,
        "MMLU_human_aging":0.5829596413,
        "MMLU_human_sexuality":0.6717557252,
        "MMLU_international_law":0.6859504132,
        "MMLU_jurisprudence":0.6203703704,
        "MMLU_logical_fallacies":0.6441717791,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.6990291262,
        "MMLU_marketing":0.7521367521,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.6922094508,
        "MMLU_moral_disputes":0.5462427746,
        "MMLU_moral_scenarios":0.3318435754,
        "MMLU_nutrition":0.5522875817,
        "MMLU_philosophy":0.5241157556,
        "MMLU_prehistory":0.549382716,
        "MMLU_professional_accounting":0.390070922,
        "MMLU_professional_law":0.4185136897,
        "MMLU_professional_medicine":0.4926470588,
        "MMLU_professional_psychology":0.5179738562,
        "MMLU_public_relations":0.5454545455,
        "MMLU_security_studies":0.6244897959,
        "MMLU_sociology":0.7711442786,
        "MMLU_us_foreign_policy":0.76,
        "MMLU_virology":0.4397590361,
        "MMLU_world_religions":0.7134502924
    },
    {
        "Model":"llama-2-13b-4bit-alpaca-gpt4",
        "URL":"https:\/\/huggingface.co\/TFLai\/llama-2-13b-4bit-alpaca-gpt4",
        "full_model_name":"TFLai\/llama-2-13b-4bit-alpaca-gpt4",
        "Parameters":13.0,
        "MMLU_average":0.5181524616,
        "arc:challenge|25":0.5383959044,
        "hellaswag|10":0.6101374228,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.4934210526,
        "MMLU_business_ethics":0.49,
        "MMLU_clinical_knowledge":0.5924528302,
        "MMLU_college_biology":0.5555555556,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.4682080925,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.67,
        "MMLU_conceptual_physics":0.429787234,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.4827586207,
        "MMLU_elementary_mathematics":0.3174603175,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.6516129032,
        "MMLU_high_school_chemistry":0.4187192118,
        "MMLU_high_school_computer_science":0.52,
        "MMLU_high_school_european_history":0.6606060606,
        "MMLU_high_school_geography":0.6868686869,
        "MMLU_high_school_government_and_politics":0.7305699482,
        "MMLU_high_school_macroeconomics":0.4717948718,
        "MMLU_high_school_mathematics":0.2888888889,
        "MMLU_high_school_microeconomics":0.5210084034,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.695412844,
        "MMLU_high_school_statistics":0.4398148148,
        "MMLU_high_school_us_history":0.6519607843,
        "MMLU_high_school_world_history":0.6962025316,
        "MMLU_human_aging":0.600896861,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.6481481481,
        "MMLU_logical_fallacies":0.6564417178,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.6893203883,
        "MMLU_marketing":0.7820512821,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.6947637292,
        "MMLU_moral_disputes":0.6011560694,
        "MMLU_moral_scenarios":0.2525139665,
        "MMLU_nutrition":0.545751634,
        "MMLU_philosophy":0.6366559486,
        "MMLU_prehistory":0.5987654321,
        "MMLU_professional_accounting":0.3936170213,
        "MMLU_professional_law":0.3924380704,
        "MMLU_professional_medicine":0.4926470588,
        "MMLU_professional_psychology":0.5196078431,
        "MMLU_public_relations":0.6,
        "MMLU_security_studies":0.5836734694,
        "MMLU_sociology":0.7213930348,
        "MMLU_us_foreign_policy":0.77,
        "MMLU_virology":0.4036144578,
        "MMLU_world_religions":0.6959064327
    },
    {
        "Model":"Stable-Platypus2-mini-7B",
        "URL":"https:\/\/huggingface.co\/edor\/Stable-Platypus2-mini-7B",
        "full_model_name":"edor\/Stable-Platypus2-mini-7B",
        "Parameters":7.0,
        "MMLU_average":0.5177997648,
        "arc:challenge|25":0.523890785,
        "hellaswag|10":0.5965943039,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.4078947368,
        "MMLU_business_ethics":0.5,
        "MMLU_clinical_knowledge":0.5924528302,
        "MMLU_college_biology":0.5416666667,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.4682080925,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.4765957447,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.4896551724,
        "MMLU_elementary_mathematics":0.3015873016,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.564516129,
        "MMLU_high_school_chemistry":0.3448275862,
        "MMLU_high_school_computer_science":0.44,
        "MMLU_high_school_european_history":0.7212121212,
        "MMLU_high_school_geography":0.6414141414,
        "MMLU_high_school_government_and_politics":0.7772020725,
        "MMLU_high_school_macroeconomics":0.4948717949,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.5168067227,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.7321100917,
        "MMLU_high_school_statistics":0.4259259259,
        "MMLU_high_school_us_history":0.6960784314,
        "MMLU_high_school_world_history":0.7130801688,
        "MMLU_human_aging":0.6143497758,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.6859504132,
        "MMLU_jurisprudence":0.5833333333,
        "MMLU_logical_fallacies":0.5460122699,
        "MMLU_machine_learning":0.4107142857,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.7820512821,
        "MMLU_medical_genetics":0.64,
        "MMLU_miscellaneous":0.7164750958,
        "MMLU_moral_disputes":0.5780346821,
        "MMLU_moral_scenarios":0.2569832402,
        "MMLU_nutrition":0.5424836601,
        "MMLU_philosophy":0.5852090032,
        "MMLU_prehistory":0.537037037,
        "MMLU_professional_accounting":0.390070922,
        "MMLU_professional_law":0.3917861799,
        "MMLU_professional_medicine":0.5183823529,
        "MMLU_professional_psychology":0.5098039216,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.6571428571,
        "MMLU_sociology":0.631840796,
        "MMLU_us_foreign_policy":0.73,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.6959064327
    },
    {
        "Model":"testlm-3",
        "URL":"https:\/\/huggingface.co\/Kiddyz\/testlm-3",
        "full_model_name":"Kiddyz\/testlm-3",
        "Parameters":null,
        "MMLU_average":0.5176567192,
        "arc:challenge|25":0.502559727,
        "hellaswag|10":0.5963951404,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.4736842105,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.558490566,
        "MMLU_college_biology":0.5208333333,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.4566473988,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.68,
        "MMLU_conceptual_physics":0.4042553191,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.3174603175,
        "MMLU_formal_logic":0.3888888889,
        "MMLU_global_facts":0.28,
        "MMLU_high_school_biology":0.5967741935,
        "MMLU_high_school_chemistry":0.354679803,
        "MMLU_high_school_computer_science":0.4,
        "MMLU_high_school_european_history":0.6848484848,
        "MMLU_high_school_geography":0.6414141414,
        "MMLU_high_school_government_and_politics":0.7150259067,
        "MMLU_high_school_macroeconomics":0.4974358974,
        "MMLU_high_school_mathematics":0.2888888889,
        "MMLU_high_school_microeconomics":0.4663865546,
        "MMLU_high_school_physics":0.3841059603,
        "MMLU_high_school_psychology":0.7009174312,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.7058823529,
        "MMLU_high_school_world_history":0.7383966245,
        "MMLU_human_aging":0.600896861,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.6280991736,
        "MMLU_jurisprudence":0.6296296296,
        "MMLU_logical_fallacies":0.5828220859,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.6699029126,
        "MMLU_marketing":0.7521367521,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.6985951469,
        "MMLU_moral_disputes":0.5433526012,
        "MMLU_moral_scenarios":0.4189944134,
        "MMLU_nutrition":0.5849673203,
        "MMLU_philosophy":0.5916398714,
        "MMLU_prehistory":0.5586419753,
        "MMLU_professional_accounting":0.3546099291,
        "MMLU_professional_law":0.3839634941,
        "MMLU_professional_medicine":0.5661764706,
        "MMLU_professional_psychology":0.4493464052,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.6285714286,
        "MMLU_sociology":0.7014925373,
        "MMLU_us_foreign_policy":0.69,
        "MMLU_virology":0.4036144578,
        "MMLU_world_religions":0.7192982456
    },
    {
        "Model":"blossom-v2-llama2-7b",
        "URL":"https:\/\/huggingface.co\/Azure99\/blossom-v2-llama2-7b",
        "full_model_name":"Azure99\/blossom-v2-llama2-7b",
        "Parameters":7.0,
        "MMLU_average":0.5165677012,
        "arc:challenge|25":0.5085324232,
        "hellaswag|10":0.598187612,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.4518518519,
        "MMLU_astronomy":0.4736842105,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.5547169811,
        "MMLU_college_biology":0.5138888889,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.4739884393,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.4170212766,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.3042328042,
        "MMLU_formal_logic":0.373015873,
        "MMLU_global_facts":0.26,
        "MMLU_high_school_biology":0.5903225806,
        "MMLU_high_school_chemistry":0.3497536946,
        "MMLU_high_school_computer_science":0.4,
        "MMLU_high_school_european_history":0.6909090909,
        "MMLU_high_school_geography":0.6515151515,
        "MMLU_high_school_government_and_politics":0.7202072539,
        "MMLU_high_school_macroeconomics":0.4846153846,
        "MMLU_high_school_mathematics":0.2925925926,
        "MMLU_high_school_microeconomics":0.4537815126,
        "MMLU_high_school_physics":0.4105960265,
        "MMLU_high_school_psychology":0.6990825688,
        "MMLU_high_school_statistics":0.4444444444,
        "MMLU_high_school_us_history":0.7058823529,
        "MMLU_high_school_world_history":0.7383966245,
        "MMLU_human_aging":0.5874439462,
        "MMLU_human_sexuality":0.6183206107,
        "MMLU_international_law":0.6363636364,
        "MMLU_jurisprudence":0.6296296296,
        "MMLU_logical_fallacies":0.5889570552,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.7564102564,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.7024265645,
        "MMLU_moral_disputes":0.5520231214,
        "MMLU_moral_scenarios":0.4033519553,
        "MMLU_nutrition":0.5849673203,
        "MMLU_philosophy":0.5948553055,
        "MMLU_prehistory":0.5401234568,
        "MMLU_professional_accounting":0.3687943262,
        "MMLU_professional_law":0.3820078227,
        "MMLU_professional_medicine":0.5588235294,
        "MMLU_professional_psychology":0.4640522876,
        "MMLU_public_relations":0.6,
        "MMLU_security_studies":0.6326530612,
        "MMLU_sociology":0.7064676617,
        "MMLU_us_foreign_policy":0.69,
        "MMLU_virology":0.4036144578,
        "MMLU_world_religions":0.7251461988
    },
    {
        "Model":"test_qkvo_adptor",
        "URL":"https:\/\/huggingface.co\/xxyyy123\/test_qkvo_adptor",
        "full_model_name":"xxyyy123\/test_qkvo_adptor",
        "Parameters":null,
        "MMLU_average":0.5164379115,
        "arc:challenge|25":0.5349829352,
        "hellaswag|10":0.6007767377,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.4342105263,
        "MMLU_business_ethics":0.5,
        "MMLU_clinical_knowledge":0.5849056604,
        "MMLU_college_biology":0.5347222222,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4682080925,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.62,
        "MMLU_conceptual_physics":0.4382978723,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4551724138,
        "MMLU_elementary_mathematics":0.3227513228,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.564516129,
        "MMLU_high_school_chemistry":0.354679803,
        "MMLU_high_school_computer_science":0.46,
        "MMLU_high_school_european_history":0.7151515152,
        "MMLU_high_school_geography":0.6363636364,
        "MMLU_high_school_government_and_politics":0.7564766839,
        "MMLU_high_school_macroeconomics":0.5282051282,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.525210084,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.7211009174,
        "MMLU_high_school_statistics":0.462962963,
        "MMLU_high_school_us_history":0.7009803922,
        "MMLU_high_school_world_history":0.7088607595,
        "MMLU_human_aging":0.5919282511,
        "MMLU_human_sexuality":0.6259541985,
        "MMLU_international_law":0.694214876,
        "MMLU_jurisprudence":0.6481481481,
        "MMLU_logical_fallacies":0.5460122699,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.6893203883,
        "MMLU_marketing":0.764957265,
        "MMLU_medical_genetics":0.64,
        "MMLU_miscellaneous":0.7164750958,
        "MMLU_moral_disputes":0.5549132948,
        "MMLU_moral_scenarios":0.2391061453,
        "MMLU_nutrition":0.5359477124,
        "MMLU_philosophy":0.575562701,
        "MMLU_prehistory":0.5524691358,
        "MMLU_professional_accounting":0.3936170213,
        "MMLU_professional_law":0.3767926988,
        "MMLU_professional_medicine":0.4889705882,
        "MMLU_professional_psychology":0.5,
        "MMLU_public_relations":0.5818181818,
        "MMLU_security_studies":0.6367346939,
        "MMLU_sociology":0.5820895522,
        "MMLU_us_foreign_policy":0.7,
        "MMLU_virology":0.3855421687,
        "MMLU_world_religions":0.7134502924
    },
    {
        "Model":"L2-7b-Beluga-WVG-Test",
        "URL":"https:\/\/huggingface.co\/LTC-AI-Labs\/L2-7b-Beluga-WVG-Test",
        "full_model_name":"LTC-AI-Labs\/L2-7b-Beluga-WVG-Test",
        "Parameters":7.0,
        "MMLU_average":0.5156882212,
        "arc:challenge|25":0.5162116041,
        "hellaswag|10":0.5930093607,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.4473684211,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.6113207547,
        "MMLU_college_biology":0.5347222222,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4393063584,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.58,
        "MMLU_conceptual_physics":0.485106383,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.4482758621,
        "MMLU_elementary_mathematics":0.2962962963,
        "MMLU_formal_logic":0.3333333333,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.5612903226,
        "MMLU_high_school_chemistry":0.3793103448,
        "MMLU_high_school_computer_science":0.48,
        "MMLU_high_school_european_history":0.6787878788,
        "MMLU_high_school_geography":0.6666666667,
        "MMLU_high_school_government_and_politics":0.7461139896,
        "MMLU_high_school_macroeconomics":0.4743589744,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.5,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.7302752294,
        "MMLU_high_school_statistics":0.412037037,
        "MMLU_high_school_us_history":0.7009803922,
        "MMLU_high_school_world_history":0.7257383966,
        "MMLU_human_aging":0.6367713004,
        "MMLU_human_sexuality":0.6183206107,
        "MMLU_international_law":0.6776859504,
        "MMLU_jurisprudence":0.5833333333,
        "MMLU_logical_fallacies":0.5766871166,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.7393162393,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.7228607918,
        "MMLU_moral_disputes":0.5664739884,
        "MMLU_moral_scenarios":0.2547486034,
        "MMLU_nutrition":0.5620915033,
        "MMLU_philosophy":0.578778135,
        "MMLU_prehistory":0.5648148148,
        "MMLU_professional_accounting":0.3865248227,
        "MMLU_professional_law":0.3787483703,
        "MMLU_professional_medicine":0.5294117647,
        "MMLU_professional_psychology":0.5114379085,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.5959183673,
        "MMLU_sociology":0.5970149254,
        "MMLU_us_foreign_policy":0.76,
        "MMLU_virology":0.4096385542,
        "MMLU_world_religions":0.7076023392
    },
    {
        "Model":"testlm2",
        "URL":"https:\/\/huggingface.co\/Kiddyz\/testlm2",
        "full_model_name":"Kiddyz\/testlm2",
        "Parameters":null,
        "MMLU_average":0.5152568212,
        "arc:challenge|25":0.4982935154,
        "hellaswag|10":0.5723959371,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.5,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.558490566,
        "MMLU_college_biology":0.5347222222,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.450867052,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.429787234,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.291005291,
        "MMLU_formal_logic":0.3333333333,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.5741935484,
        "MMLU_high_school_chemistry":0.3842364532,
        "MMLU_high_school_computer_science":0.39,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.6515151515,
        "MMLU_high_school_government_and_politics":0.7564766839,
        "MMLU_high_school_macroeconomics":0.4794871795,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.487394958,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.6880733945,
        "MMLU_high_school_statistics":0.412037037,
        "MMLU_high_school_us_history":0.7401960784,
        "MMLU_high_school_world_history":0.7215189873,
        "MMLU_human_aging":0.6367713004,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.694214876,
        "MMLU_jurisprudence":0.5833333333,
        "MMLU_logical_fallacies":0.5828220859,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.764957265,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.7100893997,
        "MMLU_moral_disputes":0.5375722543,
        "MMLU_moral_scenarios":0.2927374302,
        "MMLU_nutrition":0.5555555556,
        "MMLU_philosophy":0.5980707395,
        "MMLU_prehistory":0.5648148148,
        "MMLU_professional_accounting":0.4078014184,
        "MMLU_professional_law":0.3898305085,
        "MMLU_professional_medicine":0.4595588235,
        "MMLU_professional_psychology":0.5016339869,
        "MMLU_public_relations":0.6272727273,
        "MMLU_security_studies":0.6285714286,
        "MMLU_sociology":0.6915422886,
        "MMLU_us_foreign_policy":0.72,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.7076023392
    },
    {
        "Model":"llama_13b_sharegpt94k_fastchat",
        "URL":"https:\/\/huggingface.co\/wahaha1987\/llama_13b_sharegpt94k_fastchat",
        "full_model_name":"wahaha1987\/llama_13b_sharegpt94k_fastchat",
        "Parameters":13.0,
        "MMLU_average":0.5150096994,
        "arc:challenge|25":0.5076791809,
        "hellaswag|10":0.5913164708,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5065789474,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.5283018868,
        "MMLU_college_biology":0.5416666667,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.4219653179,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.3829787234,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4551724138,
        "MMLU_elementary_mathematics":0.3015873016,
        "MMLU_formal_logic":0.4047619048,
        "MMLU_global_facts":0.4,
        "MMLU_high_school_biology":0.5935483871,
        "MMLU_high_school_chemistry":0.3793103448,
        "MMLU_high_school_computer_science":0.53,
        "MMLU_high_school_european_history":0.6666666667,
        "MMLU_high_school_geography":0.6464646465,
        "MMLU_high_school_government_and_politics":0.7150259067,
        "MMLU_high_school_macroeconomics":0.4769230769,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.4915966387,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.7027522936,
        "MMLU_high_school_statistics":0.3796296296,
        "MMLU_high_school_us_history":0.6617647059,
        "MMLU_high_school_world_history":0.7299578059,
        "MMLU_human_aging":0.5919282511,
        "MMLU_human_sexuality":0.6030534351,
        "MMLU_international_law":0.6280991736,
        "MMLU_jurisprudence":0.5925925926,
        "MMLU_logical_fallacies":0.6012269939,
        "MMLU_machine_learning":0.4285714286,
        "MMLU_management":0.6990291262,
        "MMLU_marketing":0.7735042735,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.7228607918,
        "MMLU_moral_disputes":0.549132948,
        "MMLU_moral_scenarios":0.2547486034,
        "MMLU_nutrition":0.5490196078,
        "MMLU_philosophy":0.5337620579,
        "MMLU_prehistory":0.5524691358,
        "MMLU_professional_accounting":0.3794326241,
        "MMLU_professional_law":0.3976531943,
        "MMLU_professional_medicine":0.5183823529,
        "MMLU_professional_psychology":0.5081699346,
        "MMLU_public_relations":0.5818181818,
        "MMLU_security_studies":0.6530612245,
        "MMLU_sociology":0.7114427861,
        "MMLU_us_foreign_policy":0.73,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.7251461988
    },
    {
        "Model":"prometheus-13b-v1.0",
        "URL":"https:\/\/huggingface.co\/kaist-ai\/prometheus-13b-v1.0",
        "full_model_name":"kaist-ai\/prometheus-13b-v1.0",
        "Parameters":13.0,
        "MMLU_average":0.514850931,
        "arc:challenge|25":0.5008532423,
        "hellaswag|10":0.6132244573,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.5394736842,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.5811320755,
        "MMLU_college_biology":0.6041666667,
        "MMLU_college_chemistry":0.36,
        "MMLU_college_computer_science":0.4,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.4855491329,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.3872340426,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.5172413793,
        "MMLU_elementary_mathematics":0.335978836,
        "MMLU_formal_logic":0.3015873016,
        "MMLU_global_facts":0.28,
        "MMLU_high_school_biology":0.5419354839,
        "MMLU_high_school_chemistry":0.4285714286,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.6060606061,
        "MMLU_high_school_geography":0.6616161616,
        "MMLU_high_school_government_and_politics":0.792746114,
        "MMLU_high_school_macroeconomics":0.4461538462,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.525210084,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.6862385321,
        "MMLU_high_school_statistics":0.3796296296,
        "MMLU_high_school_us_history":0.6617647059,
        "MMLU_high_school_world_history":0.7046413502,
        "MMLU_human_aging":0.5964125561,
        "MMLU_human_sexuality":0.572519084,
        "MMLU_international_law":0.7520661157,
        "MMLU_jurisprudence":0.6481481481,
        "MMLU_logical_fallacies":0.6134969325,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.7606837607,
        "MMLU_medical_genetics":0.5,
        "MMLU_miscellaneous":0.7088122605,
        "MMLU_moral_disputes":0.549132948,
        "MMLU_moral_scenarios":0.2748603352,
        "MMLU_nutrition":0.5588235294,
        "MMLU_philosophy":0.5884244373,
        "MMLU_prehistory":0.5771604938,
        "MMLU_professional_accounting":0.390070922,
        "MMLU_professional_law":0.3820078227,
        "MMLU_professional_medicine":0.4595588235,
        "MMLU_professional_psychology":0.5065359477,
        "MMLU_public_relations":0.6272727273,
        "MMLU_security_studies":0.6285714286,
        "MMLU_sociology":0.6268656716,
        "MMLU_us_foreign_policy":0.76,
        "MMLU_virology":0.4397590361,
        "MMLU_world_religions":0.7134502924
    },
    {
        "Model":"llama2_7b_merge_orcafamily",
        "URL":"https:\/\/huggingface.co\/yeen214\/llama2_7b_merge_orcafamily",
        "full_model_name":"yeen214\/llama2_7b_merge_orcafamily",
        "Parameters":7.0,
        "MMLU_average":0.5148506473,
        "arc:challenge|25":0.5298634812,
        "hellaswag|10":0.6237801235,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.4736842105,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.6075471698,
        "MMLU_college_biology":0.5555555556,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.4855491329,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.4255319149,
        "MMLU_econometrics":0.3859649123,
        "MMLU_electrical_engineering":0.4206896552,
        "MMLU_elementary_mathematics":0.3333333333,
        "MMLU_formal_logic":0.2698412698,
        "MMLU_global_facts":0.4,
        "MMLU_high_school_biology":0.6,
        "MMLU_high_school_chemistry":0.3349753695,
        "MMLU_high_school_computer_science":0.43,
        "MMLU_high_school_european_history":0.696969697,
        "MMLU_high_school_geography":0.6616161616,
        "MMLU_high_school_government_and_politics":0.7461139896,
        "MMLU_high_school_macroeconomics":0.4871794872,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.5042016807,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.728440367,
        "MMLU_high_school_statistics":0.4166666667,
        "MMLU_high_school_us_history":0.7450980392,
        "MMLU_high_school_world_history":0.7172995781,
        "MMLU_human_aging":0.5874439462,
        "MMLU_human_sexuality":0.5572519084,
        "MMLU_international_law":0.6033057851,
        "MMLU_jurisprudence":0.5740740741,
        "MMLU_logical_fallacies":0.5705521472,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.7378640777,
        "MMLU_marketing":0.7777777778,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.7164750958,
        "MMLU_moral_disputes":0.549132948,
        "MMLU_moral_scenarios":0.2458100559,
        "MMLU_nutrition":0.522875817,
        "MMLU_philosophy":0.6045016077,
        "MMLU_prehistory":0.5895061728,
        "MMLU_professional_accounting":0.390070922,
        "MMLU_professional_law":0.3780964798,
        "MMLU_professional_medicine":0.5477941176,
        "MMLU_professional_psychology":0.4934640523,
        "MMLU_public_relations":0.6,
        "MMLU_security_studies":0.5387755102,
        "MMLU_sociology":0.6666666667,
        "MMLU_us_foreign_policy":0.67,
        "MMLU_virology":0.3975903614,
        "MMLU_world_religions":0.7485380117
    },
    {
        "Model":"zarablendex-vq-l2-7b",
        "URL":"https:\/\/huggingface.co\/zarakiquemparte\/zarablendex-vq-l2-7b",
        "full_model_name":"zarakiquemparte\/zarablendex-vq-l2-7b",
        "Parameters":7.0,
        "MMLU_average":0.5144617248,
        "arc:challenge|25":0.542662116,
        "hellaswag|10":0.6019717188,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.5185185185,
        "MMLU_astronomy":0.4210526316,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.5660377358,
        "MMLU_college_biology":0.5486111111,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.4450867052,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.62,
        "MMLU_conceptual_physics":0.4723404255,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.3121693122,
        "MMLU_formal_logic":0.2777777778,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.5741935484,
        "MMLU_high_school_chemistry":0.3694581281,
        "MMLU_high_school_computer_science":0.45,
        "MMLU_high_school_european_history":0.6363636364,
        "MMLU_high_school_geography":0.6666666667,
        "MMLU_high_school_government_and_politics":0.7668393782,
        "MMLU_high_school_macroeconomics":0.4820512821,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.474789916,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.7137614679,
        "MMLU_high_school_statistics":0.3981481481,
        "MMLU_high_school_us_history":0.7058823529,
        "MMLU_high_school_world_history":0.7088607595,
        "MMLU_human_aging":0.6233183857,
        "MMLU_human_sexuality":0.6030534351,
        "MMLU_international_law":0.6446280992,
        "MMLU_jurisprudence":0.5833333333,
        "MMLU_logical_fallacies":0.5644171779,
        "MMLU_machine_learning":0.4196428571,
        "MMLU_management":0.6990291262,
        "MMLU_marketing":0.764957265,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.7075351213,
        "MMLU_moral_disputes":0.5549132948,
        "MMLU_moral_scenarios":0.2681564246,
        "MMLU_nutrition":0.5424836601,
        "MMLU_philosophy":0.6045016077,
        "MMLU_prehistory":0.549382716,
        "MMLU_professional_accounting":0.3829787234,
        "MMLU_professional_law":0.371577575,
        "MMLU_professional_medicine":0.5404411765,
        "MMLU_professional_psychology":0.5081699346,
        "MMLU_public_relations":0.5818181818,
        "MMLU_security_studies":0.587755102,
        "MMLU_sociology":0.6965174129,
        "MMLU_us_foreign_policy":0.71,
        "MMLU_virology":0.4036144578,
        "MMLU_world_religions":0.7368421053
    },
    {
        "Model":"13B-Ouroboros",
        "URL":"https:\/\/huggingface.co\/CalderaAI\/13B-Ouroboros",
        "full_model_name":"CalderaAI\/13B-Ouroboros",
        "Parameters":13.0,
        "MMLU_average":0.5143109726,
        "arc:challenge|25":0.5605802048,
        "hellaswag|10":0.624377614,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.5065789474,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.5396226415,
        "MMLU_college_biology":0.5347222222,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.4,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.450867052,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.65,
        "MMLU_conceptual_physics":0.4340425532,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.4551724138,
        "MMLU_elementary_mathematics":0.3227513228,
        "MMLU_formal_logic":0.3015873016,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.5806451613,
        "MMLU_high_school_chemistry":0.3842364532,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.6545454545,
        "MMLU_high_school_geography":0.6717171717,
        "MMLU_high_school_government_and_politics":0.7046632124,
        "MMLU_high_school_macroeconomics":0.458974359,
        "MMLU_high_school_mathematics":0.2111111111,
        "MMLU_high_school_microeconomics":0.4915966387,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.704587156,
        "MMLU_high_school_statistics":0.3333333333,
        "MMLU_high_school_us_history":0.6862745098,
        "MMLU_high_school_world_history":0.7088607595,
        "MMLU_human_aging":0.5919282511,
        "MMLU_human_sexuality":0.6335877863,
        "MMLU_international_law":0.694214876,
        "MMLU_jurisprudence":0.5925925926,
        "MMLU_logical_fallacies":0.6073619632,
        "MMLU_machine_learning":0.4464285714,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.7735042735,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.717752235,
        "MMLU_moral_disputes":0.549132948,
        "MMLU_moral_scenarios":0.2603351955,
        "MMLU_nutrition":0.5424836601,
        "MMLU_philosophy":0.536977492,
        "MMLU_prehistory":0.5586419753,
        "MMLU_professional_accounting":0.365248227,
        "MMLU_professional_law":0.4054758801,
        "MMLU_professional_medicine":0.4816176471,
        "MMLU_professional_psychology":0.5245098039,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.5387755102,
        "MMLU_sociology":0.6915422886,
        "MMLU_us_foreign_policy":0.83,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.7602339181
    },
    {
        "Model":"openbuddy-llama2-13b-v11-bf16",
        "URL":"https:\/\/huggingface.co\/OpenBuddy\/openbuddy-llama2-13b-v11-bf16",
        "full_model_name":"OpenBuddy\/openbuddy-llama2-13b-v11-bf16",
        "Parameters":13.0,
        "MMLU_average":0.5136469958,
        "arc:challenge|25":0.4889078498,
        "hellaswag|10":0.5580561641,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.4518518519,
        "MMLU_astronomy":0.4605263158,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.5320754717,
        "MMLU_college_biology":0.5208333333,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.3988439306,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.57,
        "MMLU_conceptual_physics":0.4255319149,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.3227513228,
        "MMLU_formal_logic":0.3412698413,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.5774193548,
        "MMLU_high_school_chemistry":0.39408867,
        "MMLU_high_school_computer_science":0.49,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.7121212121,
        "MMLU_high_school_government_and_politics":0.7512953368,
        "MMLU_high_school_macroeconomics":0.441025641,
        "MMLU_high_school_mathematics":0.2925925926,
        "MMLU_high_school_microeconomics":0.5378151261,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.6825688073,
        "MMLU_high_school_statistics":0.4166666667,
        "MMLU_high_school_us_history":0.681372549,
        "MMLU_high_school_world_history":0.6708860759,
        "MMLU_human_aging":0.6322869955,
        "MMLU_human_sexuality":0.534351145,
        "MMLU_international_law":0.6859504132,
        "MMLU_jurisprudence":0.6944444444,
        "MMLU_logical_fallacies":0.6196319018,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.6601941748,
        "MMLU_marketing":0.7820512821,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.7011494253,
        "MMLU_moral_disputes":0.5346820809,
        "MMLU_moral_scenarios":0.243575419,
        "MMLU_nutrition":0.5588235294,
        "MMLU_philosophy":0.578778135,
        "MMLU_prehistory":0.5154320988,
        "MMLU_professional_accounting":0.3723404255,
        "MMLU_professional_law":0.3976531943,
        "MMLU_professional_medicine":0.4779411765,
        "MMLU_professional_psychology":0.4869281046,
        "MMLU_public_relations":0.6363636364,
        "MMLU_security_studies":0.6163265306,
        "MMLU_sociology":0.7014925373,
        "MMLU_us_foreign_policy":0.75,
        "MMLU_virology":0.4819277108,
        "MMLU_world_religions":0.6783625731
    },
    {
        "Model":"Dorflan",
        "URL":"https:\/\/huggingface.co\/formulae\/Dorflan",
        "full_model_name":"formulae\/Dorflan",
        "Parameters":null,
        "MMLU_average":0.5136132134,
        "arc:challenge|25":0.5145051195,
        "hellaswag|10":0.5850428202,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.4444444444,
        "MMLU_astronomy":0.4342105263,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.6188679245,
        "MMLU_college_biology":0.5416666667,
        "MMLU_college_chemistry":0.45,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4913294798,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.62,
        "MMLU_conceptual_physics":0.4510638298,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.4137931034,
        "MMLU_elementary_mathematics":0.3227513228,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.5709677419,
        "MMLU_high_school_chemistry":0.3793103448,
        "MMLU_high_school_computer_science":0.39,
        "MMLU_high_school_european_history":0.6848484848,
        "MMLU_high_school_geography":0.696969697,
        "MMLU_high_school_government_and_politics":0.7150259067,
        "MMLU_high_school_macroeconomics":0.5128205128,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.4957983193,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.7266055046,
        "MMLU_high_school_statistics":0.462962963,
        "MMLU_high_school_us_history":0.6911764706,
        "MMLU_high_school_world_history":0.7172995781,
        "MMLU_human_aging":0.5695067265,
        "MMLU_human_sexuality":0.5801526718,
        "MMLU_international_law":0.6776859504,
        "MMLU_jurisprudence":0.6388888889,
        "MMLU_logical_fallacies":0.5828220859,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.7307692308,
        "MMLU_medical_genetics":0.65,
        "MMLU_miscellaneous":0.7075351213,
        "MMLU_moral_disputes":0.5375722543,
        "MMLU_moral_scenarios":0.2681564246,
        "MMLU_nutrition":0.5522875817,
        "MMLU_philosophy":0.5691318328,
        "MMLU_prehistory":0.5462962963,
        "MMLU_professional_accounting":0.4113475177,
        "MMLU_professional_law":0.390482399,
        "MMLU_professional_medicine":0.5220588235,
        "MMLU_professional_psychology":0.4836601307,
        "MMLU_public_relations":0.5363636364,
        "MMLU_security_studies":0.6163265306,
        "MMLU_sociology":0.6368159204,
        "MMLU_us_foreign_policy":0.72,
        "MMLU_virology":0.4096385542,
        "MMLU_world_religions":0.6783625731
    },
    {
        "Model":"llama2-7b-hf-chat-lora-v2",
        "URL":"https:\/\/huggingface.co\/lvkaokao\/llama2-7b-hf-chat-lora-v2",
        "full_model_name":"lvkaokao\/llama2-7b-hf-chat-lora-v2",
        "Parameters":7.0,
        "MMLU_average":0.5134537641,
        "arc:challenge|25":0.5145051195,
        "hellaswag|10":0.5860386377,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.5,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.5433962264,
        "MMLU_college_biology":0.5277777778,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.4855491329,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.65,
        "MMLU_conceptual_physics":0.4638297872,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.4896551724,
        "MMLU_elementary_mathematics":0.2962962963,
        "MMLU_formal_logic":0.4047619048,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.5903225806,
        "MMLU_high_school_chemistry":0.3448275862,
        "MMLU_high_school_computer_science":0.43,
        "MMLU_high_school_european_history":0.6484848485,
        "MMLU_high_school_geography":0.6414141414,
        "MMLU_high_school_government_and_politics":0.7409326425,
        "MMLU_high_school_macroeconomics":0.4717948718,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.5042016807,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.7119266055,
        "MMLU_high_school_statistics":0.4074074074,
        "MMLU_high_school_us_history":0.7156862745,
        "MMLU_high_school_world_history":0.7088607595,
        "MMLU_human_aging":0.5695067265,
        "MMLU_human_sexuality":0.5954198473,
        "MMLU_international_law":0.6611570248,
        "MMLU_jurisprudence":0.6018518519,
        "MMLU_logical_fallacies":0.5766871166,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.7564102564,
        "MMLU_medical_genetics":0.61,
        "MMLU_miscellaneous":0.6947637292,
        "MMLU_moral_disputes":0.5404624277,
        "MMLU_moral_scenarios":0.3944134078,
        "MMLU_nutrition":0.545751634,
        "MMLU_philosophy":0.5916398714,
        "MMLU_prehistory":0.5555555556,
        "MMLU_professional_accounting":0.3439716312,
        "MMLU_professional_law":0.3774445893,
        "MMLU_professional_medicine":0.5147058824,
        "MMLU_professional_psychology":0.4820261438,
        "MMLU_public_relations":0.5818181818,
        "MMLU_security_studies":0.5551020408,
        "MMLU_sociology":0.7114427861,
        "MMLU_us_foreign_policy":0.7,
        "MMLU_virology":0.4096385542,
        "MMLU_world_religions":0.7602339181
    },
    {
        "Model":"llama2-7b-instructmining-60k-sharegpt",
        "URL":"https:\/\/huggingface.co\/yihan6324\/llama2-7b-instructmining-60k-sharegpt",
        "full_model_name":"yihan6324\/llama2-7b-instructmining-60k-sharegpt",
        "Parameters":7.0,
        "MMLU_average":0.5125729651,
        "arc:challenge|25":0.5085324232,
        "hellaswag|10":0.5971917945,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.5037037037,
        "MMLU_astronomy":0.4539473684,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.5698113208,
        "MMLU_college_biology":0.5277777778,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.37,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.4161849711,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.62,
        "MMLU_conceptual_physics":0.4595744681,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.4482758621,
        "MMLU_elementary_mathematics":0.2962962963,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.5580645161,
        "MMLU_high_school_chemistry":0.3842364532,
        "MMLU_high_school_computer_science":0.47,
        "MMLU_high_school_european_history":0.696969697,
        "MMLU_high_school_geography":0.6262626263,
        "MMLU_high_school_government_and_politics":0.7668393782,
        "MMLU_high_school_macroeconomics":0.4717948718,
        "MMLU_high_school_mathematics":0.2888888889,
        "MMLU_high_school_microeconomics":0.5,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.7119266055,
        "MMLU_high_school_statistics":0.3842592593,
        "MMLU_high_school_us_history":0.6960784314,
        "MMLU_high_school_world_history":0.7215189873,
        "MMLU_human_aging":0.6143497758,
        "MMLU_human_sexuality":0.6259541985,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.6203703704,
        "MMLU_logical_fallacies":0.5582822086,
        "MMLU_machine_learning":0.4285714286,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.7692307692,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.687100894,
        "MMLU_moral_disputes":0.5809248555,
        "MMLU_moral_scenarios":0.2983240223,
        "MMLU_nutrition":0.5522875817,
        "MMLU_philosophy":0.6270096463,
        "MMLU_prehistory":0.5679012346,
        "MMLU_professional_accounting":0.3936170213,
        "MMLU_professional_law":0.3976531943,
        "MMLU_professional_medicine":0.5183823529,
        "MMLU_professional_psychology":0.5049019608,
        "MMLU_public_relations":0.5727272727,
        "MMLU_security_studies":0.5632653061,
        "MMLU_sociology":0.7064676617,
        "MMLU_us_foreign_policy":0.64,
        "MMLU_virology":0.4337349398,
        "MMLU_world_religions":0.7076023392
    },
    {
        "Model":"llama-2-7b-instructmining-60k-sharegpt",
        "URL":"https:\/\/huggingface.co\/yihan6324\/llama-2-7b-instructmining-60k-sharegpt",
        "full_model_name":"yihan6324\/llama-2-7b-instructmining-60k-sharegpt",
        "Parameters":7.0,
        "MMLU_average":0.5125729651,
        "arc:challenge|25":0.5085324232,
        "hellaswag|10":0.5971917945,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.5037037037,
        "MMLU_astronomy":0.4539473684,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.5698113208,
        "MMLU_college_biology":0.5277777778,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.37,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.4161849711,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.62,
        "MMLU_conceptual_physics":0.4595744681,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.4482758621,
        "MMLU_elementary_mathematics":0.2962962963,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.5580645161,
        "MMLU_high_school_chemistry":0.3842364532,
        "MMLU_high_school_computer_science":0.47,
        "MMLU_high_school_european_history":0.696969697,
        "MMLU_high_school_geography":0.6262626263,
        "MMLU_high_school_government_and_politics":0.7668393782,
        "MMLU_high_school_macroeconomics":0.4717948718,
        "MMLU_high_school_mathematics":0.2888888889,
        "MMLU_high_school_microeconomics":0.5,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.7119266055,
        "MMLU_high_school_statistics":0.3842592593,
        "MMLU_high_school_us_history":0.6960784314,
        "MMLU_high_school_world_history":0.7215189873,
        "MMLU_human_aging":0.6143497758,
        "MMLU_human_sexuality":0.6259541985,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.6203703704,
        "MMLU_logical_fallacies":0.5582822086,
        "MMLU_machine_learning":0.4285714286,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.7692307692,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.687100894,
        "MMLU_moral_disputes":0.5809248555,
        "MMLU_moral_scenarios":0.2983240223,
        "MMLU_nutrition":0.5522875817,
        "MMLU_philosophy":0.6270096463,
        "MMLU_prehistory":0.5679012346,
        "MMLU_professional_accounting":0.3936170213,
        "MMLU_professional_law":0.3976531943,
        "MMLU_professional_medicine":0.5183823529,
        "MMLU_professional_psychology":0.5049019608,
        "MMLU_public_relations":0.5727272727,
        "MMLU_security_studies":0.5632653061,
        "MMLU_sociology":0.7064676617,
        "MMLU_us_foreign_policy":0.64,
        "MMLU_virology":0.4337349398,
        "MMLU_world_religions":0.7076023392
    },
    {
        "Model":"firefly-llama2-13b-pretrain",
        "URL":"https:\/\/huggingface.co\/YeungNLP\/firefly-llama2-13b-pretrain",
        "full_model_name":"YeungNLP\/firefly-llama2-13b-pretrain",
        "Parameters":13.0,
        "MMLU_average":0.5124853052,
        "arc:challenge|25":0.4982935154,
        "hellaswag|10":0.5895239992,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.5131578947,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.5509433962,
        "MMLU_college_biology":0.4652777778,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4682080925,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.64,
        "MMLU_conceptual_physics":0.4170212766,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.4275862069,
        "MMLU_elementary_mathematics":0.3306878307,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.6225806452,
        "MMLU_high_school_chemistry":0.39408867,
        "MMLU_high_school_computer_science":0.49,
        "MMLU_high_school_european_history":0.6363636364,
        "MMLU_high_school_geography":0.6363636364,
        "MMLU_high_school_government_and_politics":0.7357512953,
        "MMLU_high_school_macroeconomics":0.458974359,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.4705882353,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.6788990826,
        "MMLU_high_school_statistics":0.3564814815,
        "MMLU_high_school_us_history":0.6568627451,
        "MMLU_high_school_world_history":0.6751054852,
        "MMLU_human_aging":0.6233183857,
        "MMLU_human_sexuality":0.5801526718,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.712962963,
        "MMLU_logical_fallacies":0.6380368098,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.7777777778,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.7139208174,
        "MMLU_moral_disputes":0.6011560694,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.5784313725,
        "MMLU_philosophy":0.6012861736,
        "MMLU_prehistory":0.5462962963,
        "MMLU_professional_accounting":0.3723404255,
        "MMLU_professional_law":0.3487614081,
        "MMLU_professional_medicine":0.4411764706,
        "MMLU_professional_psychology":0.5081699346,
        "MMLU_public_relations":0.6818181818,
        "MMLU_security_studies":0.5591836735,
        "MMLU_sociology":0.6915422886,
        "MMLU_us_foreign_policy":0.77,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.7543859649
    },
    {
        "Model":"gpt4-x-vicuna-13B-HF",
        "URL":"https:\/\/huggingface.co\/TheBloke\/gpt4-x-vicuna-13B-HF",
        "full_model_name":"TheBloke\/gpt4-x-vicuna-13B-HF",
        "Parameters":13.0,
        "MMLU_average":0.5122257428,
        "arc:challenge|25":0.5110921502,
        "hellaswag|10":0.6038637722,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.5065789474,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.4867924528,
        "MMLU_college_biology":0.5486111111,
        "MMLU_college_chemistry":0.32,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.4046242775,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.57,
        "MMLU_conceptual_physics":0.3617021277,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.2883597884,
        "MMLU_formal_logic":0.4206349206,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.5741935484,
        "MMLU_high_school_chemistry":0.3891625616,
        "MMLU_high_school_computer_science":0.53,
        "MMLU_high_school_european_history":0.6424242424,
        "MMLU_high_school_geography":0.6363636364,
        "MMLU_high_school_government_and_politics":0.7046632124,
        "MMLU_high_school_macroeconomics":0.4384615385,
        "MMLU_high_school_mathematics":0.237037037,
        "MMLU_high_school_microeconomics":0.4411764706,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.6770642202,
        "MMLU_high_school_statistics":0.3657407407,
        "MMLU_high_school_us_history":0.6715686275,
        "MMLU_high_school_world_history":0.7046413502,
        "MMLU_human_aging":0.5739910314,
        "MMLU_human_sexuality":0.6717557252,
        "MMLU_international_law":0.6776859504,
        "MMLU_jurisprudence":0.6574074074,
        "MMLU_logical_fallacies":0.6625766871,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.7735042735,
        "MMLU_medical_genetics":0.62,
        "MMLU_miscellaneous":0.687100894,
        "MMLU_moral_disputes":0.5375722543,
        "MMLU_moral_scenarios":0.3173184358,
        "MMLU_nutrition":0.5424836601,
        "MMLU_philosophy":0.5530546624,
        "MMLU_prehistory":0.5740740741,
        "MMLU_professional_accounting":0.3865248227,
        "MMLU_professional_law":0.4126466754,
        "MMLU_professional_medicine":0.4448529412,
        "MMLU_professional_psychology":0.5196078431,
        "MMLU_public_relations":0.5454545455,
        "MMLU_security_studies":0.5795918367,
        "MMLU_sociology":0.7412935323,
        "MMLU_us_foreign_policy":0.79,
        "MMLU_virology":0.4518072289,
        "MMLU_world_religions":0.7426900585
    },
    {
        "Model":"testlm-1",
        "URL":"https:\/\/huggingface.co\/Kiddyz\/testlm-1",
        "full_model_name":"Kiddyz\/testlm-1",
        "Parameters":null,
        "MMLU_average":0.5120686323,
        "arc:challenge|25":0.5017064846,
        "hellaswag|10":0.5705038837,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.5131578947,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.5433962264,
        "MMLU_college_biology":0.5555555556,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.4624277457,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.67,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.4965517241,
        "MMLU_elementary_mathematics":0.3333333333,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.5903225806,
        "MMLU_high_school_chemistry":0.3596059113,
        "MMLU_high_school_computer_science":0.48,
        "MMLU_high_school_european_history":0.6484848485,
        "MMLU_high_school_geography":0.6262626263,
        "MMLU_high_school_government_and_politics":0.7202072539,
        "MMLU_high_school_macroeconomics":0.4974358974,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.5210084034,
        "MMLU_high_school_physics":0.357615894,
        "MMLU_high_school_psychology":0.7119266055,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.7156862745,
        "MMLU_high_school_world_history":0.7088607595,
        "MMLU_human_aging":0.5919282511,
        "MMLU_human_sexuality":0.5801526718,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.5833333333,
        "MMLU_logical_fallacies":0.5705521472,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.764957265,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.7088122605,
        "MMLU_moral_disputes":0.5173410405,
        "MMLU_moral_scenarios":0.2972067039,
        "MMLU_nutrition":0.5555555556,
        "MMLU_philosophy":0.6045016077,
        "MMLU_prehistory":0.5709876543,
        "MMLU_professional_accounting":0.3971631206,
        "MMLU_professional_law":0.3754889179,
        "MMLU_professional_medicine":0.4852941176,
        "MMLU_professional_psychology":0.4983660131,
        "MMLU_public_relations":0.6,
        "MMLU_security_studies":0.6081632653,
        "MMLU_sociology":0.671641791,
        "MMLU_us_foreign_policy":0.71,
        "MMLU_virology":0.3795180723,
        "MMLU_world_religions":0.6842105263
    },
    {
        "Model":"testlm",
        "URL":"https:\/\/huggingface.co\/Kiddyz\/testlm",
        "full_model_name":"Kiddyz\/testlm",
        "Parameters":null,
        "MMLU_average":0.5120686323,
        "arc:challenge|25":0.5017064846,
        "hellaswag|10":0.5705038837,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.5131578947,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.5433962264,
        "MMLU_college_biology":0.5555555556,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.4624277457,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.67,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.4965517241,
        "MMLU_elementary_mathematics":0.3333333333,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.5903225806,
        "MMLU_high_school_chemistry":0.3596059113,
        "MMLU_high_school_computer_science":0.48,
        "MMLU_high_school_european_history":0.6484848485,
        "MMLU_high_school_geography":0.6262626263,
        "MMLU_high_school_government_and_politics":0.7202072539,
        "MMLU_high_school_macroeconomics":0.4974358974,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.5210084034,
        "MMLU_high_school_physics":0.357615894,
        "MMLU_high_school_psychology":0.7119266055,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.7156862745,
        "MMLU_high_school_world_history":0.7088607595,
        "MMLU_human_aging":0.5919282511,
        "MMLU_human_sexuality":0.5801526718,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.5833333333,
        "MMLU_logical_fallacies":0.5705521472,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.764957265,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.7088122605,
        "MMLU_moral_disputes":0.5173410405,
        "MMLU_moral_scenarios":0.2972067039,
        "MMLU_nutrition":0.5555555556,
        "MMLU_philosophy":0.6045016077,
        "MMLU_prehistory":0.5709876543,
        "MMLU_professional_accounting":0.3971631206,
        "MMLU_professional_law":0.3754889179,
        "MMLU_professional_medicine":0.4852941176,
        "MMLU_professional_psychology":0.4983660131,
        "MMLU_public_relations":0.6,
        "MMLU_security_studies":0.6081632653,
        "MMLU_sociology":0.671641791,
        "MMLU_us_foreign_policy":0.71,
        "MMLU_virology":0.3795180723,
        "MMLU_world_religions":0.6842105263
    },
    {
        "Model":"testlm-1-1",
        "URL":"https:\/\/huggingface.co\/Kiddyz\/testlm-1-1",
        "full_model_name":"Kiddyz\/testlm-1-1",
        "Parameters":null,
        "MMLU_average":0.5120686323,
        "arc:challenge|25":0.5017064846,
        "hellaswag|10":0.5705038837,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.5131578947,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.5433962264,
        "MMLU_college_biology":0.5555555556,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.4624277457,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.67,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.4965517241,
        "MMLU_elementary_mathematics":0.3333333333,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.5903225806,
        "MMLU_high_school_chemistry":0.3596059113,
        "MMLU_high_school_computer_science":0.48,
        "MMLU_high_school_european_history":0.6484848485,
        "MMLU_high_school_geography":0.6262626263,
        "MMLU_high_school_government_and_politics":0.7202072539,
        "MMLU_high_school_macroeconomics":0.4974358974,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.5210084034,
        "MMLU_high_school_physics":0.357615894,
        "MMLU_high_school_psychology":0.7119266055,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.7156862745,
        "MMLU_high_school_world_history":0.7088607595,
        "MMLU_human_aging":0.5919282511,
        "MMLU_human_sexuality":0.5801526718,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.5833333333,
        "MMLU_logical_fallacies":0.5705521472,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.764957265,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.7088122605,
        "MMLU_moral_disputes":0.5173410405,
        "MMLU_moral_scenarios":0.2972067039,
        "MMLU_nutrition":0.5555555556,
        "MMLU_philosophy":0.6045016077,
        "MMLU_prehistory":0.5709876543,
        "MMLU_professional_accounting":0.3971631206,
        "MMLU_professional_law":0.3754889179,
        "MMLU_professional_medicine":0.4852941176,
        "MMLU_professional_psychology":0.4983660131,
        "MMLU_public_relations":0.6,
        "MMLU_security_studies":0.6081632653,
        "MMLU_sociology":0.671641791,
        "MMLU_us_foreign_policy":0.71,
        "MMLU_virology":0.3795180723,
        "MMLU_world_religions":0.6842105263
    },
    {
        "Model":"zarafusionix-l2-7b",
        "URL":"https:\/\/huggingface.co\/zarakiquemparte\/zarafusionix-l2-7b",
        "full_model_name":"zarakiquemparte\/zarafusionix-l2-7b",
        "Parameters":7.0,
        "MMLU_average":0.5120630833,
        "arc:challenge|25":0.5366894198,
        "hellaswag|10":0.60147381,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.5259259259,
        "MMLU_astronomy":0.4144736842,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.558490566,
        "MMLU_college_biology":0.5486111111,
        "MMLU_college_chemistry":0.33,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.4450867052,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.4638297872,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.3121693122,
        "MMLU_formal_logic":0.3015873016,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.5774193548,
        "MMLU_high_school_chemistry":0.3743842365,
        "MMLU_high_school_computer_science":0.45,
        "MMLU_high_school_european_history":0.6303030303,
        "MMLU_high_school_geography":0.6666666667,
        "MMLU_high_school_government_and_politics":0.7616580311,
        "MMLU_high_school_macroeconomics":0.4769230769,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.4663865546,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.7082568807,
        "MMLU_high_school_statistics":0.3888888889,
        "MMLU_high_school_us_history":0.7058823529,
        "MMLU_high_school_world_history":0.7130801688,
        "MMLU_human_aging":0.6143497758,
        "MMLU_human_sexuality":0.5877862595,
        "MMLU_international_law":0.6446280992,
        "MMLU_jurisprudence":0.5740740741,
        "MMLU_logical_fallacies":0.5644171779,
        "MMLU_machine_learning":0.4285714286,
        "MMLU_management":0.7087378641,
        "MMLU_marketing":0.7606837607,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.7088122605,
        "MMLU_moral_disputes":0.549132948,
        "MMLU_moral_scenarios":0.2759776536,
        "MMLU_nutrition":0.5424836601,
        "MMLU_philosophy":0.6109324759,
        "MMLU_prehistory":0.537037037,
        "MMLU_professional_accounting":0.3794326241,
        "MMLU_professional_law":0.371577575,
        "MMLU_professional_medicine":0.5294117647,
        "MMLU_professional_psychology":0.5,
        "MMLU_public_relations":0.5818181818,
        "MMLU_security_studies":0.5755102041,
        "MMLU_sociology":0.6815920398,
        "MMLU_us_foreign_policy":0.71,
        "MMLU_virology":0.4036144578,
        "MMLU_world_religions":0.7368421053
    },
    {
        "Model":"LDCC-Instruct-Llama-2-ko-13B",
        "URL":"https:\/\/huggingface.co\/krevas\/LDCC-Instruct-Llama-2-ko-13B",
        "full_model_name":"krevas\/LDCC-Instruct-Llama-2-ko-13B",
        "Parameters":13.0,
        "MMLU_average":0.5119710501,
        "arc:challenge|25":0.5392491468,
        "hellaswag|10":0.609639514,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.5197368421,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.5358490566,
        "MMLU_college_biology":0.5833333333,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.4450867052,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.67,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4275862069,
        "MMLU_elementary_mathematics":0.3201058201,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.5935483871,
        "MMLU_high_school_chemistry":0.3448275862,
        "MMLU_high_school_computer_science":0.44,
        "MMLU_high_school_european_history":0.6303030303,
        "MMLU_high_school_geography":0.6262626263,
        "MMLU_high_school_government_and_politics":0.7305699482,
        "MMLU_high_school_macroeconomics":0.4461538462,
        "MMLU_high_school_mathematics":0.3148148148,
        "MMLU_high_school_microeconomics":0.525210084,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.6678899083,
        "MMLU_high_school_statistics":0.3518518519,
        "MMLU_high_school_us_history":0.6666666667,
        "MMLU_high_school_world_history":0.6919831224,
        "MMLU_human_aging":0.600896861,
        "MMLU_human_sexuality":0.5801526718,
        "MMLU_international_law":0.6859504132,
        "MMLU_jurisprudence":0.6018518519,
        "MMLU_logical_fallacies":0.6564417178,
        "MMLU_machine_learning":0.4017857143,
        "MMLU_management":0.6601941748,
        "MMLU_marketing":0.7991452991,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.711366539,
        "MMLU_moral_disputes":0.5520231214,
        "MMLU_moral_scenarios":0.2491620112,
        "MMLU_nutrition":0.5359477124,
        "MMLU_philosophy":0.6366559486,
        "MMLU_prehistory":0.6265432099,
        "MMLU_professional_accounting":0.3617021277,
        "MMLU_professional_law":0.4106910039,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.5539215686,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.5346938776,
        "MMLU_sociology":0.7064676617,
        "MMLU_us_foreign_policy":0.75,
        "MMLU_virology":0.4638554217,
        "MMLU_world_religions":0.783625731
    },
    {
        "Model":"L2-7b-Orca-WVG-Test",
        "URL":"https:\/\/huggingface.co\/Lazycuber\/L2-7b-Orca-WVG-Test",
        "full_model_name":"Lazycuber\/L2-7b-Orca-WVG-Test",
        "Parameters":7.0,
        "MMLU_average":0.5112658076,
        "arc:challenge|25":0.5162116041,
        "hellaswag|10":0.5910177256,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.5259259259,
        "MMLU_astronomy":0.4473684211,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.6150943396,
        "MMLU_college_biology":0.5347222222,
        "MMLU_college_chemistry":0.36,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.4797687861,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.6,
        "MMLU_conceptual_physics":0.4723404255,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4482758621,
        "MMLU_elementary_mathematics":0.2857142857,
        "MMLU_formal_logic":0.2698412698,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.5387096774,
        "MMLU_high_school_chemistry":0.3842364532,
        "MMLU_high_school_computer_science":0.46,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.6616161616,
        "MMLU_high_school_government_and_politics":0.7098445596,
        "MMLU_high_school_macroeconomics":0.4487179487,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.474789916,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.7082568807,
        "MMLU_high_school_statistics":0.3796296296,
        "MMLU_high_school_us_history":0.7107843137,
        "MMLU_high_school_world_history":0.7510548523,
        "MMLU_human_aging":0.5964125561,
        "MMLU_human_sexuality":0.6259541985,
        "MMLU_international_law":0.6611570248,
        "MMLU_jurisprudence":0.6481481481,
        "MMLU_logical_fallacies":0.5705521472,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.7435897436,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7164750958,
        "MMLU_moral_disputes":0.5606936416,
        "MMLU_moral_scenarios":0.2837988827,
        "MMLU_nutrition":0.5326797386,
        "MMLU_philosophy":0.6012861736,
        "MMLU_prehistory":0.5586419753,
        "MMLU_professional_accounting":0.390070922,
        "MMLU_professional_law":0.3794002608,
        "MMLU_professional_medicine":0.4705882353,
        "MMLU_professional_psychology":0.4852941176,
        "MMLU_public_relations":0.5818181818,
        "MMLU_security_studies":0.5591836735,
        "MMLU_sociology":0.6268656716,
        "MMLU_us_foreign_policy":0.73,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.7192982456
    },
    {
        "Model":"L2-7b-Base-test-WVG",
        "URL":"https:\/\/huggingface.co\/LTC-AI-Labs\/L2-7b-Base-test-WVG",
        "full_model_name":"LTC-AI-Labs\/L2-7b-Base-test-WVG",
        "Parameters":7.0,
        "MMLU_average":0.5106737109,
        "arc:challenge|25":0.521331058,
        "hellaswag|10":0.5905198168,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.5185185185,
        "MMLU_astronomy":0.4736842105,
        "MMLU_business_ethics":0.5,
        "MMLU_clinical_knowledge":0.5735849057,
        "MMLU_college_biology":0.5486111111,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.4335260116,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.4765957447,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.2962962963,
        "MMLU_formal_logic":0.2777777778,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.564516129,
        "MMLU_high_school_chemistry":0.3596059113,
        "MMLU_high_school_computer_science":0.46,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.6515151515,
        "MMLU_high_school_government_and_politics":0.7357512953,
        "MMLU_high_school_macroeconomics":0.4641025641,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.4705882353,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.7064220183,
        "MMLU_high_school_statistics":0.375,
        "MMLU_high_school_us_history":0.6911764706,
        "MMLU_high_school_world_history":0.7172995781,
        "MMLU_human_aging":0.5560538117,
        "MMLU_human_sexuality":0.5954198473,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.6018518519,
        "MMLU_logical_fallacies":0.6012269939,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.7087378641,
        "MMLU_marketing":0.7777777778,
        "MMLU_medical_genetics":0.63,
        "MMLU_miscellaneous":0.7151979566,
        "MMLU_moral_disputes":0.5780346821,
        "MMLU_moral_scenarios":0.2748603352,
        "MMLU_nutrition":0.5392156863,
        "MMLU_philosophy":0.6270096463,
        "MMLU_prehistory":0.5648148148,
        "MMLU_professional_accounting":0.4184397163,
        "MMLU_professional_law":0.3754889179,
        "MMLU_professional_medicine":0.4669117647,
        "MMLU_professional_psychology":0.5163398693,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.5836734694,
        "MMLU_sociology":0.6417910448,
        "MMLU_us_foreign_policy":0.76,
        "MMLU_virology":0.3855421687,
        "MMLU_world_religions":0.6959064327
    },
    {
        "Model":"vicuna-7b-v1.5",
        "URL":"https:\/\/huggingface.co\/lmsys\/vicuna-7b-v1.5",
        "full_model_name":"lmsys\/vicuna-7b-v1.5",
        "Parameters":7.0,
        "MMLU_average":0.5103744902,
        "arc:challenge|25":0.5034129693,
        "hellaswag|10":0.5840470026,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.5037037037,
        "MMLU_astronomy":0.4736842105,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.5358490566,
        "MMLU_college_biology":0.5138888889,
        "MMLU_college_chemistry":0.33,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.39,
        "MMLU_college_medicine":0.4855491329,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.64,
        "MMLU_conceptual_physics":0.4510638298,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4275862069,
        "MMLU_elementary_mathematics":0.3068783069,
        "MMLU_formal_logic":0.380952381,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.5387096774,
        "MMLU_high_school_chemistry":0.3990147783,
        "MMLU_high_school_computer_science":0.44,
        "MMLU_high_school_european_history":0.6242424242,
        "MMLU_high_school_geography":0.6161616162,
        "MMLU_high_school_government_and_politics":0.7305699482,
        "MMLU_high_school_macroeconomics":0.4871794872,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.4495798319,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.6972477064,
        "MMLU_high_school_statistics":0.3842592593,
        "MMLU_high_school_us_history":0.7205882353,
        "MMLU_high_school_world_history":0.7130801688,
        "MMLU_human_aging":0.6233183857,
        "MMLU_human_sexuality":0.6335877863,
        "MMLU_international_law":0.5785123967,
        "MMLU_jurisprudence":0.5555555556,
        "MMLU_logical_fallacies":0.5337423313,
        "MMLU_machine_learning":0.4464285714,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.7692307692,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.687100894,
        "MMLU_moral_disputes":0.5606936416,
        "MMLU_moral_scenarios":0.2458100559,
        "MMLU_nutrition":0.5784313725,
        "MMLU_philosophy":0.5916398714,
        "MMLU_prehistory":0.5617283951,
        "MMLU_professional_accounting":0.365248227,
        "MMLU_professional_law":0.3728813559,
        "MMLU_professional_medicine":0.5404411765,
        "MMLU_professional_psychology":0.4918300654,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.6285714286,
        "MMLU_sociology":0.671641791,
        "MMLU_us_foreign_policy":0.76,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.7134502924
    },
    {
        "Model":"vigogne-2-7b-chat",
        "URL":"https:\/\/huggingface.co\/bofenghuang\/vigogne-2-7b-chat",
        "full_model_name":"bofenghuang\/vigogne-2-7b-chat",
        "Parameters":7.0,
        "MMLU_average":0.5098166825,
        "arc:challenge|25":0.5127986348,
        "hellaswag|10":0.5829516033,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.4605263158,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.5433962264,
        "MMLU_college_biology":0.5555555556,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.4,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.4566473988,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.4595744681,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.4344827586,
        "MMLU_elementary_mathematics":0.3386243386,
        "MMLU_formal_logic":0.373015873,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.5548387097,
        "MMLU_high_school_chemistry":0.3694581281,
        "MMLU_high_school_computer_science":0.45,
        "MMLU_high_school_european_history":0.6787878788,
        "MMLU_high_school_geography":0.6313131313,
        "MMLU_high_school_government_and_politics":0.6994818653,
        "MMLU_high_school_macroeconomics":0.4820512821,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.4957983193,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.7082568807,
        "MMLU_high_school_statistics":0.3703703704,
        "MMLU_high_school_us_history":0.6960784314,
        "MMLU_high_school_world_history":0.7172995781,
        "MMLU_human_aging":0.5784753363,
        "MMLU_human_sexuality":0.6335877863,
        "MMLU_international_law":0.6446280992,
        "MMLU_jurisprudence":0.5833333333,
        "MMLU_logical_fallacies":0.6012269939,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.7478632479,
        "MMLU_medical_genetics":0.61,
        "MMLU_miscellaneous":0.6909323116,
        "MMLU_moral_disputes":0.5346820809,
        "MMLU_moral_scenarios":0.2413407821,
        "MMLU_nutrition":0.5653594771,
        "MMLU_philosophy":0.6012861736,
        "MMLU_prehistory":0.5586419753,
        "MMLU_professional_accounting":0.3758865248,
        "MMLU_professional_law":0.3657105606,
        "MMLU_professional_medicine":0.5257352941,
        "MMLU_professional_psychology":0.4967320261,
        "MMLU_public_relations":0.5636363636,
        "MMLU_security_studies":0.6448979592,
        "MMLU_sociology":0.7313432836,
        "MMLU_us_foreign_policy":0.69,
        "MMLU_virology":0.3975903614,
        "MMLU_world_religions":0.7485380117
    },
    {
        "Model":"LLongMA-2-13b-16k",
        "URL":"https:\/\/huggingface.co\/conceptofmind\/LLongMA-2-13b-16k",
        "full_model_name":"conceptofmind\/LLongMA-2-13b-16k",
        "Parameters":13.0,
        "MMLU_average":0.5097464656,
        "arc:challenge|25":0.502559727,
        "hellaswag|10":0.5909181438,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.5197368421,
        "MMLU_business_ethics":0.49,
        "MMLU_clinical_knowledge":0.5547169811,
        "MMLU_college_biology":0.5486111111,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.450867052,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.63,
        "MMLU_conceptual_physics":0.3957446809,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.4896551724,
        "MMLU_elementary_mathematics":0.3227513228,
        "MMLU_formal_logic":0.3333333333,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.635483871,
        "MMLU_high_school_chemistry":0.39408867,
        "MMLU_high_school_computer_science":0.5,
        "MMLU_high_school_european_history":0.6242424242,
        "MMLU_high_school_geography":0.6666666667,
        "MMLU_high_school_government_and_politics":0.7150259067,
        "MMLU_high_school_macroeconomics":0.4615384615,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.5168067227,
        "MMLU_high_school_physics":0.3708609272,
        "MMLU_high_school_psychology":0.6697247706,
        "MMLU_high_school_statistics":0.4537037037,
        "MMLU_high_school_us_history":0.681372549,
        "MMLU_high_school_world_history":0.6160337553,
        "MMLU_human_aging":0.5739910314,
        "MMLU_human_sexuality":0.534351145,
        "MMLU_international_law":0.6694214876,
        "MMLU_jurisprudence":0.6111111111,
        "MMLU_logical_fallacies":0.6809815951,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.6699029126,
        "MMLU_marketing":0.7222222222,
        "MMLU_medical_genetics":0.43,
        "MMLU_miscellaneous":0.6998722861,
        "MMLU_moral_disputes":0.5722543353,
        "MMLU_moral_scenarios":0.251396648,
        "MMLU_nutrition":0.5653594771,
        "MMLU_philosophy":0.6045016077,
        "MMLU_prehistory":0.5956790123,
        "MMLU_professional_accounting":0.390070922,
        "MMLU_professional_law":0.3709256845,
        "MMLU_professional_medicine":0.4742647059,
        "MMLU_professional_psychology":0.4640522876,
        "MMLU_public_relations":0.5363636364,
        "MMLU_security_studies":0.5306122449,
        "MMLU_sociology":0.7412935323,
        "MMLU_us_foreign_policy":0.73,
        "MMLU_virology":0.4036144578,
        "MMLU_world_religions":0.7076023392
    },
    {
        "Model":"llama2-7b-instructmining-orca-90k",
        "URL":"https:\/\/huggingface.co\/yihan6324\/llama2-7b-instructmining-orca-90k",
        "full_model_name":"yihan6324\/llama2-7b-instructmining-orca-90k",
        "Parameters":7.0,
        "MMLU_average":0.5089445247,
        "arc:challenge|25":0.5093856655,
        "hellaswag|10":0.6147181836,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.5185185185,
        "MMLU_astronomy":0.4736842105,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.558490566,
        "MMLU_college_biology":0.5416666667,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.4335260116,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.4042553191,
        "MMLU_econometrics":0.3596491228,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.3015873016,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.6,
        "MMLU_high_school_chemistry":0.3349753695,
        "MMLU_high_school_computer_science":0.4,
        "MMLU_high_school_european_history":0.6909090909,
        "MMLU_high_school_geography":0.6565656566,
        "MMLU_high_school_government_and_politics":0.7461139896,
        "MMLU_high_school_macroeconomics":0.4794871795,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.4831932773,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.6935779817,
        "MMLU_high_school_statistics":0.375,
        "MMLU_high_school_us_history":0.7156862745,
        "MMLU_high_school_world_history":0.7426160338,
        "MMLU_human_aging":0.5964125561,
        "MMLU_human_sexuality":0.641221374,
        "MMLU_international_law":0.6694214876,
        "MMLU_jurisprudence":0.5833333333,
        "MMLU_logical_fallacies":0.5030674847,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.7264957265,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.7011494253,
        "MMLU_moral_disputes":0.5375722543,
        "MMLU_moral_scenarios":0.2625698324,
        "MMLU_nutrition":0.5261437908,
        "MMLU_philosophy":0.6045016077,
        "MMLU_prehistory":0.5586419753,
        "MMLU_professional_accounting":0.3758865248,
        "MMLU_professional_law":0.389178618,
        "MMLU_professional_medicine":0.5073529412,
        "MMLU_professional_psychology":0.4754901961,
        "MMLU_public_relations":0.5181818182,
        "MMLU_security_studies":0.6244897959,
        "MMLU_sociology":0.7263681592,
        "MMLU_us_foreign_policy":0.71,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.6666666667
    },
    {
        "Model":"MythoMax-L2-33b",
        "URL":"https:\/\/huggingface.co\/grimpep\/MythoMax-L2-33b",
        "full_model_name":"grimpep\/MythoMax-L2-33b",
        "Parameters":33.0,
        "MMLU_average":0.5084870722,
        "arc:challenge|25":0.5392491468,
        "hellaswag|10":0.5786695877,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.5,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.5433962264,
        "MMLU_college_biology":0.5277777778,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.4450867052,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.62,
        "MMLU_conceptual_physics":0.4255319149,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.4896551724,
        "MMLU_elementary_mathematics":0.2989417989,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.5709677419,
        "MMLU_high_school_chemistry":0.3842364532,
        "MMLU_high_school_computer_science":0.52,
        "MMLU_high_school_european_history":0.6787878788,
        "MMLU_high_school_geography":0.6313131313,
        "MMLU_high_school_government_and_politics":0.7512953368,
        "MMLU_high_school_macroeconomics":0.4641025641,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.525210084,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.6697247706,
        "MMLU_high_school_statistics":0.3055555556,
        "MMLU_high_school_us_history":0.7254901961,
        "MMLU_high_school_world_history":0.6962025316,
        "MMLU_human_aging":0.6457399103,
        "MMLU_human_sexuality":0.6030534351,
        "MMLU_international_law":0.6776859504,
        "MMLU_jurisprudence":0.6388888889,
        "MMLU_logical_fallacies":0.6441717791,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.6504854369,
        "MMLU_marketing":0.7991452991,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.69348659,
        "MMLU_moral_disputes":0.5289017341,
        "MMLU_moral_scenarios":0.2324022346,
        "MMLU_nutrition":0.5490196078,
        "MMLU_philosophy":0.6045016077,
        "MMLU_prehistory":0.5925925926,
        "MMLU_professional_accounting":0.4042553191,
        "MMLU_professional_law":0.3911342894,
        "MMLU_professional_medicine":0.4705882353,
        "MMLU_professional_psychology":0.5212418301,
        "MMLU_public_relations":0.6,
        "MMLU_security_studies":0.5714285714,
        "MMLU_sociology":0.6467661692,
        "MMLU_us_foreign_policy":0.75,
        "MMLU_virology":0.4096385542,
        "MMLU_world_religions":0.7251461988
    },
    {
        "Model":"vicuna-13b",
        "URL":"https:\/\/huggingface.co\/eachadea\/vicuna-13b",
        "full_model_name":"eachadea\/vicuna-13b",
        "Parameters":13.0,
        "MMLU_average":0.5083581477,
        "arc:challenge|25":0.5017064846,
        "hellaswag|10":0.6041625174,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.5460526316,
        "MMLU_business_ethics":0.47,
        "MMLU_clinical_knowledge":0.5018867925,
        "MMLU_college_biology":0.5486111111,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.4,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.4161849711,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.3872340426,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.4413793103,
        "MMLU_elementary_mathematics":0.3095238095,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.5612903226,
        "MMLU_high_school_chemistry":0.3645320197,
        "MMLU_high_school_computer_science":0.48,
        "MMLU_high_school_european_history":0.6606060606,
        "MMLU_high_school_geography":0.6262626263,
        "MMLU_high_school_government_and_politics":0.6994818653,
        "MMLU_high_school_macroeconomics":0.4641025641,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.525210084,
        "MMLU_high_school_physics":0.357615894,
        "MMLU_high_school_psychology":0.6844036697,
        "MMLU_high_school_statistics":0.3611111111,
        "MMLU_high_school_us_history":0.6617647059,
        "MMLU_high_school_world_history":0.7004219409,
        "MMLU_human_aging":0.5829596413,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.6363636364,
        "MMLU_jurisprudence":0.6203703704,
        "MMLU_logical_fallacies":0.6257668712,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.6990291262,
        "MMLU_marketing":0.7777777778,
        "MMLU_medical_genetics":0.53,
        "MMLU_miscellaneous":0.6858237548,
        "MMLU_moral_disputes":0.563583815,
        "MMLU_moral_scenarios":0.2703910615,
        "MMLU_nutrition":0.5588235294,
        "MMLU_philosophy":0.5434083601,
        "MMLU_prehistory":0.5277777778,
        "MMLU_professional_accounting":0.3758865248,
        "MMLU_professional_law":0.3917861799,
        "MMLU_professional_medicine":0.5294117647,
        "MMLU_professional_psychology":0.5163398693,
        "MMLU_public_relations":0.5363636364,
        "MMLU_security_studies":0.6040816327,
        "MMLU_sociology":0.7164179104,
        "MMLU_us_foreign_policy":0.76,
        "MMLU_virology":0.4518072289,
        "MMLU_world_religions":0.7192982456
    },
    {
        "Model":"WizardCoder-Python-34B-V1.0",
        "URL":"https:\/\/huggingface.co\/WizardLM\/WizardCoder-Python-34B-V1.0",
        "full_model_name":"WizardLM\/WizardCoder-Python-34B-V1.0",
        "Parameters":34.0,
        "MMLU_average":0.5075119336,
        "arc:challenge|25":0.5085324232,
        "hellaswag|10":0.5800637323,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.4074074074,
        "MMLU_astronomy":0.4671052632,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.5471698113,
        "MMLU_college_biology":0.4652777778,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.52,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.4855491329,
        "MMLU_college_physics":0.4215686275,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.4510638298,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.4100529101,
        "MMLU_formal_logic":0.3888888889,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.5193548387,
        "MMLU_high_school_chemistry":0.3793103448,
        "MMLU_high_school_computer_science":0.62,
        "MMLU_high_school_european_history":0.5818181818,
        "MMLU_high_school_geography":0.6363636364,
        "MMLU_high_school_government_and_politics":0.6943005181,
        "MMLU_high_school_macroeconomics":0.4435897436,
        "MMLU_high_school_mathematics":0.3444444444,
        "MMLU_high_school_microeconomics":0.4453781513,
        "MMLU_high_school_physics":0.357615894,
        "MMLU_high_school_psychology":0.6899082569,
        "MMLU_high_school_statistics":0.3472222222,
        "MMLU_high_school_us_history":0.6078431373,
        "MMLU_high_school_world_history":0.6835443038,
        "MMLU_human_aging":0.5246636771,
        "MMLU_human_sexuality":0.5572519084,
        "MMLU_international_law":0.7024793388,
        "MMLU_jurisprudence":0.5833333333,
        "MMLU_logical_fallacies":0.6196319018,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.7521367521,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.6717752235,
        "MMLU_moral_disputes":0.5346820809,
        "MMLU_moral_scenarios":0.3810055866,
        "MMLU_nutrition":0.4934640523,
        "MMLU_philosophy":0.5562700965,
        "MMLU_prehistory":0.512345679,
        "MMLU_professional_accounting":0.3794326241,
        "MMLU_professional_law":0.3572359844,
        "MMLU_professional_medicine":0.4044117647,
        "MMLU_professional_psychology":0.4558823529,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.5469387755,
        "MMLU_sociology":0.6019900498,
        "MMLU_us_foreign_policy":0.66,
        "MMLU_virology":0.3975903614,
        "MMLU_world_religions":0.6959064327
    },
    {
        "Model":"h2ogpt-research-oig-oasst1-512-30b",
        "URL":"https:\/\/huggingface.co\/h2oai\/h2ogpt-research-oig-oasst1-512-30b",
        "full_model_name":"h2oai\/h2ogpt-research-oig-oasst1-512-30b",
        "Parameters":30.0,
        "MMLU_average":0.5074064254,
        "arc:challenge|25":0.5418088737,
        "hellaswag|10":0.6253734316,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.5259259259,
        "MMLU_astronomy":0.5921052632,
        "MMLU_business_ethics":0.5,
        "MMLU_clinical_knowledge":0.520754717,
        "MMLU_college_biology":0.5416666667,
        "MMLU_college_chemistry":0.33,
        "MMLU_college_computer_science":0.34,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4797687861,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.65,
        "MMLU_conceptual_physics":0.4680851064,
        "MMLU_econometrics":0.3684210526,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.3333333333,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.5612903226,
        "MMLU_high_school_chemistry":0.3990147783,
        "MMLU_high_school_computer_science":0.48,
        "MMLU_high_school_european_history":0.6242424242,
        "MMLU_high_school_geography":0.5757575758,
        "MMLU_high_school_government_and_politics":0.725388601,
        "MMLU_high_school_macroeconomics":0.4487179487,
        "MMLU_high_school_mathematics":0.2888888889,
        "MMLU_high_school_microeconomics":0.5546218487,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.6972477064,
        "MMLU_high_school_statistics":0.3703703704,
        "MMLU_high_school_us_history":0.7058823529,
        "MMLU_high_school_world_history":0.7130801688,
        "MMLU_human_aging":0.600896861,
        "MMLU_human_sexuality":0.5267175573,
        "MMLU_international_law":0.5950413223,
        "MMLU_jurisprudence":0.6203703704,
        "MMLU_logical_fallacies":0.6257668712,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.5825242718,
        "MMLU_marketing":0.764957265,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.7266922095,
        "MMLU_moral_disputes":0.5780346821,
        "MMLU_moral_scenarios":0.2268156425,
        "MMLU_nutrition":0.522875817,
        "MMLU_philosophy":0.5852090032,
        "MMLU_prehistory":0.5586419753,
        "MMLU_professional_accounting":0.3794326241,
        "MMLU_professional_law":0.3637548892,
        "MMLU_professional_medicine":0.4889705882,
        "MMLU_professional_psychology":0.5130718954,
        "MMLU_public_relations":0.5363636364,
        "MMLU_security_studies":0.5183673469,
        "MMLU_sociology":0.6666666667,
        "MMLU_us_foreign_policy":0.77,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.730994152
    },
    {
        "Model":"10k_v1_lora_qkvo_rank14_v3",
        "URL":"https:\/\/huggingface.co\/xxyyy123\/10k_v1_lora_qkvo_rank14_v3",
        "full_model_name":"xxyyy123\/10k_v1_lora_qkvo_rank14_v3",
        "Parameters":null,
        "MMLU_average":0.5070887042,
        "arc:challenge|25":0.5298634812,
        "hellaswag|10":0.6050587532,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5,
        "MMLU_business_ethics":0.49,
        "MMLU_clinical_knowledge":0.558490566,
        "MMLU_college_biology":0.5486111111,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.4566473988,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.64,
        "MMLU_conceptual_physics":0.4680851064,
        "MMLU_econometrics":0.3421052632,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.2936507937,
        "MMLU_formal_logic":0.2698412698,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.5451612903,
        "MMLU_high_school_chemistry":0.3793103448,
        "MMLU_high_school_computer_science":0.39,
        "MMLU_high_school_european_history":0.7333333333,
        "MMLU_high_school_geography":0.6363636364,
        "MMLU_high_school_government_and_politics":0.725388601,
        "MMLU_high_school_macroeconomics":0.4948717949,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.512605042,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.7211009174,
        "MMLU_high_school_statistics":0.4166666667,
        "MMLU_high_school_us_history":0.6960784314,
        "MMLU_high_school_world_history":0.7341772152,
        "MMLU_human_aging":0.5515695067,
        "MMLU_human_sexuality":0.6030534351,
        "MMLU_international_law":0.7024793388,
        "MMLU_jurisprudence":0.6388888889,
        "MMLU_logical_fallacies":0.5398773006,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.764957265,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.7037037037,
        "MMLU_moral_disputes":0.5664739884,
        "MMLU_moral_scenarios":0.2547486034,
        "MMLU_nutrition":0.5359477124,
        "MMLU_philosophy":0.5819935691,
        "MMLU_prehistory":0.5524691358,
        "MMLU_professional_accounting":0.3687943262,
        "MMLU_professional_law":0.3820078227,
        "MMLU_professional_medicine":0.4816176471,
        "MMLU_professional_psychology":0.4820261438,
        "MMLU_public_relations":0.5818181818,
        "MMLU_security_studies":0.6040816327,
        "MMLU_sociology":0.5373134328,
        "MMLU_us_foreign_policy":0.69,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.6783625731
    },
    {
        "Model":"airocoder-34b-2.1",
        "URL":"https:\/\/huggingface.co\/jondurbin\/airocoder-34b-2.1",
        "full_model_name":"jondurbin\/airocoder-34b-2.1",
        "Parameters":34.0,
        "MMLU_average":0.506725892,
        "arc:challenge|25":0.5059726962,
        "hellaswag|10":0.556861183,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.4148148148,
        "MMLU_astronomy":0.5394736842,
        "MMLU_business_ethics":0.48,
        "MMLU_clinical_knowledge":0.5245283019,
        "MMLU_college_biology":0.4305555556,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.4624277457,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.4127659574,
        "MMLU_econometrics":0.4122807018,
        "MMLU_electrical_engineering":0.4965517241,
        "MMLU_elementary_mathematics":0.3677248677,
        "MMLU_formal_logic":0.4126984127,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.5967741935,
        "MMLU_high_school_chemistry":0.3694581281,
        "MMLU_high_school_computer_science":0.64,
        "MMLU_high_school_european_history":0.6424242424,
        "MMLU_high_school_geography":0.6161616162,
        "MMLU_high_school_government_and_politics":0.7357512953,
        "MMLU_high_school_macroeconomics":0.4512820513,
        "MMLU_high_school_mathematics":0.3222222222,
        "MMLU_high_school_microeconomics":0.5672268908,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.6275229358,
        "MMLU_high_school_statistics":0.3518518519,
        "MMLU_high_school_us_history":0.6666666667,
        "MMLU_high_school_world_history":0.6793248945,
        "MMLU_human_aging":0.4977578475,
        "MMLU_human_sexuality":0.4580152672,
        "MMLU_international_law":0.7438016529,
        "MMLU_jurisprudence":0.6388888889,
        "MMLU_logical_fallacies":0.6319018405,
        "MMLU_machine_learning":0.4107142857,
        "MMLU_management":0.7087378641,
        "MMLU_marketing":0.7136752137,
        "MMLU_medical_genetics":0.46,
        "MMLU_miscellaneous":0.6309067688,
        "MMLU_moral_disputes":0.5722543353,
        "MMLU_moral_scenarios":0.2581005587,
        "MMLU_nutrition":0.5032679739,
        "MMLU_philosophy":0.575562701,
        "MMLU_prehistory":0.5586419753,
        "MMLU_professional_accounting":0.3404255319,
        "MMLU_professional_law":0.3591916558,
        "MMLU_professional_medicine":0.3823529412,
        "MMLU_professional_psychology":0.4558823529,
        "MMLU_public_relations":0.6,
        "MMLU_security_studies":0.587755102,
        "MMLU_sociology":0.6815920398,
        "MMLU_us_foreign_policy":0.69,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.6432748538
    },
    {
        "Model":"20k_v1_lora_qkvo_rank14_v2",
        "URL":"https:\/\/huggingface.co\/xxyyy123\/20k_v1_lora_qkvo_rank14_v2",
        "full_model_name":"xxyyy123\/20k_v1_lora_qkvo_rank14_v2",
        "Parameters":null,
        "MMLU_average":0.5065428252,
        "arc:challenge|25":0.5264505119,
        "hellaswag|10":0.6025692093,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.4605263158,
        "MMLU_business_ethics":0.45,
        "MMLU_clinical_knowledge":0.5735849057,
        "MMLU_college_biology":0.5555555556,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4450867052,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.64,
        "MMLU_conceptual_physics":0.4638297872,
        "MMLU_econometrics":0.350877193,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.2883597884,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.4,
        "MMLU_high_school_biology":0.5161290323,
        "MMLU_high_school_chemistry":0.3743842365,
        "MMLU_high_school_computer_science":0.39,
        "MMLU_high_school_european_history":0.7212121212,
        "MMLU_high_school_geography":0.6313131313,
        "MMLU_high_school_government_and_politics":0.7202072539,
        "MMLU_high_school_macroeconomics":0.4641025641,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.4915966387,
        "MMLU_high_school_physics":0.3642384106,
        "MMLU_high_school_psychology":0.7082568807,
        "MMLU_high_school_statistics":0.3842592593,
        "MMLU_high_school_us_history":0.7058823529,
        "MMLU_high_school_world_history":0.7130801688,
        "MMLU_human_aging":0.5695067265,
        "MMLU_human_sexuality":0.5954198473,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.6666666667,
        "MMLU_logical_fallacies":0.5766871166,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.6504854369,
        "MMLU_marketing":0.7393162393,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.7088122605,
        "MMLU_moral_disputes":0.563583815,
        "MMLU_moral_scenarios":0.269273743,
        "MMLU_nutrition":0.522875817,
        "MMLU_philosophy":0.5852090032,
        "MMLU_prehistory":0.5586419753,
        "MMLU_professional_accounting":0.3829787234,
        "MMLU_professional_law":0.3748370274,
        "MMLU_professional_medicine":0.4889705882,
        "MMLU_professional_psychology":0.4852941176,
        "MMLU_public_relations":0.6,
        "MMLU_security_studies":0.6081632653,
        "MMLU_sociology":0.5323383085,
        "MMLU_us_foreign_policy":0.68,
        "MMLU_virology":0.4518072289,
        "MMLU_world_religions":0.6783625731
    },
    {
        "Model":"13B-HyperMantis",
        "URL":"https:\/\/huggingface.co\/digitous\/13B-HyperMantis",
        "full_model_name":"digitous\/13B-HyperMantis",
        "Parameters":13.0,
        "MMLU_average":0.5061198225,
        "arc:challenge|25":0.5554607509,
        "hellaswag|10":0.6267675762,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.4934210526,
        "MMLU_business_ethics":0.47,
        "MMLU_clinical_knowledge":0.5056603774,
        "MMLU_college_biology":0.5277777778,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4161849711,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.62,
        "MMLU_conceptual_physics":0.4170212766,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.4344827586,
        "MMLU_elementary_mathematics":0.2619047619,
        "MMLU_formal_logic":0.3968253968,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.5548387097,
        "MMLU_high_school_chemistry":0.3497536946,
        "MMLU_high_school_computer_science":0.53,
        "MMLU_high_school_european_history":0.6424242424,
        "MMLU_high_school_geography":0.6464646465,
        "MMLU_high_school_government_and_politics":0.6683937824,
        "MMLU_high_school_macroeconomics":0.4769230769,
        "MMLU_high_school_mathematics":0.237037037,
        "MMLU_high_school_microeconomics":0.4705882353,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.6917431193,
        "MMLU_high_school_statistics":0.3425925926,
        "MMLU_high_school_us_history":0.6862745098,
        "MMLU_high_school_world_history":0.7130801688,
        "MMLU_human_aging":0.5874439462,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.6859504132,
        "MMLU_jurisprudence":0.5648148148,
        "MMLU_logical_fallacies":0.5582822086,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.7692307692,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.7088122605,
        "MMLU_moral_disputes":0.5520231214,
        "MMLU_moral_scenarios":0.251396648,
        "MMLU_nutrition":0.5555555556,
        "MMLU_philosophy":0.5691318328,
        "MMLU_prehistory":0.5586419753,
        "MMLU_professional_accounting":0.3865248227,
        "MMLU_professional_law":0.4002607562,
        "MMLU_professional_medicine":0.5073529412,
        "MMLU_professional_psychology":0.522875817,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.5469387755,
        "MMLU_sociology":0.6815920398,
        "MMLU_us_foreign_policy":0.8,
        "MMLU_virology":0.4518072289,
        "MMLU_world_religions":0.7368421053
    },
    {
        "Model":"EverythingLM-13b-V3-16k",
        "URL":"https:\/\/huggingface.co\/totally-not-an-llm\/EverythingLM-13b-V3-16k",
        "full_model_name":"totally-not-an-llm\/EverythingLM-13b-V3-16k",
        "Parameters":13.0,
        "MMLU_average":0.5047968078,
        "arc:challenge|25":0.5366894198,
        "hellaswag|10":0.6027683728,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.4934210526,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.558490566,
        "MMLU_college_biology":0.5486111111,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.38,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.4450867052,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.370212766,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.4551724138,
        "MMLU_elementary_mathematics":0.3042328042,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.25,
        "MMLU_high_school_biology":0.6129032258,
        "MMLU_high_school_chemistry":0.4334975369,
        "MMLU_high_school_computer_science":0.51,
        "MMLU_high_school_european_history":0.6242424242,
        "MMLU_high_school_geography":0.6767676768,
        "MMLU_high_school_government_and_politics":0.7305699482,
        "MMLU_high_school_macroeconomics":0.4820512821,
        "MMLU_high_school_mathematics":0.3111111111,
        "MMLU_high_school_microeconomics":0.4915966387,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.6917431193,
        "MMLU_high_school_statistics":0.4351851852,
        "MMLU_high_school_us_history":0.6470588235,
        "MMLU_high_school_world_history":0.6244725738,
        "MMLU_human_aging":0.5964125561,
        "MMLU_human_sexuality":0.5572519084,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.6388888889,
        "MMLU_logical_fallacies":0.6257668712,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.640776699,
        "MMLU_marketing":0.7222222222,
        "MMLU_medical_genetics":0.5,
        "MMLU_miscellaneous":0.6743295019,
        "MMLU_moral_disputes":0.5549132948,
        "MMLU_moral_scenarios":0.2681564246,
        "MMLU_nutrition":0.5424836601,
        "MMLU_philosophy":0.5980707395,
        "MMLU_prehistory":0.5925925926,
        "MMLU_professional_accounting":0.3758865248,
        "MMLU_professional_law":0.3617992177,
        "MMLU_professional_medicine":0.4191176471,
        "MMLU_professional_psychology":0.4624183007,
        "MMLU_public_relations":0.5454545455,
        "MMLU_security_studies":0.4897959184,
        "MMLU_sociology":0.6865671642,
        "MMLU_us_foreign_policy":0.72,
        "MMLU_virology":0.4096385542,
        "MMLU_world_religions":0.6900584795
    },
    {
        "Model":"vicuna-7b-v1.5-lora-timedial-unit-080082",
        "URL":"https:\/\/huggingface.co\/Charlie911\/vicuna-7b-v1.5-lora-timedial-unit-080082",
        "full_model_name":"Charlie911\/vicuna-7b-v1.5-lora-timedial-unit-080082",
        "Parameters":7.0,
        "MMLU_average":0.5046805508,
        "arc:challenge|25":0.4906143345,
        "hellaswag|10":0.5622385979,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.5,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.5283018868,
        "MMLU_college_biology":0.5069444444,
        "MMLU_college_chemistry":0.33,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.42,
        "MMLU_college_medicine":0.4855491329,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.62,
        "MMLU_conceptual_physics":0.4468085106,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.4413793103,
        "MMLU_elementary_mathematics":0.3015873016,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.5483870968,
        "MMLU_high_school_chemistry":0.39408867,
        "MMLU_high_school_computer_science":0.45,
        "MMLU_high_school_european_history":0.6242424242,
        "MMLU_high_school_geography":0.6161616162,
        "MMLU_high_school_government_and_politics":0.7098445596,
        "MMLU_high_school_macroeconomics":0.4769230769,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.4411764706,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.6899082569,
        "MMLU_high_school_statistics":0.3935185185,
        "MMLU_high_school_us_history":0.7058823529,
        "MMLU_high_school_world_history":0.7130801688,
        "MMLU_human_aging":0.5964125561,
        "MMLU_human_sexuality":0.5954198473,
        "MMLU_international_law":0.5867768595,
        "MMLU_jurisprudence":0.5648148148,
        "MMLU_logical_fallacies":0.5337423313,
        "MMLU_machine_learning":0.4285714286,
        "MMLU_management":0.6699029126,
        "MMLU_marketing":0.764957265,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.6819923372,
        "MMLU_moral_disputes":0.5346820809,
        "MMLU_moral_scenarios":0.2458100559,
        "MMLU_nutrition":0.5620915033,
        "MMLU_philosophy":0.5916398714,
        "MMLU_prehistory":0.5432098765,
        "MMLU_professional_accounting":0.3723404255,
        "MMLU_professional_law":0.3637548892,
        "MMLU_professional_medicine":0.5404411765,
        "MMLU_professional_psychology":0.4901960784,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.6489795918,
        "MMLU_sociology":0.671641791,
        "MMLU_us_foreign_policy":0.7,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.7076023392
    },
    {
        "Model":"vicuna-7b-v1.5-lora-timedial",
        "URL":"https:\/\/huggingface.co\/Charlie911\/vicuna-7b-v1.5-lora-timedial",
        "full_model_name":"Charlie911\/vicuna-7b-v1.5-lora-timedial",
        "Parameters":7.0,
        "MMLU_average":0.5046584252,
        "arc:challenge|25":0.4889078498,
        "hellaswag|10":0.5650268871,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.4934210526,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.5320754717,
        "MMLU_college_biology":0.5138888889,
        "MMLU_college_chemistry":0.36,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.42,
        "MMLU_college_medicine":0.4739884393,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.65,
        "MMLU_conceptual_physics":0.4510638298,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.4413793103,
        "MMLU_elementary_mathematics":0.2989417989,
        "MMLU_formal_logic":0.3412698413,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.5483870968,
        "MMLU_high_school_chemistry":0.3793103448,
        "MMLU_high_school_computer_science":0.44,
        "MMLU_high_school_european_history":0.6242424242,
        "MMLU_high_school_geography":0.6212121212,
        "MMLU_high_school_government_and_politics":0.7150259067,
        "MMLU_high_school_macroeconomics":0.4615384615,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.4495798319,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.6862385321,
        "MMLU_high_school_statistics":0.3842592593,
        "MMLU_high_school_us_history":0.6764705882,
        "MMLU_high_school_world_history":0.7215189873,
        "MMLU_human_aging":0.600896861,
        "MMLU_human_sexuality":0.5877862595,
        "MMLU_international_law":0.5785123967,
        "MMLU_jurisprudence":0.5555555556,
        "MMLU_logical_fallacies":0.5337423313,
        "MMLU_machine_learning":0.4285714286,
        "MMLU_management":0.640776699,
        "MMLU_marketing":0.7735042735,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.680715198,
        "MMLU_moral_disputes":0.5375722543,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.568627451,
        "MMLU_philosophy":0.5884244373,
        "MMLU_prehistory":0.5432098765,
        "MMLU_professional_accounting":0.3758865248,
        "MMLU_professional_law":0.3748370274,
        "MMLU_professional_medicine":0.5367647059,
        "MMLU_professional_psychology":0.4803921569,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.6448979592,
        "MMLU_sociology":0.6865671642,
        "MMLU_us_foreign_policy":0.71,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.701754386
    },
    {
        "Model":"Nous-Hermes-13b",
        "URL":"https:\/\/huggingface.co\/NousResearch\/Nous-Hermes-13b",
        "full_model_name":"NousResearch\/Nous-Hermes-13b",
        "Parameters":13.0,
        "MMLU_average":0.5044071264,
        "arc:challenge|25":0.5494880546,
        "hellaswag|10":0.6221868154,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.5,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.5471698113,
        "MMLU_college_biology":0.5,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.4335260116,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.65,
        "MMLU_conceptual_physics":0.4127659574,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.4,
        "MMLU_elementary_mathematics":0.2857142857,
        "MMLU_formal_logic":0.3888888889,
        "MMLU_global_facts":0.4,
        "MMLU_high_school_biology":0.5516129032,
        "MMLU_high_school_chemistry":0.3497536946,
        "MMLU_high_school_computer_science":0.53,
        "MMLU_high_school_european_history":0.6424242424,
        "MMLU_high_school_geography":0.6414141414,
        "MMLU_high_school_government_and_politics":0.6839378238,
        "MMLU_high_school_macroeconomics":0.4435897436,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.5210084034,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.695412844,
        "MMLU_high_school_statistics":0.3194444444,
        "MMLU_high_school_us_history":0.681372549,
        "MMLU_high_school_world_history":0.7088607595,
        "MMLU_human_aging":0.5695067265,
        "MMLU_human_sexuality":0.6335877863,
        "MMLU_international_law":0.6694214876,
        "MMLU_jurisprudence":0.5833333333,
        "MMLU_logical_fallacies":0.5582822086,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.7521367521,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.7011494253,
        "MMLU_moral_disputes":0.5578034682,
        "MMLU_moral_scenarios":0.2324022346,
        "MMLU_nutrition":0.5653594771,
        "MMLU_philosophy":0.5498392283,
        "MMLU_prehistory":0.5709876543,
        "MMLU_professional_accounting":0.3723404255,
        "MMLU_professional_law":0.3859191656,
        "MMLU_professional_medicine":0.4852941176,
        "MMLU_professional_psychology":0.5277777778,
        "MMLU_public_relations":0.6,
        "MMLU_security_studies":0.5224489796,
        "MMLU_sociology":0.7114427861,
        "MMLU_us_foreign_policy":0.79,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.730994152
    },
    {
        "Model":"llama2-7b-instructmining-40k-sharegpt",
        "URL":"https:\/\/huggingface.co\/yihan6324\/llama2-7b-instructmining-40k-sharegpt",
        "full_model_name":"yihan6324\/llama2-7b-instructmining-40k-sharegpt",
        "Parameters":7.0,
        "MMLU_average":0.5043228368,
        "arc:challenge|25":0.5170648464,
        "hellaswag|10":0.6041625174,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.4605263158,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.5245283019,
        "MMLU_college_biology":0.4930555556,
        "MMLU_college_chemistry":0.36,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.4682080925,
        "MMLU_college_physics":0.1764705882,
        "MMLU_computer_security":0.63,
        "MMLU_conceptual_physics":0.4723404255,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.4965517241,
        "MMLU_elementary_mathematics":0.3148148148,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.5322580645,
        "MMLU_high_school_chemistry":0.354679803,
        "MMLU_high_school_computer_science":0.46,
        "MMLU_high_school_european_history":0.6484848485,
        "MMLU_high_school_geography":0.595959596,
        "MMLU_high_school_government_and_politics":0.7512953368,
        "MMLU_high_school_macroeconomics":0.4564102564,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.4243697479,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.6990825688,
        "MMLU_high_school_statistics":0.3425925926,
        "MMLU_high_school_us_history":0.6764705882,
        "MMLU_high_school_world_history":0.7172995781,
        "MMLU_human_aging":0.5964125561,
        "MMLU_human_sexuality":0.6183206107,
        "MMLU_international_law":0.6115702479,
        "MMLU_jurisprudence":0.6111111111,
        "MMLU_logical_fallacies":0.5828220859,
        "MMLU_machine_learning":0.4107142857,
        "MMLU_management":0.6601941748,
        "MMLU_marketing":0.7564102564,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.6794380587,
        "MMLU_moral_disputes":0.563583815,
        "MMLU_moral_scenarios":0.2648044693,
        "MMLU_nutrition":0.5196078431,
        "MMLU_philosophy":0.5916398714,
        "MMLU_prehistory":0.5216049383,
        "MMLU_professional_accounting":0.3936170213,
        "MMLU_professional_law":0.3820078227,
        "MMLU_professional_medicine":0.5257352941,
        "MMLU_professional_psychology":0.4918300654,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.5918367347,
        "MMLU_sociology":0.6915422886,
        "MMLU_us_foreign_policy":0.68,
        "MMLU_virology":0.3915662651,
        "MMLU_world_religions":0.7134502924
    },
    {
        "Model":"zaraxls-l2-7b",
        "URL":"https:\/\/huggingface.co\/zarakiquemparte\/zaraxls-l2-7b",
        "full_model_name":"zarakiquemparte\/zaraxls-l2-7b",
        "Parameters":7.0,
        "MMLU_average":0.5038629424,
        "arc:challenge|25":0.5290102389,
        "hellaswag|10":0.6023700458,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.4342105263,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.5622641509,
        "MMLU_college_biology":0.5138888889,
        "MMLU_college_chemistry":0.36,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.4450867052,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.429787234,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.4344827586,
        "MMLU_elementary_mathematics":0.3068783069,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.5870967742,
        "MMLU_high_school_chemistry":0.3349753695,
        "MMLU_high_school_computer_science":0.42,
        "MMLU_high_school_european_history":0.6484848485,
        "MMLU_high_school_geography":0.6515151515,
        "MMLU_high_school_government_and_politics":0.7202072539,
        "MMLU_high_school_macroeconomics":0.4794871795,
        "MMLU_high_school_mathematics":0.2185185185,
        "MMLU_high_school_microeconomics":0.4579831933,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.7174311927,
        "MMLU_high_school_statistics":0.4027777778,
        "MMLU_high_school_us_history":0.681372549,
        "MMLU_high_school_world_history":0.7088607595,
        "MMLU_human_aging":0.5919282511,
        "MMLU_human_sexuality":0.6259541985,
        "MMLU_international_law":0.6363636364,
        "MMLU_jurisprudence":0.5462962963,
        "MMLU_logical_fallacies":0.5398773006,
        "MMLU_machine_learning":0.4732142857,
        "MMLU_management":0.6504854369,
        "MMLU_marketing":0.7521367521,
        "MMLU_medical_genetics":0.6,
        "MMLU_miscellaneous":0.7037037037,
        "MMLU_moral_disputes":0.5346820809,
        "MMLU_moral_scenarios":0.3039106145,
        "MMLU_nutrition":0.5359477124,
        "MMLU_philosophy":0.5819935691,
        "MMLU_prehistory":0.5401234568,
        "MMLU_professional_accounting":0.3865248227,
        "MMLU_professional_law":0.3487614081,
        "MMLU_professional_medicine":0.4926470588,
        "MMLU_professional_psychology":0.4705882353,
        "MMLU_public_relations":0.6,
        "MMLU_security_studies":0.6204081633,
        "MMLU_sociology":0.6368159204,
        "MMLU_us_foreign_policy":0.71,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.7192982456
    },
    {
        "Model":"Stable-Vicuna-13B",
        "URL":"https:\/\/huggingface.co\/LLMs\/Stable-Vicuna-13B",
        "full_model_name":"LLMs\/Stable-Vicuna-13B",
        "Parameters":13.0,
        "MMLU_average":0.5036523509,
        "arc:challenge|25":0.5145051195,
        "hellaswag|10":0.5866361283,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5592105263,
        "MMLU_business_ethics":0.48,
        "MMLU_clinical_knowledge":0.4566037736,
        "MMLU_college_biology":0.5277777778,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.3988439306,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.4042553191,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4482758621,
        "MMLU_elementary_mathematics":0.3068783069,
        "MMLU_formal_logic":0.3412698413,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.5580645161,
        "MMLU_high_school_chemistry":0.3300492611,
        "MMLU_high_school_computer_science":0.49,
        "MMLU_high_school_european_history":0.6363636364,
        "MMLU_high_school_geography":0.6212121212,
        "MMLU_high_school_government_and_politics":0.689119171,
        "MMLU_high_school_macroeconomics":0.458974359,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.5084033613,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.6678899083,
        "MMLU_high_school_statistics":0.375,
        "MMLU_high_school_us_history":0.6323529412,
        "MMLU_high_school_world_history":0.6582278481,
        "MMLU_human_aging":0.5964125561,
        "MMLU_human_sexuality":0.6183206107,
        "MMLU_international_law":0.6280991736,
        "MMLU_jurisprudence":0.6111111111,
        "MMLU_logical_fallacies":0.6441717791,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.6601941748,
        "MMLU_marketing":0.7863247863,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.6743295019,
        "MMLU_moral_disputes":0.5578034682,
        "MMLU_moral_scenarios":0.2715083799,
        "MMLU_nutrition":0.5424836601,
        "MMLU_philosophy":0.5530546624,
        "MMLU_prehistory":0.5216049383,
        "MMLU_professional_accounting":0.3687943262,
        "MMLU_professional_law":0.3983050847,
        "MMLU_professional_medicine":0.5036764706,
        "MMLU_professional_psychology":0.5049019608,
        "MMLU_public_relations":0.5545454545,
        "MMLU_security_studies":0.6163265306,
        "MMLU_sociology":0.6517412935,
        "MMLU_us_foreign_policy":0.72,
        "MMLU_virology":0.4698795181,
        "MMLU_world_religions":0.7368421053
    },
    {
        "Model":"ALMA-13B-Pretrain",
        "URL":"https:\/\/huggingface.co\/haoranxu\/ALMA-13B-Pretrain",
        "full_model_name":"haoranxu\/ALMA-13B-Pretrain",
        "Parameters":13.0,
        "MMLU_average":0.5030810375,
        "arc:challenge|25":0.5383959044,
        "hellaswag|10":0.5947022505,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.5703703704,
        "MMLU_astronomy":0.5,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.5358490566,
        "MMLU_college_biology":0.5277777778,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.37,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.450867052,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.7,
        "MMLU_conceptual_physics":0.4,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.4344827586,
        "MMLU_elementary_mathematics":0.3412698413,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.5806451613,
        "MMLU_high_school_chemistry":0.4039408867,
        "MMLU_high_school_computer_science":0.46,
        "MMLU_high_school_european_history":0.6181818182,
        "MMLU_high_school_geography":0.6212121212,
        "MMLU_high_school_government_and_politics":0.7046632124,
        "MMLU_high_school_macroeconomics":0.4717948718,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.5042016807,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.647706422,
        "MMLU_high_school_statistics":0.3796296296,
        "MMLU_high_school_us_history":0.6421568627,
        "MMLU_high_school_world_history":0.641350211,
        "MMLU_human_aging":0.6322869955,
        "MMLU_human_sexuality":0.5801526718,
        "MMLU_international_law":0.7272727273,
        "MMLU_jurisprudence":0.6388888889,
        "MMLU_logical_fallacies":0.5705521472,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.6019417476,
        "MMLU_marketing":0.6794871795,
        "MMLU_medical_genetics":0.47,
        "MMLU_miscellaneous":0.6794380587,
        "MMLU_moral_disputes":0.598265896,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.5522875817,
        "MMLU_philosophy":0.6205787781,
        "MMLU_prehistory":0.5709876543,
        "MMLU_professional_accounting":0.4078014184,
        "MMLU_professional_law":0.3787483703,
        "MMLU_professional_medicine":0.4448529412,
        "MMLU_professional_psychology":0.5130718954,
        "MMLU_public_relations":0.5272727273,
        "MMLU_security_studies":0.5714285714,
        "MMLU_sociology":0.6766169154,
        "MMLU_us_foreign_policy":0.74,
        "MMLU_virology":0.4397590361,
        "MMLU_world_religions":0.730994152
    },
    {
        "Model":"mc_data_30k_from_platpus_orca_7b_10k_v1_lora_qkvo_rank14_v2",
        "URL":"https:\/\/huggingface.co\/xxyyy123\/mc_data_30k_from_platpus_orca_7b_10k_v1_lora_qkvo_rank14_v2",
        "full_model_name":"xxyyy123\/mc_data_30k_from_platpus_orca_7b_10k_v1_lora_qkvo_rank14_v2",
        "Parameters":7.0,
        "MMLU_average":0.5023777108,
        "arc:challenge|25":0.542662116,
        "hellaswag|10":0.6072495519,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.5065789474,
        "MMLU_business_ethics":0.44,
        "MMLU_clinical_knowledge":0.5811320755,
        "MMLU_college_biology":0.5625,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.35,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.450867052,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.63,
        "MMLU_conceptual_physics":0.4595744681,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.4551724138,
        "MMLU_elementary_mathematics":0.3015873016,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.5322580645,
        "MMLU_high_school_chemistry":0.354679803,
        "MMLU_high_school_computer_science":0.38,
        "MMLU_high_school_european_history":0.7272727273,
        "MMLU_high_school_geography":0.6363636364,
        "MMLU_high_school_government_and_politics":0.725388601,
        "MMLU_high_school_macroeconomics":0.4871794872,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.5084033613,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.719266055,
        "MMLU_high_school_statistics":0.4027777778,
        "MMLU_high_school_us_history":0.6960784314,
        "MMLU_high_school_world_history":0.7299578059,
        "MMLU_human_aging":0.5381165919,
        "MMLU_human_sexuality":0.5801526718,
        "MMLU_international_law":0.6776859504,
        "MMLU_jurisprudence":0.6296296296,
        "MMLU_logical_fallacies":0.5337423313,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.764957265,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.6922094508,
        "MMLU_moral_disputes":0.563583815,
        "MMLU_moral_scenarios":0.2491620112,
        "MMLU_nutrition":0.5130718954,
        "MMLU_philosophy":0.578778135,
        "MMLU_prehistory":0.5524691358,
        "MMLU_professional_accounting":0.3475177305,
        "MMLU_professional_law":0.3852672751,
        "MMLU_professional_medicine":0.4926470588,
        "MMLU_professional_psychology":0.4918300654,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.587755102,
        "MMLU_sociology":0.5671641791,
        "MMLU_us_foreign_policy":0.7,
        "MMLU_virology":0.4096385542,
        "MMLU_world_religions":0.701754386
    },
    {
        "Model":"Platypus2xOpenOrca-13B-LoRa-v2",
        "URL":"https:\/\/huggingface.co\/yeontaek\/Platypus2xOpenOrca-13B-LoRa-v2",
        "full_model_name":"yeontaek\/Platypus2xOpenOrca-13B-LoRa-v2",
        "Parameters":13.0,
        "MMLU_average":0.5022911788,
        "arc:challenge|25":0.5520477816,
        "hellaswag|10":0.6085441147,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.4342105263,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.5471698113,
        "MMLU_college_biology":0.4930555556,
        "MMLU_college_chemistry":0.27,
        "MMLU_college_computer_science":0.37,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.4566473988,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.63,
        "MMLU_conceptual_physics":0.4042553191,
        "MMLU_econometrics":0.2192982456,
        "MMLU_electrical_engineering":0.3724137931,
        "MMLU_elementary_mathematics":0.3174603175,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.43,
        "MMLU_high_school_biology":0.5741935484,
        "MMLU_high_school_chemistry":0.39408867,
        "MMLU_high_school_computer_science":0.53,
        "MMLU_high_school_european_history":0.6909090909,
        "MMLU_high_school_geography":0.5606060606,
        "MMLU_high_school_government_and_politics":0.6321243523,
        "MMLU_high_school_macroeconomics":0.4846153846,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.4705882353,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.7119266055,
        "MMLU_high_school_statistics":0.3518518519,
        "MMLU_high_school_us_history":0.7205882353,
        "MMLU_high_school_world_history":0.746835443,
        "MMLU_human_aging":0.6322869955,
        "MMLU_human_sexuality":0.5267175573,
        "MMLU_international_law":0.6280991736,
        "MMLU_jurisprudence":0.5648148148,
        "MMLU_logical_fallacies":0.6196319018,
        "MMLU_machine_learning":0.4017857143,
        "MMLU_management":0.6213592233,
        "MMLU_marketing":0.764957265,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.7215836526,
        "MMLU_moral_disputes":0.549132948,
        "MMLU_moral_scenarios":0.2960893855,
        "MMLU_nutrition":0.477124183,
        "MMLU_philosophy":0.6077170418,
        "MMLU_prehistory":0.6141975309,
        "MMLU_professional_accounting":0.4326241135,
        "MMLU_professional_law":0.4250325945,
        "MMLU_professional_medicine":0.4705882353,
        "MMLU_professional_psychology":0.5277777778,
        "MMLU_public_relations":0.6727272727,
        "MMLU_security_studies":0.4244897959,
        "MMLU_sociology":0.6517412935,
        "MMLU_us_foreign_policy":0.69,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.7485380117
    },
    {
        "Model":"OpenOrca-Preview1-13B",
        "URL":"https:\/\/huggingface.co\/Open-Orca\/OpenOrca-Preview1-13B",
        "full_model_name":"Open-Orca\/OpenOrca-Preview1-13B",
        "Parameters":13.0,
        "MMLU_average":0.5013607287,
        "arc:challenge|25":0.5315699659,
        "hellaswag|10":0.5820553675,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4518518519,
        "MMLU_astronomy":0.5328947368,
        "MMLU_business_ethics":0.46,
        "MMLU_clinical_knowledge":0.5094339623,
        "MMLU_college_biology":0.4791666667,
        "MMLU_college_chemistry":0.28,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.4393063584,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.63,
        "MMLU_conceptual_physics":0.3914893617,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.4827586207,
        "MMLU_elementary_mathematics":0.2989417989,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.6,
        "MMLU_high_school_chemistry":0.3497536946,
        "MMLU_high_school_computer_science":0.54,
        "MMLU_high_school_european_history":0.6545454545,
        "MMLU_high_school_geography":0.6414141414,
        "MMLU_high_school_government_and_politics":0.6994818653,
        "MMLU_high_school_macroeconomics":0.4307692308,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.4579831933,
        "MMLU_high_school_physics":0.3708609272,
        "MMLU_high_school_psychology":0.6422018349,
        "MMLU_high_school_statistics":0.3935185185,
        "MMLU_high_school_us_history":0.6078431373,
        "MMLU_high_school_world_history":0.7088607595,
        "MMLU_human_aging":0.5784753363,
        "MMLU_human_sexuality":0.6259541985,
        "MMLU_international_law":0.7024793388,
        "MMLU_jurisprudence":0.6018518519,
        "MMLU_logical_fallacies":0.5521472393,
        "MMLU_machine_learning":0.4196428571,
        "MMLU_management":0.6699029126,
        "MMLU_marketing":0.7393162393,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.6653895275,
        "MMLU_moral_disputes":0.598265896,
        "MMLU_moral_scenarios":0.2916201117,
        "MMLU_nutrition":0.5163398693,
        "MMLU_philosophy":0.5466237942,
        "MMLU_prehistory":0.5740740741,
        "MMLU_professional_accounting":0.3794326241,
        "MMLU_professional_law":0.387874837,
        "MMLU_professional_medicine":0.4522058824,
        "MMLU_professional_psychology":0.5016339869,
        "MMLU_public_relations":0.5363636364,
        "MMLU_security_studies":0.5469387755,
        "MMLU_sociology":0.671641791,
        "MMLU_us_foreign_policy":0.74,
        "MMLU_virology":0.3915662651,
        "MMLU_world_religions":0.6608187135
    },
    {
        "Model":"minotaur-13b-fixed",
        "URL":"https:\/\/huggingface.co\/openaccess-ai-collective\/minotaur-13b-fixed",
        "full_model_name":"openaccess-ai-collective\/minotaur-13b-fixed",
        "Parameters":13.0,
        "MMLU_average":0.5009816741,
        "arc:challenge|25":0.5699658703,
        "hellaswag|10":0.61621191,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.4934210526,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.5094339623,
        "MMLU_college_biology":0.5416666667,
        "MMLU_college_chemistry":0.32,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.4219653179,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.57,
        "MMLU_conceptual_physics":0.3872340426,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4,
        "MMLU_elementary_mathematics":0.2486772487,
        "MMLU_formal_logic":0.3333333333,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.5741935484,
        "MMLU_high_school_chemistry":0.3251231527,
        "MMLU_high_school_computer_science":0.54,
        "MMLU_high_school_european_history":0.6181818182,
        "MMLU_high_school_geography":0.6363636364,
        "MMLU_high_school_government_and_politics":0.7150259067,
        "MMLU_high_school_macroeconomics":0.441025641,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.4621848739,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.671559633,
        "MMLU_high_school_statistics":0.2731481481,
        "MMLU_high_school_us_history":0.7058823529,
        "MMLU_high_school_world_history":0.7004219409,
        "MMLU_human_aging":0.5919282511,
        "MMLU_human_sexuality":0.5801526718,
        "MMLU_international_law":0.6446280992,
        "MMLU_jurisprudence":0.6018518519,
        "MMLU_logical_fallacies":0.5582822086,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.6893203883,
        "MMLU_marketing":0.7863247863,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.717752235,
        "MMLU_moral_disputes":0.5375722543,
        "MMLU_moral_scenarios":0.251396648,
        "MMLU_nutrition":0.5718954248,
        "MMLU_philosophy":0.5498392283,
        "MMLU_prehistory":0.5679012346,
        "MMLU_professional_accounting":0.3723404255,
        "MMLU_professional_law":0.3911342894,
        "MMLU_professional_medicine":0.4632352941,
        "MMLU_professional_psychology":0.5212418301,
        "MMLU_public_relations":0.5727272727,
        "MMLU_security_studies":0.6081632653,
        "MMLU_sociology":0.6865671642,
        "MMLU_us_foreign_policy":0.8,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.7426900585
    },
    {
        "Model":"vicuna-shishya-7b-ep3-v1",
        "URL":"https:\/\/huggingface.co\/luffycodes\/vicuna-shishya-7b-ep3-v1",
        "full_model_name":"luffycodes\/vicuna-shishya-7b-ep3-v1",
        "Parameters":7.0,
        "MMLU_average":0.5004332666,
        "arc:challenge|25":0.4394197952,
        "hellaswag|10":0.5751842262,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.4868421053,
        "MMLU_business_ethics":0.47,
        "MMLU_clinical_knowledge":0.5169811321,
        "MMLU_college_biology":0.4861111111,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.4739884393,
        "MMLU_college_physics":0.1666666667,
        "MMLU_computer_security":0.63,
        "MMLU_conceptual_physics":0.4553191489,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.4551724138,
        "MMLU_elementary_mathematics":0.3015873016,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.535483871,
        "MMLU_high_school_chemistry":0.3891625616,
        "MMLU_high_school_computer_science":0.47,
        "MMLU_high_school_european_history":0.6424242424,
        "MMLU_high_school_geography":0.5909090909,
        "MMLU_high_school_government_and_politics":0.7305699482,
        "MMLU_high_school_macroeconomics":0.4307692308,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.4495798319,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.6862385321,
        "MMLU_high_school_statistics":0.3611111111,
        "MMLU_high_school_us_history":0.681372549,
        "MMLU_high_school_world_history":0.7341772152,
        "MMLU_human_aging":0.6188340807,
        "MMLU_human_sexuality":0.641221374,
        "MMLU_international_law":0.6198347107,
        "MMLU_jurisprudence":0.5648148148,
        "MMLU_logical_fallacies":0.509202454,
        "MMLU_machine_learning":0.4196428571,
        "MMLU_management":0.6019417476,
        "MMLU_marketing":0.7393162393,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.69348659,
        "MMLU_moral_disputes":0.5375722543,
        "MMLU_moral_scenarios":0.2636871508,
        "MMLU_nutrition":0.568627451,
        "MMLU_philosophy":0.5980707395,
        "MMLU_prehistory":0.5679012346,
        "MMLU_professional_accounting":0.3581560284,
        "MMLU_professional_law":0.368970013,
        "MMLU_professional_medicine":0.4926470588,
        "MMLU_professional_psychology":0.4983660131,
        "MMLU_public_relations":0.5727272727,
        "MMLU_security_studies":0.6204081633,
        "MMLU_sociology":0.7114427861,
        "MMLU_us_foreign_policy":0.75,
        "MMLU_virology":0.4518072289,
        "MMLU_world_religions":0.7368421053
    },
    {
        "Model":"em_german_leo_mistral",
        "URL":"https:\/\/huggingface.co\/jphme\/em_german_leo_mistral",
        "full_model_name":"jphme\/em_german_leo_mistral",
        "Parameters":null,
        "MMLU_average":0.5003470566,
        "arc:challenge|25":0.4854948805,
        "hellaswag|10":0.5837482573,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4518518519,
        "MMLU_astronomy":0.4802631579,
        "MMLU_business_ethics":0.47,
        "MMLU_clinical_knowledge":0.558490566,
        "MMLU_college_biology":0.5486111111,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.5202312139,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.62,
        "MMLU_conceptual_physics":0.3744680851,
        "MMLU_econometrics":0.350877193,
        "MMLU_electrical_engineering":0.3655172414,
        "MMLU_elementary_mathematics":0.3571428571,
        "MMLU_formal_logic":0.373015873,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.5806451613,
        "MMLU_high_school_chemistry":0.3596059113,
        "MMLU_high_school_computer_science":0.5,
        "MMLU_high_school_european_history":0.6121212121,
        "MMLU_high_school_geography":0.6666666667,
        "MMLU_high_school_government_and_politics":0.7409326425,
        "MMLU_high_school_macroeconomics":0.4871794872,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.5336134454,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.6495412844,
        "MMLU_high_school_statistics":0.4861111111,
        "MMLU_high_school_us_history":0.6519607843,
        "MMLU_high_school_world_history":0.6455696203,
        "MMLU_human_aging":0.5739910314,
        "MMLU_human_sexuality":0.5190839695,
        "MMLU_international_law":0.7107438017,
        "MMLU_jurisprudence":0.5648148148,
        "MMLU_logical_fallacies":0.6196319018,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.6893203883,
        "MMLU_marketing":0.764957265,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.6704980843,
        "MMLU_moral_disputes":0.5115606936,
        "MMLU_moral_scenarios":0.305027933,
        "MMLU_nutrition":0.5032679739,
        "MMLU_philosophy":0.5819935691,
        "MMLU_prehistory":0.5833333333,
        "MMLU_professional_accounting":0.4007092199,
        "MMLU_professional_law":0.371577575,
        "MMLU_professional_medicine":0.5,
        "MMLU_professional_psychology":0.4673202614,
        "MMLU_public_relations":0.5545454545,
        "MMLU_security_studies":0.4163265306,
        "MMLU_sociology":0.6567164179,
        "MMLU_us_foreign_policy":0.68,
        "MMLU_virology":0.3674698795,
        "MMLU_world_religions":0.649122807
    },
    {
        "Model":"airoboros-13b",
        "URL":"https:\/\/huggingface.co\/jondurbin\/airoboros-13b",
        "full_model_name":"jondurbin\/airoboros-13b",
        "Parameters":13.0,
        "MMLU_average":0.5003081959,
        "arc:challenge|25":0.5554607509,
        "hellaswag|10":0.6165106552,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.5,
        "MMLU_business_ethics":0.47,
        "MMLU_clinical_knowledge":0.5509433962,
        "MMLU_college_biology":0.4652777778,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.4,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.450867052,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.64,
        "MMLU_conceptual_physics":0.4042553191,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.4275862069,
        "MMLU_elementary_mathematics":0.2962962963,
        "MMLU_formal_logic":0.3888888889,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.5580645161,
        "MMLU_high_school_chemistry":0.3399014778,
        "MMLU_high_school_computer_science":0.54,
        "MMLU_high_school_european_history":0.6424242424,
        "MMLU_high_school_geography":0.5909090909,
        "MMLU_high_school_government_and_politics":0.7046632124,
        "MMLU_high_school_macroeconomics":0.4871794872,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.5084033613,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.6880733945,
        "MMLU_high_school_statistics":0.3425925926,
        "MMLU_high_school_us_history":0.6568627451,
        "MMLU_high_school_world_history":0.6962025316,
        "MMLU_human_aging":0.5112107623,
        "MMLU_human_sexuality":0.5801526718,
        "MMLU_international_law":0.5867768595,
        "MMLU_jurisprudence":0.5277777778,
        "MMLU_logical_fallacies":0.5398773006,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.7606837607,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.6743295019,
        "MMLU_moral_disputes":0.5375722543,
        "MMLU_moral_scenarios":0.3374301676,
        "MMLU_nutrition":0.5522875817,
        "MMLU_philosophy":0.5627009646,
        "MMLU_prehistory":0.5462962963,
        "MMLU_professional_accounting":0.365248227,
        "MMLU_professional_law":0.389178618,
        "MMLU_professional_medicine":0.5220588235,
        "MMLU_professional_psychology":0.4918300654,
        "MMLU_public_relations":0.6,
        "MMLU_security_studies":0.5346938776,
        "MMLU_sociology":0.631840796,
        "MMLU_us_foreign_policy":0.77,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.7192982456
    },
    {
        "Model":"trurl-2-7b",
        "URL":"https:\/\/huggingface.co\/Voicelab\/trurl-2-7b",
        "full_model_name":"Voicelab\/trurl-2-7b",
        "Parameters":7.0,
        "MMLU_average":0.4999844133,
        "arc:challenge|25":0.502559727,
        "hellaswag|10":0.5682135033,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.5037037037,
        "MMLU_astronomy":0.4736842105,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.558490566,
        "MMLU_college_biology":0.5416666667,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.38,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4624277457,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.4085106383,
        "MMLU_econometrics":0.3333333333,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.2857142857,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.5903225806,
        "MMLU_high_school_chemistry":0.3990147783,
        "MMLU_high_school_computer_science":0.45,
        "MMLU_high_school_european_history":0.6484848485,
        "MMLU_high_school_geography":0.601010101,
        "MMLU_high_school_government_and_politics":0.7668393782,
        "MMLU_high_school_macroeconomics":0.5051282051,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.4915966387,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.6862385321,
        "MMLU_high_school_statistics":0.3796296296,
        "MMLU_high_school_us_history":0.6568627451,
        "MMLU_high_school_world_history":0.6877637131,
        "MMLU_human_aging":0.5605381166,
        "MMLU_human_sexuality":0.5648854962,
        "MMLU_international_law":0.6115702479,
        "MMLU_jurisprudence":0.6481481481,
        "MMLU_logical_fallacies":0.5582822086,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.6699029126,
        "MMLU_marketing":0.7136752137,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.6756066411,
        "MMLU_moral_disputes":0.5317919075,
        "MMLU_moral_scenarios":0.2368715084,
        "MMLU_nutrition":0.5261437908,
        "MMLU_philosophy":0.5852090032,
        "MMLU_prehistory":0.5339506173,
        "MMLU_professional_accounting":0.3475177305,
        "MMLU_professional_law":0.3644067797,
        "MMLU_professional_medicine":0.4558823529,
        "MMLU_professional_psychology":0.4722222222,
        "MMLU_public_relations":0.5090909091,
        "MMLU_security_studies":0.6244897959,
        "MMLU_sociology":0.671641791,
        "MMLU_us_foreign_policy":0.7,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.7134502924
    },
    {
        "Model":"VA",
        "URL":"https:\/\/huggingface.co\/AA051610\/VA",
        "full_model_name":"AA051610\/VA",
        "Parameters":null,
        "MMLU_average":0.4996222822,
        "arc:challenge|25":0.3848122867,
        "hellaswag|10":0.4739095798,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4444444444,
        "MMLU_astronomy":0.4539473684,
        "MMLU_business_ethics":0.47,
        "MMLU_clinical_knowledge":0.5547169811,
        "MMLU_college_biology":0.5069444444,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.5144508671,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.63,
        "MMLU_conceptual_physics":0.4553191489,
        "MMLU_econometrics":0.3421052632,
        "MMLU_electrical_engineering":0.5103448276,
        "MMLU_elementary_mathematics":0.328042328,
        "MMLU_formal_logic":0.2698412698,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.5677419355,
        "MMLU_high_school_chemistry":0.3596059113,
        "MMLU_high_school_computer_science":0.5,
        "MMLU_high_school_european_history":0.6121212121,
        "MMLU_high_school_geography":0.5858585859,
        "MMLU_high_school_government_and_politics":0.6269430052,
        "MMLU_high_school_macroeconomics":0.4487179487,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.525210084,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.6128440367,
        "MMLU_high_school_statistics":0.3981481481,
        "MMLU_high_school_us_history":0.637254902,
        "MMLU_high_school_world_history":0.7004219409,
        "MMLU_human_aging":0.6188340807,
        "MMLU_human_sexuality":0.5648854962,
        "MMLU_international_law":0.6694214876,
        "MMLU_jurisprudence":0.6574074074,
        "MMLU_logical_fallacies":0.5153374233,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.6504854369,
        "MMLU_marketing":0.7222222222,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.6781609195,
        "MMLU_moral_disputes":0.5751445087,
        "MMLU_moral_scenarios":0.2525139665,
        "MMLU_nutrition":0.4934640523,
        "MMLU_philosophy":0.5594855305,
        "MMLU_prehistory":0.537037037,
        "MMLU_professional_accounting":0.4361702128,
        "MMLU_professional_law":0.4419817471,
        "MMLU_professional_medicine":0.4852941176,
        "MMLU_professional_psychology":0.5343137255,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.5673469388,
        "MMLU_sociology":0.6467661692,
        "MMLU_us_foreign_policy":0.72,
        "MMLU_virology":0.5180722892,
        "MMLU_world_religions":0.6081871345
    },
    {
        "Model":"13B-Chimera",
        "URL":"https:\/\/huggingface.co\/digitous\/13B-Chimera",
        "full_model_name":"digitous\/13B-Chimera",
        "Parameters":13.0,
        "MMLU_average":0.4986226747,
        "arc:challenge|25":0.5614334471,
        "hellaswag|10":0.6163114917,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.4605263158,
        "MMLU_business_ethics":0.49,
        "MMLU_clinical_knowledge":0.5094339623,
        "MMLU_college_biology":0.5208333333,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.4219653179,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.62,
        "MMLU_conceptual_physics":0.4170212766,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.4344827586,
        "MMLU_elementary_mathematics":0.2857142857,
        "MMLU_formal_logic":0.380952381,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.5548387097,
        "MMLU_high_school_chemistry":0.3201970443,
        "MMLU_high_school_computer_science":0.48,
        "MMLU_high_school_european_history":0.6363636364,
        "MMLU_high_school_geography":0.6161616162,
        "MMLU_high_school_government_and_politics":0.689119171,
        "MMLU_high_school_macroeconomics":0.4794871795,
        "MMLU_high_school_mathematics":0.2222222222,
        "MMLU_high_school_microeconomics":0.4537815126,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.6899082569,
        "MMLU_high_school_statistics":0.3101851852,
        "MMLU_high_school_us_history":0.6715686275,
        "MMLU_high_school_world_history":0.6962025316,
        "MMLU_human_aging":0.5874439462,
        "MMLU_human_sexuality":0.5801526718,
        "MMLU_international_law":0.6611570248,
        "MMLU_jurisprudence":0.5833333333,
        "MMLU_logical_fallacies":0.527607362,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.7606837607,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.7062579821,
        "MMLU_moral_disputes":0.5317919075,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.545751634,
        "MMLU_philosophy":0.5305466238,
        "MMLU_prehistory":0.5432098765,
        "MMLU_professional_accounting":0.3829787234,
        "MMLU_professional_law":0.4015645372,
        "MMLU_professional_medicine":0.4669117647,
        "MMLU_professional_psychology":0.522875817,
        "MMLU_public_relations":0.5545454545,
        "MMLU_security_studies":0.5387755102,
        "MMLU_sociology":0.6815920398,
        "MMLU_us_foreign_policy":0.8,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.7543859649
    },
    {
        "Model":"orca_mini_v2_13b",
        "URL":"https:\/\/huggingface.co\/psmathur\/orca_mini_v2_13b",
        "full_model_name":"psmathur\/orca_mini_v2_13b",
        "Parameters":13.0,
        "MMLU_average":0.4983474041,
        "arc:challenge|25":0.5375426621,
        "hellaswag|10":0.6080462059,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.5,
        "MMLU_business_ethics":0.48,
        "MMLU_clinical_knowledge":0.5471698113,
        "MMLU_college_biology":0.5138888889,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.4161849711,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.63,
        "MMLU_conceptual_physics":0.3914893617,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.2989417989,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.4,
        "MMLU_high_school_biology":0.5483870968,
        "MMLU_high_school_chemistry":0.354679803,
        "MMLU_high_school_computer_science":0.54,
        "MMLU_high_school_european_history":0.6121212121,
        "MMLU_high_school_geography":0.601010101,
        "MMLU_high_school_government_and_politics":0.6580310881,
        "MMLU_high_school_macroeconomics":0.4333333333,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.4159663866,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.6458715596,
        "MMLU_high_school_statistics":0.3194444444,
        "MMLU_high_school_us_history":0.6764705882,
        "MMLU_high_school_world_history":0.7004219409,
        "MMLU_human_aging":0.5739910314,
        "MMLU_human_sexuality":0.6259541985,
        "MMLU_international_law":0.6694214876,
        "MMLU_jurisprudence":0.5462962963,
        "MMLU_logical_fallacies":0.5521472393,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.7184466019,
        "MMLU_marketing":0.7692307692,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.7075351213,
        "MMLU_moral_disputes":0.5404624277,
        "MMLU_moral_scenarios":0.2916201117,
        "MMLU_nutrition":0.5653594771,
        "MMLU_philosophy":0.5273311897,
        "MMLU_prehistory":0.5555555556,
        "MMLU_professional_accounting":0.3510638298,
        "MMLU_professional_law":0.408083442,
        "MMLU_professional_medicine":0.5220588235,
        "MMLU_professional_psychology":0.4901960784,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.5673469388,
        "MMLU_sociology":0.6766169154,
        "MMLU_us_foreign_policy":0.73,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.730994152
    },
    {
        "Model":"Platypus2-7B",
        "URL":"https:\/\/huggingface.co\/garage-bAInd\/Platypus2-7B",
        "full_model_name":"garage-bAInd\/Platypus2-7B",
        "Parameters":7.0,
        "MMLU_average":0.4982679408,
        "arc:challenge|25":0.5102389078,
        "hellaswag|10":0.5895239992,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.4802631579,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.4679245283,
        "MMLU_college_biology":0.5138888889,
        "MMLU_college_chemistry":0.26,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.3757225434,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.6,
        "MMLU_conceptual_physics":0.4468085106,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.4,
        "MMLU_elementary_mathematics":0.3174603175,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.5580645161,
        "MMLU_high_school_chemistry":0.4187192118,
        "MMLU_high_school_computer_science":0.47,
        "MMLU_high_school_european_history":0.6909090909,
        "MMLU_high_school_geography":0.601010101,
        "MMLU_high_school_government_and_politics":0.7357512953,
        "MMLU_high_school_macroeconomics":0.4435897436,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.5042016807,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.6880733945,
        "MMLU_high_school_statistics":0.3425925926,
        "MMLU_high_school_us_history":0.6568627451,
        "MMLU_high_school_world_history":0.7552742616,
        "MMLU_human_aging":0.5919282511,
        "MMLU_human_sexuality":0.5419847328,
        "MMLU_international_law":0.694214876,
        "MMLU_jurisprudence":0.6203703704,
        "MMLU_logical_fallacies":0.5950920245,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.6699029126,
        "MMLU_marketing":0.764957265,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.6832694764,
        "MMLU_moral_disputes":0.5375722543,
        "MMLU_moral_scenarios":0.3072625698,
        "MMLU_nutrition":0.5196078431,
        "MMLU_philosophy":0.6045016077,
        "MMLU_prehistory":0.5895061728,
        "MMLU_professional_accounting":0.3865248227,
        "MMLU_professional_law":0.4315514993,
        "MMLU_professional_medicine":0.4558823529,
        "MMLU_professional_psychology":0.522875817,
        "MMLU_public_relations":0.6,
        "MMLU_security_studies":0.5224489796,
        "MMLU_sociology":0.631840796,
        "MMLU_us_foreign_policy":0.7,
        "MMLU_virology":0.3915662651,
        "MMLU_world_religions":0.7134502924
    },
    {
        "Model":"Huginn-22b-Prototype",
        "URL":"https:\/\/huggingface.co\/The-Face-Of-Goonery\/Huginn-22b-Prototype",
        "full_model_name":"The-Face-Of-Goonery\/Huginn-22b-Prototype",
        "Parameters":22.0,
        "MMLU_average":0.4981332677,
        "arc:challenge|25":0.5290102389,
        "hellaswag|10":0.6078470424,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.4934210526,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.5471698113,
        "MMLU_college_biology":0.5138888889,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.38,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.4682080925,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.6,
        "MMLU_conceptual_physics":0.4,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.2962962963,
        "MMLU_formal_logic":0.373015873,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.6064516129,
        "MMLU_high_school_chemistry":0.39408867,
        "MMLU_high_school_computer_science":0.51,
        "MMLU_high_school_european_history":0.6242424242,
        "MMLU_high_school_geography":0.6565656566,
        "MMLU_high_school_government_and_politics":0.7357512953,
        "MMLU_high_school_macroeconomics":0.5,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.4957983193,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.6642201835,
        "MMLU_high_school_statistics":0.3657407407,
        "MMLU_high_school_us_history":0.6470588235,
        "MMLU_high_school_world_history":0.6497890295,
        "MMLU_human_aging":0.5874439462,
        "MMLU_human_sexuality":0.572519084,
        "MMLU_international_law":0.6115702479,
        "MMLU_jurisprudence":0.5925925926,
        "MMLU_logical_fallacies":0.6257668712,
        "MMLU_machine_learning":0.2946428571,
        "MMLU_management":0.6699029126,
        "MMLU_marketing":0.7094017094,
        "MMLU_medical_genetics":0.47,
        "MMLU_miscellaneous":0.6692209451,
        "MMLU_moral_disputes":0.5606936416,
        "MMLU_moral_scenarios":0.2290502793,
        "MMLU_nutrition":0.568627451,
        "MMLU_philosophy":0.5819935691,
        "MMLU_prehistory":0.5277777778,
        "MMLU_professional_accounting":0.3687943262,
        "MMLU_professional_law":0.3591916558,
        "MMLU_professional_medicine":0.4338235294,
        "MMLU_professional_psychology":0.4640522876,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.5714285714,
        "MMLU_sociology":0.7014925373,
        "MMLU_us_foreign_policy":0.69,
        "MMLU_virology":0.3855421687,
        "MMLU_world_religions":0.6608187135
    },
    {
        "Model":"CAMEL-13B-Combined-Data",
        "URL":"https:\/\/huggingface.co\/camel-ai\/CAMEL-13B-Combined-Data",
        "full_model_name":"camel-ai\/CAMEL-13B-Combined-Data",
        "Parameters":13.0,
        "MMLU_average":0.4973619609,
        "arc:challenge|25":0.5145051195,
        "hellaswag|10":0.5948018323,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.4518518519,
        "MMLU_astronomy":0.4934210526,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.5132075472,
        "MMLU_college_biology":0.5138888889,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.4,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4797687861,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.64,
        "MMLU_conceptual_physics":0.3574468085,
        "MMLU_econometrics":0.3421052632,
        "MMLU_electrical_engineering":0.4413793103,
        "MMLU_elementary_mathematics":0.2724867725,
        "MMLU_formal_logic":0.3015873016,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.5548387097,
        "MMLU_high_school_chemistry":0.3349753695,
        "MMLU_high_school_computer_science":0.52,
        "MMLU_high_school_european_history":0.6121212121,
        "MMLU_high_school_geography":0.6565656566,
        "MMLU_high_school_government_and_politics":0.6632124352,
        "MMLU_high_school_macroeconomics":0.4769230769,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.4957983193,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.6825688073,
        "MMLU_high_school_statistics":0.4212962963,
        "MMLU_high_school_us_history":0.6323529412,
        "MMLU_high_school_world_history":0.6708860759,
        "MMLU_human_aging":0.5067264574,
        "MMLU_human_sexuality":0.6335877863,
        "MMLU_international_law":0.6776859504,
        "MMLU_jurisprudence":0.5648148148,
        "MMLU_logical_fallacies":0.5644171779,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.6699029126,
        "MMLU_marketing":0.7606837607,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.6615581098,
        "MMLU_moral_disputes":0.5173410405,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.5718954248,
        "MMLU_philosophy":0.5498392283,
        "MMLU_prehistory":0.5185185185,
        "MMLU_professional_accounting":0.329787234,
        "MMLU_professional_law":0.3956975228,
        "MMLU_professional_medicine":0.5257352941,
        "MMLU_professional_psychology":0.4526143791,
        "MMLU_public_relations":0.5272727273,
        "MMLU_security_studies":0.5714285714,
        "MMLU_sociology":0.6815920398,
        "MMLU_us_foreign_policy":0.77,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.7192982456
    },
    {
        "Model":"LosslessMegaCoder-llama2-7b-mini",
        "URL":"https:\/\/huggingface.co\/rombodawg\/LosslessMegaCoder-llama2-7b-mini",
        "full_model_name":"rombodawg\/LosslessMegaCoder-llama2-7b-mini",
        "Parameters":7.0,
        "MMLU_average":0.4971575134,
        "arc:challenge|25":0.5068259386,
        "hellaswag|10":0.5881298546,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.4407894737,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.5698113208,
        "MMLU_college_biology":0.4861111111,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.4161849711,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.4170212766,
        "MMLU_econometrics":0.3333333333,
        "MMLU_electrical_engineering":0.4827586207,
        "MMLU_elementary_mathematics":0.3201058201,
        "MMLU_formal_logic":0.3015873016,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.5451612903,
        "MMLU_high_school_chemistry":0.354679803,
        "MMLU_high_school_computer_science":0.48,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.601010101,
        "MMLU_high_school_government_and_politics":0.7098445596,
        "MMLU_high_school_macroeconomics":0.4205128205,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.4705882353,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.6623853211,
        "MMLU_high_school_statistics":0.3518518519,
        "MMLU_high_school_us_history":0.6715686275,
        "MMLU_high_school_world_history":0.7426160338,
        "MMLU_human_aging":0.5605381166,
        "MMLU_human_sexuality":0.5801526718,
        "MMLU_international_law":0.6446280992,
        "MMLU_jurisprudence":0.5277777778,
        "MMLU_logical_fallacies":0.5214723926,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.6504854369,
        "MMLU_marketing":0.7264957265,
        "MMLU_medical_genetics":0.53,
        "MMLU_miscellaneous":0.6973180077,
        "MMLU_moral_disputes":0.5260115607,
        "MMLU_moral_scenarios":0.2547486034,
        "MMLU_nutrition":0.545751634,
        "MMLU_philosophy":0.6045016077,
        "MMLU_prehistory":0.5679012346,
        "MMLU_professional_accounting":0.390070922,
        "MMLU_professional_law":0.3624511082,
        "MMLU_professional_medicine":0.4044117647,
        "MMLU_professional_psychology":0.5016339869,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.6653061224,
        "MMLU_sociology":0.6965174129,
        "MMLU_us_foreign_policy":0.73,
        "MMLU_virology":0.4096385542,
        "MMLU_world_religions":0.6959064327
    },
    {
        "Model":"vicuna-7b-v1.5-PL-lora_unload",
        "URL":"https:\/\/huggingface.co\/Lajonbot\/vicuna-7b-v1.5-PL-lora_unload",
        "full_model_name":"Lajonbot\/vicuna-7b-v1.5-PL-lora_unload",
        "Parameters":7.0,
        "MMLU_average":0.4969457028,
        "arc:challenge|25":0.4982935154,
        "hellaswag|10":0.5771758614,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.5,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.5169811321,
        "MMLU_college_biology":0.4930555556,
        "MMLU_college_chemistry":0.33,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.4393063584,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.4510638298,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.3068783069,
        "MMLU_formal_logic":0.3412698413,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.5322580645,
        "MMLU_high_school_chemistry":0.354679803,
        "MMLU_high_school_computer_science":0.45,
        "MMLU_high_school_european_history":0.6424242424,
        "MMLU_high_school_geography":0.601010101,
        "MMLU_high_school_government_and_politics":0.7202072539,
        "MMLU_high_school_macroeconomics":0.4333333333,
        "MMLU_high_school_mathematics":0.2259259259,
        "MMLU_high_school_microeconomics":0.4369747899,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.6862385321,
        "MMLU_high_school_statistics":0.3842592593,
        "MMLU_high_school_us_history":0.681372549,
        "MMLU_high_school_world_history":0.6962025316,
        "MMLU_human_aging":0.600896861,
        "MMLU_human_sexuality":0.5801526718,
        "MMLU_international_law":0.5619834711,
        "MMLU_jurisprudence":0.5648148148,
        "MMLU_logical_fallacies":0.5214723926,
        "MMLU_machine_learning":0.4196428571,
        "MMLU_management":0.6116504854,
        "MMLU_marketing":0.7735042735,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.6845466156,
        "MMLU_moral_disputes":0.5173410405,
        "MMLU_moral_scenarios":0.2659217877,
        "MMLU_nutrition":0.522875817,
        "MMLU_philosophy":0.575562701,
        "MMLU_prehistory":0.5617283951,
        "MMLU_professional_accounting":0.3758865248,
        "MMLU_professional_law":0.368970013,
        "MMLU_professional_medicine":0.5147058824,
        "MMLU_professional_psychology":0.4836601307,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.6,
        "MMLU_sociology":0.6567164179,
        "MMLU_us_foreign_policy":0.73,
        "MMLU_virology":0.4397590361,
        "MMLU_world_religions":0.6783625731
    },
    {
        "Model":"EverythingLM-13b-V2-16k",
        "URL":"https:\/\/huggingface.co\/totally-not-an-llm\/EverythingLM-13b-V2-16k",
        "full_model_name":"totally-not-an-llm\/EverythingLM-13b-V2-16k",
        "Parameters":13.0,
        "MMLU_average":0.4968666134,
        "arc:challenge|25":0.5511945392,
        "hellaswag|10":0.609241187,
        "MMLU_abstract_algebra":0.39,
        "MMLU_anatomy":0.4296296296,
        "MMLU_astronomy":0.5,
        "MMLU_business_ethics":0.5,
        "MMLU_clinical_knowledge":0.5018867925,
        "MMLU_college_biology":0.5069444444,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.4335260116,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.67,
        "MMLU_conceptual_physics":0.3617021277,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.3624338624,
        "MMLU_formal_logic":0.3412698413,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.6096774194,
        "MMLU_high_school_chemistry":0.3891625616,
        "MMLU_high_school_computer_science":0.47,
        "MMLU_high_school_european_history":0.6181818182,
        "MMLU_high_school_geography":0.6313131313,
        "MMLU_high_school_government_and_politics":0.6943005181,
        "MMLU_high_school_macroeconomics":0.4538461538,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.4957983193,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.6697247706,
        "MMLU_high_school_statistics":0.4259259259,
        "MMLU_high_school_us_history":0.6421568627,
        "MMLU_high_school_world_history":0.6160337553,
        "MMLU_human_aging":0.6053811659,
        "MMLU_human_sexuality":0.534351145,
        "MMLU_international_law":0.6446280992,
        "MMLU_jurisprudence":0.5925925926,
        "MMLU_logical_fallacies":0.5521472393,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.640776699,
        "MMLU_marketing":0.7094017094,
        "MMLU_medical_genetics":0.53,
        "MMLU_miscellaneous":0.6819923372,
        "MMLU_moral_disputes":0.5895953757,
        "MMLU_moral_scenarios":0.2223463687,
        "MMLU_nutrition":0.522875817,
        "MMLU_philosophy":0.5723472669,
        "MMLU_prehistory":0.5679012346,
        "MMLU_professional_accounting":0.3936170213,
        "MMLU_professional_law":0.3487614081,
        "MMLU_professional_medicine":0.4742647059,
        "MMLU_professional_psychology":0.4836601307,
        "MMLU_public_relations":0.5363636364,
        "MMLU_security_studies":0.5020408163,
        "MMLU_sociology":0.6467661692,
        "MMLU_us_foreign_policy":0.73,
        "MMLU_virology":0.3915662651,
        "MMLU_world_religions":0.7134502924
    },
    {
        "Model":"llama_chat-tv_en_luban-tv_stable_platypus2",
        "URL":"https:\/\/huggingface.co\/aqweteddy\/llama_chat-tv_en_luban-tv_stable_platypus2",
        "full_model_name":"aqweteddy\/llama_chat-tv_en_luban-tv_stable_platypus2",
        "Parameters":null,
        "MMLU_average":0.49604119,
        "arc:challenge|25":0.4351535836,
        "hellaswag|10":0.466042621,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.4736842105,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.5320754717,
        "MMLU_college_biology":0.5277777778,
        "MMLU_college_chemistry":0.33,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.4797687861,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.67,
        "MMLU_conceptual_physics":0.4255319149,
        "MMLU_econometrics":0.3333333333,
        "MMLU_electrical_engineering":0.4206896552,
        "MMLU_elementary_mathematics":0.2857142857,
        "MMLU_formal_logic":0.3412698413,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.535483871,
        "MMLU_high_school_chemistry":0.3842364532,
        "MMLU_high_school_computer_science":0.56,
        "MMLU_high_school_european_history":0.6363636364,
        "MMLU_high_school_geography":0.6515151515,
        "MMLU_high_school_government_and_politics":0.7461139896,
        "MMLU_high_school_macroeconomics":0.5153846154,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.4705882353,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.6495412844,
        "MMLU_high_school_statistics":0.287037037,
        "MMLU_high_school_us_history":0.5784313725,
        "MMLU_high_school_world_history":0.6582278481,
        "MMLU_human_aging":0.5739910314,
        "MMLU_human_sexuality":0.5572519084,
        "MMLU_international_law":0.6033057851,
        "MMLU_jurisprudence":0.5648148148,
        "MMLU_logical_fallacies":0.5705521472,
        "MMLU_machine_learning":0.4553571429,
        "MMLU_management":0.6699029126,
        "MMLU_marketing":0.7863247863,
        "MMLU_medical_genetics":0.53,
        "MMLU_miscellaneous":0.6730523627,
        "MMLU_moral_disputes":0.5144508671,
        "MMLU_moral_scenarios":0.4156424581,
        "MMLU_nutrition":0.5163398693,
        "MMLU_philosophy":0.5209003215,
        "MMLU_prehistory":0.5185185185,
        "MMLU_professional_accounting":0.3865248227,
        "MMLU_professional_law":0.3728813559,
        "MMLU_professional_medicine":0.4044117647,
        "MMLU_professional_psychology":0.4558823529,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.5306122449,
        "MMLU_sociology":0.5721393035,
        "MMLU_us_foreign_policy":0.65,
        "MMLU_virology":0.3975903614,
        "MMLU_world_religions":0.701754386
    },
    {
        "Model":"WizardLM-13B-V1.1-GPTQ",
        "URL":"https:\/\/huggingface.co\/TheBloke\/WizardLM-13B-V1.1-GPTQ",
        "full_model_name":"TheBloke\/WizardLM-13B-V1.1-GPTQ",
        "Parameters":13.0,
        "MMLU_average":0.4959229889,
        "arc:challenge|25":0.5648464164,
        "hellaswag|10":0.6042620992,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.4934210526,
        "MMLU_business_ethics":0.47,
        "MMLU_clinical_knowledge":0.5358490566,
        "MMLU_college_biology":0.4236111111,
        "MMLU_college_chemistry":0.45,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.4450867052,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.3829787234,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.4344827586,
        "MMLU_elementary_mathematics":0.2989417989,
        "MMLU_formal_logic":0.4047619048,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.5709677419,
        "MMLU_high_school_chemistry":0.3448275862,
        "MMLU_high_school_computer_science":0.52,
        "MMLU_high_school_european_history":0.6242424242,
        "MMLU_high_school_geography":0.5909090909,
        "MMLU_high_school_government_and_politics":0.6787564767,
        "MMLU_high_school_macroeconomics":0.4512820513,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.5084033613,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.6623853211,
        "MMLU_high_school_statistics":0.4305555556,
        "MMLU_high_school_us_history":0.6421568627,
        "MMLU_high_school_world_history":0.6582278481,
        "MMLU_human_aging":0.4798206278,
        "MMLU_human_sexuality":0.572519084,
        "MMLU_international_law":0.6115702479,
        "MMLU_jurisprudence":0.537037037,
        "MMLU_logical_fallacies":0.5030674847,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.6504854369,
        "MMLU_marketing":0.7179487179,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.6615581098,
        "MMLU_moral_disputes":0.5606936416,
        "MMLU_moral_scenarios":0.294972067,
        "MMLU_nutrition":0.6045751634,
        "MMLU_philosophy":0.536977492,
        "MMLU_prehistory":0.5277777778,
        "MMLU_professional_accounting":0.3936170213,
        "MMLU_professional_law":0.4035202086,
        "MMLU_professional_medicine":0.4889705882,
        "MMLU_professional_psychology":0.4787581699,
        "MMLU_public_relations":0.5636363636,
        "MMLU_security_studies":0.587755102,
        "MMLU_sociology":0.6567164179,
        "MMLU_us_foreign_policy":0.74,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.7251461988
    },
    {
        "Model":"vicuna-7b-v1.5-lora-mixed-datasets-time-unit",
        "URL":"https:\/\/huggingface.co\/Charlie911\/vicuna-7b-v1.5-lora-mixed-datasets-time-unit",
        "full_model_name":"Charlie911\/vicuna-7b-v1.5-lora-mixed-datasets-time-unit",
        "Parameters":7.0,
        "MMLU_average":0.4958463568,
        "arc:challenge|25":0.4820819113,
        "hellaswag|10":0.5653256323,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4518518519,
        "MMLU_astronomy":0.4736842105,
        "MMLU_business_ethics":0.5,
        "MMLU_clinical_knowledge":0.5320754717,
        "MMLU_college_biology":0.4930555556,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.4,
        "MMLU_college_mathematics":0.47,
        "MMLU_college_medicine":0.4277456647,
        "MMLU_college_physics":0.1764705882,
        "MMLU_computer_security":0.64,
        "MMLU_conceptual_physics":0.4510638298,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.3148148148,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.28,
        "MMLU_high_school_biology":0.5548387097,
        "MMLU_high_school_chemistry":0.3399014778,
        "MMLU_high_school_computer_science":0.44,
        "MMLU_high_school_european_history":0.6121212121,
        "MMLU_high_school_geography":0.6161616162,
        "MMLU_high_school_government_and_politics":0.6943005181,
        "MMLU_high_school_macroeconomics":0.4487179487,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.4789915966,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.6899082569,
        "MMLU_high_school_statistics":0.4074074074,
        "MMLU_high_school_us_history":0.6519607843,
        "MMLU_high_school_world_history":0.6962025316,
        "MMLU_human_aging":0.5829596413,
        "MMLU_human_sexuality":0.6030534351,
        "MMLU_international_law":0.5785123967,
        "MMLU_jurisprudence":0.5462962963,
        "MMLU_logical_fallacies":0.509202454,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.6699029126,
        "MMLU_marketing":0.7521367521,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.6794380587,
        "MMLU_moral_disputes":0.5086705202,
        "MMLU_moral_scenarios":0.2636871508,
        "MMLU_nutrition":0.5784313725,
        "MMLU_philosophy":0.5884244373,
        "MMLU_prehistory":0.5339506173,
        "MMLU_professional_accounting":0.390070922,
        "MMLU_professional_law":0.3637548892,
        "MMLU_professional_medicine":0.5514705882,
        "MMLU_professional_psychology":0.4673202614,
        "MMLU_public_relations":0.5818181818,
        "MMLU_security_studies":0.6204081633,
        "MMLU_sociology":0.6616915423,
        "MMLU_us_foreign_policy":0.69,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.6959064327
    },
    {
        "Model":"GOAT-7B-Community",
        "URL":"https:\/\/huggingface.co\/GOAT-AI\/GOAT-7B-Community",
        "full_model_name":"GOAT-AI\/GOAT-7B-Community",
        "Parameters":7.0,
        "MMLU_average":0.4958332319,
        "arc:challenge|25":0.4616040956,
        "hellaswag|10":0.5577574188,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.437037037,
        "MMLU_astronomy":0.4605263158,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.5735849057,
        "MMLU_college_biology":0.4722222222,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.39,
        "MMLU_college_medicine":0.4624277457,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.63,
        "MMLU_conceptual_physics":0.4,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.3306878307,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.5096774194,
        "MMLU_high_school_chemistry":0.3448275862,
        "MMLU_high_school_computer_science":0.55,
        "MMLU_high_school_european_history":0.6848484848,
        "MMLU_high_school_geography":0.6565656566,
        "MMLU_high_school_government_and_politics":0.6632124352,
        "MMLU_high_school_macroeconomics":0.4871794872,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.487394958,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.6862385321,
        "MMLU_high_school_statistics":0.3472222222,
        "MMLU_high_school_us_history":0.6715686275,
        "MMLU_high_school_world_history":0.6919831224,
        "MMLU_human_aging":0.5784753363,
        "MMLU_human_sexuality":0.4961832061,
        "MMLU_international_law":0.5950413223,
        "MMLU_jurisprudence":0.537037037,
        "MMLU_logical_fallacies":0.527607362,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.7735042735,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.6947637292,
        "MMLU_moral_disputes":0.549132948,
        "MMLU_moral_scenarios":0.2983240223,
        "MMLU_nutrition":0.545751634,
        "MMLU_philosophy":0.5948553055,
        "MMLU_prehistory":0.5030864198,
        "MMLU_professional_accounting":0.3510638298,
        "MMLU_professional_law":0.3709256845,
        "MMLU_professional_medicine":0.4742647059,
        "MMLU_professional_psychology":0.4950980392,
        "MMLU_public_relations":0.5636363636,
        "MMLU_security_studies":0.5265306122,
        "MMLU_sociology":0.6666666667,
        "MMLU_us_foreign_policy":0.65,
        "MMLU_virology":0.3734939759,
        "MMLU_world_religions":0.6842105263
    },
    {
        "Model":"airoboros-13b-gpt4-1.1",
        "URL":"https:\/\/huggingface.co\/jondurbin\/airoboros-13b-gpt4-1.1",
        "full_model_name":"jondurbin\/airoboros-13b-gpt4-1.1",
        "Parameters":13.0,
        "MMLU_average":0.4941229079,
        "arc:challenge|25":0.5682593857,
        "hellaswag|10":0.6384186417,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.4605263158,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.5056603774,
        "MMLU_college_biology":0.4861111111,
        "MMLU_college_chemistry":0.36,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.4566473988,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.65,
        "MMLU_conceptual_physics":0.4127659574,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.4,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.5677419355,
        "MMLU_high_school_chemistry":0.3497536946,
        "MMLU_high_school_computer_science":0.49,
        "MMLU_high_school_european_history":0.6363636364,
        "MMLU_high_school_geography":0.5909090909,
        "MMLU_high_school_government_and_politics":0.6632124352,
        "MMLU_high_school_macroeconomics":0.4717948718,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.4789915966,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.647706422,
        "MMLU_high_school_statistics":0.3101851852,
        "MMLU_high_school_us_history":0.6519607843,
        "MMLU_high_school_world_history":0.7088607595,
        "MMLU_human_aging":0.5650224215,
        "MMLU_human_sexuality":0.5877862595,
        "MMLU_international_law":0.6859504132,
        "MMLU_jurisprudence":0.5555555556,
        "MMLU_logical_fallacies":0.5644171779,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.6990291262,
        "MMLU_marketing":0.7478632479,
        "MMLU_medical_genetics":0.53,
        "MMLU_miscellaneous":0.6794380587,
        "MMLU_moral_disputes":0.5433526012,
        "MMLU_moral_scenarios":0.2525139665,
        "MMLU_nutrition":0.5588235294,
        "MMLU_philosophy":0.5305466238,
        "MMLU_prehistory":0.5401234568,
        "MMLU_professional_accounting":0.3368794326,
        "MMLU_professional_law":0.3950456323,
        "MMLU_professional_medicine":0.5294117647,
        "MMLU_professional_psychology":0.5098039216,
        "MMLU_public_relations":0.5363636364,
        "MMLU_security_studies":0.5469387755,
        "MMLU_sociology":0.6517412935,
        "MMLU_us_foreign_policy":0.81,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.6842105263
    },
    {
        "Model":"MythoLogic-13b",
        "URL":"https:\/\/huggingface.co\/Gryphe\/MythoLogic-13b",
        "full_model_name":"Gryphe\/MythoLogic-13b",
        "Parameters":13.0,
        "MMLU_average":0.4936385173,
        "arc:challenge|25":0.5597269625,
        "hellaswag|10":0.6184027086,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4518518519,
        "MMLU_astronomy":0.4473684211,
        "MMLU_business_ethics":0.5,
        "MMLU_clinical_knowledge":0.4981132075,
        "MMLU_college_biology":0.4930555556,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.36,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4335260116,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.62,
        "MMLU_conceptual_physics":0.4170212766,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.4275862069,
        "MMLU_elementary_mathematics":0.2698412698,
        "MMLU_formal_logic":0.4047619048,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.5612903226,
        "MMLU_high_school_chemistry":0.3645320197,
        "MMLU_high_school_computer_science":0.49,
        "MMLU_high_school_european_history":0.6424242424,
        "MMLU_high_school_geography":0.6111111111,
        "MMLU_high_school_government_and_politics":0.6787564767,
        "MMLU_high_school_macroeconomics":0.4692307692,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.487394958,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.6605504587,
        "MMLU_high_school_statistics":0.2824074074,
        "MMLU_high_school_us_history":0.637254902,
        "MMLU_high_school_world_history":0.6793248945,
        "MMLU_human_aging":0.5381165919,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.6198347107,
        "MMLU_jurisprudence":0.5555555556,
        "MMLU_logical_fallacies":0.5644171779,
        "MMLU_machine_learning":0.4821428571,
        "MMLU_management":0.6601941748,
        "MMLU_marketing":0.7564102564,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.6832694764,
        "MMLU_moral_disputes":0.5173410405,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.5392156863,
        "MMLU_philosophy":0.5305466238,
        "MMLU_prehistory":0.5462962963,
        "MMLU_professional_accounting":0.3687943262,
        "MMLU_professional_law":0.3911342894,
        "MMLU_professional_medicine":0.4963235294,
        "MMLU_professional_psychology":0.4918300654,
        "MMLU_public_relations":0.5636363636,
        "MMLU_security_studies":0.5469387755,
        "MMLU_sociology":0.6865671642,
        "MMLU_us_foreign_policy":0.8,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.701754386
    },
    {
        "Model":"GPT-JT-6B-v0",
        "URL":"https:\/\/huggingface.co\/togethercomputer\/GPT-JT-6B-v0",
        "full_model_name":"togethercomputer\/GPT-JT-6B-v0",
        "Parameters":6.0,
        "MMLU_average":0.4934405234,
        "arc:challenge|25":0.3899317406,
        "hellaswag|10":0.4965146385,
        "MMLU_abstract_algebra":0.41,
        "MMLU_anatomy":0.6592592593,
        "MMLU_astronomy":0.625,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.5698113208,
        "MMLU_college_biology":0.5763888889,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.44,
        "MMLU_college_medicine":0.5375722543,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.69,
        "MMLU_conceptual_physics":0.4468085106,
        "MMLU_econometrics":0.3684210526,
        "MMLU_electrical_engineering":0.5793103448,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.41,
        "MMLU_high_school_biology":0.4870967742,
        "MMLU_high_school_chemistry":0.3448275862,
        "MMLU_high_school_computer_science":0.53,
        "MMLU_high_school_european_history":0.4242424242,
        "MMLU_high_school_geography":0.6313131313,
        "MMLU_high_school_government_and_politics":0.621761658,
        "MMLU_high_school_macroeconomics":0.4153846154,
        "MMLU_high_school_mathematics":0.2259259259,
        "MMLU_high_school_microeconomics":0.4201680672,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.5504587156,
        "MMLU_high_school_statistics":0.3657407407,
        "MMLU_high_school_us_history":0.4362745098,
        "MMLU_high_school_world_history":0.4852320675,
        "MMLU_human_aging":0.5470852018,
        "MMLU_human_sexuality":0.641221374,
        "MMLU_international_law":0.5289256198,
        "MMLU_jurisprudence":0.6018518519,
        "MMLU_logical_fallacies":0.6012269939,
        "MMLU_machine_learning":0.4464285714,
        "MMLU_management":0.7281553398,
        "MMLU_marketing":0.6196581197,
        "MMLU_medical_genetics":0.69,
        "MMLU_miscellaneous":0.4904214559,
        "MMLU_moral_disputes":0.4710982659,
        "MMLU_moral_scenarios":0.2391061453,
        "MMLU_nutrition":0.4967320261,
        "MMLU_philosophy":0.459807074,
        "MMLU_prehistory":0.524691358,
        "MMLU_professional_accounting":0.3758865248,
        "MMLU_professional_law":0.3428943937,
        "MMLU_professional_medicine":0.3382352941,
        "MMLU_professional_psychology":0.4019607843,
        "MMLU_public_relations":0.6636363636,
        "MMLU_security_studies":0.5306122449,
        "MMLU_sociology":0.6417910448,
        "MMLU_us_foreign_policy":0.75,
        "MMLU_virology":0.4819277108,
        "MMLU_world_religions":0.6374269006
    },
    {
        "Model":"Synthia-7B-v1.2",
        "URL":"https:\/\/huggingface.co\/migtissera\/Synthia-7B-v1.2",
        "full_model_name":"migtissera\/Synthia-7B-v1.2",
        "Parameters":7.0,
        "MMLU_average":0.4933256388,
        "arc:challenge|25":0.5145051195,
        "hellaswag|10":0.5989842661,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.4296296296,
        "MMLU_astronomy":0.4736842105,
        "MMLU_business_ethics":0.5,
        "MMLU_clinical_knowledge":0.5132075472,
        "MMLU_college_biology":0.5069444444,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.38,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.4335260116,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.2989417989,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.5290322581,
        "MMLU_high_school_chemistry":0.3842364532,
        "MMLU_high_school_computer_science":0.42,
        "MMLU_high_school_european_history":0.7151515152,
        "MMLU_high_school_geography":0.6060606061,
        "MMLU_high_school_government_and_politics":0.7512953368,
        "MMLU_high_school_macroeconomics":0.4435897436,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.4621848739,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.6366972477,
        "MMLU_high_school_statistics":0.3842592593,
        "MMLU_high_school_us_history":0.7058823529,
        "MMLU_high_school_world_history":0.6962025316,
        "MMLU_human_aging":0.5919282511,
        "MMLU_human_sexuality":0.5572519084,
        "MMLU_international_law":0.6115702479,
        "MMLU_jurisprudence":0.5462962963,
        "MMLU_logical_fallacies":0.5030674847,
        "MMLU_machine_learning":0.4642857143,
        "MMLU_management":0.6504854369,
        "MMLU_marketing":0.7606837607,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.6717752235,
        "MMLU_moral_disputes":0.5375722543,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.5326797386,
        "MMLU_philosophy":0.575562701,
        "MMLU_prehistory":0.5524691358,
        "MMLU_professional_accounting":0.3865248227,
        "MMLU_professional_law":0.3807040417,
        "MMLU_professional_medicine":0.4889705882,
        "MMLU_professional_psychology":0.4967320261,
        "MMLU_public_relations":0.6,
        "MMLU_security_studies":0.6,
        "MMLU_sociology":0.6069651741,
        "MMLU_us_foreign_policy":0.66,
        "MMLU_virology":0.3855421687,
        "MMLU_world_religions":0.6725146199
    },
    {
        "Model":"Barcenas-7b",
        "URL":"https:\/\/huggingface.co\/Danielbrdz\/Barcenas-7b",
        "full_model_name":"Danielbrdz\/Barcenas-7b",
        "Parameters":7.0,
        "MMLU_average":0.4927027655,
        "arc:challenge|25":0.5187713311,
        "hellaswag|10":0.5879306911,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.5037037037,
        "MMLU_astronomy":0.4407894737,
        "MMLU_business_ethics":0.46,
        "MMLU_clinical_knowledge":0.5735849057,
        "MMLU_college_biology":0.4930555556,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.450867052,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.52,
        "MMLU_conceptual_physics":0.4510638298,
        "MMLU_econometrics":0.3333333333,
        "MMLU_electrical_engineering":0.4551724138,
        "MMLU_elementary_mathematics":0.2962962963,
        "MMLU_formal_logic":0.2698412698,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.564516129,
        "MMLU_high_school_chemistry":0.3743842365,
        "MMLU_high_school_computer_science":0.38,
        "MMLU_high_school_european_history":0.6181818182,
        "MMLU_high_school_geography":0.5757575758,
        "MMLU_high_school_government_and_politics":0.7150259067,
        "MMLU_high_school_macroeconomics":0.441025641,
        "MMLU_high_school_mathematics":0.237037037,
        "MMLU_high_school_microeconomics":0.4495798319,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.6825688073,
        "MMLU_high_school_statistics":0.4166666667,
        "MMLU_high_school_us_history":0.6960784314,
        "MMLU_high_school_world_history":0.6877637131,
        "MMLU_human_aging":0.5784753363,
        "MMLU_human_sexuality":0.5648854962,
        "MMLU_international_law":0.6280991736,
        "MMLU_jurisprudence":0.6018518519,
        "MMLU_logical_fallacies":0.5153374233,
        "MMLU_machine_learning":0.4107142857,
        "MMLU_management":0.6893203883,
        "MMLU_marketing":0.7478632479,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.7011494253,
        "MMLU_moral_disputes":0.5202312139,
        "MMLU_moral_scenarios":0.2636871508,
        "MMLU_nutrition":0.5032679739,
        "MMLU_philosophy":0.5659163987,
        "MMLU_prehistory":0.537037037,
        "MMLU_professional_accounting":0.3581560284,
        "MMLU_professional_law":0.368970013,
        "MMLU_professional_medicine":0.5110294118,
        "MMLU_professional_psychology":0.4967320261,
        "MMLU_public_relations":0.5727272727,
        "MMLU_security_studies":0.5551020408,
        "MMLU_sociology":0.6218905473,
        "MMLU_us_foreign_policy":0.74,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.6959064327
    },
    {
        "Model":"vicuna-7b-v1.5-lora-mctaco",
        "URL":"https:\/\/huggingface.co\/Charlie911\/vicuna-7b-v1.5-lora-mctaco",
        "full_model_name":"Charlie911\/vicuna-7b-v1.5-lora-mctaco",
        "Parameters":7.0,
        "MMLU_average":0.4926937397,
        "arc:challenge|25":0.4274744027,
        "hellaswag|10":0.5592511452,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.4518518519,
        "MMLU_astronomy":0.4407894737,
        "MMLU_business_ethics":0.46,
        "MMLU_clinical_knowledge":0.5433962264,
        "MMLU_college_biology":0.5486111111,
        "MMLU_college_chemistry":0.3,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4450867052,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.6,
        "MMLU_conceptual_physics":0.4085106383,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.4551724138,
        "MMLU_elementary_mathematics":0.3068783069,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.5451612903,
        "MMLU_high_school_chemistry":0.4137931034,
        "MMLU_high_school_computer_science":0.44,
        "MMLU_high_school_european_history":0.6606060606,
        "MMLU_high_school_geography":0.6161616162,
        "MMLU_high_school_government_and_politics":0.6943005181,
        "MMLU_high_school_macroeconomics":0.458974359,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.4831932773,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.704587156,
        "MMLU_high_school_statistics":0.3518518519,
        "MMLU_high_school_us_history":0.7058823529,
        "MMLU_high_school_world_history":0.6835443038,
        "MMLU_human_aging":0.6098654709,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.5785123967,
        "MMLU_jurisprudence":0.5740740741,
        "MMLU_logical_fallacies":0.5153374233,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.6504854369,
        "MMLU_marketing":0.7820512821,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.6679438059,
        "MMLU_moral_disputes":0.5404624277,
        "MMLU_moral_scenarios":0.2413407821,
        "MMLU_nutrition":0.5032679739,
        "MMLU_philosophy":0.5691318328,
        "MMLU_prehistory":0.5648148148,
        "MMLU_professional_accounting":0.3510638298,
        "MMLU_professional_law":0.3617992177,
        "MMLU_professional_medicine":0.5441176471,
        "MMLU_professional_psychology":0.4689542484,
        "MMLU_public_relations":0.5818181818,
        "MMLU_security_studies":0.6,
        "MMLU_sociology":0.7064676617,
        "MMLU_us_foreign_policy":0.7,
        "MMLU_virology":0.3975903614,
        "MMLU_world_religions":0.6783625731
    },
    {
        "Model":"mpt-30b-instruct",
        "URL":"https:\/\/huggingface.co\/mosaicml\/mpt-30b-instruct",
        "full_model_name":"mosaicml\/mpt-30b-instruct",
        "Parameters":30.0,
        "MMLU_average":0.4915295523,
        "arc:challenge|25":0.542662116,
        "hellaswag|10":0.6524596694,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4222222222,
        "MMLU_astronomy":0.5,
        "MMLU_business_ethics":0.5,
        "MMLU_clinical_knowledge":0.5169811321,
        "MMLU_college_biology":0.5347222222,
        "MMLU_college_chemistry":0.32,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.450867052,
        "MMLU_college_physics":0.3529411765,
        "MMLU_computer_security":0.64,
        "MMLU_conceptual_physics":0.4638297872,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.3201058201,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.5322580645,
        "MMLU_high_school_chemistry":0.3793103448,
        "MMLU_high_school_computer_science":0.51,
        "MMLU_high_school_european_history":0.6484848485,
        "MMLU_high_school_geography":0.6464646465,
        "MMLU_high_school_government_and_politics":0.6735751295,
        "MMLU_high_school_macroeconomics":0.458974359,
        "MMLU_high_school_mathematics":0.3296296296,
        "MMLU_high_school_microeconomics":0.4453781513,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.6587155963,
        "MMLU_high_school_statistics":0.2916666667,
        "MMLU_high_school_us_history":0.7205882353,
        "MMLU_high_school_world_history":0.6877637131,
        "MMLU_human_aging":0.5919282511,
        "MMLU_human_sexuality":0.5877862595,
        "MMLU_international_law":0.4462809917,
        "MMLU_jurisprudence":0.5740740741,
        "MMLU_logical_fallacies":0.490797546,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.6116504854,
        "MMLU_marketing":0.7051282051,
        "MMLU_medical_genetics":0.46,
        "MMLU_miscellaneous":0.6922094508,
        "MMLU_moral_disputes":0.4971098266,
        "MMLU_moral_scenarios":0.3027932961,
        "MMLU_nutrition":0.5163398693,
        "MMLU_philosophy":0.5659163987,
        "MMLU_prehistory":0.5771604938,
        "MMLU_professional_accounting":0.390070922,
        "MMLU_professional_law":0.370273794,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.4901960784,
        "MMLU_public_relations":0.5181818182,
        "MMLU_security_studies":0.5632653061,
        "MMLU_sociology":0.552238806,
        "MMLU_us_foreign_policy":0.69,
        "MMLU_virology":0.4638554217,
        "MMLU_world_religions":0.6666666667
    },
    {
        "Model":"Dans-RetroRodeo-13b",
        "URL":"https:\/\/huggingface.co\/PocketDoc\/Dans-RetroRodeo-13b",
        "full_model_name":"PocketDoc\/Dans-RetroRodeo-13b",
        "Parameters":13.0,
        "MMLU_average":0.4893112694,
        "arc:challenge|25":0.497440273,
        "hellaswag|10":0.5989842661,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.4473684211,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.5433962264,
        "MMLU_college_biology":0.4513888889,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.35,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.4624277457,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.63,
        "MMLU_conceptual_physics":0.4042553191,
        "MMLU_econometrics":0.350877193,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.335978836,
        "MMLU_formal_logic":0.2222222222,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.6064516129,
        "MMLU_high_school_chemistry":0.39408867,
        "MMLU_high_school_computer_science":0.5,
        "MMLU_high_school_european_history":0.6,
        "MMLU_high_school_geography":0.6161616162,
        "MMLU_high_school_government_and_politics":0.6994818653,
        "MMLU_high_school_macroeconomics":0.4564102564,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.4579831933,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.6422018349,
        "MMLU_high_school_statistics":0.3935185185,
        "MMLU_high_school_us_history":0.6470588235,
        "MMLU_high_school_world_history":0.6455696203,
        "MMLU_human_aging":0.5515695067,
        "MMLU_human_sexuality":0.5648854962,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.6388888889,
        "MMLU_logical_fallacies":0.6012269939,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.6699029126,
        "MMLU_marketing":0.7008547009,
        "MMLU_medical_genetics":0.47,
        "MMLU_miscellaneous":0.6538952746,
        "MMLU_moral_disputes":0.5780346821,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.5555555556,
        "MMLU_philosophy":0.575562701,
        "MMLU_prehistory":0.4969135802,
        "MMLU_professional_accounting":0.3581560284,
        "MMLU_professional_law":0.3996088657,
        "MMLU_professional_medicine":0.5,
        "MMLU_professional_psychology":0.4836601307,
        "MMLU_public_relations":0.5090909091,
        "MMLU_security_studies":0.5673469388,
        "MMLU_sociology":0.6467661692,
        "MMLU_us_foreign_policy":0.69,
        "MMLU_virology":0.4036144578,
        "MMLU_world_religions":0.6432748538
    },
    {
        "Model":"L2-7b-Guanaco-Uncensored",
        "URL":"https:\/\/huggingface.co\/Lazycuber\/L2-7b-Guanaco-Uncensored",
        "full_model_name":"Lazycuber\/L2-7b-Guanaco-Uncensored",
        "Parameters":7.0,
        "MMLU_average":0.4892874997,
        "arc:challenge|25":0.4667235495,
        "hellaswag|10":0.5754829715,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4222222222,
        "MMLU_astronomy":0.4605263158,
        "MMLU_business_ethics":0.47,
        "MMLU_clinical_knowledge":0.5396226415,
        "MMLU_college_biology":0.5069444444,
        "MMLU_college_chemistry":0.32,
        "MMLU_college_computer_science":0.4,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.3988439306,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.57,
        "MMLU_conceptual_physics":0.4255319149,
        "MMLU_econometrics":0.3771929825,
        "MMLU_electrical_engineering":0.524137931,
        "MMLU_elementary_mathematics":0.2936507937,
        "MMLU_formal_logic":0.2380952381,
        "MMLU_global_facts":0.41,
        "MMLU_high_school_biology":0.5322580645,
        "MMLU_high_school_chemistry":0.3990147783,
        "MMLU_high_school_computer_science":0.43,
        "MMLU_high_school_european_history":0.5939393939,
        "MMLU_high_school_geography":0.6161616162,
        "MMLU_high_school_government_and_politics":0.7098445596,
        "MMLU_high_school_macroeconomics":0.441025641,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.4411764706,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.6733944954,
        "MMLU_high_school_statistics":0.3148148148,
        "MMLU_high_school_us_history":0.6666666667,
        "MMLU_high_school_world_history":0.6582278481,
        "MMLU_human_aging":0.5650224215,
        "MMLU_human_sexuality":0.572519084,
        "MMLU_international_law":0.6363636364,
        "MMLU_jurisprudence":0.6203703704,
        "MMLU_logical_fallacies":0.5460122699,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.6990291262,
        "MMLU_marketing":0.7307692308,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.687100894,
        "MMLU_moral_disputes":0.5260115607,
        "MMLU_moral_scenarios":0.2122905028,
        "MMLU_nutrition":0.522875817,
        "MMLU_philosophy":0.5562700965,
        "MMLU_prehistory":0.5524691358,
        "MMLU_professional_accounting":0.3794326241,
        "MMLU_professional_law":0.3455019557,
        "MMLU_professional_medicine":0.4852941176,
        "MMLU_professional_psychology":0.4869281046,
        "MMLU_public_relations":0.5272727273,
        "MMLU_security_studies":0.5795918367,
        "MMLU_sociology":0.631840796,
        "MMLU_us_foreign_policy":0.65,
        "MMLU_virology":0.3975903614,
        "MMLU_world_religions":0.7192982456
    },
    {
        "Model":"firefly-llama-13b-v1.2",
        "URL":"https:\/\/huggingface.co\/YeungNLP\/firefly-llama-13b-v1.2",
        "full_model_name":"YeungNLP\/firefly-llama-13b-v1.2",
        "Parameters":13.0,
        "MMLU_average":0.4890475278,
        "arc:challenge|25":0.5494880546,
        "hellaswag|10":0.6127265485,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.4539473684,
        "MMLU_business_ethics":0.45,
        "MMLU_clinical_knowledge":0.5132075472,
        "MMLU_college_biology":0.4722222222,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.450867052,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.3744680851,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.4275862069,
        "MMLU_elementary_mathematics":0.2698412698,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.29,
        "MMLU_high_school_biology":0.5612903226,
        "MMLU_high_school_chemistry":0.3743842365,
        "MMLU_high_school_computer_science":0.47,
        "MMLU_high_school_european_history":0.6,
        "MMLU_high_school_geography":0.6161616162,
        "MMLU_high_school_government_and_politics":0.689119171,
        "MMLU_high_school_macroeconomics":0.4641025641,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.4789915966,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.623853211,
        "MMLU_high_school_statistics":0.3703703704,
        "MMLU_high_school_us_history":0.6666666667,
        "MMLU_high_school_world_history":0.6877637131,
        "MMLU_human_aging":0.5112107623,
        "MMLU_human_sexuality":0.5954198473,
        "MMLU_international_law":0.6363636364,
        "MMLU_jurisprudence":0.537037037,
        "MMLU_logical_fallacies":0.5521472393,
        "MMLU_machine_learning":0.4196428571,
        "MMLU_management":0.6116504854,
        "MMLU_marketing":0.7521367521,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.6641123883,
        "MMLU_moral_disputes":0.5289017341,
        "MMLU_moral_scenarios":0.2458100559,
        "MMLU_nutrition":0.5424836601,
        "MMLU_philosophy":0.5241157556,
        "MMLU_prehistory":0.5432098765,
        "MMLU_professional_accounting":0.3723404255,
        "MMLU_professional_law":0.3807040417,
        "MMLU_professional_medicine":0.5,
        "MMLU_professional_psychology":0.4607843137,
        "MMLU_public_relations":0.5727272727,
        "MMLU_security_studies":0.5510204082,
        "MMLU_sociology":0.6517412935,
        "MMLU_us_foreign_policy":0.7,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.6959064327
    },
    {
        "Model":"wizard-vicuna-13B-HF",
        "URL":"https:\/\/huggingface.co\/TheBloke\/wizard-vicuna-13B-HF",
        "full_model_name":"TheBloke\/wizard-vicuna-13B-HF",
        "Parameters":13.0,
        "MMLU_average":0.4887594886,
        "arc:challenge|25":0.5187713311,
        "hellaswag|10":0.587134037,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4074074074,
        "MMLU_astronomy":0.4736842105,
        "MMLU_business_ethics":0.5,
        "MMLU_clinical_knowledge":0.4716981132,
        "MMLU_college_biology":0.4861111111,
        "MMLU_college_chemistry":0.33,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.3583815029,
        "MMLU_college_physics":0.1764705882,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.3829787234,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.4068965517,
        "MMLU_elementary_mathematics":0.2751322751,
        "MMLU_formal_logic":0.380952381,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.5225806452,
        "MMLU_high_school_chemistry":0.3596059113,
        "MMLU_high_school_computer_science":0.52,
        "MMLU_high_school_european_history":0.6545454545,
        "MMLU_high_school_geography":0.6161616162,
        "MMLU_high_school_government_and_politics":0.6839378238,
        "MMLU_high_school_macroeconomics":0.4153846154,
        "MMLU_high_school_mathematics":0.237037037,
        "MMLU_high_school_microeconomics":0.4327731092,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.6495412844,
        "MMLU_high_school_statistics":0.3101851852,
        "MMLU_high_school_us_history":0.6764705882,
        "MMLU_high_school_world_history":0.6708860759,
        "MMLU_human_aging":0.5964125561,
        "MMLU_human_sexuality":0.6259541985,
        "MMLU_international_law":0.6363636364,
        "MMLU_jurisprudence":0.5925925926,
        "MMLU_logical_fallacies":0.6012269939,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.6601941748,
        "MMLU_marketing":0.7179487179,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.6756066411,
        "MMLU_moral_disputes":0.5317919075,
        "MMLU_moral_scenarios":0.3452513966,
        "MMLU_nutrition":0.5555555556,
        "MMLU_philosophy":0.4951768489,
        "MMLU_prehistory":0.524691358,
        "MMLU_professional_accounting":0.3439716312,
        "MMLU_professional_law":0.4087353325,
        "MMLU_professional_medicine":0.4338235294,
        "MMLU_professional_psychology":0.5,
        "MMLU_public_relations":0.5181818182,
        "MMLU_security_studies":0.5673469388,
        "MMLU_sociology":0.7263681592,
        "MMLU_us_foreign_policy":0.72,
        "MMLU_virology":0.4518072289,
        "MMLU_world_religions":0.6900584795
    },
    {
        "Model":"Llama-2-7B-physics",
        "URL":"https:\/\/huggingface.co\/Harshvir\/Llama-2-7B-physics",
        "full_model_name":"Harshvir\/Llama-2-7B-physics",
        "Parameters":7.0,
        "MMLU_average":0.488309655,
        "arc:challenge|25":0.4940273038,
        "hellaswag|10":0.5880302729,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4296296296,
        "MMLU_astronomy":0.4934210526,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.5358490566,
        "MMLU_college_biology":0.5069444444,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.4,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.3988439306,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.59,
        "MMLU_conceptual_physics":0.4042553191,
        "MMLU_econometrics":0.3947368421,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.3174603175,
        "MMLU_formal_logic":0.2142857143,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.535483871,
        "MMLU_high_school_chemistry":0.4137931034,
        "MMLU_high_school_computer_science":0.48,
        "MMLU_high_school_european_history":0.6060606061,
        "MMLU_high_school_geography":0.6212121212,
        "MMLU_high_school_government_and_politics":0.7046632124,
        "MMLU_high_school_macroeconomics":0.441025641,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.4201680672,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.6899082569,
        "MMLU_high_school_statistics":0.3472222222,
        "MMLU_high_school_us_history":0.6666666667,
        "MMLU_high_school_world_history":0.6497890295,
        "MMLU_human_aging":0.5829596413,
        "MMLU_human_sexuality":0.5419847328,
        "MMLU_international_law":0.6611570248,
        "MMLU_jurisprudence":0.5925925926,
        "MMLU_logical_fallacies":0.5398773006,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.7264957265,
        "MMLU_medical_genetics":0.53,
        "MMLU_miscellaneous":0.6845466156,
        "MMLU_moral_disputes":0.5028901734,
        "MMLU_moral_scenarios":0.2245810056,
        "MMLU_nutrition":0.5196078431,
        "MMLU_philosophy":0.5594855305,
        "MMLU_prehistory":0.537037037,
        "MMLU_professional_accounting":0.3723404255,
        "MMLU_professional_law":0.3409387223,
        "MMLU_professional_medicine":0.5036764706,
        "MMLU_professional_psychology":0.4950980392,
        "MMLU_public_relations":0.5272727273,
        "MMLU_security_studies":0.5591836735,
        "MMLU_sociology":0.6218905473,
        "MMLU_us_foreign_policy":0.68,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.6959064327
    },
    {
        "Model":"Capybara-7B",
        "URL":"https:\/\/huggingface.co\/NousResearch\/Capybara-7B",
        "full_model_name":"NousResearch\/Capybara-7B",
        "Parameters":7.0,
        "MMLU_average":0.4880401768,
        "arc:challenge|25":0.5170648464,
        "hellaswag|10":0.6157140012,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.4342105263,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.5320754717,
        "MMLU_college_biology":0.4930555556,
        "MMLU_college_chemistry":0.26,
        "MMLU_college_computer_science":0.34,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.4450867052,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.59,
        "MMLU_conceptual_physics":0.4468085106,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.324137931,
        "MMLU_elementary_mathematics":0.3068783069,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.5612903226,
        "MMLU_high_school_chemistry":0.3596059113,
        "MMLU_high_school_computer_science":0.41,
        "MMLU_high_school_european_history":0.6606060606,
        "MMLU_high_school_geography":0.6666666667,
        "MMLU_high_school_government_and_politics":0.7512953368,
        "MMLU_high_school_macroeconomics":0.4615384615,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.4663865546,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.6880733945,
        "MMLU_high_school_statistics":0.3240740741,
        "MMLU_high_school_us_history":0.6715686275,
        "MMLU_high_school_world_history":0.7004219409,
        "MMLU_human_aging":0.6143497758,
        "MMLU_human_sexuality":0.5496183206,
        "MMLU_international_law":0.6446280992,
        "MMLU_jurisprudence":0.6111111111,
        "MMLU_logical_fallacies":0.6196319018,
        "MMLU_machine_learning":0.4196428571,
        "MMLU_management":0.6116504854,
        "MMLU_marketing":0.7435897436,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.6985951469,
        "MMLU_moral_disputes":0.5317919075,
        "MMLU_moral_scenarios":0.2391061453,
        "MMLU_nutrition":0.4738562092,
        "MMLU_philosophy":0.6012861736,
        "MMLU_prehistory":0.5740740741,
        "MMLU_professional_accounting":0.3687943262,
        "MMLU_professional_law":0.3787483703,
        "MMLU_professional_medicine":0.5183823529,
        "MMLU_professional_psychology":0.477124183,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.4367346939,
        "MMLU_sociology":0.6865671642,
        "MMLU_us_foreign_policy":0.65,
        "MMLU_virology":0.3855421687,
        "MMLU_world_religions":0.6959064327
    },
    {
        "Model":"Llama-2-7b-chat-finetune-AUTOMATE",
        "URL":"https:\/\/huggingface.co\/revolutionarybukhari\/Llama-2-7b-chat-finetune-AUTOMATE",
        "full_model_name":"revolutionarybukhari\/Llama-2-7b-chat-finetune-AUTOMATE",
        "Parameters":7.0,
        "MMLU_average":0.4879636404,
        "arc:challenge|25":0.4906143345,
        "hellaswag|10":0.5622385979,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4296296296,
        "MMLU_astronomy":0.4671052632,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.5396226415,
        "MMLU_college_biology":0.5347222222,
        "MMLU_college_chemistry":0.28,
        "MMLU_college_computer_science":0.4,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.4046242775,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.56,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.3771929825,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.2962962963,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.41,
        "MMLU_high_school_biology":0.5322580645,
        "MMLU_high_school_chemistry":0.3596059113,
        "MMLU_high_school_computer_science":0.42,
        "MMLU_high_school_european_history":0.5696969697,
        "MMLU_high_school_geography":0.5909090909,
        "MMLU_high_school_government_and_politics":0.7098445596,
        "MMLU_high_school_macroeconomics":0.4230769231,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.4369747899,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.671559633,
        "MMLU_high_school_statistics":0.3287037037,
        "MMLU_high_school_us_history":0.681372549,
        "MMLU_high_school_world_history":0.6708860759,
        "MMLU_human_aging":0.5874439462,
        "MMLU_human_sexuality":0.5877862595,
        "MMLU_international_law":0.6446280992,
        "MMLU_jurisprudence":0.6111111111,
        "MMLU_logical_fallacies":0.5521472393,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.6893203883,
        "MMLU_marketing":0.7222222222,
        "MMLU_medical_genetics":0.53,
        "MMLU_miscellaneous":0.6704980843,
        "MMLU_moral_disputes":0.5202312139,
        "MMLU_moral_scenarios":0.2223463687,
        "MMLU_nutrition":0.5098039216,
        "MMLU_philosophy":0.5723472669,
        "MMLU_prehistory":0.5740740741,
        "MMLU_professional_accounting":0.3723404255,
        "MMLU_professional_law":0.3461538462,
        "MMLU_professional_medicine":0.4742647059,
        "MMLU_professional_psychology":0.4869281046,
        "MMLU_public_relations":0.5272727273,
        "MMLU_security_studies":0.5306122449,
        "MMLU_sociology":0.6517412935,
        "MMLU_us_foreign_policy":0.71,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.7251461988
    },
    {
        "Model":"robin-13B-v2-fp16",
        "URL":"https:\/\/huggingface.co\/TheBloke\/robin-13B-v2-fp16",
        "full_model_name":"TheBloke\/robin-13B-v2-fp16",
        "Parameters":13.0,
        "MMLU_average":0.4878673163,
        "arc:challenge|25":0.5401023891,
        "hellaswag|10":0.594503087,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.4868421053,
        "MMLU_business_ethics":0.45,
        "MMLU_clinical_knowledge":0.4679245283,
        "MMLU_college_biology":0.4722222222,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.38,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.4450867052,
        "MMLU_college_physics":0.1764705882,
        "MMLU_computer_security":0.62,
        "MMLU_conceptual_physics":0.4,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4068965517,
        "MMLU_elementary_mathematics":0.2592592593,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.4903225806,
        "MMLU_high_school_chemistry":0.3201970443,
        "MMLU_high_school_computer_science":0.47,
        "MMLU_high_school_european_history":0.6303030303,
        "MMLU_high_school_geography":0.5606060606,
        "MMLU_high_school_government_and_politics":0.6683937824,
        "MMLU_high_school_macroeconomics":0.4487179487,
        "MMLU_high_school_mathematics":0.2333333333,
        "MMLU_high_school_microeconomics":0.4411764706,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.6605504587,
        "MMLU_high_school_statistics":0.3009259259,
        "MMLU_high_school_us_history":0.6274509804,
        "MMLU_high_school_world_history":0.7215189873,
        "MMLU_human_aging":0.5695067265,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.7024793388,
        "MMLU_jurisprudence":0.5740740741,
        "MMLU_logical_fallacies":0.5828220859,
        "MMLU_machine_learning":0.5089285714,
        "MMLU_management":0.640776699,
        "MMLU_marketing":0.7521367521,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.6883780332,
        "MMLU_moral_disputes":0.5,
        "MMLU_moral_scenarios":0.2502793296,
        "MMLU_nutrition":0.5065359477,
        "MMLU_philosophy":0.5337620579,
        "MMLU_prehistory":0.5524691358,
        "MMLU_professional_accounting":0.3794326241,
        "MMLU_professional_law":0.4211212516,
        "MMLU_professional_medicine":0.5147058824,
        "MMLU_professional_psychology":0.4836601307,
        "MMLU_public_relations":0.5636363636,
        "MMLU_security_studies":0.5551020408,
        "MMLU_sociology":0.6567164179,
        "MMLU_us_foreign_policy":0.78,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.6959064327
    },
    {
        "Model":"manticore-13b-chat-pyg",
        "URL":"https:\/\/huggingface.co\/openaccess-ai-collective\/manticore-13b-chat-pyg",
        "full_model_name":"openaccess-ai-collective\/manticore-13b-chat-pyg",
        "Parameters":13.0,
        "MMLU_average":0.4876391416,
        "arc:challenge|25":0.5674061433,
        "hellaswag|10":0.6192989444,
        "MMLU_abstract_algebra":0.39,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.4605263158,
        "MMLU_business_ethics":0.45,
        "MMLU_clinical_knowledge":0.5094339623,
        "MMLU_college_biology":0.5069444444,
        "MMLU_college_chemistry":0.3,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.4219653179,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.59,
        "MMLU_conceptual_physics":0.3914893617,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.3931034483,
        "MMLU_elementary_mathematics":0.2857142857,
        "MMLU_formal_logic":0.3015873016,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.5419354839,
        "MMLU_high_school_chemistry":0.3201970443,
        "MMLU_high_school_computer_science":0.52,
        "MMLU_high_school_european_history":0.6060606061,
        "MMLU_high_school_geography":0.5909090909,
        "MMLU_high_school_government_and_politics":0.6839378238,
        "MMLU_high_school_macroeconomics":0.4615384615,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.4579831933,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.6495412844,
        "MMLU_high_school_statistics":0.2546296296,
        "MMLU_high_school_us_history":0.681372549,
        "MMLU_high_school_world_history":0.7088607595,
        "MMLU_human_aging":0.5784753363,
        "MMLU_human_sexuality":0.5419847328,
        "MMLU_international_law":0.6363636364,
        "MMLU_jurisprudence":0.5648148148,
        "MMLU_logical_fallacies":0.5766871166,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.7222222222,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.680715198,
        "MMLU_moral_disputes":0.5173410405,
        "MMLU_moral_scenarios":0.2368715084,
        "MMLU_nutrition":0.5065359477,
        "MMLU_philosophy":0.5562700965,
        "MMLU_prehistory":0.5308641975,
        "MMLU_professional_accounting":0.3829787234,
        "MMLU_professional_law":0.4087353325,
        "MMLU_professional_medicine":0.4632352941,
        "MMLU_professional_psychology":0.5049019608,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.5102040816,
        "MMLU_sociology":0.6268656716,
        "MMLU_us_foreign_policy":0.78,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.7192982456
    },
    {
        "Model":"LIMA-13b-hf",
        "URL":"https:\/\/huggingface.co\/heegyu\/LIMA-13b-hf",
        "full_model_name":"heegyu\/LIMA-13b-hf",
        "Parameters":13.0,
        "MMLU_average":0.4871528611,
        "arc:challenge|25":0.542662116,
        "hellaswag|10":0.6197968532,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.5131578947,
        "MMLU_business_ethics":0.47,
        "MMLU_clinical_knowledge":0.5094339623,
        "MMLU_college_biology":0.4652777778,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.4450867052,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.63,
        "MMLU_conceptual_physics":0.3829787234,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.2724867725,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.5322580645,
        "MMLU_high_school_chemistry":0.3645320197,
        "MMLU_high_school_computer_science":0.51,
        "MMLU_high_school_european_history":0.5878787879,
        "MMLU_high_school_geography":0.5353535354,
        "MMLU_high_school_government_and_politics":0.6735751295,
        "MMLU_high_school_macroeconomics":0.4538461538,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.4789915966,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.6275229358,
        "MMLU_high_school_statistics":0.2962962963,
        "MMLU_high_school_us_history":0.5882352941,
        "MMLU_high_school_world_history":0.7215189873,
        "MMLU_human_aging":0.5605381166,
        "MMLU_human_sexuality":0.6183206107,
        "MMLU_international_law":0.6446280992,
        "MMLU_jurisprudence":0.537037037,
        "MMLU_logical_fallacies":0.509202454,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.6310679612,
        "MMLU_marketing":0.7264957265,
        "MMLU_medical_genetics":0.51,
        "MMLU_miscellaneous":0.6768837803,
        "MMLU_moral_disputes":0.5289017341,
        "MMLU_moral_scenarios":0.261452514,
        "MMLU_nutrition":0.5294117647,
        "MMLU_philosophy":0.5530546624,
        "MMLU_prehistory":0.549382716,
        "MMLU_professional_accounting":0.3617021277,
        "MMLU_professional_law":0.3872229465,
        "MMLU_professional_medicine":0.5073529412,
        "MMLU_professional_psychology":0.4983660131,
        "MMLU_public_relations":0.5818181818,
        "MMLU_security_studies":0.5102040816,
        "MMLU_sociology":0.6119402985,
        "MMLU_us_foreign_policy":0.82,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.6959064327
    },
    {
        "Model":"Llama2-7B-guanaco-dolphin-500",
        "URL":"https:\/\/huggingface.co\/mncai\/Llama2-7B-guanaco-dolphin-500",
        "full_model_name":"mncai\/Llama2-7B-guanaco-dolphin-500",
        "Parameters":7.0,
        "MMLU_average":0.4868938338,
        "arc:challenge|25":0.5255972696,
        "hellaswag|10":0.6216889066,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.4605263158,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.4754716981,
        "MMLU_college_biology":0.4861111111,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.4,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.450867052,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.56,
        "MMLU_conceptual_physics":0.4340425532,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.5103448276,
        "MMLU_elementary_mathematics":0.3015873016,
        "MMLU_formal_logic":0.3412698413,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.5387096774,
        "MMLU_high_school_chemistry":0.3399014778,
        "MMLU_high_school_computer_science":0.48,
        "MMLU_high_school_european_history":0.6060606061,
        "MMLU_high_school_geography":0.5909090909,
        "MMLU_high_school_government_and_politics":0.6632124352,
        "MMLU_high_school_macroeconomics":0.4538461538,
        "MMLU_high_school_mathematics":0.3074074074,
        "MMLU_high_school_microeconomics":0.4621848739,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.671559633,
        "MMLU_high_school_statistics":0.3101851852,
        "MMLU_high_school_us_history":0.5490196078,
        "MMLU_high_school_world_history":0.6371308017,
        "MMLU_human_aging":0.5650224215,
        "MMLU_human_sexuality":0.5954198473,
        "MMLU_international_law":0.6446280992,
        "MMLU_jurisprudence":0.5462962963,
        "MMLU_logical_fallacies":0.5214723926,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.6019417476,
        "MMLU_marketing":0.7393162393,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.6513409962,
        "MMLU_moral_disputes":0.5462427746,
        "MMLU_moral_scenarios":0.2525139665,
        "MMLU_nutrition":0.5424836601,
        "MMLU_philosophy":0.5884244373,
        "MMLU_prehistory":0.5092592593,
        "MMLU_professional_accounting":0.3794326241,
        "MMLU_professional_law":0.3663624511,
        "MMLU_professional_medicine":0.5625,
        "MMLU_professional_psychology":0.4705882353,
        "MMLU_public_relations":0.5545454545,
        "MMLU_security_studies":0.4857142857,
        "MMLU_sociology":0.6766169154,
        "MMLU_us_foreign_policy":0.64,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.7192982456
    },
    {
        "Model":"vicuna-7b-v1.5-lora-mctaco-modified2",
        "URL":"https:\/\/huggingface.co\/Charlie911\/vicuna-7b-v1.5-lora-mctaco-modified2",
        "full_model_name":"Charlie911\/vicuna-7b-v1.5-lora-mctaco-modified2",
        "Parameters":7.0,
        "MMLU_average":0.4848959058,
        "arc:challenge|25":0.3865187713,
        "hellaswag|10":0.5488946425,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.4342105263,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.5094339623,
        "MMLU_college_biology":0.5486111111,
        "MMLU_college_chemistry":0.3,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.3757225434,
        "MMLU_college_physics":0.1666666667,
        "MMLU_computer_security":0.65,
        "MMLU_conceptual_physics":0.429787234,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.4965517241,
        "MMLU_elementary_mathematics":0.2989417989,
        "MMLU_formal_logic":0.2777777778,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.5548387097,
        "MMLU_high_school_chemistry":0.39408867,
        "MMLU_high_school_computer_science":0.47,
        "MMLU_high_school_european_history":0.6484848485,
        "MMLU_high_school_geography":0.6161616162,
        "MMLU_high_school_government_and_politics":0.6787564767,
        "MMLU_high_school_macroeconomics":0.458974359,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.4159663866,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.6770642202,
        "MMLU_high_school_statistics":0.3842592593,
        "MMLU_high_school_us_history":0.681372549,
        "MMLU_high_school_world_history":0.6919831224,
        "MMLU_human_aging":0.5695067265,
        "MMLU_human_sexuality":0.5419847328,
        "MMLU_international_law":0.5619834711,
        "MMLU_jurisprudence":0.5,
        "MMLU_logical_fallacies":0.5030674847,
        "MMLU_machine_learning":0.4285714286,
        "MMLU_management":0.6116504854,
        "MMLU_marketing":0.735042735,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.680715198,
        "MMLU_moral_disputes":0.4913294798,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.4738562092,
        "MMLU_philosophy":0.5627009646,
        "MMLU_prehistory":0.5555555556,
        "MMLU_professional_accounting":0.3475177305,
        "MMLU_professional_law":0.3650586701,
        "MMLU_professional_medicine":0.4889705882,
        "MMLU_professional_psychology":0.4624183007,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.5673469388,
        "MMLU_sociology":0.6616915423,
        "MMLU_us_foreign_policy":0.66,
        "MMLU_virology":0.3795180723,
        "MMLU_world_religions":0.6959064327
    },
    {
        "Model":"spicyboros-7b-2.2",
        "URL":"https:\/\/huggingface.co\/jondurbin\/spicyboros-7b-2.2",
        "full_model_name":"jondurbin\/spicyboros-7b-2.2",
        "Parameters":7.0,
        "MMLU_average":0.4847337181,
        "arc:challenge|25":0.5324232082,
        "hellaswag|10":0.6026687911,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.4407894737,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.4716981132,
        "MMLU_college_biology":0.5138888889,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.4161849711,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.62,
        "MMLU_conceptual_physics":0.3957446809,
        "MMLU_econometrics":0.3333333333,
        "MMLU_electrical_engineering":0.4482758621,
        "MMLU_elementary_mathematics":0.2857142857,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.5419354839,
        "MMLU_high_school_chemistry":0.354679803,
        "MMLU_high_school_computer_science":0.44,
        "MMLU_high_school_european_history":0.6181818182,
        "MMLU_high_school_geography":0.5808080808,
        "MMLU_high_school_government_and_politics":0.7305699482,
        "MMLU_high_school_macroeconomics":0.4641025641,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.474789916,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.6752293578,
        "MMLU_high_school_statistics":0.375,
        "MMLU_high_school_us_history":0.6617647059,
        "MMLU_high_school_world_history":0.7004219409,
        "MMLU_human_aging":0.5919282511,
        "MMLU_human_sexuality":0.5267175573,
        "MMLU_international_law":0.6280991736,
        "MMLU_jurisprudence":0.5277777778,
        "MMLU_logical_fallacies":0.5030674847,
        "MMLU_machine_learning":0.4107142857,
        "MMLU_management":0.5631067961,
        "MMLU_marketing":0.7393162393,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.6679438059,
        "MMLU_moral_disputes":0.5433526012,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.5065359477,
        "MMLU_philosophy":0.5691318328,
        "MMLU_prehistory":0.512345679,
        "MMLU_professional_accounting":0.3971631206,
        "MMLU_professional_law":0.3650586701,
        "MMLU_professional_medicine":0.5220588235,
        "MMLU_professional_psychology":0.4591503268,
        "MMLU_public_relations":0.5272727273,
        "MMLU_security_studies":0.4367346939,
        "MMLU_sociology":0.6567164179,
        "MMLU_us_foreign_policy":0.68,
        "MMLU_virology":0.3795180723,
        "MMLU_world_religions":0.7134502924
    },
    {
        "Model":"WizardVicuna2-13b-hf",
        "URL":"https:\/\/huggingface.co\/heegyu\/WizardVicuna2-13b-hf",
        "full_model_name":"heegyu\/WizardVicuna2-13b-hf",
        "Parameters":13.0,
        "MMLU_average":0.4845509502,
        "arc:challenge|25":0.5247440273,
        "hellaswag|10":0.5952997411,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.4407894737,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.4679245283,
        "MMLU_college_biology":0.4791666667,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.38,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.4046242775,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.59,
        "MMLU_conceptual_physics":0.4170212766,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.3015873016,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.5322580645,
        "MMLU_high_school_chemistry":0.3399014778,
        "MMLU_high_school_computer_science":0.51,
        "MMLU_high_school_european_history":0.6424242424,
        "MMLU_high_school_geography":0.595959596,
        "MMLU_high_school_government_and_politics":0.6943005181,
        "MMLU_high_school_macroeconomics":0.3897435897,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.5,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.6440366972,
        "MMLU_high_school_statistics":0.3194444444,
        "MMLU_high_school_us_history":0.6225490196,
        "MMLU_high_school_world_history":0.6666666667,
        "MMLU_human_aging":0.6053811659,
        "MMLU_human_sexuality":0.5190839695,
        "MMLU_international_law":0.6859504132,
        "MMLU_jurisprudence":0.6018518519,
        "MMLU_logical_fallacies":0.4846625767,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.5922330097,
        "MMLU_marketing":0.7478632479,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.6768837803,
        "MMLU_moral_disputes":0.5202312139,
        "MMLU_moral_scenarios":0.2905027933,
        "MMLU_nutrition":0.4934640523,
        "MMLU_philosophy":0.578778135,
        "MMLU_prehistory":0.5216049383,
        "MMLU_professional_accounting":0.390070922,
        "MMLU_professional_law":0.3774445893,
        "MMLU_professional_medicine":0.3419117647,
        "MMLU_professional_psychology":0.4950980392,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.5632653061,
        "MMLU_sociology":0.6467661692,
        "MMLU_us_foreign_policy":0.7,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.7076023392
    },
    {
        "Model":"Llama2-7b-openorca-mc-v1",
        "URL":"https:\/\/huggingface.co\/beaugogh\/Llama2-7b-openorca-mc-v1",
        "full_model_name":"beaugogh\/Llama2-7b-openorca-mc-v1",
        "Parameters":7.0,
        "MMLU_average":0.4844407119,
        "arc:challenge|25":0.5076791809,
        "hellaswag|10":0.6186018721,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.5037037037,
        "MMLU_astronomy":0.4473684211,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.5509433962,
        "MMLU_college_biology":0.4930555556,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.34,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4624277457,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.4085106383,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.4344827586,
        "MMLU_elementary_mathematics":0.328042328,
        "MMLU_formal_logic":0.2698412698,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.5161290323,
        "MMLU_high_school_chemistry":0.3842364532,
        "MMLU_high_school_computer_science":0.34,
        "MMLU_high_school_european_history":0.6606060606,
        "MMLU_high_school_geography":0.5505050505,
        "MMLU_high_school_government_and_politics":0.6943005181,
        "MMLU_high_school_macroeconomics":0.4282051282,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.4285714286,
        "MMLU_high_school_physics":0.357615894,
        "MMLU_high_school_psychology":0.6770642202,
        "MMLU_high_school_statistics":0.3888888889,
        "MMLU_high_school_us_history":0.6568627451,
        "MMLU_high_school_world_history":0.7215189873,
        "MMLU_human_aging":0.5784753363,
        "MMLU_human_sexuality":0.534351145,
        "MMLU_international_law":0.6280991736,
        "MMLU_jurisprudence":0.5092592593,
        "MMLU_logical_fallacies":0.4969325153,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.5631067961,
        "MMLU_marketing":0.7307692308,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.6653895275,
        "MMLU_moral_disputes":0.5057803468,
        "MMLU_moral_scenarios":0.251396648,
        "MMLU_nutrition":0.4705882353,
        "MMLU_philosophy":0.5562700965,
        "MMLU_prehistory":0.5277777778,
        "MMLU_professional_accounting":0.414893617,
        "MMLU_professional_law":0.3885267275,
        "MMLU_professional_medicine":0.5110294118,
        "MMLU_professional_psychology":0.4493464052,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.4979591837,
        "MMLU_sociology":0.6268656716,
        "MMLU_us_foreign_policy":0.64,
        "MMLU_virology":0.3975903614,
        "MMLU_world_religions":0.6900584795
    },
    {
        "Model":"robin-13b-v2-delta",
        "URL":"https:\/\/huggingface.co\/OptimalScale\/robin-13b-v2-delta",
        "full_model_name":"OptimalScale\/robin-13b-v2-delta",
        "Parameters":13.0,
        "MMLU_average":0.4839330978,
        "arc:challenge|25":0.5375426621,
        "hellaswag|10":0.5944035053,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.4802631579,
        "MMLU_business_ethics":0.47,
        "MMLU_clinical_knowledge":0.4566037736,
        "MMLU_college_biology":0.4652777778,
        "MMLU_college_chemistry":0.23,
        "MMLU_college_computer_science":0.37,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.4335260116,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.63,
        "MMLU_conceptual_physics":0.3957446809,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.4068965517,
        "MMLU_elementary_mathematics":0.253968254,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.4903225806,
        "MMLU_high_school_chemistry":0.3103448276,
        "MMLU_high_school_computer_science":0.46,
        "MMLU_high_school_european_history":0.6303030303,
        "MMLU_high_school_geography":0.5606060606,
        "MMLU_high_school_government_and_politics":0.6735751295,
        "MMLU_high_school_macroeconomics":0.441025641,
        "MMLU_high_school_mathematics":0.2296296296,
        "MMLU_high_school_microeconomics":0.4369747899,
        "MMLU_high_school_physics":0.2450331126,
        "MMLU_high_school_psychology":0.6458715596,
        "MMLU_high_school_statistics":0.287037037,
        "MMLU_high_school_us_history":0.6323529412,
        "MMLU_high_school_world_history":0.7172995781,
        "MMLU_human_aging":0.5695067265,
        "MMLU_human_sexuality":0.5954198473,
        "MMLU_international_law":0.7024793388,
        "MMLU_jurisprudence":0.5833333333,
        "MMLU_logical_fallacies":0.5828220859,
        "MMLU_machine_learning":0.5,
        "MMLU_management":0.6310679612,
        "MMLU_marketing":0.7521367521,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.6845466156,
        "MMLU_moral_disputes":0.4884393064,
        "MMLU_moral_scenarios":0.2491620112,
        "MMLU_nutrition":0.5163398693,
        "MMLU_philosophy":0.5305466238,
        "MMLU_prehistory":0.5462962963,
        "MMLU_professional_accounting":0.3687943262,
        "MMLU_professional_law":0.4172099087,
        "MMLU_professional_medicine":0.5036764706,
        "MMLU_professional_psychology":0.4869281046,
        "MMLU_public_relations":0.5636363636,
        "MMLU_security_studies":0.5551020408,
        "MMLU_sociology":0.6517412935,
        "MMLU_us_foreign_policy":0.77,
        "MMLU_virology":0.4518072289,
        "MMLU_world_religions":0.701754386
    },
    {
        "Model":"dolphin-llama2-7b",
        "URL":"https:\/\/huggingface.co\/ehartford\/dolphin-llama2-7b",
        "full_model_name":"ehartford\/dolphin-llama2-7b",
        "Parameters":7.0,
        "MMLU_average":0.4837069544,
        "arc:challenge|25":0.4462457338,
        "hellaswag|10":0.5125473013,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4222222222,
        "MMLU_astronomy":0.4736842105,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.5547169811,
        "MMLU_college_biology":0.5,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.4,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.4219653179,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.48,
        "MMLU_conceptual_physics":0.4553191489,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.4965517241,
        "MMLU_elementary_mathematics":0.291005291,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.5774193548,
        "MMLU_high_school_chemistry":0.3349753695,
        "MMLU_high_school_computer_science":0.48,
        "MMLU_high_school_european_history":0.6787878788,
        "MMLU_high_school_geography":0.6161616162,
        "MMLU_high_school_government_and_politics":0.7098445596,
        "MMLU_high_school_macroeconomics":0.4256410256,
        "MMLU_high_school_mathematics":0.337037037,
        "MMLU_high_school_microeconomics":0.487394958,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.6403669725,
        "MMLU_high_school_statistics":0.3935185185,
        "MMLU_high_school_us_history":0.5833333333,
        "MMLU_high_school_world_history":0.7215189873,
        "MMLU_human_aging":0.5022421525,
        "MMLU_human_sexuality":0.534351145,
        "MMLU_international_law":0.6859504132,
        "MMLU_jurisprudence":0.6018518519,
        "MMLU_logical_fallacies":0.4785276074,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.640776699,
        "MMLU_marketing":0.6581196581,
        "MMLU_medical_genetics":0.47,
        "MMLU_miscellaneous":0.5925925926,
        "MMLU_moral_disputes":0.5289017341,
        "MMLU_moral_scenarios":0.2581005587,
        "MMLU_nutrition":0.5163398693,
        "MMLU_philosophy":0.540192926,
        "MMLU_prehistory":0.5277777778,
        "MMLU_professional_accounting":0.329787234,
        "MMLU_professional_law":0.3709256845,
        "MMLU_professional_medicine":0.4080882353,
        "MMLU_professional_psychology":0.3937908497,
        "MMLU_public_relations":0.5454545455,
        "MMLU_security_studies":0.6408163265,
        "MMLU_sociology":0.6119402985,
        "MMLU_us_foreign_policy":0.74,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.567251462
    },
    {
        "Model":"Nous-Hermes-llama-2-7b",
        "URL":"https:\/\/huggingface.co\/NousResearch\/Nous-Hermes-llama-2-7b",
        "full_model_name":"NousResearch\/Nous-Hermes-llama-2-7b",
        "Parameters":7.0,
        "MMLU_average":0.4833523465,
        "arc:challenge|25":0.5230375427,
        "hellaswag|10":0.5994821749,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4444444444,
        "MMLU_astronomy":0.4144736842,
        "MMLU_business_ethics":0.57,
        "MMLU_clinical_knowledge":0.5132075472,
        "MMLU_college_biology":0.4791666667,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4566473988,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.58,
        "MMLU_conceptual_physics":0.3914893617,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.4965517241,
        "MMLU_elementary_mathematics":0.3042328042,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.5419354839,
        "MMLU_high_school_chemistry":0.3448275862,
        "MMLU_high_school_computer_science":0.45,
        "MMLU_high_school_european_history":0.6181818182,
        "MMLU_high_school_geography":0.6161616162,
        "MMLU_high_school_government_and_politics":0.7150259067,
        "MMLU_high_school_macroeconomics":0.4769230769,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.4117647059,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.6660550459,
        "MMLU_high_school_statistics":0.3101851852,
        "MMLU_high_school_us_history":0.612745098,
        "MMLU_high_school_world_history":0.6666666667,
        "MMLU_human_aging":0.5964125561,
        "MMLU_human_sexuality":0.5572519084,
        "MMLU_international_law":0.6280991736,
        "MMLU_jurisprudence":0.5555555556,
        "MMLU_logical_fallacies":0.4785276074,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.6019417476,
        "MMLU_marketing":0.7094017094,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.6590038314,
        "MMLU_moral_disputes":0.5317919075,
        "MMLU_moral_scenarios":0.2547486034,
        "MMLU_nutrition":0.5130718954,
        "MMLU_philosophy":0.5948553055,
        "MMLU_prehistory":0.5555555556,
        "MMLU_professional_accounting":0.3617021277,
        "MMLU_professional_law":0.3455019557,
        "MMLU_professional_medicine":0.5441176471,
        "MMLU_professional_psychology":0.4395424837,
        "MMLU_public_relations":0.5727272727,
        "MMLU_security_studies":0.5020408163,
        "MMLU_sociology":0.6368159204,
        "MMLU_us_foreign_policy":0.69,
        "MMLU_virology":0.4096385542,
        "MMLU_world_religions":0.6959064327
    },
    {
        "Model":"WizardLM-13B-V1-1-SuperHOT-8K-fp16",
        "URL":"https:\/\/huggingface.co\/TheBloke\/WizardLM-13B-V1-1-SuperHOT-8K-fp16",
        "full_model_name":"TheBloke\/WizardLM-13B-V1-1-SuperHOT-8K-fp16",
        "Parameters":13.0,
        "MMLU_average":0.483231746,
        "arc:challenge|25":0.5546075085,
        "hellaswag|10":0.612129058,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.5460526316,
        "MMLU_business_ethics":0.47,
        "MMLU_clinical_knowledge":0.5622641509,
        "MMLU_college_biology":0.5,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.4277456647,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.6,
        "MMLU_conceptual_physics":0.4042553191,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.2883597884,
        "MMLU_formal_logic":0.3333333333,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.5580645161,
        "MMLU_high_school_chemistry":0.3743842365,
        "MMLU_high_school_computer_science":0.46,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.6313131313,
        "MMLU_high_school_government_and_politics":0.6839378238,
        "MMLU_high_school_macroeconomics":0.4564102564,
        "MMLU_high_school_mathematics":0.2296296296,
        "MMLU_high_school_microeconomics":0.4663865546,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.671559633,
        "MMLU_high_school_statistics":0.3935185185,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.611814346,
        "MMLU_human_aging":0.5470852018,
        "MMLU_human_sexuality":0.5877862595,
        "MMLU_international_law":0.5867768595,
        "MMLU_jurisprudence":0.5648148148,
        "MMLU_logical_fallacies":0.5398773006,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.6990291262,
        "MMLU_marketing":0.7435897436,
        "MMLU_medical_genetics":0.53,
        "MMLU_miscellaneous":0.6781609195,
        "MMLU_moral_disputes":0.5664739884,
        "MMLU_moral_scenarios":0.2558659218,
        "MMLU_nutrition":0.5882352941,
        "MMLU_philosophy":0.5627009646,
        "MMLU_prehistory":0.549382716,
        "MMLU_professional_accounting":0.390070922,
        "MMLU_professional_law":0.4015645372,
        "MMLU_professional_medicine":0.5036764706,
        "MMLU_professional_psychology":0.5081699346,
        "MMLU_public_relations":0.5545454545,
        "MMLU_security_studies":0.5836734694,
        "MMLU_sociology":0.7064676617,
        "MMLU_us_foreign_policy":0.75,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.7134502924
    },
    {
        "Model":"Llama-2-7b-chat-hf",
        "URL":"https:\/\/huggingface.co\/meta-llama\/Llama-2-7b-chat-hf",
        "full_model_name":"meta-llama\/Llama-2-7b-chat-hf",
        "Parameters":7.0,
        "MMLU_average":0.4831882553,
        "arc:challenge|25":0.4948805461,
        "hellaswag|10":0.5978888668,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.4296296296,
        "MMLU_astronomy":0.4868421053,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.5358490566,
        "MMLU_college_biology":0.5208333333,
        "MMLU_college_chemistry":0.29,
        "MMLU_college_computer_science":0.38,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.3988439306,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.58,
        "MMLU_conceptual_physics":0.4085106383,
        "MMLU_econometrics":0.3771929825,
        "MMLU_electrical_engineering":0.4896551724,
        "MMLU_elementary_mathematics":0.2989417989,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.5225806452,
        "MMLU_high_school_chemistry":0.3645320197,
        "MMLU_high_school_computer_science":0.41,
        "MMLU_high_school_european_history":0.5878787879,
        "MMLU_high_school_geography":0.6060606061,
        "MMLU_high_school_government_and_politics":0.7150259067,
        "MMLU_high_school_macroeconomics":0.4256410256,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.4243697479,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.6752293578,
        "MMLU_high_school_statistics":0.3333333333,
        "MMLU_high_school_us_history":0.6764705882,
        "MMLU_high_school_world_history":0.6666666667,
        "MMLU_human_aging":0.5605381166,
        "MMLU_human_sexuality":0.572519084,
        "MMLU_international_law":0.6280991736,
        "MMLU_jurisprudence":0.5833333333,
        "MMLU_logical_fallacies":0.5521472393,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.6699029126,
        "MMLU_marketing":0.7094017094,
        "MMLU_medical_genetics":0.5,
        "MMLU_miscellaneous":0.6756066411,
        "MMLU_moral_disputes":0.5144508671,
        "MMLU_moral_scenarios":0.2201117318,
        "MMLU_nutrition":0.5196078431,
        "MMLU_philosophy":0.5659163987,
        "MMLU_prehistory":0.5679012346,
        "MMLU_professional_accounting":0.3723404255,
        "MMLU_professional_law":0.350065189,
        "MMLU_professional_medicine":0.4558823529,
        "MMLU_professional_psychology":0.4803921569,
        "MMLU_public_relations":0.5272727273,
        "MMLU_security_studies":0.5265306122,
        "MMLU_sociology":0.6467661692,
        "MMLU_us_foreign_policy":0.72,
        "MMLU_virology":0.4337349398,
        "MMLU_world_religions":0.7251461988
    },
    {
        "Model":"Llama2-7b-openorca-mc-v2",
        "URL":"https:\/\/huggingface.co\/beaugogh\/Llama2-7b-openorca-mc-v2",
        "full_model_name":"beaugogh\/Llama2-7b-openorca-mc-v2",
        "Parameters":7.0,
        "MMLU_average":0.4829891514,
        "arc:challenge|25":0.5145051195,
        "hellaswag|10":0.6220872336,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.4736842105,
        "MMLU_business_ethics":0.5,
        "MMLU_clinical_knowledge":0.5283018868,
        "MMLU_college_biology":0.4791666667,
        "MMLU_college_chemistry":0.36,
        "MMLU_college_computer_science":0.4,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.4277456647,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.56,
        "MMLU_conceptual_physics":0.4553191489,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.4827586207,
        "MMLU_elementary_mathematics":0.3201058201,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.5225806452,
        "MMLU_high_school_chemistry":0.3645320197,
        "MMLU_high_school_computer_science":0.4,
        "MMLU_high_school_european_history":0.5939393939,
        "MMLU_high_school_geography":0.5808080808,
        "MMLU_high_school_government_and_politics":0.7098445596,
        "MMLU_high_school_macroeconomics":0.4564102564,
        "MMLU_high_school_mathematics":0.3074074074,
        "MMLU_high_school_microeconomics":0.474789916,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.6660550459,
        "MMLU_high_school_statistics":0.3055555556,
        "MMLU_high_school_us_history":0.6617647059,
        "MMLU_high_school_world_history":0.7046413502,
        "MMLU_human_aging":0.5560538117,
        "MMLU_human_sexuality":0.5801526718,
        "MMLU_international_law":0.6033057851,
        "MMLU_jurisprudence":0.5277777778,
        "MMLU_logical_fallacies":0.509202454,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.6213592233,
        "MMLU_marketing":0.7264957265,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.662835249,
        "MMLU_moral_disputes":0.5115606936,
        "MMLU_moral_scenarios":0.2402234637,
        "MMLU_nutrition":0.4575163399,
        "MMLU_philosophy":0.5659163987,
        "MMLU_prehistory":0.524691358,
        "MMLU_professional_accounting":0.390070922,
        "MMLU_professional_law":0.3767926988,
        "MMLU_professional_medicine":0.5183823529,
        "MMLU_professional_psychology":0.4738562092,
        "MMLU_public_relations":0.6090909091,
        "MMLU_security_studies":0.506122449,
        "MMLU_sociology":0.6517412935,
        "MMLU_us_foreign_policy":0.67,
        "MMLU_virology":0.3795180723,
        "MMLU_world_religions":0.6666666667
    },
    {
        "Model":"guanaco-13B-HF",
        "URL":"https:\/\/huggingface.co\/TheBloke\/guanaco-13B-HF",
        "full_model_name":"TheBloke\/guanaco-13B-HF",
        "Parameters":13.0,
        "MMLU_average":0.4828075886,
        "arc:challenge|25":0.5486348123,
        "hellaswag|10":0.6353316072,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.4934210526,
        "MMLU_business_ethics":0.48,
        "MMLU_clinical_knowledge":0.479245283,
        "MMLU_college_biology":0.4513888889,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.3930635838,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.62,
        "MMLU_conceptual_physics":0.4042553191,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.4413793103,
        "MMLU_elementary_mathematics":0.2777777778,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.5387096774,
        "MMLU_high_school_chemistry":0.354679803,
        "MMLU_high_school_computer_science":0.48,
        "MMLU_high_school_european_history":0.5757575758,
        "MMLU_high_school_geography":0.5757575758,
        "MMLU_high_school_government_and_politics":0.6632124352,
        "MMLU_high_school_macroeconomics":0.4435897436,
        "MMLU_high_school_mathematics":0.2925925926,
        "MMLU_high_school_microeconomics":0.4831932773,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.6330275229,
        "MMLU_high_school_statistics":0.3148148148,
        "MMLU_high_school_us_history":0.5882352941,
        "MMLU_high_school_world_history":0.6835443038,
        "MMLU_human_aging":0.5605381166,
        "MMLU_human_sexuality":0.5954198473,
        "MMLU_international_law":0.6033057851,
        "MMLU_jurisprudence":0.537037037,
        "MMLU_logical_fallacies":0.4785276074,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.6504854369,
        "MMLU_marketing":0.7094017094,
        "MMLU_medical_genetics":0.49,
        "MMLU_miscellaneous":0.6577266922,
        "MMLU_moral_disputes":0.5375722543,
        "MMLU_moral_scenarios":0.287150838,
        "MMLU_nutrition":0.5261437908,
        "MMLU_philosophy":0.540192926,
        "MMLU_prehistory":0.5586419753,
        "MMLU_professional_accounting":0.3865248227,
        "MMLU_professional_law":0.3950456323,
        "MMLU_professional_medicine":0.5477941176,
        "MMLU_professional_psychology":0.4787581699,
        "MMLU_public_relations":0.5727272727,
        "MMLU_security_studies":0.5224489796,
        "MMLU_sociology":0.6218905473,
        "MMLU_us_foreign_policy":0.79,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.6608187135
    },
    {
        "Model":"Nous-Hermes-13B-SuperHOT-8K-fp16",
        "URL":"https:\/\/huggingface.co\/TheBloke\/Nous-Hermes-13B-SuperHOT-8K-fp16",
        "full_model_name":"TheBloke\/Nous-Hermes-13B-SuperHOT-8K-fp16",
        "Parameters":13.0,
        "MMLU_average":0.4823241046,
        "arc:challenge|25":0.5401023891,
        "hellaswag|10":0.6229834694,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.437037037,
        "MMLU_astronomy":0.5,
        "MMLU_business_ethics":0.45,
        "MMLU_clinical_knowledge":0.5283018868,
        "MMLU_college_biology":0.4722222222,
        "MMLU_college_chemistry":0.32,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.450867052,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.65,
        "MMLU_conceptual_physics":0.4127659574,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.4137931034,
        "MMLU_elementary_mathematics":0.2751322751,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.5483870968,
        "MMLU_high_school_chemistry":0.3596059113,
        "MMLU_high_school_computer_science":0.52,
        "MMLU_high_school_european_history":0.2303030303,
        "MMLU_high_school_geography":0.6616161616,
        "MMLU_high_school_government_and_politics":0.6943005181,
        "MMLU_high_school_macroeconomics":0.4333333333,
        "MMLU_high_school_mathematics":0.237037037,
        "MMLU_high_school_microeconomics":0.4957983193,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.6917431193,
        "MMLU_high_school_statistics":0.3194444444,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.6793248945,
        "MMLU_human_aging":0.5919282511,
        "MMLU_human_sexuality":0.6030534351,
        "MMLU_international_law":0.6363636364,
        "MMLU_jurisprudence":0.5925925926,
        "MMLU_logical_fallacies":0.5644171779,
        "MMLU_machine_learning":0.4196428571,
        "MMLU_management":0.6893203883,
        "MMLU_marketing":0.7606837607,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.6832694764,
        "MMLU_moral_disputes":0.5462427746,
        "MMLU_moral_scenarios":0.2391061453,
        "MMLU_nutrition":0.5490196078,
        "MMLU_philosophy":0.5241157556,
        "MMLU_prehistory":0.5308641975,
        "MMLU_professional_accounting":0.365248227,
        "MMLU_professional_law":0.3800521512,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.4967320261,
        "MMLU_public_relations":0.5636363636,
        "MMLU_security_studies":0.5428571429,
        "MMLU_sociology":0.7114427861,
        "MMLU_us_foreign_policy":0.78,
        "MMLU_virology":0.4337349398,
        "MMLU_world_religions":0.7076023392
    },
    {
        "Model":"gpt4-x-alpaca",
        "URL":"https:\/\/huggingface.co\/chavinlo\/gpt4-x-alpaca",
        "full_model_name":"chavinlo\/gpt4-x-alpaca",
        "Parameters":null,
        "MMLU_average":0.4818649159,
        "arc:challenge|25":0.5170648464,
        "hellaswag|10":0.601872137,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.5259259259,
        "MMLU_astronomy":0.5131578947,
        "MMLU_business_ethics":0.41,
        "MMLU_clinical_knowledge":0.5320754717,
        "MMLU_college_biology":0.4930555556,
        "MMLU_college_chemistry":0.29,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.4277456647,
        "MMLU_college_physics":0.1764705882,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.4340425532,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4206896552,
        "MMLU_elementary_mathematics":0.2830687831,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.5129032258,
        "MMLU_high_school_chemistry":0.3349753695,
        "MMLU_high_school_computer_science":0.52,
        "MMLU_high_school_european_history":0.5454545455,
        "MMLU_high_school_geography":0.6616161616,
        "MMLU_high_school_government_and_politics":0.6683937824,
        "MMLU_high_school_macroeconomics":0.4717948718,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.4831932773,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.6440366972,
        "MMLU_high_school_statistics":0.3564814815,
        "MMLU_high_school_us_history":0.637254902,
        "MMLU_high_school_world_history":0.6624472574,
        "MMLU_human_aging":0.5381165919,
        "MMLU_human_sexuality":0.534351145,
        "MMLU_international_law":0.6776859504,
        "MMLU_jurisprudence":0.5462962963,
        "MMLU_logical_fallacies":0.5521472393,
        "MMLU_machine_learning":0.4553571429,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.7008547009,
        "MMLU_medical_genetics":0.49,
        "MMLU_miscellaneous":0.6692209451,
        "MMLU_moral_disputes":0.5057803468,
        "MMLU_moral_scenarios":0.2972067039,
        "MMLU_nutrition":0.5032679739,
        "MMLU_philosophy":0.5112540193,
        "MMLU_prehistory":0.5185185185,
        "MMLU_professional_accounting":0.3723404255,
        "MMLU_professional_law":0.3780964798,
        "MMLU_professional_medicine":0.4448529412,
        "MMLU_professional_psychology":0.4591503268,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.4408163265,
        "MMLU_sociology":0.6119402985,
        "MMLU_us_foreign_policy":0.72,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.701754386
    },
    {
        "Model":"llama2-7b-instructmining-orca-40k",
        "URL":"https:\/\/huggingface.co\/yihan6324\/llama2-7b-instructmining-orca-40k",
        "full_model_name":"yihan6324\/llama2-7b-instructmining-orca-40k",
        "Parameters":7.0,
        "MMLU_average":0.4815517302,
        "arc:challenge|25":0.5298634812,
        "hellaswag|10":0.6196972715,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.4802631579,
        "MMLU_business_ethics":0.48,
        "MMLU_clinical_knowledge":0.5547169811,
        "MMLU_college_biology":0.4583333333,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4335260116,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.57,
        "MMLU_conceptual_physics":0.4042553191,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.4206896552,
        "MMLU_elementary_mathematics":0.291005291,
        "MMLU_formal_logic":0.2380952381,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.564516129,
        "MMLU_high_school_chemistry":0.39408867,
        "MMLU_high_school_computer_science":0.44,
        "MMLU_high_school_european_history":0.6121212121,
        "MMLU_high_school_geography":0.6212121212,
        "MMLU_high_school_government_and_politics":0.6528497409,
        "MMLU_high_school_macroeconomics":0.4615384615,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.4831932773,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.6623853211,
        "MMLU_high_school_statistics":0.4351851852,
        "MMLU_high_school_us_history":0.637254902,
        "MMLU_high_school_world_history":0.6582278481,
        "MMLU_human_aging":0.5156950673,
        "MMLU_human_sexuality":0.6030534351,
        "MMLU_international_law":0.6363636364,
        "MMLU_jurisprudence":0.5092592593,
        "MMLU_logical_fallacies":0.4846625767,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.6310679612,
        "MMLU_marketing":0.6752136752,
        "MMLU_medical_genetics":0.5,
        "MMLU_miscellaneous":0.6704980843,
        "MMLU_moral_disputes":0.4710982659,
        "MMLU_moral_scenarios":0.2502793296,
        "MMLU_nutrition":0.5522875817,
        "MMLU_philosophy":0.575562701,
        "MMLU_prehistory":0.5308641975,
        "MMLU_professional_accounting":0.3404255319,
        "MMLU_professional_law":0.3872229465,
        "MMLU_professional_medicine":0.4779411765,
        "MMLU_professional_psychology":0.4395424837,
        "MMLU_public_relations":0.5454545455,
        "MMLU_security_studies":0.6081632653,
        "MMLU_sociology":0.6766169154,
        "MMLU_us_foreign_policy":0.68,
        "MMLU_virology":0.3915662651,
        "MMLU_world_religions":0.6666666667
    },
    {
        "Model":"kuchiki-1.1-l2-7b",
        "URL":"https:\/\/huggingface.co\/zarakiquemparte\/kuchiki-1.1-l2-7b",
        "full_model_name":"zarakiquemparte\/kuchiki-1.1-l2-7b",
        "Parameters":7.0,
        "MMLU_average":0.4813762477,
        "arc:challenge|25":0.5273037543,
        "hellaswag|10":0.5950009958,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.3947368421,
        "MMLU_business_ethics":0.5,
        "MMLU_clinical_knowledge":0.5094339623,
        "MMLU_college_biology":0.4861111111,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.4277456647,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.58,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.291005291,
        "MMLU_formal_logic":0.2698412698,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.5258064516,
        "MMLU_high_school_chemistry":0.3793103448,
        "MMLU_high_school_computer_science":0.42,
        "MMLU_high_school_european_history":0.6060606061,
        "MMLU_high_school_geography":0.6060606061,
        "MMLU_high_school_government_and_politics":0.7098445596,
        "MMLU_high_school_macroeconomics":0.4615384615,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.4201680672,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.6678899083,
        "MMLU_high_school_statistics":0.3333333333,
        "MMLU_high_school_us_history":0.6421568627,
        "MMLU_high_school_world_history":0.6751054852,
        "MMLU_human_aging":0.600896861,
        "MMLU_human_sexuality":0.534351145,
        "MMLU_international_law":0.6363636364,
        "MMLU_jurisprudence":0.537037037,
        "MMLU_logical_fallacies":0.490797546,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.572815534,
        "MMLU_marketing":0.7051282051,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.662835249,
        "MMLU_moral_disputes":0.5202312139,
        "MMLU_moral_scenarios":0.2826815642,
        "MMLU_nutrition":0.4934640523,
        "MMLU_philosophy":0.5723472669,
        "MMLU_prehistory":0.5339506173,
        "MMLU_professional_accounting":0.365248227,
        "MMLU_professional_law":0.3624511082,
        "MMLU_professional_medicine":0.5257352941,
        "MMLU_professional_psychology":0.4395424837,
        "MMLU_public_relations":0.5818181818,
        "MMLU_security_studies":0.5346938776,
        "MMLU_sociology":0.6169154229,
        "MMLU_us_foreign_policy":0.7,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.6959064327
    },
    {
        "Model":"spatial-vicuna-7b-v1.5-LoRA",
        "URL":"https:\/\/huggingface.co\/joehuangx\/spatial-vicuna-7b-v1.5-LoRA",
        "full_model_name":"joehuangx\/spatial-vicuna-7b-v1.5-LoRA",
        "Parameters":7.0,
        "MMLU_average":0.4812736347,
        "arc:challenge|25":0.476109215,
        "hellaswag|10":0.5616411073,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.4210526316,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.5056603774,
        "MMLU_college_biology":0.5,
        "MMLU_college_chemistry":0.27,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4335260116,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.62,
        "MMLU_conceptual_physics":0.4340425532,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.4551724138,
        "MMLU_elementary_mathematics":0.2830687831,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.29,
        "MMLU_high_school_biology":0.5258064516,
        "MMLU_high_school_chemistry":0.3300492611,
        "MMLU_high_school_computer_science":0.47,
        "MMLU_high_school_european_history":0.6484848485,
        "MMLU_high_school_geography":0.6111111111,
        "MMLU_high_school_government_and_politics":0.7046632124,
        "MMLU_high_school_macroeconomics":0.4282051282,
        "MMLU_high_school_mathematics":0.2296296296,
        "MMLU_high_school_microeconomics":0.4327731092,
        "MMLU_high_school_physics":0.2450331126,
        "MMLU_high_school_psychology":0.6568807339,
        "MMLU_high_school_statistics":0.337962963,
        "MMLU_high_school_us_history":0.6666666667,
        "MMLU_high_school_world_history":0.6666666667,
        "MMLU_human_aging":0.6053811659,
        "MMLU_human_sexuality":0.5648854962,
        "MMLU_international_law":0.5785123967,
        "MMLU_jurisprudence":0.537037037,
        "MMLU_logical_fallacies":0.4846625767,
        "MMLU_machine_learning":0.4107142857,
        "MMLU_management":0.6213592233,
        "MMLU_marketing":0.7735042735,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.6309067688,
        "MMLU_moral_disputes":0.4913294798,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.4901960784,
        "MMLU_philosophy":0.5498392283,
        "MMLU_prehistory":0.5339506173,
        "MMLU_professional_accounting":0.3333333333,
        "MMLU_professional_law":0.3565840939,
        "MMLU_professional_medicine":0.4742647059,
        "MMLU_professional_psychology":0.4689542484,
        "MMLU_public_relations":0.5818181818,
        "MMLU_security_studies":0.5510204082,
        "MMLU_sociology":0.6865671642,
        "MMLU_us_foreign_policy":0.75,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.649122807
    },
    {
        "Model":"Llama-2-7b-chat-orcah",
        "URL":"https:\/\/huggingface.co\/julianweng\/Llama-2-7b-chat-orcah",
        "full_model_name":"julianweng\/Llama-2-7b-chat-orcah",
        "Parameters":7.0,
        "MMLU_average":0.4810808106,
        "arc:challenge|25":0.4189419795,
        "hellaswag|10":0.5686118303,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.4296296296,
        "MMLU_astronomy":0.5,
        "MMLU_business_ethics":0.49,
        "MMLU_clinical_knowledge":0.5169811321,
        "MMLU_college_biology":0.4652777778,
        "MMLU_college_chemistry":0.29,
        "MMLU_college_computer_science":0.37,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.450867052,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.56,
        "MMLU_conceptual_physics":0.4255319149,
        "MMLU_econometrics":0.350877193,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.3174603175,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.5612903226,
        "MMLU_high_school_chemistry":0.3743842365,
        "MMLU_high_school_computer_science":0.49,
        "MMLU_high_school_european_history":0.6181818182,
        "MMLU_high_school_geography":0.6111111111,
        "MMLU_high_school_government_and_politics":0.6943005181,
        "MMLU_high_school_macroeconomics":0.4538461538,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.4243697479,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.6642201835,
        "MMLU_high_school_statistics":0.3564814815,
        "MMLU_high_school_us_history":0.6470588235,
        "MMLU_high_school_world_history":0.6540084388,
        "MMLU_human_aging":0.5201793722,
        "MMLU_human_sexuality":0.572519084,
        "MMLU_international_law":0.7190082645,
        "MMLU_jurisprudence":0.5555555556,
        "MMLU_logical_fallacies":0.5644171779,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.7264957265,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.6704980843,
        "MMLU_moral_disputes":0.5,
        "MMLU_moral_scenarios":0.243575419,
        "MMLU_nutrition":0.5424836601,
        "MMLU_philosophy":0.5723472669,
        "MMLU_prehistory":0.5555555556,
        "MMLU_professional_accounting":0.3475177305,
        "MMLU_professional_law":0.3376792699,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.4607843137,
        "MMLU_public_relations":0.5363636364,
        "MMLU_security_studies":0.5142857143,
        "MMLU_sociology":0.6368159204,
        "MMLU_us_foreign_policy":0.66,
        "MMLU_virology":0.4036144578,
        "MMLU_world_religions":0.6783625731
    },
    {
        "Model":"h2ogpt-oasst1-512-30B-HF",
        "URL":"https:\/\/huggingface.co\/TheBloke\/h2ogpt-oasst1-512-30B-HF",
        "full_model_name":"TheBloke\/h2ogpt-oasst1-512-30B-HF",
        "Parameters":30.0,
        "MMLU_average":0.4809360239,
        "arc:challenge|25":0.5273037543,
        "hellaswag|10":0.6114319857,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.4518518519,
        "MMLU_astronomy":0.4934210526,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.4943396226,
        "MMLU_college_biology":0.5069444444,
        "MMLU_college_chemistry":0.28,
        "MMLU_college_computer_science":0.37,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.4450867052,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.57,
        "MMLU_conceptual_physics":0.4,
        "MMLU_econometrics":0.3421052632,
        "MMLU_electrical_engineering":0.4413793103,
        "MMLU_elementary_mathematics":0.3492063492,
        "MMLU_formal_logic":0.2142857143,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.5516129032,
        "MMLU_high_school_chemistry":0.39408867,
        "MMLU_high_school_computer_science":0.46,
        "MMLU_high_school_european_history":0.5696969697,
        "MMLU_high_school_geography":0.6060606061,
        "MMLU_high_school_government_and_politics":0.7305699482,
        "MMLU_high_school_macroeconomics":0.4333333333,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.4243697479,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.6311926606,
        "MMLU_high_school_statistics":0.3518518519,
        "MMLU_high_school_us_history":0.6715686275,
        "MMLU_high_school_world_history":0.7299578059,
        "MMLU_human_aging":0.5829596413,
        "MMLU_human_sexuality":0.4885496183,
        "MMLU_international_law":0.5950413223,
        "MMLU_jurisprudence":0.5462962963,
        "MMLU_logical_fallacies":0.5214723926,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.5533980583,
        "MMLU_marketing":0.7692307692,
        "MMLU_medical_genetics":0.45,
        "MMLU_miscellaneous":0.6653895275,
        "MMLU_moral_disputes":0.5809248555,
        "MMLU_moral_scenarios":0.2972067039,
        "MMLU_nutrition":0.4836601307,
        "MMLU_philosophy":0.5659163987,
        "MMLU_prehistory":0.5740740741,
        "MMLU_professional_accounting":0.3829787234,
        "MMLU_professional_law":0.3754889179,
        "MMLU_professional_medicine":0.4926470588,
        "MMLU_professional_psychology":0.4934640523,
        "MMLU_public_relations":0.5727272727,
        "MMLU_security_studies":0.4530612245,
        "MMLU_sociology":0.592039801,
        "MMLU_us_foreign_policy":0.75,
        "MMLU_virology":0.4096385542,
        "MMLU_world_religions":0.7251461988
    },
    {
        "Model":"Dans-CreepingSenseOfDoom",
        "URL":"https:\/\/huggingface.co\/PocketDoc\/Dans-CreepingSenseOfDoom",
        "full_model_name":"PocketDoc\/Dans-CreepingSenseOfDoom",
        "Parameters":null,
        "MMLU_average":0.4808711047,
        "arc:challenge|25":0.4948805461,
        "hellaswag|10":0.5898227445,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.4148148148,
        "MMLU_astronomy":0.4802631579,
        "MMLU_business_ethics":0.5,
        "MMLU_clinical_knowledge":0.5433962264,
        "MMLU_college_biology":0.3888888889,
        "MMLU_college_chemistry":0.47,
        "MMLU_college_computer_science":0.37,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4797687861,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.56,
        "MMLU_conceptual_physics":0.4127659574,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.4413793103,
        "MMLU_elementary_mathematics":0.3253968254,
        "MMLU_formal_logic":0.2698412698,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.6,
        "MMLU_high_school_chemistry":0.3645320197,
        "MMLU_high_school_computer_science":0.47,
        "MMLU_high_school_european_history":0.6121212121,
        "MMLU_high_school_geography":0.6313131313,
        "MMLU_high_school_government_and_politics":0.6580310881,
        "MMLU_high_school_macroeconomics":0.5128205128,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.4957983193,
        "MMLU_high_school_physics":0.3708609272,
        "MMLU_high_school_psychology":0.6311926606,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.5931372549,
        "MMLU_high_school_world_history":0.5907172996,
        "MMLU_human_aging":0.5156950673,
        "MMLU_human_sexuality":0.5648854962,
        "MMLU_international_law":0.6115702479,
        "MMLU_jurisprudence":0.6388888889,
        "MMLU_logical_fallacies":0.5337423313,
        "MMLU_machine_learning":0.2142857143,
        "MMLU_management":0.6990291262,
        "MMLU_marketing":0.6709401709,
        "MMLU_medical_genetics":0.48,
        "MMLU_miscellaneous":0.6360153257,
        "MMLU_moral_disputes":0.5578034682,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.5392156863,
        "MMLU_philosophy":0.575562701,
        "MMLU_prehistory":0.4783950617,
        "MMLU_professional_accounting":0.2978723404,
        "MMLU_professional_law":0.3963494133,
        "MMLU_professional_medicine":0.5220588235,
        "MMLU_professional_psychology":0.4215686275,
        "MMLU_public_relations":0.4818181818,
        "MMLU_security_studies":0.5591836735,
        "MMLU_sociology":0.6616915423,
        "MMLU_us_foreign_policy":0.64,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.6140350877
    },
    {
        "Model":"llama2_7b_code",
        "URL":"https:\/\/huggingface.co\/itsliupeng\/llama2_7b_code",
        "full_model_name":"itsliupeng\/llama2_7b_code",
        "Parameters":7.0,
        "MMLU_average":0.4805078853,
        "arc:challenge|25":0.4872013652,
        "hellaswag|10":0.5621390161,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.4605263158,
        "MMLU_business_ethics":0.37,
        "MMLU_clinical_knowledge":0.4679245283,
        "MMLU_college_biology":0.4375,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.4219653179,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.56,
        "MMLU_conceptual_physics":0.4340425532,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.3121693122,
        "MMLU_formal_logic":0.3015873016,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.5193548387,
        "MMLU_high_school_chemistry":0.3004926108,
        "MMLU_high_school_computer_science":0.53,
        "MMLU_high_school_european_history":0.6303030303,
        "MMLU_high_school_geography":0.5808080808,
        "MMLU_high_school_government_and_politics":0.6528497409,
        "MMLU_high_school_macroeconomics":0.4179487179,
        "MMLU_high_school_mathematics":0.3111111111,
        "MMLU_high_school_microeconomics":0.4201680672,
        "MMLU_high_school_physics":0.3841059603,
        "MMLU_high_school_psychology":0.6366972477,
        "MMLU_high_school_statistics":0.3009259259,
        "MMLU_high_school_us_history":0.5882352941,
        "MMLU_high_school_world_history":0.6708860759,
        "MMLU_human_aging":0.5112107623,
        "MMLU_human_sexuality":0.5877862595,
        "MMLU_international_law":0.6115702479,
        "MMLU_jurisprudence":0.537037037,
        "MMLU_logical_fallacies":0.527607362,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.6116504854,
        "MMLU_marketing":0.7051282051,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.6309067688,
        "MMLU_moral_disputes":0.549132948,
        "MMLU_moral_scenarios":0.3184357542,
        "MMLU_nutrition":0.5555555556,
        "MMLU_philosophy":0.5916398714,
        "MMLU_prehistory":0.5216049383,
        "MMLU_professional_accounting":0.3475177305,
        "MMLU_professional_law":0.3481095176,
        "MMLU_professional_medicine":0.5036764706,
        "MMLU_professional_psychology":0.4248366013,
        "MMLU_public_relations":0.5181818182,
        "MMLU_security_studies":0.493877551,
        "MMLU_sociology":0.6417910448,
        "MMLU_us_foreign_policy":0.7,
        "MMLU_virology":0.4036144578,
        "MMLU_world_religions":0.6842105263
    },
    {
        "Model":"llama_mirror_13b_v1.0",
        "URL":"https:\/\/huggingface.co\/lizhuang144\/llama_mirror_13b_v1.0",
        "full_model_name":"lizhuang144\/llama_mirror_13b_v1.0",
        "Parameters":13.0,
        "MMLU_average":0.4799791355,
        "arc:challenge|25":0.5614334471,
        "hellaswag|10":0.6033658634,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.5,
        "MMLU_business_ethics":0.45,
        "MMLU_clinical_knowledge":0.4754716981,
        "MMLU_college_biology":0.5,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4335260116,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.65,
        "MMLU_conceptual_physics":0.3744680851,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.535483871,
        "MMLU_high_school_chemistry":0.3103448276,
        "MMLU_high_school_computer_science":0.49,
        "MMLU_high_school_european_history":0.6181818182,
        "MMLU_high_school_geography":0.5808080808,
        "MMLU_high_school_government_and_politics":0.7098445596,
        "MMLU_high_school_macroeconomics":0.4692307692,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.4789915966,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.6183486239,
        "MMLU_high_school_statistics":0.2592592593,
        "MMLU_high_school_us_history":0.6176470588,
        "MMLU_high_school_world_history":0.7004219409,
        "MMLU_human_aging":0.5201793722,
        "MMLU_human_sexuality":0.5496183206,
        "MMLU_international_law":0.5950413223,
        "MMLU_jurisprudence":0.537037037,
        "MMLU_logical_fallacies":0.5521472393,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.6601941748,
        "MMLU_marketing":0.7051282051,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.6462324393,
        "MMLU_moral_disputes":0.5057803468,
        "MMLU_moral_scenarios":0.2636871508,
        "MMLU_nutrition":0.5032679739,
        "MMLU_philosophy":0.5498392283,
        "MMLU_prehistory":0.524691358,
        "MMLU_professional_accounting":0.3581560284,
        "MMLU_professional_law":0.3917861799,
        "MMLU_professional_medicine":0.5147058824,
        "MMLU_professional_psychology":0.4836601307,
        "MMLU_public_relations":0.5272727273,
        "MMLU_security_studies":0.5428571429,
        "MMLU_sociology":0.6616915423,
        "MMLU_us_foreign_policy":0.77,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.6725146199
    },
    {
        "Model":"Llama2-7B-guanaco-1k",
        "URL":"https:\/\/huggingface.co\/mncai\/Llama2-7B-guanaco-1k",
        "full_model_name":"mncai\/Llama2-7B-guanaco-1k",
        "Parameters":7.0,
        "MMLU_average":0.4792895517,
        "arc:challenge|25":0.5204778157,
        "hellaswag|10":0.612129058,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.4144736842,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.5056603774,
        "MMLU_college_biology":0.4722222222,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.4624277457,
        "MMLU_college_physics":0.1764705882,
        "MMLU_computer_security":0.55,
        "MMLU_conceptual_physics":0.4382978723,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.5103448276,
        "MMLU_elementary_mathematics":0.2989417989,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.5225806452,
        "MMLU_high_school_chemistry":0.3645320197,
        "MMLU_high_school_computer_science":0.42,
        "MMLU_high_school_european_history":0.6121212121,
        "MMLU_high_school_geography":0.6111111111,
        "MMLU_high_school_government_and_politics":0.6735751295,
        "MMLU_high_school_macroeconomics":0.4692307692,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.4453781513,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.6697247706,
        "MMLU_high_school_statistics":0.3055555556,
        "MMLU_high_school_us_history":0.5441176471,
        "MMLU_high_school_world_history":0.6371308017,
        "MMLU_human_aging":0.5426008969,
        "MMLU_human_sexuality":0.6030534351,
        "MMLU_international_law":0.6280991736,
        "MMLU_jurisprudence":0.537037037,
        "MMLU_logical_fallacies":0.527607362,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.5825242718,
        "MMLU_marketing":0.7222222222,
        "MMLU_medical_genetics":0.53,
        "MMLU_miscellaneous":0.6475095785,
        "MMLU_moral_disputes":0.5346820809,
        "MMLU_moral_scenarios":0.2983240223,
        "MMLU_nutrition":0.522875817,
        "MMLU_philosophy":0.5884244373,
        "MMLU_prehistory":0.487654321,
        "MMLU_professional_accounting":0.365248227,
        "MMLU_professional_law":0.368970013,
        "MMLU_professional_medicine":0.5367647059,
        "MMLU_professional_psychology":0.4509803922,
        "MMLU_public_relations":0.5363636364,
        "MMLU_security_studies":0.4897959184,
        "MMLU_sociology":0.6666666667,
        "MMLU_us_foreign_policy":0.61,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.7076023392
    },
    {
        "Model":"mpt-30b",
        "URL":"https:\/\/huggingface.co\/mosaicml\/mpt-30b",
        "full_model_name":"mosaicml\/mpt-30b",
        "Parameters":30.0,
        "MMLU_average":0.4792873324,
        "arc:challenge|25":0.5290102389,
        "hellaswag|10":0.6195976897,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.4078947368,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.479245283,
        "MMLU_college_biology":0.5208333333,
        "MMLU_college_chemistry":0.32,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.4450867052,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.59,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.328042328,
        "MMLU_formal_logic":0.2777777778,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.5387096774,
        "MMLU_high_school_chemistry":0.3497536946,
        "MMLU_high_school_computer_science":0.53,
        "MMLU_high_school_european_history":0.6,
        "MMLU_high_school_geography":0.595959596,
        "MMLU_high_school_government_and_politics":0.6424870466,
        "MMLU_high_school_macroeconomics":0.4615384615,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.4369747899,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.6752293578,
        "MMLU_high_school_statistics":0.3796296296,
        "MMLU_high_school_us_history":0.6666666667,
        "MMLU_high_school_world_history":0.6708860759,
        "MMLU_human_aging":0.5426008969,
        "MMLU_human_sexuality":0.5419847328,
        "MMLU_international_law":0.4380165289,
        "MMLU_jurisprudence":0.4537037037,
        "MMLU_logical_fallacies":0.4846625767,
        "MMLU_machine_learning":0.4553571429,
        "MMLU_management":0.5533980583,
        "MMLU_marketing":0.7136752137,
        "MMLU_medical_genetics":0.49,
        "MMLU_miscellaneous":0.687100894,
        "MMLU_moral_disputes":0.5115606936,
        "MMLU_moral_scenarios":0.2670391061,
        "MMLU_nutrition":0.5163398693,
        "MMLU_philosophy":0.5466237942,
        "MMLU_prehistory":0.5648148148,
        "MMLU_professional_accounting":0.3581560284,
        "MMLU_professional_law":0.3780964798,
        "MMLU_professional_medicine":0.3823529412,
        "MMLU_professional_psychology":0.4591503268,
        "MMLU_public_relations":0.5818181818,
        "MMLU_security_studies":0.5510204082,
        "MMLU_sociology":0.5472636816,
        "MMLU_us_foreign_policy":0.67,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.6783625731
    },
    {
        "Model":"Wizard-Vicuna-13B-Uncensored-HF",
        "URL":"https:\/\/huggingface.co\/TheBloke\/Wizard-Vicuna-13B-Uncensored-HF",
        "full_model_name":"TheBloke\/Wizard-Vicuna-13B-Uncensored-HF",
        "Parameters":13.0,
        "MMLU_average":0.4792227876,
        "arc:challenge|25":0.5656996587,
        "hellaswag|10":0.6218880701,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.4736842105,
        "MMLU_business_ethics":0.49,
        "MMLU_clinical_knowledge":0.4716981132,
        "MMLU_college_biology":0.4861111111,
        "MMLU_college_chemistry":0.3,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.4046242775,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.59,
        "MMLU_conceptual_physics":0.4170212766,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.4,
        "MMLU_elementary_mathematics":0.253968254,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.5193548387,
        "MMLU_high_school_chemistry":0.3201970443,
        "MMLU_high_school_computer_science":0.52,
        "MMLU_high_school_european_history":0.6242424242,
        "MMLU_high_school_geography":0.5555555556,
        "MMLU_high_school_government_and_politics":0.6632124352,
        "MMLU_high_school_macroeconomics":0.4230769231,
        "MMLU_high_school_mathematics":0.2111111111,
        "MMLU_high_school_microeconomics":0.4411764706,
        "MMLU_high_school_physics":0.2251655629,
        "MMLU_high_school_psychology":0.623853211,
        "MMLU_high_school_statistics":0.2314814815,
        "MMLU_high_school_us_history":0.6519607843,
        "MMLU_high_school_world_history":0.6919831224,
        "MMLU_human_aging":0.5695067265,
        "MMLU_human_sexuality":0.5954198473,
        "MMLU_international_law":0.6694214876,
        "MMLU_jurisprudence":0.5462962963,
        "MMLU_logical_fallacies":0.5460122699,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.6019417476,
        "MMLU_marketing":0.764957265,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.6768837803,
        "MMLU_moral_disputes":0.5086705202,
        "MMLU_moral_scenarios":0.2480446927,
        "MMLU_nutrition":0.5,
        "MMLU_philosophy":0.5530546624,
        "MMLU_prehistory":0.537037037,
        "MMLU_professional_accounting":0.3617021277,
        "MMLU_professional_law":0.4054758801,
        "MMLU_professional_medicine":0.4926470588,
        "MMLU_professional_psychology":0.5081699346,
        "MMLU_public_relations":0.5272727273,
        "MMLU_security_studies":0.5306122449,
        "MMLU_sociology":0.6368159204,
        "MMLU_us_foreign_policy":0.77,
        "MMLU_virology":0.4518072289,
        "MMLU_world_religions":0.6900584795
    },
    {
        "Model":"Wizard-Vicuna-13B-Uncensored",
        "URL":"https:\/\/huggingface.co\/ehartford\/Wizard-Vicuna-13B-Uncensored",
        "full_model_name":"ehartford\/Wizard-Vicuna-13B-Uncensored",
        "Parameters":13.0,
        "MMLU_average":0.4792227876,
        "arc:challenge|25":0.5656996587,
        "hellaswag|10":0.6218880701,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.4736842105,
        "MMLU_business_ethics":0.49,
        "MMLU_clinical_knowledge":0.4716981132,
        "MMLU_college_biology":0.4861111111,
        "MMLU_college_chemistry":0.3,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.4046242775,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.59,
        "MMLU_conceptual_physics":0.4170212766,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.4,
        "MMLU_elementary_mathematics":0.253968254,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.5193548387,
        "MMLU_high_school_chemistry":0.3201970443,
        "MMLU_high_school_computer_science":0.52,
        "MMLU_high_school_european_history":0.6242424242,
        "MMLU_high_school_geography":0.5555555556,
        "MMLU_high_school_government_and_politics":0.6632124352,
        "MMLU_high_school_macroeconomics":0.4230769231,
        "MMLU_high_school_mathematics":0.2111111111,
        "MMLU_high_school_microeconomics":0.4411764706,
        "MMLU_high_school_physics":0.2251655629,
        "MMLU_high_school_psychology":0.623853211,
        "MMLU_high_school_statistics":0.2314814815,
        "MMLU_high_school_us_history":0.6519607843,
        "MMLU_high_school_world_history":0.6919831224,
        "MMLU_human_aging":0.5695067265,
        "MMLU_human_sexuality":0.5954198473,
        "MMLU_international_law":0.6694214876,
        "MMLU_jurisprudence":0.5462962963,
        "MMLU_logical_fallacies":0.5460122699,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.6019417476,
        "MMLU_marketing":0.764957265,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.6768837803,
        "MMLU_moral_disputes":0.5086705202,
        "MMLU_moral_scenarios":0.2480446927,
        "MMLU_nutrition":0.5,
        "MMLU_philosophy":0.5530546624,
        "MMLU_prehistory":0.537037037,
        "MMLU_professional_accounting":0.3617021277,
        "MMLU_professional_law":0.4054758801,
        "MMLU_professional_medicine":0.4926470588,
        "MMLU_professional_psychology":0.5081699346,
        "MMLU_public_relations":0.5272727273,
        "MMLU_security_studies":0.5306122449,
        "MMLU_sociology":0.6368159204,
        "MMLU_us_foreign_policy":0.77,
        "MMLU_virology":0.4518072289,
        "MMLU_world_religions":0.6900584795
    },
    {
        "Model":"Alpacino13b",
        "URL":"https:\/\/huggingface.co\/digitous\/Alpacino13b",
        "full_model_name":"digitous\/Alpacino13b",
        "Parameters":13.0,
        "MMLU_average":0.4791768276,
        "arc:challenge|25":0.5503412969,
        "hellaswag|10":0.6141206931,
        "MMLU_abstract_algebra":0.38,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.4342105263,
        "MMLU_business_ethics":0.47,
        "MMLU_clinical_knowledge":0.4716981132,
        "MMLU_college_biology":0.4652777778,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.4335260116,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.429787234,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.3931034483,
        "MMLU_elementary_mathematics":0.246031746,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.5161290323,
        "MMLU_high_school_chemistry":0.3054187192,
        "MMLU_high_school_computer_science":0.49,
        "MMLU_high_school_european_history":0.6121212121,
        "MMLU_high_school_geography":0.5303030303,
        "MMLU_high_school_government_and_politics":0.6476683938,
        "MMLU_high_school_macroeconomics":0.4384615385,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.4705882353,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.6293577982,
        "MMLU_high_school_statistics":0.2731481481,
        "MMLU_high_school_us_history":0.5931372549,
        "MMLU_high_school_world_history":0.6877637131,
        "MMLU_human_aging":0.5650224215,
        "MMLU_human_sexuality":0.5648854962,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.537037037,
        "MMLU_logical_fallacies":0.5644171779,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.6504854369,
        "MMLU_marketing":0.7264957265,
        "MMLU_medical_genetics":0.51,
        "MMLU_miscellaneous":0.6666666667,
        "MMLU_moral_disputes":0.5144508671,
        "MMLU_moral_scenarios":0.243575419,
        "MMLU_nutrition":0.5130718954,
        "MMLU_philosophy":0.5434083601,
        "MMLU_prehistory":0.5339506173,
        "MMLU_professional_accounting":0.3510638298,
        "MMLU_professional_law":0.371577575,
        "MMLU_professional_medicine":0.5147058824,
        "MMLU_professional_psychology":0.4852941176,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.5387755102,
        "MMLU_sociology":0.6567164179,
        "MMLU_us_foreign_policy":0.76,
        "MMLU_virology":0.4397590361,
        "MMLU_world_religions":0.6666666667
    },
    {
        "Model":"zarablend-1.1-l2-7b",
        "URL":"https:\/\/huggingface.co\/zarakiquemparte\/zarablend-1.1-l2-7b",
        "full_model_name":"zarakiquemparte\/zarablend-1.1-l2-7b",
        "Parameters":7.0,
        "MMLU_average":0.4789250731,
        "arc:challenge|25":0.5221843003,
        "hellaswag|10":0.6020713005,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.5185185185,
        "MMLU_astronomy":0.3881578947,
        "MMLU_business_ethics":0.48,
        "MMLU_clinical_knowledge":0.5094339623,
        "MMLU_college_biology":0.5,
        "MMLU_college_chemistry":0.36,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4450867052,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.58,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.4551724138,
        "MMLU_elementary_mathematics":0.2962962963,
        "MMLU_formal_logic":0.2619047619,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.5064516129,
        "MMLU_high_school_chemistry":0.3694581281,
        "MMLU_high_school_computer_science":0.42,
        "MMLU_high_school_european_history":0.6,
        "MMLU_high_school_geography":0.5757575758,
        "MMLU_high_school_government_and_politics":0.6943005181,
        "MMLU_high_school_macroeconomics":0.4538461538,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.4369747899,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.6660550459,
        "MMLU_high_school_statistics":0.3101851852,
        "MMLU_high_school_us_history":0.6323529412,
        "MMLU_high_school_world_history":0.6624472574,
        "MMLU_human_aging":0.5829596413,
        "MMLU_human_sexuality":0.5496183206,
        "MMLU_international_law":0.6446280992,
        "MMLU_jurisprudence":0.5462962963,
        "MMLU_logical_fallacies":0.4969325153,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.5922330097,
        "MMLU_marketing":0.7136752137,
        "MMLU_medical_genetics":0.53,
        "MMLU_miscellaneous":0.6679438059,
        "MMLU_moral_disputes":0.5144508671,
        "MMLU_moral_scenarios":0.2994413408,
        "MMLU_nutrition":0.5,
        "MMLU_philosophy":0.578778135,
        "MMLU_prehistory":0.5277777778,
        "MMLU_professional_accounting":0.3581560284,
        "MMLU_professional_law":0.3559322034,
        "MMLU_professional_medicine":0.5073529412,
        "MMLU_professional_psychology":0.4281045752,
        "MMLU_public_relations":0.5727272727,
        "MMLU_security_studies":0.5551020408,
        "MMLU_sociology":0.6119402985,
        "MMLU_us_foreign_policy":0.66,
        "MMLU_virology":0.4397590361,
        "MMLU_world_religions":0.6783625731
    },
    {
        "Model":"instruct-13b",
        "URL":"https:\/\/huggingface.co\/llama-anon\/instruct-13b",
        "full_model_name":"llama-anon\/instruct-13b",
        "Parameters":13.0,
        "MMLU_average":0.4789212994,
        "arc:challenge|25":0.5401023891,
        "hellaswag|10":0.6188010357,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.4868421053,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.4981132075,
        "MMLU_college_biology":0.5138888889,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.4161849711,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.56,
        "MMLU_conceptual_physics":0.4085106383,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.4896551724,
        "MMLU_elementary_mathematics":0.2592592593,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.27,
        "MMLU_high_school_biology":0.5709677419,
        "MMLU_high_school_chemistry":0.3103448276,
        "MMLU_high_school_computer_science":0.48,
        "MMLU_high_school_european_history":0.6363636364,
        "MMLU_high_school_geography":0.6363636364,
        "MMLU_high_school_government_and_politics":0.6632124352,
        "MMLU_high_school_macroeconomics":0.4205128205,
        "MMLU_high_school_mathematics":0.2185185185,
        "MMLU_high_school_microeconomics":0.487394958,
        "MMLU_high_school_physics":0.238410596,
        "MMLU_high_school_psychology":0.6770642202,
        "MMLU_high_school_statistics":0.3148148148,
        "MMLU_high_school_us_history":0.6323529412,
        "MMLU_high_school_world_history":0.6075949367,
        "MMLU_human_aging":0.5246636771,
        "MMLU_human_sexuality":0.5419847328,
        "MMLU_international_law":0.6446280992,
        "MMLU_jurisprudence":0.5555555556,
        "MMLU_logical_fallacies":0.6012269939,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.6504854369,
        "MMLU_marketing":0.764957265,
        "MMLU_medical_genetics":0.46,
        "MMLU_miscellaneous":0.6730523627,
        "MMLU_moral_disputes":0.5144508671,
        "MMLU_moral_scenarios":0.2446927374,
        "MMLU_nutrition":0.4803921569,
        "MMLU_philosophy":0.5209003215,
        "MMLU_prehistory":0.5401234568,
        "MMLU_professional_accounting":0.3687943262,
        "MMLU_professional_law":0.3917861799,
        "MMLU_professional_medicine":0.4742647059,
        "MMLU_professional_psychology":0.4836601307,
        "MMLU_public_relations":0.5727272727,
        "MMLU_security_studies":0.4816326531,
        "MMLU_sociology":0.5721393035,
        "MMLU_us_foreign_policy":0.69,
        "MMLU_virology":0.4337349398,
        "MMLU_world_religions":0.6666666667
    },
    {
        "Model":"airoboros-13b-gpt4",
        "URL":"https:\/\/huggingface.co\/jondurbin\/airoboros-13b-gpt4",
        "full_model_name":"jondurbin\/airoboros-13b-gpt4",
        "Parameters":13.0,
        "MMLU_average":0.4788532537,
        "arc:challenge|25":0.566552901,
        "hellaswag|10":0.6411073491,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.4736842105,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.5018867925,
        "MMLU_college_biology":0.4861111111,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.4393063584,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.3957446809,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.5451612903,
        "MMLU_high_school_chemistry":0.3497536946,
        "MMLU_high_school_computer_science":0.51,
        "MMLU_high_school_european_history":0.2363636364,
        "MMLU_high_school_geography":0.5353535354,
        "MMLU_high_school_government_and_politics":0.6528497409,
        "MMLU_high_school_macroeconomics":0.4692307692,
        "MMLU_high_school_mathematics":0.237037037,
        "MMLU_high_school_microeconomics":0.4705882353,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.6458715596,
        "MMLU_high_school_statistics":0.2962962963,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.7257383966,
        "MMLU_human_aging":0.5515695067,
        "MMLU_human_sexuality":0.572519084,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.5277777778,
        "MMLU_logical_fallacies":0.5766871166,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.6893203883,
        "MMLU_marketing":0.7393162393,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.6794380587,
        "MMLU_moral_disputes":0.5144508671,
        "MMLU_moral_scenarios":0.2636871508,
        "MMLU_nutrition":0.5359477124,
        "MMLU_philosophy":0.5434083601,
        "MMLU_prehistory":0.5462962963,
        "MMLU_professional_accounting":0.3581560284,
        "MMLU_professional_law":0.3963494133,
        "MMLU_professional_medicine":0.5257352941,
        "MMLU_professional_psychology":0.522875817,
        "MMLU_public_relations":0.5636363636,
        "MMLU_security_studies":0.5142857143,
        "MMLU_sociology":0.6368159204,
        "MMLU_us_foreign_policy":0.81,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.6842105263
    },
    {
        "Model":"Manticore-13b-Chat-Pyg-Guanaco",
        "URL":"https:\/\/huggingface.co\/Monero\/Manticore-13b-Chat-Pyg-Guanaco",
        "full_model_name":"Monero\/Manticore-13b-Chat-Pyg-Guanaco",
        "Parameters":13.0,
        "MMLU_average":0.4780930464,
        "arc:challenge|25":0.5290102389,
        "hellaswag|10":0.6149173471,
        "MMLU_abstract_algebra":0.4,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.4736842105,
        "MMLU_business_ethics":0.45,
        "MMLU_clinical_knowledge":0.5056603774,
        "MMLU_college_biology":0.5069444444,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.3757225434,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.6,
        "MMLU_conceptual_physics":0.4127659574,
        "MMLU_econometrics":0.350877193,
        "MMLU_electrical_engineering":0.3793103448,
        "MMLU_elementary_mathematics":0.291005291,
        "MMLU_formal_logic":0.3015873016,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.5129032258,
        "MMLU_high_school_chemistry":0.3448275862,
        "MMLU_high_school_computer_science":0.49,
        "MMLU_high_school_european_history":0.5515151515,
        "MMLU_high_school_geography":0.5808080808,
        "MMLU_high_school_government_and_politics":0.6839378238,
        "MMLU_high_school_macroeconomics":0.4128205128,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.487394958,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.6293577982,
        "MMLU_high_school_statistics":0.3009259259,
        "MMLU_high_school_us_history":0.6323529412,
        "MMLU_high_school_world_history":0.6371308017,
        "MMLU_human_aging":0.5919282511,
        "MMLU_human_sexuality":0.5267175573,
        "MMLU_international_law":0.5702479339,
        "MMLU_jurisprudence":0.6018518519,
        "MMLU_logical_fallacies":0.5214723926,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.640776699,
        "MMLU_marketing":0.7222222222,
        "MMLU_medical_genetics":0.47,
        "MMLU_miscellaneous":0.6551724138,
        "MMLU_moral_disputes":0.5317919075,
        "MMLU_moral_scenarios":0.2625698324,
        "MMLU_nutrition":0.4705882353,
        "MMLU_philosophy":0.5080385852,
        "MMLU_prehistory":0.5648148148,
        "MMLU_professional_accounting":0.4078014184,
        "MMLU_professional_law":0.387874837,
        "MMLU_professional_medicine":0.4816176471,
        "MMLU_professional_psychology":0.4640522876,
        "MMLU_public_relations":0.5636363636,
        "MMLU_security_studies":0.5224489796,
        "MMLU_sociology":0.5671641791,
        "MMLU_us_foreign_policy":0.76,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.6842105263
    },
    {
        "Model":"gpt4-alpaca-lora-13b-decapoda-1024",
        "URL":"https:\/\/huggingface.co\/chansung\/gpt4-alpaca-lora-13b-decapoda-1024",
        "full_model_name":"chansung\/gpt4-alpaca-lora-13b-decapoda-1024",
        "Parameters":13.0,
        "MMLU_average":0.4775150834,
        "arc:challenge|25":0.5682593857,
        "hellaswag|10":0.6201951802,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.4210526316,
        "MMLU_business_ethics":0.44,
        "MMLU_clinical_knowledge":0.5018867925,
        "MMLU_college_biology":0.4513888889,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.37,
        "MMLU_college_mathematics":0.41,
        "MMLU_college_medicine":0.3988439306,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.59,
        "MMLU_conceptual_physics":0.3914893617,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.3655172414,
        "MMLU_elementary_mathematics":0.2486772487,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.535483871,
        "MMLU_high_school_chemistry":0.315270936,
        "MMLU_high_school_computer_science":0.5,
        "MMLU_high_school_european_history":0.6484848485,
        "MMLU_high_school_geography":0.5353535354,
        "MMLU_high_school_government_and_politics":0.6683937824,
        "MMLU_high_school_macroeconomics":0.4358974359,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.474789916,
        "MMLU_high_school_physics":0.238410596,
        "MMLU_high_school_psychology":0.6293577982,
        "MMLU_high_school_statistics":0.3009259259,
        "MMLU_high_school_us_history":0.5980392157,
        "MMLU_high_school_world_history":0.6877637131,
        "MMLU_human_aging":0.5605381166,
        "MMLU_human_sexuality":0.5648854962,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.5,
        "MMLU_logical_fallacies":0.5214723926,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.640776699,
        "MMLU_marketing":0.7393162393,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.6666666667,
        "MMLU_moral_disputes":0.5260115607,
        "MMLU_moral_scenarios":0.2491620112,
        "MMLU_nutrition":0.5163398693,
        "MMLU_philosophy":0.5594855305,
        "MMLU_prehistory":0.5277777778,
        "MMLU_professional_accounting":0.365248227,
        "MMLU_professional_law":0.3709256845,
        "MMLU_professional_medicine":0.5147058824,
        "MMLU_professional_psychology":0.4803921569,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.5306122449,
        "MMLU_sociology":0.6368159204,
        "MMLU_us_foreign_policy":0.74,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.6959064327
    },
    {
        "Model":"MetaMath-13B-V1.0",
        "URL":"https:\/\/huggingface.co\/meta-math\/MetaMath-13B-V1.0",
        "full_model_name":"meta-math\/MetaMath-13B-V1.0",
        "Parameters":13.0,
        "MMLU_average":0.4774362391,
        "arc:challenge|25":0.4667235495,
        "hellaswag|10":0.5875323641,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.4518518519,
        "MMLU_astronomy":0.4605263158,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.5245283019,
        "MMLU_college_biology":0.5277777778,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.28,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.3930635838,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.4,
        "MMLU_econometrics":0.350877193,
        "MMLU_electrical_engineering":0.4206896552,
        "MMLU_elementary_mathematics":0.3492063492,
        "MMLU_formal_logic":0.2619047619,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.5548387097,
        "MMLU_high_school_chemistry":0.3842364532,
        "MMLU_high_school_computer_science":0.44,
        "MMLU_high_school_european_history":0.6060606061,
        "MMLU_high_school_geography":0.6717171717,
        "MMLU_high_school_government_and_politics":0.7150259067,
        "MMLU_high_school_macroeconomics":0.4435897436,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.474789916,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.671559633,
        "MMLU_high_school_statistics":0.3518518519,
        "MMLU_high_school_us_history":0.6862745098,
        "MMLU_high_school_world_history":0.6160337553,
        "MMLU_human_aging":0.5695067265,
        "MMLU_human_sexuality":0.5038167939,
        "MMLU_international_law":0.6198347107,
        "MMLU_jurisprudence":0.6018518519,
        "MMLU_logical_fallacies":0.5153374233,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.6601941748,
        "MMLU_marketing":0.7264957265,
        "MMLU_medical_genetics":0.48,
        "MMLU_miscellaneous":0.6845466156,
        "MMLU_moral_disputes":0.549132948,
        "MMLU_moral_scenarios":0.3072625698,
        "MMLU_nutrition":0.4738562092,
        "MMLU_philosophy":0.5466237942,
        "MMLU_prehistory":0.5432098765,
        "MMLU_professional_accounting":0.304964539,
        "MMLU_professional_law":0.3331160365,
        "MMLU_professional_medicine":0.3676470588,
        "MMLU_professional_psychology":0.4346405229,
        "MMLU_public_relations":0.6,
        "MMLU_security_studies":0.4653061224,
        "MMLU_sociology":0.6417910448,
        "MMLU_us_foreign_policy":0.73,
        "MMLU_virology":0.4337349398,
        "MMLU_world_religions":0.6842105263
    },
    {
        "Model":"kuchiki-l2-7b",
        "URL":"https:\/\/huggingface.co\/zarakiquemparte\/kuchiki-l2-7b",
        "full_model_name":"zarakiquemparte\/kuchiki-l2-7b",
        "Parameters":7.0,
        "MMLU_average":0.4774331684,
        "arc:challenge|25":0.5255972696,
        "hellaswag|10":0.6005775742,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.3881578947,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.5056603774,
        "MMLU_college_biology":0.5,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.37,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4450867052,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.6,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.4827586207,
        "MMLU_elementary_mathematics":0.2962962963,
        "MMLU_formal_logic":0.2619047619,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.5096774194,
        "MMLU_high_school_chemistry":0.3596059113,
        "MMLU_high_school_computer_science":0.42,
        "MMLU_high_school_european_history":0.6181818182,
        "MMLU_high_school_geography":0.5909090909,
        "MMLU_high_school_government_and_politics":0.7046632124,
        "MMLU_high_school_macroeconomics":0.4615384615,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.4075630252,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.6678899083,
        "MMLU_high_school_statistics":0.3055555556,
        "MMLU_high_school_us_history":0.6421568627,
        "MMLU_high_school_world_history":0.6540084388,
        "MMLU_human_aging":0.5829596413,
        "MMLU_human_sexuality":0.5267175573,
        "MMLU_international_law":0.6363636364,
        "MMLU_jurisprudence":0.5277777778,
        "MMLU_logical_fallacies":0.5030674847,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.5533980583,
        "MMLU_marketing":0.7051282051,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.6602809706,
        "MMLU_moral_disputes":0.5173410405,
        "MMLU_moral_scenarios":0.3005586592,
        "MMLU_nutrition":0.4967320261,
        "MMLU_philosophy":0.578778135,
        "MMLU_prehistory":0.5277777778,
        "MMLU_professional_accounting":0.3617021277,
        "MMLU_professional_law":0.3546284224,
        "MMLU_professional_medicine":0.5110294118,
        "MMLU_professional_psychology":0.4460784314,
        "MMLU_public_relations":0.5818181818,
        "MMLU_security_studies":0.506122449,
        "MMLU_sociology":0.631840796,
        "MMLU_us_foreign_policy":0.69,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.6900584795
    },
    {
        "Model":"openbuddy-openllama-13b-v7-fp16",
        "URL":"https:\/\/huggingface.co\/OpenBuddy\/openbuddy-openllama-13b-v7-fp16",
        "full_model_name":"OpenBuddy\/openbuddy-openllama-13b-v7-fp16",
        "Parameters":13.0,
        "MMLU_average":0.4774221837,
        "arc:challenge|25":0.4462457338,
        "hellaswag|10":0.5474009162,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.5131578947,
        "MMLU_business_ethics":0.39,
        "MMLU_clinical_knowledge":0.4867924528,
        "MMLU_college_biology":0.4791666667,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.4219653179,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.62,
        "MMLU_conceptual_physics":0.3872340426,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.4827586207,
        "MMLU_elementary_mathematics":0.2751322751,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.5258064516,
        "MMLU_high_school_chemistry":0.3448275862,
        "MMLU_high_school_computer_science":0.46,
        "MMLU_high_school_european_history":0.6060606061,
        "MMLU_high_school_geography":0.6060606061,
        "MMLU_high_school_government_and_politics":0.6683937824,
        "MMLU_high_school_macroeconomics":0.4205128205,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.4159663866,
        "MMLU_high_school_physics":0.357615894,
        "MMLU_high_school_psychology":0.6055045872,
        "MMLU_high_school_statistics":0.3055555556,
        "MMLU_high_school_us_history":0.6225490196,
        "MMLU_high_school_world_history":0.5864978903,
        "MMLU_human_aging":0.5739910314,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.537037037,
        "MMLU_logical_fallacies":0.6134969325,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.6019417476,
        "MMLU_marketing":0.7222222222,
        "MMLU_medical_genetics":0.59,
        "MMLU_miscellaneous":0.6653895275,
        "MMLU_moral_disputes":0.5,
        "MMLU_moral_scenarios":0.2413407821,
        "MMLU_nutrition":0.5326797386,
        "MMLU_philosophy":0.5273311897,
        "MMLU_prehistory":0.5462962963,
        "MMLU_professional_accounting":0.3333333333,
        "MMLU_professional_law":0.3683181226,
        "MMLU_professional_medicine":0.4595588235,
        "MMLU_professional_psychology":0.4526143791,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.4857142857,
        "MMLU_sociology":0.6119402985,
        "MMLU_us_foreign_policy":0.75,
        "MMLU_virology":0.4759036145,
        "MMLU_world_religions":0.7251461988
    },
    {
        "Model":"Llama2-Chinese-7b-Chat",
        "URL":"https:\/\/huggingface.co\/FlagAlpha\/Llama2-Chinese-7b-Chat",
        "full_model_name":"FlagAlpha\/Llama2-Chinese-7b-Chat",
        "Parameters":7.0,
        "MMLU_average":0.4771947411,
        "arc:challenge|25":0.4829351536,
        "hellaswag|10":0.5773750249,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4518518519,
        "MMLU_astronomy":0.4802631579,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.5547169811,
        "MMLU_college_biology":0.4861111111,
        "MMLU_college_chemistry":0.3,
        "MMLU_college_computer_science":0.37,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.4161849711,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.59,
        "MMLU_conceptual_physics":0.4042553191,
        "MMLU_econometrics":0.3333333333,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.3015873016,
        "MMLU_formal_logic":0.2380952381,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.5451612903,
        "MMLU_high_school_chemistry":0.4088669951,
        "MMLU_high_school_computer_science":0.48,
        "MMLU_high_school_european_history":0.5939393939,
        "MMLU_high_school_geography":0.5858585859,
        "MMLU_high_school_government_and_politics":0.7046632124,
        "MMLU_high_school_macroeconomics":0.4205128205,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.4159663866,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.6385321101,
        "MMLU_high_school_statistics":0.3333333333,
        "MMLU_high_school_us_history":0.6568627451,
        "MMLU_high_school_world_history":0.6624472574,
        "MMLU_human_aging":0.5605381166,
        "MMLU_human_sexuality":0.5038167939,
        "MMLU_international_law":0.6198347107,
        "MMLU_jurisprudence":0.5462962963,
        "MMLU_logical_fallacies":0.5460122699,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.640776699,
        "MMLU_marketing":0.7008547009,
        "MMLU_medical_genetics":0.45,
        "MMLU_miscellaneous":0.6781609195,
        "MMLU_moral_disputes":0.5520231214,
        "MMLU_moral_scenarios":0.2312849162,
        "MMLU_nutrition":0.4836601307,
        "MMLU_philosophy":0.5852090032,
        "MMLU_prehistory":0.5339506173,
        "MMLU_professional_accounting":0.3829787234,
        "MMLU_professional_law":0.3409387223,
        "MMLU_professional_medicine":0.4227941176,
        "MMLU_professional_psychology":0.477124183,
        "MMLU_public_relations":0.4909090909,
        "MMLU_security_studies":0.4693877551,
        "MMLU_sociology":0.6417910448,
        "MMLU_us_foreign_policy":0.69,
        "MMLU_virology":0.4397590361,
        "MMLU_world_religions":0.7134502924
    },
    {
        "Model":"llama-13b",
        "URL":"https:\/\/huggingface.co\/huggingface\/llama-13b",
        "full_model_name":"huggingface\/llama-13b",
        "Parameters":13.0,
        "MMLU_average":0.4767159887,
        "arc:challenge|25":0.5290102389,
        "hellaswag|10":0.6074487154,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.4736842105,
        "MMLU_business_ethics":0.46,
        "MMLU_clinical_knowledge":0.4603773585,
        "MMLU_college_biology":0.4652777778,
        "MMLU_college_chemistry":0.29,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.4277456647,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.63,
        "MMLU_conceptual_physics":0.3872340426,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.4275862069,
        "MMLU_elementary_mathematics":0.2645502646,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.5225806452,
        "MMLU_high_school_chemistry":0.2807881773,
        "MMLU_high_school_computer_science":0.49,
        "MMLU_high_school_european_history":0.6242424242,
        "MMLU_high_school_geography":0.5505050505,
        "MMLU_high_school_government_and_politics":0.6683937824,
        "MMLU_high_school_macroeconomics":0.4641025641,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.4789915966,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.6110091743,
        "MMLU_high_school_statistics":0.3009259259,
        "MMLU_high_school_us_history":0.5637254902,
        "MMLU_high_school_world_history":0.6751054852,
        "MMLU_human_aging":0.5426008969,
        "MMLU_human_sexuality":0.572519084,
        "MMLU_international_law":0.6446280992,
        "MMLU_jurisprudence":0.5185185185,
        "MMLU_logical_fallacies":0.527607362,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.6601941748,
        "MMLU_marketing":0.7179487179,
        "MMLU_medical_genetics":0.53,
        "MMLU_miscellaneous":0.6449553001,
        "MMLU_moral_disputes":0.5,
        "MMLU_moral_scenarios":0.2916201117,
        "MMLU_nutrition":0.5032679739,
        "MMLU_philosophy":0.540192926,
        "MMLU_prehistory":0.512345679,
        "MMLU_professional_accounting":0.3404255319,
        "MMLU_professional_law":0.3722294654,
        "MMLU_professional_medicine":0.5110294118,
        "MMLU_professional_psychology":0.4901960784,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.5265306122,
        "MMLU_sociology":0.6119402985,
        "MMLU_us_foreign_policy":0.78,
        "MMLU_virology":0.4337349398,
        "MMLU_world_religions":0.6783625731
    },
    {
        "Model":"gpt4all-alpaca-oa-codealpaca-lora-13b",
        "URL":"https:\/\/huggingface.co\/jordiclive\/gpt4all-alpaca-oa-codealpaca-lora-13b",
        "full_model_name":"jordiclive\/gpt4all-alpaca-oa-codealpaca-lora-13b",
        "Parameters":13.0,
        "MMLU_average":0.4766377195,
        "arc:challenge|25":0.5298634812,
        "hellaswag|10":0.6074487154,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.4736842105,
        "MMLU_business_ethics":0.46,
        "MMLU_clinical_knowledge":0.4641509434,
        "MMLU_college_biology":0.4722222222,
        "MMLU_college_chemistry":0.28,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.4219653179,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.63,
        "MMLU_conceptual_physics":0.3957446809,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.4275862069,
        "MMLU_elementary_mathematics":0.2645502646,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.5258064516,
        "MMLU_high_school_chemistry":0.2807881773,
        "MMLU_high_school_computer_science":0.49,
        "MMLU_high_school_european_history":0.6303030303,
        "MMLU_high_school_geography":0.5505050505,
        "MMLU_high_school_government_and_politics":0.6683937824,
        "MMLU_high_school_macroeconomics":0.4641025641,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.4789915966,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.6091743119,
        "MMLU_high_school_statistics":0.2962962963,
        "MMLU_high_school_us_history":0.568627451,
        "MMLU_high_school_world_history":0.6751054852,
        "MMLU_human_aging":0.5426008969,
        "MMLU_human_sexuality":0.572519084,
        "MMLU_international_law":0.6446280992,
        "MMLU_jurisprudence":0.5185185185,
        "MMLU_logical_fallacies":0.527607362,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.6601941748,
        "MMLU_marketing":0.7179487179,
        "MMLU_medical_genetics":0.53,
        "MMLU_miscellaneous":0.6436781609,
        "MMLU_moral_disputes":0.5,
        "MMLU_moral_scenarios":0.2938547486,
        "MMLU_nutrition":0.5032679739,
        "MMLU_philosophy":0.5337620579,
        "MMLU_prehistory":0.512345679,
        "MMLU_professional_accounting":0.3333333333,
        "MMLU_professional_law":0.3709256845,
        "MMLU_professional_medicine":0.5147058824,
        "MMLU_professional_psychology":0.4885620915,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.5265306122,
        "MMLU_sociology":0.6069651741,
        "MMLU_us_foreign_policy":0.78,
        "MMLU_virology":0.4337349398,
        "MMLU_world_religions":0.6783625731
    },
    {
        "Model":"L2-7b-Guanaco-Random-Test",
        "URL":"https:\/\/huggingface.co\/Lazycuber\/L2-7b-Guanaco-Random-Test",
        "full_model_name":"Lazycuber\/L2-7b-Guanaco-Random-Test",
        "Parameters":7.0,
        "MMLU_average":0.4765877408,
        "arc:challenge|25":0.476109215,
        "hellaswag|10":0.5723959371,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.437037037,
        "MMLU_astronomy":0.5131578947,
        "MMLU_business_ethics":0.46,
        "MMLU_clinical_knowledge":0.5169811321,
        "MMLU_college_biology":0.5138888889,
        "MMLU_college_chemistry":0.28,
        "MMLU_college_computer_science":0.38,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.3699421965,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.57,
        "MMLU_conceptual_physics":0.4255319149,
        "MMLU_econometrics":0.3859649123,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.3121693122,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.5290322581,
        "MMLU_high_school_chemistry":0.3793103448,
        "MMLU_high_school_computer_science":0.46,
        "MMLU_high_school_european_history":0.5818181818,
        "MMLU_high_school_geography":0.5909090909,
        "MMLU_high_school_government_and_politics":0.6683937824,
        "MMLU_high_school_macroeconomics":0.4128205128,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.3907563025,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.6642201835,
        "MMLU_high_school_statistics":0.2962962963,
        "MMLU_high_school_us_history":0.6421568627,
        "MMLU_high_school_world_history":0.6286919831,
        "MMLU_human_aging":0.5560538117,
        "MMLU_human_sexuality":0.5572519084,
        "MMLU_international_law":0.6694214876,
        "MMLU_jurisprudence":0.5925925926,
        "MMLU_logical_fallacies":0.5153374233,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.640776699,
        "MMLU_marketing":0.7307692308,
        "MMLU_medical_genetics":0.51,
        "MMLU_miscellaneous":0.6845466156,
        "MMLU_moral_disputes":0.5260115607,
        "MMLU_moral_scenarios":0.2268156425,
        "MMLU_nutrition":0.5392156863,
        "MMLU_philosophy":0.5466237942,
        "MMLU_prehistory":0.5586419753,
        "MMLU_professional_accounting":0.3617021277,
        "MMLU_professional_law":0.3279009126,
        "MMLU_professional_medicine":0.3970588235,
        "MMLU_professional_psychology":0.4689542484,
        "MMLU_public_relations":0.5090909091,
        "MMLU_security_studies":0.5224489796,
        "MMLU_sociology":0.6218905473,
        "MMLU_us_foreign_policy":0.66,
        "MMLU_virology":0.4337349398,
        "MMLU_world_religions":0.6959064327
    },
    {
        "Model":"airoboros-l2-7b-2.2.1",
        "URL":"https:\/\/huggingface.co\/jondurbin\/airoboros-l2-7b-2.2.1",
        "full_model_name":"jondurbin\/airoboros-l2-7b-2.2.1",
        "Parameters":7.0,
        "MMLU_average":0.4764308729,
        "arc:challenge|25":0.5153583618,
        "hellaswag|10":0.609639514,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.5037037037,
        "MMLU_astronomy":0.4539473684,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.4981132075,
        "MMLU_college_biology":0.4930555556,
        "MMLU_college_chemistry":0.25,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.4046242775,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.57,
        "MMLU_conceptual_physics":0.4,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.3121693122,
        "MMLU_formal_logic":0.2698412698,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.5258064516,
        "MMLU_high_school_chemistry":0.3743842365,
        "MMLU_high_school_computer_science":0.43,
        "MMLU_high_school_european_history":0.6060606061,
        "MMLU_high_school_geography":0.595959596,
        "MMLU_high_school_government_and_politics":0.6632124352,
        "MMLU_high_school_macroeconomics":0.4615384615,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.4537815126,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.6385321101,
        "MMLU_high_school_statistics":0.287037037,
        "MMLU_high_school_us_history":0.6029411765,
        "MMLU_high_school_world_history":0.7172995781,
        "MMLU_human_aging":0.5739910314,
        "MMLU_human_sexuality":0.572519084,
        "MMLU_international_law":0.6280991736,
        "MMLU_jurisprudence":0.5555555556,
        "MMLU_logical_fallacies":0.5398773006,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.572815534,
        "MMLU_marketing":0.6923076923,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.6577266922,
        "MMLU_moral_disputes":0.5433526012,
        "MMLU_moral_scenarios":0.2391061453,
        "MMLU_nutrition":0.5032679739,
        "MMLU_philosophy":0.5884244373,
        "MMLU_prehistory":0.4969135802,
        "MMLU_professional_accounting":0.4042553191,
        "MMLU_professional_law":0.3559322034,
        "MMLU_professional_medicine":0.5073529412,
        "MMLU_professional_psychology":0.4722222222,
        "MMLU_public_relations":0.5181818182,
        "MMLU_security_studies":0.5469387755,
        "MMLU_sociology":0.6069651741,
        "MMLU_us_foreign_policy":0.67,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.6900584795
    },
    {
        "Model":"tulu-7B-fp16",
        "URL":"https:\/\/huggingface.co\/TheBloke\/tulu-7B-fp16",
        "full_model_name":"TheBloke\/tulu-7B-fp16",
        "Parameters":7.0,
        "MMLU_average":0.4763284385,
        "arc:challenge|25":0.4624573379,
        "hellaswag|10":0.574885481,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.4518518519,
        "MMLU_astronomy":0.4736842105,
        "MMLU_business_ethics":0.56,
        "MMLU_clinical_knowledge":0.4905660377,
        "MMLU_college_biology":0.5,
        "MMLU_college_chemistry":0.3,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.4277456647,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.64,
        "MMLU_conceptual_physics":0.3744680851,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.4068965517,
        "MMLU_elementary_mathematics":0.2513227513,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.4806451613,
        "MMLU_high_school_chemistry":0.3103448276,
        "MMLU_high_school_computer_science":0.45,
        "MMLU_high_school_european_history":0.6606060606,
        "MMLU_high_school_geography":0.5909090909,
        "MMLU_high_school_government_and_politics":0.6735751295,
        "MMLU_high_school_macroeconomics":0.4358974359,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.3907563025,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.671559633,
        "MMLU_high_school_statistics":0.3842592593,
        "MMLU_high_school_us_history":0.6225490196,
        "MMLU_high_school_world_history":0.6624472574,
        "MMLU_human_aging":0.5291479821,
        "MMLU_human_sexuality":0.5114503817,
        "MMLU_international_law":0.6115702479,
        "MMLU_jurisprudence":0.5648148148,
        "MMLU_logical_fallacies":0.5705521472,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.6601941748,
        "MMLU_marketing":0.7564102564,
        "MMLU_medical_genetics":0.61,
        "MMLU_miscellaneous":0.6526181354,
        "MMLU_moral_disputes":0.5115606936,
        "MMLU_moral_scenarios":0.2960893855,
        "MMLU_nutrition":0.5032679739,
        "MMLU_philosophy":0.4919614148,
        "MMLU_prehistory":0.537037037,
        "MMLU_professional_accounting":0.3439716312,
        "MMLU_professional_law":0.3546284224,
        "MMLU_professional_medicine":0.4375,
        "MMLU_professional_psychology":0.4575163399,
        "MMLU_public_relations":0.5636363636,
        "MMLU_security_studies":0.4897959184,
        "MMLU_sociology":0.6616915423,
        "MMLU_us_foreign_policy":0.69,
        "MMLU_virology":0.4096385542,
        "MMLU_world_religions":0.6549707602
    },
    {
        "Model":"llama-13b",
        "URL":"https:\/\/huggingface.co\/huggyllama\/llama-13b",
        "full_model_name":"huggyllama\/llama-13b",
        "Parameters":13.0,
        "MMLU_average":0.476105883,
        "arc:challenge|25":0.5315699659,
        "hellaswag|10":0.6074487154,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.4736842105,
        "MMLU_business_ethics":0.46,
        "MMLU_clinical_knowledge":0.4641509434,
        "MMLU_college_biology":0.4583333333,
        "MMLU_college_chemistry":0.28,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.4161849711,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.63,
        "MMLU_conceptual_physics":0.3914893617,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.4137931034,
        "MMLU_elementary_mathematics":0.2645502646,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.5225806452,
        "MMLU_high_school_chemistry":0.2807881773,
        "MMLU_high_school_computer_science":0.48,
        "MMLU_high_school_european_history":0.6242424242,
        "MMLU_high_school_geography":0.5505050505,
        "MMLU_high_school_government_and_politics":0.6683937824,
        "MMLU_high_school_macroeconomics":0.4615384615,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.4789915966,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.6091743119,
        "MMLU_high_school_statistics":0.2962962963,
        "MMLU_high_school_us_history":0.568627451,
        "MMLU_high_school_world_history":0.6751054852,
        "MMLU_human_aging":0.5515695067,
        "MMLU_human_sexuality":0.572519084,
        "MMLU_international_law":0.6446280992,
        "MMLU_jurisprudence":0.5185185185,
        "MMLU_logical_fallacies":0.527607362,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.6601941748,
        "MMLU_marketing":0.7179487179,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.6462324393,
        "MMLU_moral_disputes":0.5,
        "MMLU_moral_scenarios":0.2983240223,
        "MMLU_nutrition":0.5032679739,
        "MMLU_philosophy":0.5434083601,
        "MMLU_prehistory":0.5154320988,
        "MMLU_professional_accounting":0.3368794326,
        "MMLU_professional_law":0.3722294654,
        "MMLU_professional_medicine":0.5147058824,
        "MMLU_professional_psychology":0.4885620915,
        "MMLU_public_relations":0.6181818182,
        "MMLU_security_studies":0.5265306122,
        "MMLU_sociology":0.6119402985,
        "MMLU_us_foreign_policy":0.78,
        "MMLU_virology":0.4397590361,
        "MMLU_world_religions":0.6842105263
    },
    {
        "Model":"Dans-PersonalityEngine-13b",
        "URL":"https:\/\/huggingface.co\/PocketDoc\/Dans-PersonalityEngine-13b",
        "full_model_name":"PocketDoc\/Dans-PersonalityEngine-13b",
        "Parameters":13.0,
        "MMLU_average":0.4757631985,
        "arc:challenge|25":0.5546075085,
        "hellaswag|10":0.6231826329,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.4539473684,
        "MMLU_business_ethics":0.47,
        "MMLU_clinical_knowledge":0.4641509434,
        "MMLU_college_biology":0.4930555556,
        "MMLU_college_chemistry":0.29,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.4219653179,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.64,
        "MMLU_conceptual_physics":0.3914893617,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.4206896552,
        "MMLU_elementary_mathematics":0.2857142857,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.5225806452,
        "MMLU_high_school_chemistry":0.3004926108,
        "MMLU_high_school_computer_science":0.5,
        "MMLU_high_school_european_history":0.5696969697,
        "MMLU_high_school_geography":0.5606060606,
        "MMLU_high_school_government_and_politics":0.6735751295,
        "MMLU_high_school_macroeconomics":0.4384615385,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.4621848739,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.6330275229,
        "MMLU_high_school_statistics":0.2685185185,
        "MMLU_high_school_us_history":0.5735294118,
        "MMLU_high_school_world_history":0.7046413502,
        "MMLU_human_aging":0.5515695067,
        "MMLU_human_sexuality":0.5496183206,
        "MMLU_international_law":0.6446280992,
        "MMLU_jurisprudence":0.5277777778,
        "MMLU_logical_fallacies":0.509202454,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.6893203883,
        "MMLU_marketing":0.7094017094,
        "MMLU_medical_genetics":0.5,
        "MMLU_miscellaneous":0.6730523627,
        "MMLU_moral_disputes":0.5086705202,
        "MMLU_moral_scenarios":0.2391061453,
        "MMLU_nutrition":0.5294117647,
        "MMLU_philosophy":0.536977492,
        "MMLU_prehistory":0.5339506173,
        "MMLU_professional_accounting":0.3439716312,
        "MMLU_professional_law":0.3761408083,
        "MMLU_professional_medicine":0.5441176471,
        "MMLU_professional_psychology":0.4754901961,
        "MMLU_public_relations":0.5818181818,
        "MMLU_security_studies":0.5510204082,
        "MMLU_sociology":0.6417910448,
        "MMLU_us_foreign_policy":0.74,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.6725146199
    },
    {
        "Model":"galactica-30b",
        "URL":"https:\/\/huggingface.co\/facebook\/galactica-30b",
        "full_model_name":"facebook\/galactica-30b",
        "Parameters":30.0,
        "MMLU_average":0.47561811,
        "arc:challenge|25":0.4419795222,
        "hellaswag|10":0.4569806811,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.5481481481,
        "MMLU_astronomy":0.5328947368,
        "MMLU_business_ethics":0.48,
        "MMLU_clinical_knowledge":0.5547169811,
        "MMLU_college_biology":0.5833333333,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.5028901734,
        "MMLU_college_physics":0.3431372549,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.4723404255,
        "MMLU_econometrics":0.3421052632,
        "MMLU_electrical_engineering":0.5655172414,
        "MMLU_elementary_mathematics":0.3201058201,
        "MMLU_formal_logic":0.2777777778,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.5548387097,
        "MMLU_high_school_chemistry":0.4482758621,
        "MMLU_high_school_computer_science":0.54,
        "MMLU_high_school_european_history":0.6303030303,
        "MMLU_high_school_geography":0.5353535354,
        "MMLU_high_school_government_and_politics":0.5751295337,
        "MMLU_high_school_macroeconomics":0.4256410256,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.4495798319,
        "MMLU_high_school_physics":0.3774834437,
        "MMLU_high_school_psychology":0.6256880734,
        "MMLU_high_school_statistics":0.3518518519,
        "MMLU_high_school_us_history":0.4264705882,
        "MMLU_high_school_world_history":0.5991561181,
        "MMLU_human_aging":0.5695067265,
        "MMLU_human_sexuality":0.572519084,
        "MMLU_international_law":0.6694214876,
        "MMLU_jurisprudence":0.5185185185,
        "MMLU_logical_fallacies":0.4969325153,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.6213592233,
        "MMLU_marketing":0.7051282051,
        "MMLU_medical_genetics":0.58,
        "MMLU_miscellaneous":0.5019157088,
        "MMLU_moral_disputes":0.4566473988,
        "MMLU_moral_scenarios":0.2402234637,
        "MMLU_nutrition":0.5294117647,
        "MMLU_philosophy":0.4855305466,
        "MMLU_prehistory":0.5308641975,
        "MMLU_professional_accounting":0.3581560284,
        "MMLU_professional_law":0.3441981747,
        "MMLU_professional_medicine":0.4889705882,
        "MMLU_professional_psychology":0.5049019608,
        "MMLU_public_relations":0.5,
        "MMLU_security_studies":0.4,
        "MMLU_sociology":0.5721393035,
        "MMLU_us_foreign_policy":0.52,
        "MMLU_virology":0.4337349398,
        "MMLU_world_religions":0.4152046784
    },
    {
        "Model":"mamba-gpt-7b",
        "URL":"https:\/\/huggingface.co\/CobraMamba\/mamba-gpt-7b",
        "full_model_name":"CobraMamba\/mamba-gpt-7b",
        "Parameters":7.0,
        "MMLU_average":0.4747107697,
        "arc:challenge|25":0.4684300341,
        "hellaswag|10":0.5593507269,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.4934210526,
        "MMLU_business_ethics":0.47,
        "MMLU_clinical_knowledge":0.4716981132,
        "MMLU_college_biology":0.4861111111,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.36,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.4219653179,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.58,
        "MMLU_conceptual_physics":0.3574468085,
        "MMLU_econometrics":0.3421052632,
        "MMLU_electrical_engineering":0.4551724138,
        "MMLU_elementary_mathematics":0.2830687831,
        "MMLU_formal_logic":0.2698412698,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.5258064516,
        "MMLU_high_school_chemistry":0.3349753695,
        "MMLU_high_school_computer_science":0.44,
        "MMLU_high_school_european_history":0.6484848485,
        "MMLU_high_school_geography":0.5909090909,
        "MMLU_high_school_government_and_politics":0.6269430052,
        "MMLU_high_school_macroeconomics":0.4282051282,
        "MMLU_high_school_mathematics":0.2888888889,
        "MMLU_high_school_microeconomics":0.4327731092,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.6110091743,
        "MMLU_high_school_statistics":0.3703703704,
        "MMLU_high_school_us_history":0.6176470588,
        "MMLU_high_school_world_history":0.6329113924,
        "MMLU_human_aging":0.5739910314,
        "MMLU_human_sexuality":0.5419847328,
        "MMLU_international_law":0.6198347107,
        "MMLU_jurisprudence":0.537037037,
        "MMLU_logical_fallacies":0.5030674847,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.7087378641,
        "MMLU_marketing":0.7435897436,
        "MMLU_medical_genetics":0.53,
        "MMLU_miscellaneous":0.6679438059,
        "MMLU_moral_disputes":0.5144508671,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.5653594771,
        "MMLU_philosophy":0.536977492,
        "MMLU_prehistory":0.5216049383,
        "MMLU_professional_accounting":0.3687943262,
        "MMLU_professional_law":0.3683181226,
        "MMLU_professional_medicine":0.3970588235,
        "MMLU_professional_psychology":0.4754901961,
        "MMLU_public_relations":0.5181818182,
        "MMLU_security_studies":0.6,
        "MMLU_sociology":0.5323383085,
        "MMLU_us_foreign_policy":0.69,
        "MMLU_virology":0.4096385542,
        "MMLU_world_religions":0.6666666667
    },
    {
        "Model":"Llama2-7b-sharegpt4",
        "URL":"https:\/\/huggingface.co\/HWERI\/Llama2-7b-sharegpt4",
        "full_model_name":"HWERI\/Llama2-7b-sharegpt4",
        "Parameters":7.0,
        "MMLU_average":0.4746814125,
        "arc:challenge|25":0.5307167235,
        "hellaswag|10":0.6209918343,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.437037037,
        "MMLU_astronomy":0.4802631579,
        "MMLU_business_ethics":0.46,
        "MMLU_clinical_knowledge":0.4905660377,
        "MMLU_college_biology":0.4791666667,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.4,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.450867052,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.58,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.2192982456,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.2883597884,
        "MMLU_formal_logic":0.3412698413,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.5258064516,
        "MMLU_high_school_chemistry":0.3891625616,
        "MMLU_high_school_computer_science":0.44,
        "MMLU_high_school_european_history":0.6181818182,
        "MMLU_high_school_geography":0.4949494949,
        "MMLU_high_school_government_and_politics":0.7046632124,
        "MMLU_high_school_macroeconomics":0.4435897436,
        "MMLU_high_school_mathematics":0.2888888889,
        "MMLU_high_school_microeconomics":0.4621848739,
        "MMLU_high_school_physics":0.238410596,
        "MMLU_high_school_psychology":0.6495412844,
        "MMLU_high_school_statistics":0.2685185185,
        "MMLU_high_school_us_history":0.5637254902,
        "MMLU_high_school_world_history":0.6286919831,
        "MMLU_human_aging":0.5381165919,
        "MMLU_human_sexuality":0.5572519084,
        "MMLU_international_law":0.6611570248,
        "MMLU_jurisprudence":0.5277777778,
        "MMLU_logical_fallacies":0.4785276074,
        "MMLU_machine_learning":0.4017857143,
        "MMLU_management":0.5533980583,
        "MMLU_marketing":0.6837606838,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.656449553,
        "MMLU_moral_disputes":0.5115606936,
        "MMLU_moral_scenarios":0.243575419,
        "MMLU_nutrition":0.5098039216,
        "MMLU_philosophy":0.5273311897,
        "MMLU_prehistory":0.5308641975,
        "MMLU_professional_accounting":0.3794326241,
        "MMLU_professional_law":0.3787483703,
        "MMLU_professional_medicine":0.5330882353,
        "MMLU_professional_psychology":0.4607843137,
        "MMLU_public_relations":0.5727272727,
        "MMLU_security_studies":0.5306122449,
        "MMLU_sociology":0.6666666667,
        "MMLU_us_foreign_policy":0.68,
        "MMLU_virology":0.3975903614,
        "MMLU_world_religions":0.6900584795
    },
    {
        "Model":"vicuna-7b-v1.3-instruct-pl-lora_unload",
        "URL":"https:\/\/huggingface.co\/Aspik101\/vicuna-7b-v1.3-instruct-pl-lora_unload",
        "full_model_name":"Aspik101\/vicuna-7b-v1.3-instruct-pl-lora_unload",
        "Parameters":7.0,
        "MMLU_average":0.4741586956,
        "arc:challenge|25":0.4368600683,
        "hellaswag|10":0.5687114121,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.4518518519,
        "MMLU_astronomy":0.4144736842,
        "MMLU_business_ethics":0.46,
        "MMLU_clinical_knowledge":0.5358490566,
        "MMLU_college_biology":0.4513888889,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.3988439306,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.63,
        "MMLU_conceptual_physics":0.3744680851,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.4275862069,
        "MMLU_elementary_mathematics":0.2857142857,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.5258064516,
        "MMLU_high_school_chemistry":0.3990147783,
        "MMLU_high_school_computer_science":0.42,
        "MMLU_high_school_european_history":0.5696969697,
        "MMLU_high_school_geography":0.6363636364,
        "MMLU_high_school_government_and_politics":0.6580310881,
        "MMLU_high_school_macroeconomics":0.4820512821,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.4285714286,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.6550458716,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.6225490196,
        "MMLU_high_school_world_history":0.6244725738,
        "MMLU_human_aging":0.5246636771,
        "MMLU_human_sexuality":0.5419847328,
        "MMLU_international_law":0.6776859504,
        "MMLU_jurisprudence":0.6481481481,
        "MMLU_logical_fallacies":0.5950920245,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.7087378641,
        "MMLU_marketing":0.6837606838,
        "MMLU_medical_genetics":0.51,
        "MMLU_miscellaneous":0.6513409962,
        "MMLU_moral_disputes":0.5289017341,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.5130718954,
        "MMLU_philosophy":0.5305466238,
        "MMLU_prehistory":0.5432098765,
        "MMLU_professional_accounting":0.3546099291,
        "MMLU_professional_law":0.3520208605,
        "MMLU_professional_medicine":0.4779411765,
        "MMLU_professional_psychology":0.4068627451,
        "MMLU_public_relations":0.4363636364,
        "MMLU_security_studies":0.5265306122,
        "MMLU_sociology":0.6268656716,
        "MMLU_us_foreign_policy":0.7,
        "MMLU_virology":0.3975903614,
        "MMLU_world_religions":0.6725146199
    },
    {
        "Model":"LLaMa-2-PeanutButter_v37_SFT-R1-DPO-R2-7B",
        "URL":"https:\/\/huggingface.co\/PeanutJar\/LLaMa-2-PeanutButter_v37_SFT-R1-DPO-R2-7B",
        "full_model_name":"PeanutJar\/LLaMa-2-PeanutButter_v37_SFT-R1-DPO-R2-7B",
        "Parameters":7.0,
        "MMLU_average":0.4731864612,
        "arc:challenge|25":0.5042662116,
        "hellaswag|10":0.5925114519,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.4210526316,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.4716981132,
        "MMLU_college_biology":0.4652777778,
        "MMLU_college_chemistry":0.33,
        "MMLU_college_computer_science":0.34,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.4393063584,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.4255319149,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.2830687831,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.4903225806,
        "MMLU_high_school_chemistry":0.3743842365,
        "MMLU_high_school_computer_science":0.4,
        "MMLU_high_school_european_history":0.5818181818,
        "MMLU_high_school_geography":0.5101010101,
        "MMLU_high_school_government_and_politics":0.6735751295,
        "MMLU_high_school_macroeconomics":0.4512820513,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.4411764706,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.6366972477,
        "MMLU_high_school_statistics":0.287037037,
        "MMLU_high_school_us_history":0.5539215686,
        "MMLU_high_school_world_history":0.6497890295,
        "MMLU_human_aging":0.5560538117,
        "MMLU_human_sexuality":0.5496183206,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.5185185185,
        "MMLU_logical_fallacies":0.5398773006,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.572815534,
        "MMLU_marketing":0.6965811966,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.6398467433,
        "MMLU_moral_disputes":0.5144508671,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.5032679739,
        "MMLU_philosophy":0.6173633441,
        "MMLU_prehistory":0.4783950617,
        "MMLU_professional_accounting":0.3617021277,
        "MMLU_professional_law":0.3611473272,
        "MMLU_professional_medicine":0.5183823529,
        "MMLU_professional_psychology":0.4509803922,
        "MMLU_public_relations":0.5545454545,
        "MMLU_security_studies":0.5142857143,
        "MMLU_sociology":0.6417910448,
        "MMLU_us_foreign_policy":0.63,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.7192982456
    },
    {
        "Model":"vicuna-7b-v1.5-lora-mctaco-modified4",
        "URL":"https:\/\/huggingface.co\/Charlie911\/vicuna-7b-v1.5-lora-mctaco-modified4",
        "full_model_name":"Charlie911\/vicuna-7b-v1.5-lora-mctaco-modified4",
        "Parameters":7.0,
        "MMLU_average":0.472570115,
        "arc:challenge|25":0.364334471,
        "hellaswag|10":0.5421230831,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.437037037,
        "MMLU_astronomy":0.3947368421,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.4754716981,
        "MMLU_college_biology":0.4791666667,
        "MMLU_college_chemistry":0.29,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.387283237,
        "MMLU_college_physics":0.1470588235,
        "MMLU_computer_security":0.62,
        "MMLU_conceptual_physics":0.3872340426,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.4482758621,
        "MMLU_elementary_mathematics":0.3306878307,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.5387096774,
        "MMLU_high_school_chemistry":0.3743842365,
        "MMLU_high_school_computer_science":0.46,
        "MMLU_high_school_european_history":0.6606060606,
        "MMLU_high_school_geography":0.595959596,
        "MMLU_high_school_government_and_politics":0.6424870466,
        "MMLU_high_school_macroeconomics":0.4,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.3949579832,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.6385321101,
        "MMLU_high_school_statistics":0.3055555556,
        "MMLU_high_school_us_history":0.6666666667,
        "MMLU_high_school_world_history":0.6540084388,
        "MMLU_human_aging":0.5874439462,
        "MMLU_human_sexuality":0.5648854962,
        "MMLU_international_law":0.5785123967,
        "MMLU_jurisprudence":0.5185185185,
        "MMLU_logical_fallacies":0.5582822086,
        "MMLU_machine_learning":0.4732142857,
        "MMLU_management":0.5922330097,
        "MMLU_marketing":0.7307692308,
        "MMLU_medical_genetics":0.61,
        "MMLU_miscellaneous":0.6526181354,
        "MMLU_moral_disputes":0.5115606936,
        "MMLU_moral_scenarios":0.243575419,
        "MMLU_nutrition":0.4934640523,
        "MMLU_philosophy":0.540192926,
        "MMLU_prehistory":0.5432098765,
        "MMLU_professional_accounting":0.3510638298,
        "MMLU_professional_law":0.3709256845,
        "MMLU_professional_medicine":0.4375,
        "MMLU_professional_psychology":0.4509803922,
        "MMLU_public_relations":0.5181818182,
        "MMLU_security_studies":0.5714285714,
        "MMLU_sociology":0.6616915423,
        "MMLU_us_foreign_policy":0.69,
        "MMLU_virology":0.3493975904,
        "MMLU_world_religions":0.649122807
    },
    {
        "Model":"llama-13b-pretrained-sft-do2",
        "URL":"https:\/\/huggingface.co\/dvruette\/llama-13b-pretrained-sft-do2",
        "full_model_name":"dvruette\/llama-13b-pretrained-sft-do2",
        "Parameters":13.0,
        "MMLU_average":0.4724864075,
        "arc:challenge|25":0.5648464164,
        "hellaswag|10":0.6081457877,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.4473684211,
        "MMLU_business_ethics":0.45,
        "MMLU_clinical_knowledge":0.5018867925,
        "MMLU_college_biology":0.4722222222,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.4219653179,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.64,
        "MMLU_conceptual_physics":0.3744680851,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.4344827586,
        "MMLU_elementary_mathematics":0.3095238095,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.5096774194,
        "MMLU_high_school_chemistry":0.3054187192,
        "MMLU_high_school_computer_science":0.5,
        "MMLU_high_school_european_history":0.6060606061,
        "MMLU_high_school_geography":0.6060606061,
        "MMLU_high_school_government_and_politics":0.6632124352,
        "MMLU_high_school_macroeconomics":0.4307692308,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.4663865546,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.623853211,
        "MMLU_high_school_statistics":0.375,
        "MMLU_high_school_us_history":0.637254902,
        "MMLU_high_school_world_history":0.6708860759,
        "MMLU_human_aging":0.4977578475,
        "MMLU_human_sexuality":0.4961832061,
        "MMLU_international_law":0.6033057851,
        "MMLU_jurisprudence":0.5277777778,
        "MMLU_logical_fallacies":0.4539877301,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.6601941748,
        "MMLU_marketing":0.7564102564,
        "MMLU_medical_genetics":0.49,
        "MMLU_miscellaneous":0.6475095785,
        "MMLU_moral_disputes":0.4942196532,
        "MMLU_moral_scenarios":0.3296089385,
        "MMLU_nutrition":0.4967320261,
        "MMLU_philosophy":0.498392283,
        "MMLU_prehistory":0.512345679,
        "MMLU_professional_accounting":0.3191489362,
        "MMLU_professional_law":0.3859191656,
        "MMLU_professional_medicine":0.4522058824,
        "MMLU_professional_psychology":0.4509803922,
        "MMLU_public_relations":0.5818181818,
        "MMLU_security_studies":0.4857142857,
        "MMLU_sociology":0.6119402985,
        "MMLU_us_foreign_policy":0.7,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.6725146199
    },
    {
        "Model":"LLaMa-2-PeanutButter_v4-7B",
        "URL":"https:\/\/huggingface.co\/PeanutJar\/LLaMa-2-PeanutButter_v4-7B",
        "full_model_name":"PeanutJar\/LLaMa-2-PeanutButter_v4-7B",
        "Parameters":7.0,
        "MMLU_average":0.4723733668,
        "arc:challenge|25":0.5076791809,
        "hellaswag|10":0.6188010357,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.4276315789,
        "MMLU_business_ethics":0.47,
        "MMLU_clinical_knowledge":0.4867924528,
        "MMLU_college_biology":0.5,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.4335260116,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.58,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.3333333333,
        "MMLU_electrical_engineering":0.4482758621,
        "MMLU_elementary_mathematics":0.3015873016,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.5032258065,
        "MMLU_high_school_chemistry":0.3694581281,
        "MMLU_high_school_computer_science":0.42,
        "MMLU_high_school_european_history":0.6242424242,
        "MMLU_high_school_geography":0.5909090909,
        "MMLU_high_school_government_and_politics":0.7202072539,
        "MMLU_high_school_macroeconomics":0.4666666667,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.4579831933,
        "MMLU_high_school_physics":0.357615894,
        "MMLU_high_school_psychology":0.6385321101,
        "MMLU_high_school_statistics":0.375,
        "MMLU_high_school_us_history":0.568627451,
        "MMLU_high_school_world_history":0.5949367089,
        "MMLU_human_aging":0.5381165919,
        "MMLU_human_sexuality":0.5114503817,
        "MMLU_international_law":0.5619834711,
        "MMLU_jurisprudence":0.4722222222,
        "MMLU_logical_fallacies":0.4969325153,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.6019417476,
        "MMLU_marketing":0.688034188,
        "MMLU_medical_genetics":0.5,
        "MMLU_miscellaneous":0.6155810983,
        "MMLU_moral_disputes":0.4768786127,
        "MMLU_moral_scenarios":0.2994413408,
        "MMLU_nutrition":0.5261437908,
        "MMLU_philosophy":0.578778135,
        "MMLU_prehistory":0.5061728395,
        "MMLU_professional_accounting":0.3723404255,
        "MMLU_professional_law":0.3676662321,
        "MMLU_professional_medicine":0.5367647059,
        "MMLU_professional_psychology":0.4346405229,
        "MMLU_public_relations":0.5090909091,
        "MMLU_security_studies":0.4897959184,
        "MMLU_sociology":0.6169154229,
        "MMLU_us_foreign_policy":0.68,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.7076023392
    },
    {
        "Model":"Llama2-7b-openorca-mc-v2-dpo",
        "URL":"https:\/\/huggingface.co\/beaugogh\/Llama2-7b-openorca-mc-v2-dpo",
        "full_model_name":"beaugogh\/Llama2-7b-openorca-mc-v2-dpo",
        "Parameters":7.0,
        "MMLU_average":0.4719933201,
        "arc:challenge|25":0.5017064846,
        "hellaswag|10":0.6238797052,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.437037037,
        "MMLU_astronomy":0.4802631579,
        "MMLU_business_ethics":0.49,
        "MMLU_clinical_knowledge":0.5547169811,
        "MMLU_college_biology":0.4513888889,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.4277456647,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.55,
        "MMLU_conceptual_physics":0.4425531915,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.3306878307,
        "MMLU_formal_logic":0.3333333333,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.5129032258,
        "MMLU_high_school_chemistry":0.3793103448,
        "MMLU_high_school_computer_science":0.38,
        "MMLU_high_school_european_history":0.5878787879,
        "MMLU_high_school_geography":0.5303030303,
        "MMLU_high_school_government_and_politics":0.6787564767,
        "MMLU_high_school_macroeconomics":0.4487179487,
        "MMLU_high_school_mathematics":0.2888888889,
        "MMLU_high_school_microeconomics":0.4243697479,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.6403669725,
        "MMLU_high_school_statistics":0.287037037,
        "MMLU_high_school_us_history":0.6519607843,
        "MMLU_high_school_world_history":0.7046413502,
        "MMLU_human_aging":0.5650224215,
        "MMLU_human_sexuality":0.5648854962,
        "MMLU_international_law":0.5950413223,
        "MMLU_jurisprudence":0.5092592593,
        "MMLU_logical_fallacies":0.5030674847,
        "MMLU_machine_learning":0.4285714286,
        "MMLU_management":0.5825242718,
        "MMLU_marketing":0.7179487179,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.6704980843,
        "MMLU_moral_disputes":0.4971098266,
        "MMLU_moral_scenarios":0.2391061453,
        "MMLU_nutrition":0.4575163399,
        "MMLU_philosophy":0.5498392283,
        "MMLU_prehistory":0.524691358,
        "MMLU_professional_accounting":0.3936170213,
        "MMLU_professional_law":0.3709256845,
        "MMLU_professional_medicine":0.4632352941,
        "MMLU_professional_psychology":0.4705882353,
        "MMLU_public_relations":0.5818181818,
        "MMLU_security_studies":0.4816326531,
        "MMLU_sociology":0.631840796,
        "MMLU_us_foreign_policy":0.67,
        "MMLU_virology":0.3614457831,
        "MMLU_world_religions":0.6608187135
    },
    {
        "Model":"guanaco-13b-llama-2",
        "URL":"https:\/\/huggingface.co\/jlevin\/guanaco-13b-llama-2",
        "full_model_name":"jlevin\/guanaco-13b-llama-2",
        "Parameters":13.0,
        "MMLU_average":0.471913468,
        "arc:challenge|25":0.5179180887,
        "hellaswag|10":0.6226847242,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.4407894737,
        "MMLU_business_ethics":0.47,
        "MMLU_clinical_knowledge":0.4981132075,
        "MMLU_college_biology":0.4513888889,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.4277456647,
        "MMLU_college_physics":0.1470588235,
        "MMLU_computer_security":0.52,
        "MMLU_conceptual_physics":0.429787234,
        "MMLU_econometrics":0.350877193,
        "MMLU_electrical_engineering":0.4896551724,
        "MMLU_elementary_mathematics":0.2645502646,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.5387096774,
        "MMLU_high_school_chemistry":0.3694581281,
        "MMLU_high_school_computer_science":0.47,
        "MMLU_high_school_european_history":0.6303030303,
        "MMLU_high_school_geography":0.5303030303,
        "MMLU_high_school_government_and_politics":0.7046632124,
        "MMLU_high_school_macroeconomics":0.4666666667,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.4369747899,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.6348623853,
        "MMLU_high_school_statistics":0.2962962963,
        "MMLU_high_school_us_history":0.568627451,
        "MMLU_high_school_world_history":0.6540084388,
        "MMLU_human_aging":0.5874439462,
        "MMLU_human_sexuality":0.5801526718,
        "MMLU_international_law":0.5950413223,
        "MMLU_jurisprudence":0.5277777778,
        "MMLU_logical_fallacies":0.5030674847,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.5339805825,
        "MMLU_marketing":0.6965811966,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.656449553,
        "MMLU_moral_disputes":0.5289017341,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.4934640523,
        "MMLU_philosophy":0.575562701,
        "MMLU_prehistory":0.5185185185,
        "MMLU_professional_accounting":0.3581560284,
        "MMLU_professional_law":0.35136897,
        "MMLU_professional_medicine":0.5110294118,
        "MMLU_professional_psychology":0.4493464052,
        "MMLU_public_relations":0.5636363636,
        "MMLU_security_studies":0.4897959184,
        "MMLU_sociology":0.592039801,
        "MMLU_us_foreign_policy":0.68,
        "MMLU_virology":0.3975903614,
        "MMLU_world_religions":0.6959064327
    },
    {
        "Model":"Metharme-13b-Merged",
        "URL":"https:\/\/huggingface.co\/TehVenom\/Metharme-13b-Merged",
        "full_model_name":"TehVenom\/Metharme-13b-Merged",
        "Parameters":13.0,
        "MMLU_average":0.4717695321,
        "arc:challenge|25":0.5733788396,
        "hellaswag|10":0.6122286397,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4444444444,
        "MMLU_astronomy":0.4276315789,
        "MMLU_business_ethics":0.45,
        "MMLU_clinical_knowledge":0.4716981132,
        "MMLU_college_biology":0.5,
        "MMLU_college_chemistry":0.32,
        "MMLU_college_computer_science":0.4,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.3930635838,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.62,
        "MMLU_conceptual_physics":0.3744680851,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.4137931034,
        "MMLU_elementary_mathematics":0.2751322751,
        "MMLU_formal_logic":0.3968253968,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.4935483871,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.49,
        "MMLU_high_school_european_history":0.6121212121,
        "MMLU_high_school_geography":0.5202020202,
        "MMLU_high_school_government_and_politics":0.6683937824,
        "MMLU_high_school_macroeconomics":0.4102564103,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.4285714286,
        "MMLU_high_school_physics":0.2317880795,
        "MMLU_high_school_psychology":0.5963302752,
        "MMLU_high_school_statistics":0.25,
        "MMLU_high_school_us_history":0.612745098,
        "MMLU_high_school_world_history":0.6919831224,
        "MMLU_human_aging":0.5560538117,
        "MMLU_human_sexuality":0.5038167939,
        "MMLU_international_law":0.7024793388,
        "MMLU_jurisprudence":0.537037037,
        "MMLU_logical_fallacies":0.5766871166,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.6699029126,
        "MMLU_marketing":0.6965811966,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.6730523627,
        "MMLU_moral_disputes":0.4913294798,
        "MMLU_moral_scenarios":0.2357541899,
        "MMLU_nutrition":0.4869281046,
        "MMLU_philosophy":0.5144694534,
        "MMLU_prehistory":0.537037037,
        "MMLU_professional_accounting":0.365248227,
        "MMLU_professional_law":0.3930899609,
        "MMLU_professional_medicine":0.4816176471,
        "MMLU_professional_psychology":0.5081699346,
        "MMLU_public_relations":0.5545454545,
        "MMLU_security_studies":0.5102040816,
        "MMLU_sociology":0.671641791,
        "MMLU_us_foreign_policy":0.77,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.6608187135
    },
    {
        "Model":"elliott_Llama-2-7b-hf",
        "URL":"https:\/\/huggingface.co\/elliotthwang\/elliott_Llama-2-7b-hf",
        "full_model_name":"elliotthwang\/elliott_Llama-2-7b-hf",
        "Parameters":7.0,
        "MMLU_average":0.4709013085,
        "arc:challenge|25":0.4948805461,
        "hellaswag|10":0.5872336188,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.4144736842,
        "MMLU_business_ethics":0.47,
        "MMLU_clinical_knowledge":0.4716981132,
        "MMLU_college_biology":0.4513888889,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.38,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.4566473988,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.58,
        "MMLU_conceptual_physics":0.4340425532,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.2619047619,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.5,
        "MMLU_high_school_chemistry":0.3399014778,
        "MMLU_high_school_computer_science":0.45,
        "MMLU_high_school_european_history":0.5757575758,
        "MMLU_high_school_geography":0.5505050505,
        "MMLU_high_school_government_and_politics":0.6787564767,
        "MMLU_high_school_macroeconomics":0.4487179487,
        "MMLU_high_school_mathematics":0.3148148148,
        "MMLU_high_school_microeconomics":0.4411764706,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.6366972477,
        "MMLU_high_school_statistics":0.3055555556,
        "MMLU_high_school_us_history":0.5588235294,
        "MMLU_high_school_world_history":0.611814346,
        "MMLU_human_aging":0.5784753363,
        "MMLU_human_sexuality":0.572519084,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.5462962963,
        "MMLU_logical_fallacies":0.490797546,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.572815534,
        "MMLU_marketing":0.6837606838,
        "MMLU_medical_genetics":0.51,
        "MMLU_miscellaneous":0.6347381865,
        "MMLU_moral_disputes":0.5115606936,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.4934640523,
        "MMLU_philosophy":0.6045016077,
        "MMLU_prehistory":0.4969135802,
        "MMLU_professional_accounting":0.3829787234,
        "MMLU_professional_law":0.3657105606,
        "MMLU_professional_medicine":0.5330882353,
        "MMLU_professional_psychology":0.4379084967,
        "MMLU_public_relations":0.5454545455,
        "MMLU_security_studies":0.4653061224,
        "MMLU_sociology":0.6169154229,
        "MMLU_us_foreign_policy":0.65,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.6959064327
    },
    {
        "Model":"Hermes-Platypus2-mini-7B",
        "URL":"https:\/\/huggingface.co\/edor\/Hermes-Platypus2-mini-7B",
        "full_model_name":"edor\/Hermes-Platypus2-mini-7B",
        "Parameters":7.0,
        "MMLU_average":0.4708275733,
        "arc:challenge|25":0.5230375427,
        "hellaswag|10":0.6015733918,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.4210526316,
        "MMLU_business_ethics":0.48,
        "MMLU_clinical_knowledge":0.5056603774,
        "MMLU_college_biology":0.5,
        "MMLU_college_chemistry":0.3,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.4161849711,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.57,
        "MMLU_conceptual_physics":0.4,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.4344827586,
        "MMLU_elementary_mathematics":0.3015873016,
        "MMLU_formal_logic":0.3333333333,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.5225806452,
        "MMLU_high_school_chemistry":0.3300492611,
        "MMLU_high_school_computer_science":0.46,
        "MMLU_high_school_european_history":0.6181818182,
        "MMLU_high_school_geography":0.5707070707,
        "MMLU_high_school_government_and_politics":0.6683937824,
        "MMLU_high_school_macroeconomics":0.4307692308,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.4117647059,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.6440366972,
        "MMLU_high_school_statistics":0.2731481481,
        "MMLU_high_school_us_history":0.6323529412,
        "MMLU_high_school_world_history":0.6666666667,
        "MMLU_human_aging":0.5515695067,
        "MMLU_human_sexuality":0.5190839695,
        "MMLU_international_law":0.6611570248,
        "MMLU_jurisprudence":0.537037037,
        "MMLU_logical_fallacies":0.4601226994,
        "MMLU_machine_learning":0.4375,
        "MMLU_management":0.5825242718,
        "MMLU_marketing":0.7222222222,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.6577266922,
        "MMLU_moral_disputes":0.5346820809,
        "MMLU_moral_scenarios":0.2525139665,
        "MMLU_nutrition":0.4967320261,
        "MMLU_philosophy":0.5691318328,
        "MMLU_prehistory":0.5061728395,
        "MMLU_professional_accounting":0.390070922,
        "MMLU_professional_law":0.3539765319,
        "MMLU_professional_medicine":0.4705882353,
        "MMLU_professional_psychology":0.4460784314,
        "MMLU_public_relations":0.5181818182,
        "MMLU_security_studies":0.5632653061,
        "MMLU_sociology":0.6218905473,
        "MMLU_us_foreign_policy":0.68,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.6374269006
    },
    {
        "Model":"ELYZA-japanese-Llama-2-7b-instruct",
        "URL":"https:\/\/huggingface.co\/elyza\/ELYZA-japanese-Llama-2-7b-instruct",
        "full_model_name":"elyza\/ELYZA-japanese-Llama-2-7b-instruct",
        "Parameters":7.0,
        "MMLU_average":0.4706542275,
        "arc:challenge|25":0.4948805461,
        "hellaswag|10":0.5803624776,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.5197368421,
        "MMLU_business_ethics":0.47,
        "MMLU_clinical_knowledge":0.5169811321,
        "MMLU_college_biology":0.4861111111,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.4393063584,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.6,
        "MMLU_conceptual_physics":0.4255319149,
        "MMLU_econometrics":0.3333333333,
        "MMLU_electrical_engineering":0.4482758621,
        "MMLU_elementary_mathematics":0.3015873016,
        "MMLU_formal_logic":0.1984126984,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.535483871,
        "MMLU_high_school_chemistry":0.3596059113,
        "MMLU_high_school_computer_science":0.46,
        "MMLU_high_school_european_history":0.5575757576,
        "MMLU_high_school_geography":0.5808080808,
        "MMLU_high_school_government_and_politics":0.6994818653,
        "MMLU_high_school_macroeconomics":0.4307692308,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.4159663866,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.6550458716,
        "MMLU_high_school_statistics":0.3564814815,
        "MMLU_high_school_us_history":0.5784313725,
        "MMLU_high_school_world_history":0.5907172996,
        "MMLU_human_aging":0.5784753363,
        "MMLU_human_sexuality":0.6106870229,
        "MMLU_international_law":0.6446280992,
        "MMLU_jurisprudence":0.5462962963,
        "MMLU_logical_fallacies":0.5766871166,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.6019417476,
        "MMLU_marketing":0.7008547009,
        "MMLU_medical_genetics":0.47,
        "MMLU_miscellaneous":0.6858237548,
        "MMLU_moral_disputes":0.5144508671,
        "MMLU_moral_scenarios":0.2502793296,
        "MMLU_nutrition":0.4836601307,
        "MMLU_philosophy":0.5691318328,
        "MMLU_prehistory":0.5462962963,
        "MMLU_professional_accounting":0.3510638298,
        "MMLU_professional_law":0.3174706649,
        "MMLU_professional_medicine":0.375,
        "MMLU_professional_psychology":0.4526143791,
        "MMLU_public_relations":0.5090909091,
        "MMLU_security_studies":0.5306122449,
        "MMLU_sociology":0.6019900498,
        "MMLU_us_foreign_policy":0.68,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.701754386
    },
    {
        "Model":"llama-13b-pretrained",
        "URL":"https:\/\/huggingface.co\/dvruette\/llama-13b-pretrained",
        "full_model_name":"dvruette\/llama-13b-pretrained",
        "Parameters":13.0,
        "MMLU_average":0.4702611278,
        "arc:challenge|25":0.5332764505,
        "hellaswag|10":0.5932085242,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.4444444444,
        "MMLU_astronomy":0.4342105263,
        "MMLU_business_ethics":0.42,
        "MMLU_clinical_knowledge":0.4830188679,
        "MMLU_college_biology":0.4583333333,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.4161849711,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.4042553191,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.4482758621,
        "MMLU_elementary_mathematics":0.3068783069,
        "MMLU_formal_logic":0.2777777778,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.5193548387,
        "MMLU_high_school_chemistry":0.3201970443,
        "MMLU_high_school_computer_science":0.46,
        "MMLU_high_school_european_history":0.5818181818,
        "MMLU_high_school_geography":0.5404040404,
        "MMLU_high_school_government_and_politics":0.6528497409,
        "MMLU_high_school_macroeconomics":0.4358974359,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.4831932773,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.6366972477,
        "MMLU_high_school_statistics":0.287037037,
        "MMLU_high_school_us_history":0.6078431373,
        "MMLU_high_school_world_history":0.6835443038,
        "MMLU_human_aging":0.5515695067,
        "MMLU_human_sexuality":0.5496183206,
        "MMLU_international_law":0.6694214876,
        "MMLU_jurisprudence":0.5277777778,
        "MMLU_logical_fallacies":0.4846625767,
        "MMLU_machine_learning":0.2946428571,
        "MMLU_management":0.6601941748,
        "MMLU_marketing":0.7222222222,
        "MMLU_medical_genetics":0.53,
        "MMLU_miscellaneous":0.6653895275,
        "MMLU_moral_disputes":0.4942196532,
        "MMLU_moral_scenarios":0.2558659218,
        "MMLU_nutrition":0.5098039216,
        "MMLU_philosophy":0.5466237942,
        "MMLU_prehistory":0.5092592593,
        "MMLU_professional_accounting":0.3581560284,
        "MMLU_professional_law":0.3826597132,
        "MMLU_professional_medicine":0.4779411765,
        "MMLU_professional_psychology":0.4558823529,
        "MMLU_public_relations":0.5363636364,
        "MMLU_security_studies":0.493877551,
        "MMLU_sociology":0.5771144279,
        "MMLU_us_foreign_policy":0.75,
        "MMLU_virology":0.4337349398,
        "MMLU_world_religions":0.6783625731
    },
    {
        "Model":"LLaMa-2-PeanutButter_v10-7B",
        "URL":"https:\/\/huggingface.co\/PeanutJar\/LLaMa-2-PeanutButter_v10-7B",
        "full_model_name":"PeanutJar\/LLaMa-2-PeanutButter_v10-7B",
        "Parameters":7.0,
        "MMLU_average":0.4697318134,
        "arc:challenge|25":0.5093856655,
        "hellaswag|10":0.6230830512,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.4078947368,
        "MMLU_business_ethics":0.47,
        "MMLU_clinical_knowledge":0.4867924528,
        "MMLU_college_biology":0.4722222222,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.38,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.4161849711,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.54,
        "MMLU_conceptual_physics":0.4595744681,
        "MMLU_econometrics":0.3771929825,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.3174603175,
        "MMLU_formal_logic":0.2380952381,
        "MMLU_global_facts":0.4,
        "MMLU_high_school_biology":0.5193548387,
        "MMLU_high_school_chemistry":0.3842364532,
        "MMLU_high_school_computer_science":0.47,
        "MMLU_high_school_european_history":0.5757575758,
        "MMLU_high_school_geography":0.5303030303,
        "MMLU_high_school_government_and_politics":0.6839378238,
        "MMLU_high_school_macroeconomics":0.441025641,
        "MMLU_high_school_mathematics":0.3148148148,
        "MMLU_high_school_microeconomics":0.4453781513,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.6275229358,
        "MMLU_high_school_statistics":0.2592592593,
        "MMLU_high_school_us_history":0.5735294118,
        "MMLU_high_school_world_history":0.611814346,
        "MMLU_human_aging":0.5291479821,
        "MMLU_human_sexuality":0.5496183206,
        "MMLU_international_law":0.6115702479,
        "MMLU_jurisprudence":0.4907407407,
        "MMLU_logical_fallacies":0.490797546,
        "MMLU_machine_learning":0.4017857143,
        "MMLU_management":0.5436893204,
        "MMLU_marketing":0.6794871795,
        "MMLU_medical_genetics":0.5,
        "MMLU_miscellaneous":0.6398467433,
        "MMLU_moral_disputes":0.4884393064,
        "MMLU_moral_scenarios":0.2849162011,
        "MMLU_nutrition":0.4967320261,
        "MMLU_philosophy":0.575562701,
        "MMLU_prehistory":0.5092592593,
        "MMLU_professional_accounting":0.3723404255,
        "MMLU_professional_law":0.3441981747,
        "MMLU_professional_medicine":0.4963235294,
        "MMLU_professional_psychology":0.4493464052,
        "MMLU_public_relations":0.5181818182,
        "MMLU_security_studies":0.5469387755,
        "MMLU_sociology":0.6169154229,
        "MMLU_us_foreign_policy":0.69,
        "MMLU_virology":0.4096385542,
        "MMLU_world_religions":0.6959064327
    },
    {
        "Model":"airoboros-13b-gpt4-1.3",
        "URL":"https:\/\/huggingface.co\/jondurbin\/airoboros-13b-gpt4-1.3",
        "full_model_name":"jondurbin\/airoboros-13b-gpt4-1.3",
        "Parameters":13.0,
        "MMLU_average":0.4695510565,
        "arc:challenge|25":0.5571672355,
        "hellaswag|10":0.632144991,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.4078947368,
        "MMLU_business_ethics":0.45,
        "MMLU_clinical_knowledge":0.4754716981,
        "MMLU_college_biology":0.4861111111,
        "MMLU_college_chemistry":0.29,
        "MMLU_college_computer_science":0.37,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.3988439306,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.4468085106,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4,
        "MMLU_elementary_mathematics":0.253968254,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.5193548387,
        "MMLU_high_school_chemistry":0.2906403941,
        "MMLU_high_school_computer_science":0.44,
        "MMLU_high_school_european_history":0.6060606061,
        "MMLU_high_school_geography":0.5454545455,
        "MMLU_high_school_government_and_politics":0.6476683938,
        "MMLU_high_school_macroeconomics":0.3794871795,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.4033613445,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.6422018349,
        "MMLU_high_school_statistics":0.2175925926,
        "MMLU_high_school_us_history":0.5882352941,
        "MMLU_high_school_world_history":0.7088607595,
        "MMLU_human_aging":0.600896861,
        "MMLU_human_sexuality":0.572519084,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.5277777778,
        "MMLU_logical_fallacies":0.5521472393,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.6990291262,
        "MMLU_marketing":0.7264957265,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.6781609195,
        "MMLU_moral_disputes":0.5115606936,
        "MMLU_moral_scenarios":0.269273743,
        "MMLU_nutrition":0.5,
        "MMLU_philosophy":0.536977492,
        "MMLU_prehistory":0.512345679,
        "MMLU_professional_accounting":0.3439716312,
        "MMLU_professional_law":0.3839634941,
        "MMLU_professional_medicine":0.4301470588,
        "MMLU_professional_psychology":0.4869281046,
        "MMLU_public_relations":0.5454545455,
        "MMLU_security_studies":0.4367346939,
        "MMLU_sociology":0.6467661692,
        "MMLU_us_foreign_policy":0.71,
        "MMLU_virology":0.4397590361,
        "MMLU_world_religions":0.6900584795
    },
    {
        "Model":"baichuan-vicuna-chinese-7b",
        "URL":"https:\/\/huggingface.co\/fireballoon\/baichuan-vicuna-chinese-7b",
        "full_model_name":"fireballoon\/baichuan-vicuna-chinese-7b",
        "Parameters":7.0,
        "MMLU_average":0.468747793,
        "arc:challenge|25":0.4112627986,
        "hellaswag|10":0.534355706,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.5328947368,
        "MMLU_business_ethics":0.55,
        "MMLU_clinical_knowledge":0.4528301887,
        "MMLU_college_biology":0.4444444444,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.4046242775,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.64,
        "MMLU_conceptual_physics":0.3829787234,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.328042328,
        "MMLU_formal_logic":0.2619047619,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.4935483871,
        "MMLU_high_school_chemistry":0.3399014778,
        "MMLU_high_school_computer_science":0.49,
        "MMLU_high_school_european_history":0.5696969697,
        "MMLU_high_school_geography":0.5909090909,
        "MMLU_high_school_government_and_politics":0.6424870466,
        "MMLU_high_school_macroeconomics":0.3871794872,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.3781512605,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.6183486239,
        "MMLU_high_school_statistics":0.2824074074,
        "MMLU_high_school_us_history":0.5637254902,
        "MMLU_high_school_world_history":0.6329113924,
        "MMLU_human_aging":0.5605381166,
        "MMLU_human_sexuality":0.5419847328,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.5648148148,
        "MMLU_logical_fallacies":0.490797546,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.572815534,
        "MMLU_marketing":0.688034188,
        "MMLU_medical_genetics":0.53,
        "MMLU_miscellaneous":0.656449553,
        "MMLU_moral_disputes":0.4971098266,
        "MMLU_moral_scenarios":0.2625698324,
        "MMLU_nutrition":0.4673202614,
        "MMLU_philosophy":0.501607717,
        "MMLU_prehistory":0.5061728395,
        "MMLU_professional_accounting":0.4219858156,
        "MMLU_professional_law":0.350065189,
        "MMLU_professional_medicine":0.3455882353,
        "MMLU_professional_psychology":0.4754901961,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.5306122449,
        "MMLU_sociology":0.6567164179,
        "MMLU_us_foreign_policy":0.68,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.6783625731
    },
    {
        "Model":"Llama-2-7b-hf",
        "URL":"https:\/\/huggingface.co\/meta-llama\/Llama-2-7b-hf",
        "full_model_name":"meta-llama\/Llama-2-7b-hf",
        "Parameters":7.0,
        "MMLU_average":0.46866075,
        "arc:challenge|25":0.4923208191,
        "hellaswag|10":0.5883290181,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.4078947368,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.4641509434,
        "MMLU_college_biology":0.4652777778,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.4277456647,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.6,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.2724867725,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.5,
        "MMLU_high_school_chemistry":0.3694581281,
        "MMLU_high_school_computer_science":0.4,
        "MMLU_high_school_european_history":0.6424242424,
        "MMLU_high_school_geography":0.4848484848,
        "MMLU_high_school_government_and_politics":0.6735751295,
        "MMLU_high_school_macroeconomics":0.458974359,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.4411764706,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.6293577982,
        "MMLU_high_school_statistics":0.2731481481,
        "MMLU_high_school_us_history":0.5343137255,
        "MMLU_high_school_world_history":0.6286919831,
        "MMLU_human_aging":0.5695067265,
        "MMLU_human_sexuality":0.5648854962,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.537037037,
        "MMLU_logical_fallacies":0.509202454,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.5533980583,
        "MMLU_marketing":0.6923076923,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.6411238825,
        "MMLU_moral_disputes":0.4942196532,
        "MMLU_moral_scenarios":0.2391061453,
        "MMLU_nutrition":0.4934640523,
        "MMLU_philosophy":0.6012861736,
        "MMLU_prehistory":0.4907407407,
        "MMLU_professional_accounting":0.3617021277,
        "MMLU_professional_law":0.3617992177,
        "MMLU_professional_medicine":0.5257352941,
        "MMLU_professional_psychology":0.4411764706,
        "MMLU_public_relations":0.5272727273,
        "MMLU_security_studies":0.4775510204,
        "MMLU_sociology":0.631840796,
        "MMLU_us_foreign_policy":0.65,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.701754386
    },
    {
        "Model":"ToolLLaMA-7b-LoRA",
        "URL":"https:\/\/huggingface.co\/ToolBench\/ToolLLaMA-7b-LoRA",
        "full_model_name":"ToolBench\/ToolLLaMA-7b-LoRA",
        "Parameters":7.0,
        "MMLU_average":0.4686590949,
        "arc:challenge|25":0.4923208191,
        "hellaswag|10":0.5880302729,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.4078947368,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.4641509434,
        "MMLU_college_biology":0.4652777778,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.4277456647,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.6,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.2724867725,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.5064516129,
        "MMLU_high_school_chemistry":0.3596059113,
        "MMLU_high_school_computer_science":0.41,
        "MMLU_high_school_european_history":0.6424242424,
        "MMLU_high_school_geography":0.5050505051,
        "MMLU_high_school_government_and_politics":0.6580310881,
        "MMLU_high_school_macroeconomics":0.4641025641,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.4243697479,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.6275229358,
        "MMLU_high_school_statistics":0.287037037,
        "MMLU_high_school_us_history":0.5294117647,
        "MMLU_high_school_world_history":0.641350211,
        "MMLU_human_aging":0.5695067265,
        "MMLU_human_sexuality":0.5648854962,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.5277777778,
        "MMLU_logical_fallacies":0.5337423313,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.5533980583,
        "MMLU_marketing":0.688034188,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.6462324393,
        "MMLU_moral_disputes":0.5057803468,
        "MMLU_moral_scenarios":0.2402234637,
        "MMLU_nutrition":0.4934640523,
        "MMLU_philosophy":0.5916398714,
        "MMLU_prehistory":0.4907407407,
        "MMLU_professional_accounting":0.3617021277,
        "MMLU_professional_law":0.3637548892,
        "MMLU_professional_medicine":0.5147058824,
        "MMLU_professional_psychology":0.4493464052,
        "MMLU_public_relations":0.5363636364,
        "MMLU_security_studies":0.4775510204,
        "MMLU_sociology":0.631840796,
        "MMLU_us_foreign_policy":0.65,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.7192982456
    },
    {
        "Model":"test-llama-2-7b",
        "URL":"https:\/\/huggingface.co\/bongchoi\/test-llama-2-7b",
        "full_model_name":"bongchoi\/test-llama-2-7b",
        "Parameters":7.0,
        "MMLU_average":0.4685598909,
        "arc:challenge|25":0.4931740614,
        "hellaswag|10":0.5884285999,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.4078947368,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.4641509434,
        "MMLU_college_biology":0.4652777778,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4219653179,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.6,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.5,
        "MMLU_high_school_chemistry":0.3645320197,
        "MMLU_high_school_computer_science":0.4,
        "MMLU_high_school_european_history":0.6363636364,
        "MMLU_high_school_geography":0.4898989899,
        "MMLU_high_school_government_and_politics":0.6787564767,
        "MMLU_high_school_macroeconomics":0.458974359,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.4411764706,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.6311926606,
        "MMLU_high_school_statistics":0.2731481481,
        "MMLU_high_school_us_history":0.5343137255,
        "MMLU_high_school_world_history":0.6286919831,
        "MMLU_human_aging":0.5650224215,
        "MMLU_human_sexuality":0.5648854962,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.537037037,
        "MMLU_logical_fallacies":0.5153374233,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.5533980583,
        "MMLU_marketing":0.6923076923,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.6398467433,
        "MMLU_moral_disputes":0.4942196532,
        "MMLU_moral_scenarios":0.2391061453,
        "MMLU_nutrition":0.4967320261,
        "MMLU_philosophy":0.6012861736,
        "MMLU_prehistory":0.487654321,
        "MMLU_professional_accounting":0.3617021277,
        "MMLU_professional_law":0.3624511082,
        "MMLU_professional_medicine":0.5257352941,
        "MMLU_professional_psychology":0.4411764706,
        "MMLU_public_relations":0.5272727273,
        "MMLU_security_studies":0.4857142857,
        "MMLU_sociology":0.631840796,
        "MMLU_us_foreign_policy":0.65,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.701754386
    },
    {
        "Model":"test-llama2-7b",
        "URL":"https:\/\/huggingface.co\/bongchoi\/test-llama2-7b",
        "full_model_name":"bongchoi\/test-llama2-7b",
        "Parameters":7.0,
        "MMLU_average":0.4685598909,
        "arc:challenge|25":0.4931740614,
        "hellaswag|10":0.5884285999,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.4078947368,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.4641509434,
        "MMLU_college_biology":0.4652777778,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4219653179,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.6,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.5,
        "MMLU_high_school_chemistry":0.3645320197,
        "MMLU_high_school_computer_science":0.4,
        "MMLU_high_school_european_history":0.6363636364,
        "MMLU_high_school_geography":0.4898989899,
        "MMLU_high_school_government_and_politics":0.6787564767,
        "MMLU_high_school_macroeconomics":0.458974359,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.4411764706,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.6311926606,
        "MMLU_high_school_statistics":0.2731481481,
        "MMLU_high_school_us_history":0.5343137255,
        "MMLU_high_school_world_history":0.6286919831,
        "MMLU_human_aging":0.5650224215,
        "MMLU_human_sexuality":0.5648854962,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.537037037,
        "MMLU_logical_fallacies":0.5153374233,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.5533980583,
        "MMLU_marketing":0.6923076923,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.6398467433,
        "MMLU_moral_disputes":0.4942196532,
        "MMLU_moral_scenarios":0.2391061453,
        "MMLU_nutrition":0.4967320261,
        "MMLU_philosophy":0.6012861736,
        "MMLU_prehistory":0.487654321,
        "MMLU_professional_accounting":0.3617021277,
        "MMLU_professional_law":0.3624511082,
        "MMLU_professional_medicine":0.5257352941,
        "MMLU_professional_psychology":0.4411764706,
        "MMLU_public_relations":0.5272727273,
        "MMLU_security_studies":0.4857142857,
        "MMLU_sociology":0.631840796,
        "MMLU_us_foreign_policy":0.65,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.701754386
    },
    {
        "Model":"araproje-llama2-7b-hf",
        "URL":"https:\/\/huggingface.co\/ibranze\/araproje-llama2-7b-hf",
        "full_model_name":"ibranze\/araproje-llama2-7b-hf",
        "Parameters":7.0,
        "MMLU_average":0.4679969504,
        "arc:challenge|25":0.4914675768,
        "hellaswag|10":0.5884285999,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.4078947368,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.4641509434,
        "MMLU_college_biology":0.4652777778,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4277456647,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.6,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.5,
        "MMLU_high_school_chemistry":0.3645320197,
        "MMLU_high_school_computer_science":0.4,
        "MMLU_high_school_european_history":0.5939393939,
        "MMLU_high_school_geography":0.4898989899,
        "MMLU_high_school_government_and_politics":0.6787564767,
        "MMLU_high_school_macroeconomics":0.458974359,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.4411764706,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.6311926606,
        "MMLU_high_school_statistics":0.2731481481,
        "MMLU_high_school_us_history":0.5441176471,
        "MMLU_high_school_world_history":0.6329113924,
        "MMLU_human_aging":0.5650224215,
        "MMLU_human_sexuality":0.5648854962,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.537037037,
        "MMLU_logical_fallacies":0.5153374233,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.5533980583,
        "MMLU_marketing":0.6923076923,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.6398467433,
        "MMLU_moral_disputes":0.4942196532,
        "MMLU_moral_scenarios":0.2391061453,
        "MMLU_nutrition":0.4967320261,
        "MMLU_philosophy":0.6012861736,
        "MMLU_prehistory":0.487654321,
        "MMLU_professional_accounting":0.3617021277,
        "MMLU_professional_law":0.3611473272,
        "MMLU_professional_medicine":0.5257352941,
        "MMLU_professional_psychology":0.4411764706,
        "MMLU_public_relations":0.5272727273,
        "MMLU_security_studies":0.4775510204,
        "MMLU_sociology":0.631840796,
        "MMLU_us_foreign_policy":0.65,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.701754386
    },
    {
        "Model":"Starlight-7B",
        "URL":"https:\/\/huggingface.co\/NewstaR\/Starlight-7B",
        "full_model_name":"NewstaR\/Starlight-7B",
        "Parameters":7.0,
        "MMLU_average":0.4679969504,
        "arc:challenge|25":0.4914675768,
        "hellaswag|10":0.5884285999,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.4078947368,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.4641509434,
        "MMLU_college_biology":0.4652777778,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4277456647,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.6,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.5,
        "MMLU_high_school_chemistry":0.3645320197,
        "MMLU_high_school_computer_science":0.4,
        "MMLU_high_school_european_history":0.5939393939,
        "MMLU_high_school_geography":0.4898989899,
        "MMLU_high_school_government_and_politics":0.6787564767,
        "MMLU_high_school_macroeconomics":0.458974359,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.4411764706,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.6311926606,
        "MMLU_high_school_statistics":0.2731481481,
        "MMLU_high_school_us_history":0.5441176471,
        "MMLU_high_school_world_history":0.6329113924,
        "MMLU_human_aging":0.5650224215,
        "MMLU_human_sexuality":0.5648854962,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.537037037,
        "MMLU_logical_fallacies":0.5153374233,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.5533980583,
        "MMLU_marketing":0.6923076923,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.6398467433,
        "MMLU_moral_disputes":0.4942196532,
        "MMLU_moral_scenarios":0.2391061453,
        "MMLU_nutrition":0.4967320261,
        "MMLU_philosophy":0.6012861736,
        "MMLU_prehistory":0.487654321,
        "MMLU_professional_accounting":0.3617021277,
        "MMLU_professional_law":0.3611473272,
        "MMLU_professional_medicine":0.5257352941,
        "MMLU_professional_psychology":0.4411764706,
        "MMLU_public_relations":0.5272727273,
        "MMLU_security_studies":0.4775510204,
        "MMLU_sociology":0.631840796,
        "MMLU_us_foreign_policy":0.65,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.701754386
    },
    {
        "Model":"stack-llama-2",
        "URL":"https:\/\/huggingface.co\/kashif\/stack-llama-2",
        "full_model_name":"kashif\/stack-llama-2",
        "Parameters":null,
        "MMLU_average":0.4679969504,
        "arc:challenge|25":0.4914675768,
        "hellaswag|10":0.5884285999,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.4078947368,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.4641509434,
        "MMLU_college_biology":0.4652777778,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4277456647,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.6,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.5,
        "MMLU_high_school_chemistry":0.3645320197,
        "MMLU_high_school_computer_science":0.4,
        "MMLU_high_school_european_history":0.5939393939,
        "MMLU_high_school_geography":0.4898989899,
        "MMLU_high_school_government_and_politics":0.6787564767,
        "MMLU_high_school_macroeconomics":0.458974359,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.4411764706,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.6311926606,
        "MMLU_high_school_statistics":0.2731481481,
        "MMLU_high_school_us_history":0.5441176471,
        "MMLU_high_school_world_history":0.6329113924,
        "MMLU_human_aging":0.5650224215,
        "MMLU_human_sexuality":0.5648854962,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.537037037,
        "MMLU_logical_fallacies":0.5153374233,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.5533980583,
        "MMLU_marketing":0.6923076923,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.6398467433,
        "MMLU_moral_disputes":0.4942196532,
        "MMLU_moral_scenarios":0.2391061453,
        "MMLU_nutrition":0.4967320261,
        "MMLU_philosophy":0.6012861736,
        "MMLU_prehistory":0.487654321,
        "MMLU_professional_accounting":0.3617021277,
        "MMLU_professional_law":0.3611473272,
        "MMLU_professional_medicine":0.5257352941,
        "MMLU_professional_psychology":0.4411764706,
        "MMLU_public_relations":0.5272727273,
        "MMLU_security_studies":0.4775510204,
        "MMLU_sociology":0.631840796,
        "MMLU_us_foreign_policy":0.65,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.701754386
    },
    {
        "Model":"llama2-7b-chat-hf-v4",
        "URL":"https:\/\/huggingface.co\/TheTravellingEngineer\/llama2-7b-chat-hf-v4",
        "full_model_name":"TheTravellingEngineer\/llama2-7b-chat-hf-v4",
        "Parameters":7.0,
        "MMLU_average":0.4679969504,
        "arc:challenge|25":0.4914675768,
        "hellaswag|10":0.5884285999,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.4078947368,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.4641509434,
        "MMLU_college_biology":0.4652777778,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4277456647,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.6,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.5,
        "MMLU_high_school_chemistry":0.3645320197,
        "MMLU_high_school_computer_science":0.4,
        "MMLU_high_school_european_history":0.5939393939,
        "MMLU_high_school_geography":0.4898989899,
        "MMLU_high_school_government_and_politics":0.6787564767,
        "MMLU_high_school_macroeconomics":0.458974359,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.4411764706,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.6311926606,
        "MMLU_high_school_statistics":0.2731481481,
        "MMLU_high_school_us_history":0.5441176471,
        "MMLU_high_school_world_history":0.6329113924,
        "MMLU_human_aging":0.5650224215,
        "MMLU_human_sexuality":0.5648854962,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.537037037,
        "MMLU_logical_fallacies":0.5153374233,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.5533980583,
        "MMLU_marketing":0.6923076923,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.6398467433,
        "MMLU_moral_disputes":0.4942196532,
        "MMLU_moral_scenarios":0.2391061453,
        "MMLU_nutrition":0.4967320261,
        "MMLU_philosophy":0.6012861736,
        "MMLU_prehistory":0.487654321,
        "MMLU_professional_accounting":0.3617021277,
        "MMLU_professional_law":0.3611473272,
        "MMLU_professional_medicine":0.5257352941,
        "MMLU_professional_psychology":0.4411764706,
        "MMLU_public_relations":0.5272727273,
        "MMLU_security_studies":0.4775510204,
        "MMLU_sociology":0.631840796,
        "MMLU_us_foreign_policy":0.65,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.701754386
    },
    {
        "Model":"llama2-7b-chat-hf-dpo",
        "URL":"https:\/\/huggingface.co\/TheTravellingEngineer\/llama2-7b-chat-hf-dpo",
        "full_model_name":"TheTravellingEngineer\/llama2-7b-chat-hf-dpo",
        "Parameters":7.0,
        "MMLU_average":0.4678408871,
        "arc:challenge|25":0.5017064846,
        "hellaswag|10":0.5978888668,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4148148148,
        "MMLU_astronomy":0.4934210526,
        "MMLU_business_ethics":0.43,
        "MMLU_clinical_knowledge":0.5169811321,
        "MMLU_college_biology":0.5416666667,
        "MMLU_college_chemistry":0.25,
        "MMLU_college_computer_science":0.36,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.4046242775,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.54,
        "MMLU_conceptual_physics":0.3914893617,
        "MMLU_econometrics":0.3333333333,
        "MMLU_electrical_engineering":0.4275862069,
        "MMLU_elementary_mathematics":0.2883597884,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.5258064516,
        "MMLU_high_school_chemistry":0.3497536946,
        "MMLU_high_school_computer_science":0.45,
        "MMLU_high_school_european_history":0.5818181818,
        "MMLU_high_school_geography":0.595959596,
        "MMLU_high_school_government_and_politics":0.6683937824,
        "MMLU_high_school_macroeconomics":0.4128205128,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.4285714286,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.6495412844,
        "MMLU_high_school_statistics":0.3287037037,
        "MMLU_high_school_us_history":0.6764705882,
        "MMLU_high_school_world_history":0.6582278481,
        "MMLU_human_aging":0.5695067265,
        "MMLU_human_sexuality":0.4961832061,
        "MMLU_international_law":0.5619834711,
        "MMLU_jurisprudence":0.5833333333,
        "MMLU_logical_fallacies":0.5398773006,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.6699029126,
        "MMLU_marketing":0.7393162393,
        "MMLU_medical_genetics":0.5,
        "MMLU_miscellaneous":0.6730523627,
        "MMLU_moral_disputes":0.5202312139,
        "MMLU_moral_scenarios":0.2156424581,
        "MMLU_nutrition":0.5196078431,
        "MMLU_philosophy":0.5691318328,
        "MMLU_prehistory":0.5648148148,
        "MMLU_professional_accounting":0.3546099291,
        "MMLU_professional_law":0.3344198175,
        "MMLU_professional_medicine":0.3566176471,
        "MMLU_professional_psychology":0.4722222222,
        "MMLU_public_relations":0.5181818182,
        "MMLU_security_studies":0.4693877551,
        "MMLU_sociology":0.5771144279,
        "MMLU_us_foreign_policy":0.67,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.701754386
    },
    {
        "Model":"Guanaco-Vicuna-7B-L2",
        "URL":"https:\/\/huggingface.co\/LTC-AI-Labs\/Guanaco-Vicuna-7B-L2",
        "full_model_name":"LTC-AI-Labs\/Guanaco-Vicuna-7B-L2",
        "Parameters":7.0,
        "MMLU_average":0.4676824405,
        "arc:challenge|25":0.4957337884,
        "hellaswag|10":0.5924118701,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.4210526316,
        "MMLU_business_ethics":0.49,
        "MMLU_clinical_knowledge":0.4716981132,
        "MMLU_college_biology":0.4236111111,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.37,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.4450867052,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.4255319149,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.3862068966,
        "MMLU_elementary_mathematics":0.2698412698,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.4870967742,
        "MMLU_high_school_chemistry":0.3201970443,
        "MMLU_high_school_computer_science":0.44,
        "MMLU_high_school_european_history":0.6,
        "MMLU_high_school_geography":0.5454545455,
        "MMLU_high_school_government_and_politics":0.6787564767,
        "MMLU_high_school_macroeconomics":0.4384615385,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.4411764706,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.6550458716,
        "MMLU_high_school_statistics":0.2314814815,
        "MMLU_high_school_us_history":0.5490196078,
        "MMLU_high_school_world_history":0.6540084388,
        "MMLU_human_aging":0.5650224215,
        "MMLU_human_sexuality":0.5419847328,
        "MMLU_international_law":0.6198347107,
        "MMLU_jurisprudence":0.5462962963,
        "MMLU_logical_fallacies":0.5153374233,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.6116504854,
        "MMLU_marketing":0.7008547009,
        "MMLU_medical_genetics":0.53,
        "MMLU_miscellaneous":0.632183908,
        "MMLU_moral_disputes":0.5173410405,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.5065359477,
        "MMLU_philosophy":0.61414791,
        "MMLU_prehistory":0.4845679012,
        "MMLU_professional_accounting":0.365248227,
        "MMLU_professional_law":0.3507170795,
        "MMLU_professional_medicine":0.5073529412,
        "MMLU_professional_psychology":0.4607843137,
        "MMLU_public_relations":0.5636363636,
        "MMLU_security_studies":0.4489795918,
        "MMLU_sociology":0.671641791,
        "MMLU_us_foreign_policy":0.62,
        "MMLU_virology":0.4096385542,
        "MMLU_world_religions":0.7076023392
    },
    {
        "Model":"llama-2-7b-claude-chat",
        "URL":"https:\/\/huggingface.co\/Norquinal\/llama-2-7b-claude-chat",
        "full_model_name":"Norquinal\/llama-2-7b-claude-chat",
        "Parameters":7.0,
        "MMLU_average":0.4674187763,
        "arc:challenge|25":0.5136518771,
        "hellaswag|10":0.6053574985,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4518518519,
        "MMLU_astronomy":0.4078947368,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.4566037736,
        "MMLU_college_biology":0.4444444444,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.450867052,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.56,
        "MMLU_conceptual_physics":0.429787234,
        "MMLU_econometrics":0.3596491228,
        "MMLU_electrical_engineering":0.4344827586,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.28,
        "MMLU_high_school_biology":0.5161290323,
        "MMLU_high_school_chemistry":0.3300492611,
        "MMLU_high_school_computer_science":0.42,
        "MMLU_high_school_european_history":0.5878787879,
        "MMLU_high_school_geography":0.5353535354,
        "MMLU_high_school_government_and_politics":0.6787564767,
        "MMLU_high_school_macroeconomics":0.4384615385,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.4159663866,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.6311926606,
        "MMLU_high_school_statistics":0.2824074074,
        "MMLU_high_school_us_history":0.5931372549,
        "MMLU_high_school_world_history":0.6708860759,
        "MMLU_human_aging":0.5515695067,
        "MMLU_human_sexuality":0.5114503817,
        "MMLU_international_law":0.6198347107,
        "MMLU_jurisprudence":0.537037037,
        "MMLU_logical_fallacies":0.5337423313,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.5533980583,
        "MMLU_marketing":0.7051282051,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.6296296296,
        "MMLU_moral_disputes":0.5028901734,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.5163398693,
        "MMLU_philosophy":0.6012861736,
        "MMLU_prehistory":0.5061728395,
        "MMLU_professional_accounting":0.3546099291,
        "MMLU_professional_law":0.3611473272,
        "MMLU_professional_medicine":0.5257352941,
        "MMLU_professional_psychology":0.454248366,
        "MMLU_public_relations":0.5636363636,
        "MMLU_security_studies":0.4816326531,
        "MMLU_sociology":0.6417910448,
        "MMLU_us_foreign_policy":0.6,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.7134502924
    },
    {
        "Model":"Koss-7B-chat",
        "URL":"https:\/\/huggingface.co\/NewstaR\/Koss-7B-chat",
        "full_model_name":"NewstaR\/Koss-7B-chat",
        "Parameters":7.0,
        "MMLU_average":0.4671512718,
        "arc:challenge|25":0.5008532423,
        "hellaswag|10":0.5978888668,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4148148148,
        "MMLU_astronomy":0.4934210526,
        "MMLU_business_ethics":0.43,
        "MMLU_clinical_knowledge":0.5169811321,
        "MMLU_college_biology":0.5416666667,
        "MMLU_college_chemistry":0.25,
        "MMLU_college_computer_science":0.36,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.4046242775,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.54,
        "MMLU_conceptual_physics":0.3914893617,
        "MMLU_econometrics":0.3333333333,
        "MMLU_electrical_engineering":0.4275862069,
        "MMLU_elementary_mathematics":0.2883597884,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.5258064516,
        "MMLU_high_school_chemistry":0.3497536946,
        "MMLU_high_school_computer_science":0.45,
        "MMLU_high_school_european_history":0.5454545455,
        "MMLU_high_school_geography":0.595959596,
        "MMLU_high_school_government_and_politics":0.6683937824,
        "MMLU_high_school_macroeconomics":0.4128205128,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.4285714286,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.6495412844,
        "MMLU_high_school_statistics":0.3287037037,
        "MMLU_high_school_us_history":0.6666666667,
        "MMLU_high_school_world_history":0.6582278481,
        "MMLU_human_aging":0.5695067265,
        "MMLU_human_sexuality":0.4961832061,
        "MMLU_international_law":0.5619834711,
        "MMLU_jurisprudence":0.5833333333,
        "MMLU_logical_fallacies":0.5398773006,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.6699029126,
        "MMLU_marketing":0.7393162393,
        "MMLU_medical_genetics":0.5,
        "MMLU_miscellaneous":0.6730523627,
        "MMLU_moral_disputes":0.5202312139,
        "MMLU_moral_scenarios":0.2156424581,
        "MMLU_nutrition":0.5196078431,
        "MMLU_philosophy":0.5691318328,
        "MMLU_prehistory":0.5648148148,
        "MMLU_professional_accounting":0.3546099291,
        "MMLU_professional_law":0.3331160365,
        "MMLU_professional_medicine":0.3566176471,
        "MMLU_professional_psychology":0.4722222222,
        "MMLU_public_relations":0.5181818182,
        "MMLU_security_studies":0.4775510204,
        "MMLU_sociology":0.5771144279,
        "MMLU_us_foreign_policy":0.67,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.701754386
    },
    {
        "Model":"Luna-AI-Llama2-Uncensored",
        "URL":"https:\/\/huggingface.co\/Tap-M\/Luna-AI-Llama2-Uncensored",
        "full_model_name":"Tap-M\/Luna-AI-Llama2-Uncensored",
        "Parameters":null,
        "MMLU_average":0.4669608332,
        "arc:challenge|25":0.5059726962,
        "hellaswag|10":0.5972913762,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4296296296,
        "MMLU_astronomy":0.3815789474,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.479245283,
        "MMLU_college_biology":0.4236111111,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.48,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.3988439306,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.52,
        "MMLU_conceptual_physics":0.3957446809,
        "MMLU_econometrics":0.3771929825,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.291005291,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.5193548387,
        "MMLU_high_school_chemistry":0.3054187192,
        "MMLU_high_school_computer_science":0.39,
        "MMLU_high_school_european_history":0.6484848485,
        "MMLU_high_school_geography":0.5404040404,
        "MMLU_high_school_government_and_politics":0.6632124352,
        "MMLU_high_school_macroeconomics":0.4333333333,
        "MMLU_high_school_mathematics":0.2888888889,
        "MMLU_high_school_microeconomics":0.4075630252,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.6073394495,
        "MMLU_high_school_statistics":0.3194444444,
        "MMLU_high_school_us_history":0.5637254902,
        "MMLU_high_school_world_history":0.6624472574,
        "MMLU_human_aging":0.5739910314,
        "MMLU_human_sexuality":0.5419847328,
        "MMLU_international_law":0.5950413223,
        "MMLU_jurisprudence":0.5185185185,
        "MMLU_logical_fallacies":0.490797546,
        "MMLU_machine_learning":0.4285714286,
        "MMLU_management":0.4951456311,
        "MMLU_marketing":0.7136752137,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.6296296296,
        "MMLU_moral_disputes":0.5375722543,
        "MMLU_moral_scenarios":0.2592178771,
        "MMLU_nutrition":0.522875817,
        "MMLU_philosophy":0.5466237942,
        "MMLU_prehistory":0.5154320988,
        "MMLU_professional_accounting":0.3936170213,
        "MMLU_professional_law":0.3585397653,
        "MMLU_professional_medicine":0.4963235294,
        "MMLU_professional_psychology":0.4575163399,
        "MMLU_public_relations":0.5636363636,
        "MMLU_security_studies":0.4693877551,
        "MMLU_sociology":0.5870646766,
        "MMLU_us_foreign_policy":0.7,
        "MMLU_virology":0.4036144578,
        "MMLU_world_religions":0.6725146199
    },
    {
        "Model":"llama2-7b-oa",
        "URL":"https:\/\/huggingface.co\/TinyPixel\/llama2-7b-oa",
        "full_model_name":"TinyPixel\/llama2-7b-oa",
        "Parameters":7.0,
        "MMLU_average":0.4667861825,
        "arc:challenge|25":0.497440273,
        "hellaswag|10":0.5893248357,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.4013157895,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.4641509434,
        "MMLU_college_biology":0.4513888889,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.36,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.4335260116,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.4255319149,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.2698412698,
        "MMLU_formal_logic":0.2777777778,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.5,
        "MMLU_high_school_chemistry":0.3596059113,
        "MMLU_high_school_computer_science":0.4,
        "MMLU_high_school_european_history":0.6242424242,
        "MMLU_high_school_geography":0.4949494949,
        "MMLU_high_school_government_and_politics":0.6943005181,
        "MMLU_high_school_macroeconomics":0.441025641,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.4495798319,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.6330275229,
        "MMLU_high_school_statistics":0.2731481481,
        "MMLU_high_school_us_history":0.5294117647,
        "MMLU_high_school_world_history":0.5991561181,
        "MMLU_human_aging":0.5605381166,
        "MMLU_human_sexuality":0.5419847328,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.5277777778,
        "MMLU_logical_fallacies":0.5214723926,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.5631067961,
        "MMLU_marketing":0.7008547009,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.6449553001,
        "MMLU_moral_disputes":0.5057803468,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.4934640523,
        "MMLU_philosophy":0.6045016077,
        "MMLU_prehistory":0.5,
        "MMLU_professional_accounting":0.3581560284,
        "MMLU_professional_law":0.3670143416,
        "MMLU_professional_medicine":0.5147058824,
        "MMLU_professional_psychology":0.4477124183,
        "MMLU_public_relations":0.5363636364,
        "MMLU_security_studies":0.4857142857,
        "MMLU_sociology":0.6467661692,
        "MMLU_us_foreign_policy":0.63,
        "MMLU_virology":0.3975903614,
        "MMLU_world_religions":0.701754386
    },
    {
        "Model":"chatglm2-6b",
        "URL":"https:\/\/huggingface.co\/THUDM\/chatglm2-6b",
        "full_model_name":"THUDM\/chatglm2-6b",
        "Parameters":6.0,
        "MMLU_average":0.4666058488,
        "arc:challenge|25":0.3745733788,
        "hellaswag|10":0.4596693886,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.4148148148,
        "MMLU_astronomy":0.5328947368,
        "MMLU_business_ethics":0.61,
        "MMLU_clinical_knowledge":0.4867924528,
        "MMLU_college_biology":0.3888888889,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.4219653179,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.56,
        "MMLU_conceptual_physics":0.4,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.3174603175,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.5580645161,
        "MMLU_high_school_chemistry":0.4532019704,
        "MMLU_high_school_computer_science":0.42,
        "MMLU_high_school_european_history":0.6,
        "MMLU_high_school_geography":0.5555555556,
        "MMLU_high_school_government_and_politics":0.5699481865,
        "MMLU_high_school_macroeconomics":0.4487179487,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.4327731092,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.5926605505,
        "MMLU_high_school_statistics":0.337962963,
        "MMLU_high_school_us_history":0.5882352941,
        "MMLU_high_school_world_history":0.6624472574,
        "MMLU_human_aging":0.466367713,
        "MMLU_human_sexuality":0.4809160305,
        "MMLU_international_law":0.6033057851,
        "MMLU_jurisprudence":0.5833333333,
        "MMLU_logical_fallacies":0.490797546,
        "MMLU_machine_learning":0.4375,
        "MMLU_management":0.6213592233,
        "MMLU_marketing":0.6923076923,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.5964240102,
        "MMLU_moral_disputes":0.5289017341,
        "MMLU_moral_scenarios":0.2357541899,
        "MMLU_nutrition":0.5326797386,
        "MMLU_philosophy":0.498392283,
        "MMLU_prehistory":0.4938271605,
        "MMLU_professional_accounting":0.3333333333,
        "MMLU_professional_law":0.3559322034,
        "MMLU_professional_medicine":0.3529411765,
        "MMLU_professional_psychology":0.4444444444,
        "MMLU_public_relations":0.5818181818,
        "MMLU_security_studies":0.5673469388,
        "MMLU_sociology":0.6218905473,
        "MMLU_us_foreign_policy":0.73,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.4736842105
    },
    {
        "Model":"L2-7b-Base-WVG-Uncensored",
        "URL":"https:\/\/huggingface.co\/LTC-AI-Labs\/L2-7b-Base-WVG-Uncensored",
        "full_model_name":"LTC-AI-Labs\/L2-7b-Base-WVG-Uncensored",
        "Parameters":7.0,
        "MMLU_average":0.4664581275,
        "arc:challenge|25":0.4957337884,
        "hellaswag|10":0.593706433,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.4444444444,
        "MMLU_astronomy":0.4342105263,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.4679245283,
        "MMLU_college_biology":0.4722222222,
        "MMLU_college_chemistry":0.33,
        "MMLU_college_computer_science":0.34,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.4335260116,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.4425531915,
        "MMLU_econometrics":0.3859649123,
        "MMLU_electrical_engineering":0.4275862069,
        "MMLU_elementary_mathematics":0.2751322751,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.4677419355,
        "MMLU_high_school_chemistry":0.3054187192,
        "MMLU_high_school_computer_science":0.46,
        "MMLU_high_school_european_history":0.6,
        "MMLU_high_school_geography":0.5050505051,
        "MMLU_high_school_government_and_politics":0.6528497409,
        "MMLU_high_school_macroeconomics":0.4307692308,
        "MMLU_high_school_mathematics":0.2925925926,
        "MMLU_high_school_microeconomics":0.4369747899,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.6293577982,
        "MMLU_high_school_statistics":0.2546296296,
        "MMLU_high_school_us_history":0.5539215686,
        "MMLU_high_school_world_history":0.6244725738,
        "MMLU_human_aging":0.5515695067,
        "MMLU_human_sexuality":0.5496183206,
        "MMLU_international_law":0.6198347107,
        "MMLU_jurisprudence":0.5277777778,
        "MMLU_logical_fallacies":0.5337423313,
        "MMLU_machine_learning":0.4017857143,
        "MMLU_management":0.572815534,
        "MMLU_marketing":0.688034188,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.650063857,
        "MMLU_moral_disputes":0.5202312139,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.5196078431,
        "MMLU_philosophy":0.6109324759,
        "MMLU_prehistory":0.5030864198,
        "MMLU_professional_accounting":0.365248227,
        "MMLU_professional_law":0.3441981747,
        "MMLU_professional_medicine":0.5036764706,
        "MMLU_professional_psychology":0.4493464052,
        "MMLU_public_relations":0.5727272727,
        "MMLU_security_studies":0.4040816327,
        "MMLU_sociology":0.6218905473,
        "MMLU_us_foreign_policy":0.61,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.7076023392
    },
    {
        "Model":"llama-2-7b-instruct-peft",
        "URL":"https:\/\/huggingface.co\/dfurman\/llama-2-7b-instruct-peft",
        "full_model_name":"dfurman\/llama-2-7b-instruct-peft",
        "Parameters":7.0,
        "MMLU_average":0.4662934762,
        "arc:challenge|25":0.4744027304,
        "hellaswag|10":0.5892252539,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.5037037037,
        "MMLU_astronomy":0.4868421053,
        "MMLU_business_ethics":0.49,
        "MMLU_clinical_knowledge":0.4867924528,
        "MMLU_college_biology":0.4305555556,
        "MMLU_college_chemistry":0.32,
        "MMLU_college_computer_science":0.4,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.4566473988,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.58,
        "MMLU_conceptual_physics":0.4340425532,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.4413793103,
        "MMLU_elementary_mathematics":0.3121693122,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.5,
        "MMLU_high_school_chemistry":0.3596059113,
        "MMLU_high_school_computer_science":0.46,
        "MMLU_high_school_european_history":0.6121212121,
        "MMLU_high_school_geography":0.5353535354,
        "MMLU_high_school_government_and_politics":0.6632124352,
        "MMLU_high_school_macroeconomics":0.4256410256,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.4453781513,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.6165137615,
        "MMLU_high_school_statistics":0.3611111111,
        "MMLU_high_school_us_history":0.6176470588,
        "MMLU_high_school_world_history":0.6455696203,
        "MMLU_human_aging":0.5515695067,
        "MMLU_human_sexuality":0.465648855,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.5277777778,
        "MMLU_logical_fallacies":0.4355828221,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.5631067961,
        "MMLU_marketing":0.6752136752,
        "MMLU_medical_genetics":0.49,
        "MMLU_miscellaneous":0.6526181354,
        "MMLU_moral_disputes":0.5202312139,
        "MMLU_moral_scenarios":0.2715083799,
        "MMLU_nutrition":0.4934640523,
        "MMLU_philosophy":0.5852090032,
        "MMLU_prehistory":0.5061728395,
        "MMLU_professional_accounting":0.3687943262,
        "MMLU_professional_law":0.3428943937,
        "MMLU_professional_medicine":0.5257352941,
        "MMLU_professional_psychology":0.4624183007,
        "MMLU_public_relations":0.4909090909,
        "MMLU_security_studies":0.4897959184,
        "MMLU_sociology":0.6019900498,
        "MMLU_us_foreign_policy":0.58,
        "MMLU_virology":0.3734939759,
        "MMLU_world_religions":0.6900584795
    },
    {
        "Model":"testmodel2",
        "URL":"https:\/\/huggingface.co\/TinyPixel\/testmodel2",
        "full_model_name":"TinyPixel\/testmodel2",
        "Parameters":null,
        "MMLU_average":0.4660876664,
        "arc:challenge|25":0.4914675768,
        "hellaswag|10":0.5907189803,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.3881578947,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.441509434,
        "MMLU_college_biology":0.4583333333,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.35,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.4335260116,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.6,
        "MMLU_conceptual_physics":0.4212765957,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.2777777778,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.5096774194,
        "MMLU_high_school_chemistry":0.3497536946,
        "MMLU_high_school_computer_science":0.4,
        "MMLU_high_school_european_history":0.6303030303,
        "MMLU_high_school_geography":0.4848484848,
        "MMLU_high_school_government_and_politics":0.7046632124,
        "MMLU_high_school_macroeconomics":0.4512820513,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.4369747899,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.6293577982,
        "MMLU_high_school_statistics":0.2777777778,
        "MMLU_high_school_us_history":0.5294117647,
        "MMLU_high_school_world_history":0.6202531646,
        "MMLU_human_aging":0.5695067265,
        "MMLU_human_sexuality":0.5496183206,
        "MMLU_international_law":0.6611570248,
        "MMLU_jurisprudence":0.5185185185,
        "MMLU_logical_fallacies":0.5153374233,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.5631067961,
        "MMLU_marketing":0.7051282051,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.6411238825,
        "MMLU_moral_disputes":0.4826589595,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.4934640523,
        "MMLU_philosophy":0.5948553055,
        "MMLU_prehistory":0.4969135802,
        "MMLU_professional_accounting":0.365248227,
        "MMLU_professional_law":0.3611473272,
        "MMLU_professional_medicine":0.5073529412,
        "MMLU_professional_psychology":0.4460784314,
        "MMLU_public_relations":0.5090909091,
        "MMLU_security_studies":0.4816326531,
        "MMLU_sociology":0.631840796,
        "MMLU_us_foreign_policy":0.63,
        "MMLU_virology":0.4096385542,
        "MMLU_world_religions":0.7076023392
    },
    {
        "Model":"CAMEL-13B-Role-Playing-Data",
        "URL":"https:\/\/huggingface.co\/camel-ai\/CAMEL-13B-Role-Playing-Data",
        "full_model_name":"camel-ai\/CAMEL-13B-Role-Playing-Data",
        "Parameters":13.0,
        "MMLU_average":0.4660841782,
        "arc:challenge|25":0.5034129693,
        "hellaswag|10":0.5967934674,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.437037037,
        "MMLU_astronomy":0.4473684211,
        "MMLU_business_ethics":0.43,
        "MMLU_clinical_knowledge":0.4867924528,
        "MMLU_college_biology":0.4166666667,
        "MMLU_college_chemistry":0.32,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.3757225434,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.58,
        "MMLU_conceptual_physics":0.3787234043,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.4,
        "MMLU_elementary_mathematics":0.2513227513,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.4935483871,
        "MMLU_high_school_chemistry":0.2906403941,
        "MMLU_high_school_computer_science":0.5,
        "MMLU_high_school_european_history":0.6121212121,
        "MMLU_high_school_geography":0.5454545455,
        "MMLU_high_school_government_and_politics":0.6476683938,
        "MMLU_high_school_macroeconomics":0.4102564103,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.4285714286,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.6128440367,
        "MMLU_high_school_statistics":0.2731481481,
        "MMLU_high_school_us_history":0.6519607843,
        "MMLU_high_school_world_history":0.6793248945,
        "MMLU_human_aging":0.5739910314,
        "MMLU_human_sexuality":0.5572519084,
        "MMLU_international_law":0.6611570248,
        "MMLU_jurisprudence":0.4907407407,
        "MMLU_logical_fallacies":0.490797546,
        "MMLU_machine_learning":0.4553571429,
        "MMLU_management":0.6310679612,
        "MMLU_marketing":0.7435897436,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.6743295019,
        "MMLU_moral_disputes":0.4942196532,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.522875817,
        "MMLU_philosophy":0.5562700965,
        "MMLU_prehistory":0.5216049383,
        "MMLU_professional_accounting":0.3226950355,
        "MMLU_professional_law":0.3833116037,
        "MMLU_professional_medicine":0.4632352941,
        "MMLU_professional_psychology":0.4656862745,
        "MMLU_public_relations":0.5454545455,
        "MMLU_security_studies":0.5428571429,
        "MMLU_sociology":0.6218905473,
        "MMLU_us_foreign_policy":0.73,
        "MMLU_virology":0.4096385542,
        "MMLU_world_religions":0.6666666667
    },
    {
        "Model":"Llama-2-7b-ft-instruct-es",
        "URL":"https:\/\/huggingface.co\/clibrain\/Llama-2-7b-ft-instruct-es",
        "full_model_name":"clibrain\/Llama-2-7b-ft-instruct-es",
        "Parameters":7.0,
        "MMLU_average":0.4658080682,
        "arc:challenge|25":0.5076791809,
        "hellaswag|10":0.5840470026,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.4210526316,
        "MMLU_business_ethics":0.47,
        "MMLU_clinical_knowledge":0.5132075472,
        "MMLU_college_biology":0.4861111111,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.47,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.4104046243,
        "MMLU_college_physics":0.1568627451,
        "MMLU_computer_security":0.6,
        "MMLU_conceptual_physics":0.4170212766,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.3931034483,
        "MMLU_elementary_mathematics":0.2989417989,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.5,
        "MMLU_high_school_chemistry":0.3054187192,
        "MMLU_high_school_computer_science":0.41,
        "MMLU_high_school_european_history":0.6,
        "MMLU_high_school_geography":0.5656565657,
        "MMLU_high_school_government_and_politics":0.6787564767,
        "MMLU_high_school_macroeconomics":0.441025641,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.4369747899,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.6275229358,
        "MMLU_high_school_statistics":0.3101851852,
        "MMLU_high_school_us_history":0.5882352941,
        "MMLU_high_school_world_history":0.641350211,
        "MMLU_human_aging":0.5605381166,
        "MMLU_human_sexuality":0.5267175573,
        "MMLU_international_law":0.6446280992,
        "MMLU_jurisprudence":0.5,
        "MMLU_logical_fallacies":0.527607362,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.5922330097,
        "MMLU_marketing":0.6965811966,
        "MMLU_medical_genetics":0.49,
        "MMLU_miscellaneous":0.6270753512,
        "MMLU_moral_disputes":0.4797687861,
        "MMLU_moral_scenarios":0.2715083799,
        "MMLU_nutrition":0.5,
        "MMLU_philosophy":0.5434083601,
        "MMLU_prehistory":0.475308642,
        "MMLU_professional_accounting":0.3687943262,
        "MMLU_professional_law":0.3611473272,
        "MMLU_professional_medicine":0.5183823529,
        "MMLU_professional_psychology":0.4150326797,
        "MMLU_public_relations":0.5545454545,
        "MMLU_security_studies":0.4244897959,
        "MMLU_sociology":0.6467661692,
        "MMLU_us_foreign_policy":0.61,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.6608187135
    },
    {
        "Model":"LLaMa-2-PeanutButter_v19_R8-7B",
        "URL":"https:\/\/huggingface.co\/PeanutJar\/LLaMa-2-PeanutButter_v19_R8-7B",
        "full_model_name":"PeanutJar\/LLaMa-2-PeanutButter_v19_R8-7B",
        "Parameters":7.0,
        "MMLU_average":0.4648011487,
        "arc:challenge|25":0.4948805461,
        "hellaswag|10":0.5902210715,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.4078947368,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.4377358491,
        "MMLU_college_biology":0.4583333333,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.35,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.4335260116,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.59,
        "MMLU_conceptual_physics":0.4255319149,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.4275862069,
        "MMLU_elementary_mathematics":0.2724867725,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.5064516129,
        "MMLU_high_school_chemistry":0.3349753695,
        "MMLU_high_school_computer_science":0.38,
        "MMLU_high_school_european_history":0.5818181818,
        "MMLU_high_school_geography":0.5151515152,
        "MMLU_high_school_government_and_politics":0.6580310881,
        "MMLU_high_school_macroeconomics":0.4435897436,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.4117647059,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.6330275229,
        "MMLU_high_school_statistics":0.2685185185,
        "MMLU_high_school_us_history":0.5735294118,
        "MMLU_high_school_world_history":0.6244725738,
        "MMLU_human_aging":0.5605381166,
        "MMLU_human_sexuality":0.572519084,
        "MMLU_international_law":0.6115702479,
        "MMLU_jurisprudence":0.5185185185,
        "MMLU_logical_fallacies":0.5214723926,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.5631067961,
        "MMLU_marketing":0.6923076923,
        "MMLU_medical_genetics":0.53,
        "MMLU_miscellaneous":0.6462324393,
        "MMLU_moral_disputes":0.5,
        "MMLU_moral_scenarios":0.2402234637,
        "MMLU_nutrition":0.477124183,
        "MMLU_philosophy":0.5884244373,
        "MMLU_prehistory":0.4845679012,
        "MMLU_professional_accounting":0.3829787234,
        "MMLU_professional_law":0.3637548892,
        "MMLU_professional_medicine":0.5330882353,
        "MMLU_professional_psychology":0.4607843137,
        "MMLU_public_relations":0.5454545455,
        "MMLU_security_studies":0.506122449,
        "MMLU_sociology":0.631840796,
        "MMLU_us_foreign_policy":0.64,
        "MMLU_virology":0.4096385542,
        "MMLU_world_religions":0.701754386
    },
    {
        "Model":"cria-llama2-7b-v1.3_peft",
        "URL":"https:\/\/huggingface.co\/davzoku\/cria-llama2-7b-v1.3_peft",
        "full_model_name":"davzoku\/cria-llama2-7b-v1.3_peft",
        "Parameters":7.0,
        "MMLU_average":0.4646977405,
        "arc:challenge|25":0.4889078498,
        "hellaswag|10":0.587134037,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.437037037,
        "MMLU_astronomy":0.4802631579,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.5283018868,
        "MMLU_college_biology":0.5277777778,
        "MMLU_college_chemistry":0.26,
        "MMLU_college_computer_science":0.4,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.3815028902,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.55,
        "MMLU_conceptual_physics":0.3914893617,
        "MMLU_econometrics":0.350877193,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.3227513228,
        "MMLU_formal_logic":0.2301587302,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.5161290323,
        "MMLU_high_school_chemistry":0.3793103448,
        "MMLU_high_school_computer_science":0.43,
        "MMLU_high_school_european_history":0.5696969697,
        "MMLU_high_school_geography":0.5909090909,
        "MMLU_high_school_government_and_politics":0.6787564767,
        "MMLU_high_school_macroeconomics":0.4128205128,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.4285714286,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.6403669725,
        "MMLU_high_school_statistics":0.3009259259,
        "MMLU_high_school_us_history":0.6078431373,
        "MMLU_high_school_world_history":0.5949367089,
        "MMLU_human_aging":0.5515695067,
        "MMLU_human_sexuality":0.5496183206,
        "MMLU_international_law":0.6363636364,
        "MMLU_jurisprudence":0.5648148148,
        "MMLU_logical_fallacies":0.5030674847,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.6213592233,
        "MMLU_marketing":0.688034188,
        "MMLU_medical_genetics":0.5,
        "MMLU_miscellaneous":0.6577266922,
        "MMLU_moral_disputes":0.5346820809,
        "MMLU_moral_scenarios":0.2391061453,
        "MMLU_nutrition":0.4934640523,
        "MMLU_philosophy":0.5562700965,
        "MMLU_prehistory":0.524691358,
        "MMLU_professional_accounting":0.3510638298,
        "MMLU_professional_law":0.3363754889,
        "MMLU_professional_medicine":0.4191176471,
        "MMLU_professional_psychology":0.4460784314,
        "MMLU_public_relations":0.5,
        "MMLU_security_studies":0.4612244898,
        "MMLU_sociology":0.5572139303,
        "MMLU_us_foreign_policy":0.64,
        "MMLU_virology":0.4096385542,
        "MMLU_world_religions":0.6842105263
    },
    {
        "Model":"lima-test",
        "URL":"https:\/\/huggingface.co\/TinyPixel\/lima-test",
        "full_model_name":"TinyPixel\/lima-test",
        "Parameters":null,
        "MMLU_average":0.4641652595,
        "arc:challenge|25":0.4914675768,
        "hellaswag|10":0.590021908,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.3881578947,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.4603773585,
        "MMLU_college_biology":0.4513888889,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.36,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.4219653179,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.4382978723,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.2592592593,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.4967741935,
        "MMLU_high_school_chemistry":0.3399014778,
        "MMLU_high_school_computer_science":0.39,
        "MMLU_high_school_european_history":0.6363636364,
        "MMLU_high_school_geography":0.4747474747,
        "MMLU_high_school_government_and_politics":0.6787564767,
        "MMLU_high_school_macroeconomics":0.4487179487,
        "MMLU_high_school_mathematics":0.2888888889,
        "MMLU_high_school_microeconomics":0.4243697479,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.6275229358,
        "MMLU_high_school_statistics":0.2777777778,
        "MMLU_high_school_us_history":0.5294117647,
        "MMLU_high_school_world_history":0.5949367089,
        "MMLU_human_aging":0.5560538117,
        "MMLU_human_sexuality":0.572519084,
        "MMLU_international_law":0.6280991736,
        "MMLU_jurisprudence":0.537037037,
        "MMLU_logical_fallacies":0.5153374233,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.5631067961,
        "MMLU_marketing":0.6965811966,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.6462324393,
        "MMLU_moral_disputes":0.4942196532,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.4869281046,
        "MMLU_philosophy":0.6012861736,
        "MMLU_prehistory":0.5092592593,
        "MMLU_professional_accounting":0.3581560284,
        "MMLU_professional_law":0.3676662321,
        "MMLU_professional_medicine":0.5220588235,
        "MMLU_professional_psychology":0.454248366,
        "MMLU_public_relations":0.5636363636,
        "MMLU_security_studies":0.4571428571,
        "MMLU_sociology":0.631840796,
        "MMLU_us_foreign_policy":0.64,
        "MMLU_virology":0.4036144578,
        "MMLU_world_religions":0.7134502924
    },
    {
        "Model":"MedLLaMA_13B",
        "URL":"https:\/\/huggingface.co\/chaoyi-wu\/MedLLaMA_13B",
        "full_model_name":"chaoyi-wu\/MedLLaMA_13B",
        "Parameters":13.0,
        "MMLU_average":0.4639960846,
        "arc:challenge|25":0.5102389078,
        "hellaswag|10":0.5862378012,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.5259259259,
        "MMLU_astronomy":0.4802631579,
        "MMLU_business_ethics":0.44,
        "MMLU_clinical_knowledge":0.4905660377,
        "MMLU_college_biology":0.4791666667,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.38,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.4219653179,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.58,
        "MMLU_conceptual_physics":0.4,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.3793103448,
        "MMLU_elementary_mathematics":0.2380952381,
        "MMLU_formal_logic":0.3333333333,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.5129032258,
        "MMLU_high_school_chemistry":0.2807881773,
        "MMLU_high_school_computer_science":0.44,
        "MMLU_high_school_european_history":0.5757575758,
        "MMLU_high_school_geography":0.5151515152,
        "MMLU_high_school_government_and_politics":0.6580310881,
        "MMLU_high_school_macroeconomics":0.4384615385,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.4453781513,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.5871559633,
        "MMLU_high_school_statistics":0.3472222222,
        "MMLU_high_school_us_history":0.5294117647,
        "MMLU_high_school_world_history":0.6244725738,
        "MMLU_human_aging":0.5291479821,
        "MMLU_human_sexuality":0.534351145,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.4907407407,
        "MMLU_logical_fallacies":0.4294478528,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.5922330097,
        "MMLU_marketing":0.6495726496,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.6206896552,
        "MMLU_moral_disputes":0.5028901734,
        "MMLU_moral_scenarios":0.2558659218,
        "MMLU_nutrition":0.545751634,
        "MMLU_philosophy":0.4951768489,
        "MMLU_prehistory":0.5030864198,
        "MMLU_professional_accounting":0.3546099291,
        "MMLU_professional_law":0.332464146,
        "MMLU_professional_medicine":0.5257352941,
        "MMLU_professional_psychology":0.4689542484,
        "MMLU_public_relations":0.5636363636,
        "MMLU_security_studies":0.5387755102,
        "MMLU_sociology":0.631840796,
        "MMLU_us_foreign_policy":0.72,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.6549707602
    },
    {
        "Model":"llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged",
        "URL":"https:\/\/huggingface.co\/dhmeltzer\/llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged",
        "full_model_name":"dhmeltzer\/llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged",
        "Parameters":7.0,
        "MMLU_average":0.4633882749,
        "arc:challenge|25":0.5017064846,
        "hellaswag|10":0.5919139614,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.4276315789,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.4566037736,
        "MMLU_college_biology":0.4513888889,
        "MMLU_college_chemistry":0.33,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.3988439306,
        "MMLU_college_physics":0.1666666667,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.4,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.4068965517,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.380952381,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.5096774194,
        "MMLU_high_school_chemistry":0.3349753695,
        "MMLU_high_school_computer_science":0.46,
        "MMLU_high_school_european_history":0.5939393939,
        "MMLU_high_school_geography":0.4797979798,
        "MMLU_high_school_government_and_politics":0.6839378238,
        "MMLU_high_school_macroeconomics":0.4461538462,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.4201680672,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.6110091743,
        "MMLU_high_school_statistics":0.2731481481,
        "MMLU_high_school_us_history":0.5294117647,
        "MMLU_high_school_world_history":0.5949367089,
        "MMLU_human_aging":0.5515695067,
        "MMLU_human_sexuality":0.5648854962,
        "MMLU_international_law":0.6198347107,
        "MMLU_jurisprudence":0.5092592593,
        "MMLU_logical_fallacies":0.4846625767,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.5825242718,
        "MMLU_marketing":0.688034188,
        "MMLU_medical_genetics":0.53,
        "MMLU_miscellaneous":0.6449553001,
        "MMLU_moral_disputes":0.4971098266,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.4901960784,
        "MMLU_philosophy":0.5916398714,
        "MMLU_prehistory":0.4907407407,
        "MMLU_professional_accounting":0.3581560284,
        "MMLU_professional_law":0.3578878748,
        "MMLU_professional_medicine":0.5183823529,
        "MMLU_professional_psychology":0.4395424837,
        "MMLU_public_relations":0.5909090909,
        "MMLU_security_studies":0.506122449,
        "MMLU_sociology":0.6368159204,
        "MMLU_us_foreign_policy":0.64,
        "MMLU_virology":0.4096385542,
        "MMLU_world_religions":0.7192982456
    },
    {
        "Model":"llama-2-7b-int4-python-code-18k",
        "URL":"https:\/\/huggingface.co\/qualis2006\/llama-2-7b-int4-python-code-18k",
        "full_model_name":"qualis2006\/llama-2-7b-int4-python-code-18k",
        "Parameters":7.0,
        "MMLU_average":0.4625343255,
        "arc:challenge|25":0.4940273038,
        "hellaswag|10":0.5891256722,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.4078947368,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.4679245283,
        "MMLU_college_biology":0.4513888889,
        "MMLU_college_chemistry":0.32,
        "MMLU_college_computer_science":0.38,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.4161849711,
        "MMLU_college_physics":0.1764705882,
        "MMLU_computer_security":0.59,
        "MMLU_conceptual_physics":0.429787234,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.4206896552,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.3015873016,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.4935483871,
        "MMLU_high_school_chemistry":0.3251231527,
        "MMLU_high_school_computer_science":0.4,
        "MMLU_high_school_european_history":0.5575757576,
        "MMLU_high_school_geography":0.5101010101,
        "MMLU_high_school_government_and_politics":0.6839378238,
        "MMLU_high_school_macroeconomics":0.4333333333,
        "MMLU_high_school_mathematics":0.2888888889,
        "MMLU_high_school_microeconomics":0.4369747899,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.6366972477,
        "MMLU_high_school_statistics":0.2685185185,
        "MMLU_high_school_us_history":0.5931372549,
        "MMLU_high_school_world_history":0.6329113924,
        "MMLU_human_aging":0.5560538117,
        "MMLU_human_sexuality":0.5267175573,
        "MMLU_international_law":0.6280991736,
        "MMLU_jurisprudence":0.5092592593,
        "MMLU_logical_fallacies":0.5337423313,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.5922330097,
        "MMLU_marketing":0.6752136752,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.6270753512,
        "MMLU_moral_disputes":0.4971098266,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.4901960784,
        "MMLU_philosophy":0.5980707395,
        "MMLU_prehistory":0.5,
        "MMLU_professional_accounting":0.3581560284,
        "MMLU_professional_law":0.3683181226,
        "MMLU_professional_medicine":0.5147058824,
        "MMLU_professional_psychology":0.4395424837,
        "MMLU_public_relations":0.5181818182,
        "MMLU_security_studies":0.4816326531,
        "MMLU_sociology":0.6417910448,
        "MMLU_us_foreign_policy":0.66,
        "MMLU_virology":0.3855421687,
        "MMLU_world_religions":0.6900584795
    },
    {
        "Model":"llama-2-coder-7b",
        "URL":"https:\/\/huggingface.co\/mrm8488\/llama-2-coder-7b",
        "full_model_name":"mrm8488\/llama-2-coder-7b",
        "Parameters":7.0,
        "MMLU_average":0.4625147761,
        "arc:challenge|25":0.4991467577,
        "hellaswag|10":0.5925114519,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.4342105263,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.4641509434,
        "MMLU_college_biology":0.4444444444,
        "MMLU_college_chemistry":0.33,
        "MMLU_college_computer_science":0.35,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.4046242775,
        "MMLU_college_physics":0.1764705882,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.4255319149,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.4413793103,
        "MMLU_elementary_mathematics":0.2513227513,
        "MMLU_formal_logic":0.3412698413,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.4967741935,
        "MMLU_high_school_chemistry":0.3004926108,
        "MMLU_high_school_computer_science":0.41,
        "MMLU_high_school_european_history":0.5818181818,
        "MMLU_high_school_geography":0.5151515152,
        "MMLU_high_school_government_and_politics":0.6735751295,
        "MMLU_high_school_macroeconomics":0.4384615385,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.4243697479,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.6366972477,
        "MMLU_high_school_statistics":0.2361111111,
        "MMLU_high_school_us_history":0.5539215686,
        "MMLU_high_school_world_history":0.6202531646,
        "MMLU_human_aging":0.5381165919,
        "MMLU_human_sexuality":0.572519084,
        "MMLU_international_law":0.6280991736,
        "MMLU_jurisprudence":0.5185185185,
        "MMLU_logical_fallacies":0.5153374233,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.5631067961,
        "MMLU_marketing":0.7179487179,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.6360153257,
        "MMLU_moral_disputes":0.5028901734,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.4967320261,
        "MMLU_philosophy":0.6077170418,
        "MMLU_prehistory":0.4938271605,
        "MMLU_professional_accounting":0.3546099291,
        "MMLU_professional_law":0.3676662321,
        "MMLU_professional_medicine":0.5220588235,
        "MMLU_professional_psychology":0.4395424837,
        "MMLU_public_relations":0.5454545455,
        "MMLU_security_studies":0.4693877551,
        "MMLU_sociology":0.6019900498,
        "MMLU_us_foreign_policy":0.68,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.6900584795
    },
    {
        "Model":"perry-7b",
        "URL":"https:\/\/huggingface.co\/dotvignesh\/perry-7b",
        "full_model_name":"dotvignesh\/perry-7b",
        "Parameters":7.0,
        "MMLU_average":0.4617906511,
        "arc:challenge|25":0.4692832765,
        "hellaswag|10":0.5706034654,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.4,
        "MMLU_astronomy":0.5,
        "MMLU_business_ethics":0.41,
        "MMLU_clinical_knowledge":0.5018867925,
        "MMLU_college_biology":0.4722222222,
        "MMLU_college_chemistry":0.27,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.387283237,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.54,
        "MMLU_conceptual_physics":0.4042553191,
        "MMLU_econometrics":0.3596491228,
        "MMLU_electrical_engineering":0.5034482759,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.5096774194,
        "MMLU_high_school_chemistry":0.315270936,
        "MMLU_high_school_computer_science":0.45,
        "MMLU_high_school_european_history":0.5939393939,
        "MMLU_high_school_geography":0.5808080808,
        "MMLU_high_school_government_and_politics":0.6839378238,
        "MMLU_high_school_macroeconomics":0.4256410256,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.3865546218,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.6073394495,
        "MMLU_high_school_statistics":0.3009259259,
        "MMLU_high_school_us_history":0.5882352941,
        "MMLU_high_school_world_history":0.611814346,
        "MMLU_human_aging":0.5381165919,
        "MMLU_human_sexuality":0.5419847328,
        "MMLU_international_law":0.6115702479,
        "MMLU_jurisprudence":0.5555555556,
        "MMLU_logical_fallacies":0.4417177914,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.6601941748,
        "MMLU_marketing":0.7136752137,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.650063857,
        "MMLU_moral_disputes":0.4826589595,
        "MMLU_moral_scenarios":0.2402234637,
        "MMLU_nutrition":0.4803921569,
        "MMLU_philosophy":0.5434083601,
        "MMLU_prehistory":0.4907407407,
        "MMLU_professional_accounting":0.3368794326,
        "MMLU_professional_law":0.3226857888,
        "MMLU_professional_medicine":0.4227941176,
        "MMLU_professional_psychology":0.4199346405,
        "MMLU_public_relations":0.5090909091,
        "MMLU_security_studies":0.587755102,
        "MMLU_sociology":0.6119402985,
        "MMLU_us_foreign_policy":0.72,
        "MMLU_virology":0.4337349398,
        "MMLU_world_religions":0.6666666667
    },
    {
        "Model":"llama-shishya-7b-ep3-v1",
        "URL":"https:\/\/huggingface.co\/luffycodes\/llama-shishya-7b-ep3-v1",
        "full_model_name":"luffycodes\/llama-shishya-7b-ep3-v1",
        "Parameters":7.0,
        "MMLU_average":0.4611929735,
        "arc:challenge|25":0.4530716724,
        "hellaswag|10":0.5934076877,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.4605263158,
        "MMLU_business_ethics":0.45,
        "MMLU_clinical_knowledge":0.4943396226,
        "MMLU_college_biology":0.4583333333,
        "MMLU_college_chemistry":0.28,
        "MMLU_college_computer_science":0.4,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.3988439306,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.53,
        "MMLU_conceptual_physics":0.3957446809,
        "MMLU_econometrics":0.3333333333,
        "MMLU_electrical_engineering":0.4206896552,
        "MMLU_elementary_mathematics":0.3015873016,
        "MMLU_formal_logic":0.1984126984,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.535483871,
        "MMLU_high_school_chemistry":0.3596059113,
        "MMLU_high_school_computer_science":0.48,
        "MMLU_high_school_european_history":0.6424242424,
        "MMLU_high_school_geography":0.5454545455,
        "MMLU_high_school_government_and_politics":0.6735751295,
        "MMLU_high_school_macroeconomics":0.3897435897,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.4117647059,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.6440366972,
        "MMLU_high_school_statistics":0.2916666667,
        "MMLU_high_school_us_history":0.6029411765,
        "MMLU_high_school_world_history":0.6455696203,
        "MMLU_human_aging":0.5426008969,
        "MMLU_human_sexuality":0.572519084,
        "MMLU_international_law":0.6363636364,
        "MMLU_jurisprudence":0.5092592593,
        "MMLU_logical_fallacies":0.4969325153,
        "MMLU_machine_learning":0.2321428571,
        "MMLU_management":0.6601941748,
        "MMLU_marketing":0.7307692308,
        "MMLU_medical_genetics":0.49,
        "MMLU_miscellaneous":0.6883780332,
        "MMLU_moral_disputes":0.4942196532,
        "MMLU_moral_scenarios":0.2536312849,
        "MMLU_nutrition":0.5130718954,
        "MMLU_philosophy":0.5594855305,
        "MMLU_prehistory":0.5308641975,
        "MMLU_professional_accounting":0.3439716312,
        "MMLU_professional_law":0.3187744459,
        "MMLU_professional_medicine":0.4264705882,
        "MMLU_professional_psychology":0.4493464052,
        "MMLU_public_relations":0.4727272727,
        "MMLU_security_studies":0.5265306122,
        "MMLU_sociology":0.6119402985,
        "MMLU_us_foreign_policy":0.67,
        "MMLU_virology":0.4036144578,
        "MMLU_world_religions":0.6725146199
    },
    {
        "Model":"llama2-7b-instruct",
        "URL":"https:\/\/huggingface.co\/TinyPixel\/llama2-7b-instruct",
        "full_model_name":"TinyPixel\/llama2-7b-instruct",
        "Parameters":7.0,
        "MMLU_average":0.4611185896,
        "arc:challenge|25":0.4982935154,
        "hellaswag|10":0.5910177256,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.3947368421,
        "MMLU_business_ethics":0.5,
        "MMLU_clinical_knowledge":0.4528301887,
        "MMLU_college_biology":0.4513888889,
        "MMLU_college_chemistry":0.33,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.4335260116,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.4340425532,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.2724867725,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.4935483871,
        "MMLU_high_school_chemistry":0.3201970443,
        "MMLU_high_school_computer_science":0.36,
        "MMLU_high_school_european_history":0.6303030303,
        "MMLU_high_school_geography":0.4848484848,
        "MMLU_high_school_government_and_politics":0.6839378238,
        "MMLU_high_school_macroeconomics":0.4282051282,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.4411764706,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.6220183486,
        "MMLU_high_school_statistics":0.2453703704,
        "MMLU_high_school_us_history":0.5245098039,
        "MMLU_high_school_world_history":0.6033755274,
        "MMLU_human_aging":0.5515695067,
        "MMLU_human_sexuality":0.5267175573,
        "MMLU_international_law":0.6280991736,
        "MMLU_jurisprudence":0.537037037,
        "MMLU_logical_fallacies":0.509202454,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.572815534,
        "MMLU_marketing":0.688034188,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.6411238825,
        "MMLU_moral_disputes":0.4971098266,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.4967320261,
        "MMLU_philosophy":0.6045016077,
        "MMLU_prehistory":0.5092592593,
        "MMLU_professional_accounting":0.365248227,
        "MMLU_professional_law":0.368970013,
        "MMLU_professional_medicine":0.5073529412,
        "MMLU_professional_psychology":0.4428104575,
        "MMLU_public_relations":0.5272727273,
        "MMLU_security_studies":0.4612244898,
        "MMLU_sociology":0.6218905473,
        "MMLU_us_foreign_policy":0.65,
        "MMLU_virology":0.3915662651,
        "MMLU_world_religions":0.7192982456
    },
    {
        "Model":"GPT4-x-Alpasta-13b",
        "URL":"https:\/\/huggingface.co\/Aeala\/GPT4-x-Alpasta-13b",
        "full_model_name":"Aeala\/GPT4-x-Alpasta-13b",
        "Parameters":13.0,
        "MMLU_average":0.4602774699,
        "arc:challenge|25":0.5588737201,
        "hellaswag|10":0.6035650269,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.4013157895,
        "MMLU_business_ethics":0.46,
        "MMLU_clinical_knowledge":0.4754716981,
        "MMLU_college_biology":0.4375,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.46,
        "MMLU_college_mathematics":0.39,
        "MMLU_college_medicine":0.3815028902,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.6,
        "MMLU_conceptual_physics":0.3914893617,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.3793103448,
        "MMLU_elementary_mathematics":0.3042328042,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.27,
        "MMLU_high_school_biology":0.5129032258,
        "MMLU_high_school_chemistry":0.275862069,
        "MMLU_high_school_computer_science":0.5,
        "MMLU_high_school_european_history":0.6121212121,
        "MMLU_high_school_geography":0.601010101,
        "MMLU_high_school_government_and_politics":0.6476683938,
        "MMLU_high_school_macroeconomics":0.4256410256,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.4663865546,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.6073394495,
        "MMLU_high_school_statistics":0.3564814815,
        "MMLU_high_school_us_history":0.6274509804,
        "MMLU_high_school_world_history":0.6497890295,
        "MMLU_human_aging":0.5112107623,
        "MMLU_human_sexuality":0.4809160305,
        "MMLU_international_law":0.5950413223,
        "MMLU_jurisprudence":0.5185185185,
        "MMLU_logical_fallacies":0.4478527607,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.6796116505,
        "MMLU_marketing":0.7435897436,
        "MMLU_medical_genetics":0.47,
        "MMLU_miscellaneous":0.6398467433,
        "MMLU_moral_disputes":0.5057803468,
        "MMLU_moral_scenarios":0.3016759777,
        "MMLU_nutrition":0.4901960784,
        "MMLU_philosophy":0.5080385852,
        "MMLU_prehistory":0.4938271605,
        "MMLU_professional_accounting":0.3085106383,
        "MMLU_professional_law":0.3780964798,
        "MMLU_professional_medicine":0.4558823529,
        "MMLU_professional_psychology":0.4362745098,
        "MMLU_public_relations":0.5454545455,
        "MMLU_security_studies":0.4408163265,
        "MMLU_sociology":0.5721393035,
        "MMLU_us_foreign_policy":0.68,
        "MMLU_virology":0.3975903614,
        "MMLU_world_religions":0.6257309942
    },
    {
        "Model":"lacda-2-7B-chat-v0.1",
        "URL":"https:\/\/huggingface.co\/willnguyen\/lacda-2-7B-chat-v0.1",
        "full_model_name":"willnguyen\/lacda-2-7B-chat-v0.1",
        "Parameters":7.0,
        "MMLU_average":0.4602669374,
        "arc:challenge|25":0.4803754266,
        "hellaswag|10":0.5796654053,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.437037037,
        "MMLU_astronomy":0.4802631579,
        "MMLU_business_ethics":0.46,
        "MMLU_clinical_knowledge":0.4566037736,
        "MMLU_college_biology":0.4375,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.35,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.4277456647,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.63,
        "MMLU_conceptual_physics":0.3957446809,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.524137931,
        "MMLU_elementary_mathematics":0.2619047619,
        "MMLU_formal_logic":0.3015873016,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.464516129,
        "MMLU_high_school_chemistry":0.354679803,
        "MMLU_high_school_computer_science":0.43,
        "MMLU_high_school_european_history":0.6303030303,
        "MMLU_high_school_geography":0.5,
        "MMLU_high_school_government_and_politics":0.6062176166,
        "MMLU_high_school_macroeconomics":0.4461538462,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.4621848739,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.6165137615,
        "MMLU_high_school_statistics":0.2314814815,
        "MMLU_high_school_us_history":0.5,
        "MMLU_high_school_world_history":0.5780590717,
        "MMLU_human_aging":0.5560538117,
        "MMLU_human_sexuality":0.572519084,
        "MMLU_international_law":0.6198347107,
        "MMLU_jurisprudence":0.537037037,
        "MMLU_logical_fallacies":0.490797546,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.6019417476,
        "MMLU_marketing":0.6709401709,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.6143039591,
        "MMLU_moral_disputes":0.4884393064,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.4673202614,
        "MMLU_philosophy":0.5884244373,
        "MMLU_prehistory":0.4783950617,
        "MMLU_professional_accounting":0.3475177305,
        "MMLU_professional_law":0.3572359844,
        "MMLU_professional_medicine":0.4926470588,
        "MMLU_professional_psychology":0.431372549,
        "MMLU_public_relations":0.5818181818,
        "MMLU_security_studies":0.4612244898,
        "MMLU_sociology":0.6069651741,
        "MMLU_us_foreign_policy":0.64,
        "MMLU_virology":0.3975903614,
        "MMLU_world_religions":0.6725146199
    },
    {
        "Model":"LLaMa-2-PeanutButter_v14-7B",
        "URL":"https:\/\/huggingface.co\/PeanutJar\/LLaMa-2-PeanutButter_v14-7B",
        "full_model_name":"PeanutJar\/LLaMa-2-PeanutButter_v14-7B",
        "Parameters":7.0,
        "MMLU_average":0.4597492388,
        "arc:challenge|25":0.5051194539,
        "hellaswag|10":0.6148177654,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.4407894737,
        "MMLU_business_ethics":0.48,
        "MMLU_clinical_knowledge":0.4867924528,
        "MMLU_college_biology":0.4861111111,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.4104046243,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.51,
        "MMLU_conceptual_physics":0.4255319149,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.4413793103,
        "MMLU_elementary_mathematics":0.3148148148,
        "MMLU_formal_logic":0.2380952381,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.4935483871,
        "MMLU_high_school_chemistry":0.3891625616,
        "MMLU_high_school_computer_science":0.45,
        "MMLU_high_school_european_history":0.5393939394,
        "MMLU_high_school_geography":0.5303030303,
        "MMLU_high_school_government_and_politics":0.6528497409,
        "MMLU_high_school_macroeconomics":0.4333333333,
        "MMLU_high_school_mathematics":0.2925925926,
        "MMLU_high_school_microeconomics":0.4327731092,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.623853211,
        "MMLU_high_school_statistics":0.2731481481,
        "MMLU_high_school_us_history":0.568627451,
        "MMLU_high_school_world_history":0.5907172996,
        "MMLU_human_aging":0.4887892377,
        "MMLU_human_sexuality":0.5114503817,
        "MMLU_international_law":0.5950413223,
        "MMLU_jurisprudence":0.4722222222,
        "MMLU_logical_fallacies":0.5030674847,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.5145631068,
        "MMLU_marketing":0.6752136752,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.6372924649,
        "MMLU_moral_disputes":0.4797687861,
        "MMLU_moral_scenarios":0.3083798883,
        "MMLU_nutrition":0.5130718954,
        "MMLU_philosophy":0.5305466238,
        "MMLU_prehistory":0.5061728395,
        "MMLU_professional_accounting":0.3687943262,
        "MMLU_professional_law":0.3507170795,
        "MMLU_professional_medicine":0.5036764706,
        "MMLU_professional_psychology":0.4330065359,
        "MMLU_public_relations":0.5363636364,
        "MMLU_security_studies":0.4979591837,
        "MMLU_sociology":0.5970149254,
        "MMLU_us_foreign_policy":0.72,
        "MMLU_virology":0.3975903614,
        "MMLU_world_religions":0.6842105263
    },
    {
        "Model":"WizardMath-7B-V1.0",
        "URL":"https:\/\/huggingface.co\/WizardLM\/WizardMath-7B-V1.0",
        "full_model_name":"WizardLM\/WizardMath-7B-V1.0",
        "Parameters":7.0,
        "MMLU_average":0.4597027436,
        "arc:challenge|25":0.5076791809,
        "hellaswag|10":0.6099382593,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4296296296,
        "MMLU_astronomy":0.4473684211,
        "MMLU_business_ethics":0.47,
        "MMLU_clinical_knowledge":0.4641509434,
        "MMLU_college_biology":0.4166666667,
        "MMLU_college_chemistry":0.33,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.3815028902,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.58,
        "MMLU_conceptual_physics":0.3574468085,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.2777777778,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.4774193548,
        "MMLU_high_school_chemistry":0.3300492611,
        "MMLU_high_school_computer_science":0.39,
        "MMLU_high_school_european_history":0.5939393939,
        "MMLU_high_school_geography":0.5404040404,
        "MMLU_high_school_government_and_politics":0.6787564767,
        "MMLU_high_school_macroeconomics":0.4256410256,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.4243697479,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.6183486239,
        "MMLU_high_school_statistics":0.2592592593,
        "MMLU_high_school_us_history":0.5441176471,
        "MMLU_high_school_world_history":0.6329113924,
        "MMLU_human_aging":0.5470852018,
        "MMLU_human_sexuality":0.5648854962,
        "MMLU_international_law":0.6198347107,
        "MMLU_jurisprudence":0.5555555556,
        "MMLU_logical_fallacies":0.4662576687,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.6213592233,
        "MMLU_marketing":0.6923076923,
        "MMLU_medical_genetics":0.43,
        "MMLU_miscellaneous":0.6334610473,
        "MMLU_moral_disputes":0.4739884393,
        "MMLU_moral_scenarios":0.2547486034,
        "MMLU_nutrition":0.5196078431,
        "MMLU_philosophy":0.5691318328,
        "MMLU_prehistory":0.5030864198,
        "MMLU_professional_accounting":0.3581560284,
        "MMLU_professional_law":0.3376792699,
        "MMLU_professional_medicine":0.5367647059,
        "MMLU_professional_psychology":0.4428104575,
        "MMLU_public_relations":0.5363636364,
        "MMLU_security_studies":0.4244897959,
        "MMLU_sociology":0.6766169154,
        "MMLU_us_foreign_policy":0.68,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.6432748538
    },
    {
        "Model":"llama2-7b-chat-hf-v3",
        "URL":"https:\/\/huggingface.co\/TheTravellingEngineer\/llama2-7b-chat-hf-v3",
        "full_model_name":"TheTravellingEngineer\/llama2-7b-chat-hf-v3",
        "Parameters":7.0,
        "MMLU_average":0.4589029146,
        "arc:challenge|25":0.4752559727,
        "hellaswag|10":0.5782712607,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4444444444,
        "MMLU_astronomy":0.4078947368,
        "MMLU_business_ethics":0.5,
        "MMLU_clinical_knowledge":0.4490566038,
        "MMLU_college_biology":0.4583333333,
        "MMLU_college_chemistry":0.36,
        "MMLU_college_computer_science":0.36,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.4393063584,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.62,
        "MMLU_conceptual_physics":0.4425531915,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.4413793103,
        "MMLU_elementary_mathematics":0.2698412698,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.464516129,
        "MMLU_high_school_chemistry":0.3251231527,
        "MMLU_high_school_computer_science":0.39,
        "MMLU_high_school_european_history":0.5878787879,
        "MMLU_high_school_geography":0.4949494949,
        "MMLU_high_school_government_and_politics":0.6683937824,
        "MMLU_high_school_macroeconomics":0.4333333333,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.4159663866,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.6110091743,
        "MMLU_high_school_statistics":0.25,
        "MMLU_high_school_us_history":0.5539215686,
        "MMLU_high_school_world_history":0.6286919831,
        "MMLU_human_aging":0.5470852018,
        "MMLU_human_sexuality":0.5419847328,
        "MMLU_international_law":0.6198347107,
        "MMLU_jurisprudence":0.5277777778,
        "MMLU_logical_fallacies":0.509202454,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.572815534,
        "MMLU_marketing":0.6666666667,
        "MMLU_medical_genetics":0.53,
        "MMLU_miscellaneous":0.619412516,
        "MMLU_moral_disputes":0.5115606936,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.4738562092,
        "MMLU_philosophy":0.5723472669,
        "MMLU_prehistory":0.475308642,
        "MMLU_professional_accounting":0.3333333333,
        "MMLU_professional_law":0.3578878748,
        "MMLU_professional_medicine":0.5257352941,
        "MMLU_professional_psychology":0.431372549,
        "MMLU_public_relations":0.5545454545,
        "MMLU_security_studies":0.4857142857,
        "MMLU_sociology":0.6169154229,
        "MMLU_us_foreign_policy":0.63,
        "MMLU_virology":0.4036144578,
        "MMLU_world_religions":0.6666666667
    },
    {
        "Model":"Uncensored-Jordan-7B",
        "URL":"https:\/\/huggingface.co\/ajibawa-2023\/Uncensored-Jordan-7B",
        "full_model_name":"ajibawa-2023\/Uncensored-Jordan-7B",
        "Parameters":7.0,
        "MMLU_average":0.4569452676,
        "arc:challenge|25":0.4957337884,
        "hellaswag|10":0.58673571,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.4518518519,
        "MMLU_astronomy":0.4210526316,
        "MMLU_business_ethics":0.49,
        "MMLU_clinical_knowledge":0.5056603774,
        "MMLU_college_biology":0.4791666667,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.3988439306,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.6,
        "MMLU_conceptual_physics":0.3829787234,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.4413793103,
        "MMLU_elementary_mathematics":0.2830687831,
        "MMLU_formal_logic":0.2698412698,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.5032258065,
        "MMLU_high_school_chemistry":0.3201970443,
        "MMLU_high_school_computer_science":0.41,
        "MMLU_high_school_european_history":0.5757575758,
        "MMLU_high_school_geography":0.5656565657,
        "MMLU_high_school_government_and_politics":0.6528497409,
        "MMLU_high_school_macroeconomics":0.4358974359,
        "MMLU_high_school_mathematics":0.237037037,
        "MMLU_high_school_microeconomics":0.3991596639,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.5486238532,
        "MMLU_high_school_statistics":0.4444444444,
        "MMLU_high_school_us_history":0.6029411765,
        "MMLU_high_school_world_history":0.582278481,
        "MMLU_human_aging":0.5381165919,
        "MMLU_human_sexuality":0.4961832061,
        "MMLU_international_law":0.5950413223,
        "MMLU_jurisprudence":0.5648148148,
        "MMLU_logical_fallacies":0.5153374233,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.5631067961,
        "MMLU_marketing":0.688034188,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.6462324393,
        "MMLU_moral_disputes":0.5173410405,
        "MMLU_moral_scenarios":0.243575419,
        "MMLU_nutrition":0.4673202614,
        "MMLU_philosophy":0.4951768489,
        "MMLU_prehistory":0.512345679,
        "MMLU_professional_accounting":0.3439716312,
        "MMLU_professional_law":0.3461538462,
        "MMLU_professional_medicine":0.4632352941,
        "MMLU_professional_psychology":0.4183006536,
        "MMLU_public_relations":0.4909090909,
        "MMLU_security_studies":0.4857142857,
        "MMLU_sociology":0.6218905473,
        "MMLU_us_foreign_policy":0.68,
        "MMLU_virology":0.3554216867,
        "MMLU_world_religions":0.6374269006
    },
    {
        "Model":"vicuna-7b-1.1",
        "URL":"https:\/\/huggingface.co\/eachadea\/vicuna-7b-1.1",
        "full_model_name":"eachadea\/vicuna-7b-1.1",
        "Parameters":7.0,
        "MMLU_average":0.4562882055,
        "arc:challenge|25":0.4991467577,
        "hellaswag|10":0.5844453296,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.4,
        "MMLU_astronomy":0.4934210526,
        "MMLU_business_ethics":0.46,
        "MMLU_clinical_knowledge":0.5132075472,
        "MMLU_college_biology":0.4444444444,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.3526011561,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.56,
        "MMLU_conceptual_physics":0.3829787234,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.2883597884,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.27,
        "MMLU_high_school_biology":0.4612903226,
        "MMLU_high_school_chemistry":0.3251231527,
        "MMLU_high_school_computer_science":0.39,
        "MMLU_high_school_european_history":0.5878787879,
        "MMLU_high_school_geography":0.595959596,
        "MMLU_high_school_government_and_politics":0.6683937824,
        "MMLU_high_school_macroeconomics":0.4,
        "MMLU_high_school_mathematics":0.2296296296,
        "MMLU_high_school_microeconomics":0.3865546218,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.6,
        "MMLU_high_school_statistics":0.3888888889,
        "MMLU_high_school_us_history":0.6029411765,
        "MMLU_high_school_world_history":0.5780590717,
        "MMLU_human_aging":0.5381165919,
        "MMLU_human_sexuality":0.5648854962,
        "MMLU_international_law":0.5702479339,
        "MMLU_jurisprudence":0.5555555556,
        "MMLU_logical_fallacies":0.509202454,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.572815534,
        "MMLU_marketing":0.6837606838,
        "MMLU_medical_genetics":0.48,
        "MMLU_miscellaneous":0.6296296296,
        "MMLU_moral_disputes":0.5057803468,
        "MMLU_moral_scenarios":0.2446927374,
        "MMLU_nutrition":0.4869281046,
        "MMLU_philosophy":0.4823151125,
        "MMLU_prehistory":0.5185185185,
        "MMLU_professional_accounting":0.3404255319,
        "MMLU_professional_law":0.3487614081,
        "MMLU_professional_medicine":0.5036764706,
        "MMLU_professional_psychology":0.4395424837,
        "MMLU_public_relations":0.5181818182,
        "MMLU_security_studies":0.5387755102,
        "MMLU_sociology":0.631840796,
        "MMLU_us_foreign_policy":0.63,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.6666666667
    },
    {
        "Model":"vicuna_7B_vanilla_1.1",
        "URL":"https:\/\/huggingface.co\/Ejafa\/vicuna_7B_vanilla_1.1",
        "full_model_name":"Ejafa\/vicuna_7B_vanilla_1.1",
        "Parameters":7.0,
        "MMLU_average":0.4562882055,
        "arc:challenge|25":0.4991467577,
        "hellaswag|10":0.5844453296,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.4,
        "MMLU_astronomy":0.4934210526,
        "MMLU_business_ethics":0.46,
        "MMLU_clinical_knowledge":0.5132075472,
        "MMLU_college_biology":0.4444444444,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.3526011561,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.56,
        "MMLU_conceptual_physics":0.3829787234,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.2883597884,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.27,
        "MMLU_high_school_biology":0.4612903226,
        "MMLU_high_school_chemistry":0.3251231527,
        "MMLU_high_school_computer_science":0.39,
        "MMLU_high_school_european_history":0.5878787879,
        "MMLU_high_school_geography":0.595959596,
        "MMLU_high_school_government_and_politics":0.6683937824,
        "MMLU_high_school_macroeconomics":0.4,
        "MMLU_high_school_mathematics":0.2296296296,
        "MMLU_high_school_microeconomics":0.3865546218,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.6,
        "MMLU_high_school_statistics":0.3888888889,
        "MMLU_high_school_us_history":0.6029411765,
        "MMLU_high_school_world_history":0.5780590717,
        "MMLU_human_aging":0.5381165919,
        "MMLU_human_sexuality":0.5648854962,
        "MMLU_international_law":0.5702479339,
        "MMLU_jurisprudence":0.5555555556,
        "MMLU_logical_fallacies":0.509202454,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.572815534,
        "MMLU_marketing":0.6837606838,
        "MMLU_medical_genetics":0.48,
        "MMLU_miscellaneous":0.6296296296,
        "MMLU_moral_disputes":0.5057803468,
        "MMLU_moral_scenarios":0.2446927374,
        "MMLU_nutrition":0.4869281046,
        "MMLU_philosophy":0.4823151125,
        "MMLU_prehistory":0.5185185185,
        "MMLU_professional_accounting":0.3404255319,
        "MMLU_professional_law":0.3487614081,
        "MMLU_professional_medicine":0.5036764706,
        "MMLU_professional_psychology":0.4395424837,
        "MMLU_public_relations":0.5181818182,
        "MMLU_security_studies":0.5387755102,
        "MMLU_sociology":0.631840796,
        "MMLU_us_foreign_policy":0.63,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.6666666667
    },
    {
        "Model":"leo-hessianai-7b-chat",
        "URL":"https:\/\/huggingface.co\/LeoLM\/leo-hessianai-7b-chat",
        "full_model_name":"LeoLM\/leo-hessianai-7b-chat",
        "Parameters":7.0,
        "MMLU_average":0.455772694,
        "arc:challenge|25":0.4752559727,
        "hellaswag|10":0.5787691695,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4518518519,
        "MMLU_astronomy":0.4539473684,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.4943396226,
        "MMLU_college_biology":0.4444444444,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.4277456647,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.55,
        "MMLU_conceptual_physics":0.3659574468,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.4482758621,
        "MMLU_elementary_mathematics":0.3015873016,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.5451612903,
        "MMLU_high_school_chemistry":0.3399014778,
        "MMLU_high_school_computer_science":0.46,
        "MMLU_high_school_european_history":0.6060606061,
        "MMLU_high_school_geography":0.5101010101,
        "MMLU_high_school_government_and_politics":0.621761658,
        "MMLU_high_school_macroeconomics":0.3794871795,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.3739495798,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.6,
        "MMLU_high_school_statistics":0.3564814815,
        "MMLU_high_school_us_history":0.5931372549,
        "MMLU_high_school_world_history":0.6329113924,
        "MMLU_human_aging":0.5201793722,
        "MMLU_human_sexuality":0.4503816794,
        "MMLU_international_law":0.5867768595,
        "MMLU_jurisprudence":0.5092592593,
        "MMLU_logical_fallacies":0.4969325153,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.6019417476,
        "MMLU_marketing":0.6495726496,
        "MMLU_medical_genetics":0.5,
        "MMLU_miscellaneous":0.6028097063,
        "MMLU_moral_disputes":0.4450867052,
        "MMLU_moral_scenarios":0.2402234637,
        "MMLU_nutrition":0.5065359477,
        "MMLU_philosophy":0.5466237942,
        "MMLU_prehistory":0.5154320988,
        "MMLU_professional_accounting":0.3687943262,
        "MMLU_professional_law":0.3650586701,
        "MMLU_professional_medicine":0.5073529412,
        "MMLU_professional_psychology":0.4379084967,
        "MMLU_public_relations":0.4727272727,
        "MMLU_security_studies":0.4448979592,
        "MMLU_sociology":0.5870646766,
        "MMLU_us_foreign_policy":0.7,
        "MMLU_virology":0.3674698795,
        "MMLU_world_religions":0.5906432749
    },
    {
        "Model":"LDCC-Instruct-Llama-2-ko-13B-v2",
        "URL":"https:\/\/huggingface.co\/krevas\/LDCC-Instruct-Llama-2-ko-13B-v2",
        "full_model_name":"krevas\/LDCC-Instruct-Llama-2-ko-13B-v2",
        "Parameters":13.0,
        "MMLU_average":0.4557077549,
        "arc:challenge|25":0.5298634812,
        "hellaswag|10":0.6105357499,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.437037037,
        "MMLU_astronomy":0.4342105263,
        "MMLU_business_ethics":0.53,
        "MMLU_clinical_knowledge":0.4603773585,
        "MMLU_college_biology":0.5486111111,
        "MMLU_college_chemistry":0.25,
        "MMLU_college_computer_science":0.27,
        "MMLU_college_mathematics":0.22,
        "MMLU_college_medicine":0.3352601156,
        "MMLU_college_physics":0.1764705882,
        "MMLU_computer_security":0.62,
        "MMLU_conceptual_physics":0.3617021277,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.3103448276,
        "MMLU_elementary_mathematics":0.2407407407,
        "MMLU_formal_logic":0.2301587302,
        "MMLU_global_facts":0.27,
        "MMLU_high_school_biology":0.5096774194,
        "MMLU_high_school_chemistry":0.3399014778,
        "MMLU_high_school_computer_science":0.43,
        "MMLU_high_school_european_history":0.5818181818,
        "MMLU_high_school_geography":0.5404040404,
        "MMLU_high_school_government_and_politics":0.689119171,
        "MMLU_high_school_macroeconomics":0.4051282051,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.4075630252,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.6183486239,
        "MMLU_high_school_statistics":0.2175925926,
        "MMLU_high_school_us_history":0.6078431373,
        "MMLU_high_school_world_history":0.6244725738,
        "MMLU_human_aging":0.5470852018,
        "MMLU_human_sexuality":0.5419847328,
        "MMLU_international_law":0.694214876,
        "MMLU_jurisprudence":0.5833333333,
        "MMLU_logical_fallacies":0.6134969325,
        "MMLU_machine_learning":0.4017857143,
        "MMLU_management":0.6116504854,
        "MMLU_marketing":0.7564102564,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.6398467433,
        "MMLU_moral_disputes":0.5115606936,
        "MMLU_moral_scenarios":0.2569832402,
        "MMLU_nutrition":0.4803921569,
        "MMLU_philosophy":0.5691318328,
        "MMLU_prehistory":0.5586419753,
        "MMLU_professional_accounting":0.3510638298,
        "MMLU_professional_law":0.371577575,
        "MMLU_professional_medicine":0.3713235294,
        "MMLU_professional_psychology":0.4836601307,
        "MMLU_public_relations":0.5727272727,
        "MMLU_security_studies":0.3428571429,
        "MMLU_sociology":0.6368159204,
        "MMLU_us_foreign_policy":0.63,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.7251461988
    },
    {
        "Model":"LLaMa-2-PeanutButter_v18_A-7B",
        "URL":"https:\/\/huggingface.co\/PeanutJar\/LLaMa-2-PeanutButter_v18_A-7B",
        "full_model_name":"PeanutJar\/LLaMa-2-PeanutButter_v18_A-7B",
        "Parameters":7.0,
        "MMLU_average":0.4554146384,
        "arc:challenge|25":0.4846416382,
        "hellaswag|10":0.5869348735,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4222222222,
        "MMLU_astronomy":0.3881578947,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.4528301887,
        "MMLU_college_biology":0.4444444444,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.36,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.3930635838,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.59,
        "MMLU_conceptual_physics":0.4382978723,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.4068965517,
        "MMLU_elementary_mathematics":0.2513227513,
        "MMLU_formal_logic":0.380952381,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.4838709677,
        "MMLU_high_school_chemistry":0.3054187192,
        "MMLU_high_school_computer_science":0.37,
        "MMLU_high_school_european_history":0.5878787879,
        "MMLU_high_school_geography":0.4848484848,
        "MMLU_high_school_government_and_politics":0.6632124352,
        "MMLU_high_school_macroeconomics":0.441025641,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.3907563025,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.6018348624,
        "MMLU_high_school_statistics":0.2222222222,
        "MMLU_high_school_us_history":0.5392156863,
        "MMLU_high_school_world_history":0.582278481,
        "MMLU_human_aging":0.5246636771,
        "MMLU_human_sexuality":0.572519084,
        "MMLU_international_law":0.5950413223,
        "MMLU_jurisprudence":0.5,
        "MMLU_logical_fallacies":0.5030674847,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.5436893204,
        "MMLU_marketing":0.6752136752,
        "MMLU_medical_genetics":0.51,
        "MMLU_miscellaneous":0.6385696041,
        "MMLU_moral_disputes":0.4855491329,
        "MMLU_moral_scenarios":0.2402234637,
        "MMLU_nutrition":0.4836601307,
        "MMLU_philosophy":0.5723472669,
        "MMLU_prehistory":0.4814814815,
        "MMLU_professional_accounting":0.3404255319,
        "MMLU_professional_law":0.3604954368,
        "MMLU_professional_medicine":0.5110294118,
        "MMLU_professional_psychology":0.4477124183,
        "MMLU_public_relations":0.5454545455,
        "MMLU_security_studies":0.5102040816,
        "MMLU_sociology":0.6467661692,
        "MMLU_us_foreign_policy":0.66,
        "MMLU_virology":0.4036144578,
        "MMLU_world_religions":0.7076023392
    },
    {
        "Model":"denas-llama2",
        "URL":"https:\/\/huggingface.co\/tianyil1\/denas-llama2",
        "full_model_name":"tianyil1\/denas-llama2",
        "Parameters":null,
        "MMLU_average":0.4549506065,
        "arc:challenge|25":0.4991467577,
        "hellaswag|10":0.5880302729,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.437037037,
        "MMLU_astronomy":0.4144736842,
        "MMLU_business_ethics":0.46,
        "MMLU_clinical_knowledge":0.4603773585,
        "MMLU_college_biology":0.4236111111,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.4104046243,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.58,
        "MMLU_conceptual_physics":0.4340425532,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.4275862069,
        "MMLU_elementary_mathematics":0.291005291,
        "MMLU_formal_logic":0.2698412698,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.4612903226,
        "MMLU_high_school_chemistry":0.3399014778,
        "MMLU_high_school_computer_science":0.41,
        "MMLU_high_school_european_history":0.6,
        "MMLU_high_school_geography":0.5505050505,
        "MMLU_high_school_government_and_politics":0.6787564767,
        "MMLU_high_school_macroeconomics":0.4230769231,
        "MMLU_high_school_mathematics":0.2888888889,
        "MMLU_high_school_microeconomics":0.3781512605,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.6220183486,
        "MMLU_high_school_statistics":0.2777777778,
        "MMLU_high_school_us_history":0.5392156863,
        "MMLU_high_school_world_history":0.6033755274,
        "MMLU_human_aging":0.5426008969,
        "MMLU_human_sexuality":0.5190839695,
        "MMLU_international_law":0.6363636364,
        "MMLU_jurisprudence":0.4814814815,
        "MMLU_logical_fallacies":0.4662576687,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.5242718447,
        "MMLU_marketing":0.7094017094,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.6168582375,
        "MMLU_moral_disputes":0.4855491329,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.454248366,
        "MMLU_philosophy":0.6045016077,
        "MMLU_prehistory":0.5154320988,
        "MMLU_professional_accounting":0.3439716312,
        "MMLU_professional_law":0.3741851369,
        "MMLU_professional_medicine":0.5183823529,
        "MMLU_professional_psychology":0.4133986928,
        "MMLU_public_relations":0.5454545455,
        "MMLU_security_studies":0.4,
        "MMLU_sociology":0.5572139303,
        "MMLU_us_foreign_policy":0.7,
        "MMLU_virology":0.3915662651,
        "MMLU_world_religions":0.6432748538
    },
    {
        "Model":"firefly-llama2-7b-chat-temp",
        "URL":"https:\/\/huggingface.co\/YeungNLP\/firefly-llama2-7b-chat-temp",
        "full_model_name":"YeungNLP\/firefly-llama2-7b-chat-temp",
        "Parameters":7.0,
        "MMLU_average":0.4547045009,
        "arc:challenge|25":0.4829351536,
        "hellaswag|10":0.5476000797,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4518518519,
        "MMLU_astronomy":0.4144736842,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.5056603774,
        "MMLU_college_biology":0.4305555556,
        "MMLU_college_chemistry":0.32,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.39,
        "MMLU_college_medicine":0.3988439306,
        "MMLU_college_physics":0.1568627451,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.3829787234,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.4206896552,
        "MMLU_elementary_mathematics":0.2592592593,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.29,
        "MMLU_high_school_biology":0.4774193548,
        "MMLU_high_school_chemistry":0.3004926108,
        "MMLU_high_school_computer_science":0.4,
        "MMLU_high_school_european_history":0.5515151515,
        "MMLU_high_school_geography":0.5656565657,
        "MMLU_high_school_government_and_politics":0.5751295337,
        "MMLU_high_school_macroeconomics":0.4128205128,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.3991596639,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.5834862385,
        "MMLU_high_school_statistics":0.3101851852,
        "MMLU_high_school_us_history":0.5637254902,
        "MMLU_high_school_world_history":0.5991561181,
        "MMLU_human_aging":0.5112107623,
        "MMLU_human_sexuality":0.5267175573,
        "MMLU_international_law":0.6280991736,
        "MMLU_jurisprudence":0.4907407407,
        "MMLU_logical_fallacies":0.4969325153,
        "MMLU_machine_learning":0.4196428571,
        "MMLU_management":0.5825242718,
        "MMLU_marketing":0.6666666667,
        "MMLU_medical_genetics":0.51,
        "MMLU_miscellaneous":0.5862068966,
        "MMLU_moral_disputes":0.5057803468,
        "MMLU_moral_scenarios":0.2413407821,
        "MMLU_nutrition":0.4901960784,
        "MMLU_philosophy":0.5144694534,
        "MMLU_prehistory":0.5092592593,
        "MMLU_professional_accounting":0.3510638298,
        "MMLU_professional_law":0.3376792699,
        "MMLU_professional_medicine":0.4264705882,
        "MMLU_professional_psychology":0.4428104575,
        "MMLU_public_relations":0.6,
        "MMLU_security_studies":0.4040816327,
        "MMLU_sociology":0.6467661692,
        "MMLU_us_foreign_policy":0.7,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.6725146199
    },
    {
        "Model":"llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged",
        "URL":"https:\/\/huggingface.co\/dhmeltzer\/llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged",
        "full_model_name":"dhmeltzer\/llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged",
        "Parameters":7.0,
        "MMLU_average":0.453454313,
        "arc:challenge|25":0.5,
        "hellaswag|10":0.5782712607,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4296296296,
        "MMLU_astronomy":0.4342105263,
        "MMLU_business_ethics":0.44,
        "MMLU_clinical_knowledge":0.4603773585,
        "MMLU_college_biology":0.4513888889,
        "MMLU_college_chemistry":0.27,
        "MMLU_college_computer_science":0.35,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.4161849711,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.62,
        "MMLU_conceptual_physics":0.4085106383,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.4551724138,
        "MMLU_elementary_mathematics":0.2777777778,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.5064516129,
        "MMLU_high_school_chemistry":0.3054187192,
        "MMLU_high_school_computer_science":0.4,
        "MMLU_high_school_european_history":0.6,
        "MMLU_high_school_geography":0.4797979798,
        "MMLU_high_school_government_and_politics":0.6580310881,
        "MMLU_high_school_macroeconomics":0.4282051282,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.4453781513,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.6128440367,
        "MMLU_high_school_statistics":0.2407407407,
        "MMLU_high_school_us_history":0.5,
        "MMLU_high_school_world_history":0.5907172996,
        "MMLU_human_aging":0.5739910314,
        "MMLU_human_sexuality":0.5572519084,
        "MMLU_international_law":0.6198347107,
        "MMLU_jurisprudence":0.5,
        "MMLU_logical_fallacies":0.4846625767,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.5922330097,
        "MMLU_marketing":0.6709401709,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.6181353768,
        "MMLU_moral_disputes":0.5,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.454248366,
        "MMLU_philosophy":0.5884244373,
        "MMLU_prehistory":0.462962963,
        "MMLU_professional_accounting":0.3510638298,
        "MMLU_professional_law":0.3683181226,
        "MMLU_professional_medicine":0.5183823529,
        "MMLU_professional_psychology":0.4297385621,
        "MMLU_public_relations":0.5363636364,
        "MMLU_security_studies":0.4897959184,
        "MMLU_sociology":0.6119402985,
        "MMLU_us_foreign_policy":0.67,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.6666666667
    },
    {
        "Model":"AtomGPT_56k",
        "URL":"https:\/\/huggingface.co\/AtomEchoAI\/AtomGPT_56k",
        "full_model_name":"AtomEchoAI\/AtomGPT_56k",
        "Parameters":null,
        "MMLU_average":0.4531489195,
        "arc:challenge|25":0.4872013652,
        "hellaswag|10":0.567516431,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4222222222,
        "MMLU_astronomy":0.4671052632,
        "MMLU_business_ethics":0.44,
        "MMLU_clinical_knowledge":0.5018867925,
        "MMLU_college_biology":0.4236111111,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.34,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.4393063584,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.54,
        "MMLU_conceptual_physics":0.3574468085,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.4482758621,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.373015873,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.4677419355,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.48,
        "MMLU_high_school_european_history":0.5515151515,
        "MMLU_high_school_geography":0.5858585859,
        "MMLU_high_school_government_and_politics":0.6580310881,
        "MMLU_high_school_macroeconomics":0.4487179487,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.4789915966,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.5871559633,
        "MMLU_high_school_statistics":0.3935185185,
        "MMLU_high_school_us_history":0.4852941176,
        "MMLU_high_school_world_history":0.5696202532,
        "MMLU_human_aging":0.5470852018,
        "MMLU_human_sexuality":0.5419847328,
        "MMLU_international_law":0.5702479339,
        "MMLU_jurisprudence":0.5092592593,
        "MMLU_logical_fallacies":0.4478527607,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.5922330097,
        "MMLU_marketing":0.6709401709,
        "MMLU_medical_genetics":0.53,
        "MMLU_miscellaneous":0.6028097063,
        "MMLU_moral_disputes":0.436416185,
        "MMLU_moral_scenarios":0.2480446927,
        "MMLU_nutrition":0.5359477124,
        "MMLU_philosophy":0.5112540193,
        "MMLU_prehistory":0.487654321,
        "MMLU_professional_accounting":0.3617021277,
        "MMLU_professional_law":0.3252933507,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.3676470588,
        "MMLU_public_relations":0.5181818182,
        "MMLU_security_studies":0.4693877551,
        "MMLU_sociology":0.6218905473,
        "MMLU_us_foreign_policy":0.61,
        "MMLU_virology":0.3855421687,
        "MMLU_world_religions":0.6549707602
    },
    {
        "Model":"QuantumLM-7B",
        "URL":"https:\/\/huggingface.co\/quantumaikr\/QuantumLM-7B",
        "full_model_name":"quantumaikr\/QuantumLM-7B",
        "Parameters":7.0,
        "MMLU_average":0.4526694234,
        "arc:challenge|25":0.4650170648,
        "hellaswag|10":0.5738896634,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4222222222,
        "MMLU_astronomy":0.4342105263,
        "MMLU_business_ethics":0.39,
        "MMLU_clinical_knowledge":0.4981132075,
        "MMLU_college_biology":0.4861111111,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.37,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.3526011561,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.54,
        "MMLU_conceptual_physics":0.4042553191,
        "MMLU_econometrics":0.3421052632,
        "MMLU_electrical_engineering":0.4275862069,
        "MMLU_elementary_mathematics":0.2804232804,
        "MMLU_formal_logic":0.2222222222,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.5258064516,
        "MMLU_high_school_chemistry":0.3793103448,
        "MMLU_high_school_computer_science":0.38,
        "MMLU_high_school_european_history":0.5939393939,
        "MMLU_high_school_geography":0.5808080808,
        "MMLU_high_school_government_and_politics":0.6113989637,
        "MMLU_high_school_macroeconomics":0.3871794872,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.3781512605,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.6165137615,
        "MMLU_high_school_statistics":0.2685185185,
        "MMLU_high_school_us_history":0.637254902,
        "MMLU_high_school_world_history":0.6708860759,
        "MMLU_human_aging":0.5470852018,
        "MMLU_human_sexuality":0.4427480916,
        "MMLU_international_law":0.6115702479,
        "MMLU_jurisprudence":0.5740740741,
        "MMLU_logical_fallacies":0.4785276074,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.640776699,
        "MMLU_marketing":0.7478632479,
        "MMLU_medical_genetics":0.48,
        "MMLU_miscellaneous":0.650063857,
        "MMLU_moral_disputes":0.5202312139,
        "MMLU_moral_scenarios":0.2301675978,
        "MMLU_nutrition":0.4673202614,
        "MMLU_philosophy":0.540192926,
        "MMLU_prehistory":0.524691358,
        "MMLU_professional_accounting":0.3439716312,
        "MMLU_professional_law":0.3344198175,
        "MMLU_professional_medicine":0.3786764706,
        "MMLU_professional_psychology":0.4428104575,
        "MMLU_public_relations":0.5090909091,
        "MMLU_security_studies":0.4408163265,
        "MMLU_sociology":0.5472636816,
        "MMLU_us_foreign_policy":0.66,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.6608187135
    },
    {
        "Model":"longchat-13b-16k",
        "URL":"https:\/\/huggingface.co\/lmsys\/longchat-13b-16k",
        "full_model_name":"lmsys\/longchat-13b-16k",
        "Parameters":13.0,
        "MMLU_average":0.4523770366,
        "arc:challenge|25":0.5059726962,
        "hellaswag|10":0.590021908,
        "MMLU_abstract_algebra":0.39,
        "MMLU_anatomy":0.4222222222,
        "MMLU_astronomy":0.4013157895,
        "MMLU_business_ethics":0.47,
        "MMLU_clinical_knowledge":0.4754716981,
        "MMLU_college_biology":0.4444444444,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.3410404624,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.54,
        "MMLU_conceptual_physics":0.3617021277,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.4827586207,
        "MMLU_elementary_mathematics":0.2936507937,
        "MMLU_formal_logic":0.2777777778,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.464516129,
        "MMLU_high_school_chemistry":0.3596059113,
        "MMLU_high_school_computer_science":0.5,
        "MMLU_high_school_european_history":0.5939393939,
        "MMLU_high_school_geography":0.5909090909,
        "MMLU_high_school_government_and_politics":0.6269430052,
        "MMLU_high_school_macroeconomics":0.3974358974,
        "MMLU_high_school_mathematics":0.2333333333,
        "MMLU_high_school_microeconomics":0.4285714286,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.5633027523,
        "MMLU_high_school_statistics":0.375,
        "MMLU_high_school_us_history":0.5735294118,
        "MMLU_high_school_world_history":0.6962025316,
        "MMLU_human_aging":0.4125560538,
        "MMLU_human_sexuality":0.4503816794,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.5277777778,
        "MMLU_logical_fallacies":0.3987730061,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.6116504854,
        "MMLU_marketing":0.6196581197,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.5606641124,
        "MMLU_moral_disputes":0.4595375723,
        "MMLU_moral_scenarios":0.2581005587,
        "MMLU_nutrition":0.4836601307,
        "MMLU_philosophy":0.5241157556,
        "MMLU_prehistory":0.5,
        "MMLU_professional_accounting":0.3439716312,
        "MMLU_professional_law":0.371577575,
        "MMLU_professional_medicine":0.3602941176,
        "MMLU_professional_psychology":0.4395424837,
        "MMLU_public_relations":0.5090909091,
        "MMLU_security_studies":0.5102040816,
        "MMLU_sociology":0.6218905473,
        "MMLU_us_foreign_policy":0.69,
        "MMLU_virology":0.4518072289,
        "MMLU_world_religions":0.6666666667
    },
    {
        "Model":"tigerbot-7b-base",
        "URL":"https:\/\/huggingface.co\/TigerResearch\/tigerbot-7b-base",
        "full_model_name":"TigerResearch\/tigerbot-7b-base",
        "Parameters":7.0,
        "MMLU_average":0.451054941,
        "arc:challenge|25":0.457337884,
        "hellaswag|10":0.5317665804,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.4671052632,
        "MMLU_business_ethics":0.45,
        "MMLU_clinical_knowledge":0.4830188679,
        "MMLU_college_biology":0.4166666667,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.4277456647,
        "MMLU_college_physics":0.1764705882,
        "MMLU_computer_security":0.56,
        "MMLU_conceptual_physics":0.3489361702,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.4551724138,
        "MMLU_elementary_mathematics":0.2619047619,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.4967741935,
        "MMLU_high_school_chemistry":0.3448275862,
        "MMLU_high_school_computer_science":0.34,
        "MMLU_high_school_european_history":0.5636363636,
        "MMLU_high_school_geography":0.5909090909,
        "MMLU_high_school_government_and_politics":0.6683937824,
        "MMLU_high_school_macroeconomics":0.4820512821,
        "MMLU_high_school_mathematics":0.2259259259,
        "MMLU_high_school_microeconomics":0.4369747899,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.6293577982,
        "MMLU_high_school_statistics":0.3472222222,
        "MMLU_high_school_us_history":0.5196078431,
        "MMLU_high_school_world_history":0.6329113924,
        "MMLU_human_aging":0.4843049327,
        "MMLU_human_sexuality":0.5190839695,
        "MMLU_international_law":0.5371900826,
        "MMLU_jurisprudence":0.5277777778,
        "MMLU_logical_fallacies":0.5705521472,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.5145631068,
        "MMLU_marketing":0.6837606838,
        "MMLU_medical_genetics":0.53,
        "MMLU_miscellaneous":0.6053639847,
        "MMLU_moral_disputes":0.4739884393,
        "MMLU_moral_scenarios":0.2715083799,
        "MMLU_nutrition":0.5098039216,
        "MMLU_philosophy":0.5530546624,
        "MMLU_prehistory":0.4783950617,
        "MMLU_professional_accounting":0.3368794326,
        "MMLU_professional_law":0.3233376793,
        "MMLU_professional_medicine":0.3713235294,
        "MMLU_professional_psychology":0.3774509804,
        "MMLU_public_relations":0.5363636364,
        "MMLU_security_studies":0.493877551,
        "MMLU_sociology":0.5820895522,
        "MMLU_us_foreign_policy":0.69,
        "MMLU_virology":0.4036144578,
        "MMLU_world_religions":0.6608187135
    },
    {
        "Model":"stablelm-base-alpha-7b-v2",
        "URL":"https:\/\/huggingface.co\/stabilityai\/stablelm-base-alpha-7b-v2",
        "full_model_name":"stabilityai\/stablelm-base-alpha-7b-v2",
        "Parameters":7.0,
        "MMLU_average":0.4509893993,
        "arc:challenge|25":0.4300341297,
        "hellaswag|10":0.5716988648,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4518518519,
        "MMLU_astronomy":0.5394736842,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.4641509434,
        "MMLU_college_biology":0.4791666667,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.4046242775,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.58,
        "MMLU_conceptual_physics":0.3744680851,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.4551724138,
        "MMLU_elementary_mathematics":0.2857142857,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.5,
        "MMLU_high_school_chemistry":0.3497536946,
        "MMLU_high_school_computer_science":0.46,
        "MMLU_high_school_european_history":0.5151515152,
        "MMLU_high_school_geography":0.5202020202,
        "MMLU_high_school_government_and_politics":0.6321243523,
        "MMLU_high_school_macroeconomics":0.4,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.3823529412,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.6,
        "MMLU_high_school_statistics":0.2685185185,
        "MMLU_high_school_us_history":0.5294117647,
        "MMLU_high_school_world_history":0.5864978903,
        "MMLU_human_aging":0.466367713,
        "MMLU_human_sexuality":0.5496183206,
        "MMLU_international_law":0.520661157,
        "MMLU_jurisprudence":0.5462962963,
        "MMLU_logical_fallacies":0.4969325153,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.5825242718,
        "MMLU_marketing":0.6196581197,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.6475095785,
        "MMLU_moral_disputes":0.4797687861,
        "MMLU_moral_scenarios":0.2648044693,
        "MMLU_nutrition":0.4705882353,
        "MMLU_philosophy":0.5209003215,
        "MMLU_prehistory":0.5432098765,
        "MMLU_professional_accounting":0.3368794326,
        "MMLU_professional_law":0.3761408083,
        "MMLU_professional_medicine":0.4742647059,
        "MMLU_professional_psychology":0.408496732,
        "MMLU_public_relations":0.4818181818,
        "MMLU_security_studies":0.4653061224,
        "MMLU_sociology":0.6616915423,
        "MMLU_us_foreign_policy":0.58,
        "MMLU_virology":0.4036144578,
        "MMLU_world_religions":0.6549707602
    },
    {
        "Model":"airoboros-l2-7b-gpt4-2.0",
        "URL":"https:\/\/huggingface.co\/jondurbin\/airoboros-l2-7b-gpt4-2.0",
        "full_model_name":"jondurbin\/airoboros-l2-7b-gpt4-2.0",
        "Parameters":7.0,
        "MMLU_average":0.4508843435,
        "arc:challenge|25":0.502559727,
        "hellaswag|10":0.5952997411,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.3815789474,
        "MMLU_business_ethics":0.47,
        "MMLU_clinical_knowledge":0.4867924528,
        "MMLU_college_biology":0.4513888889,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.37,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.3699421965,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.58,
        "MMLU_conceptual_physics":0.3489361702,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.4413793103,
        "MMLU_elementary_mathematics":0.3121693122,
        "MMLU_formal_logic":0.2142857143,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.5,
        "MMLU_high_school_chemistry":0.3694581281,
        "MMLU_high_school_computer_science":0.44,
        "MMLU_high_school_european_history":0.5575757576,
        "MMLU_high_school_geography":0.5202020202,
        "MMLU_high_school_government_and_politics":0.6062176166,
        "MMLU_high_school_macroeconomics":0.4230769231,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.4117647059,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.5889908257,
        "MMLU_high_school_statistics":0.2314814815,
        "MMLU_high_school_us_history":0.5735294118,
        "MMLU_high_school_world_history":0.611814346,
        "MMLU_human_aging":0.5426008969,
        "MMLU_human_sexuality":0.4961832061,
        "MMLU_international_law":0.6776859504,
        "MMLU_jurisprudence":0.5092592593,
        "MMLU_logical_fallacies":0.4662576687,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.5631067961,
        "MMLU_marketing":0.6324786325,
        "MMLU_medical_genetics":0.5,
        "MMLU_miscellaneous":0.5964240102,
        "MMLU_moral_disputes":0.4913294798,
        "MMLU_moral_scenarios":0.2558659218,
        "MMLU_nutrition":0.4836601307,
        "MMLU_philosophy":0.5498392283,
        "MMLU_prehistory":0.4660493827,
        "MMLU_professional_accounting":0.390070922,
        "MMLU_professional_law":0.3448500652,
        "MMLU_professional_medicine":0.4632352941,
        "MMLU_professional_psychology":0.4558823529,
        "MMLU_public_relations":0.4909090909,
        "MMLU_security_studies":0.5428571429,
        "MMLU_sociology":0.5721393035,
        "MMLU_us_foreign_policy":0.7,
        "MMLU_virology":0.4036144578,
        "MMLU_world_religions":0.6725146199
    },
    {
        "Model":"CodeLlama13B-Finetune-v1",
        "URL":"https:\/\/huggingface.co\/FelixChao\/CodeLlama13B-Finetune-v1",
        "full_model_name":"FelixChao\/CodeLlama13B-Finetune-v1",
        "Parameters":13.0,
        "MMLU_average":0.4504944013,
        "arc:challenge|25":0.4402730375,
        "hellaswag|10":0.5123481378,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.3333333333,
        "MMLU_astronomy":0.4342105263,
        "MMLU_business_ethics":0.44,
        "MMLU_clinical_knowledge":0.4490566038,
        "MMLU_college_biology":0.3680555556,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.43,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.3930635838,
        "MMLU_college_physics":0.3431372549,
        "MMLU_computer_security":0.61,
        "MMLU_conceptual_physics":0.3191489362,
        "MMLU_econometrics":0.3596491228,
        "MMLU_electrical_engineering":0.4689655172,
        "MMLU_elementary_mathematics":0.2989417989,
        "MMLU_formal_logic":0.373015873,
        "MMLU_global_facts":0.27,
        "MMLU_high_school_biology":0.4580645161,
        "MMLU_high_school_chemistry":0.3497536946,
        "MMLU_high_school_computer_science":0.58,
        "MMLU_high_school_european_history":0.5818181818,
        "MMLU_high_school_geography":0.5757575758,
        "MMLU_high_school_government_and_politics":0.6062176166,
        "MMLU_high_school_macroeconomics":0.4153846154,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.4705882353,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.5908256881,
        "MMLU_high_school_statistics":0.4259259259,
        "MMLU_high_school_us_history":0.5294117647,
        "MMLU_high_school_world_history":0.5611814346,
        "MMLU_human_aging":0.4170403587,
        "MMLU_human_sexuality":0.4961832061,
        "MMLU_international_law":0.5950413223,
        "MMLU_jurisprudence":0.5,
        "MMLU_logical_fallacies":0.5337423313,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.6116504854,
        "MMLU_marketing":0.7264957265,
        "MMLU_medical_genetics":0.48,
        "MMLU_miscellaneous":0.5325670498,
        "MMLU_moral_disputes":0.436416185,
        "MMLU_moral_scenarios":0.2748603352,
        "MMLU_nutrition":0.4803921569,
        "MMLU_philosophy":0.5273311897,
        "MMLU_prehistory":0.4351851852,
        "MMLU_professional_accounting":0.3439716312,
        "MMLU_professional_law":0.3207301173,
        "MMLU_professional_medicine":0.3970588235,
        "MMLU_professional_psychology":0.3758169935,
        "MMLU_public_relations":0.5363636364,
        "MMLU_security_studies":0.5510204082,
        "MMLU_sociology":0.6069651741,
        "MMLU_us_foreign_policy":0.62,
        "MMLU_virology":0.3915662651,
        "MMLU_world_religions":0.567251462
    },
    {
        "Model":"persimmon-8b-chat",
        "URL":"https:\/\/huggingface.co\/adept\/persimmon-8b-chat",
        "full_model_name":"adept\/persimmon-8b-chat",
        "Parameters":8.0,
        "MMLU_average":0.449779136,
        "arc:challenge|25":0.4385665529,
        "hellaswag|10":0.5435172276,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.4407894737,
        "MMLU_business_ethics":0.54,
        "MMLU_clinical_knowledge":0.5132075472,
        "MMLU_college_biology":0.5347222222,
        "MMLU_college_chemistry":0.29,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.4219653179,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.62,
        "MMLU_conceptual_physics":0.4085106383,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.4275862069,
        "MMLU_elementary_mathematics":0.2883597884,
        "MMLU_formal_logic":0.2777777778,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.5225806452,
        "MMLU_high_school_chemistry":0.3349753695,
        "MMLU_high_school_computer_science":0.5,
        "MMLU_high_school_european_history":0.5939393939,
        "MMLU_high_school_geography":0.5202020202,
        "MMLU_high_school_government_and_politics":0.585492228,
        "MMLU_high_school_macroeconomics":0.4358974359,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.4159663866,
        "MMLU_high_school_physics":0.2450331126,
        "MMLU_high_school_psychology":0.5596330275,
        "MMLU_high_school_statistics":0.2777777778,
        "MMLU_high_school_us_history":0.5539215686,
        "MMLU_high_school_world_history":0.6286919831,
        "MMLU_human_aging":0.5515695067,
        "MMLU_human_sexuality":0.5267175573,
        "MMLU_international_law":0.5867768595,
        "MMLU_jurisprudence":0.4537037037,
        "MMLU_logical_fallacies":0.5582822086,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.6019417476,
        "MMLU_marketing":0.6752136752,
        "MMLU_medical_genetics":0.5,
        "MMLU_miscellaneous":0.5747126437,
        "MMLU_moral_disputes":0.4797687861,
        "MMLU_moral_scenarios":0.2402234637,
        "MMLU_nutrition":0.4477124183,
        "MMLU_philosophy":0.4790996785,
        "MMLU_prehistory":0.4598765432,
        "MMLU_professional_accounting":0.329787234,
        "MMLU_professional_law":0.3546284224,
        "MMLU_professional_medicine":0.3345588235,
        "MMLU_professional_psychology":0.4346405229,
        "MMLU_public_relations":0.5363636364,
        "MMLU_security_studies":0.3959183673,
        "MMLU_sociology":0.6218905473,
        "MMLU_us_foreign_policy":0.64,
        "MMLU_virology":0.4096385542,
        "MMLU_world_religions":0.6023391813
    },
    {
        "Model":"leo-hessianai-7b-chat-bilingual",
        "URL":"https:\/\/huggingface.co\/LeoLM\/leo-hessianai-7b-chat-bilingual",
        "full_model_name":"LeoLM\/leo-hessianai-7b-chat-bilingual",
        "Parameters":7.0,
        "MMLU_average":0.4468402994,
        "arc:challenge|25":0.4880546075,
        "hellaswag|10":0.567516431,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4074074074,
        "MMLU_astronomy":0.4934210526,
        "MMLU_business_ethics":0.39,
        "MMLU_clinical_knowledge":0.4754716981,
        "MMLU_college_biology":0.4027777778,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.36,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.387283237,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.59,
        "MMLU_conceptual_physics":0.3744680851,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.3862068966,
        "MMLU_elementary_mathematics":0.2857142857,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.5483870968,
        "MMLU_high_school_chemistry":0.3891625616,
        "MMLU_high_school_computer_science":0.35,
        "MMLU_high_school_european_history":0.6,
        "MMLU_high_school_geography":0.5606060606,
        "MMLU_high_school_government_and_politics":0.621761658,
        "MMLU_high_school_macroeconomics":0.3846153846,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.4117647059,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.6183486239,
        "MMLU_high_school_statistics":0.3981481481,
        "MMLU_high_school_us_history":0.5441176471,
        "MMLU_high_school_world_history":0.6202531646,
        "MMLU_human_aging":0.4304932735,
        "MMLU_human_sexuality":0.5801526718,
        "MMLU_international_law":0.5950413223,
        "MMLU_jurisprudence":0.5185185185,
        "MMLU_logical_fallacies":0.4601226994,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.6116504854,
        "MMLU_marketing":0.5854700855,
        "MMLU_medical_genetics":0.45,
        "MMLU_miscellaneous":0.6104725415,
        "MMLU_moral_disputes":0.4277456647,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.4967320261,
        "MMLU_philosophy":0.5627009646,
        "MMLU_prehistory":0.5092592593,
        "MMLU_professional_accounting":0.3404255319,
        "MMLU_professional_law":0.3898305085,
        "MMLU_professional_medicine":0.4779411765,
        "MMLU_professional_psychology":0.4133986928,
        "MMLU_public_relations":0.5272727273,
        "MMLU_security_studies":0.4571428571,
        "MMLU_sociology":0.6169154229,
        "MMLU_us_foreign_policy":0.57,
        "MMLU_virology":0.4397590361,
        "MMLU_world_religions":0.6023391813
    },
    {
        "Model":"ELYZA-japanese-Llama-2-7b",
        "URL":"https:\/\/huggingface.co\/elyza\/ELYZA-japanese-Llama-2-7b",
        "full_model_name":"elyza\/ELYZA-japanese-Llama-2-7b",
        "Parameters":7.0,
        "MMLU_average":0.4459585935,
        "arc:challenge|25":0.4778156997,
        "hellaswag|10":0.5593507269,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.4342105263,
        "MMLU_business_ethics":0.45,
        "MMLU_clinical_knowledge":0.5169811321,
        "MMLU_college_biology":0.4791666667,
        "MMLU_college_chemistry":0.28,
        "MMLU_college_computer_science":0.34,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.3930635838,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.55,
        "MMLU_conceptual_physics":0.3957446809,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.3862068966,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.2222222222,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.4774193548,
        "MMLU_high_school_chemistry":0.4039408867,
        "MMLU_high_school_computer_science":0.43,
        "MMLU_high_school_european_history":0.5454545455,
        "MMLU_high_school_geography":0.5606060606,
        "MMLU_high_school_government_and_politics":0.6580310881,
        "MMLU_high_school_macroeconomics":0.3717948718,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.4243697479,
        "MMLU_high_school_physics":0.2450331126,
        "MMLU_high_school_psychology":0.5963302752,
        "MMLU_high_school_statistics":0.2546296296,
        "MMLU_high_school_us_history":0.5588235294,
        "MMLU_high_school_world_history":0.5485232068,
        "MMLU_human_aging":0.5739910314,
        "MMLU_human_sexuality":0.4732824427,
        "MMLU_international_law":0.5785123967,
        "MMLU_jurisprudence":0.5,
        "MMLU_logical_fallacies":0.5153374233,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.6019417476,
        "MMLU_marketing":0.6923076923,
        "MMLU_medical_genetics":0.46,
        "MMLU_miscellaneous":0.6590038314,
        "MMLU_moral_disputes":0.5,
        "MMLU_moral_scenarios":0.2402234637,
        "MMLU_nutrition":0.4607843137,
        "MMLU_philosophy":0.575562701,
        "MMLU_prehistory":0.5185185185,
        "MMLU_professional_accounting":0.365248227,
        "MMLU_professional_law":0.3187744459,
        "MMLU_professional_medicine":0.3198529412,
        "MMLU_professional_psychology":0.4460784314,
        "MMLU_public_relations":0.5272727273,
        "MMLU_security_studies":0.4693877551,
        "MMLU_sociology":0.5771144279,
        "MMLU_us_foreign_policy":0.66,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.6549707602
    },
    {
        "Model":"gogpt2-13b-chat",
        "URL":"https:\/\/huggingface.co\/golaxy\/gogpt2-13b-chat",
        "full_model_name":"golaxy\/gogpt2-13b-chat",
        "Parameters":13.0,
        "MMLU_average":0.4450391266,
        "arc:challenge|25":0.4411262799,
        "hellaswag|10":0.5416251743,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.4,
        "MMLU_astronomy":0.4078947368,
        "MMLU_business_ethics":0.45,
        "MMLU_clinical_knowledge":0.4301886792,
        "MMLU_college_biology":0.4513888889,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.3699421965,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.55,
        "MMLU_conceptual_physics":0.3446808511,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4137931034,
        "MMLU_elementary_mathematics":0.2592592593,
        "MMLU_formal_logic":0.3015873016,
        "MMLU_global_facts":0.29,
        "MMLU_high_school_biology":0.4838709677,
        "MMLU_high_school_chemistry":0.2906403941,
        "MMLU_high_school_computer_science":0.54,
        "MMLU_high_school_european_history":0.5212121212,
        "MMLU_high_school_geography":0.5656565657,
        "MMLU_high_school_government_and_politics":0.6010362694,
        "MMLU_high_school_macroeconomics":0.3692307692,
        "MMLU_high_school_mathematics":0.2888888889,
        "MMLU_high_school_microeconomics":0.3529411765,
        "MMLU_high_school_physics":0.2450331126,
        "MMLU_high_school_psychology":0.5743119266,
        "MMLU_high_school_statistics":0.2824074074,
        "MMLU_high_school_us_history":0.5392156863,
        "MMLU_high_school_world_history":0.5738396624,
        "MMLU_human_aging":0.5426008969,
        "MMLU_human_sexuality":0.465648855,
        "MMLU_international_law":0.652892562,
        "MMLU_jurisprudence":0.4814814815,
        "MMLU_logical_fallacies":0.5398773006,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.5922330097,
        "MMLU_marketing":0.7393162393,
        "MMLU_medical_genetics":0.45,
        "MMLU_miscellaneous":0.632183908,
        "MMLU_moral_disputes":0.5086705202,
        "MMLU_moral_scenarios":0.261452514,
        "MMLU_nutrition":0.4640522876,
        "MMLU_philosophy":0.5048231511,
        "MMLU_prehistory":0.4537037037,
        "MMLU_professional_accounting":0.3581560284,
        "MMLU_professional_law":0.3448500652,
        "MMLU_professional_medicine":0.3676470588,
        "MMLU_professional_psychology":0.4460784314,
        "MMLU_public_relations":0.5545454545,
        "MMLU_security_studies":0.5102040816,
        "MMLU_sociology":0.5970149254,
        "MMLU_us_foreign_policy":0.69,
        "MMLU_virology":0.4096385542,
        "MMLU_world_religions":0.6783625731
    },
    {
        "Model":"llama2_7b_chat_uncensored",
        "URL":"https:\/\/huggingface.co\/georgesung\/llama2_7b_chat_uncensored",
        "full_model_name":"georgesung\/llama2_7b_chat_uncensored",
        "Parameters":7.0,
        "MMLU_average":0.4448623629,
        "arc:challenge|25":0.4991467577,
        "hellaswag|10":0.5963951404,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4222222222,
        "MMLU_astronomy":0.4013157895,
        "MMLU_business_ethics":0.5,
        "MMLU_clinical_knowledge":0.4226415094,
        "MMLU_college_biology":0.4375,
        "MMLU_college_chemistry":0.32,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.3757225434,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.54,
        "MMLU_conceptual_physics":0.4255319149,
        "MMLU_econometrics":0.3333333333,
        "MMLU_electrical_engineering":0.3448275862,
        "MMLU_elementary_mathematics":0.2592592593,
        "MMLU_formal_logic":0.3968253968,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.4741935484,
        "MMLU_high_school_chemistry":0.3300492611,
        "MMLU_high_school_computer_science":0.41,
        "MMLU_high_school_european_history":0.6,
        "MMLU_high_school_geography":0.4848484848,
        "MMLU_high_school_government_and_politics":0.6476683938,
        "MMLU_high_school_macroeconomics":0.4,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.3949579832,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.5871559633,
        "MMLU_high_school_statistics":0.1898148148,
        "MMLU_high_school_us_history":0.5245098039,
        "MMLU_high_school_world_history":0.5443037975,
        "MMLU_human_aging":0.5426008969,
        "MMLU_human_sexuality":0.5419847328,
        "MMLU_international_law":0.6280991736,
        "MMLU_jurisprudence":0.4907407407,
        "MMLU_logical_fallacies":0.490797546,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.5242718447,
        "MMLU_marketing":0.6623931624,
        "MMLU_medical_genetics":0.51,
        "MMLU_miscellaneous":0.6245210728,
        "MMLU_moral_disputes":0.5086705202,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.4673202614,
        "MMLU_philosophy":0.5819935691,
        "MMLU_prehistory":0.512345679,
        "MMLU_professional_accounting":0.3404255319,
        "MMLU_professional_law":0.3487614081,
        "MMLU_professional_medicine":0.4522058824,
        "MMLU_professional_psychology":0.4526143791,
        "MMLU_public_relations":0.5545454545,
        "MMLU_security_studies":0.4081632653,
        "MMLU_sociology":0.5870646766,
        "MMLU_us_foreign_policy":0.69,
        "MMLU_virology":0.3975903614,
        "MMLU_world_religions":0.6549707602
    },
    {
        "Model":"llama-v2-7b-32kC-Security",
        "URL":"https:\/\/huggingface.co\/venkycs\/llama-v2-7b-32kC-Security",
        "full_model_name":"venkycs\/llama-v2-7b-32kC-Security",
        "Parameters":7.0,
        "MMLU_average":0.444102986,
        "arc:challenge|25":0.45221843,
        "hellaswag|10":0.5764787891,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.437037037,
        "MMLU_astronomy":0.4407894737,
        "MMLU_business_ethics":0.47,
        "MMLU_clinical_knowledge":0.479245283,
        "MMLU_college_biology":0.5,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.38,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.4104046243,
        "MMLU_college_physics":0.1666666667,
        "MMLU_computer_security":0.59,
        "MMLU_conceptual_physics":0.3914893617,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.4,
        "MMLU_elementary_mathematics":0.2936507937,
        "MMLU_formal_logic":0.2380952381,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.5096774194,
        "MMLU_high_school_chemistry":0.3399014778,
        "MMLU_high_school_computer_science":0.42,
        "MMLU_high_school_european_history":0.6727272727,
        "MMLU_high_school_geography":0.5050505051,
        "MMLU_high_school_government_and_politics":0.621761658,
        "MMLU_high_school_macroeconomics":0.3717948718,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.3907563025,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.6201834862,
        "MMLU_high_school_statistics":0.3101851852,
        "MMLU_high_school_us_history":0.612745098,
        "MMLU_high_school_world_history":0.6202531646,
        "MMLU_human_aging":0.5112107623,
        "MMLU_human_sexuality":0.4503816794,
        "MMLU_international_law":0.5785123967,
        "MMLU_jurisprudence":0.5648148148,
        "MMLU_logical_fallacies":0.4539877301,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.5436893204,
        "MMLU_marketing":0.6623931624,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.6181353768,
        "MMLU_moral_disputes":0.4768786127,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.4248366013,
        "MMLU_philosophy":0.5337620579,
        "MMLU_prehistory":0.4814814815,
        "MMLU_professional_accounting":0.3617021277,
        "MMLU_professional_law":0.3604954368,
        "MMLU_professional_medicine":0.3823529412,
        "MMLU_professional_psychology":0.4624183007,
        "MMLU_public_relations":0.5454545455,
        "MMLU_security_studies":0.412244898,
        "MMLU_sociology":0.6069651741,
        "MMLU_us_foreign_policy":0.59,
        "MMLU_virology":0.3373493976,
        "MMLU_world_religions":0.6140350877
    },
    {
        "Model":"Qwen-VL-LLaMAfied-7B-Chat",
        "URL":"https:\/\/huggingface.co\/JosephusCheung\/Qwen-VL-LLaMAfied-7B-Chat",
        "full_model_name":"JosephusCheung\/Qwen-VL-LLaMAfied-7B-Chat",
        "Parameters":7.0,
        "MMLU_average":0.4412395853,
        "arc:challenge|25":0.457337884,
        "hellaswag|10":0.5238996216,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.4074074074,
        "MMLU_astronomy":0.4210526316,
        "MMLU_business_ethics":0.44,
        "MMLU_clinical_knowledge":0.4679245283,
        "MMLU_college_biology":0.3472222222,
        "MMLU_college_chemistry":0.26,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.387283237,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.49,
        "MMLU_conceptual_physics":0.3829787234,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.4482758621,
        "MMLU_elementary_mathematics":0.2486772487,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.5387096774,
        "MMLU_high_school_chemistry":0.3349753695,
        "MMLU_high_school_computer_science":0.47,
        "MMLU_high_school_european_history":0.4242424242,
        "MMLU_high_school_geography":0.5656565657,
        "MMLU_high_school_government_and_politics":0.6269430052,
        "MMLU_high_school_macroeconomics":0.4205128205,
        "MMLU_high_school_mathematics":0.237037037,
        "MMLU_high_school_microeconomics":0.4453781513,
        "MMLU_high_school_physics":0.2317880795,
        "MMLU_high_school_psychology":0.5724770642,
        "MMLU_high_school_statistics":0.3055555556,
        "MMLU_high_school_us_history":0.5490196078,
        "MMLU_high_school_world_history":0.5949367089,
        "MMLU_human_aging":0.5784753363,
        "MMLU_human_sexuality":0.4351145038,
        "MMLU_international_law":0.6611570248,
        "MMLU_jurisprudence":0.537037037,
        "MMLU_logical_fallacies":0.4785276074,
        "MMLU_machine_learning":0.4107142857,
        "MMLU_management":0.5048543689,
        "MMLU_marketing":0.7521367521,
        "MMLU_medical_genetics":0.47,
        "MMLU_miscellaneous":0.6143039591,
        "MMLU_moral_disputes":0.5260115607,
        "MMLU_moral_scenarios":0.2715083799,
        "MMLU_nutrition":0.431372549,
        "MMLU_philosophy":0.4758842444,
        "MMLU_prehistory":0.487654321,
        "MMLU_professional_accounting":0.3156028369,
        "MMLU_professional_law":0.350065189,
        "MMLU_professional_medicine":0.3198529412,
        "MMLU_professional_psychology":0.4248366013,
        "MMLU_public_relations":0.5272727273,
        "MMLU_security_studies":0.5551020408,
        "MMLU_sociology":0.5820895522,
        "MMLU_us_foreign_policy":0.65,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.5964912281
    },
    {
        "Model":"Llama-2-7B-GPTQ",
        "URL":"https:\/\/huggingface.co\/TheBloke\/Llama-2-7B-GPTQ",
        "full_model_name":"TheBloke\/Llama-2-7B-GPTQ",
        "Parameters":7.0,
        "MMLU_average":0.4398604976,
        "arc:challenge|25":0.478668942,
        "hellaswag|10":0.5760804621,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.3947368421,
        "MMLU_business_ethics":0.46,
        "MMLU_clinical_knowledge":0.4188679245,
        "MMLU_college_biology":0.4097222222,
        "MMLU_college_chemistry":0.23,
        "MMLU_college_computer_science":0.36,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.3757225434,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.57,
        "MMLU_conceptual_physics":0.4085106383,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.2433862434,
        "MMLU_formal_logic":0.3412698413,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.4548387097,
        "MMLU_high_school_chemistry":0.3054187192,
        "MMLU_high_school_computer_science":0.4,
        "MMLU_high_school_european_history":0.5393939394,
        "MMLU_high_school_geography":0.4797979798,
        "MMLU_high_school_government_and_politics":0.6476683938,
        "MMLU_high_school_macroeconomics":0.3794871795,
        "MMLU_high_school_mathematics":0.2925925926,
        "MMLU_high_school_microeconomics":0.3823529412,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.5834862385,
        "MMLU_high_school_statistics":0.2222222222,
        "MMLU_high_school_us_history":0.4901960784,
        "MMLU_high_school_world_history":0.5021097046,
        "MMLU_human_aging":0.5291479821,
        "MMLU_human_sexuality":0.5114503817,
        "MMLU_international_law":0.6115702479,
        "MMLU_jurisprudence":0.4907407407,
        "MMLU_logical_fallacies":0.490797546,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.4854368932,
        "MMLU_marketing":0.6666666667,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.5810983397,
        "MMLU_moral_disputes":0.4971098266,
        "MMLU_moral_scenarios":0.2391061453,
        "MMLU_nutrition":0.4705882353,
        "MMLU_philosophy":0.5659163987,
        "MMLU_prehistory":0.4907407407,
        "MMLU_professional_accounting":0.329787234,
        "MMLU_professional_law":0.3578878748,
        "MMLU_professional_medicine":0.4264705882,
        "MMLU_professional_psychology":0.4411764706,
        "MMLU_public_relations":0.5,
        "MMLU_security_studies":0.493877551,
        "MMLU_sociology":0.6069651741,
        "MMLU_us_foreign_policy":0.64,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.6783625731
    },
    {
        "Model":"WizardLM-13B-Uncensored",
        "URL":"https:\/\/huggingface.co\/ehartford\/WizardLM-13B-Uncensored",
        "full_model_name":"ehartford\/WizardLM-13B-Uncensored",
        "Parameters":13.0,
        "MMLU_average":0.4395625285,
        "arc:challenge|25":0.4897610922,
        "hellaswag|10":0.5810595499,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4,
        "MMLU_astronomy":0.3486842105,
        "MMLU_business_ethics":0.46,
        "MMLU_clinical_knowledge":0.4490566038,
        "MMLU_college_biology":0.3958333333,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.39,
        "MMLU_college_medicine":0.3757225434,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.54,
        "MMLU_conceptual_physics":0.4127659574,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.3655172414,
        "MMLU_elementary_mathematics":0.2698412698,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.4548387097,
        "MMLU_high_school_chemistry":0.2660098522,
        "MMLU_high_school_computer_science":0.48,
        "MMLU_high_school_european_history":0.5333333333,
        "MMLU_high_school_geography":0.5353535354,
        "MMLU_high_school_government_and_politics":0.6113989637,
        "MMLU_high_school_macroeconomics":0.4205128205,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.4243697479,
        "MMLU_high_school_physics":0.2185430464,
        "MMLU_high_school_psychology":0.5559633028,
        "MMLU_high_school_statistics":0.2546296296,
        "MMLU_high_school_us_history":0.5882352941,
        "MMLU_high_school_world_history":0.6075949367,
        "MMLU_human_aging":0.5515695067,
        "MMLU_human_sexuality":0.4351145038,
        "MMLU_international_law":0.5702479339,
        "MMLU_jurisprudence":0.5185185185,
        "MMLU_logical_fallacies":0.4478527607,
        "MMLU_machine_learning":0.4017857143,
        "MMLU_management":0.6019417476,
        "MMLU_marketing":0.6923076923,
        "MMLU_medical_genetics":0.5,
        "MMLU_miscellaneous":0.6206896552,
        "MMLU_moral_disputes":0.4450867052,
        "MMLU_moral_scenarios":0.2849162011,
        "MMLU_nutrition":0.4836601307,
        "MMLU_philosophy":0.463022508,
        "MMLU_prehistory":0.4814814815,
        "MMLU_professional_accounting":0.3333333333,
        "MMLU_professional_law":0.3539765319,
        "MMLU_professional_medicine":0.4080882353,
        "MMLU_professional_psychology":0.4591503268,
        "MMLU_public_relations":0.5636363636,
        "MMLU_security_studies":0.4653061224,
        "MMLU_sociology":0.5273631841,
        "MMLU_us_foreign_policy":0.66,
        "MMLU_virology":0.3915662651,
        "MMLU_world_religions":0.6374269006
    },
    {
        "Model":"llama-2-7b-dolphin_10w-test",
        "URL":"https:\/\/huggingface.co\/CHIH-HUNG\/llama-2-7b-dolphin_10w-test",
        "full_model_name":"CHIH-HUNG\/llama-2-7b-dolphin_10w-test",
        "Parameters":7.0,
        "MMLU_average":0.4390323664,
        "arc:challenge|25":0.4684300341,
        "hellaswag|10":0.5467038439,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.4210526316,
        "MMLU_business_ethics":0.42,
        "MMLU_clinical_knowledge":0.4905660377,
        "MMLU_college_biology":0.4236111111,
        "MMLU_college_chemistry":0.3,
        "MMLU_college_computer_science":0.4,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.4219653179,
        "MMLU_college_physics":0.1666666667,
        "MMLU_computer_security":0.55,
        "MMLU_conceptual_physics":0.4085106383,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.4206896552,
        "MMLU_elementary_mathematics":0.2698412698,
        "MMLU_formal_logic":0.2619047619,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.4548387097,
        "MMLU_high_school_chemistry":0.3103448276,
        "MMLU_high_school_computer_science":0.47,
        "MMLU_high_school_european_history":0.5575757576,
        "MMLU_high_school_geography":0.5404040404,
        "MMLU_high_school_government_and_politics":0.5958549223,
        "MMLU_high_school_macroeconomics":0.4153846154,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.3781512605,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.5853211009,
        "MMLU_high_school_statistics":0.4166666667,
        "MMLU_high_school_us_history":0.5637254902,
        "MMLU_high_school_world_history":0.6666666667,
        "MMLU_human_aging":0.4394618834,
        "MMLU_human_sexuality":0.4885496183,
        "MMLU_international_law":0.6033057851,
        "MMLU_jurisprudence":0.5185185185,
        "MMLU_logical_fallacies":0.4171779141,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.572815534,
        "MMLU_marketing":0.6666666667,
        "MMLU_medical_genetics":0.43,
        "MMLU_miscellaneous":0.6079182631,
        "MMLU_moral_disputes":0.450867052,
        "MMLU_moral_scenarios":0.2625698324,
        "MMLU_nutrition":0.4248366013,
        "MMLU_philosophy":0.4951768489,
        "MMLU_prehistory":0.475308642,
        "MMLU_professional_accounting":0.3617021277,
        "MMLU_professional_law":0.3383311604,
        "MMLU_professional_medicine":0.4080882353,
        "MMLU_professional_psychology":0.4264705882,
        "MMLU_public_relations":0.5454545455,
        "MMLU_security_studies":0.4571428571,
        "MMLU_sociology":0.5074626866,
        "MMLU_us_foreign_policy":0.62,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.6140350877
    },
    {
        "Model":"llama-shishya-7b-ep3-v2",
        "URL":"https:\/\/huggingface.co\/luffycodes\/llama-shishya-7b-ep3-v2",
        "full_model_name":"luffycodes\/llama-shishya-7b-ep3-v2",
        "Parameters":7.0,
        "MMLU_average":0.4383784502,
        "arc:challenge|25":0.4419795222,
        "hellaswag|10":0.5865365465,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.4222222222,
        "MMLU_astronomy":0.4144736842,
        "MMLU_business_ethics":0.38,
        "MMLU_clinical_knowledge":0.4716981132,
        "MMLU_college_biology":0.4236111111,
        "MMLU_college_chemistry":0.28,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.4104046243,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.54,
        "MMLU_conceptual_physics":0.3957446809,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.3931034483,
        "MMLU_elementary_mathematics":0.3042328042,
        "MMLU_formal_logic":0.1904761905,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.5096774194,
        "MMLU_high_school_chemistry":0.3201970443,
        "MMLU_high_school_computer_science":0.44,
        "MMLU_high_school_european_history":0.5696969697,
        "MMLU_high_school_geography":0.5101010101,
        "MMLU_high_school_government_and_politics":0.585492228,
        "MMLU_high_school_macroeconomics":0.3743589744,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.4201680672,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.5944954128,
        "MMLU_high_school_statistics":0.2175925926,
        "MMLU_high_school_us_history":0.5784313725,
        "MMLU_high_school_world_history":0.6835443038,
        "MMLU_human_aging":0.5605381166,
        "MMLU_human_sexuality":0.4809160305,
        "MMLU_international_law":0.6280991736,
        "MMLU_jurisprudence":0.4907407407,
        "MMLU_logical_fallacies":0.4417177914,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.6116504854,
        "MMLU_marketing":0.6923076923,
        "MMLU_medical_genetics":0.47,
        "MMLU_miscellaneous":0.6590038314,
        "MMLU_moral_disputes":0.4942196532,
        "MMLU_moral_scenarios":0.261452514,
        "MMLU_nutrition":0.4640522876,
        "MMLU_philosophy":0.536977492,
        "MMLU_prehistory":0.4845679012,
        "MMLU_professional_accounting":0.365248227,
        "MMLU_professional_law":0.3044328553,
        "MMLU_professional_medicine":0.3308823529,
        "MMLU_professional_psychology":0.4199346405,
        "MMLU_public_relations":0.4909090909,
        "MMLU_security_studies":0.4653061224,
        "MMLU_sociology":0.5223880597,
        "MMLU_us_foreign_policy":0.65,
        "MMLU_virology":0.3855421687,
        "MMLU_world_religions":0.6725146199
    },
    {
        "Model":"llama-2-13b-mathgpt-v4",
        "URL":"https:\/\/huggingface.co\/rameshm\/llama-2-13b-mathgpt-v4",
        "full_model_name":"rameshm\/llama-2-13b-mathgpt-v4",
        "Parameters":13.0,
        "MMLU_average":0.4378415605,
        "arc:challenge|25":0.4581911263,
        "hellaswag|10":0.5766779526,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.3777777778,
        "MMLU_astronomy":0.4276315789,
        "MMLU_business_ethics":0.44,
        "MMLU_clinical_knowledge":0.4641509434,
        "MMLU_college_biology":0.4375,
        "MMLU_college_chemistry":0.33,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.4046242775,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.5,
        "MMLU_conceptual_physics":0.3872340426,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4896551724,
        "MMLU_elementary_mathematics":0.3042328042,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.26,
        "MMLU_high_school_biology":0.5064516129,
        "MMLU_high_school_chemistry":0.3645320197,
        "MMLU_high_school_computer_science":0.4,
        "MMLU_high_school_european_history":0.4424242424,
        "MMLU_high_school_geography":0.5353535354,
        "MMLU_high_school_government_and_politics":0.6269430052,
        "MMLU_high_school_macroeconomics":0.4051282051,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.4453781513,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.5743119266,
        "MMLU_high_school_statistics":0.4305555556,
        "MMLU_high_school_us_history":0.5245098039,
        "MMLU_high_school_world_history":0.5611814346,
        "MMLU_human_aging":0.5246636771,
        "MMLU_human_sexuality":0.4961832061,
        "MMLU_international_law":0.5950413223,
        "MMLU_jurisprudence":0.5185185185,
        "MMLU_logical_fallacies":0.4601226994,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.5145631068,
        "MMLU_marketing":0.6282051282,
        "MMLU_medical_genetics":0.49,
        "MMLU_miscellaneous":0.5504469987,
        "MMLU_moral_disputes":0.4624277457,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.4869281046,
        "MMLU_philosophy":0.4919614148,
        "MMLU_prehistory":0.4814814815,
        "MMLU_professional_accounting":0.3120567376,
        "MMLU_professional_law":0.3305084746,
        "MMLU_professional_medicine":0.4117647059,
        "MMLU_professional_psychology":0.387254902,
        "MMLU_public_relations":0.5181818182,
        "MMLU_security_studies":0.506122449,
        "MMLU_sociology":0.6069651741,
        "MMLU_us_foreign_policy":0.69,
        "MMLU_virology":0.4036144578,
        "MMLU_world_religions":0.5906432749
    },
    {
        "Model":"galpaca-30b",
        "URL":"https:\/\/huggingface.co\/GeorgiaTechResearchInstitute\/galpaca-30b",
        "full_model_name":"GeorgiaTechResearchInstitute\/galpaca-30b",
        "Parameters":30.0,
        "MMLU_average":0.4378069305,
        "arc:challenge|25":0.4598976109,
        "hellaswag|10":0.4446325433,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.5111111111,
        "MMLU_astronomy":0.4934210526,
        "MMLU_business_ethics":0.43,
        "MMLU_clinical_knowledge":0.4981132075,
        "MMLU_college_biology":0.4930555556,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.4450867052,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.62,
        "MMLU_conceptual_physics":0.4255319149,
        "MMLU_econometrics":0.3684210526,
        "MMLU_electrical_engineering":0.524137931,
        "MMLU_elementary_mathematics":0.3121693122,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.5516129032,
        "MMLU_high_school_chemistry":0.3743842365,
        "MMLU_high_school_computer_science":0.45,
        "MMLU_high_school_european_history":0.496969697,
        "MMLU_high_school_geography":0.5252525253,
        "MMLU_high_school_government_and_politics":0.518134715,
        "MMLU_high_school_macroeconomics":0.4025641026,
        "MMLU_high_school_mathematics":0.2888888889,
        "MMLU_high_school_microeconomics":0.4243697479,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.5853211009,
        "MMLU_high_school_statistics":0.337962963,
        "MMLU_high_school_us_history":0.4117647059,
        "MMLU_high_school_world_history":0.5316455696,
        "MMLU_human_aging":0.5201793722,
        "MMLU_human_sexuality":0.4885496183,
        "MMLU_international_law":0.5537190083,
        "MMLU_jurisprudence":0.4814814815,
        "MMLU_logical_fallacies":0.4539877301,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.4951456311,
        "MMLU_marketing":0.6025641026,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.4763729246,
        "MMLU_moral_disputes":0.4421965318,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.5065359477,
        "MMLU_philosophy":0.4180064309,
        "MMLU_prehistory":0.4320987654,
        "MMLU_professional_accounting":0.3014184397,
        "MMLU_professional_law":0.3213820078,
        "MMLU_professional_medicine":0.4191176471,
        "MMLU_professional_psychology":0.4395424837,
        "MMLU_public_relations":0.4272727273,
        "MMLU_security_studies":0.3836734694,
        "MMLU_sociology":0.5074626866,
        "MMLU_us_foreign_policy":0.56,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.3918128655
    },
    {
        "Model":"open_llama_13b",
        "URL":"https:\/\/huggingface.co\/openlm-research\/open_llama_13b",
        "full_model_name":"openlm-research\/open_llama_13b",
        "Parameters":13.0,
        "MMLU_average":0.4375452834,
        "arc:challenge|25":0.4633105802,
        "hellaswag|10":0.564230233,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.4802631579,
        "MMLU_business_ethics":0.39,
        "MMLU_clinical_knowledge":0.4150943396,
        "MMLU_college_biology":0.4305555556,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.3757225434,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.59,
        "MMLU_conceptual_physics":0.3617021277,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4551724138,
        "MMLU_elementary_mathematics":0.246031746,
        "MMLU_formal_logic":0.2301587302,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.4580645161,
        "MMLU_high_school_chemistry":0.3103448276,
        "MMLU_high_school_computer_science":0.34,
        "MMLU_high_school_european_history":0.4787878788,
        "MMLU_high_school_geography":0.5454545455,
        "MMLU_high_school_government_and_politics":0.6010362694,
        "MMLU_high_school_macroeconomics":0.4487179487,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.4621848739,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.5834862385,
        "MMLU_high_school_statistics":0.2685185185,
        "MMLU_high_school_us_history":0.5637254902,
        "MMLU_high_school_world_history":0.4936708861,
        "MMLU_human_aging":0.4573991031,
        "MMLU_human_sexuality":0.5267175573,
        "MMLU_international_law":0.4958677686,
        "MMLU_jurisprudence":0.4907407407,
        "MMLU_logical_fallacies":0.5766871166,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.5922330097,
        "MMLU_marketing":0.5982905983,
        "MMLU_medical_genetics":0.53,
        "MMLU_miscellaneous":0.595146871,
        "MMLU_moral_disputes":0.5028901734,
        "MMLU_moral_scenarios":0.2681564246,
        "MMLU_nutrition":0.4183006536,
        "MMLU_philosophy":0.501607717,
        "MMLU_prehistory":0.5432098765,
        "MMLU_professional_accounting":0.3226950355,
        "MMLU_professional_law":0.3174706649,
        "MMLU_professional_medicine":0.3860294118,
        "MMLU_professional_psychology":0.3839869281,
        "MMLU_public_relations":0.4727272727,
        "MMLU_security_studies":0.4285714286,
        "MMLU_sociology":0.552238806,
        "MMLU_us_foreign_policy":0.68,
        "MMLU_virology":0.4397590361,
        "MMLU_world_religions":0.6198830409
    },
    {
        "Model":"persimmon-8b-base",
        "URL":"https:\/\/huggingface.co\/adept\/persimmon-8b-base",
        "full_model_name":"adept\/persimmon-8b-base",
        "Parameters":8.0,
        "MMLU_average":0.4362651078,
        "arc:challenge|25":0.4155290102,
        "hellaswag|10":0.5203146784,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.4276315789,
        "MMLU_business_ethics":0.5,
        "MMLU_clinical_knowledge":0.4377358491,
        "MMLU_college_biology":0.5208333333,
        "MMLU_college_chemistry":0.28,
        "MMLU_college_computer_science":0.35,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.3988439306,
        "MMLU_college_physics":0.1764705882,
        "MMLU_computer_security":0.57,
        "MMLU_conceptual_physics":0.3617021277,
        "MMLU_econometrics":0.3421052632,
        "MMLU_electrical_engineering":0.475862069,
        "MMLU_elementary_mathematics":0.2804232804,
        "MMLU_formal_logic":0.4047619048,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.4838709677,
        "MMLU_high_school_chemistry":0.3300492611,
        "MMLU_high_school_computer_science":0.45,
        "MMLU_high_school_european_history":0.5696969697,
        "MMLU_high_school_geography":0.5050505051,
        "MMLU_high_school_government_and_politics":0.518134715,
        "MMLU_high_school_macroeconomics":0.3948717949,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.3991596639,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.5321100917,
        "MMLU_high_school_statistics":0.2824074074,
        "MMLU_high_school_us_history":0.5882352941,
        "MMLU_high_school_world_history":0.5569620253,
        "MMLU_human_aging":0.4215246637,
        "MMLU_human_sexuality":0.5419847328,
        "MMLU_international_law":0.5289256198,
        "MMLU_jurisprudence":0.4259259259,
        "MMLU_logical_fallacies":0.5889570552,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.4951456311,
        "MMLU_marketing":0.6367521368,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.5466155811,
        "MMLU_moral_disputes":0.4653179191,
        "MMLU_moral_scenarios":0.2402234637,
        "MMLU_nutrition":0.4869281046,
        "MMLU_philosophy":0.4758842444,
        "MMLU_prehistory":0.4660493827,
        "MMLU_professional_accounting":0.3581560284,
        "MMLU_professional_law":0.3344198175,
        "MMLU_professional_medicine":0.3970588235,
        "MMLU_professional_psychology":0.385620915,
        "MMLU_public_relations":0.5181818182,
        "MMLU_security_studies":0.4040816327,
        "MMLU_sociology":0.5721393035,
        "MMLU_us_foreign_policy":0.58,
        "MMLU_virology":0.4397590361,
        "MMLU_world_religions":0.5964912281
    },
    {
        "Model":"Asclepius-Llama2-7B",
        "URL":"https:\/\/huggingface.co\/starmpcc\/Asclepius-Llama2-7B",
        "full_model_name":"starmpcc\/Asclepius-Llama2-7B",
        "Parameters":7.0,
        "MMLU_average":0.436129554,
        "arc:challenge|25":0.4752559727,
        "hellaswag|10":0.5856403107,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.3555555556,
        "MMLU_astronomy":0.3289473684,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.4716981132,
        "MMLU_college_biology":0.4305555556,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.45,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.2947976879,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.52,
        "MMLU_conceptual_physics":0.370212766,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.3015873016,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.28,
        "MMLU_high_school_biology":0.464516129,
        "MMLU_high_school_chemistry":0.3054187192,
        "MMLU_high_school_computer_science":0.44,
        "MMLU_high_school_european_history":0.5878787879,
        "MMLU_high_school_geography":0.4545454545,
        "MMLU_high_school_government_and_politics":0.5751295337,
        "MMLU_high_school_macroeconomics":0.4230769231,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.3949579832,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.5633027523,
        "MMLU_high_school_statistics":0.2731481481,
        "MMLU_high_school_us_history":0.5,
        "MMLU_high_school_world_history":0.5611814346,
        "MMLU_human_aging":0.4843049327,
        "MMLU_human_sexuality":0.4961832061,
        "MMLU_international_law":0.6363636364,
        "MMLU_jurisprudence":0.4537037037,
        "MMLU_logical_fallacies":0.4846625767,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.5048543689,
        "MMLU_marketing":0.6367521368,
        "MMLU_medical_genetics":0.56,
        "MMLU_miscellaneous":0.5964240102,
        "MMLU_moral_disputes":0.4768786127,
        "MMLU_moral_scenarios":0.2815642458,
        "MMLU_nutrition":0.4346405229,
        "MMLU_philosophy":0.5434083601,
        "MMLU_prehistory":0.4228395062,
        "MMLU_professional_accounting":0.3475177305,
        "MMLU_professional_law":0.3428943937,
        "MMLU_professional_medicine":0.4375,
        "MMLU_professional_psychology":0.4166666667,
        "MMLU_public_relations":0.4909090909,
        "MMLU_security_studies":0.3795918367,
        "MMLU_sociology":0.6169154229,
        "MMLU_us_foreign_policy":0.6,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.6374269006
    },
    {
        "Model":"Pygmalion-Vicuna-1.1-7b",
        "URL":"https:\/\/huggingface.co\/TehVenom\/Pygmalion-Vicuna-1.1-7b",
        "full_model_name":"TehVenom\/Pygmalion-Vicuna-1.1-7b",
        "Parameters":7.0,
        "MMLU_average":0.4361128217,
        "arc:challenge|25":0.4957337884,
        "hellaswag|10":0.5923122884,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4148148148,
        "MMLU_astronomy":0.4276315789,
        "MMLU_business_ethics":0.47,
        "MMLU_clinical_knowledge":0.5018867925,
        "MMLU_college_biology":0.4444444444,
        "MMLU_college_chemistry":0.38,
        "MMLU_college_computer_science":0.35,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.3468208092,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.51,
        "MMLU_conceptual_physics":0.3957446809,
        "MMLU_econometrics":0.2192982456,
        "MMLU_electrical_engineering":0.4482758621,
        "MMLU_elementary_mathematics":0.2698412698,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.27,
        "MMLU_high_school_biology":0.4258064516,
        "MMLU_high_school_chemistry":0.315270936,
        "MMLU_high_school_computer_science":0.39,
        "MMLU_high_school_european_history":0.5636363636,
        "MMLU_high_school_geography":0.5101010101,
        "MMLU_high_school_government_and_politics":0.5958549223,
        "MMLU_high_school_macroeconomics":0.3794871795,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.3487394958,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.5669724771,
        "MMLU_high_school_statistics":0.3842592593,
        "MMLU_high_school_us_history":0.5294117647,
        "MMLU_high_school_world_history":0.5991561181,
        "MMLU_human_aging":0.4977578475,
        "MMLU_human_sexuality":0.5038167939,
        "MMLU_international_law":0.5950413223,
        "MMLU_jurisprudence":0.5277777778,
        "MMLU_logical_fallacies":0.472392638,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.5922330097,
        "MMLU_marketing":0.6581196581,
        "MMLU_medical_genetics":0.49,
        "MMLU_miscellaneous":0.5977011494,
        "MMLU_moral_disputes":0.5144508671,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.4803921569,
        "MMLU_philosophy":0.4887459807,
        "MMLU_prehistory":0.4969135802,
        "MMLU_professional_accounting":0.3262411348,
        "MMLU_professional_law":0.3402868318,
        "MMLU_professional_medicine":0.4742647059,
        "MMLU_professional_psychology":0.4117647059,
        "MMLU_public_relations":0.5,
        "MMLU_security_studies":0.4816326531,
        "MMLU_sociology":0.592039801,
        "MMLU_us_foreign_policy":0.59,
        "MMLU_virology":0.3795180723,
        "MMLU_world_religions":0.6081871345
    },
    {
        "Model":"llama-2-7b-hf_open-platypus",
        "URL":"https:\/\/huggingface.co\/lgaalves\/llama-2-7b-hf_open-platypus",
        "full_model_name":"lgaalves\/llama-2-7b-hf_open-platypus",
        "Parameters":7.0,
        "MMLU_average":0.4360057754,
        "arc:challenge|25":0.4829351536,
        "hellaswag|10":0.5880302729,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.3684210526,
        "MMLU_business_ethics":0.45,
        "MMLU_clinical_knowledge":0.4,
        "MMLU_college_biology":0.4097222222,
        "MMLU_college_chemistry":0.22,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.3641618497,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.57,
        "MMLU_conceptual_physics":0.3957446809,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4137931034,
        "MMLU_elementary_mathematics":0.2486772487,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.4322580645,
        "MMLU_high_school_chemistry":0.2955665025,
        "MMLU_high_school_computer_science":0.44,
        "MMLU_high_school_european_history":0.5696969697,
        "MMLU_high_school_geography":0.4696969697,
        "MMLU_high_school_government_and_politics":0.6424870466,
        "MMLU_high_school_macroeconomics":0.3794871795,
        "MMLU_high_school_mathematics":0.2259259259,
        "MMLU_high_school_microeconomics":0.4117647059,
        "MMLU_high_school_physics":0.2251655629,
        "MMLU_high_school_psychology":0.5633027523,
        "MMLU_high_school_statistics":0.2638888889,
        "MMLU_high_school_us_history":0.4656862745,
        "MMLU_high_school_world_history":0.5232067511,
        "MMLU_human_aging":0.5112107623,
        "MMLU_human_sexuality":0.4885496183,
        "MMLU_international_law":0.6446280992,
        "MMLU_jurisprudence":0.4814814815,
        "MMLU_logical_fallacies":0.4785276074,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.5339805825,
        "MMLU_marketing":0.7051282051,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.6245210728,
        "MMLU_moral_disputes":0.4479768786,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.4705882353,
        "MMLU_philosophy":0.5434083601,
        "MMLU_prehistory":0.475308642,
        "MMLU_professional_accounting":0.3546099291,
        "MMLU_professional_law":0.3428943937,
        "MMLU_professional_medicine":0.4816176471,
        "MMLU_professional_psychology":0.4346405229,
        "MMLU_public_relations":0.4909090909,
        "MMLU_security_studies":0.4653061224,
        "MMLU_sociology":0.6069651741,
        "MMLU_us_foreign_policy":0.65,
        "MMLU_virology":0.4096385542,
        "MMLU_world_religions":0.6608187135
    },
    {
        "Model":"Baichuan-7B",
        "URL":"https:\/\/huggingface.co\/baichuan-inc\/Baichuan-7B",
        "full_model_name":"baichuan-inc\/Baichuan-7B",
        "Parameters":7.0,
        "MMLU_average":0.435884605,
        "arc:challenge|25":0.3924914676,
        "hellaswag|10":0.5197171878,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4222222222,
        "MMLU_astronomy":0.5065789474,
        "MMLU_business_ethics":0.5,
        "MMLU_clinical_knowledge":0.441509434,
        "MMLU_college_biology":0.4166666667,
        "MMLU_college_chemistry":0.29,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.4219653179,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.63,
        "MMLU_conceptual_physics":0.3276595745,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.4965517241,
        "MMLU_elementary_mathematics":0.2830687831,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.4516129032,
        "MMLU_high_school_chemistry":0.3645320197,
        "MMLU_high_school_computer_science":0.41,
        "MMLU_high_school_european_history":0.4242424242,
        "MMLU_high_school_geography":0.5505050505,
        "MMLU_high_school_government_and_politics":0.6062176166,
        "MMLU_high_school_macroeconomics":0.3641025641,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.3949579832,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.5853211009,
        "MMLU_high_school_statistics":0.2731481481,
        "MMLU_high_school_us_history":0.4950980392,
        "MMLU_high_school_world_history":0.5274261603,
        "MMLU_human_aging":0.4618834081,
        "MMLU_human_sexuality":0.5496183206,
        "MMLU_international_law":0.6033057851,
        "MMLU_jurisprudence":0.5092592593,
        "MMLU_logical_fallacies":0.490797546,
        "MMLU_machine_learning":0.4196428571,
        "MMLU_management":0.572815534,
        "MMLU_marketing":0.6111111111,
        "MMLU_medical_genetics":0.48,
        "MMLU_miscellaneous":0.6053639847,
        "MMLU_moral_disputes":0.4479768786,
        "MMLU_moral_scenarios":0.2491620112,
        "MMLU_nutrition":0.4477124183,
        "MMLU_philosophy":0.459807074,
        "MMLU_prehistory":0.5154320988,
        "MMLU_professional_accounting":0.3617021277,
        "MMLU_professional_law":0.3265971317,
        "MMLU_professional_medicine":0.3455882353,
        "MMLU_professional_psychology":0.4330065359,
        "MMLU_public_relations":0.5272727273,
        "MMLU_security_studies":0.4857142857,
        "MMLU_sociology":0.5771144279,
        "MMLU_us_foreign_policy":0.58,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.6315789474
    },
    {
        "Model":"LLaMA-2-7B-32K",
        "URL":"https:\/\/huggingface.co\/togethercomputer\/LLaMA-2-7B-32K",
        "full_model_name":"togethercomputer\/LLaMA-2-7B-32K",
        "Parameters":7.0,
        "MMLU_average":0.4332570702,
        "arc:challenge|25":0.4343003413,
        "hellaswag|10":0.5612427803,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.4444444444,
        "MMLU_astronomy":0.4473684211,
        "MMLU_business_ethics":0.47,
        "MMLU_clinical_knowledge":0.4490566038,
        "MMLU_college_biology":0.4027777778,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.44,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.3352601156,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.55,
        "MMLU_conceptual_physics":0.3659574468,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.4068965517,
        "MMLU_elementary_mathematics":0.3042328042,
        "MMLU_formal_logic":0.1904761905,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.4935483871,
        "MMLU_high_school_chemistry":0.315270936,
        "MMLU_high_school_computer_science":0.44,
        "MMLU_high_school_european_history":0.5878787879,
        "MMLU_high_school_geography":0.5101010101,
        "MMLU_high_school_government_and_politics":0.6476683938,
        "MMLU_high_school_macroeconomics":0.3846153846,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.3613445378,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.5981651376,
        "MMLU_high_school_statistics":0.3981481481,
        "MMLU_high_school_us_history":0.5931372549,
        "MMLU_high_school_world_history":0.6455696203,
        "MMLU_human_aging":0.4573991031,
        "MMLU_human_sexuality":0.3969465649,
        "MMLU_international_law":0.5785123967,
        "MMLU_jurisprudence":0.4722222222,
        "MMLU_logical_fallacies":0.4539877301,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.4757281553,
        "MMLU_marketing":0.641025641,
        "MMLU_medical_genetics":0.54,
        "MMLU_miscellaneous":0.6206896552,
        "MMLU_moral_disputes":0.4219653179,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.4346405229,
        "MMLU_philosophy":0.5305466238,
        "MMLU_prehistory":0.4691358025,
        "MMLU_professional_accounting":0.3368794326,
        "MMLU_professional_law":0.3487614081,
        "MMLU_professional_medicine":0.4411764706,
        "MMLU_professional_psychology":0.3888888889,
        "MMLU_public_relations":0.5181818182,
        "MMLU_security_studies":0.4163265306,
        "MMLU_sociology":0.5472636816,
        "MMLU_us_foreign_policy":0.61,
        "MMLU_virology":0.3373493976,
        "MMLU_world_religions":0.6140350877
    },
    {
        "Model":"speechless-codellama-airoboros-orca-platypus-13b",
        "URL":"https:\/\/huggingface.co\/speechlessai\/speechless-codellama-airoboros-orca-platypus-13b",
        "full_model_name":"speechlessai\/speechless-codellama-airoboros-orca-platypus-13b",
        "Parameters":13.0,
        "MMLU_average":0.4316034424,
        "arc:challenge|25":0.4104095563,
        "hellaswag|10":0.4919338777,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.3555555556,
        "MMLU_astronomy":0.4407894737,
        "MMLU_business_ethics":0.43,
        "MMLU_clinical_knowledge":0.4528301887,
        "MMLU_college_biology":0.3958333333,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.37,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.3757225434,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.52,
        "MMLU_conceptual_physics":0.4170212766,
        "MMLU_econometrics":0.3421052632,
        "MMLU_electrical_engineering":0.4482758621,
        "MMLU_elementary_mathematics":0.3306878307,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.4806451613,
        "MMLU_high_school_chemistry":0.3054187192,
        "MMLU_high_school_computer_science":0.54,
        "MMLU_high_school_european_history":0.5636363636,
        "MMLU_high_school_geography":0.5,
        "MMLU_high_school_government_and_politics":0.5388601036,
        "MMLU_high_school_macroeconomics":0.3564102564,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.4033613445,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.5467889908,
        "MMLU_high_school_statistics":0.3287037037,
        "MMLU_high_school_us_history":0.5539215686,
        "MMLU_high_school_world_history":0.5654008439,
        "MMLU_human_aging":0.466367713,
        "MMLU_human_sexuality":0.3969465649,
        "MMLU_international_law":0.5950413223,
        "MMLU_jurisprudence":0.462962963,
        "MMLU_logical_fallacies":0.509202454,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.6310679612,
        "MMLU_marketing":0.6965811966,
        "MMLU_medical_genetics":0.43,
        "MMLU_miscellaneous":0.5312899106,
        "MMLU_moral_disputes":0.4624277457,
        "MMLU_moral_scenarios":0.2603351955,
        "MMLU_nutrition":0.4019607843,
        "MMLU_philosophy":0.4758842444,
        "MMLU_prehistory":0.4413580247,
        "MMLU_professional_accounting":0.3794326241,
        "MMLU_professional_law":0.3031290743,
        "MMLU_professional_medicine":0.3014705882,
        "MMLU_professional_psychology":0.3921568627,
        "MMLU_public_relations":0.5363636364,
        "MMLU_security_studies":0.5714285714,
        "MMLU_sociology":0.4825870647,
        "MMLU_us_foreign_policy":0.64,
        "MMLU_virology":0.4036144578,
        "MMLU_world_religions":0.5438596491
    },
    {
        "Model":"gowizardlm",
        "URL":"https:\/\/huggingface.co\/golaxy\/gowizardlm",
        "full_model_name":"golaxy\/gowizardlm",
        "Parameters":null,
        "MMLU_average":0.4295715468,
        "arc:challenge|25":0.4479522184,
        "hellaswag|10":0.5437163912,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.437037037,
        "MMLU_astronomy":0.3881578947,
        "MMLU_business_ethics":0.46,
        "MMLU_clinical_knowledge":0.4528301887,
        "MMLU_college_biology":0.4166666667,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.3410404624,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.52,
        "MMLU_conceptual_physics":0.4127659574,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.3862068966,
        "MMLU_elementary_mathematics":0.2777777778,
        "MMLU_formal_logic":0.2777777778,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.4193548387,
        "MMLU_high_school_chemistry":0.2709359606,
        "MMLU_high_school_computer_science":0.43,
        "MMLU_high_school_european_history":0.5333333333,
        "MMLU_high_school_geography":0.5454545455,
        "MMLU_high_school_government_and_politics":0.5647668394,
        "MMLU_high_school_macroeconomics":0.3871794872,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.3781512605,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.5412844037,
        "MMLU_high_school_statistics":0.2407407407,
        "MMLU_high_school_us_history":0.5392156863,
        "MMLU_high_school_world_history":0.582278481,
        "MMLU_human_aging":0.5067264574,
        "MMLU_human_sexuality":0.4809160305,
        "MMLU_international_law":0.5950413223,
        "MMLU_jurisprudence":0.4722222222,
        "MMLU_logical_fallacies":0.4785276074,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.5145631068,
        "MMLU_marketing":0.7179487179,
        "MMLU_medical_genetics":0.45,
        "MMLU_miscellaneous":0.5798212005,
        "MMLU_moral_disputes":0.4913294798,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.4738562092,
        "MMLU_philosophy":0.5466237942,
        "MMLU_prehistory":0.4814814815,
        "MMLU_professional_accounting":0.365248227,
        "MMLU_professional_law":0.3129074316,
        "MMLU_professional_medicine":0.3382352941,
        "MMLU_professional_psychology":0.4297385621,
        "MMLU_public_relations":0.5727272727,
        "MMLU_security_studies":0.4448979592,
        "MMLU_sociology":0.5870646766,
        "MMLU_us_foreign_policy":0.58,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.6432748538
    },
    {
        "Model":"leo-hessianai-7b",
        "URL":"https:\/\/huggingface.co\/LeoLM\/leo-hessianai-7b",
        "full_model_name":"LeoLM\/leo-hessianai-7b",
        "Parameters":7.0,
        "MMLU_average":0.4285387777,
        "arc:challenge|25":0.4752559727,
        "hellaswag|10":0.5645289783,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.4518518519,
        "MMLU_astronomy":0.4407894737,
        "MMLU_business_ethics":0.5,
        "MMLU_clinical_knowledge":0.441509434,
        "MMLU_college_biology":0.4097222222,
        "MMLU_college_chemistry":0.26,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.387283237,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.56,
        "MMLU_conceptual_physics":0.3957446809,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.4344827586,
        "MMLU_elementary_mathematics":0.291005291,
        "MMLU_formal_logic":0.2777777778,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.4419354839,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.43,
        "MMLU_high_school_european_history":0.5575757576,
        "MMLU_high_school_geography":0.4696969697,
        "MMLU_high_school_government_and_politics":0.5906735751,
        "MMLU_high_school_macroeconomics":0.4051282051,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.3403361345,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.5724770642,
        "MMLU_high_school_statistics":0.25,
        "MMLU_high_school_us_history":0.5392156863,
        "MMLU_high_school_world_history":0.5611814346,
        "MMLU_human_aging":0.4349775785,
        "MMLU_human_sexuality":0.4503816794,
        "MMLU_international_law":0.5785123967,
        "MMLU_jurisprudence":0.4537037037,
        "MMLU_logical_fallacies":0.4294478528,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.5048543689,
        "MMLU_marketing":0.5726495726,
        "MMLU_medical_genetics":0.53,
        "MMLU_miscellaneous":0.5823754789,
        "MMLU_moral_disputes":0.4132947977,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.454248366,
        "MMLU_philosophy":0.5273311897,
        "MMLU_prehistory":0.475308642,
        "MMLU_professional_accounting":0.3439716312,
        "MMLU_professional_law":0.316166884,
        "MMLU_professional_medicine":0.4852941176,
        "MMLU_professional_psychology":0.4215686275,
        "MMLU_public_relations":0.4636363636,
        "MMLU_security_studies":0.506122449,
        "MMLU_sociology":0.5771144279,
        "MMLU_us_foreign_policy":0.67,
        "MMLU_virology":0.3313253012,
        "MMLU_world_religions":0.6257309942
    },
    {
        "Model":"speechless-codellama-platypus-13b",
        "URL":"https:\/\/huggingface.co\/uukuguy\/speechless-codellama-platypus-13b",
        "full_model_name":"uukuguy\/speechless-codellama-platypus-13b",
        "Parameters":13.0,
        "MMLU_average":0.4281676598,
        "arc:challenge|25":0.4146757679,
        "hellaswag|10":0.5031866162,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.3481481481,
        "MMLU_astronomy":0.375,
        "MMLU_business_ethics":0.46,
        "MMLU_clinical_knowledge":0.3773584906,
        "MMLU_college_biology":0.4236111111,
        "MMLU_college_chemistry":0.25,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.3583815029,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.66,
        "MMLU_conceptual_physics":0.3787234043,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.3724137931,
        "MMLU_elementary_mathematics":0.2724867725,
        "MMLU_formal_logic":0.3333333333,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.435483871,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.52,
        "MMLU_high_school_european_history":0.5939393939,
        "MMLU_high_school_geography":0.4545454545,
        "MMLU_high_school_government_and_politics":0.5647668394,
        "MMLU_high_school_macroeconomics":0.3358974359,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.3613445378,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.4917431193,
        "MMLU_high_school_statistics":0.287037037,
        "MMLU_high_school_us_history":0.6274509804,
        "MMLU_high_school_world_history":0.6540084388,
        "MMLU_human_aging":0.4753363229,
        "MMLU_human_sexuality":0.4580152672,
        "MMLU_international_law":0.6446280992,
        "MMLU_jurisprudence":0.5277777778,
        "MMLU_logical_fallacies":0.4539877301,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.6019417476,
        "MMLU_marketing":0.6581196581,
        "MMLU_medical_genetics":0.4,
        "MMLU_miscellaneous":0.558109834,
        "MMLU_moral_disputes":0.4797687861,
        "MMLU_moral_scenarios":0.3061452514,
        "MMLU_nutrition":0.3758169935,
        "MMLU_philosophy":0.5112540193,
        "MMLU_prehistory":0.4537037037,
        "MMLU_professional_accounting":0.3191489362,
        "MMLU_professional_law":0.3533246415,
        "MMLU_professional_medicine":0.2757352941,
        "MMLU_professional_psychology":0.4035947712,
        "MMLU_public_relations":0.5454545455,
        "MMLU_security_studies":0.4897959184,
        "MMLU_sociology":0.5223880597,
        "MMLU_us_foreign_policy":0.61,
        "MMLU_virology":0.3554216867,
        "MMLU_world_religions":0.5204678363
    },
    {
        "Model":"opencoderplus",
        "URL":"https:\/\/huggingface.co\/openchat\/opencoderplus",
        "full_model_name":"openchat\/opencoderplus",
        "Parameters":null,
        "MMLU_average":0.4272655164,
        "arc:challenge|25":0.4812286689,
        "hellaswag|10":0.6070503884,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.4962962963,
        "MMLU_astronomy":0.4342105263,
        "MMLU_business_ethics":0.42,
        "MMLU_clinical_knowledge":0.4679245283,
        "MMLU_college_biology":0.4513888889,
        "MMLU_college_chemistry":0.28,
        "MMLU_college_computer_science":0.42,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.3930635838,
        "MMLU_college_physics":0.1078431373,
        "MMLU_computer_security":0.57,
        "MMLU_conceptual_physics":0.3531914894,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4344827586,
        "MMLU_elementary_mathematics":0.3174603175,
        "MMLU_formal_logic":0.3015873016,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.4967741935,
        "MMLU_high_school_chemistry":0.2906403941,
        "MMLU_high_school_computer_science":0.46,
        "MMLU_high_school_european_history":0.5151515152,
        "MMLU_high_school_geography":0.5,
        "MMLU_high_school_government_and_politics":0.5647668394,
        "MMLU_high_school_macroeconomics":0.3897435897,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.3319327731,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.5412844037,
        "MMLU_high_school_statistics":0.25,
        "MMLU_high_school_us_history":0.4901960784,
        "MMLU_high_school_world_history":0.5949367089,
        "MMLU_human_aging":0.4798206278,
        "MMLU_human_sexuality":0.5496183206,
        "MMLU_international_law":0.4958677686,
        "MMLU_jurisprudence":0.4444444444,
        "MMLU_logical_fallacies":0.4662576687,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.5242718447,
        "MMLU_marketing":0.6452991453,
        "MMLU_medical_genetics":0.47,
        "MMLU_miscellaneous":0.5530012771,
        "MMLU_moral_disputes":0.4335260116,
        "MMLU_moral_scenarios":0.2368715084,
        "MMLU_nutrition":0.4705882353,
        "MMLU_philosophy":0.4405144695,
        "MMLU_prehistory":0.4259259259,
        "MMLU_professional_accounting":0.3333333333,
        "MMLU_professional_law":0.3305084746,
        "MMLU_professional_medicine":0.3345588235,
        "MMLU_professional_psychology":0.387254902,
        "MMLU_public_relations":0.5,
        "MMLU_security_studies":0.5265306122,
        "MMLU_sociology":0.592039801,
        "MMLU_us_foreign_policy":0.62,
        "MMLU_virology":0.4939759036,
        "MMLU_world_religions":0.5730994152
    },
    {
        "Model":"llama-13b-4bit-alpaca",
        "URL":"https:\/\/huggingface.co\/TFLai\/llama-13b-4bit-alpaca",
        "full_model_name":"TFLai\/llama-13b-4bit-alpaca",
        "Parameters":13.0,
        "MMLU_average":0.4241692415,
        "arc:challenge|25":0.5349829352,
        "hellaswag|10":0.6217884883,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.4518518519,
        "MMLU_astronomy":0.4473684211,
        "MMLU_business_ethics":0.4,
        "MMLU_clinical_knowledge":0.4188679245,
        "MMLU_college_biology":0.4722222222,
        "MMLU_college_chemistry":0.25,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.3526011561,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.59,
        "MMLU_conceptual_physics":0.3829787234,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.4068965517,
        "MMLU_elementary_mathematics":0.2275132275,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.25,
        "MMLU_high_school_biology":0.4419354839,
        "MMLU_high_school_chemistry":0.2709359606,
        "MMLU_high_school_computer_science":0.42,
        "MMLU_high_school_european_history":0.4484848485,
        "MMLU_high_school_geography":0.4747474747,
        "MMLU_high_school_government_and_politics":0.6113989637,
        "MMLU_high_school_macroeconomics":0.3794871795,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.4117647059,
        "MMLU_high_school_physics":0.238410596,
        "MMLU_high_school_psychology":0.5559633028,
        "MMLU_high_school_statistics":0.2268518519,
        "MMLU_high_school_us_history":0.431372549,
        "MMLU_high_school_world_history":0.4725738397,
        "MMLU_human_aging":0.4887892377,
        "MMLU_human_sexuality":0.4198473282,
        "MMLU_international_law":0.6033057851,
        "MMLU_jurisprudence":0.4907407407,
        "MMLU_logical_fallacies":0.4417177914,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.6116504854,
        "MMLU_marketing":0.6239316239,
        "MMLU_medical_genetics":0.5,
        "MMLU_miscellaneous":0.6104725415,
        "MMLU_moral_disputes":0.4393063584,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.4607843137,
        "MMLU_philosophy":0.4726688103,
        "MMLU_prehistory":0.4814814815,
        "MMLU_professional_accounting":0.2943262411,
        "MMLU_professional_law":0.3102998696,
        "MMLU_professional_medicine":0.4117647059,
        "MMLU_professional_psychology":0.4428104575,
        "MMLU_public_relations":0.5454545455,
        "MMLU_security_studies":0.4857142857,
        "MMLU_sociology":0.6417910448,
        "MMLU_us_foreign_policy":0.7,
        "MMLU_virology":0.3795180723,
        "MMLU_world_religions":0.6315789474
    },
    {
        "Model":"open_llama_7b_v2_med_instruct",
        "URL":"https:\/\/huggingface.co\/yhyhy3\/open_llama_7b_v2_med_instruct",
        "full_model_name":"yhyhy3\/open_llama_7b_v2_med_instruct",
        "Parameters":7.0,
        "MMLU_average":0.4232433102,
        "arc:challenge|25":0.4436860068,
        "hellaswag|10":0.5783708425,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.4473684211,
        "MMLU_business_ethics":0.4,
        "MMLU_clinical_knowledge":0.4905660377,
        "MMLU_college_biology":0.3958333333,
        "MMLU_college_chemistry":0.28,
        "MMLU_college_computer_science":0.34,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.3699421965,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.6,
        "MMLU_conceptual_physics":0.3829787234,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.4275862069,
        "MMLU_elementary_mathematics":0.2989417989,
        "MMLU_formal_logic":0.2619047619,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.4483870968,
        "MMLU_high_school_chemistry":0.2955665025,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.4727272727,
        "MMLU_high_school_geography":0.5,
        "MMLU_high_school_government_and_politics":0.5751295337,
        "MMLU_high_school_macroeconomics":0.3948717949,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.3949579832,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.5651376147,
        "MMLU_high_school_statistics":0.2916666667,
        "MMLU_high_school_us_history":0.4460784314,
        "MMLU_high_school_world_history":0.5443037975,
        "MMLU_human_aging":0.5156950673,
        "MMLU_human_sexuality":0.5114503817,
        "MMLU_international_law":0.5619834711,
        "MMLU_jurisprudence":0.4907407407,
        "MMLU_logical_fallacies":0.490797546,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.6019417476,
        "MMLU_marketing":0.6581196581,
        "MMLU_medical_genetics":0.48,
        "MMLU_miscellaneous":0.6130268199,
        "MMLU_moral_disputes":0.4768786127,
        "MMLU_moral_scenarios":0.2703910615,
        "MMLU_nutrition":0.4379084967,
        "MMLU_philosophy":0.463022508,
        "MMLU_prehistory":0.4104938272,
        "MMLU_professional_accounting":0.3226950355,
        "MMLU_professional_law":0.3233376793,
        "MMLU_professional_medicine":0.4007352941,
        "MMLU_professional_psychology":0.4003267974,
        "MMLU_public_relations":0.5,
        "MMLU_security_studies":0.3714285714,
        "MMLU_sociology":0.552238806,
        "MMLU_us_foreign_policy":0.6,
        "MMLU_virology":0.4036144578,
        "MMLU_world_religions":0.5614035088
    },
    {
        "Model":"kollama2-7b",
        "URL":"https:\/\/huggingface.co\/psyche\/kollama2-7b",
        "full_model_name":"psyche\/kollama2-7b",
        "Parameters":7.0,
        "MMLU_average":0.4231366921,
        "arc:challenge|25":0.4889078498,
        "hellaswag|10":0.5910177256,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4296296296,
        "MMLU_astronomy":0.3815789474,
        "MMLU_business_ethics":0.52,
        "MMLU_clinical_knowledge":0.4188679245,
        "MMLU_college_biology":0.3888888889,
        "MMLU_college_chemistry":0.3,
        "MMLU_college_computer_science":0.4,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.3583815029,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.49,
        "MMLU_conceptual_physics":0.4,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.4137931034,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.435483871,
        "MMLU_high_school_chemistry":0.3399014778,
        "MMLU_high_school_computer_science":0.37,
        "MMLU_high_school_european_history":0.5393939394,
        "MMLU_high_school_geography":0.4444444444,
        "MMLU_high_school_government_and_politics":0.585492228,
        "MMLU_high_school_macroeconomics":0.3615384615,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.3865546218,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.5211009174,
        "MMLU_high_school_statistics":0.2268518519,
        "MMLU_high_school_us_history":0.5,
        "MMLU_high_school_world_history":0.5358649789,
        "MMLU_human_aging":0.4977578475,
        "MMLU_human_sexuality":0.4503816794,
        "MMLU_international_law":0.6363636364,
        "MMLU_jurisprudence":0.5,
        "MMLU_logical_fallacies":0.4233128834,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.4660194175,
        "MMLU_marketing":0.6239316239,
        "MMLU_medical_genetics":0.47,
        "MMLU_miscellaneous":0.5989782886,
        "MMLU_moral_disputes":0.4537572254,
        "MMLU_moral_scenarios":0.2558659218,
        "MMLU_nutrition":0.4183006536,
        "MMLU_philosophy":0.5080385852,
        "MMLU_prehistory":0.450617284,
        "MMLU_professional_accounting":0.3581560284,
        "MMLU_professional_law":0.3298565841,
        "MMLU_professional_medicine":0.4558823529,
        "MMLU_professional_psychology":0.3970588235,
        "MMLU_public_relations":0.5181818182,
        "MMLU_security_studies":0.4367346939,
        "MMLU_sociology":0.5323383085,
        "MMLU_us_foreign_policy":0.64,
        "MMLU_virology":0.3795180723,
        "MMLU_world_religions":0.6140350877
    },
    {
        "Model":"airoboros-7b-gpt4-1.3",
        "URL":"https:\/\/huggingface.co\/jondurbin\/airoboros-7b-gpt4-1.3",
        "full_model_name":"jondurbin\/airoboros-7b-gpt4-1.3",
        "Parameters":7.0,
        "MMLU_average":0.4196844322,
        "arc:challenge|25":0.4889078498,
        "hellaswag|10":0.5921131249,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.4740740741,
        "MMLU_astronomy":0.3881578947,
        "MMLU_business_ethics":0.46,
        "MMLU_clinical_knowledge":0.4150943396,
        "MMLU_college_biology":0.4375,
        "MMLU_college_chemistry":0.28,
        "MMLU_college_computer_science":0.37,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.3005780347,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.51,
        "MMLU_conceptual_physics":0.3404255319,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.3586206897,
        "MMLU_elementary_mathematics":0.2645502646,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.4387096774,
        "MMLU_high_school_chemistry":0.2709359606,
        "MMLU_high_school_computer_science":0.39,
        "MMLU_high_school_european_history":0.5515151515,
        "MMLU_high_school_geography":0.5202020202,
        "MMLU_high_school_government_and_politics":0.6113989637,
        "MMLU_high_school_macroeconomics":0.3692307692,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.3151260504,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.5816513761,
        "MMLU_high_school_statistics":0.2731481481,
        "MMLU_high_school_us_history":0.5049019608,
        "MMLU_high_school_world_history":0.5991561181,
        "MMLU_human_aging":0.4753363229,
        "MMLU_human_sexuality":0.4351145038,
        "MMLU_international_law":0.6033057851,
        "MMLU_jurisprudence":0.5277777778,
        "MMLU_logical_fallacies":0.509202454,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.5436893204,
        "MMLU_marketing":0.6324786325,
        "MMLU_medical_genetics":0.55,
        "MMLU_miscellaneous":0.6117496807,
        "MMLU_moral_disputes":0.4537572254,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.3986928105,
        "MMLU_philosophy":0.4694533762,
        "MMLU_prehistory":0.524691358,
        "MMLU_professional_accounting":0.2872340426,
        "MMLU_professional_law":0.3422425033,
        "MMLU_professional_medicine":0.4558823529,
        "MMLU_professional_psychology":0.4297385621,
        "MMLU_public_relations":0.5363636364,
        "MMLU_security_studies":0.3632653061,
        "MMLU_sociology":0.5422885572,
        "MMLU_us_foreign_policy":0.61,
        "MMLU_virology":0.3734939759,
        "MMLU_world_religions":0.6081871345
    },
    {
        "Model":"guanaco-unchained-llama-2-7b",
        "URL":"https:\/\/huggingface.co\/jlevin\/guanaco-unchained-llama-2-7b",
        "full_model_name":"jlevin\/guanaco-unchained-llama-2-7b",
        "Parameters":7.0,
        "MMLU_average":0.4175890314,
        "arc:challenge|25":0.4496587031,
        "hellaswag|10":0.5437163912,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.437037037,
        "MMLU_astronomy":0.3881578947,
        "MMLU_business_ethics":0.5,
        "MMLU_clinical_knowledge":0.3886792453,
        "MMLU_college_biology":0.4305555556,
        "MMLU_college_chemistry":0.28,
        "MMLU_college_computer_science":0.38,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.3641618497,
        "MMLU_college_physics":0.1764705882,
        "MMLU_computer_security":0.55,
        "MMLU_conceptual_physics":0.3744680851,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.4482758621,
        "MMLU_elementary_mathematics":0.2962962963,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.25,
        "MMLU_high_school_biology":0.4129032258,
        "MMLU_high_school_chemistry":0.3399014778,
        "MMLU_high_school_computer_science":0.38,
        "MMLU_high_school_european_history":0.5818181818,
        "MMLU_high_school_geography":0.4898989899,
        "MMLU_high_school_government_and_politics":0.6062176166,
        "MMLU_high_school_macroeconomics":0.3666666667,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.3361344538,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.5596330275,
        "MMLU_high_school_statistics":0.2314814815,
        "MMLU_high_school_us_history":0.4460784314,
        "MMLU_high_school_world_history":0.5274261603,
        "MMLU_human_aging":0.4484304933,
        "MMLU_human_sexuality":0.4809160305,
        "MMLU_international_law":0.5785123967,
        "MMLU_jurisprudence":0.4722222222,
        "MMLU_logical_fallacies":0.4846625767,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.4368932039,
        "MMLU_marketing":0.6196581197,
        "MMLU_medical_genetics":0.49,
        "MMLU_miscellaneous":0.5542784163,
        "MMLU_moral_disputes":0.4884393064,
        "MMLU_moral_scenarios":0.3396648045,
        "MMLU_nutrition":0.4477124183,
        "MMLU_philosophy":0.5048231511,
        "MMLU_prehistory":0.4475308642,
        "MMLU_professional_accounting":0.3368794326,
        "MMLU_professional_law":0.3168187744,
        "MMLU_professional_medicine":0.4411764706,
        "MMLU_professional_psychology":0.3905228758,
        "MMLU_public_relations":0.4272727273,
        "MMLU_security_studies":0.4530612245,
        "MMLU_sociology":0.5970149254,
        "MMLU_us_foreign_policy":0.57,
        "MMLU_virology":0.3855421687,
        "MMLU_world_religions":0.5789473684
    },
    {
        "Model":"GPT-JT-Moderation-6B",
        "URL":"https:\/\/huggingface.co\/togethercomputer\/GPT-JT-Moderation-6B",
        "full_model_name":"togethercomputer\/GPT-JT-Moderation-6B",
        "Parameters":6.0,
        "MMLU_average":0.4163462463,
        "arc:challenge|25":0.3822525597,
        "hellaswag|10":0.5098585939,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.5555555556,
        "MMLU_astronomy":0.4276315789,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.4528301887,
        "MMLU_college_biology":0.3611111111,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.23,
        "MMLU_college_mathematics":0.39,
        "MMLU_college_medicine":0.3930635838,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.52,
        "MMLU_conceptual_physics":0.3872340426,
        "MMLU_econometrics":0.3859649123,
        "MMLU_electrical_engineering":0.4206896552,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.3015873016,
        "MMLU_global_facts":0.24,
        "MMLU_high_school_biology":0.4677419355,
        "MMLU_high_school_chemistry":0.354679803,
        "MMLU_high_school_computer_science":0.43,
        "MMLU_high_school_european_history":0.3878787879,
        "MMLU_high_school_geography":0.4848484848,
        "MMLU_high_school_government_and_politics":0.5336787565,
        "MMLU_high_school_macroeconomics":0.3846153846,
        "MMLU_high_school_mathematics":0.2222222222,
        "MMLU_high_school_microeconomics":0.3571428571,
        "MMLU_high_school_physics":0.2251655629,
        "MMLU_high_school_psychology":0.480733945,
        "MMLU_high_school_statistics":0.3981481481,
        "MMLU_high_school_us_history":0.3676470588,
        "MMLU_high_school_world_history":0.3966244726,
        "MMLU_human_aging":0.5426008969,
        "MMLU_human_sexuality":0.5267175573,
        "MMLU_international_law":0.4545454545,
        "MMLU_jurisprudence":0.5555555556,
        "MMLU_logical_fallacies":0.4233128834,
        "MMLU_machine_learning":0.4196428571,
        "MMLU_management":0.4951456311,
        "MMLU_marketing":0.5512820513,
        "MMLU_medical_genetics":0.57,
        "MMLU_miscellaneous":0.4521072797,
        "MMLU_moral_disputes":0.4219653179,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.4509803922,
        "MMLU_philosophy":0.424437299,
        "MMLU_prehistory":0.4537037037,
        "MMLU_professional_accounting":0.2836879433,
        "MMLU_professional_law":0.3102998696,
        "MMLU_professional_medicine":0.4117647059,
        "MMLU_professional_psychology":0.3660130719,
        "MMLU_public_relations":0.5545454545,
        "MMLU_security_studies":0.4040816327,
        "MMLU_sociology":0.5422885572,
        "MMLU_us_foreign_policy":0.64,
        "MMLU_virology":0.4397590361,
        "MMLU_world_religions":0.5555555556
    },
    {
        "Model":"alpaca-native",
        "URL":"https:\/\/huggingface.co\/chavinlo\/alpaca-native",
        "full_model_name":"chavinlo\/alpaca-native",
        "Parameters":null,
        "MMLU_average":0.4145348598,
        "arc:challenge|25":0.5127986348,
        "hellaswag|10":0.5959968134,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.3618421053,
        "MMLU_business_ethics":0.46,
        "MMLU_clinical_knowledge":0.441509434,
        "MMLU_college_biology":0.3819444444,
        "MMLU_college_chemistry":0.32,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.3815028902,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.54,
        "MMLU_conceptual_physics":0.3744680851,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.3655172414,
        "MMLU_elementary_mathematics":0.2883597884,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.4290322581,
        "MMLU_high_school_chemistry":0.3103448276,
        "MMLU_high_school_computer_science":0.45,
        "MMLU_high_school_european_history":0.5333333333,
        "MMLU_high_school_geography":0.4797979798,
        "MMLU_high_school_government_and_politics":0.6062176166,
        "MMLU_high_school_macroeconomics":0.3871794872,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.3529411765,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.5449541284,
        "MMLU_high_school_statistics":0.375,
        "MMLU_high_school_us_history":0.5343137255,
        "MMLU_high_school_world_history":0.5654008439,
        "MMLU_human_aging":0.5022421525,
        "MMLU_human_sexuality":0.4122137405,
        "MMLU_international_law":0.5454545455,
        "MMLU_jurisprudence":0.4444444444,
        "MMLU_logical_fallacies":0.3987730061,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.4757281553,
        "MMLU_marketing":0.6068376068,
        "MMLU_medical_genetics":0.45,
        "MMLU_miscellaneous":0.5504469987,
        "MMLU_moral_disputes":0.4248554913,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.4117647059,
        "MMLU_philosophy":0.4662379421,
        "MMLU_prehistory":0.4722222222,
        "MMLU_professional_accounting":0.3085106383,
        "MMLU_professional_law":0.3213820078,
        "MMLU_professional_medicine":0.4044117647,
        "MMLU_professional_psychology":0.3790849673,
        "MMLU_public_relations":0.4454545455,
        "MMLU_security_studies":0.4040816327,
        "MMLU_sociology":0.4726368159,
        "MMLU_us_foreign_policy":0.59,
        "MMLU_virology":0.4036144578,
        "MMLU_world_religions":0.5263157895
    },
    {
        "Model":"KoreanLM-hf",
        "URL":"https:\/\/huggingface.co\/quantumaikr\/KoreanLM-hf",
        "full_model_name":"quantumaikr\/KoreanLM-hf",
        "Parameters":null,
        "MMLU_average":0.4060667225,
        "arc:challenge|25":0.4897610922,
        "hellaswag|10":0.5686118303,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.4222222222,
        "MMLU_astronomy":0.4078947368,
        "MMLU_business_ethics":0.45,
        "MMLU_clinical_knowledge":0.4490566038,
        "MMLU_college_biology":0.4097222222,
        "MMLU_college_chemistry":0.25,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.3410404624,
        "MMLU_college_physics":0.1764705882,
        "MMLU_computer_security":0.49,
        "MMLU_conceptual_physics":0.4553191489,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.3724137931,
        "MMLU_elementary_mathematics":0.2989417989,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.3677419355,
        "MMLU_high_school_chemistry":0.2807881773,
        "MMLU_high_school_computer_science":0.31,
        "MMLU_high_school_european_history":0.4787878788,
        "MMLU_high_school_geography":0.4242424242,
        "MMLU_high_school_government_and_politics":0.5336787565,
        "MMLU_high_school_macroeconomics":0.358974359,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.3487394958,
        "MMLU_high_school_physics":0.2119205298,
        "MMLU_high_school_psychology":0.5155963303,
        "MMLU_high_school_statistics":0.162037037,
        "MMLU_high_school_us_history":0.4656862745,
        "MMLU_high_school_world_history":0.5485232068,
        "MMLU_human_aging":0.5964125561,
        "MMLU_human_sexuality":0.4427480916,
        "MMLU_international_law":0.6033057851,
        "MMLU_jurisprudence":0.5185185185,
        "MMLU_logical_fallacies":0.4049079755,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.5339805825,
        "MMLU_marketing":0.641025641,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.5555555556,
        "MMLU_moral_disputes":0.4335260116,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.4183006536,
        "MMLU_philosophy":0.4855305466,
        "MMLU_prehistory":0.4074074074,
        "MMLU_professional_accounting":0.3226950355,
        "MMLU_professional_law":0.3226857888,
        "MMLU_professional_medicine":0.3382352941,
        "MMLU_professional_psychology":0.4199346405,
        "MMLU_public_relations":0.4727272727,
        "MMLU_security_studies":0.3591836735,
        "MMLU_sociology":0.5124378109,
        "MMLU_us_foreign_policy":0.64,
        "MMLU_virology":0.3734939759,
        "MMLU_world_religions":0.5964912281
    },
    {
        "Model":"kollama2-7b-v3",
        "URL":"https:\/\/huggingface.co\/psyche\/kollama2-7b-v3",
        "full_model_name":"psyche\/kollama2-7b-v3",
        "Parameters":7.0,
        "MMLU_average":0.4040926595,
        "arc:challenge|25":0.4539249147,
        "hellaswag|10":0.5855407289,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.3925925926,
        "MMLU_astronomy":0.3552631579,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.3773584906,
        "MMLU_college_biology":0.4097222222,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.323699422,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.54,
        "MMLU_conceptual_physics":0.4127659574,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.3862068966,
        "MMLU_elementary_mathematics":0.2354497354,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.28,
        "MMLU_high_school_biology":0.3935483871,
        "MMLU_high_school_chemistry":0.3054187192,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.4909090909,
        "MMLU_high_school_geography":0.4191919192,
        "MMLU_high_school_government_and_politics":0.5958549223,
        "MMLU_high_school_macroeconomics":0.3487179487,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.3445378151,
        "MMLU_high_school_physics":0.238410596,
        "MMLU_high_school_psychology":0.4880733945,
        "MMLU_high_school_statistics":0.2314814815,
        "MMLU_high_school_us_history":0.4460784314,
        "MMLU_high_school_world_history":0.4430379747,
        "MMLU_human_aging":0.5156950673,
        "MMLU_human_sexuality":0.4732824427,
        "MMLU_international_law":0.5785123967,
        "MMLU_jurisprudence":0.4259259259,
        "MMLU_logical_fallacies":0.4294478528,
        "MMLU_machine_learning":0.4017857143,
        "MMLU_management":0.4757281553,
        "MMLU_marketing":0.6025641026,
        "MMLU_medical_genetics":0.5,
        "MMLU_miscellaneous":0.5938697318,
        "MMLU_moral_disputes":0.4566473988,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.4052287582,
        "MMLU_philosophy":0.5273311897,
        "MMLU_prehistory":0.4567901235,
        "MMLU_professional_accounting":0.3439716312,
        "MMLU_professional_law":0.3357235984,
        "MMLU_professional_medicine":0.3676470588,
        "MMLU_professional_psychology":0.4150326797,
        "MMLU_public_relations":0.5181818182,
        "MMLU_security_studies":0.2775510204,
        "MMLU_sociology":0.5074626866,
        "MMLU_us_foreign_policy":0.64,
        "MMLU_virology":0.3674698795,
        "MMLU_world_religions":0.6081871345
    },
    {
        "Model":"openthaigpt-1.0.0-alpha-7b-chat-ckpt-hf",
        "URL":"https:\/\/huggingface.co\/openthaigpt\/openthaigpt-1.0.0-alpha-7b-chat-ckpt-hf",
        "full_model_name":"openthaigpt\/openthaigpt-1.0.0-alpha-7b-chat-ckpt-hf",
        "Parameters":7.0,
        "MMLU_average":0.4001686627,
        "arc:challenge|25":0.4829351536,
        "hellaswag|10":0.5540728938,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.437037037,
        "MMLU_astronomy":0.4539473684,
        "MMLU_business_ethics":0.41,
        "MMLU_clinical_knowledge":0.4603773585,
        "MMLU_college_biology":0.4305555556,
        "MMLU_college_chemistry":0.3,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.3468208092,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.47,
        "MMLU_conceptual_physics":0.4085106383,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.3517241379,
        "MMLU_elementary_mathematics":0.3015873016,
        "MMLU_formal_logic":0.2222222222,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.4096774194,
        "MMLU_high_school_chemistry":0.3448275862,
        "MMLU_high_school_computer_science":0.35,
        "MMLU_high_school_european_history":0.4606060606,
        "MMLU_high_school_geography":0.595959596,
        "MMLU_high_school_government_and_politics":0.5388601036,
        "MMLU_high_school_macroeconomics":0.3256410256,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.3277310924,
        "MMLU_high_school_physics":0.2185430464,
        "MMLU_high_school_psychology":0.5174311927,
        "MMLU_high_school_statistics":0.212962963,
        "MMLU_high_school_us_history":0.4117647059,
        "MMLU_high_school_world_history":0.4599156118,
        "MMLU_human_aging":0.4618834081,
        "MMLU_human_sexuality":0.4045801527,
        "MMLU_international_law":0.5619834711,
        "MMLU_jurisprudence":0.4351851852,
        "MMLU_logical_fallacies":0.4233128834,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.4951456311,
        "MMLU_marketing":0.641025641,
        "MMLU_medical_genetics":0.47,
        "MMLU_miscellaneous":0.5657726692,
        "MMLU_moral_disputes":0.4537572254,
        "MMLU_moral_scenarios":0.251396648,
        "MMLU_nutrition":0.4477124183,
        "MMLU_philosophy":0.5209003215,
        "MMLU_prehistory":0.4567901235,
        "MMLU_professional_accounting":0.3191489362,
        "MMLU_professional_law":0.2803129074,
        "MMLU_professional_medicine":0.2720588235,
        "MMLU_professional_psychology":0.3839869281,
        "MMLU_public_relations":0.4636363636,
        "MMLU_security_studies":0.2816326531,
        "MMLU_sociology":0.4378109453,
        "MMLU_us_foreign_policy":0.66,
        "MMLU_virology":0.3915662651,
        "MMLU_world_religions":0.6081871345
    },
    {
        "Model":"CodeLlama-34b-Instruct-hf",
        "URL":"https:\/\/huggingface.co\/ehartford\/CodeLlama-34b-Instruct-hf",
        "full_model_name":"ehartford\/CodeLlama-34b-Instruct-hf",
        "Parameters":34.0,
        "MMLU_average":0.397452464,
        "arc:challenge|25":0.3788395904,
        "hellaswag|10":0.2998406692,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.3555555556,
        "MMLU_astronomy":0.3684210526,
        "MMLU_business_ethics":0.44,
        "MMLU_clinical_knowledge":0.4037735849,
        "MMLU_college_biology":0.3680555556,
        "MMLU_college_chemistry":0.29,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.41,
        "MMLU_college_medicine":0.3583815029,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.54,
        "MMLU_conceptual_physics":0.3872340426,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.3448275862,
        "MMLU_elementary_mathematics":0.3042328042,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.4419354839,
        "MMLU_high_school_chemistry":0.3645320197,
        "MMLU_high_school_computer_science":0.42,
        "MMLU_high_school_european_history":0.2848484848,
        "MMLU_high_school_geography":0.5202020202,
        "MMLU_high_school_government_and_politics":0.5336787565,
        "MMLU_high_school_macroeconomics":0.3564102564,
        "MMLU_high_school_mathematics":0.3111111111,
        "MMLU_high_school_microeconomics":0.3991596639,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.4532110092,
        "MMLU_high_school_statistics":0.2407407407,
        "MMLU_high_school_us_history":0.2843137255,
        "MMLU_high_school_world_history":0.4008438819,
        "MMLU_human_aging":0.4260089686,
        "MMLU_human_sexuality":0.4351145038,
        "MMLU_international_law":0.5702479339,
        "MMLU_jurisprudence":0.537037037,
        "MMLU_logical_fallacies":0.3803680982,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.5436893204,
        "MMLU_marketing":0.6196581197,
        "MMLU_medical_genetics":0.45,
        "MMLU_miscellaneous":0.5657726692,
        "MMLU_moral_disputes":0.4104046243,
        "MMLU_moral_scenarios":0.2044692737,
        "MMLU_nutrition":0.4117647059,
        "MMLU_philosophy":0.5048231511,
        "MMLU_prehistory":0.4567901235,
        "MMLU_professional_accounting":0.3156028369,
        "MMLU_professional_law":0.2724902216,
        "MMLU_professional_medicine":0.3308823529,
        "MMLU_professional_psychology":0.3431372549,
        "MMLU_public_relations":0.5272727273,
        "MMLU_security_studies":0.2897959184,
        "MMLU_sociology":0.4825870647,
        "MMLU_us_foreign_policy":0.57,
        "MMLU_virology":0.4036144578,
        "MMLU_world_religions":0.6432748538
    },
    {
        "Model":"CodeLlama-34B-Instruct-fp16",
        "URL":"https:\/\/huggingface.co\/TheBloke\/CodeLlama-34B-Instruct-fp16",
        "full_model_name":"TheBloke\/CodeLlama-34B-Instruct-fp16",
        "Parameters":34.0,
        "MMLU_average":0.3972483572,
        "arc:challenge|25":0.3796928328,
        "hellaswag|10":0.2998406692,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.3555555556,
        "MMLU_astronomy":0.3684210526,
        "MMLU_business_ethics":0.44,
        "MMLU_clinical_knowledge":0.4037735849,
        "MMLU_college_biology":0.3680555556,
        "MMLU_college_chemistry":0.29,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.41,
        "MMLU_college_medicine":0.3583815029,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.54,
        "MMLU_conceptual_physics":0.3872340426,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.3448275862,
        "MMLU_elementary_mathematics":0.3042328042,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.4419354839,
        "MMLU_high_school_chemistry":0.3645320197,
        "MMLU_high_school_computer_science":0.42,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.5202020202,
        "MMLU_high_school_government_and_politics":0.5336787565,
        "MMLU_high_school_macroeconomics":0.3564102564,
        "MMLU_high_school_mathematics":0.3111111111,
        "MMLU_high_school_microeconomics":0.3991596639,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.4532110092,
        "MMLU_high_school_statistics":0.2407407407,
        "MMLU_high_school_us_history":0.2843137255,
        "MMLU_high_school_world_history":0.4092827004,
        "MMLU_human_aging":0.4260089686,
        "MMLU_human_sexuality":0.4351145038,
        "MMLU_international_law":0.5702479339,
        "MMLU_jurisprudence":0.537037037,
        "MMLU_logical_fallacies":0.3803680982,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.5436893204,
        "MMLU_marketing":0.6196581197,
        "MMLU_medical_genetics":0.45,
        "MMLU_miscellaneous":0.5657726692,
        "MMLU_moral_disputes":0.4104046243,
        "MMLU_moral_scenarios":0.2044692737,
        "MMLU_nutrition":0.4117647059,
        "MMLU_philosophy":0.5048231511,
        "MMLU_prehistory":0.4567901235,
        "MMLU_professional_accounting":0.3156028369,
        "MMLU_professional_law":0.278357236,
        "MMLU_professional_medicine":0.3272058824,
        "MMLU_professional_psychology":0.3431372549,
        "MMLU_public_relations":0.5272727273,
        "MMLU_security_studies":0.2857142857,
        "MMLU_sociology":0.4825870647,
        "MMLU_us_foreign_policy":0.57,
        "MMLU_virology":0.4036144578,
        "MMLU_world_religions":0.6432748538
    },
    {
        "Model":"CodeLlama-34b-Instruct-hf",
        "URL":"https:\/\/huggingface.co\/codellama\/CodeLlama-34b-Instruct-hf",
        "full_model_name":"codellama\/CodeLlama-34b-Instruct-hf",
        "Parameters":34.0,
        "MMLU_average":0.3972483572,
        "arc:challenge|25":0.3796928328,
        "hellaswag|10":0.2998406692,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.3555555556,
        "MMLU_astronomy":0.3684210526,
        "MMLU_business_ethics":0.44,
        "MMLU_clinical_knowledge":0.4037735849,
        "MMLU_college_biology":0.3680555556,
        "MMLU_college_chemistry":0.29,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.41,
        "MMLU_college_medicine":0.3583815029,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.54,
        "MMLU_conceptual_physics":0.3872340426,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.3448275862,
        "MMLU_elementary_mathematics":0.3042328042,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.4419354839,
        "MMLU_high_school_chemistry":0.3645320197,
        "MMLU_high_school_computer_science":0.42,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.5202020202,
        "MMLU_high_school_government_and_politics":0.5336787565,
        "MMLU_high_school_macroeconomics":0.3564102564,
        "MMLU_high_school_mathematics":0.3111111111,
        "MMLU_high_school_microeconomics":0.3991596639,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.4532110092,
        "MMLU_high_school_statistics":0.2407407407,
        "MMLU_high_school_us_history":0.2843137255,
        "MMLU_high_school_world_history":0.4092827004,
        "MMLU_human_aging":0.4260089686,
        "MMLU_human_sexuality":0.4351145038,
        "MMLU_international_law":0.5702479339,
        "MMLU_jurisprudence":0.537037037,
        "MMLU_logical_fallacies":0.3803680982,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.5436893204,
        "MMLU_marketing":0.6196581197,
        "MMLU_medical_genetics":0.45,
        "MMLU_miscellaneous":0.5657726692,
        "MMLU_moral_disputes":0.4104046243,
        "MMLU_moral_scenarios":0.2044692737,
        "MMLU_nutrition":0.4117647059,
        "MMLU_philosophy":0.5048231511,
        "MMLU_prehistory":0.4567901235,
        "MMLU_professional_accounting":0.3156028369,
        "MMLU_professional_law":0.278357236,
        "MMLU_professional_medicine":0.3272058824,
        "MMLU_professional_psychology":0.3431372549,
        "MMLU_public_relations":0.5272727273,
        "MMLU_security_studies":0.2857142857,
        "MMLU_sociology":0.4825870647,
        "MMLU_us_foreign_policy":0.57,
        "MMLU_virology":0.4036144578,
        "MMLU_world_religions":0.6432748538
    },
    {
        "Model":"baize-v2-7b",
        "URL":"https:\/\/huggingface.co\/project-baize\/baize-v2-7b",
        "full_model_name":"project-baize\/baize-v2-7b",
        "Parameters":7.0,
        "MMLU_average":0.3960239685,
        "arc:challenge|25":0.4556313993,
        "hellaswag|10":0.5692093209,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4592592593,
        "MMLU_astronomy":0.3684210526,
        "MMLU_business_ethics":0.48,
        "MMLU_clinical_knowledge":0.4754716981,
        "MMLU_college_biology":0.3819444444,
        "MMLU_college_chemistry":0.3,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.3526011561,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.46,
        "MMLU_conceptual_physics":0.3787234043,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.3310344828,
        "MMLU_elementary_mathematics":0.2698412698,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.4064516129,
        "MMLU_high_school_chemistry":0.315270936,
        "MMLU_high_school_computer_science":0.3,
        "MMLU_high_school_european_history":0.5272727273,
        "MMLU_high_school_geography":0.4292929293,
        "MMLU_high_school_government_and_politics":0.5388601036,
        "MMLU_high_school_macroeconomics":0.3897435897,
        "MMLU_high_school_mathematics":0.2148148148,
        "MMLU_high_school_microeconomics":0.3067226891,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.471559633,
        "MMLU_high_school_statistics":0.2962962963,
        "MMLU_high_school_us_history":0.5,
        "MMLU_high_school_world_history":0.5105485232,
        "MMLU_human_aging":0.4618834081,
        "MMLU_human_sexuality":0.4427480916,
        "MMLU_international_law":0.5702479339,
        "MMLU_jurisprudence":0.4259259259,
        "MMLU_logical_fallacies":0.4049079755,
        "MMLU_machine_learning":0.2410714286,
        "MMLU_management":0.4563106796,
        "MMLU_marketing":0.5683760684,
        "MMLU_medical_genetics":0.46,
        "MMLU_miscellaneous":0.5249042146,
        "MMLU_moral_disputes":0.436416185,
        "MMLU_moral_scenarios":0.2446927374,
        "MMLU_nutrition":0.4248366013,
        "MMLU_philosophy":0.424437299,
        "MMLU_prehistory":0.4290123457,
        "MMLU_professional_accounting":0.3226950355,
        "MMLU_professional_law":0.3318122555,
        "MMLU_professional_medicine":0.4669117647,
        "MMLU_professional_psychology":0.3790849673,
        "MMLU_public_relations":0.4,
        "MMLU_security_studies":0.3959183673,
        "MMLU_sociology":0.4029850746,
        "MMLU_us_foreign_policy":0.61,
        "MMLU_virology":0.3734939759,
        "MMLU_world_religions":0.5730994152
    },
    {
        "Model":"vigogne-7b-chat",
        "URL":"https:\/\/huggingface.co\/bofenghuang\/vigogne-7b-chat",
        "full_model_name":"bofenghuang\/vigogne-7b-chat",
        "Parameters":7.0,
        "MMLU_average":0.3951019509,
        "arc:challenge|25":0.4812286689,
        "hellaswag|10":0.5875323641,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.4148148148,
        "MMLU_astronomy":0.3881578947,
        "MMLU_business_ethics":0.47,
        "MMLU_clinical_knowledge":0.4867924528,
        "MMLU_college_biology":0.3819444444,
        "MMLU_college_chemistry":0.3,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.3063583815,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.5,
        "MMLU_conceptual_physics":0.3361702128,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.324137931,
        "MMLU_elementary_mathematics":0.2777777778,
        "MMLU_formal_logic":0.2619047619,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.4258064516,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.28,
        "MMLU_high_school_european_history":0.5212121212,
        "MMLU_high_school_geography":0.4595959596,
        "MMLU_high_school_government_and_politics":0.5284974093,
        "MMLU_high_school_macroeconomics":0.3333333333,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.3571428571,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.5064220183,
        "MMLU_high_school_statistics":0.3425925926,
        "MMLU_high_school_us_history":0.4901960784,
        "MMLU_high_school_world_history":0.4767932489,
        "MMLU_human_aging":0.4439461883,
        "MMLU_human_sexuality":0.4503816794,
        "MMLU_international_law":0.5702479339,
        "MMLU_jurisprudence":0.4444444444,
        "MMLU_logical_fallacies":0.4294478528,
        "MMLU_machine_learning":0.2053571429,
        "MMLU_management":0.4466019417,
        "MMLU_marketing":0.6025641026,
        "MMLU_medical_genetics":0.44,
        "MMLU_miscellaneous":0.5619412516,
        "MMLU_moral_disputes":0.436416185,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.4117647059,
        "MMLU_philosophy":0.421221865,
        "MMLU_prehistory":0.4783950617,
        "MMLU_professional_accounting":0.3156028369,
        "MMLU_professional_law":0.3272490222,
        "MMLU_professional_medicine":0.4301470588,
        "MMLU_professional_psychology":0.3741830065,
        "MMLU_public_relations":0.4090909091,
        "MMLU_security_studies":0.4,
        "MMLU_sociology":0.5472636816,
        "MMLU_us_foreign_policy":0.51,
        "MMLU_virology":0.3795180723,
        "MMLU_world_religions":0.5730994152
    },
    {
        "Model":"orca_mini_v2_7b",
        "URL":"https:\/\/huggingface.co\/psmathur\/orca_mini_v2_7b",
        "full_model_name":"psmathur\/orca_mini_v2_7b",
        "Parameters":7.0,
        "MMLU_average":0.3949730429,
        "arc:challenge|25":0.4820819113,
        "hellaswag|10":0.5706034654,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.4148148148,
        "MMLU_astronomy":0.3486842105,
        "MMLU_business_ethics":0.44,
        "MMLU_clinical_knowledge":0.4716981132,
        "MMLU_college_biology":0.3680555556,
        "MMLU_college_chemistry":0.23,
        "MMLU_college_computer_science":0.37,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.2716763006,
        "MMLU_college_physics":0.1666666667,
        "MMLU_computer_security":0.55,
        "MMLU_conceptual_physics":0.3659574468,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.3862068966,
        "MMLU_elementary_mathematics":0.253968254,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.4,
        "MMLU_high_school_chemistry":0.275862069,
        "MMLU_high_school_computer_science":0.43,
        "MMLU_high_school_european_history":0.4909090909,
        "MMLU_high_school_geography":0.4949494949,
        "MMLU_high_school_government_and_politics":0.585492228,
        "MMLU_high_school_macroeconomics":0.3615384615,
        "MMLU_high_school_mathematics":0.2185185185,
        "MMLU_high_school_microeconomics":0.3487394958,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.4880733945,
        "MMLU_high_school_statistics":0.2685185185,
        "MMLU_high_school_us_history":0.4852941176,
        "MMLU_high_school_world_history":0.4936708861,
        "MMLU_human_aging":0.4843049327,
        "MMLU_human_sexuality":0.4580152672,
        "MMLU_international_law":0.6198347107,
        "MMLU_jurisprudence":0.4814814815,
        "MMLU_logical_fallacies":0.4539877301,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.359223301,
        "MMLU_marketing":0.5854700855,
        "MMLU_medical_genetics":0.43,
        "MMLU_miscellaneous":0.5517241379,
        "MMLU_moral_disputes":0.4450867052,
        "MMLU_moral_scenarios":0.243575419,
        "MMLU_nutrition":0.3758169935,
        "MMLU_philosophy":0.4340836013,
        "MMLU_prehistory":0.4567901235,
        "MMLU_professional_accounting":0.3120567376,
        "MMLU_professional_law":0.3096479791,
        "MMLU_professional_medicine":0.4191176471,
        "MMLU_professional_psychology":0.4150326797,
        "MMLU_public_relations":0.3909090909,
        "MMLU_security_studies":0.3428571429,
        "MMLU_sociology":0.5024875622,
        "MMLU_us_foreign_policy":0.59,
        "MMLU_virology":0.3493975904,
        "MMLU_world_religions":0.5438596491
    },
    {
        "Model":"Robin-v2",
        "URL":"https:\/\/huggingface.co\/HanningZhang\/Robin-v2",
        "full_model_name":"HanningZhang\/Robin-v2",
        "Parameters":null,
        "MMLU_average":0.3926804732,
        "arc:challenge|25":0.4351535836,
        "hellaswag|10":0.5453096993,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.437037037,
        "MMLU_astronomy":0.3355263158,
        "MMLU_business_ethics":0.46,
        "MMLU_clinical_knowledge":0.4075471698,
        "MMLU_college_biology":0.3611111111,
        "MMLU_college_chemistry":0.27,
        "MMLU_college_computer_science":0.29,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.3294797688,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.48,
        "MMLU_conceptual_physics":0.3191489362,
        "MMLU_econometrics":0.2105263158,
        "MMLU_electrical_engineering":0.3034482759,
        "MMLU_elementary_mathematics":0.2513227513,
        "MMLU_formal_logic":0.2222222222,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.4032258065,
        "MMLU_high_school_chemistry":0.2660098522,
        "MMLU_high_school_computer_science":0.39,
        "MMLU_high_school_european_history":0.5272727273,
        "MMLU_high_school_geography":0.4545454545,
        "MMLU_high_school_government_and_politics":0.5233160622,
        "MMLU_high_school_macroeconomics":0.3307692308,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.3445378151,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.4642201835,
        "MMLU_high_school_statistics":0.4259259259,
        "MMLU_high_school_us_history":0.4754901961,
        "MMLU_high_school_world_history":0.5189873418,
        "MMLU_human_aging":0.5156950673,
        "MMLU_human_sexuality":0.4351145038,
        "MMLU_international_law":0.5867768595,
        "MMLU_jurisprudence":0.5277777778,
        "MMLU_logical_fallacies":0.3803680982,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.4660194175,
        "MMLU_marketing":0.5897435897,
        "MMLU_medical_genetics":0.46,
        "MMLU_miscellaneous":0.5363984674,
        "MMLU_moral_disputes":0.4393063584,
        "MMLU_moral_scenarios":0.2737430168,
        "MMLU_nutrition":0.3823529412,
        "MMLU_philosophy":0.4469453376,
        "MMLU_prehistory":0.4907407407,
        "MMLU_professional_accounting":0.2907801418,
        "MMLU_professional_law":0.3024771838,
        "MMLU_professional_medicine":0.3823529412,
        "MMLU_professional_psychology":0.3741830065,
        "MMLU_public_relations":0.4454545455,
        "MMLU_security_studies":0.3265306122,
        "MMLU_sociology":0.4577114428,
        "MMLU_us_foreign_policy":0.59,
        "MMLU_virology":0.3795180723,
        "MMLU_world_religions":0.5906432749
    },
    {
        "Model":"Robin-v2",
        "URL":"https:\/\/huggingface.co\/LMFlow\/Robin-v2",
        "full_model_name":"LMFlow\/Robin-v2",
        "Parameters":null,
        "MMLU_average":0.3926804732,
        "arc:challenge|25":0.4351535836,
        "hellaswag|10":0.5453096993,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.437037037,
        "MMLU_astronomy":0.3355263158,
        "MMLU_business_ethics":0.46,
        "MMLU_clinical_knowledge":0.4075471698,
        "MMLU_college_biology":0.3611111111,
        "MMLU_college_chemistry":0.27,
        "MMLU_college_computer_science":0.29,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.3294797688,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.48,
        "MMLU_conceptual_physics":0.3191489362,
        "MMLU_econometrics":0.2105263158,
        "MMLU_electrical_engineering":0.3034482759,
        "MMLU_elementary_mathematics":0.2513227513,
        "MMLU_formal_logic":0.2222222222,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.4032258065,
        "MMLU_high_school_chemistry":0.2660098522,
        "MMLU_high_school_computer_science":0.39,
        "MMLU_high_school_european_history":0.5272727273,
        "MMLU_high_school_geography":0.4545454545,
        "MMLU_high_school_government_and_politics":0.5233160622,
        "MMLU_high_school_macroeconomics":0.3307692308,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.3445378151,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.4642201835,
        "MMLU_high_school_statistics":0.4259259259,
        "MMLU_high_school_us_history":0.4754901961,
        "MMLU_high_school_world_history":0.5189873418,
        "MMLU_human_aging":0.5156950673,
        "MMLU_human_sexuality":0.4351145038,
        "MMLU_international_law":0.5867768595,
        "MMLU_jurisprudence":0.5277777778,
        "MMLU_logical_fallacies":0.3803680982,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.4660194175,
        "MMLU_marketing":0.5897435897,
        "MMLU_medical_genetics":0.46,
        "MMLU_miscellaneous":0.5363984674,
        "MMLU_moral_disputes":0.4393063584,
        "MMLU_moral_scenarios":0.2737430168,
        "MMLU_nutrition":0.3823529412,
        "MMLU_philosophy":0.4469453376,
        "MMLU_prehistory":0.4907407407,
        "MMLU_professional_accounting":0.2907801418,
        "MMLU_professional_law":0.3024771838,
        "MMLU_professional_medicine":0.3823529412,
        "MMLU_professional_psychology":0.3741830065,
        "MMLU_public_relations":0.4454545455,
        "MMLU_security_studies":0.3265306122,
        "MMLU_sociology":0.4577114428,
        "MMLU_us_foreign_policy":0.59,
        "MMLU_virology":0.3795180723,
        "MMLU_world_religions":0.5906432749
    },
    {
        "Model":"Robin-7b-v2",
        "URL":"https:\/\/huggingface.co\/LMFlow\/Robin-7b-v2",
        "full_model_name":"LMFlow\/Robin-7b-v2",
        "Parameters":7.0,
        "MMLU_average":0.3926804732,
        "arc:challenge|25":0.4351535836,
        "hellaswag|10":0.5453096993,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.437037037,
        "MMLU_astronomy":0.3355263158,
        "MMLU_business_ethics":0.46,
        "MMLU_clinical_knowledge":0.4075471698,
        "MMLU_college_biology":0.3611111111,
        "MMLU_college_chemistry":0.27,
        "MMLU_college_computer_science":0.29,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.3294797688,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.48,
        "MMLU_conceptual_physics":0.3191489362,
        "MMLU_econometrics":0.2105263158,
        "MMLU_electrical_engineering":0.3034482759,
        "MMLU_elementary_mathematics":0.2513227513,
        "MMLU_formal_logic":0.2222222222,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.4032258065,
        "MMLU_high_school_chemistry":0.2660098522,
        "MMLU_high_school_computer_science":0.39,
        "MMLU_high_school_european_history":0.5272727273,
        "MMLU_high_school_geography":0.4545454545,
        "MMLU_high_school_government_and_politics":0.5233160622,
        "MMLU_high_school_macroeconomics":0.3307692308,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.3445378151,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.4642201835,
        "MMLU_high_school_statistics":0.4259259259,
        "MMLU_high_school_us_history":0.4754901961,
        "MMLU_high_school_world_history":0.5189873418,
        "MMLU_human_aging":0.5156950673,
        "MMLU_human_sexuality":0.4351145038,
        "MMLU_international_law":0.5867768595,
        "MMLU_jurisprudence":0.5277777778,
        "MMLU_logical_fallacies":0.3803680982,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.4660194175,
        "MMLU_marketing":0.5897435897,
        "MMLU_medical_genetics":0.46,
        "MMLU_miscellaneous":0.5363984674,
        "MMLU_moral_disputes":0.4393063584,
        "MMLU_moral_scenarios":0.2737430168,
        "MMLU_nutrition":0.3823529412,
        "MMLU_philosophy":0.4469453376,
        "MMLU_prehistory":0.4907407407,
        "MMLU_professional_accounting":0.2907801418,
        "MMLU_professional_law":0.3024771838,
        "MMLU_professional_medicine":0.3823529412,
        "MMLU_professional_psychology":0.3741830065,
        "MMLU_public_relations":0.4454545455,
        "MMLU_security_studies":0.3265306122,
        "MMLU_sociology":0.4577114428,
        "MMLU_us_foreign_policy":0.59,
        "MMLU_virology":0.3795180723,
        "MMLU_world_religions":0.5906432749
    },
    {
        "Model":"airoboros-7b-gpt4-1.1",
        "URL":"https:\/\/huggingface.co\/jondurbin\/airoboros-7b-gpt4-1.1",
        "full_model_name":"jondurbin\/airoboros-7b-gpt4-1.1",
        "Parameters":7.0,
        "MMLU_average":0.3924967082,
        "arc:challenge|25":0.5051194539,
        "hellaswag|10":0.6128261303,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.4222222222,
        "MMLU_astronomy":0.3815789474,
        "MMLU_business_ethics":0.4,
        "MMLU_clinical_knowledge":0.4301886792,
        "MMLU_college_biology":0.3819444444,
        "MMLU_college_chemistry":0.29,
        "MMLU_college_computer_science":0.27,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.3294797688,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.48,
        "MMLU_conceptual_physics":0.3659574468,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.3448275862,
        "MMLU_elementary_mathematics":0.2830687831,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.3870967742,
        "MMLU_high_school_chemistry":0.3300492611,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.5151515152,
        "MMLU_high_school_geography":0.4696969697,
        "MMLU_high_school_government_and_politics":0.5388601036,
        "MMLU_high_school_macroeconomics":0.3461538462,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.3277310924,
        "MMLU_high_school_physics":0.2450331126,
        "MMLU_high_school_psychology":0.5064220183,
        "MMLU_high_school_statistics":0.3611111111,
        "MMLU_high_school_us_history":0.4705882353,
        "MMLU_high_school_world_history":0.447257384,
        "MMLU_human_aging":0.4753363229,
        "MMLU_human_sexuality":0.4198473282,
        "MMLU_international_law":0.5867768595,
        "MMLU_jurisprudence":0.4259259259,
        "MMLU_logical_fallacies":0.4355828221,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.3883495146,
        "MMLU_marketing":0.5598290598,
        "MMLU_medical_genetics":0.44,
        "MMLU_miscellaneous":0.558109834,
        "MMLU_moral_disputes":0.4277456647,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.4019607843,
        "MMLU_philosophy":0.424437299,
        "MMLU_prehistory":0.4043209877,
        "MMLU_professional_accounting":0.3226950355,
        "MMLU_professional_law":0.3298565841,
        "MMLU_professional_medicine":0.4889705882,
        "MMLU_professional_psychology":0.4117647059,
        "MMLU_public_relations":0.5,
        "MMLU_security_studies":0.3632653061,
        "MMLU_sociology":0.4776119403,
        "MMLU_us_foreign_policy":0.57,
        "MMLU_virology":0.3313253012,
        "MMLU_world_religions":0.5380116959
    },
    {
        "Model":"MiniChat-3B",
        "URL":"https:\/\/huggingface.co\/GeneZC\/MiniChat-3B",
        "full_model_name":"GeneZC\/MiniChat-3B",
        "Parameters":3.0,
        "MMLU_average":0.3917205248,
        "arc:challenge|25":0.4112627986,
        "hellaswag|10":0.495518821,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4,
        "MMLU_astronomy":0.4013157895,
        "MMLU_business_ethics":0.33,
        "MMLU_clinical_knowledge":0.4830188679,
        "MMLU_college_biology":0.4305555556,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.35,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.3410404624,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.54,
        "MMLU_conceptual_physics":0.3489361702,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.3724137931,
        "MMLU_elementary_mathematics":0.2486772487,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.27,
        "MMLU_high_school_biology":0.3903225806,
        "MMLU_high_school_chemistry":0.3201970443,
        "MMLU_high_school_computer_science":0.38,
        "MMLU_high_school_european_history":0.5090909091,
        "MMLU_high_school_geography":0.4898989899,
        "MMLU_high_school_government_and_politics":0.4611398964,
        "MMLU_high_school_macroeconomics":0.3743589744,
        "MMLU_high_school_mathematics":0.2111111111,
        "MMLU_high_school_microeconomics":0.3445378151,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.5082568807,
        "MMLU_high_school_statistics":0.2592592593,
        "MMLU_high_school_us_history":0.4558823529,
        "MMLU_high_school_world_history":0.5021097046,
        "MMLU_human_aging":0.3811659193,
        "MMLU_human_sexuality":0.465648855,
        "MMLU_international_law":0.5289256198,
        "MMLU_jurisprudence":0.4166666667,
        "MMLU_logical_fallacies":0.3374233129,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.4466019417,
        "MMLU_marketing":0.6282051282,
        "MMLU_medical_genetics":0.52,
        "MMLU_miscellaneous":0.4533844189,
        "MMLU_moral_disputes":0.3988439306,
        "MMLU_moral_scenarios":0.2480446927,
        "MMLU_nutrition":0.4477124183,
        "MMLU_philosophy":0.4405144695,
        "MMLU_prehistory":0.4012345679,
        "MMLU_professional_accounting":0.3404255319,
        "MMLU_professional_law":0.3285528031,
        "MMLU_professional_medicine":0.3161764706,
        "MMLU_professional_psychology":0.385620915,
        "MMLU_public_relations":0.4454545455,
        "MMLU_security_studies":0.5102040816,
        "MMLU_sociology":0.5223880597,
        "MMLU_us_foreign_policy":0.57,
        "MMLU_virology":0.4457831325,
        "MMLU_world_religions":0.4385964912
    },
    {
        "Model":"robin-7b-v2-delta",
        "URL":"https:\/\/huggingface.co\/OptimalScale\/robin-7b-v2-delta",
        "full_model_name":"OptimalScale\/robin-7b-v2-delta",
        "Parameters":7.0,
        "MMLU_average":0.3895756693,
        "arc:challenge|25":0.4351535836,
        "hellaswag|10":0.5452101175,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4444444444,
        "MMLU_astronomy":0.3552631579,
        "MMLU_business_ethics":0.47,
        "MMLU_clinical_knowledge":0.3924528302,
        "MMLU_college_biology":0.3611111111,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.3,
        "MMLU_college_mathematics":0.23,
        "MMLU_college_medicine":0.323699422,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.46,
        "MMLU_conceptual_physics":0.3276595745,
        "MMLU_econometrics":0.2105263158,
        "MMLU_electrical_engineering":0.3172413793,
        "MMLU_elementary_mathematics":0.2513227513,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.4,
        "MMLU_high_school_chemistry":0.2610837438,
        "MMLU_high_school_computer_science":0.38,
        "MMLU_high_school_european_history":0.5212121212,
        "MMLU_high_school_geography":0.4545454545,
        "MMLU_high_school_government_and_politics":0.5129533679,
        "MMLU_high_school_macroeconomics":0.3256410256,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.3277310924,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.4587155963,
        "MMLU_high_school_statistics":0.4259259259,
        "MMLU_high_school_us_history":0.4754901961,
        "MMLU_high_school_world_history":0.5232067511,
        "MMLU_human_aging":0.5112107623,
        "MMLU_human_sexuality":0.4198473282,
        "MMLU_international_law":0.5867768595,
        "MMLU_jurisprudence":0.5092592593,
        "MMLU_logical_fallacies":0.3926380368,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.4757281553,
        "MMLU_marketing":0.594017094,
        "MMLU_medical_genetics":0.46,
        "MMLU_miscellaneous":0.5389527458,
        "MMLU_moral_disputes":0.4421965318,
        "MMLU_moral_scenarios":0.2480446927,
        "MMLU_nutrition":0.3888888889,
        "MMLU_philosophy":0.4405144695,
        "MMLU_prehistory":0.487654321,
        "MMLU_professional_accounting":0.3014184397,
        "MMLU_professional_law":0.294654498,
        "MMLU_professional_medicine":0.3786764706,
        "MMLU_professional_psychology":0.3676470588,
        "MMLU_public_relations":0.4363636364,
        "MMLU_security_studies":0.3142857143,
        "MMLU_sociology":0.4527363184,
        "MMLU_us_foreign_policy":0.58,
        "MMLU_virology":0.3554216867,
        "MMLU_world_religions":0.5847953216
    },
    {
        "Model":"giraffe-7b",
        "URL":"https:\/\/huggingface.co\/ashercn97\/giraffe-7b",
        "full_model_name":"ashercn97\/giraffe-7b",
        "Parameters":7.0,
        "MMLU_average":0.3888917904,
        "arc:challenge|25":0.4453924915,
        "hellaswag|10":0.5799641506,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.4814814815,
        "MMLU_astronomy":0.4013157895,
        "MMLU_business_ethics":0.36,
        "MMLU_clinical_knowledge":0.4490566038,
        "MMLU_college_biology":0.3402777778,
        "MMLU_college_chemistry":0.15,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.3699421965,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.57,
        "MMLU_conceptual_physics":0.3319148936,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.4206896552,
        "MMLU_elementary_mathematics":0.2645502646,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.3677419355,
        "MMLU_high_school_chemistry":0.2561576355,
        "MMLU_high_school_computer_science":0.36,
        "MMLU_high_school_european_history":0.4121212121,
        "MMLU_high_school_geography":0.4494949495,
        "MMLU_high_school_government_and_politics":0.5233160622,
        "MMLU_high_school_macroeconomics":0.3923076923,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.3529411765,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.4880733945,
        "MMLU_high_school_statistics":0.2083333333,
        "MMLU_high_school_us_history":0.3676470588,
        "MMLU_high_school_world_history":0.4810126582,
        "MMLU_human_aging":0.4573991031,
        "MMLU_human_sexuality":0.5038167939,
        "MMLU_international_law":0.5123966942,
        "MMLU_jurisprudence":0.4259259259,
        "MMLU_logical_fallacies":0.4171779141,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.4563106796,
        "MMLU_marketing":0.6025641026,
        "MMLU_medical_genetics":0.45,
        "MMLU_miscellaneous":0.5172413793,
        "MMLU_moral_disputes":0.4132947977,
        "MMLU_moral_scenarios":0.2703910615,
        "MMLU_nutrition":0.4215686275,
        "MMLU_philosophy":0.4405144695,
        "MMLU_prehistory":0.4228395062,
        "MMLU_professional_accounting":0.3404255319,
        "MMLU_professional_law":0.2900912647,
        "MMLU_professional_medicine":0.3382352941,
        "MMLU_professional_psychology":0.3758169935,
        "MMLU_public_relations":0.4454545455,
        "MMLU_security_studies":0.3918367347,
        "MMLU_sociology":0.5074626866,
        "MMLU_us_foreign_policy":0.54,
        "MMLU_virology":0.3795180723,
        "MMLU_world_religions":0.5321637427
    },
    {
        "Model":"CodeLlama-13b-Instruct-hf",
        "URL":"https:\/\/huggingface.co\/codellama\/CodeLlama-13b-Instruct-hf",
        "full_model_name":"codellama\/CodeLlama-13b-Instruct-hf",
        "Parameters":13.0,
        "MMLU_average":0.388870536,
        "arc:challenge|25":0.4087030717,
        "hellaswag|10":0.481179048,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.3259259259,
        "MMLU_astronomy":0.4144736842,
        "MMLU_business_ethics":0.37,
        "MMLU_clinical_knowledge":0.3811320755,
        "MMLU_college_biology":0.3611111111,
        "MMLU_college_chemistry":0.36,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.323699422,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.49,
        "MMLU_conceptual_physics":0.3617021277,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.4206896552,
        "MMLU_elementary_mathematics":0.2751322751,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.4064516129,
        "MMLU_high_school_chemistry":0.3054187192,
        "MMLU_high_school_computer_science":0.49,
        "MMLU_high_school_european_history":0.4,
        "MMLU_high_school_geography":0.5151515152,
        "MMLU_high_school_government_and_politics":0.5077720207,
        "MMLU_high_school_macroeconomics":0.3692307692,
        "MMLU_high_school_mathematics":0.2222222222,
        "MMLU_high_school_microeconomics":0.3781512605,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.4990825688,
        "MMLU_high_school_statistics":0.3796296296,
        "MMLU_high_school_us_history":0.3970588235,
        "MMLU_high_school_world_history":0.3839662447,
        "MMLU_human_aging":0.4260089686,
        "MMLU_human_sexuality":0.4198473282,
        "MMLU_international_law":0.3801652893,
        "MMLU_jurisprudence":0.4537037037,
        "MMLU_logical_fallacies":0.4049079755,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.4563106796,
        "MMLU_marketing":0.7051282051,
        "MMLU_medical_genetics":0.43,
        "MMLU_miscellaneous":0.4955300128,
        "MMLU_moral_disputes":0.3468208092,
        "MMLU_moral_scenarios":0.2681564246,
        "MMLU_nutrition":0.3921568627,
        "MMLU_philosophy":0.4083601286,
        "MMLU_prehistory":0.3888888889,
        "MMLU_professional_accounting":0.304964539,
        "MMLU_professional_law":0.2985658409,
        "MMLU_professional_medicine":0.3455882353,
        "MMLU_professional_psychology":0.3284313725,
        "MMLU_public_relations":0.4727272727,
        "MMLU_security_studies":0.4367346939,
        "MMLU_sociology":0.4577114428,
        "MMLU_us_foreign_policy":0.56,
        "MMLU_virology":0.3975903614,
        "MMLU_world_religions":0.4678362573
    },
    {
        "Model":"CodeLlama-13B-Instruct-fp16",
        "URL":"https:\/\/huggingface.co\/TheBloke\/CodeLlama-13B-Instruct-fp16",
        "full_model_name":"TheBloke\/CodeLlama-13B-Instruct-fp16",
        "Parameters":13.0,
        "MMLU_average":0.3876937155,
        "arc:challenge|25":0.4087030717,
        "hellaswag|10":0.4812786298,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.3259259259,
        "MMLU_astronomy":0.4144736842,
        "MMLU_business_ethics":0.37,
        "MMLU_clinical_knowledge":0.3811320755,
        "MMLU_college_biology":0.3611111111,
        "MMLU_college_chemistry":0.36,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.323699422,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.49,
        "MMLU_conceptual_physics":0.3617021277,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.4206896552,
        "MMLU_elementary_mathematics":0.2751322751,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.4064516129,
        "MMLU_high_school_chemistry":0.3054187192,
        "MMLU_high_school_computer_science":0.49,
        "MMLU_high_school_european_history":0.3575757576,
        "MMLU_high_school_geography":0.5151515152,
        "MMLU_high_school_government_and_politics":0.5077720207,
        "MMLU_high_school_macroeconomics":0.3692307692,
        "MMLU_high_school_mathematics":0.2222222222,
        "MMLU_high_school_microeconomics":0.3781512605,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.4990825688,
        "MMLU_high_school_statistics":0.3796296296,
        "MMLU_high_school_us_history":0.3823529412,
        "MMLU_high_school_world_history":0.3839662447,
        "MMLU_human_aging":0.4260089686,
        "MMLU_human_sexuality":0.4198473282,
        "MMLU_international_law":0.3801652893,
        "MMLU_jurisprudence":0.4537037037,
        "MMLU_logical_fallacies":0.4049079755,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.4563106796,
        "MMLU_marketing":0.7051282051,
        "MMLU_medical_genetics":0.43,
        "MMLU_miscellaneous":0.4955300128,
        "MMLU_moral_disputes":0.3468208092,
        "MMLU_moral_scenarios":0.2681564246,
        "MMLU_nutrition":0.3921568627,
        "MMLU_philosophy":0.4083601286,
        "MMLU_prehistory":0.3888888889,
        "MMLU_professional_accounting":0.304964539,
        "MMLU_professional_law":0.2926988266,
        "MMLU_professional_medicine":0.3455882353,
        "MMLU_professional_psychology":0.3284313725,
        "MMLU_public_relations":0.4727272727,
        "MMLU_security_studies":0.4326530612,
        "MMLU_sociology":0.4577114428,
        "MMLU_us_foreign_policy":0.56,
        "MMLU_virology":0.3975903614,
        "MMLU_world_religions":0.4678362573
    },
    {
        "Model":"airoboros-7b-gpt4-1.4.1-qlora",
        "URL":"https:\/\/huggingface.co\/jondurbin\/airoboros-7b-gpt4-1.4.1-qlora",
        "full_model_name":"jondurbin\/airoboros-7b-gpt4-1.4.1-qlora",
        "Parameters":7.0,
        "MMLU_average":0.3876607733,
        "arc:challenge|25":0.4889078498,
        "hellaswag|10":0.5955984864,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.437037037,
        "MMLU_astronomy":0.3618421053,
        "MMLU_business_ethics":0.43,
        "MMLU_clinical_knowledge":0.3773584906,
        "MMLU_college_biology":0.4097222222,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.36,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.3063583815,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.49,
        "MMLU_conceptual_physics":0.3446808511,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.3862068966,
        "MMLU_elementary_mathematics":0.2407407407,
        "MMLU_formal_logic":0.2698412698,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.3612903226,
        "MMLU_high_school_chemistry":0.275862069,
        "MMLU_high_school_computer_science":0.35,
        "MMLU_high_school_european_history":0.5272727273,
        "MMLU_high_school_geography":0.4393939394,
        "MMLU_high_school_government_and_politics":0.4559585492,
        "MMLU_high_school_macroeconomics":0.3794871795,
        "MMLU_high_school_mathematics":0.2296296296,
        "MMLU_high_school_microeconomics":0.3277310924,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.5137614679,
        "MMLU_high_school_statistics":0.3842592593,
        "MMLU_high_school_us_history":0.4607843137,
        "MMLU_high_school_world_history":0.4978902954,
        "MMLU_human_aging":0.4349775785,
        "MMLU_human_sexuality":0.4198473282,
        "MMLU_international_law":0.5123966942,
        "MMLU_jurisprudence":0.462962963,
        "MMLU_logical_fallacies":0.3865030675,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.427184466,
        "MMLU_marketing":0.5854700855,
        "MMLU_medical_genetics":0.46,
        "MMLU_miscellaneous":0.588761175,
        "MMLU_moral_disputes":0.4335260116,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.3921568627,
        "MMLU_philosophy":0.424437299,
        "MMLU_prehistory":0.4259259259,
        "MMLU_professional_accounting":0.2765957447,
        "MMLU_professional_law":0.3096479791,
        "MMLU_professional_medicine":0.4448529412,
        "MMLU_professional_psychology":0.4019607843,
        "MMLU_public_relations":0.4909090909,
        "MMLU_security_studies":0.3142857143,
        "MMLU_sociology":0.4676616915,
        "MMLU_us_foreign_policy":0.56,
        "MMLU_virology":0.3493975904,
        "MMLU_world_religions":0.5555555556
    },
    {
        "Model":"yayi-13b-llama2",
        "URL":"https:\/\/huggingface.co\/wenge-research\/yayi-13b-llama2",
        "full_model_name":"wenge-research\/yayi-13b-llama2",
        "Parameters":13.0,
        "MMLU_average":0.3868304268,
        "arc:challenge|25":0.4402730375,
        "hellaswag|10":0.5553674567,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.3703703704,
        "MMLU_astronomy":0.3355263158,
        "MMLU_business_ethics":0.39,
        "MMLU_clinical_knowledge":0.4452830189,
        "MMLU_college_biology":0.4305555556,
        "MMLU_college_chemistry":0.27,
        "MMLU_college_computer_science":0.27,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.2832369942,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.51,
        "MMLU_conceptual_physics":0.2978723404,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.4344827586,
        "MMLU_elementary_mathematics":0.2619047619,
        "MMLU_formal_logic":0.2380952381,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.4903225806,
        "MMLU_high_school_chemistry":0.3251231527,
        "MMLU_high_school_computer_science":0.32,
        "MMLU_high_school_european_history":0.4848484848,
        "MMLU_high_school_geography":0.5505050505,
        "MMLU_high_school_government_and_politics":0.5336787565,
        "MMLU_high_school_macroeconomics":0.3666666667,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.3445378151,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.5100917431,
        "MMLU_high_school_statistics":0.25,
        "MMLU_high_school_us_history":0.4362745098,
        "MMLU_high_school_world_history":0.4978902954,
        "MMLU_human_aging":0.3542600897,
        "MMLU_human_sexuality":0.5190839695,
        "MMLU_international_law":0.5041322314,
        "MMLU_jurisprudence":0.4074074074,
        "MMLU_logical_fallacies":0.4662576687,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.5339805825,
        "MMLU_marketing":0.5427350427,
        "MMLU_medical_genetics":0.35,
        "MMLU_miscellaneous":0.5478927203,
        "MMLU_moral_disputes":0.436416185,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.4281045752,
        "MMLU_philosophy":0.4726688103,
        "MMLU_prehistory":0.4691358025,
        "MMLU_professional_accounting":0.304964539,
        "MMLU_professional_law":0.3272490222,
        "MMLU_professional_medicine":0.3235294118,
        "MMLU_professional_psychology":0.3349673203,
        "MMLU_public_relations":0.4090909091,
        "MMLU_security_studies":0.4081632653,
        "MMLU_sociology":0.4378109453,
        "MMLU_us_foreign_policy":0.52,
        "MMLU_virology":0.4036144578,
        "MMLU_world_religions":0.432748538
    },
    {
        "Model":"airoboros-7b-gpt4-1.4",
        "URL":"https:\/\/huggingface.co\/jondurbin\/airoboros-7b-gpt4-1.4",
        "full_model_name":"jondurbin\/airoboros-7b-gpt4-1.4",
        "Parameters":7.0,
        "MMLU_average":0.3860682309,
        "arc:challenge|25":0.5042662116,
        "hellaswag|10":0.6114319857,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.3925925926,
        "MMLU_astronomy":0.4013157895,
        "MMLU_business_ethics":0.4,
        "MMLU_clinical_knowledge":0.4264150943,
        "MMLU_college_biology":0.375,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.3179190751,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.46,
        "MMLU_conceptual_physics":0.3659574468,
        "MMLU_econometrics":0.2192982456,
        "MMLU_electrical_engineering":0.3655172414,
        "MMLU_elementary_mathematics":0.2645502646,
        "MMLU_formal_logic":0.2619047619,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.364516129,
        "MMLU_high_school_chemistry":0.3004926108,
        "MMLU_high_school_computer_science":0.38,
        "MMLU_high_school_european_history":0.4909090909,
        "MMLU_high_school_geography":0.4444444444,
        "MMLU_high_school_government_and_politics":0.518134715,
        "MMLU_high_school_macroeconomics":0.3538461538,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.3487394958,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.4917431193,
        "MMLU_high_school_statistics":0.2175925926,
        "MMLU_high_school_us_history":0.431372549,
        "MMLU_high_school_world_history":0.4725738397,
        "MMLU_human_aging":0.4349775785,
        "MMLU_human_sexuality":0.4045801527,
        "MMLU_international_law":0.6280991736,
        "MMLU_jurisprudence":0.4444444444,
        "MMLU_logical_fallacies":0.4294478528,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.3689320388,
        "MMLU_marketing":0.5427350427,
        "MMLU_medical_genetics":0.41,
        "MMLU_miscellaneous":0.5389527458,
        "MMLU_moral_disputes":0.436416185,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.3888888889,
        "MMLU_philosophy":0.3987138264,
        "MMLU_prehistory":0.4320987654,
        "MMLU_professional_accounting":0.3156028369,
        "MMLU_professional_law":0.3109517601,
        "MMLU_professional_medicine":0.4779411765,
        "MMLU_professional_psychology":0.4035947712,
        "MMLU_public_relations":0.5090909091,
        "MMLU_security_studies":0.3142857143,
        "MMLU_sociology":0.4626865672,
        "MMLU_us_foreign_policy":0.58,
        "MMLU_virology":0.3373493976,
        "MMLU_world_religions":0.5380116959
    },
    {
        "Model":"speechless-tora-code-7b-v1.0",
        "URL":"https:\/\/huggingface.co\/uukuguy\/speechless-tora-code-7b-v1.0",
        "full_model_name":"uukuguy\/speechless-tora-code-7b-v1.0",
        "Parameters":7.0,
        "MMLU_average":0.3856177055,
        "arc:challenge|25":0.3822525597,
        "hellaswag|10":0.4890460068,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.3111111111,
        "MMLU_astronomy":0.3684210526,
        "MMLU_business_ethics":0.46,
        "MMLU_clinical_knowledge":0.3811320755,
        "MMLU_college_biology":0.3680555556,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.34,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.323699422,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.57,
        "MMLU_conceptual_physics":0.3319148936,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.3586206897,
        "MMLU_elementary_mathematics":0.2724867725,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.3838709677,
        "MMLU_high_school_chemistry":0.3103448276,
        "MMLU_high_school_computer_science":0.44,
        "MMLU_high_school_european_history":0.4909090909,
        "MMLU_high_school_geography":0.4848484848,
        "MMLU_high_school_government_and_politics":0.4715025907,
        "MMLU_high_school_macroeconomics":0.3615384615,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.3907563025,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.4568807339,
        "MMLU_high_school_statistics":0.2962962963,
        "MMLU_high_school_us_history":0.4019607843,
        "MMLU_high_school_world_history":0.5147679325,
        "MMLU_human_aging":0.4618834081,
        "MMLU_human_sexuality":0.3969465649,
        "MMLU_international_law":0.5454545455,
        "MMLU_jurisprudence":0.4351851852,
        "MMLU_logical_fallacies":0.4294478528,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.5825242718,
        "MMLU_marketing":0.6239316239,
        "MMLU_medical_genetics":0.41,
        "MMLU_miscellaneous":0.4891443167,
        "MMLU_moral_disputes":0.4046242775,
        "MMLU_moral_scenarios":0.261452514,
        "MMLU_nutrition":0.3594771242,
        "MMLU_philosophy":0.421221865,
        "MMLU_prehistory":0.3981481481,
        "MMLU_professional_accounting":0.3085106383,
        "MMLU_professional_law":0.3142112125,
        "MMLU_professional_medicine":0.2904411765,
        "MMLU_professional_psychology":0.3480392157,
        "MMLU_public_relations":0.4454545455,
        "MMLU_security_studies":0.3918367347,
        "MMLU_sociology":0.4776119403,
        "MMLU_us_foreign_policy":0.55,
        "MMLU_virology":0.3674698795,
        "MMLU_world_religions":0.4502923977
    },
    {
        "Model":"CAlign-alpaca-7b",
        "URL":"https:\/\/huggingface.co\/jxhong\/CAlign-alpaca-7b",
        "full_model_name":"jxhong\/CAlign-alpaca-7b",
        "Parameters":7.0,
        "MMLU_average":0.3856156299,
        "arc:challenge|25":0.4633105802,
        "hellaswag|10":0.5503883689,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.3684210526,
        "MMLU_business_ethics":0.44,
        "MMLU_clinical_knowledge":0.4188679245,
        "MMLU_college_biology":0.375,
        "MMLU_college_chemistry":0.29,
        "MMLU_college_computer_science":0.38,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.2947976879,
        "MMLU_college_physics":0.1764705882,
        "MMLU_computer_security":0.51,
        "MMLU_conceptual_physics":0.3872340426,
        "MMLU_econometrics":0.2105263158,
        "MMLU_electrical_engineering":0.3103448276,
        "MMLU_elementary_mathematics":0.2645502646,
        "MMLU_formal_logic":0.2222222222,
        "MMLU_global_facts":0.29,
        "MMLU_high_school_biology":0.4064516129,
        "MMLU_high_school_chemistry":0.3004926108,
        "MMLU_high_school_computer_science":0.36,
        "MMLU_high_school_european_history":0.5212121212,
        "MMLU_high_school_geography":0.4444444444,
        "MMLU_high_school_government_and_politics":0.5388601036,
        "MMLU_high_school_macroeconomics":0.3538461538,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.3445378151,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.5119266055,
        "MMLU_high_school_statistics":0.4398148148,
        "MMLU_high_school_us_history":0.4019607843,
        "MMLU_high_school_world_history":0.4978902954,
        "MMLU_human_aging":0.4977578475,
        "MMLU_human_sexuality":0.4122137405,
        "MMLU_international_law":0.6115702479,
        "MMLU_jurisprudence":0.4351851852,
        "MMLU_logical_fallacies":0.3742331288,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.4174757282,
        "MMLU_marketing":0.5683760684,
        "MMLU_medical_genetics":0.43,
        "MMLU_miscellaneous":0.4942528736,
        "MMLU_moral_disputes":0.3699421965,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.3562091503,
        "MMLU_philosophy":0.4372990354,
        "MMLU_prehistory":0.4197530864,
        "MMLU_professional_accounting":0.2836879433,
        "MMLU_professional_law":0.3005215124,
        "MMLU_professional_medicine":0.4522058824,
        "MMLU_professional_psychology":0.3839869281,
        "MMLU_public_relations":0.4454545455,
        "MMLU_security_studies":0.2734693878,
        "MMLU_sociology":0.4129353234,
        "MMLU_us_foreign_policy":0.5,
        "MMLU_virology":0.3614457831,
        "MMLU_world_religions":0.514619883
    },
    {
        "Model":"LLongMA-2-7b-16k",
        "URL":"https:\/\/huggingface.co\/conceptofmind\/LLongMA-2-7b-16k",
        "full_model_name":"conceptofmind\/LLongMA-2-7b-16k",
        "Parameters":7.0,
        "MMLU_average":0.3846195532,
        "arc:challenge|25":0.4803754266,
        "hellaswag|10":0.5653256323,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4222222222,
        "MMLU_astronomy":0.3355263158,
        "MMLU_business_ethics":0.43,
        "MMLU_clinical_knowledge":0.3773584906,
        "MMLU_college_biology":0.375,
        "MMLU_college_chemistry":0.29,
        "MMLU_college_computer_science":0.36,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.3294797688,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.51,
        "MMLU_conceptual_physics":0.3617021277,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.3793103448,
        "MMLU_elementary_mathematics":0.2777777778,
        "MMLU_formal_logic":0.1825396825,
        "MMLU_global_facts":0.26,
        "MMLU_high_school_biology":0.364516129,
        "MMLU_high_school_chemistry":0.275862069,
        "MMLU_high_school_computer_science":0.41,
        "MMLU_high_school_european_history":0.4787878788,
        "MMLU_high_school_geography":0.3333333333,
        "MMLU_high_school_government_and_politics":0.5544041451,
        "MMLU_high_school_macroeconomics":0.3179487179,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.3067226891,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.4385321101,
        "MMLU_high_school_statistics":0.3194444444,
        "MMLU_high_school_us_history":0.4166666667,
        "MMLU_high_school_world_history":0.4894514768,
        "MMLU_human_aging":0.4753363229,
        "MMLU_human_sexuality":0.4122137405,
        "MMLU_international_law":0.5785123967,
        "MMLU_jurisprudence":0.3611111111,
        "MMLU_logical_fallacies":0.3865030675,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.4174757282,
        "MMLU_marketing":0.5042735043,
        "MMLU_medical_genetics":0.43,
        "MMLU_miscellaneous":0.503192848,
        "MMLU_moral_disputes":0.4450867052,
        "MMLU_moral_scenarios":0.2491620112,
        "MMLU_nutrition":0.4346405229,
        "MMLU_philosophy":0.4308681672,
        "MMLU_prehistory":0.4290123457,
        "MMLU_professional_accounting":0.3333333333,
        "MMLU_professional_law":0.3070404172,
        "MMLU_professional_medicine":0.4044117647,
        "MMLU_professional_psychology":0.4052287582,
        "MMLU_public_relations":0.4,
        "MMLU_security_studies":0.4285714286,
        "MMLU_sociology":0.4875621891,
        "MMLU_us_foreign_policy":0.6,
        "MMLU_virology":0.3855421687,
        "MMLU_world_religions":0.5204678363
    },
    {
        "Model":"vigogne-7b-instruct",
        "URL":"https:\/\/huggingface.co\/bofenghuang\/vigogne-7b-instruct",
        "full_model_name":"bofenghuang\/vigogne-7b-instruct",
        "Parameters":7.0,
        "MMLU_average":0.3843495851,
        "arc:challenge|25":0.4854948805,
        "hellaswag|10":0.58673571,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.4518518519,
        "MMLU_astronomy":0.3947368421,
        "MMLU_business_ethics":0.42,
        "MMLU_clinical_knowledge":0.4264150943,
        "MMLU_college_biology":0.3888888889,
        "MMLU_college_chemistry":0.32,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.3179190751,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.5,
        "MMLU_conceptual_physics":0.3574468085,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.3448275862,
        "MMLU_elementary_mathematics":0.2645502646,
        "MMLU_formal_logic":0.2777777778,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.4,
        "MMLU_high_school_chemistry":0.2955665025,
        "MMLU_high_school_computer_science":0.34,
        "MMLU_high_school_european_history":0.503030303,
        "MMLU_high_school_geography":0.4494949495,
        "MMLU_high_school_government_and_politics":0.5388601036,
        "MMLU_high_school_macroeconomics":0.341025641,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.3025210084,
        "MMLU_high_school_physics":0.2450331126,
        "MMLU_high_school_psychology":0.4623853211,
        "MMLU_high_school_statistics":0.3935185185,
        "MMLU_high_school_us_history":0.4509803922,
        "MMLU_high_school_world_history":0.4430379747,
        "MMLU_human_aging":0.4439461883,
        "MMLU_human_sexuality":0.3740458015,
        "MMLU_international_law":0.6033057851,
        "MMLU_jurisprudence":0.4259259259,
        "MMLU_logical_fallacies":0.4110429448,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.359223301,
        "MMLU_marketing":0.5427350427,
        "MMLU_medical_genetics":0.4,
        "MMLU_miscellaneous":0.5070242656,
        "MMLU_moral_disputes":0.4219653179,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.3496732026,
        "MMLU_philosophy":0.4276527331,
        "MMLU_prehistory":0.462962963,
        "MMLU_professional_accounting":0.3014184397,
        "MMLU_professional_law":0.3200782269,
        "MMLU_professional_medicine":0.4080882353,
        "MMLU_professional_psychology":0.3823529412,
        "MMLU_public_relations":0.4545454545,
        "MMLU_security_studies":0.3469387755,
        "MMLU_sociology":0.4328358209,
        "MMLU_us_foreign_policy":0.54,
        "MMLU_virology":0.3614457831,
        "MMLU_world_religions":0.5204678363
    },
    {
        "Model":"chinese-alpaca-plus-7b-hf",
        "URL":"https:\/\/huggingface.co\/shibing624\/chinese-alpaca-plus-7b-hf",
        "full_model_name":"shibing624\/chinese-alpaca-plus-7b-hf",
        "Parameters":7.0,
        "MMLU_average":0.3838712635,
        "arc:challenge|25":0.4530716724,
        "hellaswag|10":0.526588329,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.3925925926,
        "MMLU_astronomy":0.3289473684,
        "MMLU_business_ethics":0.39,
        "MMLU_clinical_knowledge":0.4037735849,
        "MMLU_college_biology":0.375,
        "MMLU_college_chemistry":0.23,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.289017341,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.37,
        "MMLU_conceptual_physics":0.3617021277,
        "MMLU_econometrics":0.1842105263,
        "MMLU_electrical_engineering":0.3448275862,
        "MMLU_elementary_mathematics":0.2645502646,
        "MMLU_formal_logic":0.2063492063,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.4258064516,
        "MMLU_high_school_chemistry":0.3054187192,
        "MMLU_high_school_computer_science":0.41,
        "MMLU_high_school_european_history":0.4727272727,
        "MMLU_high_school_geography":0.4595959596,
        "MMLU_high_school_government_and_politics":0.5284974093,
        "MMLU_high_school_macroeconomics":0.3487179487,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.3319327731,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.4513761468,
        "MMLU_high_school_statistics":0.4537037037,
        "MMLU_high_school_us_history":0.5,
        "MMLU_high_school_world_history":0.5105485232,
        "MMLU_human_aging":0.4573991031,
        "MMLU_human_sexuality":0.465648855,
        "MMLU_international_law":0.5867768595,
        "MMLU_jurisprudence":0.4444444444,
        "MMLU_logical_fallacies":0.4601226994,
        "MMLU_machine_learning":0.2946428571,
        "MMLU_management":0.4174757282,
        "MMLU_marketing":0.5641025641,
        "MMLU_medical_genetics":0.37,
        "MMLU_miscellaneous":0.5440613027,
        "MMLU_moral_disputes":0.3930635838,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.4248366013,
        "MMLU_philosophy":0.4276527331,
        "MMLU_prehistory":0.4351851852,
        "MMLU_professional_accounting":0.3085106383,
        "MMLU_professional_law":0.3044328553,
        "MMLU_professional_medicine":0.3198529412,
        "MMLU_professional_psychology":0.3954248366,
        "MMLU_public_relations":0.4272727273,
        "MMLU_security_studies":0.4081632653,
        "MMLU_sociology":0.4129353234,
        "MMLU_us_foreign_policy":0.57,
        "MMLU_virology":0.421686747,
        "MMLU_world_religions":0.4795321637
    },
    {
        "Model":"wizardLM-7B-HF",
        "URL":"https:\/\/huggingface.co\/TheBloke\/wizardLM-7B-HF",
        "full_model_name":"TheBloke\/wizardLM-7B-HF",
        "Parameters":7.0,
        "MMLU_average":0.3807240327,
        "arc:challenge|25":0.4846416382,
        "hellaswag|10":0.5685122486,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.437037037,
        "MMLU_astronomy":0.4013157895,
        "MMLU_business_ethics":0.48,
        "MMLU_clinical_knowledge":0.4377358491,
        "MMLU_college_biology":0.3680555556,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.29,
        "MMLU_college_mathematics":0.23,
        "MMLU_college_medicine":0.3526011561,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.49,
        "MMLU_conceptual_physics":0.4,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.324137931,
        "MMLU_elementary_mathematics":0.3068783069,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.3612903226,
        "MMLU_high_school_chemistry":0.3004926108,
        "MMLU_high_school_computer_science":0.34,
        "MMLU_high_school_european_history":0.4545454545,
        "MMLU_high_school_geography":0.4242424242,
        "MMLU_high_school_government_and_politics":0.4663212435,
        "MMLU_high_school_macroeconomics":0.3538461538,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.3277310924,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.4660550459,
        "MMLU_high_school_statistics":0.2175925926,
        "MMLU_high_school_us_history":0.4558823529,
        "MMLU_high_school_world_history":0.4303797468,
        "MMLU_human_aging":0.5112107623,
        "MMLU_human_sexuality":0.3893129771,
        "MMLU_international_law":0.5785123967,
        "MMLU_jurisprudence":0.4537037037,
        "MMLU_logical_fallacies":0.3987730061,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.359223301,
        "MMLU_marketing":0.5170940171,
        "MMLU_medical_genetics":0.42,
        "MMLU_miscellaneous":0.5453384419,
        "MMLU_moral_disputes":0.3843930636,
        "MMLU_moral_scenarios":0.2301675978,
        "MMLU_nutrition":0.408496732,
        "MMLU_philosophy":0.38585209,
        "MMLU_prehistory":0.3981481481,
        "MMLU_professional_accounting":0.3191489362,
        "MMLU_professional_law":0.3220338983,
        "MMLU_professional_medicine":0.3860294118,
        "MMLU_professional_psychology":0.4003267974,
        "MMLU_public_relations":0.4181818182,
        "MMLU_security_studies":0.3142857143,
        "MMLU_sociology":0.4776119403,
        "MMLU_us_foreign_policy":0.55,
        "MMLU_virology":0.3915662651,
        "MMLU_world_religions":0.5380116959
    },
    {
        "Model":"galactica-6.7b-finetuned",
        "URL":"https:\/\/huggingface.co\/OpenAssistant\/galactica-6.7b-finetuned",
        "full_model_name":"OpenAssistant\/galactica-6.7b-finetuned",
        "Parameters":6.7,
        "MMLU_average":0.3803292411,
        "arc:challenge|25":0.3788395904,
        "hellaswag|10":0.3993228441,
        "MMLU_abstract_algebra":0.41,
        "MMLU_anatomy":0.4888888889,
        "MMLU_astronomy":0.4078947368,
        "MMLU_business_ethics":0.32,
        "MMLU_clinical_knowledge":0.441509434,
        "MMLU_college_biology":0.4583333333,
        "MMLU_college_chemistry":0.44,
        "MMLU_college_computer_science":0.28,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.3641618497,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.46,
        "MMLU_conceptual_physics":0.3276595745,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.4137931034,
        "MMLU_elementary_mathematics":0.2777777778,
        "MMLU_formal_logic":0.2698412698,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.4806451613,
        "MMLU_high_school_chemistry":0.3201970443,
        "MMLU_high_school_computer_science":0.41,
        "MMLU_high_school_european_history":0.4242424242,
        "MMLU_high_school_geography":0.4696969697,
        "MMLU_high_school_government_and_politics":0.4766839378,
        "MMLU_high_school_macroeconomics":0.3743589744,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.3697478992,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.4770642202,
        "MMLU_high_school_statistics":0.3657407407,
        "MMLU_high_school_us_history":0.3137254902,
        "MMLU_high_school_world_history":0.4345991561,
        "MMLU_human_aging":0.4035874439,
        "MMLU_human_sexuality":0.427480916,
        "MMLU_international_law":0.479338843,
        "MMLU_jurisprudence":0.4074074074,
        "MMLU_logical_fallacies":0.3619631902,
        "MMLU_machine_learning":0.2946428571,
        "MMLU_management":0.4368932039,
        "MMLU_marketing":0.3290598291,
        "MMLU_medical_genetics":0.47,
        "MMLU_miscellaneous":0.4125159642,
        "MMLU_moral_disputes":0.387283237,
        "MMLU_moral_scenarios":0.2592178771,
        "MMLU_nutrition":0.4346405229,
        "MMLU_philosophy":0.4051446945,
        "MMLU_prehistory":0.4043209877,
        "MMLU_professional_accounting":0.2730496454,
        "MMLU_professional_law":0.3155149935,
        "MMLU_professional_medicine":0.4411764706,
        "MMLU_professional_psychology":0.3447712418,
        "MMLU_public_relations":0.3727272727,
        "MMLU_security_studies":0.3428571429,
        "MMLU_sociology":0.4328358209,
        "MMLU_us_foreign_policy":0.5,
        "MMLU_virology":0.4578313253,
        "MMLU_world_religions":0.269005848
    },
    {
        "Model":"preded-title-amazongoogle-abtbuy",
        "URL":"https:\/\/huggingface.co\/u-chom\/preded-title-amazongoogle-abtbuy",
        "full_model_name":"u-chom\/preded-title-amazongoogle-abtbuy",
        "Parameters":null,
        "MMLU_average":0.3798981452,
        "arc:challenge|25":0.4667235495,
        "hellaswag|10":0.5873332006,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4222222222,
        "MMLU_astronomy":0.3486842105,
        "MMLU_business_ethics":0.43,
        "MMLU_clinical_knowledge":0.358490566,
        "MMLU_college_biology":0.3888888889,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.36,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.323699422,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.53,
        "MMLU_conceptual_physics":0.3829787234,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.3586206897,
        "MMLU_elementary_mathematics":0.2116402116,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.29,
        "MMLU_high_school_biology":0.3516129032,
        "MMLU_high_school_chemistry":0.2413793103,
        "MMLU_high_school_computer_science":0.36,
        "MMLU_high_school_european_history":0.4484848485,
        "MMLU_high_school_geography":0.404040404,
        "MMLU_high_school_government_and_politics":0.5492227979,
        "MMLU_high_school_macroeconomics":0.3256410256,
        "MMLU_high_school_mathematics":0.2185185185,
        "MMLU_high_school_microeconomics":0.3445378151,
        "MMLU_high_school_physics":0.2251655629,
        "MMLU_high_school_psychology":0.4385321101,
        "MMLU_high_school_statistics":0.1712962963,
        "MMLU_high_school_us_history":0.4019607843,
        "MMLU_high_school_world_history":0.388185654,
        "MMLU_human_aging":0.4798206278,
        "MMLU_human_sexuality":0.4198473282,
        "MMLU_international_law":0.5123966942,
        "MMLU_jurisprudence":0.4444444444,
        "MMLU_logical_fallacies":0.4049079755,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.4077669903,
        "MMLU_marketing":0.6367521368,
        "MMLU_medical_genetics":0.45,
        "MMLU_miscellaneous":0.5491698595,
        "MMLU_moral_disputes":0.4219653179,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.4052287582,
        "MMLU_philosophy":0.5080385852,
        "MMLU_prehistory":0.4320987654,
        "MMLU_professional_accounting":0.3014184397,
        "MMLU_professional_law":0.3357235984,
        "MMLU_professional_medicine":0.2463235294,
        "MMLU_professional_psychology":0.4035947712,
        "MMLU_public_relations":0.4636363636,
        "MMLU_security_studies":0.2653061224,
        "MMLU_sociology":0.5572139303,
        "MMLU_us_foreign_policy":0.55,
        "MMLU_virology":0.3855421687,
        "MMLU_world_religions":0.6140350877
    },
    {
        "Model":"mpt-7b-chat",
        "URL":"https:\/\/huggingface.co\/mosaicml\/mpt-7b-chat",
        "full_model_name":"mosaicml\/mpt-7b-chat",
        "Parameters":7.0,
        "MMLU_average":0.3762359444,
        "arc:challenge|25":0.4317406143,
        "hellaswag|10":0.5710017925,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.3925925926,
        "MMLU_astronomy":0.3684210526,
        "MMLU_business_ethics":0.39,
        "MMLU_clinical_knowledge":0.3849056604,
        "MMLU_college_biology":0.3888888889,
        "MMLU_college_chemistry":0.32,
        "MMLU_college_computer_science":0.35,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.3294797688,
        "MMLU_college_physics":0.1764705882,
        "MMLU_computer_security":0.44,
        "MMLU_conceptual_physics":0.370212766,
        "MMLU_econometrics":0.2192982456,
        "MMLU_electrical_engineering":0.4551724138,
        "MMLU_elementary_mathematics":0.2433862434,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.26,
        "MMLU_high_school_biology":0.4032258065,
        "MMLU_high_school_chemistry":0.2413793103,
        "MMLU_high_school_computer_science":0.41,
        "MMLU_high_school_european_history":0.3212121212,
        "MMLU_high_school_geography":0.3636363636,
        "MMLU_high_school_government_and_politics":0.4922279793,
        "MMLU_high_school_macroeconomics":0.3871794872,
        "MMLU_high_school_mathematics":0.2185185185,
        "MMLU_high_school_microeconomics":0.3907563025,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.5100917431,
        "MMLU_high_school_statistics":0.3518518519,
        "MMLU_high_school_us_history":0.3284313725,
        "MMLU_high_school_world_history":0.4092827004,
        "MMLU_human_aging":0.4843049327,
        "MMLU_human_sexuality":0.465648855,
        "MMLU_international_law":0.4049586777,
        "MMLU_jurisprudence":0.4074074074,
        "MMLU_logical_fallacies":0.2883435583,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.3689320388,
        "MMLU_marketing":0.5512820513,
        "MMLU_medical_genetics":0.47,
        "MMLU_miscellaneous":0.5325670498,
        "MMLU_moral_disputes":0.4104046243,
        "MMLU_moral_scenarios":0.2413407821,
        "MMLU_nutrition":0.4019607843,
        "MMLU_philosophy":0.4083601286,
        "MMLU_prehistory":0.3672839506,
        "MMLU_professional_accounting":0.2695035461,
        "MMLU_professional_law":0.2907431551,
        "MMLU_professional_medicine":0.3897058824,
        "MMLU_professional_psychology":0.3202614379,
        "MMLU_public_relations":0.4636363636,
        "MMLU_security_studies":0.4857142857,
        "MMLU_sociology":0.4825870647,
        "MMLU_us_foreign_policy":0.57,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.4970760234
    },
    {
        "Model":"RedPajama-INCITE-Instruct-7B-v0.1",
        "URL":"https:\/\/huggingface.co\/togethercomputer\/RedPajama-INCITE-Instruct-7B-v0.1",
        "full_model_name":"togethercomputer\/RedPajama-INCITE-Instruct-7B-v0.1",
        "Parameters":7.0,
        "MMLU_average":0.3761836019,
        "arc:challenge|25":0.4138225256,
        "hellaswag|10":0.5320653256,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.4518518519,
        "MMLU_astronomy":0.3947368421,
        "MMLU_business_ethics":0.41,
        "MMLU_clinical_knowledge":0.4339622642,
        "MMLU_college_biology":0.3958333333,
        "MMLU_college_chemistry":0.29,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.3468208092,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.54,
        "MMLU_conceptual_physics":0.2978723404,
        "MMLU_econometrics":0.2105263158,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.291005291,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.26,
        "MMLU_high_school_biology":0.4290322581,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.4121212121,
        "MMLU_high_school_geography":0.4141414141,
        "MMLU_high_school_government_and_politics":0.5336787565,
        "MMLU_high_school_macroeconomics":0.3820512821,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.3403361345,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.4678899083,
        "MMLU_high_school_statistics":0.1990740741,
        "MMLU_high_school_us_history":0.4607843137,
        "MMLU_high_school_world_history":0.4725738397,
        "MMLU_human_aging":0.5067264574,
        "MMLU_human_sexuality":0.3893129771,
        "MMLU_international_law":0.5537190083,
        "MMLU_jurisprudence":0.4074074074,
        "MMLU_logical_fallacies":0.4049079755,
        "MMLU_machine_learning":0.2946428571,
        "MMLU_management":0.4466019417,
        "MMLU_marketing":0.5256410256,
        "MMLU_medical_genetics":0.42,
        "MMLU_miscellaneous":0.533844189,
        "MMLU_moral_disputes":0.3930635838,
        "MMLU_moral_scenarios":0.243575419,
        "MMLU_nutrition":0.3594771242,
        "MMLU_philosophy":0.3922829582,
        "MMLU_prehistory":0.4228395062,
        "MMLU_professional_accounting":0.3262411348,
        "MMLU_professional_law":0.3194263364,
        "MMLU_professional_medicine":0.3014705882,
        "MMLU_professional_psychology":0.3839869281,
        "MMLU_public_relations":0.5,
        "MMLU_security_studies":0.2571428571,
        "MMLU_sociology":0.4278606965,
        "MMLU_us_foreign_policy":0.52,
        "MMLU_virology":0.3192771084,
        "MMLU_world_religions":0.514619883
    },
    {
        "Model":"autotrain-llama-alpaca-peft-52508123785",
        "URL":"https:\/\/huggingface.co\/abhishek\/autotrain-llama-alpaca-peft-52508123785",
        "full_model_name":"abhishek\/autotrain-llama-alpaca-peft-52508123785",
        "Parameters":null,
        "MMLU_average":0.3760057766,
        "arc:challenge|25":0.4931740614,
        "hellaswag|10":0.5831507668,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4,
        "MMLU_astronomy":0.4144736842,
        "MMLU_business_ethics":0.35,
        "MMLU_clinical_knowledge":0.4113207547,
        "MMLU_college_biology":0.3958333333,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.3005780347,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.45,
        "MMLU_conceptual_physics":0.3361702128,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.3586206897,
        "MMLU_elementary_mathematics":0.2354497354,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.4096774194,
        "MMLU_high_school_chemistry":0.3349753695,
        "MMLU_high_school_computer_science":0.36,
        "MMLU_high_school_european_history":0.3878787879,
        "MMLU_high_school_geography":0.3636363636,
        "MMLU_high_school_government_and_politics":0.5492227979,
        "MMLU_high_school_macroeconomics":0.3051282051,
        "MMLU_high_school_mathematics":0.2296296296,
        "MMLU_high_school_microeconomics":0.3025210084,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.4330275229,
        "MMLU_high_school_statistics":0.4166666667,
        "MMLU_high_school_us_history":0.3970588235,
        "MMLU_high_school_world_history":0.4388185654,
        "MMLU_human_aging":0.4977578475,
        "MMLU_human_sexuality":0.4122137405,
        "MMLU_international_law":0.520661157,
        "MMLU_jurisprudence":0.3981481481,
        "MMLU_logical_fallacies":0.4294478528,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.3786407767,
        "MMLU_marketing":0.4401709402,
        "MMLU_medical_genetics":0.41,
        "MMLU_miscellaneous":0.4712643678,
        "MMLU_moral_disputes":0.3699421965,
        "MMLU_moral_scenarios":0.243575419,
        "MMLU_nutrition":0.4215686275,
        "MMLU_philosophy":0.421221865,
        "MMLU_prehistory":0.4043209877,
        "MMLU_professional_accounting":0.3191489362,
        "MMLU_professional_law":0.3181225554,
        "MMLU_professional_medicine":0.4595588235,
        "MMLU_professional_psychology":0.3692810458,
        "MMLU_public_relations":0.3818181818,
        "MMLU_security_studies":0.4408163265,
        "MMLU_sociology":0.4378109453,
        "MMLU_us_foreign_policy":0.46,
        "MMLU_virology":0.3373493976,
        "MMLU_world_religions":0.5087719298
    },
    {
        "Model":"AlpacaGPT4-7B-elina",
        "URL":"https:\/\/huggingface.co\/LLMs\/AlpacaGPT4-7B-elina",
        "full_model_name":"LLMs\/AlpacaGPT4-7B-elina",
        "Parameters":7.0,
        "MMLU_average":0.3749994047,
        "arc:challenge|25":0.5153583618,
        "hellaswag|10":0.5927106154,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4222222222,
        "MMLU_astronomy":0.3947368421,
        "MMLU_business_ethics":0.46,
        "MMLU_clinical_knowledge":0.4226415094,
        "MMLU_college_biology":0.3819444444,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.36,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.3005780347,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.46,
        "MMLU_conceptual_physics":0.3617021277,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.275862069,
        "MMLU_elementary_mathematics":0.2645502646,
        "MMLU_formal_logic":0.2222222222,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.3774193548,
        "MMLU_high_school_chemistry":0.2660098522,
        "MMLU_high_school_computer_science":0.38,
        "MMLU_high_school_european_history":0.4363636364,
        "MMLU_high_school_geography":0.398989899,
        "MMLU_high_school_government_and_politics":0.5233160622,
        "MMLU_high_school_macroeconomics":0.358974359,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.3319327731,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.4733944954,
        "MMLU_high_school_statistics":0.3472222222,
        "MMLU_high_school_us_history":0.4019607843,
        "MMLU_high_school_world_history":0.4430379747,
        "MMLU_human_aging":0.4394618834,
        "MMLU_human_sexuality":0.3816793893,
        "MMLU_international_law":0.5371900826,
        "MMLU_jurisprudence":0.4074074074,
        "MMLU_logical_fallacies":0.3680981595,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.3980582524,
        "MMLU_marketing":0.5,
        "MMLU_medical_genetics":0.43,
        "MMLU_miscellaneous":0.4406130268,
        "MMLU_moral_disputes":0.3815028902,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.4117647059,
        "MMLU_philosophy":0.424437299,
        "MMLU_prehistory":0.3796296296,
        "MMLU_professional_accounting":0.3120567376,
        "MMLU_professional_law":0.2998696219,
        "MMLU_professional_medicine":0.4338235294,
        "MMLU_professional_psychology":0.3741830065,
        "MMLU_public_relations":0.4545454545,
        "MMLU_security_studies":0.3632653061,
        "MMLU_sociology":0.5074626866,
        "MMLU_us_foreign_policy":0.45,
        "MMLU_virology":0.3192771084,
        "MMLU_world_religions":0.4970760234
    },
    {
        "Model":"chinese-llama-2-7b",
        "URL":"https:\/\/huggingface.co\/ziqingyang\/chinese-llama-2-7b",
        "full_model_name":"ziqingyang\/chinese-llama-2-7b",
        "Parameters":7.0,
        "MMLU_average":0.3747497883,
        "arc:challenge|25":0.406996587,
        "hellaswag|10":0.5042820155,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.2888888889,
        "MMLU_astronomy":0.3618421053,
        "MMLU_business_ethics":0.41,
        "MMLU_clinical_knowledge":0.4264150943,
        "MMLU_college_biology":0.4027777778,
        "MMLU_college_chemistry":0.33,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.3583815029,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.42,
        "MMLU_conceptual_physics":0.3744680851,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.324137931,
        "MMLU_elementary_mathematics":0.2883597884,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.364516129,
        "MMLU_high_school_chemistry":0.2413793103,
        "MMLU_high_school_computer_science":0.34,
        "MMLU_high_school_european_history":0.3757575758,
        "MMLU_high_school_geography":0.4292929293,
        "MMLU_high_school_government_and_politics":0.4300518135,
        "MMLU_high_school_macroeconomics":0.3794871795,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.4033613445,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.4587155963,
        "MMLU_high_school_statistics":0.3009259259,
        "MMLU_high_school_us_history":0.362745098,
        "MMLU_high_school_world_history":0.3502109705,
        "MMLU_human_aging":0.3856502242,
        "MMLU_human_sexuality":0.3511450382,
        "MMLU_international_law":0.5702479339,
        "MMLU_jurisprudence":0.3888888889,
        "MMLU_logical_fallacies":0.3680981595,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.3883495146,
        "MMLU_marketing":0.5769230769,
        "MMLU_medical_genetics":0.4,
        "MMLU_miscellaneous":0.4648786718,
        "MMLU_moral_disputes":0.3786127168,
        "MMLU_moral_scenarios":0.2525139665,
        "MMLU_nutrition":0.3954248366,
        "MMLU_philosophy":0.4405144695,
        "MMLU_prehistory":0.3888888889,
        "MMLU_professional_accounting":0.2553191489,
        "MMLU_professional_law":0.2933507171,
        "MMLU_professional_medicine":0.4632352941,
        "MMLU_professional_psychology":0.318627451,
        "MMLU_public_relations":0.4818181818,
        "MMLU_security_studies":0.4040816327,
        "MMLU_sociology":0.4776119403,
        "MMLU_us_foreign_policy":0.5,
        "MMLU_virology":0.3192771084,
        "MMLU_world_religions":0.5614035088
    },
    {
        "Model":"instructmining-platypus-15k",
        "URL":"https:\/\/huggingface.co\/yihan6324\/instructmining-platypus-15k",
        "full_model_name":"yihan6324\/instructmining-platypus-15k",
        "Parameters":null,
        "MMLU_average":0.3744084688,
        "arc:challenge|25":0.5034129693,
        "hellaswag|10":0.613324039,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.4,
        "MMLU_astronomy":0.3026315789,
        "MMLU_business_ethics":0.46,
        "MMLU_clinical_knowledge":0.4830188679,
        "MMLU_college_biology":0.3819444444,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.3352601156,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.38,
        "MMLU_conceptual_physics":0.3276595745,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.275862069,
        "MMLU_elementary_mathematics":0.253968254,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.27,
        "MMLU_high_school_biology":0.4064516129,
        "MMLU_high_school_chemistry":0.3054187192,
        "MMLU_high_school_computer_science":0.38,
        "MMLU_high_school_european_history":0.4848484848,
        "MMLU_high_school_geography":0.3888888889,
        "MMLU_high_school_government_and_politics":0.518134715,
        "MMLU_high_school_macroeconomics":0.3974358974,
        "MMLU_high_school_mathematics":0.2333333333,
        "MMLU_high_school_microeconomics":0.3865546218,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.4752293578,
        "MMLU_high_school_statistics":0.3333333333,
        "MMLU_high_school_us_history":0.4215686275,
        "MMLU_high_school_world_history":0.5358649789,
        "MMLU_human_aging":0.466367713,
        "MMLU_human_sexuality":0.358778626,
        "MMLU_international_law":0.5041322314,
        "MMLU_jurisprudence":0.4537037037,
        "MMLU_logical_fallacies":0.4478527607,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.427184466,
        "MMLU_marketing":0.5,
        "MMLU_medical_genetics":0.4,
        "MMLU_miscellaneous":0.5555555556,
        "MMLU_moral_disputes":0.3901734104,
        "MMLU_moral_scenarios":0.243575419,
        "MMLU_nutrition":0.4183006536,
        "MMLU_philosophy":0.4887459807,
        "MMLU_prehistory":0.462962963,
        "MMLU_professional_accounting":0.2482269504,
        "MMLU_professional_law":0.2861799218,
        "MMLU_professional_medicine":0.2794117647,
        "MMLU_professional_psychology":0.3758169935,
        "MMLU_public_relations":0.4272727273,
        "MMLU_security_studies":0.2775510204,
        "MMLU_sociology":0.4975124378,
        "MMLU_us_foreign_policy":0.48,
        "MMLU_virology":0.3072289157,
        "MMLU_world_religions":0.4269005848
    },
    {
        "Model":"airoboros-7b",
        "URL":"https:\/\/huggingface.co\/jondurbin\/airoboros-7b",
        "full_model_name":"jondurbin\/airoboros-7b",
        "Parameters":7.0,
        "MMLU_average":0.3734016705,
        "arc:challenge|25":0.5051194539,
        "hellaswag|10":0.593706433,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.3851851852,
        "MMLU_astronomy":0.3552631579,
        "MMLU_business_ethics":0.4,
        "MMLU_clinical_knowledge":0.4377358491,
        "MMLU_college_biology":0.3819444444,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.323699422,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.46,
        "MMLU_conceptual_physics":0.3787234043,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.324137931,
        "MMLU_elementary_mathematics":0.2724867725,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.27,
        "MMLU_high_school_biology":0.3741935484,
        "MMLU_high_school_chemistry":0.3300492611,
        "MMLU_high_school_computer_science":0.31,
        "MMLU_high_school_european_history":0.4363636364,
        "MMLU_high_school_geography":0.4191919192,
        "MMLU_high_school_government_and_politics":0.481865285,
        "MMLU_high_school_macroeconomics":0.3358974359,
        "MMLU_high_school_mathematics":0.2296296296,
        "MMLU_high_school_microeconomics":0.3361344538,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.495412844,
        "MMLU_high_school_statistics":0.3148148148,
        "MMLU_high_school_us_history":0.4754901961,
        "MMLU_high_school_world_history":0.3966244726,
        "MMLU_human_aging":0.3766816143,
        "MMLU_human_sexuality":0.3893129771,
        "MMLU_international_law":0.4876033058,
        "MMLU_jurisprudence":0.3888888889,
        "MMLU_logical_fallacies":0.4294478528,
        "MMLU_machine_learning":0.2946428571,
        "MMLU_management":0.3786407767,
        "MMLU_marketing":0.4871794872,
        "MMLU_medical_genetics":0.42,
        "MMLU_miscellaneous":0.5108556833,
        "MMLU_moral_disputes":0.3843930636,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.431372549,
        "MMLU_philosophy":0.3729903537,
        "MMLU_prehistory":0.3888888889,
        "MMLU_professional_accounting":0.3156028369,
        "MMLU_professional_law":0.2966101695,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.3431372549,
        "MMLU_public_relations":0.4181818182,
        "MMLU_security_studies":0.3469387755,
        "MMLU_sociology":0.4825870647,
        "MMLU_us_foreign_policy":0.49,
        "MMLU_virology":0.3614457831,
        "MMLU_world_religions":0.5087719298
    },
    {
        "Model":"CodeLlama-34b-hf",
        "URL":"https:\/\/huggingface.co\/NousResearch\/CodeLlama-34b-hf",
        "full_model_name":"NousResearch\/CodeLlama-34b-hf",
        "Parameters":34.0,
        "MMLU_average":0.3719944391,
        "arc:challenge|25":0.3361774744,
        "hellaswag|10":0.2818163712,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.4,
        "MMLU_astronomy":0.3684210526,
        "MMLU_business_ethics":0.33,
        "MMLU_clinical_knowledge":0.3962264151,
        "MMLU_college_biology":0.3541666667,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.35,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.3930635838,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.45,
        "MMLU_conceptual_physics":0.3489361702,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.3586206897,
        "MMLU_elementary_mathematics":0.2883597884,
        "MMLU_formal_logic":0.3333333333,
        "MMLU_global_facts":0.22,
        "MMLU_high_school_biology":0.4387096774,
        "MMLU_high_school_chemistry":0.3497536946,
        "MMLU_high_school_computer_science":0.44,
        "MMLU_high_school_european_history":0.2363636364,
        "MMLU_high_school_geography":0.4747474747,
        "MMLU_high_school_government_and_politics":0.4766839378,
        "MMLU_high_school_macroeconomics":0.3846153846,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.3823529412,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.4073394495,
        "MMLU_high_school_statistics":0.337962963,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.2784810127,
        "MMLU_human_aging":0.3452914798,
        "MMLU_human_sexuality":0.3282442748,
        "MMLU_international_law":0.5289256198,
        "MMLU_jurisprudence":0.5092592593,
        "MMLU_logical_fallacies":0.3435582822,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.5048543689,
        "MMLU_marketing":0.5341880342,
        "MMLU_medical_genetics":0.39,
        "MMLU_miscellaneous":0.4929757344,
        "MMLU_moral_disputes":0.3699421965,
        "MMLU_moral_scenarios":0.2223463687,
        "MMLU_nutrition":0.3529411765,
        "MMLU_philosophy":0.4855305466,
        "MMLU_prehistory":0.3888888889,
        "MMLU_professional_accounting":0.280141844,
        "MMLU_professional_law":0.2679269883,
        "MMLU_professional_medicine":0.4375,
        "MMLU_professional_psychology":0.295751634,
        "MMLU_public_relations":0.3909090909,
        "MMLU_security_studies":0.3224489796,
        "MMLU_sociology":0.3731343284,
        "MMLU_us_foreign_policy":0.51,
        "MMLU_virology":0.3614457831,
        "MMLU_world_religions":0.6198830409
    },
    {
        "Model":"Wizard-Vicuna-7B-Uncensored-HF",
        "URL":"https:\/\/huggingface.co\/TheBloke\/Wizard-Vicuna-7B-Uncensored-HF",
        "full_model_name":"TheBloke\/Wizard-Vicuna-7B-Uncensored-HF",
        "Parameters":7.0,
        "MMLU_average":0.3708892477,
        "arc:challenge|25":0.5008532423,
        "hellaswag|10":0.605556662,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.3777777778,
        "MMLU_astronomy":0.3552631579,
        "MMLU_business_ethics":0.42,
        "MMLU_clinical_knowledge":0.4037735849,
        "MMLU_college_biology":0.3611111111,
        "MMLU_college_chemistry":0.28,
        "MMLU_college_computer_science":0.28,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.2716763006,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.5,
        "MMLU_conceptual_physics":0.4042553191,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.3172413793,
        "MMLU_elementary_mathematics":0.2380952381,
        "MMLU_formal_logic":0.2619047619,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.3774193548,
        "MMLU_high_school_chemistry":0.236453202,
        "MMLU_high_school_computer_science":0.38,
        "MMLU_high_school_european_history":0.4848484848,
        "MMLU_high_school_geography":0.4242424242,
        "MMLU_high_school_government_and_politics":0.4922279793,
        "MMLU_high_school_macroeconomics":0.3333333333,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.3193277311,
        "MMLU_high_school_physics":0.238410596,
        "MMLU_high_school_psychology":0.471559633,
        "MMLU_high_school_statistics":0.1944444444,
        "MMLU_high_school_us_history":0.4656862745,
        "MMLU_high_school_world_history":0.4810126582,
        "MMLU_human_aging":0.466367713,
        "MMLU_human_sexuality":0.3816793893,
        "MMLU_international_law":0.5950413223,
        "MMLU_jurisprudence":0.4259259259,
        "MMLU_logical_fallacies":0.3680981595,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.3495145631,
        "MMLU_marketing":0.5427350427,
        "MMLU_medical_genetics":0.48,
        "MMLU_miscellaneous":0.5312899106,
        "MMLU_moral_disputes":0.4132947977,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.3823529412,
        "MMLU_philosophy":0.3826366559,
        "MMLU_prehistory":0.4135802469,
        "MMLU_professional_accounting":0.3333333333,
        "MMLU_professional_law":0.3037809648,
        "MMLU_professional_medicine":0.4632352941,
        "MMLU_professional_psychology":0.387254902,
        "MMLU_public_relations":0.4181818182,
        "MMLU_security_studies":0.3020408163,
        "MMLU_sociology":0.3930348259,
        "MMLU_us_foreign_policy":0.49,
        "MMLU_virology":0.3192771084,
        "MMLU_world_religions":0.5263157895
    },
    {
        "Model":"test-custom-llama",
        "URL":"https:\/\/huggingface.co\/illuin\/test-custom-llama",
        "full_model_name":"illuin\/test-custom-llama",
        "Parameters":null,
        "MMLU_average":0.3661087291,
        "arc:challenge|25":0.4982935154,
        "hellaswag|10":0.5818562039,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4148148148,
        "MMLU_astronomy":0.3881578947,
        "MMLU_business_ethics":0.41,
        "MMLU_clinical_knowledge":0.3735849057,
        "MMLU_college_biology":0.4166666667,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.3,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.3005780347,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.4,
        "MMLU_conceptual_physics":0.3829787234,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.3103448276,
        "MMLU_elementary_mathematics":0.2724867725,
        "MMLU_formal_logic":0.2222222222,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.3516129032,
        "MMLU_high_school_chemistry":0.2906403941,
        "MMLU_high_school_computer_science":0.26,
        "MMLU_high_school_european_history":0.4727272727,
        "MMLU_high_school_geography":0.3686868687,
        "MMLU_high_school_government_and_politics":0.4766839378,
        "MMLU_high_school_macroeconomics":0.3666666667,
        "MMLU_high_school_mathematics":0.2222222222,
        "MMLU_high_school_microeconomics":0.3403361345,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.4348623853,
        "MMLU_high_school_statistics":0.3518518519,
        "MMLU_high_school_us_history":0.387254902,
        "MMLU_high_school_world_history":0.4303797468,
        "MMLU_human_aging":0.4394618834,
        "MMLU_human_sexuality":0.3664122137,
        "MMLU_international_law":0.5537190083,
        "MMLU_jurisprudence":0.4444444444,
        "MMLU_logical_fallacies":0.4233128834,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.3883495146,
        "MMLU_marketing":0.5042735043,
        "MMLU_medical_genetics":0.45,
        "MMLU_miscellaneous":0.4482758621,
        "MMLU_moral_disputes":0.3641618497,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.4150326797,
        "MMLU_philosophy":0.3762057878,
        "MMLU_prehistory":0.3796296296,
        "MMLU_professional_accounting":0.3156028369,
        "MMLU_professional_law":0.278357236,
        "MMLU_professional_medicine":0.4301470588,
        "MMLU_professional_psychology":0.3660130719,
        "MMLU_public_relations":0.4727272727,
        "MMLU_security_studies":0.2897959184,
        "MMLU_sociology":0.4378109453,
        "MMLU_us_foreign_policy":0.44,
        "MMLU_virology":0.3313253012,
        "MMLU_world_religions":0.5263157895
    },
    {
        "Model":"GPT-R",
        "URL":"https:\/\/huggingface.co\/digitous\/GPT-R",
        "full_model_name":"digitous\/GPT-R",
        "Parameters":null,
        "MMLU_average":0.3650240586,
        "arc:challenge|25":0.383105802,
        "hellaswag|10":0.4980083649,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.4666666667,
        "MMLU_astronomy":0.4934210526,
        "MMLU_business_ethics":0.33,
        "MMLU_clinical_knowledge":0.3886792453,
        "MMLU_college_biology":0.3472222222,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.38,
        "MMLU_college_medicine":0.3410404624,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.47,
        "MMLU_conceptual_physics":0.3234042553,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.3931034483,
        "MMLU_elementary_mathematics":0.2962962963,
        "MMLU_formal_logic":0.2222222222,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.3838709677,
        "MMLU_high_school_chemistry":0.3251231527,
        "MMLU_high_school_computer_science":0.32,
        "MMLU_high_school_european_history":0.3454545455,
        "MMLU_high_school_geography":0.4292929293,
        "MMLU_high_school_government_and_politics":0.4611398964,
        "MMLU_high_school_macroeconomics":0.3641025641,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.3655462185,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.3541284404,
        "MMLU_high_school_statistics":0.2777777778,
        "MMLU_high_school_us_history":0.362745098,
        "MMLU_high_school_world_history":0.3544303797,
        "MMLU_human_aging":0.4080717489,
        "MMLU_human_sexuality":0.3816793893,
        "MMLU_international_law":0.5041322314,
        "MMLU_jurisprudence":0.3611111111,
        "MMLU_logical_fallacies":0.3742331288,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.359223301,
        "MMLU_marketing":0.4273504274,
        "MMLU_medical_genetics":0.41,
        "MMLU_miscellaneous":0.3780332056,
        "MMLU_moral_disputes":0.3641618497,
        "MMLU_moral_scenarios":0.2446927374,
        "MMLU_nutrition":0.4281045752,
        "MMLU_philosophy":0.38585209,
        "MMLU_prehistory":0.3827160494,
        "MMLU_professional_accounting":0.2765957447,
        "MMLU_professional_law":0.3037809648,
        "MMLU_professional_medicine":0.3602941176,
        "MMLU_professional_psychology":0.3153594771,
        "MMLU_public_relations":0.4363636364,
        "MMLU_security_studies":0.4448979592,
        "MMLU_sociology":0.5174129353,
        "MMLU_us_foreign_policy":0.5,
        "MMLU_virology":0.343373494,
        "MMLU_world_religions":0.4152046784
    },
    {
        "Model":"CodeLlama-7b-Instruct-hf",
        "URL":"https:\/\/huggingface.co\/codellama\/CodeLlama-7b-Instruct-hf",
        "full_model_name":"codellama\/CodeLlama-7b-Instruct-hf",
        "Parameters":7.0,
        "MMLU_average":0.3648418531,
        "arc:challenge|25":0.3566552901,
        "hellaswag|10":0.4442342163,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.3703703704,
        "MMLU_astronomy":0.3026315789,
        "MMLU_business_ethics":0.35,
        "MMLU_clinical_knowledge":0.3622641509,
        "MMLU_college_biology":0.3402777778,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.38,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.2774566474,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.46,
        "MMLU_conceptual_physics":0.3744680851,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.4,
        "MMLU_elementary_mathematics":0.2962962963,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.3806451613,
        "MMLU_high_school_chemistry":0.2413793103,
        "MMLU_high_school_computer_science":0.37,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.4747474747,
        "MMLU_high_school_government_and_politics":0.414507772,
        "MMLU_high_school_macroeconomics":0.3179487179,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.3655462185,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.4110091743,
        "MMLU_high_school_statistics":0.2916666667,
        "MMLU_high_school_us_history":0.3431372549,
        "MMLU_high_school_world_history":0.3459915612,
        "MMLU_human_aging":0.4349775785,
        "MMLU_human_sexuality":0.3969465649,
        "MMLU_international_law":0.4462809917,
        "MMLU_jurisprudence":0.3888888889,
        "MMLU_logical_fallacies":0.3987730061,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.4660194175,
        "MMLU_marketing":0.594017094,
        "MMLU_medical_genetics":0.41,
        "MMLU_miscellaneous":0.4584929757,
        "MMLU_moral_disputes":0.3612716763,
        "MMLU_moral_scenarios":0.2402234637,
        "MMLU_nutrition":0.3692810458,
        "MMLU_philosophy":0.3794212219,
        "MMLU_prehistory":0.4135802469,
        "MMLU_professional_accounting":0.2695035461,
        "MMLU_professional_law":0.2809647979,
        "MMLU_professional_medicine":0.3860294118,
        "MMLU_professional_psychology":0.3022875817,
        "MMLU_public_relations":0.5090909091,
        "MMLU_security_studies":0.387755102,
        "MMLU_sociology":0.4527363184,
        "MMLU_us_foreign_policy":0.48,
        "MMLU_virology":0.3674698795,
        "MMLU_world_religions":0.4912280702
    },
    {
        "Model":"yayi-7b",
        "URL":"https:\/\/huggingface.co\/wenge-research\/yayi-7b",
        "full_model_name":"wenge-research\/yayi-7b",
        "Parameters":7.0,
        "MMLU_average":0.3634053125,
        "arc:challenge|25":0.4146757679,
        "hellaswag|10":0.4647480582,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.3703703704,
        "MMLU_astronomy":0.2434210526,
        "MMLU_business_ethics":0.45,
        "MMLU_clinical_knowledge":0.4037735849,
        "MMLU_college_biology":0.3125,
        "MMLU_college_chemistry":0.27,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.42,
        "MMLU_college_medicine":0.3294797688,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.38,
        "MMLU_conceptual_physics":0.3914893617,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.3310344828,
        "MMLU_elementary_mathematics":0.3095238095,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.23,
        "MMLU_high_school_biology":0.335483871,
        "MMLU_high_school_chemistry":0.3251231527,
        "MMLU_high_school_computer_science":0.48,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.4393939394,
        "MMLU_high_school_government_and_politics":0.3886010363,
        "MMLU_high_school_macroeconomics":0.3179487179,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.3739495798,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.4752293578,
        "MMLU_high_school_statistics":0.2546296296,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.4894514768,
        "MMLU_human_aging":0.4573991031,
        "MMLU_human_sexuality":0.3893129771,
        "MMLU_international_law":0.4049586777,
        "MMLU_jurisprudence":0.4722222222,
        "MMLU_logical_fallacies":0.3496932515,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.3398058252,
        "MMLU_marketing":0.5897435897,
        "MMLU_medical_genetics":0.45,
        "MMLU_miscellaneous":0.4636015326,
        "MMLU_moral_disputes":0.4190751445,
        "MMLU_moral_scenarios":0.2324022346,
        "MMLU_nutrition":0.3692810458,
        "MMLU_philosophy":0.3729903537,
        "MMLU_prehistory":0.3765432099,
        "MMLU_professional_accounting":0.304964539,
        "MMLU_professional_law":0.2666232073,
        "MMLU_professional_medicine":0.3860294118,
        "MMLU_professional_psychology":0.3807189542,
        "MMLU_public_relations":0.5272727273,
        "MMLU_security_studies":0.3469387755,
        "MMLU_sociology":0.3432835821,
        "MMLU_us_foreign_policy":0.49,
        "MMLU_virology":0.3614457831,
        "MMLU_world_religions":0.4035087719
    },
    {
        "Model":"llama7b_alpaca_1gpu_bf16",
        "URL":"https:\/\/huggingface.co\/DevaMalla\/llama7b_alpaca_1gpu_bf16",
        "full_model_name":"DevaMalla\/llama7b_alpaca_1gpu_bf16",
        "Parameters":7.0,
        "MMLU_average":0.3626197327,
        "arc:challenge|25":0.4940273038,
        "hellaswag|10":0.5932085242,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.362962963,
        "MMLU_astronomy":0.3684210526,
        "MMLU_business_ethics":0.41,
        "MMLU_clinical_knowledge":0.3735849057,
        "MMLU_college_biology":0.375,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.3526011561,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.41,
        "MMLU_conceptual_physics":0.3489361702,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.2689655172,
        "MMLU_elementary_mathematics":0.2777777778,
        "MMLU_formal_logic":0.2301587302,
        "MMLU_global_facts":0.29,
        "MMLU_high_school_biology":0.3548387097,
        "MMLU_high_school_chemistry":0.2807881773,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.4727272727,
        "MMLU_high_school_geography":0.3838383838,
        "MMLU_high_school_government_and_politics":0.481865285,
        "MMLU_high_school_macroeconomics":0.3641025641,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.3235294118,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.4788990826,
        "MMLU_high_school_statistics":0.337962963,
        "MMLU_high_school_us_history":0.3774509804,
        "MMLU_high_school_world_history":0.4599156118,
        "MMLU_human_aging":0.3766816143,
        "MMLU_human_sexuality":0.3435114504,
        "MMLU_international_law":0.5537190083,
        "MMLU_jurisprudence":0.3888888889,
        "MMLU_logical_fallacies":0.4110429448,
        "MMLU_machine_learning":0.2232142857,
        "MMLU_management":0.3689320388,
        "MMLU_marketing":0.5085470085,
        "MMLU_medical_genetics":0.44,
        "MMLU_miscellaneous":0.441890166,
        "MMLU_moral_disputes":0.4075144509,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.4019607843,
        "MMLU_philosophy":0.3794212219,
        "MMLU_prehistory":0.3641975309,
        "MMLU_professional_accounting":0.2907801418,
        "MMLU_professional_law":0.2985658409,
        "MMLU_professional_medicine":0.4411764706,
        "MMLU_professional_psychology":0.3660130719,
        "MMLU_public_relations":0.3909090909,
        "MMLU_security_studies":0.3265306122,
        "MMLU_sociology":0.4527363184,
        "MMLU_us_foreign_policy":0.44,
        "MMLU_virology":0.3674698795,
        "MMLU_world_religions":0.4795321637
    },
    {
        "Model":"scarlett-7b",
        "URL":"https:\/\/huggingface.co\/ajibawa-2023\/scarlett-7b",
        "full_model_name":"ajibawa-2023\/scarlett-7b",
        "Parameters":7.0,
        "MMLU_average":0.3611069786,
        "arc:challenge|25":0.5366894198,
        "hellaswag|10":0.6256721769,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.4074074074,
        "MMLU_astronomy":0.3618421053,
        "MMLU_business_ethics":0.42,
        "MMLU_clinical_knowledge":0.4113207547,
        "MMLU_college_biology":0.3541666667,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.3,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.323699422,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.47,
        "MMLU_conceptual_physics":0.3404255319,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.3034482759,
        "MMLU_elementary_mathematics":0.2645502646,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.3322580645,
        "MMLU_high_school_chemistry":0.2561576355,
        "MMLU_high_school_computer_science":0.35,
        "MMLU_high_school_european_history":0.4606060606,
        "MMLU_high_school_geography":0.4191919192,
        "MMLU_high_school_government_and_politics":0.5336787565,
        "MMLU_high_school_macroeconomics":0.3538461538,
        "MMLU_high_school_mathematics":0.2222222222,
        "MMLU_high_school_microeconomics":0.3067226891,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.480733945,
        "MMLU_high_school_statistics":0.3333333333,
        "MMLU_high_school_us_history":0.387254902,
        "MMLU_high_school_world_history":0.4514767932,
        "MMLU_human_aging":0.4215246637,
        "MMLU_human_sexuality":0.3435114504,
        "MMLU_international_law":0.4545454545,
        "MMLU_jurisprudence":0.4166666667,
        "MMLU_logical_fallacies":0.3312883436,
        "MMLU_machine_learning":0.2321428571,
        "MMLU_management":0.3980582524,
        "MMLU_marketing":0.5,
        "MMLU_medical_genetics":0.39,
        "MMLU_miscellaneous":0.4674329502,
        "MMLU_moral_disputes":0.4104046243,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.3921568627,
        "MMLU_philosophy":0.4180064309,
        "MMLU_prehistory":0.3981481481,
        "MMLU_professional_accounting":0.280141844,
        "MMLU_professional_law":0.3083441982,
        "MMLU_professional_medicine":0.4411764706,
        "MMLU_professional_psychology":0.3758169935,
        "MMLU_public_relations":0.4181818182,
        "MMLU_security_studies":0.293877551,
        "MMLU_sociology":0.4577114428,
        "MMLU_us_foreign_policy":0.41,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.4561403509
    },
    {
        "Model":"pygmalion-instruct",
        "URL":"https:\/\/huggingface.co\/AlpinDale\/pygmalion-instruct",
        "full_model_name":"AlpinDale\/pygmalion-instruct",
        "Parameters":null,
        "MMLU_average":0.35935578,
        "arc:challenge|25":0.5051194539,
        "hellaswag|10":0.5846444931,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.4,
        "MMLU_astronomy":0.3289473684,
        "MMLU_business_ethics":0.39,
        "MMLU_clinical_knowledge":0.4452830189,
        "MMLU_college_biology":0.3194444444,
        "MMLU_college_chemistry":0.25,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.2716763006,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.47,
        "MMLU_conceptual_physics":0.2978723404,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2965517241,
        "MMLU_elementary_mathematics":0.2513227513,
        "MMLU_formal_logic":0.2301587302,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.3612903226,
        "MMLU_high_school_chemistry":0.2955665025,
        "MMLU_high_school_computer_science":0.37,
        "MMLU_high_school_european_history":0.3939393939,
        "MMLU_high_school_geography":0.4292929293,
        "MMLU_high_school_government_and_politics":0.5077720207,
        "MMLU_high_school_macroeconomics":0.3615384615,
        "MMLU_high_school_mathematics":0.237037037,
        "MMLU_high_school_microeconomics":0.2941176471,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.4697247706,
        "MMLU_high_school_statistics":0.2916666667,
        "MMLU_high_school_us_history":0.4019607843,
        "MMLU_high_school_world_history":0.3924050633,
        "MMLU_human_aging":0.4080717489,
        "MMLU_human_sexuality":0.3664122137,
        "MMLU_international_law":0.5454545455,
        "MMLU_jurisprudence":0.3518518519,
        "MMLU_logical_fallacies":0.4171779141,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.3689320388,
        "MMLU_marketing":0.4914529915,
        "MMLU_medical_genetics":0.39,
        "MMLU_miscellaneous":0.4814814815,
        "MMLU_moral_disputes":0.4075144509,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.3725490196,
        "MMLU_philosophy":0.4083601286,
        "MMLU_prehistory":0.3858024691,
        "MMLU_professional_accounting":0.2695035461,
        "MMLU_professional_law":0.3037809648,
        "MMLU_professional_medicine":0.3933823529,
        "MMLU_professional_psychology":0.364379085,
        "MMLU_public_relations":0.4363636364,
        "MMLU_security_studies":0.3102040816,
        "MMLU_sociology":0.4129353234,
        "MMLU_us_foreign_policy":0.41,
        "MMLU_virology":0.3674698795,
        "MMLU_world_religions":0.4795321637
    },
    {
        "Model":"Pygmalion_AlpacaLora-7b",
        "URL":"https:\/\/huggingface.co\/TehVenom\/Pygmalion_AlpacaLora-7b",
        "full_model_name":"TehVenom\/Pygmalion_AlpacaLora-7b",
        "Parameters":7.0,
        "MMLU_average":0.3591793806,
        "arc:challenge|25":0.4889078498,
        "hellaswag|10":0.5796654053,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.4296296296,
        "MMLU_astronomy":0.3486842105,
        "MMLU_business_ethics":0.37,
        "MMLU_clinical_knowledge":0.3773584906,
        "MMLU_college_biology":0.3472222222,
        "MMLU_college_chemistry":0.27,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.3641618497,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.45,
        "MMLU_conceptual_physics":0.3574468085,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.324137931,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.2380952381,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.3225806452,
        "MMLU_high_school_chemistry":0.2512315271,
        "MMLU_high_school_computer_science":0.29,
        "MMLU_high_school_european_history":0.4060606061,
        "MMLU_high_school_geography":0.398989899,
        "MMLU_high_school_government_and_politics":0.4870466321,
        "MMLU_high_school_macroeconomics":0.3717948718,
        "MMLU_high_school_mathematics":0.2333333333,
        "MMLU_high_school_microeconomics":0.3487394958,
        "MMLU_high_school_physics":0.2317880795,
        "MMLU_high_school_psychology":0.4348623853,
        "MMLU_high_school_statistics":0.2731481481,
        "MMLU_high_school_us_history":0.3921568627,
        "MMLU_high_school_world_history":0.4430379747,
        "MMLU_human_aging":0.4304932735,
        "MMLU_human_sexuality":0.3511450382,
        "MMLU_international_law":0.5041322314,
        "MMLU_jurisprudence":0.3981481481,
        "MMLU_logical_fallacies":0.3435582822,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.4077669903,
        "MMLU_marketing":0.4572649573,
        "MMLU_medical_genetics":0.45,
        "MMLU_miscellaneous":0.4610472542,
        "MMLU_moral_disputes":0.3815028902,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.3137254902,
        "MMLU_philosophy":0.3633440514,
        "MMLU_prehistory":0.4166666667,
        "MMLU_professional_accounting":0.2907801418,
        "MMLU_professional_law":0.2868318123,
        "MMLU_professional_medicine":0.4044117647,
        "MMLU_professional_psychology":0.3513071895,
        "MMLU_public_relations":0.4818181818,
        "MMLU_security_studies":0.3224489796,
        "MMLU_sociology":0.3731343284,
        "MMLU_us_foreign_policy":0.49,
        "MMLU_virology":0.343373494,
        "MMLU_world_religions":0.5321637427
    },
    {
        "Model":"effi-7b",
        "URL":"https:\/\/huggingface.co\/aiplanet\/effi-7b",
        "full_model_name":"aiplanet\/effi-7b",
        "Parameters":7.0,
        "MMLU_average":0.3591437561,
        "arc:challenge|25":0.502559727,
        "hellaswag|10":0.5952997411,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.3851851852,
        "MMLU_astronomy":0.3815789474,
        "MMLU_business_ethics":0.33,
        "MMLU_clinical_knowledge":0.3622641509,
        "MMLU_college_biology":0.3680555556,
        "MMLU_college_chemistry":0.32,
        "MMLU_college_computer_science":0.34,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.3063583815,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.37,
        "MMLU_conceptual_physics":0.2553191489,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.3448275862,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.2380952381,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.3161290323,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.39,
        "MMLU_high_school_european_history":0.4303030303,
        "MMLU_high_school_geography":0.4949494949,
        "MMLU_high_school_government_and_politics":0.5129533679,
        "MMLU_high_school_macroeconomics":0.3435897436,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.2731092437,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.4587155963,
        "MMLU_high_school_statistics":0.3472222222,
        "MMLU_high_school_us_history":0.4460784314,
        "MMLU_high_school_world_history":0.417721519,
        "MMLU_human_aging":0.3677130045,
        "MMLU_human_sexuality":0.3511450382,
        "MMLU_international_law":0.4214876033,
        "MMLU_jurisprudence":0.3611111111,
        "MMLU_logical_fallacies":0.3742331288,
        "MMLU_machine_learning":0.1875,
        "MMLU_management":0.4660194175,
        "MMLU_marketing":0.5,
        "MMLU_medical_genetics":0.42,
        "MMLU_miscellaneous":0.4878671775,
        "MMLU_moral_disputes":0.3699421965,
        "MMLU_moral_scenarios":0.243575419,
        "MMLU_nutrition":0.3823529412,
        "MMLU_philosophy":0.3987138264,
        "MMLU_prehistory":0.4228395062,
        "MMLU_professional_accounting":0.2872340426,
        "MMLU_professional_law":0.3044328553,
        "MMLU_professional_medicine":0.4264705882,
        "MMLU_professional_psychology":0.3316993464,
        "MMLU_public_relations":0.3272727273,
        "MMLU_security_studies":0.3591836735,
        "MMLU_sociology":0.447761194,
        "MMLU_us_foreign_policy":0.5,
        "MMLU_virology":0.3554216867,
        "MMLU_world_religions":0.4385964912
    },
    {
        "Model":"metharme-7b",
        "URL":"https:\/\/huggingface.co\/Neko-Institute-of-Science\/metharme-7b",
        "full_model_name":"Neko-Institute-of-Science\/metharme-7b",
        "Parameters":7.0,
        "MMLU_average":0.3591045322,
        "arc:challenge|25":0.502559727,
        "hellaswag|10":0.5842461661,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4296296296,
        "MMLU_astronomy":0.3223684211,
        "MMLU_business_ethics":0.44,
        "MMLU_clinical_knowledge":0.3962264151,
        "MMLU_college_biology":0.3958333333,
        "MMLU_college_chemistry":0.25,
        "MMLU_college_computer_science":0.3,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.323699422,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.43,
        "MMLU_conceptual_physics":0.3617021277,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.2896551724,
        "MMLU_elementary_mathematics":0.2883597884,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.335483871,
        "MMLU_high_school_chemistry":0.2463054187,
        "MMLU_high_school_computer_science":0.28,
        "MMLU_high_school_european_history":0.4848484848,
        "MMLU_high_school_geography":0.3282828283,
        "MMLU_high_school_government_and_politics":0.4352331606,
        "MMLU_high_school_macroeconomics":0.3384615385,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.3445378151,
        "MMLU_high_school_physics":0.238410596,
        "MMLU_high_school_psychology":0.4348623853,
        "MMLU_high_school_statistics":0.2638888889,
        "MMLU_high_school_us_history":0.4166666667,
        "MMLU_high_school_world_history":0.4430379747,
        "MMLU_human_aging":0.4035874439,
        "MMLU_human_sexuality":0.3435114504,
        "MMLU_international_law":0.6033057851,
        "MMLU_jurisprudence":0.3796296296,
        "MMLU_logical_fallacies":0.3987730061,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.3300970874,
        "MMLU_marketing":0.4444444444,
        "MMLU_medical_genetics":0.43,
        "MMLU_miscellaneous":0.4201787995,
        "MMLU_moral_disputes":0.3815028902,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.4019607843,
        "MMLU_philosophy":0.3633440514,
        "MMLU_prehistory":0.3641975309,
        "MMLU_professional_accounting":0.3120567376,
        "MMLU_professional_law":0.3037809648,
        "MMLU_professional_medicine":0.4632352941,
        "MMLU_professional_psychology":0.3676470588,
        "MMLU_public_relations":0.4363636364,
        "MMLU_security_studies":0.3306122449,
        "MMLU_sociology":0.4378109453,
        "MMLU_us_foreign_policy":0.45,
        "MMLU_virology":0.343373494,
        "MMLU_world_religions":0.4795321637
    },
    {
        "Model":"llama-7b",
        "URL":"https:\/\/huggingface.co\/huggingface\/llama-7b",
        "full_model_name":"huggingface\/llama-7b",
        "Parameters":7.0,
        "MMLU_average":0.3571106114,
        "arc:challenge|25":0.476109215,
        "hellaswag|10":0.575682135,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.3851851852,
        "MMLU_astronomy":0.3486842105,
        "MMLU_business_ethics":0.41,
        "MMLU_clinical_knowledge":0.3547169811,
        "MMLU_college_biology":0.375,
        "MMLU_college_chemistry":0.3,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.3294797688,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.45,
        "MMLU_conceptual_physics":0.370212766,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.2344827586,
        "MMLU_elementary_mathematics":0.2645502646,
        "MMLU_formal_logic":0.2619047619,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.3290322581,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.4424242424,
        "MMLU_high_school_geography":0.3333333333,
        "MMLU_high_school_government_and_politics":0.4455958549,
        "MMLU_high_school_macroeconomics":0.3384615385,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.3319327731,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.4788990826,
        "MMLU_high_school_statistics":0.3101851852,
        "MMLU_high_school_us_history":0.362745098,
        "MMLU_high_school_world_history":0.4303797468,
        "MMLU_human_aging":0.399103139,
        "MMLU_human_sexuality":0.3511450382,
        "MMLU_international_law":0.520661157,
        "MMLU_jurisprudence":0.4166666667,
        "MMLU_logical_fallacies":0.4294478528,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.3398058252,
        "MMLU_marketing":0.4786324786,
        "MMLU_medical_genetics":0.37,
        "MMLU_miscellaneous":0.4252873563,
        "MMLU_moral_disputes":0.3901734104,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.3921568627,
        "MMLU_philosophy":0.3987138264,
        "MMLU_prehistory":0.3456790123,
        "MMLU_professional_accounting":0.2730496454,
        "MMLU_professional_law":0.3011734029,
        "MMLU_professional_medicine":0.4375,
        "MMLU_professional_psychology":0.3529411765,
        "MMLU_public_relations":0.4,
        "MMLU_security_studies":0.3387755102,
        "MMLU_sociology":0.4676616915,
        "MMLU_us_foreign_policy":0.43,
        "MMLU_virology":0.3373493976,
        "MMLU_world_religions":0.4912280702
    },
    {
        "Model":"llama_7b_qlora",
        "URL":"https:\/\/huggingface.co\/DevaMalla\/llama_7b_qlora",
        "full_model_name":"DevaMalla\/llama_7b_qlora",
        "Parameters":7.0,
        "MMLU_average":0.3570957608,
        "arc:challenge|25":0.5093856655,
        "hellaswag|10":0.5960963951,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.3777777778,
        "MMLU_astronomy":0.3947368421,
        "MMLU_business_ethics":0.37,
        "MMLU_clinical_knowledge":0.4188679245,
        "MMLU_college_biology":0.3680555556,
        "MMLU_college_chemistry":0.23,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.3063583815,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.41,
        "MMLU_conceptual_physics":0.2510638298,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.3103448276,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.2142857143,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.3483870968,
        "MMLU_high_school_chemistry":0.2906403941,
        "MMLU_high_school_computer_science":0.4,
        "MMLU_high_school_european_history":0.3575757576,
        "MMLU_high_school_geography":0.4242424242,
        "MMLU_high_school_government_and_politics":0.518134715,
        "MMLU_high_school_macroeconomics":0.3128205128,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.268907563,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.423853211,
        "MMLU_high_school_statistics":0.3472222222,
        "MMLU_high_school_us_history":0.3823529412,
        "MMLU_high_school_world_history":0.4050632911,
        "MMLU_human_aging":0.3497757848,
        "MMLU_human_sexuality":0.3816793893,
        "MMLU_international_law":0.4958677686,
        "MMLU_jurisprudence":0.3888888889,
        "MMLU_logical_fallacies":0.3987730061,
        "MMLU_machine_learning":0.2410714286,
        "MMLU_management":0.4951456311,
        "MMLU_marketing":0.4829059829,
        "MMLU_medical_genetics":0.37,
        "MMLU_miscellaneous":0.4942528736,
        "MMLU_moral_disputes":0.401734104,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.3562091503,
        "MMLU_philosophy":0.4276527331,
        "MMLU_prehistory":0.4135802469,
        "MMLU_professional_accounting":0.280141844,
        "MMLU_professional_law":0.2887874837,
        "MMLU_professional_medicine":0.4117647059,
        "MMLU_professional_psychology":0.3431372549,
        "MMLU_public_relations":0.3818181818,
        "MMLU_security_studies":0.3224489796,
        "MMLU_sociology":0.3930348259,
        "MMLU_us_foreign_policy":0.49,
        "MMLU_virology":0.3795180723,
        "MMLU_world_religions":0.485380117
    },
    {
        "Model":"pygmalion-7b",
        "URL":"https:\/\/huggingface.co\/Neko-Institute-of-Science\/pygmalion-7b",
        "full_model_name":"Neko-Institute-of-Science\/pygmalion-7b",
        "Parameters":7.0,
        "MMLU_average":0.3568354878,
        "arc:challenge|25":0.4795221843,
        "hellaswag|10":0.5760804621,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.4148148148,
        "MMLU_astronomy":0.3157894737,
        "MMLU_business_ethics":0.44,
        "MMLU_clinical_knowledge":0.3924528302,
        "MMLU_college_biology":0.3680555556,
        "MMLU_college_chemistry":0.27,
        "MMLU_college_computer_science":0.3,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.3121387283,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.44,
        "MMLU_conceptual_physics":0.3787234043,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.275862069,
        "MMLU_elementary_mathematics":0.2513227513,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.3580645161,
        "MMLU_high_school_chemistry":0.2906403941,
        "MMLU_high_school_computer_science":0.3,
        "MMLU_high_school_european_history":0.4484848485,
        "MMLU_high_school_geography":0.3484848485,
        "MMLU_high_school_government_and_politics":0.4611398964,
        "MMLU_high_school_macroeconomics":0.3615384615,
        "MMLU_high_school_mathematics":0.237037037,
        "MMLU_high_school_microeconomics":0.3445378151,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.4752293578,
        "MMLU_high_school_statistics":0.287037037,
        "MMLU_high_school_us_history":0.4117647059,
        "MMLU_high_school_world_history":0.4219409283,
        "MMLU_human_aging":0.399103139,
        "MMLU_human_sexuality":0.3358778626,
        "MMLU_international_law":0.5289256198,
        "MMLU_jurisprudence":0.3703703704,
        "MMLU_logical_fallacies":0.3742331288,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.3300970874,
        "MMLU_marketing":0.4615384615,
        "MMLU_medical_genetics":0.36,
        "MMLU_miscellaneous":0.4367816092,
        "MMLU_moral_disputes":0.3959537572,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.3986928105,
        "MMLU_philosophy":0.3826366559,
        "MMLU_prehistory":0.3580246914,
        "MMLU_professional_accounting":0.2624113475,
        "MMLU_professional_law":0.2985658409,
        "MMLU_professional_medicine":0.4448529412,
        "MMLU_professional_psychology":0.3562091503,
        "MMLU_public_relations":0.4909090909,
        "MMLU_security_studies":0.3224489796,
        "MMLU_sociology":0.4328358209,
        "MMLU_us_foreign_policy":0.43,
        "MMLU_virology":0.3493975904,
        "MMLU_world_religions":0.4912280702
    },
    {
        "Model":"llama-base-7b",
        "URL":"https:\/\/huggingface.co\/DevaMalla\/llama-base-7b",
        "full_model_name":"DevaMalla\/llama-base-7b",
        "Parameters":7.0,
        "MMLU_average":0.356688027,
        "arc:challenge|25":0.4769624573,
        "hellaswag|10":0.5754829715,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.3851851852,
        "MMLU_astronomy":0.3421052632,
        "MMLU_business_ethics":0.41,
        "MMLU_clinical_knowledge":0.3622641509,
        "MMLU_college_biology":0.3819444444,
        "MMLU_college_chemistry":0.28,
        "MMLU_college_computer_science":0.3,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.323699422,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.45,
        "MMLU_conceptual_physics":0.3744680851,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.2275862069,
        "MMLU_elementary_mathematics":0.2619047619,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.335483871,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.4363636364,
        "MMLU_high_school_geography":0.3333333333,
        "MMLU_high_school_government_and_politics":0.4507772021,
        "MMLU_high_school_macroeconomics":0.341025641,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.3277310924,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.4844036697,
        "MMLU_high_school_statistics":0.3009259259,
        "MMLU_high_school_us_history":0.3578431373,
        "MMLU_high_school_world_history":0.4303797468,
        "MMLU_human_aging":0.399103139,
        "MMLU_human_sexuality":0.3435114504,
        "MMLU_international_law":0.520661157,
        "MMLU_jurisprudence":0.4166666667,
        "MMLU_logical_fallacies":0.4294478528,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.3398058252,
        "MMLU_marketing":0.4786324786,
        "MMLU_medical_genetics":0.38,
        "MMLU_miscellaneous":0.4278416347,
        "MMLU_moral_disputes":0.3901734104,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.3954248366,
        "MMLU_philosophy":0.3987138264,
        "MMLU_prehistory":0.3487654321,
        "MMLU_professional_accounting":0.2730496454,
        "MMLU_professional_law":0.295958279,
        "MMLU_professional_medicine":0.4411764706,
        "MMLU_professional_psychology":0.3529411765,
        "MMLU_public_relations":0.4181818182,
        "MMLU_security_studies":0.3428571429,
        "MMLU_sociology":0.4726368159,
        "MMLU_us_foreign_policy":0.43,
        "MMLU_virology":0.3313253012,
        "MMLU_world_religions":0.485380117
    },
    {
        "Model":"lora_moe_7b_baseline",
        "URL":"https:\/\/huggingface.co\/AGI-inc\/lora_moe_7b_baseline",
        "full_model_name":"AGI-inc\/lora_moe_7b_baseline",
        "Parameters":7.0,
        "MMLU_average":0.356688027,
        "arc:challenge|25":0.4769624573,
        "hellaswag|10":0.5754829715,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.3851851852,
        "MMLU_astronomy":0.3421052632,
        "MMLU_business_ethics":0.41,
        "MMLU_clinical_knowledge":0.3622641509,
        "MMLU_college_biology":0.3819444444,
        "MMLU_college_chemistry":0.28,
        "MMLU_college_computer_science":0.3,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.323699422,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.45,
        "MMLU_conceptual_physics":0.3744680851,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.2275862069,
        "MMLU_elementary_mathematics":0.2619047619,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.335483871,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.4363636364,
        "MMLU_high_school_geography":0.3333333333,
        "MMLU_high_school_government_and_politics":0.4507772021,
        "MMLU_high_school_macroeconomics":0.341025641,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.3277310924,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.4844036697,
        "MMLU_high_school_statistics":0.3009259259,
        "MMLU_high_school_us_history":0.3578431373,
        "MMLU_high_school_world_history":0.4303797468,
        "MMLU_human_aging":0.399103139,
        "MMLU_human_sexuality":0.3435114504,
        "MMLU_international_law":0.520661157,
        "MMLU_jurisprudence":0.4166666667,
        "MMLU_logical_fallacies":0.4294478528,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.3398058252,
        "MMLU_marketing":0.4786324786,
        "MMLU_medical_genetics":0.38,
        "MMLU_miscellaneous":0.4278416347,
        "MMLU_moral_disputes":0.3901734104,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.3954248366,
        "MMLU_philosophy":0.3987138264,
        "MMLU_prehistory":0.3487654321,
        "MMLU_professional_accounting":0.2730496454,
        "MMLU_professional_law":0.295958279,
        "MMLU_professional_medicine":0.4411764706,
        "MMLU_professional_psychology":0.3529411765,
        "MMLU_public_relations":0.4181818182,
        "MMLU_security_studies":0.3428571429,
        "MMLU_sociology":0.4726368159,
        "MMLU_us_foreign_policy":0.43,
        "MMLU_virology":0.3313253012,
        "MMLU_world_religions":0.485380117
    },
    {
        "Model":"lora_moe_7b",
        "URL":"https:\/\/huggingface.co\/AGI-inc\/lora_moe_7b",
        "full_model_name":"AGI-inc\/lora_moe_7b",
        "Parameters":7.0,
        "MMLU_average":0.356688027,
        "arc:challenge|25":0.4769624573,
        "hellaswag|10":0.5754829715,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.3851851852,
        "MMLU_astronomy":0.3421052632,
        "MMLU_business_ethics":0.41,
        "MMLU_clinical_knowledge":0.3622641509,
        "MMLU_college_biology":0.3819444444,
        "MMLU_college_chemistry":0.28,
        "MMLU_college_computer_science":0.3,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.323699422,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.45,
        "MMLU_conceptual_physics":0.3744680851,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.2275862069,
        "MMLU_elementary_mathematics":0.2619047619,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.335483871,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.4363636364,
        "MMLU_high_school_geography":0.3333333333,
        "MMLU_high_school_government_and_politics":0.4507772021,
        "MMLU_high_school_macroeconomics":0.341025641,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.3277310924,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.4844036697,
        "MMLU_high_school_statistics":0.3009259259,
        "MMLU_high_school_us_history":0.3578431373,
        "MMLU_high_school_world_history":0.4303797468,
        "MMLU_human_aging":0.399103139,
        "MMLU_human_sexuality":0.3435114504,
        "MMLU_international_law":0.520661157,
        "MMLU_jurisprudence":0.4166666667,
        "MMLU_logical_fallacies":0.4294478528,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.3398058252,
        "MMLU_marketing":0.4786324786,
        "MMLU_medical_genetics":0.38,
        "MMLU_miscellaneous":0.4278416347,
        "MMLU_moral_disputes":0.3901734104,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.3954248366,
        "MMLU_philosophy":0.3987138264,
        "MMLU_prehistory":0.3487654321,
        "MMLU_professional_accounting":0.2730496454,
        "MMLU_professional_law":0.295958279,
        "MMLU_professional_medicine":0.4411764706,
        "MMLU_professional_psychology":0.3529411765,
        "MMLU_public_relations":0.4181818182,
        "MMLU_security_studies":0.3428571429,
        "MMLU_sociology":0.4726368159,
        "MMLU_us_foreign_policy":0.43,
        "MMLU_virology":0.3313253012,
        "MMLU_world_religions":0.485380117
    },
    {
        "Model":"orca_mini_13b",
        "URL":"https:\/\/huggingface.co\/psmathur\/orca_mini_13b",
        "full_model_name":"psmathur\/orca_mini_13b",
        "Parameters":13.0,
        "MMLU_average":0.3542524393,
        "arc:challenge|25":0.4027303754,
        "hellaswag|10":0.4871539534,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.3925925926,
        "MMLU_astronomy":0.3092105263,
        "MMLU_business_ethics":0.37,
        "MMLU_clinical_knowledge":0.3660377358,
        "MMLU_college_biology":0.3333333333,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.4,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.2832369942,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.41,
        "MMLU_conceptual_physics":0.3021276596,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.4137931034,
        "MMLU_elementary_mathematics":0.2486772487,
        "MMLU_formal_logic":0.2380952381,
        "MMLU_global_facts":0.29,
        "MMLU_high_school_biology":0.3774193548,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.32,
        "MMLU_high_school_european_history":0.4363636364,
        "MMLU_high_school_geography":0.4292929293,
        "MMLU_high_school_government_and_politics":0.4352331606,
        "MMLU_high_school_macroeconomics":0.2923076923,
        "MMLU_high_school_mathematics":0.2333333333,
        "MMLU_high_school_microeconomics":0.3025210084,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.3669724771,
        "MMLU_high_school_statistics":0.3055555556,
        "MMLU_high_school_us_history":0.4607843137,
        "MMLU_high_school_world_history":0.4599156118,
        "MMLU_human_aging":0.4035874439,
        "MMLU_human_sexuality":0.3740458015,
        "MMLU_international_law":0.4049586777,
        "MMLU_jurisprudence":0.4074074074,
        "MMLU_logical_fallacies":0.3374233129,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.2912621359,
        "MMLU_marketing":0.4871794872,
        "MMLU_medical_genetics":0.4,
        "MMLU_miscellaneous":0.5006385696,
        "MMLU_moral_disputes":0.4277456647,
        "MMLU_moral_scenarios":0.2525139665,
        "MMLU_nutrition":0.3986928105,
        "MMLU_philosophy":0.3697749196,
        "MMLU_prehistory":0.3487654321,
        "MMLU_professional_accounting":0.3014184397,
        "MMLU_professional_law":0.3109517601,
        "MMLU_professional_medicine":0.2389705882,
        "MMLU_professional_psychology":0.3594771242,
        "MMLU_public_relations":0.4181818182,
        "MMLU_security_studies":0.3632653061,
        "MMLU_sociology":0.4875621891,
        "MMLU_us_foreign_policy":0.55,
        "MMLU_virology":0.3554216867,
        "MMLU_world_religions":0.4678362573
    },
    {
        "Model":"WizardLM-7B-Uncensored",
        "URL":"https:\/\/huggingface.co\/ehartford\/WizardLM-7B-Uncensored",
        "full_model_name":"ehartford\/WizardLM-7B-Uncensored",
        "Parameters":7.0,
        "MMLU_average":0.3541572207,
        "arc:challenge|25":0.4496587031,
        "hellaswag|10":0.5542720574,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.3851851852,
        "MMLU_astronomy":0.3289473684,
        "MMLU_business_ethics":0.49,
        "MMLU_clinical_knowledge":0.3886792453,
        "MMLU_college_biology":0.3472222222,
        "MMLU_college_chemistry":0.26,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.2658959538,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.42,
        "MMLU_conceptual_physics":0.370212766,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2827586207,
        "MMLU_elementary_mathematics":0.2380952381,
        "MMLU_formal_logic":0.2698412698,
        "MMLU_global_facts":0.26,
        "MMLU_high_school_biology":0.3258064516,
        "MMLU_high_school_chemistry":0.2463054187,
        "MMLU_high_school_computer_science":0.38,
        "MMLU_high_school_european_history":0.4484848485,
        "MMLU_high_school_geography":0.3939393939,
        "MMLU_high_school_government_and_politics":0.4248704663,
        "MMLU_high_school_macroeconomics":0.3333333333,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.2983193277,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.4440366972,
        "MMLU_high_school_statistics":0.2083333333,
        "MMLU_high_school_us_history":0.4607843137,
        "MMLU_high_school_world_history":0.4641350211,
        "MMLU_human_aging":0.4573991031,
        "MMLU_human_sexuality":0.358778626,
        "MMLU_international_law":0.4958677686,
        "MMLU_jurisprudence":0.4074074074,
        "MMLU_logical_fallacies":0.3619631902,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.3398058252,
        "MMLU_marketing":0.5085470085,
        "MMLU_medical_genetics":0.39,
        "MMLU_miscellaneous":0.4648786718,
        "MMLU_moral_disputes":0.3757225434,
        "MMLU_moral_scenarios":0.2491620112,
        "MMLU_nutrition":0.3202614379,
        "MMLU_philosophy":0.3826366559,
        "MMLU_prehistory":0.3672839506,
        "MMLU_professional_accounting":0.3014184397,
        "MMLU_professional_law":0.3174706649,
        "MMLU_professional_medicine":0.4117647059,
        "MMLU_professional_psychology":0.3513071895,
        "MMLU_public_relations":0.4454545455,
        "MMLU_security_studies":0.306122449,
        "MMLU_sociology":0.4129353234,
        "MMLU_us_foreign_policy":0.45,
        "MMLU_virology":0.2891566265,
        "MMLU_world_religions":0.4678362573
    },
    {
        "Model":"palmyra-20b-chat",
        "URL":"https:\/\/huggingface.co\/Writer\/palmyra-20b-chat",
        "full_model_name":"Writer\/palmyra-20b-chat",
        "Parameters":20.0,
        "MMLU_average":0.3518478078,
        "arc:challenge|25":0.4010238908,
        "hellaswag|10":0.5491933878,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.3851851852,
        "MMLU_astronomy":0.3421052632,
        "MMLU_business_ethics":0.29,
        "MMLU_clinical_knowledge":0.3773584906,
        "MMLU_college_biology":0.3611111111,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.3294797688,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.43,
        "MMLU_conceptual_physics":0.3276595745,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.4137931034,
        "MMLU_elementary_mathematics":0.2777777778,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.3903225806,
        "MMLU_high_school_chemistry":0.315270936,
        "MMLU_high_school_computer_science":0.27,
        "MMLU_high_school_european_history":0.2909090909,
        "MMLU_high_school_geography":0.398989899,
        "MMLU_high_school_government_and_politics":0.4507772021,
        "MMLU_high_school_macroeconomics":0.3487179487,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.3613445378,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.4055045872,
        "MMLU_high_school_statistics":0.2916666667,
        "MMLU_high_school_us_history":0.3088235294,
        "MMLU_high_school_world_history":0.4050632911,
        "MMLU_human_aging":0.3766816143,
        "MMLU_human_sexuality":0.3664122137,
        "MMLU_international_law":0.4545454545,
        "MMLU_jurisprudence":0.3333333333,
        "MMLU_logical_fallacies":0.4233128834,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.3786407767,
        "MMLU_marketing":0.5213675214,
        "MMLU_medical_genetics":0.42,
        "MMLU_miscellaneous":0.4010217114,
        "MMLU_moral_disputes":0.2919075145,
        "MMLU_moral_scenarios":0.2446927374,
        "MMLU_nutrition":0.3823529412,
        "MMLU_philosophy":0.4083601286,
        "MMLU_prehistory":0.3395061728,
        "MMLU_professional_accounting":0.280141844,
        "MMLU_professional_law":0.3018252934,
        "MMLU_professional_medicine":0.3713235294,
        "MMLU_professional_psychology":0.3169934641,
        "MMLU_public_relations":0.3727272727,
        "MMLU_security_studies":0.4204081633,
        "MMLU_sociology":0.3930348259,
        "MMLU_us_foreign_policy":0.46,
        "MMLU_virology":0.4156626506,
        "MMLU_world_religions":0.3859649123
    },
    {
        "Model":"open-llama-7b-v2-open-instruct",
        "URL":"https:\/\/huggingface.co\/VMware\/open-llama-7b-v2-open-instruct",
        "full_model_name":"VMware\/open-llama-7b-v2-open-instruct",
        "Parameters":7.0,
        "MMLU_average":0.3515947304,
        "arc:challenge|25":0.3754266212,
        "hellaswag|10":0.5279824736,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.4074074074,
        "MMLU_astronomy":0.3026315789,
        "MMLU_business_ethics":0.51,
        "MMLU_clinical_knowledge":0.3283018868,
        "MMLU_college_biology":0.3125,
        "MMLU_college_chemistry":0.25,
        "MMLU_college_computer_science":0.34,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.3179190751,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.49,
        "MMLU_conceptual_physics":0.3234042553,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.3586206897,
        "MMLU_elementary_mathematics":0.3148148148,
        "MMLU_formal_logic":0.2063492063,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.364516129,
        "MMLU_high_school_chemistry":0.2463054187,
        "MMLU_high_school_computer_science":0.38,
        "MMLU_high_school_european_history":0.4,
        "MMLU_high_school_geography":0.3787878788,
        "MMLU_high_school_government_and_politics":0.4559585492,
        "MMLU_high_school_macroeconomics":0.341025641,
        "MMLU_high_school_mathematics":0.237037037,
        "MMLU_high_school_microeconomics":0.256302521,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.3633027523,
        "MMLU_high_school_statistics":0.2361111111,
        "MMLU_high_school_us_history":0.3921568627,
        "MMLU_high_school_world_history":0.5105485232,
        "MMLU_human_aging":0.5022421525,
        "MMLU_human_sexuality":0.4045801527,
        "MMLU_international_law":0.4876033058,
        "MMLU_jurisprudence":0.3333333333,
        "MMLU_logical_fallacies":0.3067484663,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.3786407767,
        "MMLU_marketing":0.452991453,
        "MMLU_medical_genetics":0.38,
        "MMLU_miscellaneous":0.466155811,
        "MMLU_moral_disputes":0.2803468208,
        "MMLU_moral_scenarios":0.2592178771,
        "MMLU_nutrition":0.362745098,
        "MMLU_philosophy":0.3440514469,
        "MMLU_prehistory":0.4074074074,
        "MMLU_professional_accounting":0.2836879433,
        "MMLU_professional_law":0.2835723598,
        "MMLU_professional_medicine":0.2977941176,
        "MMLU_professional_psychology":0.3218954248,
        "MMLU_public_relations":0.4181818182,
        "MMLU_security_studies":0.2897959184,
        "MMLU_sociology":0.4427860697,
        "MMLU_us_foreign_policy":0.54,
        "MMLU_virology":0.3493975904,
        "MMLU_world_religions":0.4035087719
    },
    {
        "Model":"nart-100k-7b",
        "URL":"https:\/\/huggingface.co\/jerryjalapeno\/nart-100k-7b",
        "full_model_name":"jerryjalapeno\/nart-100k-7b",
        "Parameters":7.0,
        "MMLU_average":0.3497996982,
        "arc:challenge|25":0.5136518771,
        "hellaswag|10":0.5916152161,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.2296296296,
        "MMLU_astronomy":0.375,
        "MMLU_business_ethics":0.42,
        "MMLU_clinical_knowledge":0.3698113208,
        "MMLU_college_biology":0.2916666667,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.3352601156,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.45,
        "MMLU_conceptual_physics":0.3361702128,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.3379310345,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.380952381,
        "MMLU_global_facts":0.26,
        "MMLU_high_school_biology":0.4129032258,
        "MMLU_high_school_chemistry":0.3004926108,
        "MMLU_high_school_computer_science":0.27,
        "MMLU_high_school_european_history":0.4424242424,
        "MMLU_high_school_geography":0.4292929293,
        "MMLU_high_school_government_and_politics":0.4352331606,
        "MMLU_high_school_macroeconomics":0.3846153846,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.3823529412,
        "MMLU_high_school_physics":0.3642384106,
        "MMLU_high_school_psychology":0.4550458716,
        "MMLU_high_school_statistics":0.4074074074,
        "MMLU_high_school_us_history":0.4558823529,
        "MMLU_high_school_world_history":0.3797468354,
        "MMLU_human_aging":0.2376681614,
        "MMLU_human_sexuality":0.358778626,
        "MMLU_international_law":0.2727272727,
        "MMLU_jurisprudence":0.3796296296,
        "MMLU_logical_fallacies":0.3680981595,
        "MMLU_machine_learning":0.2053571429,
        "MMLU_management":0.4563106796,
        "MMLU_marketing":0.5,
        "MMLU_medical_genetics":0.39,
        "MMLU_miscellaneous":0.3882503193,
        "MMLU_moral_disputes":0.4161849711,
        "MMLU_moral_scenarios":0.2346368715,
        "MMLU_nutrition":0.3888888889,
        "MMLU_philosophy":0.3697749196,
        "MMLU_prehistory":0.3672839506,
        "MMLU_professional_accounting":0.2482269504,
        "MMLU_professional_law":0.2737940026,
        "MMLU_professional_medicine":0.4448529412,
        "MMLU_professional_psychology":0.2875816993,
        "MMLU_public_relations":0.3181818182,
        "MMLU_security_studies":0.412244898,
        "MMLU_sociology":0.4129353234,
        "MMLU_us_foreign_policy":0.32,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.3742690058
    },
    {
        "Model":"ko-ref-llama2-13b",
        "URL":"https:\/\/huggingface.co\/hyunseoki\/ko-ref-llama2-13b",
        "full_model_name":"hyunseoki\/ko-ref-llama2-13b",
        "Parameters":13.0,
        "MMLU_average":0.3483448551,
        "arc:challenge|25":0.4590443686,
        "hellaswag|10":0.5243975304,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.3777777778,
        "MMLU_astronomy":0.3618421053,
        "MMLU_business_ethics":0.31,
        "MMLU_clinical_knowledge":0.3773584906,
        "MMLU_college_biology":0.3611111111,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.25,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.3352601156,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.51,
        "MMLU_conceptual_physics":0.3063829787,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.324137931,
        "MMLU_elementary_mathematics":0.3042328042,
        "MMLU_formal_logic":0.1904761905,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.3419354839,
        "MMLU_high_school_chemistry":0.315270936,
        "MMLU_high_school_computer_science":0.37,
        "MMLU_high_school_european_history":0.303030303,
        "MMLU_high_school_geography":0.4949494949,
        "MMLU_high_school_government_and_politics":0.414507772,
        "MMLU_high_school_macroeconomics":0.3230769231,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.3235294118,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.4752293578,
        "MMLU_high_school_statistics":0.2361111111,
        "MMLU_high_school_us_history":0.3137254902,
        "MMLU_high_school_world_history":0.3966244726,
        "MMLU_human_aging":0.4304932735,
        "MMLU_human_sexuality":0.3664122137,
        "MMLU_international_law":0.4876033058,
        "MMLU_jurisprudence":0.4537037037,
        "MMLU_logical_fallacies":0.3374233129,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.359223301,
        "MMLU_marketing":0.5,
        "MMLU_medical_genetics":0.29,
        "MMLU_miscellaneous":0.4789272031,
        "MMLU_moral_disputes":0.4421965318,
        "MMLU_moral_scenarios":0.2480446927,
        "MMLU_nutrition":0.3921568627,
        "MMLU_philosophy":0.4405144695,
        "MMLU_prehistory":0.3919753086,
        "MMLU_professional_accounting":0.280141844,
        "MMLU_professional_law":0.3252933507,
        "MMLU_professional_medicine":0.2279411765,
        "MMLU_professional_psychology":0.3284313725,
        "MMLU_public_relations":0.2909090909,
        "MMLU_security_studies":0.3387755102,
        "MMLU_sociology":0.4029850746,
        "MMLU_us_foreign_policy":0.48,
        "MMLU_virology":0.343373494,
        "MMLU_world_religions":0.5204678363
    },
    {
        "Model":"CodeLlama-34b-Python-hf",
        "URL":"https:\/\/huggingface.co\/codellama\/CodeLlama-34b-Python-hf",
        "full_model_name":"codellama\/CodeLlama-34b-Python-hf",
        "Parameters":34.0,
        "MMLU_average":0.3478936714,
        "arc:challenge|25":0.3703071672,
        "hellaswag|10":0.3076080462,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.3333333333,
        "MMLU_astronomy":0.3684210526,
        "MMLU_business_ethics":0.36,
        "MMLU_clinical_knowledge":0.3811320755,
        "MMLU_college_biology":0.3402777778,
        "MMLU_college_chemistry":0.29,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.3179190751,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.42,
        "MMLU_conceptual_physics":0.3531914894,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.3310344828,
        "MMLU_elementary_mathematics":0.3148148148,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.2,
        "MMLU_high_school_biology":0.3741935484,
        "MMLU_high_school_chemistry":0.2167487685,
        "MMLU_high_school_computer_science":0.32,
        "MMLU_high_school_european_history":0.2060606061,
        "MMLU_high_school_geography":0.4393939394,
        "MMLU_high_school_government_and_politics":0.4300518135,
        "MMLU_high_school_macroeconomics":0.3205128205,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.3739495798,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.3559633028,
        "MMLU_high_school_statistics":0.3425925926,
        "MMLU_high_school_us_history":0.2696078431,
        "MMLU_high_school_world_history":0.4219409283,
        "MMLU_human_aging":0.3452914798,
        "MMLU_human_sexuality":0.358778626,
        "MMLU_international_law":0.3801652893,
        "MMLU_jurisprudence":0.4259259259,
        "MMLU_logical_fallacies":0.3006134969,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.5339805825,
        "MMLU_marketing":0.5042735043,
        "MMLU_medical_genetics":0.34,
        "MMLU_miscellaneous":0.4865900383,
        "MMLU_moral_disputes":0.3670520231,
        "MMLU_moral_scenarios":0.2525139665,
        "MMLU_nutrition":0.3790849673,
        "MMLU_philosophy":0.4147909968,
        "MMLU_prehistory":0.3518518519,
        "MMLU_professional_accounting":0.2411347518,
        "MMLU_professional_law":0.258148631,
        "MMLU_professional_medicine":0.3235294118,
        "MMLU_professional_psychology":0.295751634,
        "MMLU_public_relations":0.3818181818,
        "MMLU_security_studies":0.3673469388,
        "MMLU_sociology":0.3432835821,
        "MMLU_us_foreign_policy":0.41,
        "MMLU_virology":0.3795180723,
        "MMLU_world_religions":0.5614035088
    },
    {
        "Model":"CodeLLaMA-chat-13b-Chinese",
        "URL":"https:\/\/huggingface.co\/shareAI\/CodeLLaMA-chat-13b-Chinese",
        "full_model_name":"shareAI\/CodeLLaMA-chat-13b-Chinese",
        "Parameters":13.0,
        "MMLU_average":0.3428563918,
        "arc:challenge|25":0.3976109215,
        "hellaswag|10":0.4841665007,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.3925925926,
        "MMLU_astronomy":0.3092105263,
        "MMLU_business_ethics":0.34,
        "MMLU_clinical_knowledge":0.3509433962,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.2,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.3583815029,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.39,
        "MMLU_conceptual_physics":0.2978723404,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.4068965517,
        "MMLU_elementary_mathematics":0.2751322751,
        "MMLU_formal_logic":0.2301587302,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.3709677419,
        "MMLU_high_school_chemistry":0.2561576355,
        "MMLU_high_school_computer_science":0.23,
        "MMLU_high_school_european_history":0.2909090909,
        "MMLU_high_school_geography":0.4494949495,
        "MMLU_high_school_government_and_politics":0.4766839378,
        "MMLU_high_school_macroeconomics":0.3512820513,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.3487394958,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.3724770642,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2990196078,
        "MMLU_high_school_world_history":0.3670886076,
        "MMLU_human_aging":0.4439461883,
        "MMLU_human_sexuality":0.3816793893,
        "MMLU_international_law":0.2644628099,
        "MMLU_jurisprudence":0.3240740741,
        "MMLU_logical_fallacies":0.3312883436,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.427184466,
        "MMLU_marketing":0.6239316239,
        "MMLU_medical_genetics":0.41,
        "MMLU_miscellaneous":0.4444444444,
        "MMLU_moral_disputes":0.2427745665,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.3333333333,
        "MMLU_philosophy":0.4083601286,
        "MMLU_prehistory":0.3611111111,
        "MMLU_professional_accounting":0.2765957447,
        "MMLU_professional_law":0.2835723598,
        "MMLU_professional_medicine":0.3382352941,
        "MMLU_professional_psychology":0.2761437908,
        "MMLU_public_relations":0.3454545455,
        "MMLU_security_studies":0.3591836735,
        "MMLU_sociology":0.4179104478,
        "MMLU_us_foreign_policy":0.5,
        "MMLU_virology":0.3493975904,
        "MMLU_world_religions":0.4502923977
    },
    {
        "Model":"Chinese-LLaMA-2-13B-hf",
        "URL":"https:\/\/huggingface.co\/Linly-AI\/Chinese-LLaMA-2-13B-hf",
        "full_model_name":"Linly-AI\/Chinese-LLaMA-2-13B-hf",
        "Parameters":13.0,
        "MMLU_average":0.3397284862,
        "arc:challenge|25":0.29778157,
        "hellaswag|10":0.3216490739,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.3925925926,
        "MMLU_astronomy":0.3421052632,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.3433962264,
        "MMLU_college_biology":0.3125,
        "MMLU_college_chemistry":0.36,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.3815028902,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.26,
        "MMLU_conceptual_physics":0.3106382979,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.3310344828,
        "MMLU_elementary_mathematics":0.253968254,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.3322580645,
        "MMLU_high_school_chemistry":0.2463054187,
        "MMLU_high_school_computer_science":0.22,
        "MMLU_high_school_european_history":0.6363636364,
        "MMLU_high_school_geography":0.4444444444,
        "MMLU_high_school_government_and_politics":0.4455958549,
        "MMLU_high_school_macroeconomics":0.358974359,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.3991596639,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.3908256881,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.6960784314,
        "MMLU_high_school_world_history":0.3080168776,
        "MMLU_human_aging":0.2959641256,
        "MMLU_human_sexuality":0.3969465649,
        "MMLU_international_law":0.3636363636,
        "MMLU_jurisprudence":0.2962962963,
        "MMLU_logical_fallacies":0.245398773,
        "MMLU_machine_learning":0.1607142857,
        "MMLU_management":0.3689320388,
        "MMLU_marketing":0.3205128205,
        "MMLU_medical_genetics":0.39,
        "MMLU_miscellaneous":0.4559386973,
        "MMLU_moral_disputes":0.2658959538,
        "MMLU_moral_scenarios":0.261452514,
        "MMLU_nutrition":0.4052287582,
        "MMLU_philosophy":0.3697749196,
        "MMLU_prehistory":0.3364197531,
        "MMLU_professional_accounting":0.2553191489,
        "MMLU_professional_law":0.2790091265,
        "MMLU_professional_medicine":0.4448529412,
        "MMLU_professional_psychology":0.2549019608,
        "MMLU_public_relations":0.2909090909,
        "MMLU_security_studies":0.3959183673,
        "MMLU_sociology":0.4427860697,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.3012048193,
        "MMLU_world_religions":0.3742690058
    },
    {
        "Model":"carl-7b",
        "URL":"https:\/\/huggingface.co\/ajibawa-2023\/carl-7b",
        "full_model_name":"ajibawa-2023\/carl-7b",
        "Parameters":7.0,
        "MMLU_average":0.3396318345,
        "arc:challenge|25":0.5127986348,
        "hellaswag|10":0.5873332006,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.2222222222,
        "MMLU_astronomy":0.3815789474,
        "MMLU_business_ethics":0.42,
        "MMLU_clinical_knowledge":0.3622641509,
        "MMLU_college_biology":0.2916666667,
        "MMLU_college_chemistry":0.43,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.3352601156,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.4,
        "MMLU_conceptual_physics":0.2978723404,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.3172413793,
        "MMLU_elementary_mathematics":0.2645502646,
        "MMLU_formal_logic":0.380952381,
        "MMLU_global_facts":0.2,
        "MMLU_high_school_biology":0.3677419355,
        "MMLU_high_school_chemistry":0.2955665025,
        "MMLU_high_school_computer_science":0.27,
        "MMLU_high_school_european_history":0.3878787879,
        "MMLU_high_school_geography":0.4141414141,
        "MMLU_high_school_government_and_politics":0.4300518135,
        "MMLU_high_school_macroeconomics":0.3820512821,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.3949579832,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.4330275229,
        "MMLU_high_school_statistics":0.3796296296,
        "MMLU_high_school_us_history":0.4460784314,
        "MMLU_high_school_world_history":0.3375527426,
        "MMLU_human_aging":0.2331838565,
        "MMLU_human_sexuality":0.3358778626,
        "MMLU_international_law":0.1983471074,
        "MMLU_jurisprudence":0.3611111111,
        "MMLU_logical_fallacies":0.3251533742,
        "MMLU_machine_learning":0.1875,
        "MMLU_management":0.4854368932,
        "MMLU_marketing":0.5042735043,
        "MMLU_medical_genetics":0.37,
        "MMLU_miscellaneous":0.3818646232,
        "MMLU_moral_disputes":0.4161849711,
        "MMLU_moral_scenarios":0.269273743,
        "MMLU_nutrition":0.3562091503,
        "MMLU_philosophy":0.3376205788,
        "MMLU_prehistory":0.3333333333,
        "MMLU_professional_accounting":0.2411347518,
        "MMLU_professional_law":0.2542372881,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2908496732,
        "MMLU_public_relations":0.3454545455,
        "MMLU_security_studies":0.3836734694,
        "MMLU_sociology":0.3731343284,
        "MMLU_us_foreign_policy":0.35,
        "MMLU_virology":0.2891566265,
        "MMLU_world_religions":0.4093567251
    },
    {
        "Model":"ennodata-7b",
        "URL":"https:\/\/huggingface.co\/Enno-Ai\/ennodata-7b",
        "full_model_name":"Enno-Ai\/ennodata-7b",
        "Parameters":7.0,
        "MMLU_average":0.339450372,
        "arc:challenge|25":0.4744027304,
        "hellaswag|10":0.5731925911,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.3851851852,
        "MMLU_astronomy":0.3092105263,
        "MMLU_business_ethics":0.38,
        "MMLU_clinical_knowledge":0.3245283019,
        "MMLU_college_biology":0.3680555556,
        "MMLU_college_chemistry":0.28,
        "MMLU_college_computer_science":0.27,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.3063583815,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.42,
        "MMLU_conceptual_physics":0.3361702128,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.275862069,
        "MMLU_elementary_mathematics":0.253968254,
        "MMLU_formal_logic":0.2380952381,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.3258064516,
        "MMLU_high_school_chemistry":0.2561576355,
        "MMLU_high_school_computer_science":0.3,
        "MMLU_high_school_european_history":0.4121212121,
        "MMLU_high_school_geography":0.3232323232,
        "MMLU_high_school_government_and_politics":0.3937823834,
        "MMLU_high_school_macroeconomics":0.3333333333,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.3235294118,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.4293577982,
        "MMLU_high_school_statistics":0.2731481481,
        "MMLU_high_school_us_history":0.3284313725,
        "MMLU_high_school_world_history":0.4050632911,
        "MMLU_human_aging":0.3946188341,
        "MMLU_human_sexuality":0.3358778626,
        "MMLU_international_law":0.5123966942,
        "MMLU_jurisprudence":0.3888888889,
        "MMLU_logical_fallacies":0.3926380368,
        "MMLU_machine_learning":0.2410714286,
        "MMLU_management":0.3106796117,
        "MMLU_marketing":0.452991453,
        "MMLU_medical_genetics":0.37,
        "MMLU_miscellaneous":0.4022988506,
        "MMLU_moral_disputes":0.3728323699,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.3954248366,
        "MMLU_philosophy":0.4083601286,
        "MMLU_prehistory":0.3364197531,
        "MMLU_professional_accounting":0.2695035461,
        "MMLU_professional_law":0.294654498,
        "MMLU_professional_medicine":0.4375,
        "MMLU_professional_psychology":0.339869281,
        "MMLU_public_relations":0.4363636364,
        "MMLU_security_studies":0.2775510204,
        "MMLU_sociology":0.4228855721,
        "MMLU_us_foreign_policy":0.44,
        "MMLU_virology":0.3614457831,
        "MMLU_world_religions":0.4444444444
    },
    {
        "Model":"Galactica-6.7B-EssayWriter",
        "URL":"https:\/\/huggingface.co\/KnutJaegersberg\/Galactica-6.7B-EssayWriter",
        "full_model_name":"KnutJaegersberg\/Galactica-6.7B-EssayWriter",
        "Parameters":6.7,
        "MMLU_average":0.3388024519,
        "arc:challenge|25":0.3686006826,
        "hellaswag|10":0.3904600677,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.4148148148,
        "MMLU_astronomy":0.3947368421,
        "MMLU_business_ethics":0.31,
        "MMLU_clinical_knowledge":0.3509433962,
        "MMLU_college_biology":0.4027777778,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.2774566474,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.38,
        "MMLU_conceptual_physics":0.4042553191,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.3310344828,
        "MMLU_elementary_mathematics":0.2645502646,
        "MMLU_formal_logic":0.1984126984,
        "MMLU_global_facts":0.27,
        "MMLU_high_school_biology":0.3451612903,
        "MMLU_high_school_chemistry":0.3054187192,
        "MMLU_high_school_computer_science":0.3,
        "MMLU_high_school_european_history":0.3090909091,
        "MMLU_high_school_geography":0.2828282828,
        "MMLU_high_school_government_and_politics":0.4093264249,
        "MMLU_high_school_macroeconomics":0.3564102564,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.3151260504,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.3981651376,
        "MMLU_high_school_statistics":0.287037037,
        "MMLU_high_school_us_history":0.2401960784,
        "MMLU_high_school_world_history":0.3755274262,
        "MMLU_human_aging":0.4394618834,
        "MMLU_human_sexuality":0.320610687,
        "MMLU_international_law":0.4132231405,
        "MMLU_jurisprudence":0.3333333333,
        "MMLU_logical_fallacies":0.2760736196,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.4466019417,
        "MMLU_marketing":0.3205128205,
        "MMLU_medical_genetics":0.48,
        "MMLU_miscellaneous":0.3767560664,
        "MMLU_moral_disputes":0.289017341,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.3986928105,
        "MMLU_philosophy":0.3762057878,
        "MMLU_prehistory":0.3487654321,
        "MMLU_professional_accounting":0.304964539,
        "MMLU_professional_law":0.2913950456,
        "MMLU_professional_medicine":0.3860294118,
        "MMLU_professional_psychology":0.3447712418,
        "MMLU_public_relations":0.3272727273,
        "MMLU_security_studies":0.3020408163,
        "MMLU_sociology":0.407960199,
        "MMLU_us_foreign_policy":0.45,
        "MMLU_virology":0.3554216867,
        "MMLU_world_religions":0.2865497076
    },
    {
        "Model":"gogpt-7b-bloom",
        "URL":"https:\/\/huggingface.co\/golaxy\/gogpt-7b-bloom",
        "full_model_name":"golaxy\/gogpt-7b-bloom",
        "Parameters":7.0,
        "MMLU_average":0.3381108922,
        "arc:challenge|25":0.4112627986,
        "hellaswag|10":0.4633539136,
        "MMLU_abstract_algebra":0.21,
        "MMLU_anatomy":0.3185185185,
        "MMLU_astronomy":0.3157894737,
        "MMLU_business_ethics":0.29,
        "MMLU_clinical_knowledge":0.3471698113,
        "MMLU_college_biology":0.3541666667,
        "MMLU_college_chemistry":0.46,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.3699421965,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.4170212766,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.3379310345,
        "MMLU_elementary_mathematics":0.2724867725,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.3935483871,
        "MMLU_high_school_chemistry":0.2955665025,
        "MMLU_high_school_computer_science":0.28,
        "MMLU_high_school_european_history":0.2909090909,
        "MMLU_high_school_geography":0.4090909091,
        "MMLU_high_school_government_and_politics":0.4248704663,
        "MMLU_high_school_macroeconomics":0.3717948718,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.3949579832,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.4513761468,
        "MMLU_high_school_statistics":0.4768518519,
        "MMLU_high_school_us_history":0.2745098039,
        "MMLU_high_school_world_history":0.358649789,
        "MMLU_human_aging":0.3856502242,
        "MMLU_human_sexuality":0.3893129771,
        "MMLU_international_law":0.2148760331,
        "MMLU_jurisprudence":0.3055555556,
        "MMLU_logical_fallacies":0.2883435583,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.4077669903,
        "MMLU_marketing":0.3461538462,
        "MMLU_medical_genetics":0.42,
        "MMLU_miscellaneous":0.3997445722,
        "MMLU_moral_disputes":0.2919075145,
        "MMLU_moral_scenarios":0.2648044693,
        "MMLU_nutrition":0.3366013072,
        "MMLU_philosophy":0.3183279743,
        "MMLU_prehistory":0.2932098765,
        "MMLU_professional_accounting":0.2765957447,
        "MMLU_professional_law":0.2529335072,
        "MMLU_professional_medicine":0.4448529412,
        "MMLU_professional_psychology":0.3120915033,
        "MMLU_public_relations":0.3909090909,
        "MMLU_security_studies":0.3836734694,
        "MMLU_sociology":0.4228855721,
        "MMLU_us_foreign_policy":0.34,
        "MMLU_virology":0.3192771084,
        "MMLU_world_religions":0.3157894737
    },
    {
        "Model":"Alpaca-7B-v1",
        "URL":"https:\/\/huggingface.co\/WeOpenML\/Alpaca-7B-v1",
        "full_model_name":"WeOpenML\/Alpaca-7B-v1",
        "Parameters":7.0,
        "MMLU_average":0.3376245732,
        "arc:challenge|25":0.4820819113,
        "hellaswag|10":0.5763792073,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.4296296296,
        "MMLU_astronomy":0.3026315789,
        "MMLU_business_ethics":0.44,
        "MMLU_clinical_knowledge":0.3622641509,
        "MMLU_college_biology":0.3333333333,
        "MMLU_college_chemistry":0.27,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.3005780347,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.39,
        "MMLU_conceptual_physics":0.3191489362,
        "MMLU_econometrics":0.1842105263,
        "MMLU_electrical_engineering":0.275862069,
        "MMLU_elementary_mathematics":0.2883597884,
        "MMLU_formal_logic":0.2142857143,
        "MMLU_global_facts":0.29,
        "MMLU_high_school_biology":0.2870967742,
        "MMLU_high_school_chemistry":0.2463054187,
        "MMLU_high_school_computer_science":0.27,
        "MMLU_high_school_european_history":0.496969697,
        "MMLU_high_school_geography":0.3333333333,
        "MMLU_high_school_government_and_politics":0.378238342,
        "MMLU_high_school_macroeconomics":0.3333333333,
        "MMLU_high_school_mathematics":0.2296296296,
        "MMLU_high_school_microeconomics":0.3067226891,
        "MMLU_high_school_physics":0.238410596,
        "MMLU_high_school_psychology":0.3798165138,
        "MMLU_high_school_statistics":0.2037037037,
        "MMLU_high_school_us_history":0.4411764706,
        "MMLU_high_school_world_history":0.3502109705,
        "MMLU_human_aging":0.4035874439,
        "MMLU_human_sexuality":0.320610687,
        "MMLU_international_law":0.6115702479,
        "MMLU_jurisprudence":0.3055555556,
        "MMLU_logical_fallacies":0.3865030675,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.3203883495,
        "MMLU_marketing":0.3717948718,
        "MMLU_medical_genetics":0.39,
        "MMLU_miscellaneous":0.3856960409,
        "MMLU_moral_disputes":0.3583815029,
        "MMLU_moral_scenarios":0.2558659218,
        "MMLU_nutrition":0.3267973856,
        "MMLU_philosophy":0.3344051447,
        "MMLU_prehistory":0.3549382716,
        "MMLU_professional_accounting":0.2695035461,
        "MMLU_professional_law":0.2757496741,
        "MMLU_professional_medicine":0.4669117647,
        "MMLU_professional_psychology":0.3300653595,
        "MMLU_public_relations":0.4181818182,
        "MMLU_security_studies":0.2816326531,
        "MMLU_sociology":0.3532338308,
        "MMLU_us_foreign_policy":0.41,
        "MMLU_virology":0.343373494,
        "MMLU_world_religions":0.485380117
    },
    {
        "Model":"llama_7b_lora",
        "URL":"https:\/\/huggingface.co\/DevaMalla\/llama_7b_lora",
        "full_model_name":"DevaMalla\/llama_7b_lora",
        "Parameters":7.0,
        "MMLU_average":0.3362844194,
        "arc:challenge|25":0.5162116041,
        "hellaswag|10":0.6069508066,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.4296296296,
        "MMLU_astronomy":0.2828947368,
        "MMLU_business_ethics":0.33,
        "MMLU_clinical_knowledge":0.3547169811,
        "MMLU_college_biology":0.3472222222,
        "MMLU_college_chemistry":0.22,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.2947976879,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.45,
        "MMLU_conceptual_physics":0.3319148936,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.3103448276,
        "MMLU_elementary_mathematics":0.2724867725,
        "MMLU_formal_logic":0.2142857143,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.3290322581,
        "MMLU_high_school_chemistry":0.3103448276,
        "MMLU_high_school_computer_science":0.37,
        "MMLU_high_school_european_history":0.3575757576,
        "MMLU_high_school_geography":0.3282828283,
        "MMLU_high_school_government_and_politics":0.3626943005,
        "MMLU_high_school_macroeconomics":0.2820512821,
        "MMLU_high_school_mathematics":0.2333333333,
        "MMLU_high_school_microeconomics":0.281512605,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.3963302752,
        "MMLU_high_school_statistics":0.2222222222,
        "MMLU_high_school_us_history":0.3921568627,
        "MMLU_high_school_world_history":0.388185654,
        "MMLU_human_aging":0.4304932735,
        "MMLU_human_sexuality":0.3511450382,
        "MMLU_international_law":0.5041322314,
        "MMLU_jurisprudence":0.3703703704,
        "MMLU_logical_fallacies":0.3067484663,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.3203883495,
        "MMLU_marketing":0.4914529915,
        "MMLU_medical_genetics":0.33,
        "MMLU_miscellaneous":0.404853129,
        "MMLU_moral_disputes":0.3757225434,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.3529411765,
        "MMLU_philosophy":0.3279742765,
        "MMLU_prehistory":0.3641975309,
        "MMLU_professional_accounting":0.2765957447,
        "MMLU_professional_law":0.2920469361,
        "MMLU_professional_medicine":0.4154411765,
        "MMLU_professional_psychology":0.3676470588,
        "MMLU_public_relations":0.4090909091,
        "MMLU_security_studies":0.3346938776,
        "MMLU_sociology":0.3383084577,
        "MMLU_us_foreign_policy":0.4,
        "MMLU_virology":0.313253012,
        "MMLU_world_religions":0.4678362573
    },
    {
        "Model":"RWKV-v4-raven-14B-one-state",
        "URL":"https:\/\/huggingface.co\/xiaol\/RWKV-v4-raven-14B-one-state",
        "full_model_name":"xiaol\/RWKV-v4-raven-14B-one-state",
        "Parameters":14.0,
        "MMLU_average":0.3346997422,
        "arc:challenge|25":0.4146757679,
        "hellaswag|10":0.5230033858,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.3407407407,
        "MMLU_astronomy":0.2236842105,
        "MMLU_business_ethics":0.41,
        "MMLU_clinical_knowledge":0.3698113208,
        "MMLU_college_biology":0.2916666667,
        "MMLU_college_chemistry":0.17,
        "MMLU_college_computer_science":0.27,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.3005780347,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.41,
        "MMLU_conceptual_physics":0.3404255319,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.3448275862,
        "MMLU_elementary_mathematics":0.2486772487,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.3419354839,
        "MMLU_high_school_chemistry":0.2610837438,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.4848484848,
        "MMLU_high_school_geography":0.2929292929,
        "MMLU_high_school_government_and_politics":0.3730569948,
        "MMLU_high_school_macroeconomics":0.2769230769,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.268907563,
        "MMLU_high_school_physics":0.2119205298,
        "MMLU_high_school_psychology":0.3798165138,
        "MMLU_high_school_statistics":0.2037037037,
        "MMLU_high_school_us_history":0.4166666667,
        "MMLU_high_school_world_history":0.5232067511,
        "MMLU_human_aging":0.4708520179,
        "MMLU_human_sexuality":0.3435114504,
        "MMLU_international_law":0.4132231405,
        "MMLU_jurisprudence":0.4351851852,
        "MMLU_logical_fallacies":0.3128834356,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.2718446602,
        "MMLU_marketing":0.4487179487,
        "MMLU_medical_genetics":0.36,
        "MMLU_miscellaneous":0.4150702427,
        "MMLU_moral_disputes":0.3988439306,
        "MMLU_moral_scenarios":0.2458100559,
        "MMLU_nutrition":0.3137254902,
        "MMLU_philosophy":0.3408360129,
        "MMLU_prehistory":0.3364197531,
        "MMLU_professional_accounting":0.2659574468,
        "MMLU_professional_law":0.3311603651,
        "MMLU_professional_medicine":0.2867647059,
        "MMLU_professional_psychology":0.3235294118,
        "MMLU_public_relations":0.3727272727,
        "MMLU_security_studies":0.2448979592,
        "MMLU_sociology":0.4228855721,
        "MMLU_us_foreign_policy":0.43,
        "MMLU_virology":0.4277108434,
        "MMLU_world_religions":0.4444444444
    },
    {
        "Model":"CodeBarcenas-7b",
        "URL":"https:\/\/huggingface.co\/Danielbrdz\/CodeBarcenas-7b",
        "full_model_name":"Danielbrdz\/CodeBarcenas-7b",
        "Parameters":7.0,
        "MMLU_average":0.3338975584,
        "arc:challenge|25":0.3745733788,
        "hellaswag|10":0.4898426608,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.3259259259,
        "MMLU_astronomy":0.2039473684,
        "MMLU_business_ethics":0.32,
        "MMLU_clinical_knowledge":0.3735849057,
        "MMLU_college_biology":0.3194444444,
        "MMLU_college_chemistry":0.22,
        "MMLU_college_computer_science":0.22,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.2832369942,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.51,
        "MMLU_conceptual_physics":0.3063829787,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.4482758621,
        "MMLU_elementary_mathematics":0.2883597884,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.2903225806,
        "MMLU_high_school_chemistry":0.2463054187,
        "MMLU_high_school_computer_science":0.4,
        "MMLU_high_school_european_history":0.4060606061,
        "MMLU_high_school_geography":0.3585858586,
        "MMLU_high_school_government_and_politics":0.310880829,
        "MMLU_high_school_macroeconomics":0.2974358974,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.3025210084,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.3155963303,
        "MMLU_high_school_statistics":0.2453703704,
        "MMLU_high_school_us_history":0.3480392157,
        "MMLU_high_school_world_history":0.4388185654,
        "MMLU_human_aging":0.3901345291,
        "MMLU_human_sexuality":0.3893129771,
        "MMLU_international_law":0.347107438,
        "MMLU_jurisprudence":0.3796296296,
        "MMLU_logical_fallacies":0.3006134969,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.3495145631,
        "MMLU_marketing":0.5555555556,
        "MMLU_medical_genetics":0.34,
        "MMLU_miscellaneous":0.4610472542,
        "MMLU_moral_disputes":0.3612716763,
        "MMLU_moral_scenarios":0.2938547486,
        "MMLU_nutrition":0.3529411765,
        "MMLU_philosophy":0.347266881,
        "MMLU_prehistory":0.3117283951,
        "MMLU_professional_accounting":0.304964539,
        "MMLU_professional_law":0.2698826597,
        "MMLU_professional_medicine":0.2132352941,
        "MMLU_professional_psychology":0.3088235294,
        "MMLU_public_relations":0.4272727273,
        "MMLU_security_studies":0.4081632653,
        "MMLU_sociology":0.3731343284,
        "MMLU_us_foreign_policy":0.41,
        "MMLU_virology":0.4036144578,
        "MMLU_world_religions":0.4736842105
    },
    {
        "Model":"tora-code-7b-v1.0",
        "URL":"https:\/\/huggingface.co\/llm-agents\/tora-code-7b-v1.0",
        "full_model_name":"llm-agents\/tora-code-7b-v1.0",
        "Parameters":7.0,
        "MMLU_average":0.3333657268,
        "arc:challenge|25":0.3788395904,
        "hellaswag|10":0.5027882892,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.3037037037,
        "MMLU_astronomy":0.2697368421,
        "MMLU_business_ethics":0.46,
        "MMLU_clinical_knowledge":0.3056603774,
        "MMLU_college_biology":0.3472222222,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.289017341,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.5,
        "MMLU_conceptual_physics":0.314893617,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.3103448276,
        "MMLU_elementary_mathematics":0.2883597884,
        "MMLU_formal_logic":0.2777777778,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.3451612903,
        "MMLU_high_school_chemistry":0.2266009852,
        "MMLU_high_school_computer_science":0.37,
        "MMLU_high_school_european_history":0.4848484848,
        "MMLU_high_school_geography":0.297979798,
        "MMLU_high_school_government_and_politics":0.3212435233,
        "MMLU_high_school_macroeconomics":0.3051282051,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.2857142857,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.3504587156,
        "MMLU_high_school_statistics":0.2777777778,
        "MMLU_high_school_us_history":0.3725490196,
        "MMLU_high_school_world_history":0.4978902954,
        "MMLU_human_aging":0.3677130045,
        "MMLU_human_sexuality":0.3816793893,
        "MMLU_international_law":0.3388429752,
        "MMLU_jurisprudence":0.3981481481,
        "MMLU_logical_fallacies":0.2760736196,
        "MMLU_machine_learning":0.2232142857,
        "MMLU_management":0.3300970874,
        "MMLU_marketing":0.5042735043,
        "MMLU_medical_genetics":0.4,
        "MMLU_miscellaneous":0.4214559387,
        "MMLU_moral_disputes":0.3410404624,
        "MMLU_moral_scenarios":0.2391061453,
        "MMLU_nutrition":0.3562091503,
        "MMLU_philosophy":0.3697749196,
        "MMLU_prehistory":0.3271604938,
        "MMLU_professional_accounting":0.2765957447,
        "MMLU_professional_law":0.2926988266,
        "MMLU_professional_medicine":0.2279411765,
        "MMLU_professional_psychology":0.3055555556,
        "MMLU_public_relations":0.3909090909,
        "MMLU_security_studies":0.4,
        "MMLU_sociology":0.4129353234,
        "MMLU_us_foreign_policy":0.4,
        "MMLU_virology":0.343373494,
        "MMLU_world_religions":0.4269005848
    },
    {
        "Model":"openthaigpt-0.1.0-beta-full-model_for_open_llm_leaderboard",
        "URL":"https:\/\/huggingface.co\/wannaphong\/openthaigpt-0.1.0-beta-full-model_for_open_llm_leaderboard",
        "full_model_name":"wannaphong\/openthaigpt-0.1.0-beta-full-model_for_open_llm_leaderboard",
        "Parameters":null,
        "MMLU_average":0.3318487741,
        "arc:challenge|25":0.4684300341,
        "hellaswag|10":0.5763792073,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.3481481481,
        "MMLU_astronomy":0.3157894737,
        "MMLU_business_ethics":0.42,
        "MMLU_clinical_knowledge":0.3773584906,
        "MMLU_college_biology":0.2916666667,
        "MMLU_college_chemistry":0.27,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.3063583815,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.41,
        "MMLU_conceptual_physics":0.3617021277,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.2275862069,
        "MMLU_elementary_mathematics":0.2433862434,
        "MMLU_formal_logic":0.2222222222,
        "MMLU_global_facts":0.28,
        "MMLU_high_school_biology":0.3387096774,
        "MMLU_high_school_chemistry":0.2807881773,
        "MMLU_high_school_computer_science":0.31,
        "MMLU_high_school_european_history":0.4181818182,
        "MMLU_high_school_geography":0.4191919192,
        "MMLU_high_school_government_and_politics":0.4248704663,
        "MMLU_high_school_macroeconomics":0.3128205128,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.2941176471,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.3577981651,
        "MMLU_high_school_statistics":0.2824074074,
        "MMLU_high_school_us_history":0.3725490196,
        "MMLU_high_school_world_history":0.3755274262,
        "MMLU_human_aging":0.3677130045,
        "MMLU_human_sexuality":0.320610687,
        "MMLU_international_law":0.5702479339,
        "MMLU_jurisprudence":0.3518518519,
        "MMLU_logical_fallacies":0.3435582822,
        "MMLU_machine_learning":0.2142857143,
        "MMLU_management":0.3203883495,
        "MMLU_marketing":0.4188034188,
        "MMLU_medical_genetics":0.34,
        "MMLU_miscellaneous":0.4125159642,
        "MMLU_moral_disputes":0.3439306358,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.3333333333,
        "MMLU_philosophy":0.3376205788,
        "MMLU_prehistory":0.3580246914,
        "MMLU_professional_accounting":0.2695035461,
        "MMLU_professional_law":0.3089960887,
        "MMLU_professional_medicine":0.3308823529,
        "MMLU_professional_psychology":0.3284313725,
        "MMLU_public_relations":0.4,
        "MMLU_security_studies":0.2775510204,
        "MMLU_sociology":0.4228855721,
        "MMLU_us_foreign_policy":0.4,
        "MMLU_virology":0.3614457831,
        "MMLU_world_religions":0.3801169591
    },
    {
        "Model":"CodeLlama-34B-Python-fp16",
        "URL":"https:\/\/huggingface.co\/TheBloke\/CodeLlama-34B-Python-fp16",
        "full_model_name":"TheBloke\/CodeLlama-34B-Python-fp16",
        "Parameters":34.0,
        "MMLU_average":0.3294843621,
        "arc:challenge|25":0.3575085324,
        "hellaswag|10":0.2992431786,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.3259259259,
        "MMLU_astronomy":0.3223684211,
        "MMLU_business_ethics":0.27,
        "MMLU_clinical_knowledge":0.3660377358,
        "MMLU_college_biology":0.3819444444,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.34,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.323699422,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.41,
        "MMLU_conceptual_physics":0.3191489362,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.275862069,
        "MMLU_elementary_mathematics":0.2592592593,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.21,
        "MMLU_high_school_biology":0.3580645161,
        "MMLU_high_school_chemistry":0.2019704433,
        "MMLU_high_school_computer_science":0.37,
        "MMLU_high_school_european_history":0.2545454545,
        "MMLU_high_school_geography":0.398989899,
        "MMLU_high_school_government_and_politics":0.4766839378,
        "MMLU_high_school_macroeconomics":0.3435897436,
        "MMLU_high_school_mathematics":0.2074074074,
        "MMLU_high_school_microeconomics":0.3613445378,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.3596330275,
        "MMLU_high_school_statistics":0.3009259259,
        "MMLU_high_school_us_history":0.2745098039,
        "MMLU_high_school_world_history":0.3966244726,
        "MMLU_human_aging":0.3228699552,
        "MMLU_human_sexuality":0.2977099237,
        "MMLU_international_law":0.3388429752,
        "MMLU_jurisprudence":0.3055555556,
        "MMLU_logical_fallacies":0.226993865,
        "MMLU_machine_learning":0.2410714286,
        "MMLU_management":0.4660194175,
        "MMLU_marketing":0.4487179487,
        "MMLU_medical_genetics":0.31,
        "MMLU_miscellaneous":0.4508301405,
        "MMLU_moral_disputes":0.3410404624,
        "MMLU_moral_scenarios":0.2636871508,
        "MMLU_nutrition":0.3823529412,
        "MMLU_philosophy":0.4308681672,
        "MMLU_prehistory":0.3055555556,
        "MMLU_professional_accounting":0.2588652482,
        "MMLU_professional_law":0.2640156454,
        "MMLU_professional_medicine":0.4007352941,
        "MMLU_professional_psychology":0.2875816993,
        "MMLU_public_relations":0.3454545455,
        "MMLU_security_studies":0.3714285714,
        "MMLU_sociology":0.2736318408,
        "MMLU_us_foreign_policy":0.34,
        "MMLU_virology":0.313253012,
        "MMLU_world_religions":0.5555555556
    },
    {
        "Model":"CodeLlama-13b-hf",
        "URL":"https:\/\/huggingface.co\/codellama\/CodeLlama-13b-hf",
        "full_model_name":"codellama\/CodeLlama-13b-hf",
        "Parameters":13.0,
        "MMLU_average":0.3280883582,
        "arc:challenge|25":0.3779863481,
        "hellaswag|10":0.4696275642,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.3259259259,
        "MMLU_astronomy":0.2960526316,
        "MMLU_business_ethics":0.27,
        "MMLU_clinical_knowledge":0.3018867925,
        "MMLU_college_biology":0.2291666667,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.27,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.3005780347,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.3,
        "MMLU_conceptual_physics":0.3361702128,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.4275862069,
        "MMLU_elementary_mathematics":0.2645502646,
        "MMLU_formal_logic":0.2222222222,
        "MMLU_global_facts":0.24,
        "MMLU_high_school_biology":0.3419354839,
        "MMLU_high_school_chemistry":0.2660098522,
        "MMLU_high_school_computer_science":0.24,
        "MMLU_high_school_european_history":0.2727272727,
        "MMLU_high_school_geography":0.4696969697,
        "MMLU_high_school_government_and_politics":0.4715025907,
        "MMLU_high_school_macroeconomics":0.341025641,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.3445378151,
        "MMLU_high_school_physics":0.238410596,
        "MMLU_high_school_psychology":0.319266055,
        "MMLU_high_school_statistics":0.4814814815,
        "MMLU_high_school_us_history":0.3088235294,
        "MMLU_high_school_world_history":0.3628691983,
        "MMLU_human_aging":0.3901345291,
        "MMLU_human_sexuality":0.358778626,
        "MMLU_international_law":0.2561983471,
        "MMLU_jurisprudence":0.3148148148,
        "MMLU_logical_fallacies":0.3006134969,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.3883495146,
        "MMLU_marketing":0.5213675214,
        "MMLU_medical_genetics":0.37,
        "MMLU_miscellaneous":0.4380587484,
        "MMLU_moral_disputes":0.2167630058,
        "MMLU_moral_scenarios":0.2536312849,
        "MMLU_nutrition":0.3431372549,
        "MMLU_philosophy":0.3890675241,
        "MMLU_prehistory":0.3672839506,
        "MMLU_professional_accounting":0.2695035461,
        "MMLU_professional_law":0.2966101695,
        "MMLU_professional_medicine":0.3051470588,
        "MMLU_professional_psychology":0.2794117647,
        "MMLU_public_relations":0.3,
        "MMLU_security_studies":0.3224489796,
        "MMLU_sociology":0.4179104478,
        "MMLU_us_foreign_policy":0.51,
        "MMLU_virology":0.3493975904,
        "MMLU_world_religions":0.432748538
    },
    {
        "Model":"Tulpar-tv_marcoroni-7b",
        "URL":"https:\/\/huggingface.co\/aqweteddy\/Tulpar-tv_marcoroni-7b",
        "full_model_name":"aqweteddy\/Tulpar-tv_marcoroni-7b",
        "Parameters":7.0,
        "MMLU_average":0.3272438704,
        "arc:challenge|25":0.3899317406,
        "hellaswag|10":0.5012945628,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.3925925926,
        "MMLU_astronomy":0.2960526316,
        "MMLU_business_ethics":0.46,
        "MMLU_clinical_knowledge":0.3660377358,
        "MMLU_college_biology":0.3333333333,
        "MMLU_college_chemistry":0.26,
        "MMLU_college_computer_science":0.14,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.2601156069,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.44,
        "MMLU_conceptual_physics":0.3446808511,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.2689655172,
        "MMLU_elementary_mathematics":0.2592592593,
        "MMLU_formal_logic":0.2063492063,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.364516129,
        "MMLU_high_school_chemistry":0.2955665025,
        "MMLU_high_school_computer_science":0.38,
        "MMLU_high_school_european_history":0.4424242424,
        "MMLU_high_school_geography":0.303030303,
        "MMLU_high_school_government_and_politics":0.3160621762,
        "MMLU_high_school_macroeconomics":0.2435897436,
        "MMLU_high_school_mathematics":0.2259259259,
        "MMLU_high_school_microeconomics":0.2394957983,
        "MMLU_high_school_physics":0.2450331126,
        "MMLU_high_school_psychology":0.3504587156,
        "MMLU_high_school_statistics":0.1944444444,
        "MMLU_high_school_us_history":0.4460784314,
        "MMLU_high_school_world_history":0.5316455696,
        "MMLU_human_aging":0.3856502242,
        "MMLU_human_sexuality":0.3358778626,
        "MMLU_international_law":0.3636363636,
        "MMLU_jurisprudence":0.3148148148,
        "MMLU_logical_fallacies":0.3006134969,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.3398058252,
        "MMLU_marketing":0.4658119658,
        "MMLU_medical_genetics":0.31,
        "MMLU_miscellaneous":0.4738186462,
        "MMLU_moral_disputes":0.3179190751,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.3071895425,
        "MMLU_philosophy":0.3118971061,
        "MMLU_prehistory":0.4104938272,
        "MMLU_professional_accounting":0.2482269504,
        "MMLU_professional_law":0.2887874837,
        "MMLU_professional_medicine":0.3235294118,
        "MMLU_professional_psychology":0.3447712418,
        "MMLU_public_relations":0.4090909091,
        "MMLU_security_studies":0.3469387755,
        "MMLU_sociology":0.3134328358,
        "MMLU_us_foreign_policy":0.42,
        "MMLU_virology":0.3734939759,
        "MMLU_world_religions":0.4678362573
    },
    {
        "Model":"llama_7b_qlora_cds",
        "URL":"https:\/\/huggingface.co\/DevaMalla\/llama_7b_qlora_cds",
        "full_model_name":"DevaMalla\/llama_7b_qlora_cds",
        "Parameters":7.0,
        "MMLU_average":0.3238338164,
        "arc:challenge|25":0.4965870307,
        "hellaswag|10":0.5828520215,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.362962963,
        "MMLU_astronomy":0.2631578947,
        "MMLU_business_ethics":0.43,
        "MMLU_clinical_knowledge":0.3698113208,
        "MMLU_college_biology":0.2986111111,
        "MMLU_college_chemistry":0.18,
        "MMLU_college_computer_science":0.29,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.2832369942,
        "MMLU_college_physics":0.1764705882,
        "MMLU_computer_security":0.36,
        "MMLU_conceptual_physics":0.3574468085,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2482758621,
        "MMLU_elementary_mathematics":0.2777777778,
        "MMLU_formal_logic":0.2142857143,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.3193548387,
        "MMLU_high_school_chemistry":0.236453202,
        "MMLU_high_school_computer_science":0.3,
        "MMLU_high_school_european_history":0.4181818182,
        "MMLU_high_school_geography":0.2878787879,
        "MMLU_high_school_government_and_politics":0.3730569948,
        "MMLU_high_school_macroeconomics":0.2717948718,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.281512605,
        "MMLU_high_school_physics":0.1920529801,
        "MMLU_high_school_psychology":0.3743119266,
        "MMLU_high_school_statistics":0.1851851852,
        "MMLU_high_school_us_history":0.3382352941,
        "MMLU_high_school_world_history":0.3459915612,
        "MMLU_human_aging":0.4260089686,
        "MMLU_human_sexuality":0.3053435115,
        "MMLU_international_law":0.5619834711,
        "MMLU_jurisprudence":0.3796296296,
        "MMLU_logical_fallacies":0.3680981595,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.3203883495,
        "MMLU_marketing":0.4316239316,
        "MMLU_medical_genetics":0.42,
        "MMLU_miscellaneous":0.3920817369,
        "MMLU_moral_disputes":0.3612716763,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.3496732026,
        "MMLU_philosophy":0.2797427653,
        "MMLU_prehistory":0.3641975309,
        "MMLU_professional_accounting":0.2907801418,
        "MMLU_professional_law":0.2848761408,
        "MMLU_professional_medicine":0.3235294118,
        "MMLU_professional_psychology":0.3480392157,
        "MMLU_public_relations":0.4090909091,
        "MMLU_security_studies":0.2367346939,
        "MMLU_sociology":0.3333333333,
        "MMLU_us_foreign_policy":0.41,
        "MMLU_virology":0.3253012048,
        "MMLU_world_religions":0.4795321637
    },
    {
        "Model":"codegen-16B-nl",
        "URL":"https:\/\/huggingface.co\/Salesforce\/codegen-16B-nl",
        "full_model_name":"Salesforce\/codegen-16B-nl",
        "Parameters":16.0,
        "MMLU_average":0.3235430166,
        "arc:challenge|25":0.4180887372,
        "hellaswag|10":0.5323640709,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.3259259259,
        "MMLU_astronomy":0.3026315789,
        "MMLU_business_ethics":0.28,
        "MMLU_clinical_knowledge":0.3169811321,
        "MMLU_college_biology":0.3333333333,
        "MMLU_college_chemistry":0.26,
        "MMLU_college_computer_science":0.28,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.2774566474,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.41,
        "MMLU_conceptual_physics":0.3404255319,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.4275862069,
        "MMLU_elementary_mathematics":0.2407407407,
        "MMLU_formal_logic":0.2380952381,
        "MMLU_global_facts":0.29,
        "MMLU_high_school_biology":0.3161290323,
        "MMLU_high_school_chemistry":0.2266009852,
        "MMLU_high_school_computer_science":0.31,
        "MMLU_high_school_european_history":0.303030303,
        "MMLU_high_school_geography":0.3535353535,
        "MMLU_high_school_government_and_politics":0.3367875648,
        "MMLU_high_school_macroeconomics":0.2307692308,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.243697479,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.2844036697,
        "MMLU_high_school_statistics":0.1990740741,
        "MMLU_high_school_us_history":0.3529411765,
        "MMLU_high_school_world_history":0.3417721519,
        "MMLU_human_aging":0.3766816143,
        "MMLU_human_sexuality":0.3816793893,
        "MMLU_international_law":0.4297520661,
        "MMLU_jurisprudence":0.3981481481,
        "MMLU_logical_fallacies":0.2883435583,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.2912621359,
        "MMLU_marketing":0.3675213675,
        "MMLU_medical_genetics":0.43,
        "MMLU_miscellaneous":0.374201788,
        "MMLU_moral_disputes":0.3670520231,
        "MMLU_moral_scenarios":0.2826815642,
        "MMLU_nutrition":0.3790849673,
        "MMLU_philosophy":0.3344051447,
        "MMLU_prehistory":0.3611111111,
        "MMLU_professional_accounting":0.2553191489,
        "MMLU_professional_law":0.3239895698,
        "MMLU_professional_medicine":0.2977941176,
        "MMLU_professional_psychology":0.2973856209,
        "MMLU_public_relations":0.3545454545,
        "MMLU_security_studies":0.4285714286,
        "MMLU_sociology":0.3781094527,
        "MMLU_us_foreign_policy":0.5,
        "MMLU_virology":0.3313253012,
        "MMLU_world_religions":0.3918128655
    },
    {
        "Model":"bloom-zh-3b-chat",
        "URL":"https:\/\/huggingface.co\/ikala\/bloom-zh-3b-chat",
        "full_model_name":"ikala\/bloom-zh-3b-chat",
        "Parameters":3.0,
        "MMLU_average":0.3162389835,
        "arc:challenge|25":0.3686006826,
        "hellaswag|10":0.4208325035,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.2962962963,
        "MMLU_astronomy":0.2434210526,
        "MMLU_business_ethics":0.35,
        "MMLU_clinical_knowledge":0.2867924528,
        "MMLU_college_biology":0.3541666667,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.37,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.3121387283,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.29,
        "MMLU_conceptual_physics":0.3191489362,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.2827586207,
        "MMLU_elementary_mathematics":0.2354497354,
        "MMLU_formal_logic":0.2619047619,
        "MMLU_global_facts":0.21,
        "MMLU_high_school_biology":0.2580645161,
        "MMLU_high_school_chemistry":0.2019704433,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.3333333333,
        "MMLU_high_school_geography":0.3838383838,
        "MMLU_high_school_government_and_politics":0.3523316062,
        "MMLU_high_school_macroeconomics":0.3564102564,
        "MMLU_high_school_mathematics":0.2148148148,
        "MMLU_high_school_microeconomics":0.2857142857,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.3486238532,
        "MMLU_high_school_statistics":0.3981481481,
        "MMLU_high_school_us_history":0.2941176471,
        "MMLU_high_school_world_history":0.3924050633,
        "MMLU_human_aging":0.399103139,
        "MMLU_human_sexuality":0.3358778626,
        "MMLU_international_law":0.2727272727,
        "MMLU_jurisprudence":0.3240740741,
        "MMLU_logical_fallacies":0.2576687117,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.3106796117,
        "MMLU_marketing":0.3931623932,
        "MMLU_medical_genetics":0.35,
        "MMLU_miscellaneous":0.3231162197,
        "MMLU_moral_disputes":0.3208092486,
        "MMLU_moral_scenarios":0.2681564246,
        "MMLU_nutrition":0.3039215686,
        "MMLU_philosophy":0.2893890675,
        "MMLU_prehistory":0.3518518519,
        "MMLU_professional_accounting":0.2872340426,
        "MMLU_professional_law":0.2672750978,
        "MMLU_professional_medicine":0.4080882353,
        "MMLU_professional_psychology":0.341503268,
        "MMLU_public_relations":0.4272727273,
        "MMLU_security_studies":0.3510204082,
        "MMLU_sociology":0.4228855721,
        "MMLU_us_foreign_policy":0.38,
        "MMLU_virology":0.3072289157,
        "MMLU_world_religions":0.3157894737
    },
    {
        "Model":"landmark-attention-llama7b-fp16",
        "URL":"https:\/\/huggingface.co\/TheBloke\/landmark-attention-llama7b-fp16",
        "full_model_name":"TheBloke\/landmark-attention-llama7b-fp16",
        "Parameters":7.0,
        "MMLU_average":0.3159215173,
        "arc:challenge|25":0.4445392491,
        "hellaswag|10":0.5030870345,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.362962963,
        "MMLU_astronomy":0.3026315789,
        "MMLU_business_ethics":0.42,
        "MMLU_clinical_knowledge":0.3056603774,
        "MMLU_college_biology":0.3402777778,
        "MMLU_college_chemistry":0.18,
        "MMLU_college_computer_science":0.27,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.2485549133,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.4,
        "MMLU_conceptual_physics":0.3744680851,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.275862069,
        "MMLU_elementary_mathematics":0.2645502646,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.3032258065,
        "MMLU_high_school_chemistry":0.2216748768,
        "MMLU_high_school_computer_science":0.29,
        "MMLU_high_school_european_history":0.3696969697,
        "MMLU_high_school_geography":0.2474747475,
        "MMLU_high_school_government_and_politics":0.3626943005,
        "MMLU_high_school_macroeconomics":0.2615384615,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.2773109244,
        "MMLU_high_school_physics":0.2317880795,
        "MMLU_high_school_psychology":0.3137614679,
        "MMLU_high_school_statistics":0.2083333333,
        "MMLU_high_school_us_history":0.3823529412,
        "MMLU_high_school_world_history":0.3459915612,
        "MMLU_human_aging":0.4349775785,
        "MMLU_human_sexuality":0.3129770992,
        "MMLU_international_law":0.4462809917,
        "MMLU_jurisprudence":0.3240740741,
        "MMLU_logical_fallacies":0.4049079755,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.2815533981,
        "MMLU_marketing":0.4017094017,
        "MMLU_medical_genetics":0.37,
        "MMLU_miscellaneous":0.3767560664,
        "MMLU_moral_disputes":0.3208092486,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.339869281,
        "MMLU_philosophy":0.3054662379,
        "MMLU_prehistory":0.3055555556,
        "MMLU_professional_accounting":0.2517730496,
        "MMLU_professional_law":0.2724902216,
        "MMLU_professional_medicine":0.2867647059,
        "MMLU_professional_psychology":0.362745098,
        "MMLU_public_relations":0.3727272727,
        "MMLU_security_studies":0.2244897959,
        "MMLU_sociology":0.3432835821,
        "MMLU_us_foreign_policy":0.47,
        "MMLU_virology":0.3253012048,
        "MMLU_world_religions":0.4561403509
    },
    {
        "Model":"gpt-sw3-6.7b-v2-instruct",
        "URL":"https:\/\/huggingface.co\/AI-Sweden-Models\/gpt-sw3-6.7b-v2-instruct",
        "full_model_name":"AI-Sweden-Models\/gpt-sw3-6.7b-v2-instruct",
        "Parameters":6.7,
        "MMLU_average":0.315697227,
        "arc:challenge|25":0.3575085324,
        "hellaswag|10":0.5046803426,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.362962963,
        "MMLU_astronomy":0.3289473684,
        "MMLU_business_ethics":0.35,
        "MMLU_clinical_knowledge":0.3396226415,
        "MMLU_college_biology":0.3680555556,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.3352601156,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.35,
        "MMLU_conceptual_physics":0.3191489362,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.275862069,
        "MMLU_elementary_mathematics":0.2989417989,
        "MMLU_formal_logic":0.2619047619,
        "MMLU_global_facts":0.22,
        "MMLU_high_school_biology":0.3161290323,
        "MMLU_high_school_chemistry":0.1921182266,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.3393939394,
        "MMLU_high_school_geography":0.3131313131,
        "MMLU_high_school_government_and_politics":0.3523316062,
        "MMLU_high_school_macroeconomics":0.2564102564,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.268907563,
        "MMLU_high_school_physics":0.2251655629,
        "MMLU_high_school_psychology":0.423853211,
        "MMLU_high_school_statistics":0.2037037037,
        "MMLU_high_school_us_history":0.3088235294,
        "MMLU_high_school_world_history":0.4050632911,
        "MMLU_human_aging":0.3856502242,
        "MMLU_human_sexuality":0.4045801527,
        "MMLU_international_law":0.4214876033,
        "MMLU_jurisprudence":0.3518518519,
        "MMLU_logical_fallacies":0.2699386503,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.3495145631,
        "MMLU_marketing":0.4316239316,
        "MMLU_medical_genetics":0.42,
        "MMLU_miscellaneous":0.3908045977,
        "MMLU_moral_disputes":0.3150289017,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.3496732026,
        "MMLU_philosophy":0.2893890675,
        "MMLU_prehistory":0.3240740741,
        "MMLU_professional_accounting":0.3262411348,
        "MMLU_professional_law":0.2731421121,
        "MMLU_professional_medicine":0.375,
        "MMLU_professional_psychology":0.3039215686,
        "MMLU_public_relations":0.3636363636,
        "MMLU_security_studies":0.2244897959,
        "MMLU_sociology":0.3482587065,
        "MMLU_us_foreign_policy":0.36,
        "MMLU_virology":0.3012048193,
        "MMLU_world_religions":0.3801169591
    },
    {
        "Model":"open-llama-7b-open-instruct",
        "URL":"https:\/\/huggingface.co\/VMware\/open-llama-7b-open-instruct",
        "full_model_name":"VMware\/open-llama-7b-open-instruct",
        "Parameters":7.0,
        "MMLU_average":0.3152201179,
        "arc:challenge|25":0.4598976109,
        "hellaswag|10":0.5516829317,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.2592592593,
        "MMLU_astronomy":0.2960526316,
        "MMLU_business_ethics":0.29,
        "MMLU_clinical_knowledge":0.3320754717,
        "MMLU_college_biology":0.2777777778,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.34,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.2774566474,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.29,
        "MMLU_conceptual_physics":0.3361702128,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.2777777778,
        "MMLU_formal_logic":0.4047619048,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.3161290323,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.24,
        "MMLU_high_school_european_history":0.296969697,
        "MMLU_high_school_geography":0.3232323232,
        "MMLU_high_school_government_and_politics":0.3834196891,
        "MMLU_high_school_macroeconomics":0.3102564103,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.4075630252,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.376146789,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.3333333333,
        "MMLU_high_school_world_history":0.3080168776,
        "MMLU_human_aging":0.4080717489,
        "MMLU_human_sexuality":0.3053435115,
        "MMLU_international_law":0.3140495868,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.3128834356,
        "MMLU_machine_learning":0.2232142857,
        "MMLU_management":0.2524271845,
        "MMLU_marketing":0.3504273504,
        "MMLU_medical_genetics":0.31,
        "MMLU_miscellaneous":0.3601532567,
        "MMLU_moral_disputes":0.2832369942,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.362745098,
        "MMLU_philosophy":0.2958199357,
        "MMLU_prehistory":0.3456790123,
        "MMLU_professional_accounting":0.2659574468,
        "MMLU_professional_law":0.2503259452,
        "MMLU_professional_medicine":0.3933823529,
        "MMLU_professional_psychology":0.2418300654,
        "MMLU_public_relations":0.4090909091,
        "MMLU_security_studies":0.3142857143,
        "MMLU_sociology":0.3134328358,
        "MMLU_us_foreign_policy":0.38,
        "MMLU_virology":0.3313253012,
        "MMLU_world_religions":0.4561403509
    },
    {
        "Model":"bloomz-3b-sft-chat",
        "URL":"https:\/\/huggingface.co\/cmarkea\/bloomz-3b-sft-chat",
        "full_model_name":"cmarkea\/bloomz-3b-sft-chat",
        "Parameters":3.0,
        "MMLU_average":0.3149431971,
        "arc:challenge|25":0.3489761092,
        "hellaswag|10":0.414060944,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.362962963,
        "MMLU_astronomy":0.2368421053,
        "MMLU_business_ethics":0.36,
        "MMLU_clinical_knowledge":0.3320754717,
        "MMLU_college_biology":0.3125,
        "MMLU_college_chemistry":0.29,
        "MMLU_college_computer_science":0.3,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.2427745665,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.36,
        "MMLU_conceptual_physics":0.3829787234,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.2965517241,
        "MMLU_elementary_mathematics":0.2645502646,
        "MMLU_formal_logic":0.1984126984,
        "MMLU_global_facts":0.23,
        "MMLU_high_school_biology":0.3290322581,
        "MMLU_high_school_chemistry":0.2709359606,
        "MMLU_high_school_computer_science":0.3,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.3636363636,
        "MMLU_high_school_government_and_politics":0.2901554404,
        "MMLU_high_school_macroeconomics":0.3025641026,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.2647058824,
        "MMLU_high_school_physics":0.2052980132,
        "MMLU_high_school_psychology":0.4146788991,
        "MMLU_high_school_statistics":0.2962962963,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.4556962025,
        "MMLU_human_aging":0.4708520179,
        "MMLU_human_sexuality":0.3282442748,
        "MMLU_international_law":0.3305785124,
        "MMLU_jurisprudence":0.4166666667,
        "MMLU_logical_fallacies":0.2883435583,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.3786407767,
        "MMLU_marketing":0.4914529915,
        "MMLU_medical_genetics":0.33,
        "MMLU_miscellaneous":0.4022988506,
        "MMLU_moral_disputes":0.3034682081,
        "MMLU_moral_scenarios":0.2446927374,
        "MMLU_nutrition":0.2941176471,
        "MMLU_philosophy":0.3762057878,
        "MMLU_prehistory":0.3333333333,
        "MMLU_professional_accounting":0.2624113475,
        "MMLU_professional_law":0.2750977836,
        "MMLU_professional_medicine":0.3161764706,
        "MMLU_professional_psychology":0.3513071895,
        "MMLU_public_relations":0.4454545455,
        "MMLU_security_studies":0.2244897959,
        "MMLU_sociology":0.3383084577,
        "MMLU_us_foreign_policy":0.39,
        "MMLU_virology":0.343373494,
        "MMLU_world_religions":0.2748538012
    },
    {
        "Model":"gpt-sw3-20b-instruct",
        "URL":"https:\/\/huggingface.co\/AI-Sweden-Models\/gpt-sw3-20b-instruct",
        "full_model_name":"AI-Sweden-Models\/gpt-sw3-20b-instruct",
        "Parameters":20.0,
        "MMLU_average":0.3132079185,
        "arc:challenge|25":0.4215017065,
        "hellaswag|10":0.5312686716,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.3333333333,
        "MMLU_astronomy":0.2565789474,
        "MMLU_business_ethics":0.33,
        "MMLU_clinical_knowledge":0.3283018868,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.23,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.2658959538,
        "MMLU_college_physics":0.1666666667,
        "MMLU_computer_security":0.36,
        "MMLU_conceptual_physics":0.3489361702,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.3310344828,
        "MMLU_elementary_mathematics":0.2592592593,
        "MMLU_formal_logic":0.2063492063,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.3161290323,
        "MMLU_high_school_chemistry":0.315270936,
        "MMLU_high_school_computer_science":0.36,
        "MMLU_high_school_european_history":0.4181818182,
        "MMLU_high_school_geography":0.297979798,
        "MMLU_high_school_government_and_politics":0.3367875648,
        "MMLU_high_school_macroeconomics":0.2615384615,
        "MMLU_high_school_mathematics":0.2222222222,
        "MMLU_high_school_microeconomics":0.2731092437,
        "MMLU_high_school_physics":0.2251655629,
        "MMLU_high_school_psychology":0.3577981651,
        "MMLU_high_school_statistics":0.1898148148,
        "MMLU_high_school_us_history":0.3578431373,
        "MMLU_high_school_world_history":0.4135021097,
        "MMLU_human_aging":0.4484304933,
        "MMLU_human_sexuality":0.3282442748,
        "MMLU_international_law":0.3884297521,
        "MMLU_jurisprudence":0.3796296296,
        "MMLU_logical_fallacies":0.2944785276,
        "MMLU_machine_learning":0.2946428571,
        "MMLU_management":0.3495145631,
        "MMLU_marketing":0.4444444444,
        "MMLU_medical_genetics":0.46,
        "MMLU_miscellaneous":0.3703703704,
        "MMLU_moral_disputes":0.3410404624,
        "MMLU_moral_scenarios":0.243575419,
        "MMLU_nutrition":0.2941176471,
        "MMLU_philosophy":0.270096463,
        "MMLU_prehistory":0.3209876543,
        "MMLU_professional_accounting":0.2624113475,
        "MMLU_professional_law":0.2933507171,
        "MMLU_professional_medicine":0.2904411765,
        "MMLU_professional_psychology":0.3022875817,
        "MMLU_public_relations":0.3272727273,
        "MMLU_security_studies":0.2653061224,
        "MMLU_sociology":0.3432835821,
        "MMLU_us_foreign_policy":0.42,
        "MMLU_virology":0.3373493976,
        "MMLU_world_religions":0.3625730994
    },
    {
        "Model":"Janin-R",
        "URL":"https:\/\/huggingface.co\/digitous\/Janin-R",
        "full_model_name":"digitous\/Janin-R",
        "Parameters":null,
        "MMLU_average":0.3124330475,
        "arc:challenge|25":0.3728668942,
        "hellaswag|10":0.4986058554,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.3481481481,
        "MMLU_astronomy":0.3355263158,
        "MMLU_business_ethics":0.26,
        "MMLU_clinical_knowledge":0.3660377358,
        "MMLU_college_biology":0.2916666667,
        "MMLU_college_chemistry":0.23,
        "MMLU_college_computer_science":0.23,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.3005780347,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.37,
        "MMLU_conceptual_physics":0.3404255319,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.3172413793,
        "MMLU_elementary_mathematics":0.2724867725,
        "MMLU_formal_logic":0.1746031746,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.2580645161,
        "MMLU_high_school_chemistry":0.275862069,
        "MMLU_high_school_computer_science":0.19,
        "MMLU_high_school_european_history":0.3212121212,
        "MMLU_high_school_geography":0.303030303,
        "MMLU_high_school_government_and_politics":0.3523316062,
        "MMLU_high_school_macroeconomics":0.3282051282,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.3025210084,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.2880733945,
        "MMLU_high_school_statistics":0.2222222222,
        "MMLU_high_school_us_history":0.3137254902,
        "MMLU_high_school_world_history":0.3459915612,
        "MMLU_human_aging":0.3766816143,
        "MMLU_human_sexuality":0.2748091603,
        "MMLU_international_law":0.4297520661,
        "MMLU_jurisprudence":0.3333333333,
        "MMLU_logical_fallacies":0.2944785276,
        "MMLU_machine_learning":0.2946428571,
        "MMLU_management":0.3398058252,
        "MMLU_marketing":0.311965812,
        "MMLU_medical_genetics":0.32,
        "MMLU_miscellaneous":0.3180076628,
        "MMLU_moral_disputes":0.323699422,
        "MMLU_moral_scenarios":0.2446927374,
        "MMLU_nutrition":0.3758169935,
        "MMLU_philosophy":0.3440514469,
        "MMLU_prehistory":0.3240740741,
        "MMLU_professional_accounting":0.2765957447,
        "MMLU_professional_law":0.3181225554,
        "MMLU_professional_medicine":0.3419117647,
        "MMLU_professional_psychology":0.2973856209,
        "MMLU_public_relations":0.3636363636,
        "MMLU_security_studies":0.4530612245,
        "MMLU_sociology":0.447761194,
        "MMLU_us_foreign_policy":0.38,
        "MMLU_virology":0.3373493976,
        "MMLU_world_religions":0.3450292398
    },
    {
        "Model":"CodeLlama-7b-hf",
        "URL":"https:\/\/huggingface.co\/codellama\/CodeLlama-7b-hf",
        "full_model_name":"codellama\/CodeLlama-7b-hf",
        "Parameters":7.0,
        "MMLU_average":0.3112206483,
        "arc:challenge|25":0.3660409556,
        "hellaswag|10":0.4536944832,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.3407407407,
        "MMLU_astronomy":0.1710526316,
        "MMLU_business_ethics":0.35,
        "MMLU_clinical_knowledge":0.320754717,
        "MMLU_college_biology":0.2708333333,
        "MMLU_college_chemistry":0.26,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.2369942197,
        "MMLU_college_physics":0.3823529412,
        "MMLU_computer_security":0.33,
        "MMLU_conceptual_physics":0.3446808511,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2965517241,
        "MMLU_elementary_mathematics":0.2592592593,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.21,
        "MMLU_high_school_biology":0.3741935484,
        "MMLU_high_school_chemistry":0.2906403941,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.3151515152,
        "MMLU_high_school_geography":0.3585858586,
        "MMLU_high_school_government_and_politics":0.3471502591,
        "MMLU_high_school_macroeconomics":0.2538461538,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.3361344538,
        "MMLU_high_school_physics":0.2251655629,
        "MMLU_high_school_psychology":0.3357798165,
        "MMLU_high_school_statistics":0.4351851852,
        "MMLU_high_school_us_history":0.3333333333,
        "MMLU_high_school_world_history":0.3502109705,
        "MMLU_human_aging":0.3766816143,
        "MMLU_human_sexuality":0.3511450382,
        "MMLU_international_law":0.3305785124,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.3128834356,
        "MMLU_machine_learning":0.1964285714,
        "MMLU_management":0.359223301,
        "MMLU_marketing":0.3290598291,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.4278416347,
        "MMLU_moral_disputes":0.2832369942,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.385620915,
        "MMLU_philosophy":0.3408360129,
        "MMLU_prehistory":0.3179012346,
        "MMLU_professional_accounting":0.2588652482,
        "MMLU_professional_law":0.2750977836,
        "MMLU_professional_medicine":0.2941176471,
        "MMLU_professional_psychology":0.2483660131,
        "MMLU_public_relations":0.3818181818,
        "MMLU_security_studies":0.3755102041,
        "MMLU_sociology":0.3333333333,
        "MMLU_us_foreign_policy":0.35,
        "MMLU_virology":0.343373494,
        "MMLU_world_religions":0.4269005848
    },
    {
        "Model":"Javalion-R",
        "URL":"https:\/\/huggingface.co\/digitous\/Javalion-R",
        "full_model_name":"digitous\/Javalion-R",
        "Parameters":null,
        "MMLU_average":0.3081119465,
        "arc:challenge|25":0.3788395904,
        "hellaswag|10":0.503286198,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.3481481481,
        "MMLU_astronomy":0.3223684211,
        "MMLU_business_ethics":0.26,
        "MMLU_clinical_knowledge":0.3396226415,
        "MMLU_college_biology":0.2847222222,
        "MMLU_college_chemistry":0.23,
        "MMLU_college_computer_science":0.22,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.3005780347,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.31,
        "MMLU_conceptual_physics":0.3617021277,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.3172413793,
        "MMLU_elementary_mathematics":0.2698412698,
        "MMLU_formal_logic":0.1904761905,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.2903225806,
        "MMLU_high_school_chemistry":0.3004926108,
        "MMLU_high_school_computer_science":0.24,
        "MMLU_high_school_european_history":0.303030303,
        "MMLU_high_school_geography":0.297979798,
        "MMLU_high_school_government_and_politics":0.3212435233,
        "MMLU_high_school_macroeconomics":0.3102564103,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.2773109244,
        "MMLU_high_school_physics":0.2450331126,
        "MMLU_high_school_psychology":0.280733945,
        "MMLU_high_school_statistics":0.1759259259,
        "MMLU_high_school_us_history":0.3235294118,
        "MMLU_high_school_world_history":0.3502109705,
        "MMLU_human_aging":0.399103139,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.3884297521,
        "MMLU_jurisprudence":0.3240740741,
        "MMLU_logical_fallacies":0.2515337423,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.3689320388,
        "MMLU_marketing":0.311965812,
        "MMLU_medical_genetics":0.32,
        "MMLU_miscellaneous":0.3512132822,
        "MMLU_moral_disputes":0.3092485549,
        "MMLU_moral_scenarios":0.2446927374,
        "MMLU_nutrition":0.3692810458,
        "MMLU_philosophy":0.3215434084,
        "MMLU_prehistory":0.3179012346,
        "MMLU_professional_accounting":0.2836879433,
        "MMLU_professional_law":0.3252933507,
        "MMLU_professional_medicine":0.3125,
        "MMLU_professional_psychology":0.2973856209,
        "MMLU_public_relations":0.3727272727,
        "MMLU_security_studies":0.4408163265,
        "MMLU_sociology":0.4029850746,
        "MMLU_us_foreign_policy":0.39,
        "MMLU_virology":0.3493975904,
        "MMLU_world_religions":0.3333333333
    },
    {
        "Model":"starchat-alpha",
        "URL":"https:\/\/huggingface.co\/HuggingFaceH4\/starchat-alpha",
        "full_model_name":"HuggingFaceH4\/starchat-alpha",
        "Parameters":null,
        "MMLU_average":0.3075770378,
        "arc:challenge|25":0.2994880546,
        "hellaswag|10":0.3884684326,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.3407407407,
        "MMLU_astronomy":0.3355263158,
        "MMLU_business_ethics":0.35,
        "MMLU_clinical_knowledge":0.3094339623,
        "MMLU_college_biology":0.2916666667,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.36,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.2543352601,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.48,
        "MMLU_conceptual_physics":0.2765957447,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.3862068966,
        "MMLU_elementary_mathematics":0.2724867725,
        "MMLU_formal_logic":0.2619047619,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2709677419,
        "MMLU_high_school_chemistry":0.2413793103,
        "MMLU_high_school_computer_science":0.46,
        "MMLU_high_school_european_history":0.2787878788,
        "MMLU_high_school_geography":0.2777777778,
        "MMLU_high_school_government_and_politics":0.2797927461,
        "MMLU_high_school_macroeconomics":0.2769230769,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.2478991597,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.2513761468,
        "MMLU_high_school_statistics":0.1666666667,
        "MMLU_high_school_us_history":0.318627451,
        "MMLU_high_school_world_history":0.2827004219,
        "MMLU_human_aging":0.4035874439,
        "MMLU_human_sexuality":0.2748091603,
        "MMLU_international_law":0.4297520661,
        "MMLU_jurisprudence":0.287037037,
        "MMLU_logical_fallacies":0.2944785276,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.3203883495,
        "MMLU_marketing":0.3974358974,
        "MMLU_medical_genetics":0.29,
        "MMLU_miscellaneous":0.3588761175,
        "MMLU_moral_disputes":0.3526011561,
        "MMLU_moral_scenarios":0.2536312849,
        "MMLU_nutrition":0.2941176471,
        "MMLU_philosophy":0.3794212219,
        "MMLU_prehistory":0.3580246914,
        "MMLU_professional_accounting":0.2695035461,
        "MMLU_professional_law":0.2737940026,
        "MMLU_professional_medicine":0.2022058824,
        "MMLU_professional_psychology":0.2810457516,
        "MMLU_public_relations":0.3363636364,
        "MMLU_security_studies":0.3918367347,
        "MMLU_sociology":0.3134328358,
        "MMLU_us_foreign_policy":0.43,
        "MMLU_virology":0.343373494,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"Javelin-R",
        "URL":"https:\/\/huggingface.co\/digitous\/Javelin-R",
        "full_model_name":"digitous\/Javelin-R",
        "Parameters":null,
        "MMLU_average":0.3069510608,
        "arc:challenge|25":0.3865187713,
        "hellaswag|10":0.5119498108,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.3481481481,
        "MMLU_astronomy":0.3223684211,
        "MMLU_business_ethics":0.27,
        "MMLU_clinical_knowledge":0.3358490566,
        "MMLU_college_biology":0.2708333333,
        "MMLU_college_chemistry":0.22,
        "MMLU_college_computer_science":0.24,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.3063583815,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.34,
        "MMLU_conceptual_physics":0.3574468085,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.2965517241,
        "MMLU_elementary_mathematics":0.2645502646,
        "MMLU_formal_logic":0.1666666667,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.2838709677,
        "MMLU_high_school_chemistry":0.2906403941,
        "MMLU_high_school_computer_science":0.22,
        "MMLU_high_school_european_history":0.3212121212,
        "MMLU_high_school_geography":0.303030303,
        "MMLU_high_school_government_and_politics":0.310880829,
        "MMLU_high_school_macroeconomics":0.3025641026,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.3109243697,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.295412844,
        "MMLU_high_school_statistics":0.162037037,
        "MMLU_high_school_us_history":0.3382352941,
        "MMLU_high_school_world_history":0.3670886076,
        "MMLU_human_aging":0.3946188341,
        "MMLU_human_sexuality":0.2824427481,
        "MMLU_international_law":0.3884297521,
        "MMLU_jurisprudence":0.3518518519,
        "MMLU_logical_fallacies":0.2515337423,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.359223301,
        "MMLU_marketing":0.2991452991,
        "MMLU_medical_genetics":0.31,
        "MMLU_miscellaneous":0.3601532567,
        "MMLU_moral_disputes":0.3034682081,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.3692810458,
        "MMLU_philosophy":0.308681672,
        "MMLU_prehistory":0.3117283951,
        "MMLU_professional_accounting":0.2872340426,
        "MMLU_professional_law":0.3252933507,
        "MMLU_professional_medicine":0.3014705882,
        "MMLU_professional_psychology":0.2875816993,
        "MMLU_public_relations":0.3636363636,
        "MMLU_security_studies":0.4285714286,
        "MMLU_sociology":0.4029850746,
        "MMLU_us_foreign_policy":0.39,
        "MMLU_virology":0.3554216867,
        "MMLU_world_religions":0.3333333333
    },
    {
        "Model":"open_llama_7b",
        "URL":"https:\/\/huggingface.co\/openlm-research\/open_llama_7b",
        "full_model_name":"openlm-research\/open_llama_7b",
        "Parameters":7.0,
        "MMLU_average":0.3063544149,
        "arc:challenge|25":0.4283276451,
        "hellaswag|10":0.5157339175,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.3333333333,
        "MMLU_astronomy":0.25,
        "MMLU_business_ethics":0.35,
        "MMLU_clinical_knowledge":0.3924528302,
        "MMLU_college_biology":0.3194444444,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.3352601156,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.39,
        "MMLU_conceptual_physics":0.314893617,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.2965517241,
        "MMLU_elementary_mathematics":0.2724867725,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.3096774194,
        "MMLU_high_school_chemistry":0.2463054187,
        "MMLU_high_school_computer_science":0.28,
        "MMLU_high_school_european_history":0.2909090909,
        "MMLU_high_school_geography":0.3434343434,
        "MMLU_high_school_government_and_politics":0.3626943005,
        "MMLU_high_school_macroeconomics":0.3564102564,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.2941176471,
        "MMLU_high_school_physics":0.2450331126,
        "MMLU_high_school_psychology":0.3486238532,
        "MMLU_high_school_statistics":0.3657407407,
        "MMLU_high_school_us_history":0.3382352941,
        "MMLU_high_school_world_history":0.3333333333,
        "MMLU_human_aging":0.2869955157,
        "MMLU_human_sexuality":0.2824427481,
        "MMLU_international_law":0.3388429752,
        "MMLU_jurisprudence":0.3333333333,
        "MMLU_logical_fallacies":0.2944785276,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.2524271845,
        "MMLU_marketing":0.3632478632,
        "MMLU_medical_genetics":0.27,
        "MMLU_miscellaneous":0.3716475096,
        "MMLU_moral_disputes":0.3294797688,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.3169934641,
        "MMLU_philosophy":0.2861736334,
        "MMLU_prehistory":0.3209876543,
        "MMLU_professional_accounting":0.2659574468,
        "MMLU_professional_law":0.2620599739,
        "MMLU_professional_medicine":0.2316176471,
        "MMLU_professional_psychology":0.272875817,
        "MMLU_public_relations":0.4090909091,
        "MMLU_security_studies":0.2408163265,
        "MMLU_sociology":0.263681592,
        "MMLU_us_foreign_policy":0.4,
        "MMLU_virology":0.3614457831,
        "MMLU_world_religions":0.3918128655
    },
    {
        "Model":"CodeLlama-7b-hf",
        "URL":"https:\/\/huggingface.co\/NousResearch\/CodeLlama-7b-hf",
        "full_model_name":"NousResearch\/CodeLlama-7b-hf",
        "Parameters":7.0,
        "MMLU_average":0.304733975,
        "arc:challenge|25":0.3728668942,
        "hellaswag|10":0.440250946,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.3851851852,
        "MMLU_astronomy":0.1973684211,
        "MMLU_business_ethics":0.25,
        "MMLU_clinical_knowledge":0.320754717,
        "MMLU_college_biology":0.2361111111,
        "MMLU_college_chemistry":0.28,
        "MMLU_college_computer_science":0.35,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.2427745665,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.36,
        "MMLU_conceptual_physics":0.3531914894,
        "MMLU_econometrics":0.2192982456,
        "MMLU_electrical_engineering":0.2551724138,
        "MMLU_elementary_mathematics":0.2592592593,
        "MMLU_formal_logic":0.2063492063,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.3258064516,
        "MMLU_high_school_chemistry":0.3054187192,
        "MMLU_high_school_computer_science":0.27,
        "MMLU_high_school_european_history":0.303030303,
        "MMLU_high_school_geography":0.3333333333,
        "MMLU_high_school_government_and_politics":0.3316062176,
        "MMLU_high_school_macroeconomics":0.3076923077,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.3151260504,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.295412844,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2794117647,
        "MMLU_high_school_world_history":0.3417721519,
        "MMLU_human_aging":0.4125560538,
        "MMLU_human_sexuality":0.2900763359,
        "MMLU_international_law":0.3223140496,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.3006134969,
        "MMLU_machine_learning":0.1785714286,
        "MMLU_management":0.3689320388,
        "MMLU_marketing":0.3205128205,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.398467433,
        "MMLU_moral_disputes":0.289017341,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.3594771242,
        "MMLU_philosophy":0.3504823151,
        "MMLU_prehistory":0.2962962963,
        "MMLU_professional_accounting":0.2375886525,
        "MMLU_professional_law":0.2588005215,
        "MMLU_professional_medicine":0.3823529412,
        "MMLU_professional_psychology":0.2598039216,
        "MMLU_public_relations":0.2909090909,
        "MMLU_security_studies":0.4081632653,
        "MMLU_sociology":0.2985074627,
        "MMLU_us_foreign_policy":0.38,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.4561403509
    },
    {
        "Model":"starcoder-finetune-openapi",
        "URL":"https:\/\/huggingface.co\/sartmis1\/starcoder-finetune-openapi",
        "full_model_name":"sartmis1\/starcoder-finetune-openapi",
        "Parameters":null,
        "MMLU_average":0.3039658498,
        "arc:challenge|25":0.2832764505,
        "hellaswag|10":0.379705238,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.3259259259,
        "MMLU_astronomy":0.3092105263,
        "MMLU_business_ethics":0.33,
        "MMLU_clinical_knowledge":0.2641509434,
        "MMLU_college_biology":0.2986111111,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.28,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.2485549133,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.49,
        "MMLU_conceptual_physics":0.3021276596,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.4620689655,
        "MMLU_elementary_mathematics":0.2380952381,
        "MMLU_formal_logic":0.3015873016,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.2709677419,
        "MMLU_high_school_chemistry":0.2118226601,
        "MMLU_high_school_computer_science":0.42,
        "MMLU_high_school_european_history":0.3454545455,
        "MMLU_high_school_geography":0.1818181818,
        "MMLU_high_school_government_and_politics":0.2538860104,
        "MMLU_high_school_macroeconomics":0.2512820513,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.2521008403,
        "MMLU_high_school_physics":0.2119205298,
        "MMLU_high_school_psychology":0.2256880734,
        "MMLU_high_school_statistics":0.1851851852,
        "MMLU_high_school_us_history":0.2696078431,
        "MMLU_high_school_world_history":0.3122362869,
        "MMLU_human_aging":0.3497757848,
        "MMLU_human_sexuality":0.320610687,
        "MMLU_international_law":0.4297520661,
        "MMLU_jurisprudence":0.287037037,
        "MMLU_logical_fallacies":0.245398773,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.2330097087,
        "MMLU_marketing":0.4273504274,
        "MMLU_medical_genetics":0.35,
        "MMLU_miscellaneous":0.3218390805,
        "MMLU_moral_disputes":0.3641618497,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.3071895425,
        "MMLU_philosophy":0.347266881,
        "MMLU_prehistory":0.3456790123,
        "MMLU_professional_accounting":0.2978723404,
        "MMLU_professional_law":0.2829204694,
        "MMLU_professional_medicine":0.2058823529,
        "MMLU_professional_psychology":0.3039215686,
        "MMLU_public_relations":0.3454545455,
        "MMLU_security_studies":0.2775510204,
        "MMLU_sociology":0.3781094527,
        "MMLU_us_foreign_policy":0.43,
        "MMLU_virology":0.3674698795,
        "MMLU_world_religions":0.3099415205
    },
    {
        "Model":"GPT-J-Pyg_PPO-6B-Dev-V8p4",
        "URL":"https:\/\/huggingface.co\/TehVenom\/GPT-J-Pyg_PPO-6B-Dev-V8p4",
        "full_model_name":"TehVenom\/GPT-J-Pyg_PPO-6B-Dev-V8p4",
        "Parameters":6.0,
        "MMLU_average":0.3039368076,
        "arc:challenge|25":0.3677474403,
        "hellaswag|10":0.4938259311,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.2740740741,
        "MMLU_astronomy":0.3881578947,
        "MMLU_business_ethics":0.26,
        "MMLU_clinical_knowledge":0.3169811321,
        "MMLU_college_biology":0.2361111111,
        "MMLU_college_chemistry":0.29,
        "MMLU_college_computer_science":0.23,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.3063583815,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.35,
        "MMLU_conceptual_physics":0.3106382979,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.3586206897,
        "MMLU_elementary_mathematics":0.2380952381,
        "MMLU_formal_logic":0.2380952381,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.3387096774,
        "MMLU_high_school_chemistry":0.2709359606,
        "MMLU_high_school_computer_science":0.26,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.3131313131,
        "MMLU_high_school_government_and_politics":0.3471502591,
        "MMLU_high_school_macroeconomics":0.3641025641,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.2983193277,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.2678899083,
        "MMLU_high_school_statistics":0.2777777778,
        "MMLU_high_school_us_history":0.2696078431,
        "MMLU_high_school_world_history":0.2869198312,
        "MMLU_human_aging":0.2914798206,
        "MMLU_human_sexuality":0.2442748092,
        "MMLU_international_law":0.4710743802,
        "MMLU_jurisprudence":0.2962962963,
        "MMLU_logical_fallacies":0.2944785276,
        "MMLU_machine_learning":0.2232142857,
        "MMLU_management":0.3009708738,
        "MMLU_marketing":0.2948717949,
        "MMLU_medical_genetics":0.34,
        "MMLU_miscellaneous":0.2681992337,
        "MMLU_moral_disputes":0.3410404624,
        "MMLU_moral_scenarios":0.2726256983,
        "MMLU_nutrition":0.3267973856,
        "MMLU_philosophy":0.3118971061,
        "MMLU_prehistory":0.3209876543,
        "MMLU_professional_accounting":0.2836879433,
        "MMLU_professional_law":0.29726206,
        "MMLU_professional_medicine":0.3419117647,
        "MMLU_professional_psychology":0.2908496732,
        "MMLU_public_relations":0.3545454545,
        "MMLU_security_studies":0.3959183673,
        "MMLU_sociology":0.3880597015,
        "MMLU_us_foreign_policy":0.34,
        "MMLU_virology":0.2891566265,
        "MMLU_world_religions":0.3099415205
    },
    {
        "Model":"Guanaco",
        "URL":"https:\/\/huggingface.co\/JosephusCheung\/Guanaco",
        "full_model_name":"JosephusCheung\/Guanaco",
        "Parameters":null,
        "MMLU_average":0.3030341226,
        "arc:challenge|25":0.4607508532,
        "hellaswag|10":0.5484963155,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.3037037037,
        "MMLU_astronomy":0.2434210526,
        "MMLU_business_ethics":0.37,
        "MMLU_clinical_knowledge":0.2716981132,
        "MMLU_college_biology":0.2916666667,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.225433526,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.45,
        "MMLU_conceptual_physics":0.2808510638,
        "MMLU_econometrics":0.2105263158,
        "MMLU_electrical_engineering":0.2689655172,
        "MMLU_elementary_mathematics":0.2301587302,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.2677419355,
        "MMLU_high_school_chemistry":0.2167487685,
        "MMLU_high_school_computer_science":0.29,
        "MMLU_high_school_european_history":0.3333333333,
        "MMLU_high_school_geography":0.3080808081,
        "MMLU_high_school_government_and_politics":0.310880829,
        "MMLU_high_school_macroeconomics":0.2794871795,
        "MMLU_high_school_mathematics":0.2259259259,
        "MMLU_high_school_microeconomics":0.2983193277,
        "MMLU_high_school_physics":0.2450331126,
        "MMLU_high_school_psychology":0.3486238532,
        "MMLU_high_school_statistics":0.2083333333,
        "MMLU_high_school_us_history":0.3039215686,
        "MMLU_high_school_world_history":0.3628691983,
        "MMLU_human_aging":0.4215246637,
        "MMLU_human_sexuality":0.3511450382,
        "MMLU_international_law":0.4049586777,
        "MMLU_jurisprudence":0.3425925926,
        "MMLU_logical_fallacies":0.2883435583,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.3106796117,
        "MMLU_marketing":0.4957264957,
        "MMLU_medical_genetics":0.32,
        "MMLU_miscellaneous":0.4648786718,
        "MMLU_moral_disputes":0.2976878613,
        "MMLU_moral_scenarios":0.2502793296,
        "MMLU_nutrition":0.2941176471,
        "MMLU_philosophy":0.270096463,
        "MMLU_prehistory":0.3179012346,
        "MMLU_professional_accounting":0.2872340426,
        "MMLU_professional_law":0.2724902216,
        "MMLU_professional_medicine":0.2279411765,
        "MMLU_professional_psychology":0.3284313725,
        "MMLU_public_relations":0.3727272727,
        "MMLU_security_studies":0.2040816327,
        "MMLU_sociology":0.3184079602,
        "MMLU_us_foreign_policy":0.41,
        "MMLU_virology":0.2951807229,
        "MMLU_world_religions":0.4561403509
    },
    {
        "Model":"Llama-2-ko-7b-Chat",
        "URL":"https:\/\/huggingface.co\/kfkas\/Llama-2-ko-7b-Chat",
        "full_model_name":"kfkas\/Llama-2-ko-7b-Chat",
        "Parameters":7.0,
        "MMLU_average":0.3019136542,
        "arc:challenge|25":0.3711604096,
        "hellaswag|10":0.4907388966,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.3703703704,
        "MMLU_astronomy":0.2763157895,
        "MMLU_business_ethics":0.28,
        "MMLU_clinical_knowledge":0.3245283019,
        "MMLU_college_biology":0.2361111111,
        "MMLU_college_chemistry":0.23,
        "MMLU_college_computer_science":0.25,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.2485549133,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.33,
        "MMLU_conceptual_physics":0.3361702128,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2619047619,
        "MMLU_formal_logic":0.1825396825,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.3096774194,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.24,
        "MMLU_high_school_european_history":0.2606060606,
        "MMLU_high_school_geography":0.3585858586,
        "MMLU_high_school_government_and_politics":0.2694300518,
        "MMLU_high_school_macroeconomics":0.2564102564,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.3193277311,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.3302752294,
        "MMLU_high_school_statistics":0.1898148148,
        "MMLU_high_school_us_history":0.2205882353,
        "MMLU_high_school_world_history":0.2995780591,
        "MMLU_human_aging":0.3677130045,
        "MMLU_human_sexuality":0.3129770992,
        "MMLU_international_law":0.3223140496,
        "MMLU_jurisprudence":0.3796296296,
        "MMLU_logical_fallacies":0.2699386503,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.3883495146,
        "MMLU_marketing":0.4273504274,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.3959131545,
        "MMLU_moral_disputes":0.3063583815,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.3692810458,
        "MMLU_philosophy":0.3408360129,
        "MMLU_prehistory":0.3117283951,
        "MMLU_professional_accounting":0.2730496454,
        "MMLU_professional_law":0.2477183833,
        "MMLU_professional_medicine":0.4338235294,
        "MMLU_professional_psychology":0.2924836601,
        "MMLU_public_relations":0.3818181818,
        "MMLU_security_studies":0.2979591837,
        "MMLU_sociology":0.2686567164,
        "MMLU_us_foreign_policy":0.41,
        "MMLU_virology":0.3493975904,
        "MMLU_world_religions":0.4035087719
    },
    {
        "Model":"nucleus-22B-token-500B",
        "URL":"https:\/\/huggingface.co\/NucleusAI\/nucleus-22B-token-500B",
        "full_model_name":"NucleusAI\/nucleus-22B-token-500B",
        "Parameters":22.0,
        "MMLU_average":0.3010716999,
        "arc:challenge|25":0.3575085324,
        "hellaswag|10":0.5060744872,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.2814814815,
        "MMLU_astronomy":0.3092105263,
        "MMLU_business_ethics":0.34,
        "MMLU_clinical_knowledge":0.3169811321,
        "MMLU_college_biology":0.2777777778,
        "MMLU_college_chemistry":0.36,
        "MMLU_college_computer_science":0.3,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.2947976879,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.31,
        "MMLU_conceptual_physics":0.3234042553,
        "MMLU_econometrics":0.2105263158,
        "MMLU_electrical_engineering":0.3103448276,
        "MMLU_elementary_mathematics":0.2433862434,
        "MMLU_formal_logic":0.3015873016,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.2870967742,
        "MMLU_high_school_chemistry":0.2019704433,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.3333333333,
        "MMLU_high_school_government_and_politics":0.3471502591,
        "MMLU_high_school_macroeconomics":0.3897435897,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.2941176471,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.3174311927,
        "MMLU_high_school_statistics":0.3888888889,
        "MMLU_high_school_us_history":0.2745098039,
        "MMLU_high_school_world_history":0.3080168776,
        "MMLU_human_aging":0.33632287,
        "MMLU_human_sexuality":0.3511450382,
        "MMLU_international_law":0.2809917355,
        "MMLU_jurisprudence":0.2777777778,
        "MMLU_logical_fallacies":0.2392638037,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.2427184466,
        "MMLU_marketing":0.4102564103,
        "MMLU_medical_genetics":0.29,
        "MMLU_miscellaneous":0.2567049808,
        "MMLU_moral_disputes":0.2947976879,
        "MMLU_moral_scenarios":0.2335195531,
        "MMLU_nutrition":0.3823529412,
        "MMLU_philosophy":0.2765273312,
        "MMLU_prehistory":0.2685185185,
        "MMLU_professional_accounting":0.2836879433,
        "MMLU_professional_law":0.2614080834,
        "MMLU_professional_medicine":0.3566176471,
        "MMLU_professional_psychology":0.2679738562,
        "MMLU_public_relations":0.2909090909,
        "MMLU_security_studies":0.3428571429,
        "MMLU_sociology":0.3233830846,
        "MMLU_us_foreign_policy":0.36,
        "MMLU_virology":0.2891566265,
        "MMLU_world_religions":0.3274853801
    },
    {
        "Model":"gpt-sw3-6.7b-v2",
        "URL":"https:\/\/huggingface.co\/AI-Sweden-Models\/gpt-sw3-6.7b-v2",
        "full_model_name":"AI-Sweden-Models\/gpt-sw3-6.7b-v2",
        "Parameters":6.7,
        "MMLU_average":0.3008596366,
        "arc:challenge|25":0.3540955631,
        "hellaswag|10":0.4925313683,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.3407407407,
        "MMLU_astronomy":0.2302631579,
        "MMLU_business_ethics":0.37,
        "MMLU_clinical_knowledge":0.3962264151,
        "MMLU_college_biology":0.3194444444,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.3,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.3005780347,
        "MMLU_college_physics":0.1470588235,
        "MMLU_computer_security":0.32,
        "MMLU_conceptual_physics":0.3404255319,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.3172413793,
        "MMLU_elementary_mathematics":0.2645502646,
        "MMLU_formal_logic":0.2777777778,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.3064516129,
        "MMLU_high_school_chemistry":0.2561576355,
        "MMLU_high_school_computer_science":0.28,
        "MMLU_high_school_european_history":0.3454545455,
        "MMLU_high_school_geography":0.3636363636,
        "MMLU_high_school_government_and_politics":0.2901554404,
        "MMLU_high_school_macroeconomics":0.2666666667,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.231092437,
        "MMLU_high_school_physics":0.2317880795,
        "MMLU_high_school_psychology":0.3688073394,
        "MMLU_high_school_statistics":0.1898148148,
        "MMLU_high_school_us_history":0.2794117647,
        "MMLU_high_school_world_history":0.3459915612,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.3435114504,
        "MMLU_international_law":0.3719008264,
        "MMLU_jurisprudence":0.3055555556,
        "MMLU_logical_fallacies":0.2699386503,
        "MMLU_machine_learning":0.2410714286,
        "MMLU_management":0.2815533981,
        "MMLU_marketing":0.3205128205,
        "MMLU_medical_genetics":0.29,
        "MMLU_miscellaneous":0.337164751,
        "MMLU_moral_disputes":0.3121387283,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.3431372549,
        "MMLU_philosophy":0.3601286174,
        "MMLU_prehistory":0.3148148148,
        "MMLU_professional_accounting":0.2907801418,
        "MMLU_professional_law":0.2750977836,
        "MMLU_professional_medicine":0.2867647059,
        "MMLU_professional_psychology":0.295751634,
        "MMLU_public_relations":0.3454545455,
        "MMLU_security_studies":0.2653061224,
        "MMLU_sociology":0.3582089552,
        "MMLU_us_foreign_policy":0.38,
        "MMLU_virology":0.3313253012,
        "MMLU_world_religions":0.4152046784
    },
    {
        "Model":"starcoder",
        "URL":"https:\/\/huggingface.co\/bigcode\/starcoder",
        "full_model_name":"bigcode\/starcoder",
        "Parameters":null,
        "MMLU_average":0.2999572822,
        "arc:challenge|25":0.2849829352,
        "hellaswag|10":0.3783110934,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.3111111111,
        "MMLU_astronomy":0.2960526316,
        "MMLU_business_ethics":0.32,
        "MMLU_clinical_knowledge":0.2603773585,
        "MMLU_college_biology":0.3125,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.3,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.2485549133,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.48,
        "MMLU_conceptual_physics":0.2978723404,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.4275862069,
        "MMLU_elementary_mathematics":0.2486772487,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.264516129,
        "MMLU_high_school_chemistry":0.2019704433,
        "MMLU_high_school_computer_science":0.41,
        "MMLU_high_school_european_history":0.3393939394,
        "MMLU_high_school_geography":0.202020202,
        "MMLU_high_school_government_and_politics":0.2538860104,
        "MMLU_high_school_macroeconomics":0.2384615385,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.2731092437,
        "MMLU_high_school_physics":0.2185430464,
        "MMLU_high_school_psychology":0.223853211,
        "MMLU_high_school_statistics":0.1805555556,
        "MMLU_high_school_us_history":0.2794117647,
        "MMLU_high_school_world_history":0.3164556962,
        "MMLU_human_aging":0.3587443946,
        "MMLU_human_sexuality":0.2977099237,
        "MMLU_international_law":0.4214876033,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2392638037,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.2621359223,
        "MMLU_marketing":0.4145299145,
        "MMLU_medical_genetics":0.34,
        "MMLU_miscellaneous":0.3218390805,
        "MMLU_moral_disputes":0.3583815029,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.3137254902,
        "MMLU_philosophy":0.3601286174,
        "MMLU_prehistory":0.3364197531,
        "MMLU_professional_accounting":0.2943262411,
        "MMLU_professional_law":0.2829204694,
        "MMLU_professional_medicine":0.2022058824,
        "MMLU_professional_psychology":0.2990196078,
        "MMLU_public_relations":0.3454545455,
        "MMLU_security_studies":0.2775510204,
        "MMLU_sociology":0.368159204,
        "MMLU_us_foreign_policy":0.42,
        "MMLU_virology":0.313253012,
        "MMLU_world_religions":0.3157894737
    },
    {
        "Model":"GPT-NeoXT-Chat-Base-20B",
        "URL":"https:\/\/huggingface.co\/togethercomputer\/GPT-NeoXT-Chat-Base-20B",
        "full_model_name":"togethercomputer\/GPT-NeoXT-Chat-Base-20B",
        "Parameters":20.0,
        "MMLU_average":0.2991910118,
        "arc:challenge|25":0.4240614334,
        "hellaswag|10":0.5509858594,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.3851851852,
        "MMLU_astronomy":0.3618421053,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.3094339623,
        "MMLU_college_biology":0.3194444444,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.2601156069,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.37,
        "MMLU_conceptual_physics":0.2382978723,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.3793103448,
        "MMLU_elementary_mathematics":0.2698412698,
        "MMLU_formal_logic":0.2698412698,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.3258064516,
        "MMLU_high_school_chemistry":0.2709359606,
        "MMLU_high_school_computer_science":0.26,
        "MMLU_high_school_european_history":0.2787878788,
        "MMLU_high_school_geography":0.3787878788,
        "MMLU_high_school_government_and_politics":0.3575129534,
        "MMLU_high_school_macroeconomics":0.2974358974,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.2478991597,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.2513761468,
        "MMLU_high_school_statistics":0.2453703704,
        "MMLU_high_school_us_history":0.3480392157,
        "MMLU_high_school_world_history":0.2742616034,
        "MMLU_human_aging":0.2421524664,
        "MMLU_human_sexuality":0.3969465649,
        "MMLU_international_law":0.4214876033,
        "MMLU_jurisprudence":0.2037037037,
        "MMLU_logical_fallacies":0.3312883436,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.3106796117,
        "MMLU_marketing":0.2735042735,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.3231162197,
        "MMLU_moral_disputes":0.2861271676,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.3235294118,
        "MMLU_philosophy":0.3247588424,
        "MMLU_prehistory":0.3302469136,
        "MMLU_professional_accounting":0.2659574468,
        "MMLU_professional_law":0.2848761408,
        "MMLU_professional_medicine":0.1764705882,
        "MMLU_professional_psychology":0.2679738562,
        "MMLU_public_relations":0.2363636364,
        "MMLU_security_studies":0.3591836735,
        "MMLU_sociology":0.2587064677,
        "MMLU_us_foreign_policy":0.33,
        "MMLU_virology":0.3253012048,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"LaMini-GPT-1.5B",
        "URL":"https:\/\/huggingface.co\/MBZUAI\/LaMini-GPT-1.5B",
        "full_model_name":"MBZUAI\/LaMini-GPT-1.5B",
        "Parameters":1.5,
        "MMLU_average":0.2991850075,
        "arc:challenge|25":0.2866894198,
        "hellaswag|10":0.3923521211,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.3037037037,
        "MMLU_astronomy":0.2828947368,
        "MMLU_business_ethics":0.25,
        "MMLU_clinical_knowledge":0.3396226415,
        "MMLU_college_biology":0.2638888889,
        "MMLU_college_chemistry":0.23,
        "MMLU_college_computer_science":0.24,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.2601156069,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.3,
        "MMLU_conceptual_physics":0.3063829787,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.324137931,
        "MMLU_elementary_mathematics":0.253968254,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.26,
        "MMLU_high_school_biology":0.335483871,
        "MMLU_high_school_chemistry":0.2315270936,
        "MMLU_high_school_computer_science":0.35,
        "MMLU_high_school_european_history":0.3636363636,
        "MMLU_high_school_geography":0.398989899,
        "MMLU_high_school_government_and_politics":0.378238342,
        "MMLU_high_school_macroeconomics":0.3564102564,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.2731092437,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.3449541284,
        "MMLU_high_school_statistics":0.4074074074,
        "MMLU_high_school_us_history":0.2843137255,
        "MMLU_high_school_world_history":0.2911392405,
        "MMLU_human_aging":0.2959641256,
        "MMLU_human_sexuality":0.3282442748,
        "MMLU_international_law":0.2975206612,
        "MMLU_jurisprudence":0.287037037,
        "MMLU_logical_fallacies":0.3987730061,
        "MMLU_machine_learning":0.2946428571,
        "MMLU_management":0.3300970874,
        "MMLU_marketing":0.3205128205,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.3346104725,
        "MMLU_moral_disputes":0.274566474,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2581699346,
        "MMLU_philosophy":0.2861736334,
        "MMLU_prehistory":0.299382716,
        "MMLU_professional_accounting":0.2482269504,
        "MMLU_professional_law":0.2503259452,
        "MMLU_professional_medicine":0.4411764706,
        "MMLU_professional_psychology":0.2696078431,
        "MMLU_public_relations":0.3454545455,
        "MMLU_security_studies":0.1795918367,
        "MMLU_sociology":0.2835820896,
        "MMLU_us_foreign_policy":0.38,
        "MMLU_virology":0.3493975904,
        "MMLU_world_religions":0.2748538012
    },
    {
        "Model":"tigerbot-7b-sft",
        "URL":"https:\/\/huggingface.co\/TigerResearch\/tigerbot-7b-sft",
        "full_model_name":"TigerResearch\/tigerbot-7b-sft",
        "Parameters":7.0,
        "MMLU_average":0.2989289422,
        "arc:challenge|25":0.3754266212,
        "hellaswag|10":0.4526986656,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.3481481481,
        "MMLU_astronomy":0.2105263158,
        "MMLU_business_ethics":0.38,
        "MMLU_clinical_knowledge":0.2867924528,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.12,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.3005780347,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.46,
        "MMLU_conceptual_physics":0.3276595745,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.3931034483,
        "MMLU_elementary_mathematics":0.2883597884,
        "MMLU_formal_logic":0.2222222222,
        "MMLU_global_facts":0.25,
        "MMLU_high_school_biology":0.2161290323,
        "MMLU_high_school_chemistry":0.2610837438,
        "MMLU_high_school_computer_science":0.39,
        "MMLU_high_school_european_history":0.3454545455,
        "MMLU_high_school_geography":0.3232323232,
        "MMLU_high_school_government_and_politics":0.2694300518,
        "MMLU_high_school_macroeconomics":0.258974359,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.2394957983,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.328440367,
        "MMLU_high_school_statistics":0.1944444444,
        "MMLU_high_school_us_history":0.2696078431,
        "MMLU_high_school_world_history":0.3713080169,
        "MMLU_human_aging":0.3452914798,
        "MMLU_human_sexuality":0.3435114504,
        "MMLU_international_law":0.2314049587,
        "MMLU_jurisprudence":0.3240740741,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.3009708738,
        "MMLU_marketing":0.4145299145,
        "MMLU_medical_genetics":0.36,
        "MMLU_miscellaneous":0.3793103448,
        "MMLU_moral_disputes":0.3265895954,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2679738562,
        "MMLU_philosophy":0.2668810289,
        "MMLU_prehistory":0.2716049383,
        "MMLU_professional_accounting":0.2978723404,
        "MMLU_professional_law":0.277053455,
        "MMLU_professional_medicine":0.2463235294,
        "MMLU_professional_psychology":0.3055555556,
        "MMLU_public_relations":0.4181818182,
        "MMLU_security_studies":0.2408163265,
        "MMLU_sociology":0.2935323383,
        "MMLU_us_foreign_policy":0.34,
        "MMLU_virology":0.3674698795,
        "MMLU_world_religions":0.3918128655
    },
    {
        "Model":"llama2-ko-7b-test",
        "URL":"https:\/\/huggingface.co\/Taekyoon\/llama2-ko-7b-test",
        "full_model_name":"Taekyoon\/llama2-ko-7b-test",
        "Parameters":7.0,
        "MMLU_average":0.295507108,
        "arc:challenge|25":0.3506825939,
        "hellaswag|10":0.469727146,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.2518518519,
        "MMLU_astronomy":0.3223684211,
        "MMLU_business_ethics":0.31,
        "MMLU_clinical_knowledge":0.3320754717,
        "MMLU_college_biology":0.2430555556,
        "MMLU_college_chemistry":0.33,
        "MMLU_college_computer_science":0.29,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.2774566474,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.34,
        "MMLU_conceptual_physics":0.3234042553,
        "MMLU_econometrics":0.3333333333,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2592592593,
        "MMLU_formal_logic":0.1507936508,
        "MMLU_global_facts":0.25,
        "MMLU_high_school_biology":0.2322580645,
        "MMLU_high_school_chemistry":0.2610837438,
        "MMLU_high_school_computer_science":0.27,
        "MMLU_high_school_european_history":0.2787878788,
        "MMLU_high_school_geography":0.3737373737,
        "MMLU_high_school_government_and_politics":0.3471502591,
        "MMLU_high_school_macroeconomics":0.3205128205,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.2941176471,
        "MMLU_high_school_physics":0.3907284768,
        "MMLU_high_school_psychology":0.3357798165,
        "MMLU_high_school_statistics":0.4675925926,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3408071749,
        "MMLU_human_sexuality":0.3053435115,
        "MMLU_international_law":0.3305785124,
        "MMLU_jurisprudence":0.2777777778,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.2053571429,
        "MMLU_management":0.3689320388,
        "MMLU_marketing":0.3205128205,
        "MMLU_medical_genetics":0.27,
        "MMLU_miscellaneous":0.2924648787,
        "MMLU_moral_disputes":0.2832369942,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.2843137255,
        "MMLU_philosophy":0.3408360129,
        "MMLU_prehistory":0.3024691358,
        "MMLU_professional_accounting":0.2482269504,
        "MMLU_professional_law":0.2881355932,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.3545454545,
        "MMLU_security_studies":0.4040816327,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.24,
        "MMLU_virology":0.3012048193,
        "MMLU_world_religions":0.3157894737
    },
    {
        "Model":"test-9k-fn",
        "URL":"https:\/\/huggingface.co\/Devio\/test-9k-fn",
        "full_model_name":"Devio\/test-9k-fn",
        "Parameters":null,
        "MMLU_average":0.2947207368,
        "arc:challenge|25":0.3566552901,
        "hellaswag|10":0.5057757419,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.2888888889,
        "MMLU_astronomy":0.3092105263,
        "MMLU_business_ethics":0.34,
        "MMLU_clinical_knowledge":0.3018867925,
        "MMLU_college_biology":0.2986111111,
        "MMLU_college_chemistry":0.32,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.2774566474,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.31,
        "MMLU_conceptual_physics":0.3234042553,
        "MMLU_econometrics":0.201754386,
        "MMLU_electrical_engineering":0.3103448276,
        "MMLU_elementary_mathematics":0.2248677249,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.2709677419,
        "MMLU_high_school_chemistry":0.1871921182,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.2121212121,
        "MMLU_high_school_geography":0.3181818182,
        "MMLU_high_school_government_and_politics":0.3575129534,
        "MMLU_high_school_macroeconomics":0.3717948718,
        "MMLU_high_school_mathematics":0.2888888889,
        "MMLU_high_school_microeconomics":0.281512605,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.3211009174,
        "MMLU_high_school_statistics":0.3611111111,
        "MMLU_high_school_us_history":0.2647058824,
        "MMLU_high_school_world_history":0.3333333333,
        "MMLU_human_aging":0.3408071749,
        "MMLU_human_sexuality":0.3282442748,
        "MMLU_international_law":0.2644628099,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2515337423,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.2330097087,
        "MMLU_marketing":0.3974358974,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2554278416,
        "MMLU_moral_disputes":0.2774566474,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.3921568627,
        "MMLU_philosophy":0.2604501608,
        "MMLU_prehistory":0.2592592593,
        "MMLU_professional_accounting":0.280141844,
        "MMLU_professional_law":0.2666232073,
        "MMLU_professional_medicine":0.3198529412,
        "MMLU_professional_psychology":0.2549019608,
        "MMLU_public_relations":0.2818181818,
        "MMLU_security_studies":0.3306122449,
        "MMLU_sociology":0.3184079602,
        "MMLU_us_foreign_policy":0.35,
        "MMLU_virology":0.3012048193,
        "MMLU_world_religions":0.3274853801
    },
    {
        "Model":"WizardCoder-15B-V1.0",
        "URL":"https:\/\/huggingface.co\/WizardLM\/WizardCoder-15B-V1.0",
        "full_model_name":"WizardLM\/WizardCoder-15B-V1.0",
        "Parameters":15.0,
        "MMLU_average":0.2942653368,
        "arc:challenge|25":0.292662116,
        "hellaswag|10":0.3860784704,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.2740740741,
        "MMLU_astronomy":0.2697368421,
        "MMLU_business_ethics":0.38,
        "MMLU_clinical_knowledge":0.241509434,
        "MMLU_college_biology":0.3055555556,
        "MMLU_college_chemistry":0.22,
        "MMLU_college_computer_science":0.35,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.2485549133,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.49,
        "MMLU_conceptual_physics":0.314893617,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.3862068966,
        "MMLU_elementary_mathematics":0.2830687831,
        "MMLU_formal_logic":0.2698412698,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2580645161,
        "MMLU_high_school_chemistry":0.2315270936,
        "MMLU_high_school_computer_science":0.38,
        "MMLU_high_school_european_history":0.3393939394,
        "MMLU_high_school_geography":0.202020202,
        "MMLU_high_school_government_and_politics":0.2435233161,
        "MMLU_high_school_macroeconomics":0.2358974359,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.2478991597,
        "MMLU_high_school_physics":0.2251655629,
        "MMLU_high_school_psychology":0.2146788991,
        "MMLU_high_school_statistics":0.1805555556,
        "MMLU_high_school_us_history":0.2941176471,
        "MMLU_high_school_world_history":0.2953586498,
        "MMLU_human_aging":0.3497757848,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.3305785124,
        "MMLU_jurisprudence":0.2962962963,
        "MMLU_logical_fallacies":0.2331288344,
        "MMLU_machine_learning":0.2946428571,
        "MMLU_management":0.2621359223,
        "MMLU_marketing":0.4102564103,
        "MMLU_medical_genetics":0.27,
        "MMLU_miscellaneous":0.3103448276,
        "MMLU_moral_disputes":0.3526011561,
        "MMLU_moral_scenarios":0.2413407821,
        "MMLU_nutrition":0.3006535948,
        "MMLU_philosophy":0.3344051447,
        "MMLU_prehistory":0.287037037,
        "MMLU_professional_accounting":0.2836879433,
        "MMLU_professional_law":0.2757496741,
        "MMLU_professional_medicine":0.1985294118,
        "MMLU_professional_psychology":0.2794117647,
        "MMLU_public_relations":0.3636363636,
        "MMLU_security_studies":0.2571428571,
        "MMLU_sociology":0.3432835821,
        "MMLU_us_foreign_policy":0.38,
        "MMLU_virology":0.3373493976,
        "MMLU_world_religions":0.3040935673
    },
    {
        "Model":"starcoder_mirror",
        "URL":"https:\/\/huggingface.co\/lizhuang144\/starcoder_mirror",
        "full_model_name":"lizhuang144\/starcoder_mirror",
        "Parameters":null,
        "MMLU_average":0.2929342945,
        "arc:challenge|25":0.2696245734,
        "hellaswag|10":0.372734515,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.3037037037,
        "MMLU_astronomy":0.2565789474,
        "MMLU_business_ethics":0.35,
        "MMLU_clinical_knowledge":0.3094339623,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.23,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.2543352601,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.45,
        "MMLU_conceptual_physics":0.2808510638,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.4206896552,
        "MMLU_elementary_mathematics":0.2751322751,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.29,
        "MMLU_high_school_biology":0.2870967742,
        "MMLU_high_school_chemistry":0.2413793103,
        "MMLU_high_school_computer_science":0.41,
        "MMLU_high_school_european_history":0.2909090909,
        "MMLU_high_school_geography":0.202020202,
        "MMLU_high_school_government_and_politics":0.2953367876,
        "MMLU_high_school_macroeconomics":0.2230769231,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.2605042017,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.247706422,
        "MMLU_high_school_statistics":0.2083333333,
        "MMLU_high_school_us_history":0.2794117647,
        "MMLU_high_school_world_history":0.2911392405,
        "MMLU_human_aging":0.3587443946,
        "MMLU_human_sexuality":0.2213740458,
        "MMLU_international_law":0.3140495868,
        "MMLU_jurisprudence":0.25,
        "MMLU_logical_fallacies":0.282208589,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.3009708738,
        "MMLU_marketing":0.3803418803,
        "MMLU_medical_genetics":0.29,
        "MMLU_miscellaneous":0.3307790549,
        "MMLU_moral_disputes":0.3410404624,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2973856209,
        "MMLU_philosophy":0.3504823151,
        "MMLU_prehistory":0.3086419753,
        "MMLU_professional_accounting":0.280141844,
        "MMLU_professional_law":0.294654498,
        "MMLU_professional_medicine":0.1801470588,
        "MMLU_professional_psychology":0.3022875817,
        "MMLU_public_relations":0.3636363636,
        "MMLU_security_studies":0.2734693878,
        "MMLU_sociology":0.2885572139,
        "MMLU_us_foreign_policy":0.35,
        "MMLU_virology":0.2891566265,
        "MMLU_world_religions":0.3333333333
    },
    {
        "Model":"Barcenas-3b",
        "URL":"https:\/\/huggingface.co\/Danielbrdz\/Barcenas-3b",
        "full_model_name":"Danielbrdz\/Barcenas-3b",
        "Parameters":3.0,
        "MMLU_average":0.2916416215,
        "arc:challenge|25":0.3916382253,
        "hellaswag|10":0.5013941446,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.2814814815,
        "MMLU_astronomy":0.3289473684,
        "MMLU_business_ethics":0.25,
        "MMLU_clinical_knowledge":0.3358490566,
        "MMLU_college_biology":0.2916666667,
        "MMLU_college_chemistry":0.25,
        "MMLU_college_computer_science":0.36,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.2369942197,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.29,
        "MMLU_conceptual_physics":0.2212765957,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.3724137931,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.1587301587,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2806451613,
        "MMLU_high_school_chemistry":0.2561576355,
        "MMLU_high_school_computer_science":0.29,
        "MMLU_high_school_european_history":0.296969697,
        "MMLU_high_school_geography":0.3636363636,
        "MMLU_high_school_government_and_politics":0.3471502591,
        "MMLU_high_school_macroeconomics":0.3564102564,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.3109243697,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.2642201835,
        "MMLU_high_school_statistics":0.3055555556,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.3037974684,
        "MMLU_human_aging":0.2556053812,
        "MMLU_human_sexuality":0.3129770992,
        "MMLU_international_law":0.3553719008,
        "MMLU_jurisprudence":0.25,
        "MMLU_logical_fallacies":0.3006134969,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.2427184466,
        "MMLU_marketing":0.2692307692,
        "MMLU_medical_genetics":0.29,
        "MMLU_miscellaneous":0.2975734355,
        "MMLU_moral_disputes":0.2832369942,
        "MMLU_moral_scenarios":0.2446927374,
        "MMLU_nutrition":0.3333333333,
        "MMLU_philosophy":0.3665594855,
        "MMLU_prehistory":0.3549382716,
        "MMLU_professional_accounting":0.2730496454,
        "MMLU_professional_law":0.2822685789,
        "MMLU_professional_medicine":0.3014705882,
        "MMLU_professional_psychology":0.2614379085,
        "MMLU_public_relations":0.2272727273,
        "MMLU_security_studies":0.4,
        "MMLU_sociology":0.3233830846,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.2771084337,
        "MMLU_world_religions":0.3157894737
    },
    {
        "Model":"mpt_delta_tuned_model_v2",
        "URL":"https:\/\/huggingface.co\/nathan0\/mpt_delta_tuned_model_v2",
        "full_model_name":"nathan0\/mpt_delta_tuned_model_v2",
        "Parameters":null,
        "MMLU_average":0.2872674408,
        "arc:challenge|25":0.4581911263,
        "hellaswag|10":0.5774746067,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.2740740741,
        "MMLU_astronomy":0.2565789474,
        "MMLU_business_ethics":0.24,
        "MMLU_clinical_knowledge":0.3169811321,
        "MMLU_college_biology":0.3055555556,
        "MMLU_college_chemistry":0.23,
        "MMLU_college_computer_science":0.36,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.2196531792,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.31,
        "MMLU_conceptual_physics":0.3106382979,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.3172413793,
        "MMLU_elementary_mathematics":0.3148148148,
        "MMLU_formal_logic":0.1904761905,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.2903225806,
        "MMLU_high_school_chemistry":0.2610837438,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.303030303,
        "MMLU_high_school_government_and_politics":0.310880829,
        "MMLU_high_school_macroeconomics":0.3051282051,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.2857142857,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.2697247706,
        "MMLU_high_school_statistics":0.1851851852,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2742616034,
        "MMLU_human_aging":0.3632286996,
        "MMLU_human_sexuality":0.2824427481,
        "MMLU_international_law":0.3884297521,
        "MMLU_jurisprudence":0.3703703704,
        "MMLU_logical_fallacies":0.282208589,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.2815533981,
        "MMLU_marketing":0.3247863248,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.2975734355,
        "MMLU_moral_disputes":0.2774566474,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2679738562,
        "MMLU_philosophy":0.2861736334,
        "MMLU_prehistory":0.2839506173,
        "MMLU_professional_accounting":0.2765957447,
        "MMLU_professional_law":0.2568448501,
        "MMLU_professional_medicine":0.2022058824,
        "MMLU_professional_psychology":0.3120915033,
        "MMLU_public_relations":0.3636363636,
        "MMLU_security_studies":0.3102040816,
        "MMLU_sociology":0.2338308458,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.3493975904,
        "MMLU_world_religions":0.269005848
    },
    {
        "Model":"mpt_delta_tuned_model_v3",
        "URL":"https:\/\/huggingface.co\/nathan0\/mpt_delta_tuned_model_v3",
        "full_model_name":"nathan0\/mpt_delta_tuned_model_v3",
        "Parameters":null,
        "MMLU_average":0.2872674408,
        "arc:challenge|25":0.4581911263,
        "hellaswag|10":0.5774746067,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.2740740741,
        "MMLU_astronomy":0.2565789474,
        "MMLU_business_ethics":0.24,
        "MMLU_clinical_knowledge":0.3169811321,
        "MMLU_college_biology":0.3055555556,
        "MMLU_college_chemistry":0.23,
        "MMLU_college_computer_science":0.36,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.2196531792,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.31,
        "MMLU_conceptual_physics":0.3106382979,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.3172413793,
        "MMLU_elementary_mathematics":0.3148148148,
        "MMLU_formal_logic":0.1904761905,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.2903225806,
        "MMLU_high_school_chemistry":0.2610837438,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.303030303,
        "MMLU_high_school_government_and_politics":0.310880829,
        "MMLU_high_school_macroeconomics":0.3051282051,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.2857142857,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.2697247706,
        "MMLU_high_school_statistics":0.1851851852,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2742616034,
        "MMLU_human_aging":0.3632286996,
        "MMLU_human_sexuality":0.2824427481,
        "MMLU_international_law":0.3884297521,
        "MMLU_jurisprudence":0.3703703704,
        "MMLU_logical_fallacies":0.282208589,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.2815533981,
        "MMLU_marketing":0.3247863248,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.2975734355,
        "MMLU_moral_disputes":0.2774566474,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2679738562,
        "MMLU_philosophy":0.2861736334,
        "MMLU_prehistory":0.2839506173,
        "MMLU_professional_accounting":0.2765957447,
        "MMLU_professional_law":0.2568448501,
        "MMLU_professional_medicine":0.2022058824,
        "MMLU_professional_psychology":0.3120915033,
        "MMLU_public_relations":0.3636363636,
        "MMLU_security_studies":0.3102040816,
        "MMLU_sociology":0.2338308458,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.3493975904,
        "MMLU_world_religions":0.269005848
    },
    {
        "Model":"MiniMA-3B",
        "URL":"https:\/\/huggingface.co\/GeneZC\/MiniMA-3B",
        "full_model_name":"GeneZC\/MiniMA-3B",
        "Parameters":3.0,
        "MMLU_average":0.2868921734,
        "arc:challenge|25":0.3993174061,
        "hellaswag|10":0.4999004182,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.2592592593,
        "MMLU_astronomy":0.2828947368,
        "MMLU_business_ethics":0.24,
        "MMLU_clinical_knowledge":0.2943396226,
        "MMLU_college_biology":0.2847222222,
        "MMLU_college_chemistry":0.26,
        "MMLU_college_computer_science":0.4,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.2543352601,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.33,
        "MMLU_conceptual_physics":0.2255319149,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.4,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.1825396825,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.2677419355,
        "MMLU_high_school_chemistry":0.197044335,
        "MMLU_high_school_computer_science":0.29,
        "MMLU_high_school_european_history":0.303030303,
        "MMLU_high_school_geography":0.2727272727,
        "MMLU_high_school_government_and_politics":0.310880829,
        "MMLU_high_school_macroeconomics":0.3102564103,
        "MMLU_high_school_mathematics":0.237037037,
        "MMLU_high_school_microeconomics":0.3109243697,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.271559633,
        "MMLU_high_school_statistics":0.287037037,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2573839662,
        "MMLU_human_aging":0.2914798206,
        "MMLU_human_sexuality":0.320610687,
        "MMLU_international_law":0.3636363636,
        "MMLU_jurisprudence":0.2037037037,
        "MMLU_logical_fallacies":0.2883435583,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.1844660194,
        "MMLU_marketing":0.3418803419,
        "MMLU_medical_genetics":0.38,
        "MMLU_miscellaneous":0.2924648787,
        "MMLU_moral_disputes":0.3121387283,
        "MMLU_moral_scenarios":0.2122905028,
        "MMLU_nutrition":0.3137254902,
        "MMLU_philosophy":0.3729903537,
        "MMLU_prehistory":0.3024691358,
        "MMLU_professional_accounting":0.2872340426,
        "MMLU_professional_law":0.2940026076,
        "MMLU_professional_medicine":0.2647058824,
        "MMLU_professional_psychology":0.272875817,
        "MMLU_public_relations":0.2636363636,
        "MMLU_security_studies":0.4,
        "MMLU_sociology":0.3034825871,
        "MMLU_us_foreign_policy":0.21,
        "MMLU_virology":0.234939759,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"test-1400",
        "URL":"https:\/\/huggingface.co\/Devio\/test-1400",
        "full_model_name":"Devio\/test-1400",
        "Parameters":null,
        "MMLU_average":0.2862840123,
        "arc:challenge|25":0.3523890785,
        "hellaswag|10":0.4785899223,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.2296296296,
        "MMLU_astronomy":0.3684210526,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.3169811321,
        "MMLU_college_biology":0.2430555556,
        "MMLU_college_chemistry":0.32,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.3063583815,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.3446808511,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.2857142857,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.3225806452,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.19,
        "MMLU_high_school_european_history":0.2606060606,
        "MMLU_high_school_geography":0.3686868687,
        "MMLU_high_school_government_and_politics":0.3523316062,
        "MMLU_high_school_macroeconomics":0.3615384615,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.3361344538,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.3376146789,
        "MMLU_high_school_statistics":0.4074074074,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.1983122363,
        "MMLU_human_aging":0.269058296,
        "MMLU_human_sexuality":0.3129770992,
        "MMLU_international_law":0.132231405,
        "MMLU_jurisprudence":0.25,
        "MMLU_logical_fallacies":0.2085889571,
        "MMLU_machine_learning":0.1607142857,
        "MMLU_management":0.4077669903,
        "MMLU_marketing":0.235042735,
        "MMLU_medical_genetics":0.25,
        "MMLU_miscellaneous":0.245210728,
        "MMLU_moral_disputes":0.274566474,
        "MMLU_moral_scenarios":0.2536312849,
        "MMLU_nutrition":0.3104575163,
        "MMLU_philosophy":0.2926045016,
        "MMLU_prehistory":0.2561728395,
        "MMLU_professional_accounting":0.2836879433,
        "MMLU_professional_law":0.2438070404,
        "MMLU_professional_medicine":0.4227941176,
        "MMLU_professional_psychology":0.2352941176,
        "MMLU_public_relations":0.2818181818,
        "MMLU_security_studies":0.4,
        "MMLU_sociology":0.3333333333,
        "MMLU_us_foreign_policy":0.3,
        "MMLU_virology":0.2530120482,
        "MMLU_world_religions":0.2339181287
    },
    {
        "Model":"GPT-J-Pyg_PPO-6B",
        "URL":"https:\/\/huggingface.co\/TehVenom\/GPT-J-Pyg_PPO-6B",
        "full_model_name":"TehVenom\/GPT-J-Pyg_PPO-6B",
        "Parameters":6.0,
        "MMLU_average":0.2852396582,
        "arc:challenge|25":0.3813993174,
        "hellaswag|10":0.5008962358,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.3185185185,
        "MMLU_astronomy":0.3421052632,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.2867924528,
        "MMLU_college_biology":0.2430555556,
        "MMLU_college_chemistry":0.22,
        "MMLU_college_computer_science":0.25,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.2774566474,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.29,
        "MMLU_conceptual_physics":0.3276595745,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.3379310345,
        "MMLU_elementary_mathematics":0.2724867725,
        "MMLU_formal_logic":0.2142857143,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.2870967742,
        "MMLU_high_school_chemistry":0.2512315271,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.2727272727,
        "MMLU_high_school_geography":0.2373737374,
        "MMLU_high_school_government_and_politics":0.2849740933,
        "MMLU_high_school_macroeconomics":0.3051282051,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.2521008403,
        "MMLU_high_school_physics":0.2317880795,
        "MMLU_high_school_psychology":0.2403669725,
        "MMLU_high_school_statistics":0.1898148148,
        "MMLU_high_school_us_history":0.2303921569,
        "MMLU_high_school_world_history":0.2278481013,
        "MMLU_human_aging":0.2959641256,
        "MMLU_human_sexuality":0.2519083969,
        "MMLU_international_law":0.4380165289,
        "MMLU_jurisprudence":0.3055555556,
        "MMLU_logical_fallacies":0.2944785276,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.2621359223,
        "MMLU_marketing":0.2692307692,
        "MMLU_medical_genetics":0.23,
        "MMLU_miscellaneous":0.2809706258,
        "MMLU_moral_disputes":0.3323699422,
        "MMLU_moral_scenarios":0.2715083799,
        "MMLU_nutrition":0.3235294118,
        "MMLU_philosophy":0.3118971061,
        "MMLU_prehistory":0.3117283951,
        "MMLU_professional_accounting":0.2588652482,
        "MMLU_professional_law":0.2861799218,
        "MMLU_professional_medicine":0.2316176471,
        "MMLU_professional_psychology":0.3088235294,
        "MMLU_public_relations":0.3727272727,
        "MMLU_security_studies":0.4285714286,
        "MMLU_sociology":0.3781094527,
        "MMLU_us_foreign_policy":0.34,
        "MMLU_virology":0.2771084337,
        "MMLU_world_religions":0.298245614
    },
    {
        "Model":"Hermes-LLongMA-2-7b-8k",
        "URL":"https:\/\/huggingface.co\/conceptofmind\/Hermes-LLongMA-2-7b-8k",
        "full_model_name":"conceptofmind\/Hermes-LLongMA-2-7b-8k",
        "Parameters":7.0,
        "MMLU_average":0.2851005106,
        "arc:challenge|25":0.4692832765,
        "hellaswag|10":0.5497908783,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.2222222222,
        "MMLU_astronomy":0.2894736842,
        "MMLU_business_ethics":0.34,
        "MMLU_clinical_knowledge":0.2566037736,
        "MMLU_college_biology":0.2430555556,
        "MMLU_college_chemistry":0.23,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.37,
        "MMLU_conceptual_physics":0.285106383,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2689655172,
        "MMLU_elementary_mathematics":0.246031746,
        "MMLU_formal_logic":0.2063492063,
        "MMLU_global_facts":0.21,
        "MMLU_high_school_biology":0.2483870968,
        "MMLU_high_school_chemistry":0.1921182266,
        "MMLU_high_school_computer_science":0.29,
        "MMLU_high_school_european_history":0.3636363636,
        "MMLU_high_school_geography":0.2676767677,
        "MMLU_high_school_government_and_politics":0.2538860104,
        "MMLU_high_school_macroeconomics":0.2435897436,
        "MMLU_high_school_mathematics":0.2333333333,
        "MMLU_high_school_microeconomics":0.231092437,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.3155963303,
        "MMLU_high_school_statistics":0.2962962963,
        "MMLU_high_school_us_history":0.3333333333,
        "MMLU_high_school_world_history":0.388185654,
        "MMLU_human_aging":0.2735426009,
        "MMLU_human_sexuality":0.2748091603,
        "MMLU_international_law":0.4132231405,
        "MMLU_jurisprudence":0.3333333333,
        "MMLU_logical_fallacies":0.2147239264,
        "MMLU_machine_learning":0.2946428571,
        "MMLU_management":0.2524271845,
        "MMLU_marketing":0.3504273504,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.2988505747,
        "MMLU_moral_disputes":0.3005780347,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.3300653595,
        "MMLU_philosophy":0.2733118971,
        "MMLU_prehistory":0.3055555556,
        "MMLU_professional_accounting":0.2836879433,
        "MMLU_professional_law":0.2874837027,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.2810457516,
        "MMLU_public_relations":0.3,
        "MMLU_security_studies":0.4163265306,
        "MMLU_sociology":0.3432835821,
        "MMLU_us_foreign_policy":0.37,
        "MMLU_virology":0.2710843373,
        "MMLU_world_religions":0.3976608187
    },
    {
        "Model":"Dolly_Shygmalion-6b-Dev_V8P2",
        "URL":"https:\/\/huggingface.co\/TehVenom\/Dolly_Shygmalion-6b-Dev_V8P2",
        "full_model_name":"TehVenom\/Dolly_Shygmalion-6b-Dev_V8P2",
        "Parameters":6.0,
        "MMLU_average":0.2848332779,
        "arc:challenge|25":0.383105802,
        "hellaswag|10":0.4999004182,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.2888888889,
        "MMLU_astronomy":0.3157894737,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.3018867925,
        "MMLU_college_biology":0.2708333333,
        "MMLU_college_chemistry":0.17,
        "MMLU_college_computer_science":0.16,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.3063583815,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.37,
        "MMLU_conceptual_physics":0.2978723404,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.253968254,
        "MMLU_formal_logic":0.1984126984,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.2967741935,
        "MMLU_high_school_chemistry":0.2561576355,
        "MMLU_high_school_computer_science":0.27,
        "MMLU_high_school_european_history":0.3090909091,
        "MMLU_high_school_geography":0.2222222222,
        "MMLU_high_school_government_and_politics":0.3212435233,
        "MMLU_high_school_macroeconomics":0.3384615385,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.231092437,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.2385321101,
        "MMLU_high_school_statistics":0.1805555556,
        "MMLU_high_school_us_history":0.2941176471,
        "MMLU_high_school_world_history":0.2953586498,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2213740458,
        "MMLU_international_law":0.4380165289,
        "MMLU_jurisprudence":0.3148148148,
        "MMLU_logical_fallacies":0.2576687117,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.2233009709,
        "MMLU_marketing":0.2991452991,
        "MMLU_medical_genetics":0.32,
        "MMLU_miscellaneous":0.3269476373,
        "MMLU_moral_disputes":0.289017341,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.3300653595,
        "MMLU_philosophy":0.3247588424,
        "MMLU_prehistory":0.3271604938,
        "MMLU_professional_accounting":0.2730496454,
        "MMLU_professional_law":0.3063885267,
        "MMLU_professional_medicine":0.2132352941,
        "MMLU_professional_psychology":0.2745098039,
        "MMLU_public_relations":0.3636363636,
        "MMLU_security_studies":0.4326530612,
        "MMLU_sociology":0.3333333333,
        "MMLU_us_foreign_policy":0.34,
        "MMLU_virology":0.3072289157,
        "MMLU_world_religions":0.3040935673
    },
    {
        "Model":"gpt-sw3-20b",
        "URL":"https:\/\/huggingface.co\/AI-Sweden-Models\/gpt-sw3-20b",
        "full_model_name":"AI-Sweden-Models\/gpt-sw3-20b",
        "Parameters":20.0,
        "MMLU_average":0.2846580436,
        "arc:challenge|25":0.3813993174,
        "hellaswag|10":0.507767377,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.2222222222,
        "MMLU_astronomy":0.2697368421,
        "MMLU_business_ethics":0.32,
        "MMLU_clinical_knowledge":0.2943396226,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.29,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.2138728324,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.37,
        "MMLU_conceptual_physics":0.3234042553,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.2827586207,
        "MMLU_elementary_mathematics":0.2592592593,
        "MMLU_formal_logic":0.1984126984,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.2483870968,
        "MMLU_high_school_chemistry":0.2610837438,
        "MMLU_high_school_computer_science":0.27,
        "MMLU_high_school_european_history":0.296969697,
        "MMLU_high_school_geography":0.2878787879,
        "MMLU_high_school_government_and_politics":0.2538860104,
        "MMLU_high_school_macroeconomics":0.2256410256,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.2268907563,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.2844036697,
        "MMLU_high_school_statistics":0.162037037,
        "MMLU_high_school_us_history":0.3431372549,
        "MMLU_high_school_world_history":0.3839662447,
        "MMLU_human_aging":0.4035874439,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.3719008264,
        "MMLU_jurisprudence":0.3703703704,
        "MMLU_logical_fallacies":0.3312883436,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.2912621359,
        "MMLU_marketing":0.358974359,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2962962963,
        "MMLU_moral_disputes":0.2658959538,
        "MMLU_moral_scenarios":0.2581005587,
        "MMLU_nutrition":0.2908496732,
        "MMLU_philosophy":0.2733118971,
        "MMLU_prehistory":0.2839506173,
        "MMLU_professional_accounting":0.2907801418,
        "MMLU_professional_law":0.2887874837,
        "MMLU_professional_medicine":0.2279411765,
        "MMLU_professional_psychology":0.2745098039,
        "MMLU_public_relations":0.3454545455,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.3134328358,
        "MMLU_us_foreign_policy":0.35,
        "MMLU_virology":0.3072289157,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"LightGPT",
        "URL":"https:\/\/huggingface.co\/amazon\/LightGPT",
        "full_model_name":"amazon\/LightGPT",
        "Parameters":null,
        "MMLU_average":0.284474722,
        "arc:challenge|25":0.3720136519,
        "hellaswag|10":0.4719179446,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.2962962963,
        "MMLU_astronomy":0.2960526316,
        "MMLU_business_ethics":0.2,
        "MMLU_clinical_knowledge":0.3056603774,
        "MMLU_college_biology":0.2638888889,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.29,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.2947976879,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.36,
        "MMLU_conceptual_physics":0.3361702128,
        "MMLU_econometrics":0.2192982456,
        "MMLU_electrical_engineering":0.2896551724,
        "MMLU_elementary_mathematics":0.246031746,
        "MMLU_formal_logic":0.2380952381,
        "MMLU_global_facts":0.28,
        "MMLU_high_school_biology":0.2580645161,
        "MMLU_high_school_chemistry":0.2266009852,
        "MMLU_high_school_computer_science":0.32,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.2878787879,
        "MMLU_high_school_government_and_politics":0.3834196891,
        "MMLU_high_school_macroeconomics":0.3102564103,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.3025210084,
        "MMLU_high_school_physics":0.238410596,
        "MMLU_high_school_psychology":0.2550458716,
        "MMLU_high_school_statistics":0.2916666667,
        "MMLU_high_school_us_history":0.2843137255,
        "MMLU_high_school_world_history":0.2827004219,
        "MMLU_human_aging":0.3856502242,
        "MMLU_human_sexuality":0.1908396947,
        "MMLU_international_law":0.347107438,
        "MMLU_jurisprudence":0.2962962963,
        "MMLU_logical_fallacies":0.2515337423,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.2233009709,
        "MMLU_marketing":0.2435897436,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2873563218,
        "MMLU_moral_disputes":0.2427745665,
        "MMLU_moral_scenarios":0.2446927374,
        "MMLU_nutrition":0.2941176471,
        "MMLU_philosophy":0.3118971061,
        "MMLU_prehistory":0.3487654321,
        "MMLU_professional_accounting":0.2836879433,
        "MMLU_professional_law":0.2953063885,
        "MMLU_professional_medicine":0.2941176471,
        "MMLU_professional_psychology":0.2794117647,
        "MMLU_public_relations":0.3545454545,
        "MMLU_security_studies":0.412244898,
        "MMLU_sociology":0.2985074627,
        "MMLU_us_foreign_policy":0.19,
        "MMLU_virology":0.3192771084,
        "MMLU_world_religions":0.3274853801
    },
    {
        "Model":"lamini-cerebras-1.3b",
        "URL":"https:\/\/huggingface.co\/MBZUAI\/lamini-cerebras-1.3b",
        "full_model_name":"MBZUAI\/lamini-cerebras-1.3b",
        "Parameters":1.3,
        "MMLU_average":0.2842718915,
        "arc:challenge|25":0.2235494881,
        "hellaswag|10":0.3320055766,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.2518518519,
        "MMLU_astronomy":0.2565789474,
        "MMLU_business_ethics":0.38,
        "MMLU_clinical_knowledge":0.3094339623,
        "MMLU_college_biology":0.2638888889,
        "MMLU_college_chemistry":0.32,
        "MMLU_college_computer_science":0.29,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.2716763006,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.285106383,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.3034482759,
        "MMLU_elementary_mathematics":0.253968254,
        "MMLU_formal_logic":0.2619047619,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.2806451613,
        "MMLU_high_school_chemistry":0.2413793103,
        "MMLU_high_school_computer_science":0.3,
        "MMLU_high_school_european_history":0.2545454545,
        "MMLU_high_school_geography":0.3585858586,
        "MMLU_high_school_government_and_politics":0.3678756477,
        "MMLU_high_school_macroeconomics":0.3692307692,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.231092437,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.3486238532,
        "MMLU_high_school_statistics":0.3333333333,
        "MMLU_high_school_us_history":0.2696078431,
        "MMLU_high_school_world_history":0.2320675105,
        "MMLU_human_aging":0.134529148,
        "MMLU_human_sexuality":0.3282442748,
        "MMLU_international_law":0.305785124,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.3374233129,
        "MMLU_machine_learning":0.2142857143,
        "MMLU_management":0.359223301,
        "MMLU_marketing":0.264957265,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.2362707535,
        "MMLU_moral_disputes":0.323699422,
        "MMLU_moral_scenarios":0.2324022346,
        "MMLU_nutrition":0.2875816993,
        "MMLU_philosophy":0.2990353698,
        "MMLU_prehistory":0.2407407407,
        "MMLU_professional_accounting":0.2624113475,
        "MMLU_professional_law":0.2444589309,
        "MMLU_professional_medicine":0.4448529412,
        "MMLU_professional_psychology":0.2745098039,
        "MMLU_public_relations":0.2454545455,
        "MMLU_security_studies":0.4040816327,
        "MMLU_sociology":0.3432835821,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.186746988,
        "MMLU_world_religions":0.2631578947
    },
    {
        "Model":"PPO_Pygway-6b-Mix",
        "URL":"https:\/\/huggingface.co\/KoboldAI\/PPO_Pygway-6b-Mix",
        "full_model_name":"KoboldAI\/PPO_Pygway-6b-Mix",
        "Parameters":6.0,
        "MMLU_average":0.284227396,
        "arc:challenge|25":0.3771331058,
        "hellaswag|10":0.5007966541,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.2740740741,
        "MMLU_astronomy":0.3092105263,
        "MMLU_business_ethics":0.22,
        "MMLU_clinical_knowledge":0.3056603774,
        "MMLU_college_biology":0.25,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.25,
        "MMLU_college_mathematics":0.35,
        "MMLU_college_medicine":0.2832369942,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.31,
        "MMLU_conceptual_physics":0.3234042553,
        "MMLU_econometrics":0.3245614035,
        "MMLU_electrical_engineering":0.275862069,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.2301587302,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.3193548387,
        "MMLU_high_school_chemistry":0.275862069,
        "MMLU_high_school_computer_science":0.26,
        "MMLU_high_school_european_history":0.2848484848,
        "MMLU_high_school_geography":0.2373737374,
        "MMLU_high_school_government_and_politics":0.2538860104,
        "MMLU_high_school_macroeconomics":0.3179487179,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.2521008403,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.2385321101,
        "MMLU_high_school_statistics":0.1944444444,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.2489451477,
        "MMLU_human_aging":0.3766816143,
        "MMLU_human_sexuality":0.2748091603,
        "MMLU_international_law":0.4297520661,
        "MMLU_jurisprudence":0.2777777778,
        "MMLU_logical_fallacies":0.2392638037,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.213592233,
        "MMLU_marketing":0.3034188034,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.2899106003,
        "MMLU_moral_disputes":0.2803468208,
        "MMLU_moral_scenarios":0.2525139665,
        "MMLU_nutrition":0.3202614379,
        "MMLU_philosophy":0.3054662379,
        "MMLU_prehistory":0.3302469136,
        "MMLU_professional_accounting":0.2659574468,
        "MMLU_professional_law":0.2822685789,
        "MMLU_professional_medicine":0.2022058824,
        "MMLU_professional_psychology":0.2941176471,
        "MMLU_public_relations":0.3818181818,
        "MMLU_security_studies":0.4326530612,
        "MMLU_sociology":0.3432835821,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.2951807229,
        "MMLU_world_religions":0.3157894737
    },
    {
        "Model":"CodeLlama-7b-Python-hf",
        "URL":"https:\/\/huggingface.co\/codellama\/CodeLlama-7b-Python-hf",
        "full_model_name":"codellama\/CodeLlama-7b-Python-hf",
        "Parameters":7.0,
        "MMLU_average":0.2837187709,
        "arc:challenge|25":0.2653583618,
        "hellaswag|10":0.3919537941,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.3111111111,
        "MMLU_astronomy":0.2763157895,
        "MMLU_business_ethics":0.17,
        "MMLU_clinical_knowledge":0.2867924528,
        "MMLU_college_biology":0.2986111111,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.2658959538,
        "MMLU_college_physics":0.3529411765,
        "MMLU_computer_security":0.19,
        "MMLU_conceptual_physics":0.2255319149,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.2896551724,
        "MMLU_elementary_mathematics":0.2592592593,
        "MMLU_formal_logic":0.1746031746,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.3290322581,
        "MMLU_high_school_chemistry":0.3448275862,
        "MMLU_high_school_computer_science":0.3,
        "MMLU_high_school_european_history":0.2727272727,
        "MMLU_high_school_geography":0.3636363636,
        "MMLU_high_school_government_and_politics":0.2901554404,
        "MMLU_high_school_macroeconomics":0.2615384615,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.2941176471,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.3082568807,
        "MMLU_high_school_statistics":0.462962963,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.264573991,
        "MMLU_human_sexuality":0.2442748092,
        "MMLU_international_law":0.305785124,
        "MMLU_jurisprudence":0.212962963,
        "MMLU_logical_fallacies":0.3067484663,
        "MMLU_machine_learning":0.2053571429,
        "MMLU_management":0.213592233,
        "MMLU_marketing":0.2478632479,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.3256704981,
        "MMLU_moral_disputes":0.2052023121,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.3169934641,
        "MMLU_philosophy":0.3151125402,
        "MMLU_prehistory":0.2469135802,
        "MMLU_professional_accounting":0.2765957447,
        "MMLU_professional_law":0.2477183833,
        "MMLU_professional_medicine":0.3823529412,
        "MMLU_professional_psychology":0.2222222222,
        "MMLU_public_relations":0.2272727273,
        "MMLU_security_studies":0.3918367347,
        "MMLU_sociology":0.223880597,
        "MMLU_us_foreign_policy":0.37,
        "MMLU_virology":0.2168674699,
        "MMLU_world_religions":0.4152046784
    },
    {
        "Model":"oasst-pythia-12b-reference",
        "URL":"https:\/\/huggingface.co\/dvruette\/oasst-pythia-12b-reference",
        "full_model_name":"dvruette\/oasst-pythia-12b-reference",
        "Parameters":12.0,
        "MMLU_average":0.2832596689,
        "arc:challenge|25":0.4044368601,
        "hellaswag|10":0.5123481378,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.3111111111,
        "MMLU_astronomy":0.3157894737,
        "MMLU_business_ethics":0.36,
        "MMLU_clinical_knowledge":0.3320754717,
        "MMLU_college_biology":0.2777777778,
        "MMLU_college_chemistry":0.3,
        "MMLU_college_computer_science":0.3,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.2947976879,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.25,
        "MMLU_conceptual_physics":0.2255319149,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.275862069,
        "MMLU_elementary_mathematics":0.2698412698,
        "MMLU_formal_logic":0.2301587302,
        "MMLU_global_facts":0.27,
        "MMLU_high_school_biology":0.2870967742,
        "MMLU_high_school_chemistry":0.2512315271,
        "MMLU_high_school_computer_science":0.21,
        "MMLU_high_school_european_history":0.2787878788,
        "MMLU_high_school_geography":0.3585858586,
        "MMLU_high_school_government_and_politics":0.3367875648,
        "MMLU_high_school_macroeconomics":0.3256410256,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.2478991597,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.2642201835,
        "MMLU_high_school_statistics":0.3287037037,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.223628692,
        "MMLU_human_aging":0.264573991,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.3884297521,
        "MMLU_jurisprudence":0.2777777778,
        "MMLU_logical_fallacies":0.3067484663,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.2718446602,
        "MMLU_marketing":0.3333333333,
        "MMLU_medical_genetics":0.23,
        "MMLU_miscellaneous":0.2630906769,
        "MMLU_moral_disputes":0.2803468208,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2875816993,
        "MMLU_philosophy":0.3054662379,
        "MMLU_prehistory":0.2685185185,
        "MMLU_professional_accounting":0.2943262411,
        "MMLU_professional_law":0.2568448501,
        "MMLU_professional_medicine":0.4632352941,
        "MMLU_professional_psychology":0.2418300654,
        "MMLU_public_relations":0.2272727273,
        "MMLU_security_studies":0.3755102041,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.2530120482,
        "MMLU_world_religions":0.2339181287
    },
    {
        "Model":"Janemalion-6B",
        "URL":"https:\/\/huggingface.co\/Lazycuber\/Janemalion-6B",
        "full_model_name":"Lazycuber\/Janemalion-6B",
        "Parameters":6.0,
        "MMLU_average":0.2828258596,
        "arc:challenge|25":0.3865187713,
        "hellaswag|10":0.5052778331,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.2962962963,
        "MMLU_astronomy":0.3157894737,
        "MMLU_business_ethics":0.26,
        "MMLU_clinical_knowledge":0.3094339623,
        "MMLU_college_biology":0.2638888889,
        "MMLU_college_chemistry":0.17,
        "MMLU_college_computer_science":0.23,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.2658959538,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.36,
        "MMLU_conceptual_physics":0.3446808511,
        "MMLU_econometrics":0.3333333333,
        "MMLU_electrical_engineering":0.2482758621,
        "MMLU_elementary_mathematics":0.2513227513,
        "MMLU_formal_logic":0.1984126984,
        "MMLU_global_facts":0.28,
        "MMLU_high_school_biology":0.2548387097,
        "MMLU_high_school_chemistry":0.2660098522,
        "MMLU_high_school_computer_science":0.23,
        "MMLU_high_school_european_history":0.2727272727,
        "MMLU_high_school_geography":0.2171717172,
        "MMLU_high_school_government_and_politics":0.2538860104,
        "MMLU_high_school_macroeconomics":0.2897435897,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.2394957983,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.2605504587,
        "MMLU_high_school_statistics":0.1805555556,
        "MMLU_high_school_us_history":0.2941176471,
        "MMLU_high_school_world_history":0.3037974684,
        "MMLU_human_aging":0.3408071749,
        "MMLU_human_sexuality":0.2519083969,
        "MMLU_international_law":0.3636363636,
        "MMLU_jurisprudence":0.3240740741,
        "MMLU_logical_fallacies":0.2515337423,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2735042735,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.3243933589,
        "MMLU_moral_disputes":0.2947976879,
        "MMLU_moral_scenarios":0.2413407821,
        "MMLU_nutrition":0.3431372549,
        "MMLU_philosophy":0.2926045016,
        "MMLU_prehistory":0.3148148148,
        "MMLU_professional_accounting":0.2978723404,
        "MMLU_professional_law":0.3168187744,
        "MMLU_professional_medicine":0.2352941176,
        "MMLU_professional_psychology":0.2745098039,
        "MMLU_public_relations":0.3545454545,
        "MMLU_security_studies":0.4285714286,
        "MMLU_sociology":0.3631840796,
        "MMLU_us_foreign_policy":0.34,
        "MMLU_virology":0.2951807229,
        "MMLU_world_religions":0.3157894737
    },
    {
        "Model":"InstructPalmyra-20b",
        "URL":"https:\/\/huggingface.co\/Writer\/InstructPalmyra-20b",
        "full_model_name":"Writer\/InstructPalmyra-20b",
        "Parameters":20.0,
        "MMLU_average":0.2826404952,
        "arc:challenge|25":0.4556313993,
        "hellaswag|10":0.555666202,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.3333333333,
        "MMLU_astronomy":0.2368421053,
        "MMLU_business_ethics":0.24,
        "MMLU_clinical_knowledge":0.2905660377,
        "MMLU_college_biology":0.3125,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.17,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.1734104046,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.38,
        "MMLU_conceptual_physics":0.3106382979,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.2896551724,
        "MMLU_elementary_mathematics":0.2724867725,
        "MMLU_formal_logic":0.2142857143,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.2870967742,
        "MMLU_high_school_chemistry":0.3103448276,
        "MMLU_high_school_computer_science":0.26,
        "MMLU_high_school_european_history":0.3212121212,
        "MMLU_high_school_geography":0.303030303,
        "MMLU_high_school_government_and_politics":0.2590673575,
        "MMLU_high_school_macroeconomics":0.2435897436,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.2647058824,
        "MMLU_high_school_physics":0.2185430464,
        "MMLU_high_school_psychology":0.271559633,
        "MMLU_high_school_statistics":0.2175925926,
        "MMLU_high_school_us_history":0.318627451,
        "MMLU_high_school_world_history":0.3417721519,
        "MMLU_human_aging":0.4035874439,
        "MMLU_human_sexuality":0.2442748092,
        "MMLU_international_law":0.3719008264,
        "MMLU_jurisprudence":0.3240740741,
        "MMLU_logical_fallacies":0.3190184049,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.3495145631,
        "MMLU_marketing":0.358974359,
        "MMLU_medical_genetics":0.24,
        "MMLU_miscellaneous":0.337164751,
        "MMLU_moral_disputes":0.3179190751,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.3006535948,
        "MMLU_philosophy":0.3665594855,
        "MMLU_prehistory":0.2839506173,
        "MMLU_professional_accounting":0.2765957447,
        "MMLU_professional_law":0.2724902216,
        "MMLU_professional_medicine":0.2132352941,
        "MMLU_professional_psychology":0.2941176471,
        "MMLU_public_relations":0.3,
        "MMLU_security_studies":0.2326530612,
        "MMLU_sociology":0.3034825871,
        "MMLU_us_foreign_policy":0.32,
        "MMLU_virology":0.3373493976,
        "MMLU_world_religions":0.2456140351
    },
    {
        "Model":"gpt4all-j",
        "URL":"https:\/\/huggingface.co\/nomic-ai\/gpt4all-j",
        "full_model_name":"nomic-ai\/gpt4all-j",
        "Parameters":null,
        "MMLU_average":0.2819738937,
        "arc:challenge|25":0.3899317406,
        "hellaswag|10":0.4834694284,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.3555555556,
        "MMLU_astronomy":0.2894736842,
        "MMLU_business_ethics":0.26,
        "MMLU_clinical_knowledge":0.3245283019,
        "MMLU_college_biology":0.2986111111,
        "MMLU_college_chemistry":0.27,
        "MMLU_college_computer_science":0.19,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.2427745665,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.25,
        "MMLU_conceptual_physics":0.2510638298,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.3517241379,
        "MMLU_elementary_mathematics":0.2116402116,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.264516129,
        "MMLU_high_school_chemistry":0.2906403941,
        "MMLU_high_school_computer_science":0.19,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.3383838384,
        "MMLU_high_school_government_and_politics":0.2694300518,
        "MMLU_high_school_macroeconomics":0.3205128205,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.3067226891,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.2348623853,
        "MMLU_high_school_statistics":0.4444444444,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2784810127,
        "MMLU_human_aging":0.2511210762,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.3388429752,
        "MMLU_jurisprudence":0.2037037037,
        "MMLU_logical_fallacies":0.3006134969,
        "MMLU_machine_learning":0.2142857143,
        "MMLU_management":0.2427184466,
        "MMLU_marketing":0.2863247863,
        "MMLU_medical_genetics":0.37,
        "MMLU_miscellaneous":0.2592592593,
        "MMLU_moral_disputes":0.2398843931,
        "MMLU_moral_scenarios":0.2726256983,
        "MMLU_nutrition":0.3366013072,
        "MMLU_philosophy":0.3247588424,
        "MMLU_prehistory":0.3055555556,
        "MMLU_professional_accounting":0.2872340426,
        "MMLU_professional_law":0.2516297262,
        "MMLU_professional_medicine":0.3970588235,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2,
        "MMLU_security_studies":0.3673469388,
        "MMLU_sociology":0.3482587065,
        "MMLU_us_foreign_policy":0.33,
        "MMLU_virology":0.2289156627,
        "MMLU_world_religions":0.3099415205
    },
    {
        "Model":"falcon_7b_norobots",
        "URL":"https:\/\/huggingface.co\/qblocks\/falcon_7b_norobots",
        "full_model_name":"qblocks\/falcon_7b_norobots",
        "Parameters":7.0,
        "MMLU_average":0.2811199158,
        "arc:challenge|25":0.4266211604,
        "hellaswag|10":0.5769766979,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.2740740741,
        "MMLU_astronomy":0.2171052632,
        "MMLU_business_ethics":0.24,
        "MMLU_clinical_knowledge":0.2905660377,
        "MMLU_college_biology":0.1944444444,
        "MMLU_college_chemistry":0.22,
        "MMLU_college_computer_science":0.28,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.2312138728,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.26,
        "MMLU_conceptual_physics":0.3106382979,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2965517241,
        "MMLU_elementary_mathematics":0.2724867725,
        "MMLU_formal_logic":0.1984126984,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.2741935484,
        "MMLU_high_school_chemistry":0.2807881773,
        "MMLU_high_school_computer_science":0.36,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.2474747475,
        "MMLU_high_school_government_and_politics":0.2435233161,
        "MMLU_high_school_macroeconomics":0.2487179487,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.2857142857,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.2752293578,
        "MMLU_high_school_statistics":0.1574074074,
        "MMLU_high_school_us_history":0.2696078431,
        "MMLU_high_school_world_history":0.2827004219,
        "MMLU_human_aging":0.3811659193,
        "MMLU_human_sexuality":0.2824427481,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.3240740741,
        "MMLU_logical_fallacies":0.2576687117,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.2621359223,
        "MMLU_marketing":0.2777777778,
        "MMLU_medical_genetics":0.27,
        "MMLU_miscellaneous":0.3039591315,
        "MMLU_moral_disputes":0.2832369942,
        "MMLU_moral_scenarios":0.2413407821,
        "MMLU_nutrition":0.2712418301,
        "MMLU_philosophy":0.2926045016,
        "MMLU_prehistory":0.3086419753,
        "MMLU_professional_accounting":0.2836879433,
        "MMLU_professional_law":0.2470664928,
        "MMLU_professional_medicine":0.3970588235,
        "MMLU_professional_psychology":0.2663398693,
        "MMLU_public_relations":0.2909090909,
        "MMLU_security_studies":0.3510204082,
        "MMLU_sociology":0.328358209,
        "MMLU_us_foreign_policy":0.41,
        "MMLU_virology":0.3554216867,
        "MMLU_world_religions":0.4093567251
    },
    {
        "Model":"mpt-7b",
        "URL":"https:\/\/huggingface.co\/mosaicml\/mpt-7b",
        "full_model_name":"mosaicml\/mpt-7b",
        "Parameters":7.0,
        "MMLU_average":0.2806843136,
        "arc:challenge|25":0.4291808874,
        "hellaswag|10":0.5730930094,
        "MMLU_abstract_algebra":0.19,
        "MMLU_anatomy":0.2222222222,
        "MMLU_astronomy":0.2631578947,
        "MMLU_business_ethics":0.33,
        "MMLU_clinical_knowledge":0.2867924528,
        "MMLU_college_biology":0.2847222222,
        "MMLU_college_chemistry":0.25,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.2658959538,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.29,
        "MMLU_conceptual_physics":0.3404255319,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.2380952381,
        "MMLU_formal_logic":0.2301587302,
        "MMLU_global_facts":0.23,
        "MMLU_high_school_biology":0.2516129032,
        "MMLU_high_school_chemistry":0.2068965517,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.2222222222,
        "MMLU_high_school_government_and_politics":0.3575129534,
        "MMLU_high_school_macroeconomics":0.3205128205,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.2899159664,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.2605504587,
        "MMLU_high_school_statistics":0.3055555556,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3094170404,
        "MMLU_human_sexuality":0.2977099237,
        "MMLU_international_law":0.2975206612,
        "MMLU_jurisprudence":0.2962962963,
        "MMLU_logical_fallacies":0.2515337423,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.2330097087,
        "MMLU_marketing":0.3205128205,
        "MMLU_medical_genetics":0.35,
        "MMLU_miscellaneous":0.3001277139,
        "MMLU_moral_disputes":0.2630057803,
        "MMLU_moral_scenarios":0.2446927374,
        "MMLU_nutrition":0.2810457516,
        "MMLU_philosophy":0.2990353698,
        "MMLU_prehistory":0.3209876543,
        "MMLU_professional_accounting":0.2482269504,
        "MMLU_professional_law":0.260756193,
        "MMLU_professional_medicine":0.1948529412,
        "MMLU_professional_psychology":0.2598039216,
        "MMLU_public_relations":0.3363636364,
        "MMLU_security_studies":0.3020408163,
        "MMLU_sociology":0.2338308458,
        "MMLU_us_foreign_policy":0.4,
        "MMLU_virology":0.3493975904,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"OmegLLaMA-3B",
        "URL":"https:\/\/huggingface.co\/acrastt\/OmegLLaMA-3B",
        "full_model_name":"acrastt\/OmegLLaMA-3B",
        "Parameters":3.0,
        "MMLU_average":0.2799892471,
        "arc:challenge|25":0.3583617747,
        "hellaswag|10":0.4927305318,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.3259259259,
        "MMLU_astronomy":0.25,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.2716981132,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.26,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.2947976879,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.29,
        "MMLU_conceptual_physics":0.2340425532,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.2896551724,
        "MMLU_elementary_mathematics":0.2619047619,
        "MMLU_formal_logic":0.1666666667,
        "MMLU_global_facts":0.28,
        "MMLU_high_school_biology":0.2580645161,
        "MMLU_high_school_chemistry":0.3054187192,
        "MMLU_high_school_computer_science":0.31,
        "MMLU_high_school_european_history":0.2909090909,
        "MMLU_high_school_geography":0.3535353535,
        "MMLU_high_school_government_and_politics":0.3160621762,
        "MMLU_high_school_macroeconomics":0.3153846154,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.3321100917,
        "MMLU_high_school_statistics":0.4212962963,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.2953586498,
        "MMLU_human_aging":0.201793722,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.3966942149,
        "MMLU_jurisprudence":0.287037037,
        "MMLU_logical_fallacies":0.282208589,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.2718446602,
        "MMLU_marketing":0.2692307692,
        "MMLU_medical_genetics":0.27,
        "MMLU_miscellaneous":0.2899106003,
        "MMLU_moral_disputes":0.3092485549,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.2581699346,
        "MMLU_philosophy":0.3151125402,
        "MMLU_prehistory":0.2407407407,
        "MMLU_professional_accounting":0.2588652482,
        "MMLU_professional_law":0.2398956975,
        "MMLU_professional_medicine":0.4448529412,
        "MMLU_professional_psychology":0.2696078431,
        "MMLU_public_relations":0.2272727273,
        "MMLU_security_studies":0.2530612245,
        "MMLU_sociology":0.2537313433,
        "MMLU_us_foreign_policy":0.25,
        "MMLU_virology":0.2891566265,
        "MMLU_world_religions":0.2748538012
    },
    {
        "Model":"Phind-CodeLlama-34B-Python-v1",
        "URL":"https:\/\/huggingface.co\/Phind\/Phind-CodeLlama-34B-Python-v1",
        "full_model_name":"Phind\/Phind-CodeLlama-34B-Python-v1",
        "Parameters":34.0,
        "MMLU_average":0.2795357708,
        "arc:challenge|25":0.2039249147,
        "hellaswag|10":0.2733519219,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.2888888889,
        "MMLU_astronomy":0.3289473684,
        "MMLU_business_ethics":0.22,
        "MMLU_clinical_knowledge":0.3169811321,
        "MMLU_college_biology":0.2777777778,
        "MMLU_college_chemistry":0.36,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.3005780347,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.19,
        "MMLU_conceptual_physics":0.2723404255,
        "MMLU_econometrics":0.1842105263,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.2328042328,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.17,
        "MMLU_high_school_biology":0.3129032258,
        "MMLU_high_school_chemistry":0.2709359606,
        "MMLU_high_school_computer_science":0.23,
        "MMLU_high_school_european_history":0.2606060606,
        "MMLU_high_school_geography":0.3333333333,
        "MMLU_high_school_government_and_politics":0.3160621762,
        "MMLU_high_school_macroeconomics":0.3666666667,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.2731092437,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.3027522936,
        "MMLU_high_school_statistics":0.4398148148,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2869198312,
        "MMLU_human_aging":0.2780269058,
        "MMLU_human_sexuality":0.2824427481,
        "MMLU_international_law":0.2561983471,
        "MMLU_jurisprudence":0.2407407407,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.2142857143,
        "MMLU_management":0.3495145631,
        "MMLU_marketing":0.2051282051,
        "MMLU_medical_genetics":0.29,
        "MMLU_miscellaneous":0.3295019157,
        "MMLU_moral_disputes":0.2514450867,
        "MMLU_moral_scenarios":0.2726256983,
        "MMLU_nutrition":0.2581699346,
        "MMLU_philosophy":0.2765273312,
        "MMLU_prehistory":0.2561728395,
        "MMLU_professional_accounting":0.2482269504,
        "MMLU_professional_law":0.2425032595,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2450980392,
        "MMLU_public_relations":0.2272727273,
        "MMLU_security_studies":0.3795918367,
        "MMLU_sociology":0.2338308458,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.2108433735,
        "MMLU_world_religions":0.3157894737
    },
    {
        "Model":"xglm-7.5B",
        "URL":"https:\/\/huggingface.co\/facebook\/xglm-7.5B",
        "full_model_name":"facebook\/xglm-7.5B",
        "Parameters":7.5,
        "MMLU_average":0.2778866807,
        "arc:challenge|25":0.3071672355,
        "hellaswag|10":0.4496116311,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.2518518519,
        "MMLU_astronomy":0.2236842105,
        "MMLU_business_ethics":0.26,
        "MMLU_clinical_knowledge":0.3018867925,
        "MMLU_college_biology":0.2986111111,
        "MMLU_college_chemistry":0.27,
        "MMLU_college_computer_science":0.28,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.2543352601,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2893617021,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2068965517,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.29,
        "MMLU_high_school_biology":0.3322580645,
        "MMLU_high_school_chemistry":0.3103448276,
        "MMLU_high_school_computer_science":0.28,
        "MMLU_high_school_european_history":0.2787878788,
        "MMLU_high_school_geography":0.3585858586,
        "MMLU_high_school_government_and_politics":0.3626943005,
        "MMLU_high_school_macroeconomics":0.3564102564,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.3319327731,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.2770642202,
        "MMLU_high_school_statistics":0.4212962963,
        "MMLU_high_school_us_history":0.2352941176,
        "MMLU_high_school_world_history":0.2573839662,
        "MMLU_human_aging":0.3452914798,
        "MMLU_human_sexuality":0.2366412214,
        "MMLU_international_law":0.3636363636,
        "MMLU_jurisprudence":0.212962963,
        "MMLU_logical_fallacies":0.2576687117,
        "MMLU_machine_learning":0.2232142857,
        "MMLU_management":0.2621359223,
        "MMLU_marketing":0.1794871795,
        "MMLU_medical_genetics":0.31,
        "MMLU_miscellaneous":0.2554278416,
        "MMLU_moral_disputes":0.2312138728,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2777777778,
        "MMLU_philosophy":0.2443729904,
        "MMLU_prehistory":0.274691358,
        "MMLU_professional_accounting":0.2624113475,
        "MMLU_professional_law":0.2444589309,
        "MMLU_professional_medicine":0.4522058824,
        "MMLU_professional_psychology":0.2696078431,
        "MMLU_public_relations":0.2454545455,
        "MMLU_security_studies":0.2897959184,
        "MMLU_sociology":0.2388059701,
        "MMLU_us_foreign_policy":0.24,
        "MMLU_virology":0.2891566265,
        "MMLU_world_religions":0.2807017544
    },
    {
        "Model":"testC",
        "URL":"https:\/\/huggingface.co\/Devio\/testC",
        "full_model_name":"Devio\/testC",
        "Parameters":null,
        "MMLU_average":0.2775710674,
        "arc:challenge|25":0.3549488055,
        "hellaswag|10":0.4529974109,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.2296296296,
        "MMLU_astronomy":0.3289473684,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.2981132075,
        "MMLU_college_biology":0.2638888889,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.2716763006,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.16,
        "MMLU_conceptual_physics":0.2893617021,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2827586207,
        "MMLU_elementary_mathematics":0.2380952381,
        "MMLU_formal_logic":0.3888888889,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.3258064516,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.18,
        "MMLU_high_school_european_history":0.2303030303,
        "MMLU_high_school_geography":0.3585858586,
        "MMLU_high_school_government_and_politics":0.3678756477,
        "MMLU_high_school_macroeconomics":0.3461538462,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.3361344538,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.3431192661,
        "MMLU_high_school_statistics":0.4351851852,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2067510549,
        "MMLU_human_aging":0.1569506726,
        "MMLU_human_sexuality":0.2824427481,
        "MMLU_international_law":0.1404958678,
        "MMLU_jurisprudence":0.2314814815,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.2053571429,
        "MMLU_management":0.4174757282,
        "MMLU_marketing":0.188034188,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.2107279693,
        "MMLU_moral_disputes":0.2283236994,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.3006535948,
        "MMLU_philosophy":0.2668810289,
        "MMLU_prehistory":0.2530864198,
        "MMLU_professional_accounting":0.2624113475,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2205882353,
        "MMLU_public_relations":0.2727272727,
        "MMLU_security_studies":0.3959183673,
        "MMLU_sociology":0.263681592,
        "MMLU_us_foreign_policy":0.35,
        "MMLU_virology":0.2108433735,
        "MMLU_world_religions":0.1461988304
    },
    {
        "Model":"GPT-J-6B-Shinen",
        "URL":"https:\/\/huggingface.co\/KoboldAI\/GPT-J-6B-Shinen",
        "full_model_name":"KoboldAI\/GPT-J-6B-Shinen",
        "Parameters":6.0,
        "MMLU_average":0.2771628522,
        "arc:challenge|25":0.3558020478,
        "hellaswag|10":0.4937263493,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.2888888889,
        "MMLU_astronomy":0.2894736842,
        "MMLU_business_ethics":0.28,
        "MMLU_clinical_knowledge":0.3132075472,
        "MMLU_college_biology":0.2361111111,
        "MMLU_college_chemistry":0.19,
        "MMLU_college_computer_science":0.22,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.2716763006,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.32,
        "MMLU_conceptual_physics":0.3276595745,
        "MMLU_econometrics":0.3333333333,
        "MMLU_electrical_engineering":0.2896551724,
        "MMLU_elementary_mathematics":0.2724867725,
        "MMLU_formal_logic":0.2380952381,
        "MMLU_global_facts":0.29,
        "MMLU_high_school_biology":0.2709677419,
        "MMLU_high_school_chemistry":0.2610837438,
        "MMLU_high_school_computer_science":0.27,
        "MMLU_high_school_european_history":0.2848484848,
        "MMLU_high_school_geography":0.2525252525,
        "MMLU_high_school_government_and_politics":0.2538860104,
        "MMLU_high_school_macroeconomics":0.2743589744,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.2394957983,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.2440366972,
        "MMLU_high_school_statistics":0.1805555556,
        "MMLU_high_school_us_history":0.3088235294,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3094170404,
        "MMLU_human_sexuality":0.2061068702,
        "MMLU_international_law":0.3966942149,
        "MMLU_jurisprudence":0.3055555556,
        "MMLU_logical_fallacies":0.2515337423,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.1650485437,
        "MMLU_marketing":0.2820512821,
        "MMLU_medical_genetics":0.31,
        "MMLU_miscellaneous":0.3116219668,
        "MMLU_moral_disputes":0.3063583815,
        "MMLU_moral_scenarios":0.2413407821,
        "MMLU_nutrition":0.2941176471,
        "MMLU_philosophy":0.2829581994,
        "MMLU_prehistory":0.2962962963,
        "MMLU_professional_accounting":0.2836879433,
        "MMLU_professional_law":0.2953063885,
        "MMLU_professional_medicine":0.25,
        "MMLU_professional_psychology":0.2761437908,
        "MMLU_public_relations":0.3272727273,
        "MMLU_security_studies":0.4326530612,
        "MMLU_sociology":0.328358209,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.3012048193,
        "MMLU_world_religions":0.298245614
    },
    {
        "Model":"RedPajama-INCITE-Base-7B-v0.1",
        "URL":"https:\/\/huggingface.co\/togethercomputer\/RedPajama-INCITE-Base-7B-v0.1",
        "full_model_name":"togethercomputer\/RedPajama-INCITE-Base-7B-v0.1",
        "Parameters":7.0,
        "MMLU_average":0.2768214019,
        "arc:challenge|25":0.4223549488,
        "hellaswag|10":0.5317665804,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.2444444444,
        "MMLU_astronomy":0.2960526316,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.3358490566,
        "MMLU_college_biology":0.2916666667,
        "MMLU_college_chemistry":0.32,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.3063583815,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.36,
        "MMLU_conceptual_physics":0.285106383,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2551724138,
        "MMLU_elementary_mathematics":0.2645502646,
        "MMLU_formal_logic":0.380952381,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.2709677419,
        "MMLU_high_school_chemistry":0.275862069,
        "MMLU_high_school_computer_science":0.29,
        "MMLU_high_school_european_history":0.2545454545,
        "MMLU_high_school_geography":0.3737373737,
        "MMLU_high_school_government_and_politics":0.2383419689,
        "MMLU_high_school_macroeconomics":0.2923076923,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.2941176471,
        "MMLU_high_school_physics":0.238410596,
        "MMLU_high_school_psychology":0.3266055046,
        "MMLU_high_school_statistics":0.2916666667,
        "MMLU_high_school_us_history":0.2352941176,
        "MMLU_high_school_world_history":0.2784810127,
        "MMLU_human_aging":0.1479820628,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2644628099,
        "MMLU_jurisprudence":0.2407407407,
        "MMLU_logical_fallacies":0.2085889571,
        "MMLU_machine_learning":0.1964285714,
        "MMLU_management":0.3009708738,
        "MMLU_marketing":0.3034188034,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.275862069,
        "MMLU_moral_disputes":0.2716763006,
        "MMLU_moral_scenarios":0.243575419,
        "MMLU_nutrition":0.2810457516,
        "MMLU_philosophy":0.2797427653,
        "MMLU_prehistory":0.3240740741,
        "MMLU_professional_accounting":0.2907801418,
        "MMLU_professional_law":0.2561929596,
        "MMLU_professional_medicine":0.2022058824,
        "MMLU_professional_psychology":0.2973856209,
        "MMLU_public_relations":0.3454545455,
        "MMLU_security_studies":0.2897959184,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.29,
        "MMLU_virology":0.2168674699,
        "MMLU_world_religions":0.3040935673
    },
    {
        "Model":"bertin-gpt-j-6B-alpaca",
        "URL":"https:\/\/huggingface.co\/bertin-project\/bertin-gpt-j-6B-alpaca",
        "full_model_name":"bertin-project\/bertin-gpt-j-6B-alpaca",
        "Parameters":6.0,
        "MMLU_average":0.2765984724,
        "arc:challenge|25":0.3336177474,
        "hellaswag|10":0.4201354312,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.2148148148,
        "MMLU_astronomy":0.3157894737,
        "MMLU_business_ethics":0.24,
        "MMLU_clinical_knowledge":0.3283018868,
        "MMLU_college_biology":0.2708333333,
        "MMLU_college_chemistry":0.32,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.3352601156,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.27,
        "MMLU_conceptual_physics":0.2595744681,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.275862069,
        "MMLU_elementary_mathematics":0.246031746,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.2419354839,
        "MMLU_high_school_chemistry":0.2610837438,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.3434343434,
        "MMLU_high_school_government_and_politics":0.3471502591,
        "MMLU_high_school_macroeconomics":0.3179487179,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.281512605,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.3211009174,
        "MMLU_high_school_statistics":0.3981481481,
        "MMLU_high_school_us_history":0.2843137255,
        "MMLU_high_school_world_history":0.2362869198,
        "MMLU_human_aging":0.2421524664,
        "MMLU_human_sexuality":0.2061068702,
        "MMLU_international_law":0.2148760331,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.263803681,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.2815533981,
        "MMLU_marketing":0.2136752137,
        "MMLU_medical_genetics":0.38,
        "MMLU_miscellaneous":0.2375478927,
        "MMLU_moral_disputes":0.2543352601,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2712418301,
        "MMLU_philosophy":0.2540192926,
        "MMLU_prehistory":0.2438271605,
        "MMLU_professional_accounting":0.2730496454,
        "MMLU_professional_law":0.2522816167,
        "MMLU_professional_medicine":0.4007352941,
        "MMLU_professional_psychology":0.2434640523,
        "MMLU_public_relations":0.3,
        "MMLU_security_studies":0.3918367347,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.2771084337,
        "MMLU_world_religions":0.2280701754
    },
    {
        "Model":"open_llama_7b_400bt_preview",
        "URL":"https:\/\/huggingface.co\/klosax\/open_llama_7b_400bt_preview",
        "full_model_name":"klosax\/open_llama_7b_400bt_preview",
        "Parameters":7.0,
        "MMLU_average":0.2764443297,
        "arc:challenge|25":0.361774744,
        "hellaswag|10":0.4939255128,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.3111111111,
        "MMLU_astronomy":0.2828947368,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.2981132075,
        "MMLU_college_biology":0.2638888889,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.2543352601,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.3,
        "MMLU_conceptual_physics":0.1957446809,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2965517241,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.1666666667,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.2516129032,
        "MMLU_high_school_chemistry":0.2807881773,
        "MMLU_high_school_computer_science":0.31,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.3434343434,
        "MMLU_high_school_government_and_politics":0.3678756477,
        "MMLU_high_school_macroeconomics":0.358974359,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.3486238532,
        "MMLU_high_school_statistics":0.4398148148,
        "MMLU_high_school_us_history":0.2794117647,
        "MMLU_high_school_world_history":0.253164557,
        "MMLU_human_aging":0.1390134529,
        "MMLU_human_sexuality":0.2824427481,
        "MMLU_international_law":0.3801652893,
        "MMLU_jurisprudence":0.212962963,
        "MMLU_logical_fallacies":0.3006134969,
        "MMLU_machine_learning":0.1607142857,
        "MMLU_management":0.3786407767,
        "MMLU_marketing":0.2564102564,
        "MMLU_medical_genetics":0.2,
        "MMLU_miscellaneous":0.2043422733,
        "MMLU_moral_disputes":0.2919075145,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.2549019608,
        "MMLU_philosophy":0.2797427653,
        "MMLU_prehistory":0.225308642,
        "MMLU_professional_accounting":0.2695035461,
        "MMLU_professional_law":0.2659713168,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2254901961,
        "MMLU_public_relations":0.2090909091,
        "MMLU_security_studies":0.4,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.1927710843,
        "MMLU_world_religions":0.2923976608
    },
    {
        "Model":"Dolly_Shygmalion-6b",
        "URL":"https:\/\/huggingface.co\/TehVenom\/Dolly_Shygmalion-6b",
        "full_model_name":"TehVenom\/Dolly_Shygmalion-6b",
        "Parameters":6.0,
        "MMLU_average":0.2758133434,
        "arc:challenge|25":0.385665529,
        "hellaswag|10":0.5033857797,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.2740740741,
        "MMLU_astronomy":0.3026315789,
        "MMLU_business_ethics":0.31,
        "MMLU_clinical_knowledge":0.2867924528,
        "MMLU_college_biology":0.2361111111,
        "MMLU_college_chemistry":0.17,
        "MMLU_college_computer_science":0.19,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.2658959538,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.33,
        "MMLU_conceptual_physics":0.3319148936,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.2689655172,
        "MMLU_elementary_mathematics":0.2830687831,
        "MMLU_formal_logic":0.2301587302,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.2677419355,
        "MMLU_high_school_chemistry":0.2610837438,
        "MMLU_high_school_computer_science":0.23,
        "MMLU_high_school_european_history":0.2909090909,
        "MMLU_high_school_geography":0.202020202,
        "MMLU_high_school_government_and_politics":0.2487046632,
        "MMLU_high_school_macroeconomics":0.2743589744,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.243697479,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.2513761468,
        "MMLU_high_school_statistics":0.1666666667,
        "MMLU_high_school_us_history":0.2892156863,
        "MMLU_high_school_world_history":0.2869198312,
        "MMLU_human_aging":0.3632286996,
        "MMLU_human_sexuality":0.2061068702,
        "MMLU_international_law":0.3636363636,
        "MMLU_jurisprudence":0.3425925926,
        "MMLU_logical_fallacies":0.2331288344,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.145631068,
        "MMLU_marketing":0.2991452991,
        "MMLU_medical_genetics":0.29,
        "MMLU_miscellaneous":0.3141762452,
        "MMLU_moral_disputes":0.3005780347,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.3104575163,
        "MMLU_philosophy":0.2829581994,
        "MMLU_prehistory":0.3117283951,
        "MMLU_professional_accounting":0.280141844,
        "MMLU_professional_law":0.2940026076,
        "MMLU_professional_medicine":0.1801470588,
        "MMLU_professional_psychology":0.2826797386,
        "MMLU_public_relations":0.3181818182,
        "MMLU_security_studies":0.4367346939,
        "MMLU_sociology":0.3084577114,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2951807229,
        "MMLU_world_religions":0.2923976608
    },
    {
        "Model":"PPO_Shygmalion-6b",
        "URL":"https:\/\/huggingface.co\/TehVenom\/PPO_Shygmalion-6b",
        "full_model_name":"TehVenom\/PPO_Shygmalion-6b",
        "Parameters":6.0,
        "MMLU_average":0.2753069069,
        "arc:challenge|25":0.3626279863,
        "hellaswag|10":0.4961163115,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.2740740741,
        "MMLU_astronomy":0.2960526316,
        "MMLU_business_ethics":0.28,
        "MMLU_clinical_knowledge":0.2943396226,
        "MMLU_college_biology":0.25,
        "MMLU_college_chemistry":0.17,
        "MMLU_college_computer_science":0.23,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.2427745665,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.31,
        "MMLU_conceptual_physics":0.3276595745,
        "MMLU_econometrics":0.3421052632,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2619047619,
        "MMLU_formal_logic":0.1825396825,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.2709677419,
        "MMLU_high_school_chemistry":0.2709359606,
        "MMLU_high_school_computer_science":0.19,
        "MMLU_high_school_european_history":0.296969697,
        "MMLU_high_school_geography":0.2373737374,
        "MMLU_high_school_government_and_politics":0.2279792746,
        "MMLU_high_school_macroeconomics":0.2923076923,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.2521008403,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.2495412844,
        "MMLU_high_school_statistics":0.1527777778,
        "MMLU_high_school_us_history":0.2892156863,
        "MMLU_high_school_world_history":0.2742616034,
        "MMLU_human_aging":0.33632287,
        "MMLU_human_sexuality":0.2519083969,
        "MMLU_international_law":0.4132231405,
        "MMLU_jurisprudence":0.2777777778,
        "MMLU_logical_fallacies":0.245398773,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.2330097087,
        "MMLU_marketing":0.2948717949,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.2950191571,
        "MMLU_moral_disputes":0.289017341,
        "MMLU_moral_scenarios":0.243575419,
        "MMLU_nutrition":0.2908496732,
        "MMLU_philosophy":0.308681672,
        "MMLU_prehistory":0.299382716,
        "MMLU_professional_accounting":0.2695035461,
        "MMLU_professional_law":0.2953063885,
        "MMLU_professional_medicine":0.2095588235,
        "MMLU_professional_psychology":0.2761437908,
        "MMLU_public_relations":0.3272727273,
        "MMLU_security_studies":0.4367346939,
        "MMLU_sociology":0.3482587065,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.3012048193,
        "MMLU_world_religions":0.2923976608
    },
    {
        "Model":"nb-gpt-j-6B-alpaca",
        "URL":"https:\/\/huggingface.co\/NbAiLab\/nb-gpt-j-6B-alpaca",
        "full_model_name":"NbAiLab\/nb-gpt-j-6B-alpaca",
        "Parameters":6.0,
        "MMLU_average":0.2752648286,
        "arc:challenge|25":0.3447098976,
        "hellaswag|10":0.4460266879,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.2962962963,
        "MMLU_astronomy":0.1710526316,
        "MMLU_business_ethics":0.26,
        "MMLU_clinical_knowledge":0.2377358491,
        "MMLU_college_biology":0.3472222222,
        "MMLU_college_chemistry":0.22,
        "MMLU_college_computer_science":0.35,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.2658959538,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.22,
        "MMLU_conceptual_physics":0.314893617,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.2482758621,
        "MMLU_elementary_mathematics":0.253968254,
        "MMLU_formal_logic":0.2222222222,
        "MMLU_global_facts":0.22,
        "MMLU_high_school_biology":0.235483871,
        "MMLU_high_school_chemistry":0.2709359606,
        "MMLU_high_school_computer_science":0.27,
        "MMLU_high_school_european_history":0.2606060606,
        "MMLU_high_school_geography":0.3585858586,
        "MMLU_high_school_government_and_politics":0.3575129534,
        "MMLU_high_school_macroeconomics":0.2384615385,
        "MMLU_high_school_mathematics":0.2259259259,
        "MMLU_high_school_microeconomics":0.1932773109,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.3266055046,
        "MMLU_high_school_statistics":0.462962963,
        "MMLU_high_school_us_history":0.2205882353,
        "MMLU_high_school_world_history":0.2573839662,
        "MMLU_human_aging":0.2600896861,
        "MMLU_human_sexuality":0.2061068702,
        "MMLU_international_law":0.2975206612,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2944785276,
        "MMLU_machine_learning":0.1696428571,
        "MMLU_management":0.2038834951,
        "MMLU_marketing":0.2179487179,
        "MMLU_medical_genetics":0.4,
        "MMLU_miscellaneous":0.3052362708,
        "MMLU_moral_disputes":0.2861271676,
        "MMLU_moral_scenarios":0.2726256983,
        "MMLU_nutrition":0.3137254902,
        "MMLU_philosophy":0.2958199357,
        "MMLU_prehistory":0.2283950617,
        "MMLU_professional_accounting":0.2411347518,
        "MMLU_professional_law":0.2398956975,
        "MMLU_professional_medicine":0.4411764706,
        "MMLU_professional_psychology":0.2745098039,
        "MMLU_public_relations":0.2909090909,
        "MMLU_security_studies":0.387755102,
        "MMLU_sociology":0.2139303483,
        "MMLU_us_foreign_policy":0.3,
        "MMLU_virology":0.2771084337,
        "MMLU_world_religions":0.2573099415
    },
    {
        "Model":"shearedplats-2.7b-v2",
        "URL":"https:\/\/huggingface.co\/vihangd\/shearedplats-2.7b-v2",
        "full_model_name":"vihangd\/shearedplats-2.7b-v2",
        "Parameters":2.7,
        "MMLU_average":0.2751713971,
        "arc:challenge|25":0.3899317406,
        "hellaswag|10":0.5428201553,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.3407407407,
        "MMLU_astronomy":0.1973684211,
        "MMLU_business_ethics":0.4,
        "MMLU_clinical_knowledge":0.3094339623,
        "MMLU_college_biology":0.3055555556,
        "MMLU_college_chemistry":0.19,
        "MMLU_college_computer_science":0.27,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.1734104046,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.41,
        "MMLU_conceptual_physics":0.285106383,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.2827586207,
        "MMLU_elementary_mathematics":0.1984126984,
        "MMLU_formal_logic":0.2777777778,
        "MMLU_global_facts":0.26,
        "MMLU_high_school_biology":0.2838709677,
        "MMLU_high_school_chemistry":0.236453202,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.3393939394,
        "MMLU_high_school_geography":0.2525252525,
        "MMLU_high_school_government_and_politics":0.2849740933,
        "MMLU_high_school_macroeconomics":0.2435897436,
        "MMLU_high_school_mathematics":0.2037037037,
        "MMLU_high_school_microeconomics":0.218487395,
        "MMLU_high_school_physics":0.2450331126,
        "MMLU_high_school_psychology":0.2917431193,
        "MMLU_high_school_statistics":0.2361111111,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2742616034,
        "MMLU_human_aging":0.3721973094,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.3719008264,
        "MMLU_jurisprudence":0.3333333333,
        "MMLU_logical_fallacies":0.2699386503,
        "MMLU_machine_learning":0.2946428571,
        "MMLU_management":0.2330097087,
        "MMLU_marketing":0.2820512821,
        "MMLU_medical_genetics":0.33,
        "MMLU_miscellaneous":0.3448275862,
        "MMLU_moral_disputes":0.2774566474,
        "MMLU_moral_scenarios":0.243575419,
        "MMLU_nutrition":0.2843137255,
        "MMLU_philosophy":0.3022508039,
        "MMLU_prehistory":0.3117283951,
        "MMLU_professional_accounting":0.2304964539,
        "MMLU_professional_law":0.2653194263,
        "MMLU_professional_medicine":0.1801470588,
        "MMLU_professional_psychology":0.2794117647,
        "MMLU_public_relations":0.2909090909,
        "MMLU_security_studies":0.1959183673,
        "MMLU_sociology":0.2587064677,
        "MMLU_us_foreign_policy":0.36,
        "MMLU_virology":0.3253012048,
        "MMLU_world_religions":0.3684210526
    },
    {
        "Model":"Puma-3B",
        "URL":"https:\/\/huggingface.co\/acrastt\/Puma-3B",
        "full_model_name":"acrastt\/Puma-3B",
        "Parameters":3.0,
        "MMLU_average":0.2750709055,
        "arc:challenge|25":0.3651877133,
        "hellaswag|10":0.5407289385,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.237037037,
        "MMLU_astronomy":0.2697368421,
        "MMLU_business_ethics":0.36,
        "MMLU_clinical_knowledge":0.2641509434,
        "MMLU_college_biology":0.2638888889,
        "MMLU_college_chemistry":0.19,
        "MMLU_college_computer_science":0.25,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.2138728324,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.33,
        "MMLU_conceptual_physics":0.3361702128,
        "MMLU_econometrics":0.2192982456,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.2777777778,
        "MMLU_formal_logic":0.2063492063,
        "MMLU_global_facts":0.29,
        "MMLU_high_school_biology":0.2290322581,
        "MMLU_high_school_chemistry":0.2512315271,
        "MMLU_high_school_computer_science":0.29,
        "MMLU_high_school_european_history":0.303030303,
        "MMLU_high_school_geography":0.2525252525,
        "MMLU_high_school_government_and_politics":0.2435233161,
        "MMLU_high_school_macroeconomics":0.258974359,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.2478991597,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.2642201835,
        "MMLU_high_school_statistics":0.2407407407,
        "MMLU_high_school_us_history":0.2303921569,
        "MMLU_high_school_world_history":0.2869198312,
        "MMLU_human_aging":0.4035874439,
        "MMLU_human_sexuality":0.2442748092,
        "MMLU_international_law":0.305785124,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.226993865,
        "MMLU_machine_learning":0.2321428571,
        "MMLU_management":0.2718446602,
        "MMLU_marketing":0.2777777778,
        "MMLU_medical_genetics":0.33,
        "MMLU_miscellaneous":0.2681992337,
        "MMLU_moral_disputes":0.3034682081,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.2745098039,
        "MMLU_philosophy":0.2926045016,
        "MMLU_prehistory":0.2901234568,
        "MMLU_professional_accounting":0.2765957447,
        "MMLU_professional_law":0.2398956975,
        "MMLU_professional_medicine":0.2205882353,
        "MMLU_professional_psychology":0.2859477124,
        "MMLU_public_relations":0.2636363636,
        "MMLU_security_studies":0.3673469388,
        "MMLU_sociology":0.2736318408,
        "MMLU_us_foreign_policy":0.37,
        "MMLU_virology":0.3192771084,
        "MMLU_world_religions":0.3157894737
    },
    {
        "Model":"test-4k-fn",
        "URL":"https:\/\/huggingface.co\/NoIdeaLand\/test-4k-fn",
        "full_model_name":"NoIdeaLand\/test-4k-fn",
        "Parameters":null,
        "MMLU_average":0.2744176001,
        "arc:challenge|25":0.3566552901,
        "hellaswag|10":0.4971121291,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.237037037,
        "MMLU_astronomy":0.1842105263,
        "MMLU_business_ethics":0.34,
        "MMLU_clinical_knowledge":0.2905660377,
        "MMLU_college_biology":0.2777777778,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.21,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.2543352601,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.32,
        "MMLU_conceptual_physics":0.3021276596,
        "MMLU_econometrics":0.2192982456,
        "MMLU_electrical_engineering":0.3103448276,
        "MMLU_elementary_mathematics":0.246031746,
        "MMLU_formal_logic":0.2142857143,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2483870968,
        "MMLU_high_school_chemistry":0.2019704433,
        "MMLU_high_school_computer_science":0.27,
        "MMLU_high_school_european_history":0.2727272727,
        "MMLU_high_school_geography":0.2323232323,
        "MMLU_high_school_government_and_politics":0.3471502591,
        "MMLU_high_school_macroeconomics":0.3102564103,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.256302521,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.2403669725,
        "MMLU_high_school_statistics":0.2685185185,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.3037974684,
        "MMLU_human_aging":0.3542600897,
        "MMLU_human_sexuality":0.2900763359,
        "MMLU_international_law":0.3553719008,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.2760736196,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.2233009709,
        "MMLU_marketing":0.3803418803,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.245210728,
        "MMLU_moral_disputes":0.2543352601,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.3235294118,
        "MMLU_philosophy":0.2508038585,
        "MMLU_prehistory":0.225308642,
        "MMLU_professional_accounting":0.2553191489,
        "MMLU_professional_law":0.2777053455,
        "MMLU_professional_medicine":0.1948529412,
        "MMLU_professional_psychology":0.2810457516,
        "MMLU_public_relations":0.3181818182,
        "MMLU_security_studies":0.2530612245,
        "MMLU_sociology":0.2786069652,
        "MMLU_us_foreign_policy":0.44,
        "MMLU_virology":0.2891566265,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"Pythia-Chat-Base-7B",
        "URL":"https:\/\/huggingface.co\/togethercomputer\/Pythia-Chat-Base-7B",
        "full_model_name":"togethercomputer\/Pythia-Chat-Base-7B",
        "Parameters":7.0,
        "MMLU_average":0.2744109883,
        "arc:challenge|25":0.3506825939,
        "hellaswag|10":0.5097590121,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.3407407407,
        "MMLU_astronomy":0.2960526316,
        "MMLU_business_ethics":0.31,
        "MMLU_clinical_knowledge":0.279245283,
        "MMLU_college_biology":0.2708333333,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.37,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.2543352601,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.29,
        "MMLU_conceptual_physics":0.2085106383,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.2689655172,
        "MMLU_elementary_mathematics":0.246031746,
        "MMLU_formal_logic":0.2698412698,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.2483870968,
        "MMLU_high_school_chemistry":0.236453202,
        "MMLU_high_school_computer_science":0.36,
        "MMLU_high_school_european_history":0.296969697,
        "MMLU_high_school_geography":0.2474747475,
        "MMLU_high_school_government_and_politics":0.2538860104,
        "MMLU_high_school_macroeconomics":0.2282051282,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.2605042017,
        "MMLU_high_school_physics":0.238410596,
        "MMLU_high_school_psychology":0.2422018349,
        "MMLU_high_school_statistics":0.212962963,
        "MMLU_high_school_us_history":0.2941176471,
        "MMLU_high_school_world_history":0.2067510549,
        "MMLU_human_aging":0.2197309417,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.3553719008,
        "MMLU_jurisprudence":0.3148148148,
        "MMLU_logical_fallacies":0.2883435583,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.2038834951,
        "MMLU_marketing":0.3162393162,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.2592592593,
        "MMLU_moral_disputes":0.2919075145,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2875816993,
        "MMLU_philosophy":0.3022508039,
        "MMLU_prehistory":0.274691358,
        "MMLU_professional_accounting":0.2730496454,
        "MMLU_professional_law":0.2724902216,
        "MMLU_professional_medicine":0.1617647059,
        "MMLU_professional_psychology":0.3022875817,
        "MMLU_public_relations":0.2363636364,
        "MMLU_security_studies":0.3224489796,
        "MMLU_sociology":0.2388059701,
        "MMLU_us_foreign_policy":0.31,
        "MMLU_virology":0.2590361446,
        "MMLU_world_religions":0.298245614
    },
    {
        "Model":"pythia-12b-sft-v8-2.5k-steps",
        "URL":"https:\/\/huggingface.co\/OpenAssistant\/pythia-12b-sft-v8-2.5k-steps",
        "full_model_name":"OpenAssistant\/pythia-12b-sft-v8-2.5k-steps",
        "Parameters":12.0,
        "MMLU_average":0.2735796593,
        "arc:challenge|25":0.3933447099,
        "hellaswag|10":0.5172276439,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.1777777778,
        "MMLU_astronomy":0.2302631579,
        "MMLU_business_ethics":0.25,
        "MMLU_clinical_knowledge":0.3132075472,
        "MMLU_college_biology":0.2638888889,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.22,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.2312138728,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.32,
        "MMLU_conceptual_physics":0.3489361702,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.2344827586,
        "MMLU_elementary_mathematics":0.2380952381,
        "MMLU_formal_logic":0.2380952381,
        "MMLU_global_facts":0.29,
        "MMLU_high_school_biology":0.2870967742,
        "MMLU_high_school_chemistry":0.2512315271,
        "MMLU_high_school_computer_science":0.23,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.3080808081,
        "MMLU_high_school_government_and_politics":0.207253886,
        "MMLU_high_school_macroeconomics":0.2564102564,
        "MMLU_high_school_mathematics":0.2888888889,
        "MMLU_high_school_microeconomics":0.2394957983,
        "MMLU_high_school_physics":0.2317880795,
        "MMLU_high_school_psychology":0.2532110092,
        "MMLU_high_school_statistics":0.3240740741,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3497757848,
        "MMLU_human_sexuality":0.2824427481,
        "MMLU_international_law":0.2644628099,
        "MMLU_jurisprudence":0.3240740741,
        "MMLU_logical_fallacies":0.2699386503,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.2815533981,
        "MMLU_marketing":0.3034188034,
        "MMLU_medical_genetics":0.39,
        "MMLU_miscellaneous":0.3026819923,
        "MMLU_moral_disputes":0.2687861272,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2810457516,
        "MMLU_philosophy":0.270096463,
        "MMLU_prehistory":0.2777777778,
        "MMLU_professional_accounting":0.2304964539,
        "MMLU_professional_law":0.2398956975,
        "MMLU_professional_medicine":0.3897058824,
        "MMLU_professional_psychology":0.2581699346,
        "MMLU_public_relations":0.3181818182,
        "MMLU_security_studies":0.2653061224,
        "MMLU_sociology":0.2338308458,
        "MMLU_us_foreign_policy":0.3,
        "MMLU_virology":0.3012048193,
        "MMLU_world_religions":0.3040935673
    },
    {
        "Model":"emailgen-pythia-410m-deduped",
        "URL":"https:\/\/huggingface.co\/postbot\/emailgen-pythia-410m-deduped",
        "full_model_name":"postbot\/emailgen-pythia-410m-deduped",
        "Parameters":0.41,
        "MMLU_average":0.2735497836,
        "arc:challenge|25":0.2593856655,
        "hellaswag|10":0.3402708624,
        "MMLU_abstract_algebra":0.21,
        "MMLU_anatomy":0.2518518519,
        "MMLU_astronomy":0.2894736842,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.3358490566,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.25,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.2658959538,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.21,
        "MMLU_conceptual_physics":0.2978723404,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2206896552,
        "MMLU_elementary_mathematics":0.2513227513,
        "MMLU_formal_logic":0.3333333333,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2290322581,
        "MMLU_high_school_chemistry":0.275862069,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.3181818182,
        "MMLU_high_school_government_and_politics":0.3575129534,
        "MMLU_high_school_macroeconomics":0.3615384615,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.3445378151,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.3449541284,
        "MMLU_high_school_statistics":0.4166666667,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2109704641,
        "MMLU_human_aging":0.1255605381,
        "MMLU_human_sexuality":0.3664122137,
        "MMLU_international_law":0.2314049587,
        "MMLU_jurisprudence":0.2314814815,
        "MMLU_logical_fallacies":0.2576687117,
        "MMLU_machine_learning":0.1517857143,
        "MMLU_management":0.3689320388,
        "MMLU_marketing":0.2136752137,
        "MMLU_medical_genetics":0.27,
        "MMLU_miscellaneous":0.2298850575,
        "MMLU_moral_disputes":0.210982659,
        "MMLU_moral_scenarios":0.2715083799,
        "MMLU_nutrition":0.2647058824,
        "MMLU_philosophy":0.2347266881,
        "MMLU_prehistory":0.2345679012,
        "MMLU_professional_accounting":0.2411347518,
        "MMLU_professional_law":0.2509778357,
        "MMLU_professional_medicine":0.4227941176,
        "MMLU_professional_psychology":0.2418300654,
        "MMLU_public_relations":0.1909090909,
        "MMLU_security_studies":0.4,
        "MMLU_sociology":0.2537313433,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.2530120482,
        "MMLU_world_religions":0.2280701754
    },
    {
        "Model":"Aira-1B5",
        "URL":"https:\/\/huggingface.co\/nicholasKluge\/Aira-1B5",
        "full_model_name":"nicholasKluge\/Aira-1B5",
        "Parameters":1.0,
        "MMLU_average":0.2729189188,
        "arc:challenge|25":0.2687713311,
        "hellaswag|10":0.3618801036,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.2222222222,
        "MMLU_astronomy":0.2960526316,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.3018867925,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.36,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.3294797688,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.2,
        "MMLU_conceptual_physics":0.2085106383,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2645502646,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.2419354839,
        "MMLU_high_school_chemistry":0.2807881773,
        "MMLU_high_school_computer_science":0.22,
        "MMLU_high_school_european_history":0.2303030303,
        "MMLU_high_school_geography":0.3535353535,
        "MMLU_high_school_government_and_politics":0.3678756477,
        "MMLU_high_school_macroeconomics":0.3641025641,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.3445378151,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.3394495413,
        "MMLU_high_school_statistics":0.4583333333,
        "MMLU_high_school_us_history":0.2696078431,
        "MMLU_high_school_world_history":0.2151898734,
        "MMLU_human_aging":0.1390134529,
        "MMLU_human_sexuality":0.2824427481,
        "MMLU_international_law":0.1404958678,
        "MMLU_jurisprudence":0.212962963,
        "MMLU_logical_fallacies":0.226993865,
        "MMLU_machine_learning":0.2410714286,
        "MMLU_management":0.3786407767,
        "MMLU_marketing":0.2136752137,
        "MMLU_medical_genetics":0.24,
        "MMLU_miscellaneous":0.2056194125,
        "MMLU_moral_disputes":0.225433526,
        "MMLU_moral_scenarios":0.2726256983,
        "MMLU_nutrition":0.2875816993,
        "MMLU_philosophy":0.2411575563,
        "MMLU_prehistory":0.225308642,
        "MMLU_professional_accounting":0.2411347518,
        "MMLU_professional_law":0.2346805737,
        "MMLU_professional_medicine":0.4154411765,
        "MMLU_professional_psychology":0.2320261438,
        "MMLU_public_relations":0.2909090909,
        "MMLU_security_studies":0.3959183673,
        "MMLU_sociology":0.2736318408,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.2168674699,
        "MMLU_world_religions":0.1754385965
    },
    {
        "Model":"test100",
        "URL":"https:\/\/huggingface.co\/Devio\/test100",
        "full_model_name":"Devio\/test100",
        "Parameters":null,
        "MMLU_average":0.2728774728,
        "arc:challenge|25":0.3370307167,
        "hellaswag|10":0.4312885879,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.2296296296,
        "MMLU_astronomy":0.3355263158,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.2943396226,
        "MMLU_college_biology":0.2430555556,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.34,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.3294797688,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.18,
        "MMLU_conceptual_physics":0.2170212766,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.246031746,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.25,
        "MMLU_high_school_biology":0.3225806452,
        "MMLU_high_school_chemistry":0.2807881773,
        "MMLU_high_school_computer_science":0.18,
        "MMLU_high_school_european_history":0.2545454545,
        "MMLU_high_school_geography":0.3535353535,
        "MMLU_high_school_government_and_politics":0.3678756477,
        "MMLU_high_school_macroeconomics":0.3641025641,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.3487394958,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.352293578,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2025316456,
        "MMLU_human_aging":0.1076233184,
        "MMLU_human_sexuality":0.2824427481,
        "MMLU_international_law":0.1818181818,
        "MMLU_jurisprudence":0.212962963,
        "MMLU_logical_fallacies":0.2331288344,
        "MMLU_machine_learning":0.1607142857,
        "MMLU_management":0.3786407767,
        "MMLU_marketing":0.1965811966,
        "MMLU_medical_genetics":0.24,
        "MMLU_miscellaneous":0.2043422733,
        "MMLU_moral_disputes":0.2138728324,
        "MMLU_moral_scenarios":0.2703910615,
        "MMLU_nutrition":0.2973856209,
        "MMLU_philosophy":0.2540192926,
        "MMLU_prehistory":0.225308642,
        "MMLU_professional_accounting":0.2375886525,
        "MMLU_professional_law":0.2477183833,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2173202614,
        "MMLU_public_relations":0.2272727273,
        "MMLU_security_studies":0.4,
        "MMLU_sociology":0.2885572139,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.1927710843,
        "MMLU_world_religions":0.1695906433
    },
    {
        "Model":"LongAlpaca-7B",
        "URL":"https:\/\/huggingface.co\/Yukang\/LongAlpaca-7B",
        "full_model_name":"Yukang\/LongAlpaca-7B",
        "Parameters":7.0,
        "MMLU_average":0.2727996027,
        "arc:challenge|25":0.4001706485,
        "hellaswag|10":0.4968133838,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.3185185185,
        "MMLU_astronomy":0.2302631579,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.3509433962,
        "MMLU_college_biology":0.2291666667,
        "MMLU_college_chemistry":0.23,
        "MMLU_college_computer_science":0.36,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.2658959538,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.23,
        "MMLU_conceptual_physics":0.2170212766,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.2592592593,
        "MMLU_formal_logic":0.1349206349,
        "MMLU_global_facts":0.22,
        "MMLU_high_school_biology":0.2870967742,
        "MMLU_high_school_chemistry":0.236453202,
        "MMLU_high_school_computer_science":0.2,
        "MMLU_high_school_european_history":0.2787878788,
        "MMLU_high_school_geography":0.2828282828,
        "MMLU_high_school_government_and_politics":0.3575129534,
        "MMLU_high_school_macroeconomics":0.2923076923,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.3109243697,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.319266055,
        "MMLU_high_school_statistics":0.3888888889,
        "MMLU_high_school_us_history":0.2941176471,
        "MMLU_high_school_world_history":0.3122362869,
        "MMLU_human_aging":0.201793722,
        "MMLU_human_sexuality":0.3664122137,
        "MMLU_international_law":0.2314049587,
        "MMLU_jurisprudence":0.2407407407,
        "MMLU_logical_fallacies":0.245398773,
        "MMLU_machine_learning":0.1785714286,
        "MMLU_management":0.3203883495,
        "MMLU_marketing":0.311965812,
        "MMLU_medical_genetics":0.38,
        "MMLU_miscellaneous":0.2899106003,
        "MMLU_moral_disputes":0.2283236994,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2647058824,
        "MMLU_philosophy":0.2604501608,
        "MMLU_prehistory":0.2592592593,
        "MMLU_professional_accounting":0.2446808511,
        "MMLU_professional_law":0.2490221643,
        "MMLU_professional_medicine":0.3970588235,
        "MMLU_professional_psychology":0.227124183,
        "MMLU_public_relations":0.2636363636,
        "MMLU_security_studies":0.3265306122,
        "MMLU_sociology":0.2686567164,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.3192771084,
        "MMLU_world_religions":0.2514619883
    },
    {
        "Model":"ScarletPajama-3B-HF",
        "URL":"https:\/\/huggingface.co\/Fredithefish\/ScarletPajama-3B-HF",
        "full_model_name":"Fredithefish\/ScarletPajama-3B-HF",
        "Parameters":3.0,
        "MMLU_average":0.2727868436,
        "arc:challenge|25":0.3634812287,
        "hellaswag|10":0.4796853216,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.2222222222,
        "MMLU_astronomy":0.3026315789,
        "MMLU_business_ethics":0.22,
        "MMLU_clinical_knowledge":0.320754717,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.27,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.323699422,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.15,
        "MMLU_conceptual_physics":0.2340425532,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2751322751,
        "MMLU_formal_logic":0.2619047619,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.2838709677,
        "MMLU_high_school_chemistry":0.2709359606,
        "MMLU_high_school_computer_science":0.2,
        "MMLU_high_school_european_history":0.2484848485,
        "MMLU_high_school_geography":0.3737373737,
        "MMLU_high_school_government_and_politics":0.3575129534,
        "MMLU_high_school_macroeconomics":0.3256410256,
        "MMLU_high_school_mathematics":0.2888888889,
        "MMLU_high_school_microeconomics":0.3403361345,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.3302752294,
        "MMLU_high_school_statistics":0.3935185185,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2489451477,
        "MMLU_human_aging":0.130044843,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.347107438,
        "MMLU_jurisprudence":0.25,
        "MMLU_logical_fallacies":0.2392638037,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.3495145631,
        "MMLU_marketing":0.2179487179,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.2120051086,
        "MMLU_moral_disputes":0.2225433526,
        "MMLU_moral_scenarios":0.2726256983,
        "MMLU_nutrition":0.2941176471,
        "MMLU_philosophy":0.2829581994,
        "MMLU_prehistory":0.237654321,
        "MMLU_professional_accounting":0.2836879433,
        "MMLU_professional_law":0.2503259452,
        "MMLU_professional_medicine":0.3529411765,
        "MMLU_professional_psychology":0.2173202614,
        "MMLU_public_relations":0.2636363636,
        "MMLU_security_studies":0.3020408163,
        "MMLU_sociology":0.2537313433,
        "MMLU_us_foreign_policy":0.24,
        "MMLU_virology":0.2228915663,
        "MMLU_world_religions":0.2280701754
    },
    {
        "Model":"Aira-2-355M",
        "URL":"https:\/\/huggingface.co\/nicholasKluge\/Aira-2-355M",
        "full_model_name":"nicholasKluge\/Aira-2-355M",
        "Parameters":0.355,
        "MMLU_average":0.272557302,
        "arc:challenge|25":0.2465870307,
        "hellaswag|10":0.3311093408,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.2592592593,
        "MMLU_astronomy":0.2697368421,
        "MMLU_business_ethics":0.19,
        "MMLU_clinical_knowledge":0.3283018868,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.36,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.289017341,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.23,
        "MMLU_conceptual_physics":0.2595744681,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.2965517241,
        "MMLU_elementary_mathematics":0.2698412698,
        "MMLU_formal_logic":0.3333333333,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.2483870968,
        "MMLU_high_school_chemistry":0.315270936,
        "MMLU_high_school_computer_science":0.24,
        "MMLU_high_school_european_history":0.2484848485,
        "MMLU_high_school_geography":0.3434343434,
        "MMLU_high_school_government_and_politics":0.3523316062,
        "MMLU_high_school_macroeconomics":0.3615384615,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.3361344538,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.3467889908,
        "MMLU_high_school_statistics":0.4398148148,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2067510549,
        "MMLU_human_aging":0.1434977578,
        "MMLU_human_sexuality":0.2671755725,
        "MMLU_international_law":0.1404958678,
        "MMLU_jurisprudence":0.1851851852,
        "MMLU_logical_fallacies":0.2147239264,
        "MMLU_machine_learning":0.1696428571,
        "MMLU_management":0.3786407767,
        "MMLU_marketing":0.2393162393,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.2260536398,
        "MMLU_moral_disputes":0.2341040462,
        "MMLU_moral_scenarios":0.243575419,
        "MMLU_nutrition":0.2712418301,
        "MMLU_philosophy":0.2508038585,
        "MMLU_prehistory":0.2283950617,
        "MMLU_professional_accounting":0.2446808511,
        "MMLU_professional_law":0.240547588,
        "MMLU_professional_medicine":0.4522058824,
        "MMLU_professional_psychology":0.2483660131,
        "MMLU_public_relations":0.2,
        "MMLU_security_studies":0.4,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.1927710843,
        "MMLU_world_religions":0.2339181287
    },
    {
        "Model":"pythia-410m",
        "URL":"https:\/\/huggingface.co\/EleutherAI\/pythia-410m",
        "full_model_name":"EleutherAI\/pythia-410m",
        "Parameters":0.41,
        "MMLU_average":0.2724550741,
        "arc:challenge|25":0.2312286689,
        "hellaswag|10":0.3394742083,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.2814814815,
        "MMLU_astronomy":0.2171052632,
        "MMLU_business_ethics":0.27,
        "MMLU_clinical_knowledge":0.2490566038,
        "MMLU_college_biology":0.2777777778,
        "MMLU_college_chemistry":0.42,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.2,
        "MMLU_conceptual_physics":0.2893617021,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.2275862069,
        "MMLU_elementary_mathematics":0.2380952381,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.17,
        "MMLU_high_school_biology":0.2935483871,
        "MMLU_high_school_chemistry":0.2610837438,
        "MMLU_high_school_computer_science":0.31,
        "MMLU_high_school_european_history":0.2363636364,
        "MMLU_high_school_geography":0.303030303,
        "MMLU_high_school_government_and_politics":0.2331606218,
        "MMLU_high_school_macroeconomics":0.358974359,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.2394957983,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.3266055046,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2696078431,
        "MMLU_high_school_world_history":0.223628692,
        "MMLU_human_aging":0.2511210762,
        "MMLU_human_sexuality":0.2366412214,
        "MMLU_international_law":0.4049586777,
        "MMLU_jurisprudence":0.212962963,
        "MMLU_logical_fallacies":0.282208589,
        "MMLU_machine_learning":0.2142857143,
        "MMLU_management":0.2621359223,
        "MMLU_marketing":0.2051282051,
        "MMLU_medical_genetics":0.31,
        "MMLU_miscellaneous":0.2579821201,
        "MMLU_moral_disputes":0.2543352601,
        "MMLU_moral_scenarios":0.2446927374,
        "MMLU_nutrition":0.2549019608,
        "MMLU_philosophy":0.2443729904,
        "MMLU_prehistory":0.225308642,
        "MMLU_professional_accounting":0.2446808511,
        "MMLU_professional_law":0.2411994785,
        "MMLU_professional_medicine":0.4448529412,
        "MMLU_professional_psychology":0.2777777778,
        "MMLU_public_relations":0.2090909091,
        "MMLU_security_studies":0.4040816327,
        "MMLU_sociology":0.2338308458,
        "MMLU_us_foreign_policy":0.25,
        "MMLU_virology":0.2710843373,
        "MMLU_world_religions":0.2748538012
    },
    {
        "Model":"palmyra-base",
        "URL":"https:\/\/huggingface.co\/Writer\/palmyra-base",
        "full_model_name":"Writer\/palmyra-base",
        "Parameters":null,
        "MMLU_average":0.2715101975,
        "arc:challenge|25":0.2918088737,
        "hellaswag|10":0.4166500697,
        "MMLU_abstract_algebra":0.21,
        "MMLU_anatomy":0.2148148148,
        "MMLU_astronomy":0.25,
        "MMLU_business_ethics":0.27,
        "MMLU_clinical_knowledge":0.2981132075,
        "MMLU_college_biology":0.2638888889,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.29,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.3583815029,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.31,
        "MMLU_conceptual_physics":0.2127659574,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2482758621,
        "MMLU_elementary_mathematics":0.2751322751,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.25,
        "MMLU_high_school_biology":0.2806451613,
        "MMLU_high_school_chemistry":0.236453202,
        "MMLU_high_school_computer_science":0.2,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.3333333333,
        "MMLU_high_school_government_and_politics":0.3056994819,
        "MMLU_high_school_macroeconomics":0.3538461538,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.281512605,
        "MMLU_high_school_physics":0.2052980132,
        "MMLU_high_school_psychology":0.3412844037,
        "MMLU_high_school_statistics":0.4675925926,
        "MMLU_high_school_us_history":0.2254901961,
        "MMLU_high_school_world_history":0.2278481013,
        "MMLU_human_aging":0.1031390135,
        "MMLU_human_sexuality":0.1603053435,
        "MMLU_international_law":0.2561983471,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.226993865,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.3106796117,
        "MMLU_marketing":0.264957265,
        "MMLU_medical_genetics":0.37,
        "MMLU_miscellaneous":0.2094508301,
        "MMLU_moral_disputes":0.1936416185,
        "MMLU_moral_scenarios":0.2726256983,
        "MMLU_nutrition":0.2647058824,
        "MMLU_philosophy":0.2572347267,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2553191489,
        "MMLU_professional_law":0.2614080834,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2287581699,
        "MMLU_public_relations":0.2818181818,
        "MMLU_security_studies":0.306122449,
        "MMLU_sociology":0.2736318408,
        "MMLU_us_foreign_policy":0.29,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.2865497076
    },
    {
        "Model":"deacon-3b",
        "URL":"https:\/\/huggingface.co\/KnutJaegersberg\/deacon-3b",
        "full_model_name":"KnutJaegersberg\/deacon-3b",
        "Parameters":3.0,
        "MMLU_average":0.2713297842,
        "arc:challenge|25":0.3558020478,
        "hellaswag|10":0.4894443338,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.3111111111,
        "MMLU_astronomy":0.2565789474,
        "MMLU_business_ethics":0.19,
        "MMLU_clinical_knowledge":0.2830188679,
        "MMLU_college_biology":0.2430555556,
        "MMLU_college_chemistry":0.25,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.23,
        "MMLU_college_medicine":0.2716763006,
        "MMLU_college_physics":0.1764705882,
        "MMLU_computer_security":0.32,
        "MMLU_conceptual_physics":0.3319148936,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.275862069,
        "MMLU_elementary_mathematics":0.2989417989,
        "MMLU_formal_logic":0.1904761905,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.2677419355,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.23,
        "MMLU_high_school_european_history":0.2545454545,
        "MMLU_high_school_geography":0.3282828283,
        "MMLU_high_school_government_and_politics":0.2642487047,
        "MMLU_high_school_macroeconomics":0.3102564103,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.231092437,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.2862385321,
        "MMLU_high_school_statistics":0.3333333333,
        "MMLU_high_school_us_history":0.2745098039,
        "MMLU_high_school_world_history":0.2827004219,
        "MMLU_human_aging":0.2914798206,
        "MMLU_human_sexuality":0.1832061069,
        "MMLU_international_law":0.3223140496,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2147239264,
        "MMLU_machine_learning":0.1875,
        "MMLU_management":0.2815533981,
        "MMLU_marketing":0.2264957265,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2988505747,
        "MMLU_moral_disputes":0.2832369942,
        "MMLU_moral_scenarios":0.2402234637,
        "MMLU_nutrition":0.2647058824,
        "MMLU_philosophy":0.2829581994,
        "MMLU_prehistory":0.2654320988,
        "MMLU_professional_accounting":0.2907801418,
        "MMLU_professional_law":0.240547588,
        "MMLU_professional_medicine":0.3639705882,
        "MMLU_professional_psychology":0.2745098039,
        "MMLU_public_relations":0.3363636364,
        "MMLU_security_studies":0.2530612245,
        "MMLU_sociology":0.223880597,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.3373493976,
        "MMLU_world_religions":0.3333333333
    },
    {
        "Model":"test-22B",
        "URL":"https:\/\/huggingface.co\/Devio\/test-22B",
        "full_model_name":"Devio\/test-22B",
        "Parameters":22.0,
        "MMLU_average":0.2713116014,
        "arc:challenge|25":0.3438566553,
        "hellaswag|10":0.4690300737,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.2296296296,
        "MMLU_astronomy":0.3289473684,
        "MMLU_business_ethics":0.17,
        "MMLU_clinical_knowledge":0.2981132075,
        "MMLU_college_biology":0.2430555556,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.29,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.3063583815,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.21,
        "MMLU_conceptual_physics":0.2893617021,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2689655172,
        "MMLU_elementary_mathematics":0.2698412698,
        "MMLU_formal_logic":0.2301587302,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.3322580645,
        "MMLU_high_school_chemistry":0.2955665025,
        "MMLU_high_school_computer_science":0.22,
        "MMLU_high_school_european_history":0.2484848485,
        "MMLU_high_school_geography":0.3535353535,
        "MMLU_high_school_government_and_politics":0.3523316062,
        "MMLU_high_school_macroeconomics":0.3564102564,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.3529411765,
        "MMLU_high_school_physics":0.3642384106,
        "MMLU_high_school_psychology":0.3486238532,
        "MMLU_high_school_statistics":0.3888888889,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.2025316456,
        "MMLU_human_aging":0.1210762332,
        "MMLU_human_sexuality":0.320610687,
        "MMLU_international_law":0.173553719,
        "MMLU_jurisprudence":0.2222222222,
        "MMLU_logical_fallacies":0.2331288344,
        "MMLU_machine_learning":0.1696428571,
        "MMLU_management":0.3786407767,
        "MMLU_marketing":0.1965811966,
        "MMLU_medical_genetics":0.23,
        "MMLU_miscellaneous":0.2183908046,
        "MMLU_moral_disputes":0.2167630058,
        "MMLU_moral_scenarios":0.2413407821,
        "MMLU_nutrition":0.2941176471,
        "MMLU_philosophy":0.2604501608,
        "MMLU_prehistory":0.2283950617,
        "MMLU_professional_accounting":0.2695035461,
        "MMLU_professional_law":0.2451108214,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2173202614,
        "MMLU_public_relations":0.2545454545,
        "MMLU_security_studies":0.4,
        "MMLU_sociology":0.2985074627,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.1927710843,
        "MMLU_world_religions":0.1695906433
    },
    {
        "Model":"dlite-v1-355m",
        "URL":"https:\/\/huggingface.co\/aisquared\/dlite-v1-355m",
        "full_model_name":"aisquared\/dlite-v1-355m",
        "Parameters":0.355,
        "MMLU_average":0.2712158449,
        "arc:challenge|25":0.2372013652,
        "hellaswag|10":0.3353913563,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.3259259259,
        "MMLU_astronomy":0.3223684211,
        "MMLU_business_ethics":0.28,
        "MMLU_clinical_knowledge":0.3018867925,
        "MMLU_college_biology":0.2847222222,
        "MMLU_college_chemistry":0.25,
        "MMLU_college_computer_science":0.36,
        "MMLU_college_mathematics":0.22,
        "MMLU_college_medicine":0.2369942197,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.29,
        "MMLU_conceptual_physics":0.2170212766,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.2698412698,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.264516129,
        "MMLU_high_school_chemistry":0.315270936,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.3484848485,
        "MMLU_high_school_government_and_politics":0.3575129534,
        "MMLU_high_school_macroeconomics":0.3538461538,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.243697479,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.3486238532,
        "MMLU_high_school_statistics":0.462962963,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2151898734,
        "MMLU_human_aging":0.1928251121,
        "MMLU_human_sexuality":0.2824427481,
        "MMLU_international_law":0.1239669421,
        "MMLU_jurisprudence":0.1851851852,
        "MMLU_logical_fallacies":0.2944785276,
        "MMLU_machine_learning":0.1517857143,
        "MMLU_management":0.3495145631,
        "MMLU_marketing":0.1709401709,
        "MMLU_medical_genetics":0.17,
        "MMLU_miscellaneous":0.2337164751,
        "MMLU_moral_disputes":0.2369942197,
        "MMLU_moral_scenarios":0.2413407821,
        "MMLU_nutrition":0.2843137255,
        "MMLU_philosophy":0.2958199357,
        "MMLU_prehistory":0.2438271605,
        "MMLU_professional_accounting":0.2624113475,
        "MMLU_professional_law":0.2568448501,
        "MMLU_professional_medicine":0.4411764706,
        "MMLU_professional_psychology":0.2124183007,
        "MMLU_public_relations":0.2545454545,
        "MMLU_security_studies":0.3673469388,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.2108433735,
        "MMLU_world_religions":0.2807017544
    },
    {
        "Model":"openllama_3b_EvolInstruct_lora_merged",
        "URL":"https:\/\/huggingface.co\/KnutJaegersberg\/openllama_3b_EvolInstruct_lora_merged",
        "full_model_name":"KnutJaegersberg\/openllama_3b_EvolInstruct_lora_merged",
        "Parameters":3.0,
        "MMLU_average":0.2711798101,
        "arc:challenge|25":0.3558020478,
        "hellaswag|10":0.5310695081,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.237037037,
        "MMLU_astronomy":0.2763157895,
        "MMLU_business_ethics":0.4,
        "MMLU_clinical_knowledge":0.2679245283,
        "MMLU_college_biology":0.2430555556,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.27,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.2369942197,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.32,
        "MMLU_conceptual_physics":0.3234042553,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2068965517,
        "MMLU_elementary_mathematics":0.291005291,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.235483871,
        "MMLU_high_school_chemistry":0.2167487685,
        "MMLU_high_school_computer_science":0.24,
        "MMLU_high_school_european_history":0.2484848485,
        "MMLU_high_school_geography":0.202020202,
        "MMLU_high_school_government_and_politics":0.2383419689,
        "MMLU_high_school_macroeconomics":0.2564102564,
        "MMLU_high_school_mathematics":0.2333333333,
        "MMLU_high_school_microeconomics":0.2773109244,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.247706422,
        "MMLU_high_school_statistics":0.2083333333,
        "MMLU_high_school_us_history":0.2352941176,
        "MMLU_high_school_world_history":0.253164557,
        "MMLU_human_aging":0.399103139,
        "MMLU_human_sexuality":0.2519083969,
        "MMLU_international_law":0.2975206612,
        "MMLU_jurisprudence":0.2962962963,
        "MMLU_logical_fallacies":0.2147239264,
        "MMLU_machine_learning":0.2946428571,
        "MMLU_management":0.2718446602,
        "MMLU_marketing":0.2863247863,
        "MMLU_medical_genetics":0.23,
        "MMLU_miscellaneous":0.2809706258,
        "MMLU_moral_disputes":0.2572254335,
        "MMLU_moral_scenarios":0.2402234637,
        "MMLU_nutrition":0.2647058824,
        "MMLU_philosophy":0.2765273312,
        "MMLU_prehistory":0.299382716,
        "MMLU_professional_accounting":0.3120567376,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.2132352941,
        "MMLU_professional_psychology":0.2647058824,
        "MMLU_public_relations":0.3272727273,
        "MMLU_security_studies":0.3346938776,
        "MMLU_sociology":0.2786069652,
        "MMLU_us_foreign_policy":0.36,
        "MMLU_virology":0.3373493976,
        "MMLU_world_religions":0.298245614
    },
    {
        "Model":"mptk-1b",
        "URL":"https:\/\/huggingface.co\/team-lucid\/mptk-1b",
        "full_model_name":"team-lucid\/mptk-1b",
        "Parameters":1.0,
        "MMLU_average":0.2710704249,
        "arc:challenge|25":0.2269624573,
        "hellaswag|10":0.252041426,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.2074074074,
        "MMLU_astronomy":0.2434210526,
        "MMLU_business_ethics":0.22,
        "MMLU_clinical_knowledge":0.2679245283,
        "MMLU_college_biology":0.25,
        "MMLU_college_chemistry":0.39,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.289017341,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.22,
        "MMLU_conceptual_physics":0.229787234,
        "MMLU_econometrics":0.2105263158,
        "MMLU_electrical_engineering":0.2137931034,
        "MMLU_elementary_mathematics":0.253968254,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.2387096774,
        "MMLU_high_school_chemistry":0.275862069,
        "MMLU_high_school_computer_science":0.23,
        "MMLU_high_school_european_history":0.2363636364,
        "MMLU_high_school_geography":0.3080808081,
        "MMLU_high_school_government_and_politics":0.3575129534,
        "MMLU_high_school_macroeconomics":0.3615384615,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.3277310924,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.3266055046,
        "MMLU_high_school_statistics":0.3425925926,
        "MMLU_high_school_us_history":0.2205882353,
        "MMLU_high_school_world_history":0.2278481013,
        "MMLU_human_aging":0.134529148,
        "MMLU_human_sexuality":0.2748091603,
        "MMLU_international_law":0.2314049587,
        "MMLU_jurisprudence":0.212962963,
        "MMLU_logical_fallacies":0.1963190184,
        "MMLU_machine_learning":0.2142857143,
        "MMLU_management":0.359223301,
        "MMLU_marketing":0.2777777778,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.1992337165,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2726256983,
        "MMLU_nutrition":0.2777777778,
        "MMLU_philosophy":0.2540192926,
        "MMLU_prehistory":0.2561728395,
        "MMLU_professional_accounting":0.2659574468,
        "MMLU_professional_law":0.239243807,
        "MMLU_professional_medicine":0.4411764706,
        "MMLU_professional_psychology":0.2532679739,
        "MMLU_public_relations":0.2909090909,
        "MMLU_security_studies":0.2816326531,
        "MMLU_sociology":0.2537313433,
        "MMLU_us_foreign_policy":0.24,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.2397660819
    },
    {
        "Model":"mamba-gpt-3b-v2",
        "URL":"https:\/\/huggingface.co\/CobraMamba\/mamba-gpt-3b-v2",
        "full_model_name":"CobraMamba\/mamba-gpt-3b-v2",
        "Parameters":3.0,
        "MMLU_average":0.2710249095,
        "arc:challenge|25":0.3865187713,
        "hellaswag|10":0.5284803824,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.3259259259,
        "MMLU_astronomy":0.2368421053,
        "MMLU_business_ethics":0.4,
        "MMLU_clinical_knowledge":0.2716981132,
        "MMLU_college_biology":0.2361111111,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.3,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.1791907514,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.33,
        "MMLU_conceptual_physics":0.3404255319,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.1931034483,
        "MMLU_elementary_mathematics":0.253968254,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2387096774,
        "MMLU_high_school_chemistry":0.2512315271,
        "MMLU_high_school_computer_science":0.28,
        "MMLU_high_school_european_history":0.2606060606,
        "MMLU_high_school_geography":0.2222222222,
        "MMLU_high_school_government_and_politics":0.207253886,
        "MMLU_high_school_macroeconomics":0.2358974359,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.2647058824,
        "MMLU_high_school_physics":0.3973509934,
        "MMLU_high_school_psychology":0.2605504587,
        "MMLU_high_school_statistics":0.1944444444,
        "MMLU_high_school_us_history":0.2254901961,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3587443946,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.3223140496,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2515337423,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.2815533981,
        "MMLU_marketing":0.3076923077,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.3090676884,
        "MMLU_moral_disputes":0.2543352601,
        "MMLU_moral_scenarios":0.243575419,
        "MMLU_nutrition":0.2614379085,
        "MMLU_philosophy":0.2733118971,
        "MMLU_prehistory":0.2932098765,
        "MMLU_professional_accounting":0.2836879433,
        "MMLU_professional_law":0.2477183833,
        "MMLU_professional_medicine":0.1764705882,
        "MMLU_professional_psychology":0.2777777778,
        "MMLU_public_relations":0.3272727273,
        "MMLU_security_studies":0.2653061224,
        "MMLU_sociology":0.2587064677,
        "MMLU_us_foreign_policy":0.33,
        "MMLU_virology":0.3192771084,
        "MMLU_world_religions":0.3274853801
    },
    {
        "Model":"galactica-1.3b",
        "URL":"https:\/\/huggingface.co\/facebook\/galactica-1.3b",
        "full_model_name":"facebook\/galactica-1.3b",
        "Parameters":1.3,
        "MMLU_average":0.2709380804,
        "arc:challenge|25":0.2960750853,
        "hellaswag|10":0.3375821549,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.3407407407,
        "MMLU_astronomy":0.2368421053,
        "MMLU_business_ethics":0.2,
        "MMLU_clinical_knowledge":0.2603773585,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.25,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.2427745665,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.37,
        "MMLU_conceptual_physics":0.314893617,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.2689655172,
        "MMLU_elementary_mathematics":0.2645502646,
        "MMLU_formal_logic":0.1587301587,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.2774193548,
        "MMLU_high_school_chemistry":0.2906403941,
        "MMLU_high_school_computer_science":0.27,
        "MMLU_high_school_european_history":0.303030303,
        "MMLU_high_school_geography":0.2727272727,
        "MMLU_high_school_government_and_politics":0.2901554404,
        "MMLU_high_school_macroeconomics":0.2846153846,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.2478991597,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.2568807339,
        "MMLU_high_school_statistics":0.2638888889,
        "MMLU_high_school_us_history":0.2401960784,
        "MMLU_high_school_world_history":0.2827004219,
        "MMLU_human_aging":0.2780269058,
        "MMLU_human_sexuality":0.2213740458,
        "MMLU_international_law":0.3884297521,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.3067484663,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.2038834951,
        "MMLU_marketing":0.2777777778,
        "MMLU_medical_genetics":0.27,
        "MMLU_miscellaneous":0.2835249042,
        "MMLU_moral_disputes":0.2803468208,
        "MMLU_moral_scenarios":0.2458100559,
        "MMLU_nutrition":0.2712418301,
        "MMLU_philosophy":0.3215434084,
        "MMLU_prehistory":0.3055555556,
        "MMLU_professional_accounting":0.2659574468,
        "MMLU_professional_law":0.2685788787,
        "MMLU_professional_medicine":0.2610294118,
        "MMLU_professional_psychology":0.3022875817,
        "MMLU_public_relations":0.2363636364,
        "MMLU_security_studies":0.2653061224,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.21,
        "MMLU_virology":0.2951807229,
        "MMLU_world_religions":0.269005848
    },
    {
        "Model":"42dot_LLM-PLM-1.3B",
        "URL":"https:\/\/huggingface.co\/42dot\/42dot_LLM-PLM-1.3B",
        "full_model_name":"42dot\/42dot_LLM-PLM-1.3B",
        "Parameters":1.3,
        "MMLU_average":0.2708569878,
        "arc:challenge|25":0.3011945392,
        "hellaswag|10":0.428799044,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.237037037,
        "MMLU_astronomy":0.1842105263,
        "MMLU_business_ethics":0.22,
        "MMLU_clinical_knowledge":0.2679245283,
        "MMLU_college_biology":0.2708333333,
        "MMLU_college_chemistry":0.26,
        "MMLU_college_computer_science":0.39,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.2716763006,
        "MMLU_college_physics":0.1764705882,
        "MMLU_computer_security":0.27,
        "MMLU_conceptual_physics":0.3106382979,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.1862068966,
        "MMLU_elementary_mathematics":0.2407407407,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.2258064516,
        "MMLU_high_school_chemistry":0.2709359606,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.2222222222,
        "MMLU_high_school_government_and_politics":0.3471502591,
        "MMLU_high_school_macroeconomics":0.3461538462,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.2983193277,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.2605504587,
        "MMLU_high_school_statistics":0.4768518519,
        "MMLU_high_school_us_history":0.2647058824,
        "MMLU_high_school_world_history":0.2362869198,
        "MMLU_human_aging":0.2511210762,
        "MMLU_human_sexuality":0.2061068702,
        "MMLU_international_law":0.3884297521,
        "MMLU_jurisprudence":0.2037037037,
        "MMLU_logical_fallacies":0.2699386503,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2820512821,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.2324393359,
        "MMLU_moral_disputes":0.2138728324,
        "MMLU_moral_scenarios":0.2681564246,
        "MMLU_nutrition":0.2843137255,
        "MMLU_philosophy":0.3183279743,
        "MMLU_prehistory":0.2685185185,
        "MMLU_professional_accounting":0.2872340426,
        "MMLU_professional_law":0.239243807,
        "MMLU_professional_medicine":0.4448529412,
        "MMLU_professional_psychology":0.2565359477,
        "MMLU_public_relations":0.2454545455,
        "MMLU_security_studies":0.2693877551,
        "MMLU_sociology":0.2388059701,
        "MMLU_us_foreign_policy":0.22,
        "MMLU_virology":0.2710843373,
        "MMLU_world_religions":0.2631578947
    },
    {
        "Model":"LaMini-40k-Platypus2-7B",
        "URL":"https:\/\/huggingface.co\/marcchew\/LaMini-40k-Platypus2-7B",
        "full_model_name":"marcchew\/LaMini-40k-Platypus2-7B",
        "Parameters":7.0,
        "MMLU_average":0.2703799242,
        "arc:challenge|25":0.2295221843,
        "hellaswag|10":0.2542322247,
        "MMLU_abstract_algebra":0.21,
        "MMLU_anatomy":0.2296296296,
        "MMLU_astronomy":0.3355263158,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.2981132075,
        "MMLU_college_biology":0.2638888889,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.3352601156,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.18,
        "MMLU_conceptual_physics":0.2085106383,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.3161290323,
        "MMLU_high_school_chemistry":0.2807881773,
        "MMLU_high_school_computer_science":0.19,
        "MMLU_high_school_european_history":0.2545454545,
        "MMLU_high_school_geography":0.3535353535,
        "MMLU_high_school_government_and_politics":0.3678756477,
        "MMLU_high_school_macroeconomics":0.3641025641,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.3487394958,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.3486238532,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2025316456,
        "MMLU_human_aging":0.1076233184,
        "MMLU_human_sexuality":0.2824427481,
        "MMLU_international_law":0.1404958678,
        "MMLU_jurisprudence":0.212962963,
        "MMLU_logical_fallacies":0.2331288344,
        "MMLU_machine_learning":0.1607142857,
        "MMLU_management":0.3786407767,
        "MMLU_marketing":0.1965811966,
        "MMLU_medical_genetics":0.24,
        "MMLU_miscellaneous":0.2043422733,
        "MMLU_moral_disputes":0.2138728324,
        "MMLU_moral_scenarios":0.2726256983,
        "MMLU_nutrition":0.2941176471,
        "MMLU_philosophy":0.2411575563,
        "MMLU_prehistory":0.225308642,
        "MMLU_professional_accounting":0.2411347518,
        "MMLU_professional_law":0.2444589309,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2173202614,
        "MMLU_public_relations":0.2272727273,
        "MMLU_security_studies":0.4,
        "MMLU_sociology":0.2686567164,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.1927710843,
        "MMLU_world_religions":0.1754385965
    },
    {
        "Model":"RedPajama-INCITE-Base-3B-v1",
        "URL":"https:\/\/huggingface.co\/togethercomputer\/RedPajama-INCITE-Base-3B-v1",
        "full_model_name":"togethercomputer\/RedPajama-INCITE-Base-3B-v1",
        "Parameters":3.0,
        "MMLU_average":0.2702787422,
        "arc:challenge|25":0.3549488055,
        "hellaswag|10":0.4798844852,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.2074074074,
        "MMLU_astronomy":0.25,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.2867924528,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.28,
        "MMLU_college_computer_science":0.21,
        "MMLU_college_mathematics":0.36,
        "MMLU_college_medicine":0.3005780347,
        "MMLU_college_physics":0.1568627451,
        "MMLU_computer_security":0.18,
        "MMLU_conceptual_physics":0.2595744681,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.2344827586,
        "MMLU_elementary_mathematics":0.2698412698,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2548387097,
        "MMLU_high_school_chemistry":0.2512315271,
        "MMLU_high_school_computer_science":0.22,
        "MMLU_high_school_european_history":0.2363636364,
        "MMLU_high_school_geography":0.3888888889,
        "MMLU_high_school_government_and_politics":0.3575129534,
        "MMLU_high_school_macroeconomics":0.2923076923,
        "MMLU_high_school_mathematics":0.2296296296,
        "MMLU_high_school_microeconomics":0.3025210084,
        "MMLU_high_school_physics":0.3708609272,
        "MMLU_high_school_psychology":0.3541284404,
        "MMLU_high_school_statistics":0.3657407407,
        "MMLU_high_school_us_history":0.2990196078,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.1076233184,
        "MMLU_human_sexuality":0.2366412214,
        "MMLU_international_law":0.3884297521,
        "MMLU_jurisprudence":0.1944444444,
        "MMLU_logical_fallacies":0.2331288344,
        "MMLU_machine_learning":0.2232142857,
        "MMLU_management":0.3398058252,
        "MMLU_marketing":0.2222222222,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.2426564496,
        "MMLU_moral_disputes":0.2341040462,
        "MMLU_moral_scenarios":0.2659217877,
        "MMLU_nutrition":0.2777777778,
        "MMLU_philosophy":0.2668810289,
        "MMLU_prehistory":0.25,
        "MMLU_professional_accounting":0.2907801418,
        "MMLU_professional_law":0.2601043025,
        "MMLU_professional_medicine":0.4007352941,
        "MMLU_professional_psychology":0.2238562092,
        "MMLU_public_relations":0.3363636364,
        "MMLU_security_studies":0.3387755102,
        "MMLU_sociology":0.2686567164,
        "MMLU_us_foreign_policy":0.22,
        "MMLU_virology":0.2228915663,
        "MMLU_world_religions":0.2923976608
    },
    {
        "Model":"LaMini-Neo-1.3B-Mental-Health_lora",
        "URL":"https:\/\/huggingface.co\/Harshvir\/LaMini-Neo-1.3B-Mental-Health_lora",
        "full_model_name":"Harshvir\/LaMini-Neo-1.3B-Mental-Health_lora",
        "Parameters":1.3,
        "MMLU_average":0.2699755846,
        "arc:challenge|25":0.2098976109,
        "hellaswag|10":0.2541326429,
        "MMLU_abstract_algebra":0.2,
        "MMLU_anatomy":0.2296296296,
        "MMLU_astronomy":0.3355263158,
        "MMLU_business_ethics":0.22,
        "MMLU_clinical_knowledge":0.2981132075,
        "MMLU_college_biology":0.2708333333,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.3179190751,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.18,
        "MMLU_conceptual_physics":0.2085106383,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.3161290323,
        "MMLU_high_school_chemistry":0.2709359606,
        "MMLU_high_school_computer_science":0.19,
        "MMLU_high_school_european_history":0.2484848485,
        "MMLU_high_school_geography":0.3484848485,
        "MMLU_high_school_government_and_politics":0.3678756477,
        "MMLU_high_school_macroeconomics":0.3641025641,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.3487394958,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.3467889908,
        "MMLU_high_school_statistics":0.4768518519,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2025316456,
        "MMLU_human_aging":0.134529148,
        "MMLU_human_sexuality":0.2824427481,
        "MMLU_international_law":0.1404958678,
        "MMLU_jurisprudence":0.212962963,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.1607142857,
        "MMLU_management":0.3786407767,
        "MMLU_marketing":0.1965811966,
        "MMLU_medical_genetics":0.24,
        "MMLU_miscellaneous":0.2094508301,
        "MMLU_moral_disputes":0.2138728324,
        "MMLU_moral_scenarios":0.2715083799,
        "MMLU_nutrition":0.2810457516,
        "MMLU_philosophy":0.2443729904,
        "MMLU_prehistory":0.237654321,
        "MMLU_professional_accounting":0.2375886525,
        "MMLU_professional_law":0.2438070404,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2238562092,
        "MMLU_public_relations":0.2272727273,
        "MMLU_security_studies":0.4,
        "MMLU_sociology":0.2686567164,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.1927710843,
        "MMLU_world_religions":0.1754385965
    },
    {
        "Model":"pythia-6.9b-HC3",
        "URL":"https:\/\/huggingface.co\/pszemraj\/pythia-6.9b-HC3",
        "full_model_name":"pszemraj\/pythia-6.9b-HC3",
        "Parameters":6.9,
        "MMLU_average":0.269438271,
        "arc:challenge|25":0.3319112628,
        "hellaswag|10":0.4769966142,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.2814814815,
        "MMLU_astronomy":0.2631578947,
        "MMLU_business_ethics":0.19,
        "MMLU_clinical_knowledge":0.2679245283,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.35,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.3063583815,
        "MMLU_college_physics":0.362745098,
        "MMLU_computer_security":0.23,
        "MMLU_conceptual_physics":0.1914893617,
        "MMLU_econometrics":0.2192982456,
        "MMLU_electrical_engineering":0.2344827586,
        "MMLU_elementary_mathematics":0.2433862434,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.23,
        "MMLU_high_school_biology":0.2516129032,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.18,
        "MMLU_high_school_european_history":0.2060606061,
        "MMLU_high_school_geography":0.2525252525,
        "MMLU_high_school_government_and_politics":0.3005181347,
        "MMLU_high_school_macroeconomics":0.3461538462,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.2647058824,
        "MMLU_high_school_physics":0.3708609272,
        "MMLU_high_school_psychology":0.2990825688,
        "MMLU_high_school_statistics":0.4675925926,
        "MMLU_high_school_us_history":0.2205882353,
        "MMLU_high_school_world_history":0.2362869198,
        "MMLU_human_aging":0.1883408072,
        "MMLU_human_sexuality":0.2366412214,
        "MMLU_international_law":0.2892561983,
        "MMLU_jurisprudence":0.2407407407,
        "MMLU_logical_fallacies":0.3251533742,
        "MMLU_machine_learning":0.1607142857,
        "MMLU_management":0.3300970874,
        "MMLU_marketing":0.2863247863,
        "MMLU_medical_genetics":0.32,
        "MMLU_miscellaneous":0.2234993614,
        "MMLU_moral_disputes":0.2196531792,
        "MMLU_moral_scenarios":0.2726256983,
        "MMLU_nutrition":0.2418300654,
        "MMLU_philosophy":0.2604501608,
        "MMLU_prehistory":0.2191358025,
        "MMLU_professional_accounting":0.2836879433,
        "MMLU_professional_law":0.2425032595,
        "MMLU_professional_medicine":0.4227941176,
        "MMLU_professional_psychology":0.2532679739,
        "MMLU_public_relations":0.1818181818,
        "MMLU_security_studies":0.3510204082,
        "MMLU_sociology":0.2089552239,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.3012048193,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"open_llama_3b",
        "URL":"https:\/\/huggingface.co\/openlm-research\/open_llama_3b",
        "full_model_name":"openlm-research\/open_llama_3b",
        "Parameters":3.0,
        "MMLU_average":0.2693628704,
        "arc:challenge|25":0.3634812287,
        "hellaswag|10":0.4769966142,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.3333333333,
        "MMLU_astronomy":0.1973684211,
        "MMLU_business_ethics":0.28,
        "MMLU_clinical_knowledge":0.3018867925,
        "MMLU_college_biology":0.2847222222,
        "MMLU_college_chemistry":0.23,
        "MMLU_college_computer_science":0.24,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.3121387283,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.29,
        "MMLU_conceptual_physics":0.2978723404,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.3103448276,
        "MMLU_elementary_mathematics":0.2592592593,
        "MMLU_formal_logic":0.2222222222,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2548387097,
        "MMLU_high_school_chemistry":0.275862069,
        "MMLU_high_school_computer_science":0.31,
        "MMLU_high_school_european_history":0.2303030303,
        "MMLU_high_school_geography":0.2222222222,
        "MMLU_high_school_government_and_politics":0.2176165803,
        "MMLU_high_school_macroeconomics":0.2615384615,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.2142857143,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.2752293578,
        "MMLU_high_school_statistics":0.3148148148,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2911392405,
        "MMLU_human_aging":0.2780269058,
        "MMLU_human_sexuality":0.1984732824,
        "MMLU_international_law":0.2314049587,
        "MMLU_jurisprudence":0.2962962963,
        "MMLU_logical_fallacies":0.2331288344,
        "MMLU_machine_learning":0.1964285714,
        "MMLU_management":0.2524271845,
        "MMLU_marketing":0.2692307692,
        "MMLU_medical_genetics":0.25,
        "MMLU_miscellaneous":0.2835249042,
        "MMLU_moral_disputes":0.2947976879,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.2614379085,
        "MMLU_philosophy":0.3183279743,
        "MMLU_prehistory":0.2654320988,
        "MMLU_professional_accounting":0.2446808511,
        "MMLU_professional_law":0.2268578879,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2745098039,
        "MMLU_public_relations":0.2909090909,
        "MMLU_security_studies":0.2081632653,
        "MMLU_sociology":0.2537313433,
        "MMLU_us_foreign_policy":0.24,
        "MMLU_virology":0.3253012048,
        "MMLU_world_religions":0.2865497076
    },
    {
        "Model":"Marcoroni-7B-LaMini-40K",
        "URL":"https:\/\/huggingface.co\/marcchew\/Marcoroni-7B-LaMini-40K",
        "full_model_name":"marcchew\/Marcoroni-7B-LaMini-40K",
        "Parameters":7.0,
        "MMLU_average":0.2691864264,
        "arc:challenge|25":0.2226962457,
        "hellaswag|10":0.2536347341,
        "MMLU_abstract_algebra":0.21,
        "MMLU_anatomy":0.2296296296,
        "MMLU_astronomy":0.3355263158,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.2981132075,
        "MMLU_college_biology":0.2638888889,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.3352601156,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.18,
        "MMLU_conceptual_physics":0.2765957447,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.3161290323,
        "MMLU_high_school_chemistry":0.2807881773,
        "MMLU_high_school_computer_science":0.19,
        "MMLU_high_school_european_history":0.2545454545,
        "MMLU_high_school_geography":0.3535353535,
        "MMLU_high_school_government_and_politics":0.3678756477,
        "MMLU_high_school_macroeconomics":0.3641025641,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.3487394958,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.3486238532,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2025316456,
        "MMLU_human_aging":0.1076233184,
        "MMLU_human_sexuality":0.2671755725,
        "MMLU_international_law":0.1404958678,
        "MMLU_jurisprudence":0.212962963,
        "MMLU_logical_fallacies":0.2331288344,
        "MMLU_machine_learning":0.1607142857,
        "MMLU_management":0.1941747573,
        "MMLU_marketing":0.1965811966,
        "MMLU_medical_genetics":0.23,
        "MMLU_miscellaneous":0.220945083,
        "MMLU_moral_disputes":0.2138728324,
        "MMLU_moral_scenarios":0.2726256983,
        "MMLU_nutrition":0.2941176471,
        "MMLU_philosophy":0.2347266881,
        "MMLU_prehistory":0.225308642,
        "MMLU_professional_accounting":0.2411347518,
        "MMLU_professional_law":0.2444589309,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2173202614,
        "MMLU_public_relations":0.2272727273,
        "MMLU_security_studies":0.4,
        "MMLU_sociology":0.2686567164,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.1626506024,
        "MMLU_world_religions":0.269005848
    },
    {
        "Model":"gpt2_platypus-camel_physics",
        "URL":"https:\/\/huggingface.co\/lgaalves\/gpt2_platypus-camel_physics",
        "full_model_name":"lgaalves\/gpt2_platypus-camel_physics",
        "Parameters":null,
        "MMLU_average":0.2691462922,
        "arc:challenge|25":0.1979522184,
        "hellaswag|10":0.2918741287,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.2888888889,
        "MMLU_astronomy":0.2302631579,
        "MMLU_business_ethics":0.24,
        "MMLU_clinical_knowledge":0.279245283,
        "MMLU_college_biology":0.2708333333,
        "MMLU_college_chemistry":0.25,
        "MMLU_college_computer_science":0.36,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.2543352601,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.23,
        "MMLU_conceptual_physics":0.2595744681,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.3103448276,
        "MMLU_elementary_mathematics":0.2486772487,
        "MMLU_formal_logic":0.2301587302,
        "MMLU_global_facts":0.16,
        "MMLU_high_school_biology":0.2870967742,
        "MMLU_high_school_chemistry":0.2807881773,
        "MMLU_high_school_computer_science":0.3,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.3535353535,
        "MMLU_high_school_government_and_politics":0.3730569948,
        "MMLU_high_school_macroeconomics":0.341025641,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.2142857143,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.3486238532,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.1076233184,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.3719008264,
        "MMLU_jurisprudence":0.2037037037,
        "MMLU_logical_fallacies":0.3006134969,
        "MMLU_machine_learning":0.1785714286,
        "MMLU_management":0.3786407767,
        "MMLU_marketing":0.1965811966,
        "MMLU_medical_genetics":0.31,
        "MMLU_miscellaneous":0.2030651341,
        "MMLU_moral_disputes":0.210982659,
        "MMLU_moral_scenarios":0.243575419,
        "MMLU_nutrition":0.2516339869,
        "MMLU_philosophy":0.2893890675,
        "MMLU_prehistory":0.225308642,
        "MMLU_professional_accounting":0.2695035461,
        "MMLU_professional_law":0.2451108214,
        "MMLU_professional_medicine":0.4448529412,
        "MMLU_professional_psychology":0.2614379085,
        "MMLU_public_relations":0.2090909091,
        "MMLU_security_studies":0.4,
        "MMLU_sociology":0.2189054726,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.1927710843,
        "MMLU_world_religions":0.2514619883
    },
    {
        "Model":"fairseq-dense-6.7B",
        "URL":"https:\/\/huggingface.co\/KoboldAI\/fairseq-dense-6.7B",
        "full_model_name":"KoboldAI\/fairseq-dense-6.7B",
        "Parameters":6.7,
        "MMLU_average":0.2690662766,
        "arc:challenge|25":0.3549488055,
        "hellaswag|10":0.5287791277,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.2074074074,
        "MMLU_astronomy":0.2302631579,
        "MMLU_business_ethics":0.2,
        "MMLU_clinical_knowledge":0.2490566038,
        "MMLU_college_biology":0.2847222222,
        "MMLU_college_chemistry":0.26,
        "MMLU_college_computer_science":0.27,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.3005780347,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.22,
        "MMLU_conceptual_physics":0.2170212766,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2551724138,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.1746031746,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.3032258065,
        "MMLU_high_school_chemistry":0.2906403941,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.2060606061,
        "MMLU_high_school_geography":0.2323232323,
        "MMLU_high_school_government_and_politics":0.3264248705,
        "MMLU_high_school_macroeconomics":0.3128205128,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.2352941176,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.3357798165,
        "MMLU_high_school_statistics":0.4398148148,
        "MMLU_high_school_us_history":0.2352941176,
        "MMLU_high_school_world_history":0.2573839662,
        "MMLU_human_aging":0.1838565022,
        "MMLU_human_sexuality":0.2061068702,
        "MMLU_international_law":0.3553719008,
        "MMLU_jurisprudence":0.287037037,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.3495145631,
        "MMLU_marketing":0.235042735,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.2554278416,
        "MMLU_moral_disputes":0.2803468208,
        "MMLU_moral_scenarios":0.243575419,
        "MMLU_nutrition":0.2908496732,
        "MMLU_philosophy":0.2733118971,
        "MMLU_prehistory":0.2592592593,
        "MMLU_professional_accounting":0.2588652482,
        "MMLU_professional_law":0.2724902216,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2189542484,
        "MMLU_public_relations":0.3,
        "MMLU_security_studies":0.3632653061,
        "MMLU_sociology":0.2388059701,
        "MMLU_us_foreign_policy":0.2,
        "MMLU_virology":0.2409638554,
        "MMLU_world_religions":0.2573099415
    },
    {
        "Model":"Flash-Llama-3B",
        "URL":"https:\/\/huggingface.co\/TaylorAI\/Flash-Llama-3B",
        "full_model_name":"TaylorAI\/Flash-Llama-3B",
        "Parameters":3.0,
        "MMLU_average":0.2688067235,
        "arc:challenge|25":0.3575085324,
        "hellaswag|10":0.5300736905,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.2296296296,
        "MMLU_astronomy":0.2763157895,
        "MMLU_business_ethics":0.41,
        "MMLU_clinical_knowledge":0.2679245283,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.25,
        "MMLU_college_computer_science":0.27,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.2312138728,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.34,
        "MMLU_conceptual_physics":0.314893617,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2206896552,
        "MMLU_elementary_mathematics":0.2962962963,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2129032258,
        "MMLU_high_school_chemistry":0.2216748768,
        "MMLU_high_school_computer_science":0.23,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.1868686869,
        "MMLU_high_school_government_and_politics":0.2383419689,
        "MMLU_high_school_macroeconomics":0.2666666667,
        "MMLU_high_school_mathematics":0.2222222222,
        "MMLU_high_school_microeconomics":0.268907563,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.2422018349,
        "MMLU_high_school_statistics":0.212962963,
        "MMLU_high_school_us_history":0.2254901961,
        "MMLU_high_school_world_history":0.2489451477,
        "MMLU_human_aging":0.3856502242,
        "MMLU_human_sexuality":0.2519083969,
        "MMLU_international_law":0.2892561983,
        "MMLU_jurisprudence":0.2777777778,
        "MMLU_logical_fallacies":0.2392638037,
        "MMLU_machine_learning":0.2946428571,
        "MMLU_management":0.2815533981,
        "MMLU_marketing":0.2692307692,
        "MMLU_medical_genetics":0.24,
        "MMLU_miscellaneous":0.2771392082,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2402234637,
        "MMLU_nutrition":0.2516339869,
        "MMLU_philosophy":0.2797427653,
        "MMLU_prehistory":0.2932098765,
        "MMLU_professional_accounting":0.3085106383,
        "MMLU_professional_law":0.2431551499,
        "MMLU_professional_medicine":0.2169117647,
        "MMLU_professional_psychology":0.2712418301,
        "MMLU_public_relations":0.2909090909,
        "MMLU_security_studies":0.3224489796,
        "MMLU_sociology":0.2736318408,
        "MMLU_us_foreign_policy":0.35,
        "MMLU_virology":0.3373493976,
        "MMLU_world_religions":0.298245614
    },
    {
        "Model":"open-llama-3b-everything-v2",
        "URL":"https:\/\/huggingface.co\/harborwater\/open-llama-3b-everything-v2",
        "full_model_name":"harborwater\/open-llama-3b-everything-v2",
        "Parameters":3.0,
        "MMLU_average":0.2687395738,
        "arc:challenge|25":0.3950511945,
        "hellaswag|10":0.5509858594,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.2740740741,
        "MMLU_astronomy":0.3092105263,
        "MMLU_business_ethics":0.29,
        "MMLU_clinical_knowledge":0.2867924528,
        "MMLU_college_biology":0.2152777778,
        "MMLU_college_chemistry":0.23,
        "MMLU_college_computer_science":0.22,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.2023121387,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.3,
        "MMLU_conceptual_physics":0.3234042553,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.2,
        "MMLU_elementary_mathematics":0.291005291,
        "MMLU_formal_logic":0.2777777778,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2516129032,
        "MMLU_high_school_chemistry":0.2660098522,
        "MMLU_high_school_computer_science":0.29,
        "MMLU_high_school_european_history":0.2363636364,
        "MMLU_high_school_geography":0.2474747475,
        "MMLU_high_school_government_and_politics":0.2487046632,
        "MMLU_high_school_macroeconomics":0.241025641,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.281512605,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.2422018349,
        "MMLU_high_school_statistics":0.1759259259,
        "MMLU_high_school_us_history":0.2205882353,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3811659193,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.3719008264,
        "MMLU_jurisprudence":0.2777777778,
        "MMLU_logical_fallacies":0.263803681,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.2621359223,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.22,
        "MMLU_miscellaneous":0.2873563218,
        "MMLU_moral_disputes":0.2658959538,
        "MMLU_moral_scenarios":0.2346368715,
        "MMLU_nutrition":0.2352941176,
        "MMLU_philosophy":0.270096463,
        "MMLU_prehistory":0.262345679,
        "MMLU_professional_accounting":0.2836879433,
        "MMLU_professional_law":0.2490221643,
        "MMLU_professional_medicine":0.1948529412,
        "MMLU_professional_psychology":0.2679738562,
        "MMLU_public_relations":0.3272727273,
        "MMLU_security_studies":0.2857142857,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.32,
        "MMLU_virology":0.3253012048,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"bloom-1b1-RLHF",
        "URL":"https:\/\/huggingface.co\/TheTravellingEngineer\/bloom-1b1-RLHF",
        "full_model_name":"TheTravellingEngineer\/bloom-1b1-RLHF",
        "Parameters":1.0,
        "MMLU_average":0.2686002175,
        "arc:challenge|25":0.226109215,
        "hellaswag|10":0.2568213503,
        "MMLU_abstract_algebra":0.21,
        "MMLU_anatomy":0.2296296296,
        "MMLU_astronomy":0.3092105263,
        "MMLU_business_ethics":0.26,
        "MMLU_clinical_knowledge":0.2981132075,
        "MMLU_college_biology":0.2708333333,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.3,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.3641618497,
        "MMLU_college_physics":0.431372549,
        "MMLU_computer_security":0.18,
        "MMLU_conceptual_physics":0.2085106383,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.253968254,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.264516129,
        "MMLU_high_school_chemistry":0.275862069,
        "MMLU_high_school_computer_science":0.23,
        "MMLU_high_school_european_history":0.2545454545,
        "MMLU_high_school_geography":0.3535353535,
        "MMLU_high_school_government_and_politics":0.3471502591,
        "MMLU_high_school_macroeconomics":0.3641025641,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.3445378151,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.3155963303,
        "MMLU_high_school_statistics":0.3611111111,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2025316456,
        "MMLU_human_aging":0.1076233184,
        "MMLU_human_sexuality":0.2824427481,
        "MMLU_international_law":0.2231404959,
        "MMLU_jurisprudence":0.2222222222,
        "MMLU_logical_fallacies":0.2392638037,
        "MMLU_machine_learning":0.2946428571,
        "MMLU_management":0.3786407767,
        "MMLU_marketing":0.1965811966,
        "MMLU_medical_genetics":0.24,
        "MMLU_miscellaneous":0.2043422733,
        "MMLU_moral_disputes":0.2225433526,
        "MMLU_moral_scenarios":0.2536312849,
        "MMLU_nutrition":0.2549019608,
        "MMLU_philosophy":0.2411575563,
        "MMLU_prehistory":0.2407407407,
        "MMLU_professional_accounting":0.2553191489,
        "MMLU_professional_law":0.2444589309,
        "MMLU_professional_medicine":0.3529411765,
        "MMLU_professional_psychology":0.2336601307,
        "MMLU_public_relations":0.2272727273,
        "MMLU_security_studies":0.3346938776,
        "MMLU_sociology":0.263681592,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.1927710843,
        "MMLU_world_religions":0.1754385965
    },
    {
        "Model":"Cerebras-GPT-256M",
        "URL":"https:\/\/huggingface.co\/cerebras\/Cerebras-GPT-256M",
        "full_model_name":"cerebras\/Cerebras-GPT-256M",
        "Parameters":0.256,
        "MMLU_average":0.2683072205,
        "arc:challenge|25":0.1843003413,
        "hellaswag|10":0.277833101,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.2518518519,
        "MMLU_astronomy":0.1842105263,
        "MMLU_business_ethics":0.18,
        "MMLU_clinical_knowledge":0.2339622642,
        "MMLU_college_biology":0.2708333333,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.29,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.2023121387,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.23,
        "MMLU_conceptual_physics":0.3021276596,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2344827586,
        "MMLU_elementary_mathematics":0.2513227513,
        "MMLU_formal_logic":0.1825396825,
        "MMLU_global_facts":0.21,
        "MMLU_high_school_biology":0.3225806452,
        "MMLU_high_school_chemistry":0.2610837438,
        "MMLU_high_school_computer_science":0.31,
        "MMLU_high_school_european_history":0.2363636364,
        "MMLU_high_school_geography":0.3585858586,
        "MMLU_high_school_government_and_politics":0.3626943005,
        "MMLU_high_school_macroeconomics":0.3717948718,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.2521008403,
        "MMLU_high_school_physics":0.3509933775,
        "MMLU_high_school_psychology":0.3486238532,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2151898734,
        "MMLU_human_aging":0.130044843,
        "MMLU_human_sexuality":0.2213740458,
        "MMLU_international_law":0.2892561983,
        "MMLU_jurisprudence":0.2222222222,
        "MMLU_logical_fallacies":0.2699386503,
        "MMLU_machine_learning":0.2232142857,
        "MMLU_management":0.3203883495,
        "MMLU_marketing":0.2179487179,
        "MMLU_medical_genetics":0.32,
        "MMLU_miscellaneous":0.2298850575,
        "MMLU_moral_disputes":0.2369942197,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2745098039,
        "MMLU_philosophy":0.270096463,
        "MMLU_prehistory":0.2222222222,
        "MMLU_professional_accounting":0.2765957447,
        "MMLU_professional_law":0.2470664928,
        "MMLU_professional_medicine":0.4448529412,
        "MMLU_professional_psychology":0.2549019608,
        "MMLU_public_relations":0.2818181818,
        "MMLU_security_studies":0.4,
        "MMLU_sociology":0.2388059701,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.156626506,
        "MMLU_world_religions":0.2514619883
    },
    {
        "Model":"firefly-bloom-7b1",
        "URL":"https:\/\/huggingface.co\/YeungNLP\/firefly-bloom-7b1",
        "full_model_name":"YeungNLP\/firefly-bloom-7b1",
        "Parameters":7.0,
        "MMLU_average":0.2682902401,
        "arc:challenge|25":0.3660409556,
        "hellaswag|10":0.4663413663,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.237037037,
        "MMLU_astronomy":0.2105263158,
        "MMLU_business_ethics":0.25,
        "MMLU_clinical_knowledge":0.2830188679,
        "MMLU_college_biology":0.25,
        "MMLU_college_chemistry":0.26,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.2427745665,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.25,
        "MMLU_conceptual_physics":0.3234042553,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.2206896552,
        "MMLU_elementary_mathematics":0.2724867725,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2483870968,
        "MMLU_high_school_chemistry":0.3054187192,
        "MMLU_high_school_computer_science":0.27,
        "MMLU_high_school_european_history":0.3212121212,
        "MMLU_high_school_geography":0.2525252525,
        "MMLU_high_school_government_and_politics":0.2176165803,
        "MMLU_high_school_macroeconomics":0.2333333333,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.268907563,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.304587156,
        "MMLU_high_school_statistics":0.3425925926,
        "MMLU_high_school_us_history":0.2401960784,
        "MMLU_high_school_world_history":0.2362869198,
        "MMLU_human_aging":0.3452914798,
        "MMLU_human_sexuality":0.2213740458,
        "MMLU_international_law":0.2809917355,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2699386503,
        "MMLU_machine_learning":0.2232142857,
        "MMLU_management":0.2427184466,
        "MMLU_marketing":0.2863247863,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.288633461,
        "MMLU_moral_disputes":0.2572254335,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2745098039,
        "MMLU_philosophy":0.2733118971,
        "MMLU_prehistory":0.2839506173,
        "MMLU_professional_accounting":0.2588652482,
        "MMLU_professional_law":0.2535853977,
        "MMLU_professional_medicine":0.2757352941,
        "MMLU_professional_psychology":0.2516339869,
        "MMLU_public_relations":0.3181818182,
        "MMLU_security_studies":0.4040816327,
        "MMLU_sociology":0.2288557214,
        "MMLU_us_foreign_policy":0.24,
        "MMLU_virology":0.3072289157,
        "MMLU_world_religions":0.2807017544
    },
    {
        "Model":"oasst-sft-4-pythia-12b-epoch-3.5",
        "URL":"https:\/\/huggingface.co\/OpenAssistant\/oasst-sft-4-pythia-12b-epoch-3.5",
        "full_model_name":"OpenAssistant\/oasst-sft-4-pythia-12b-epoch-3.5",
        "Parameters":12.0,
        "MMLU_average":0.2681816525,
        "arc:challenge|25":0.412116041,
        "hellaswag|10":0.5173272257,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.3185185185,
        "MMLU_astronomy":0.2302631579,
        "MMLU_business_ethics":0.35,
        "MMLU_clinical_knowledge":0.3358490566,
        "MMLU_college_biology":0.2847222222,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.29,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.2485549133,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.26,
        "MMLU_conceptual_physics":0.3063829787,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.2551724138,
        "MMLU_elementary_mathematics":0.2513227513,
        "MMLU_formal_logic":0.1746031746,
        "MMLU_global_facts":0.29,
        "MMLU_high_school_biology":0.2677419355,
        "MMLU_high_school_chemistry":0.2167487685,
        "MMLU_high_school_computer_science":0.3,
        "MMLU_high_school_european_history":0.2848484848,
        "MMLU_high_school_geography":0.2575757576,
        "MMLU_high_school_government_and_politics":0.2383419689,
        "MMLU_high_school_macroeconomics":0.2641025641,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.243697479,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.2458715596,
        "MMLU_high_school_statistics":0.2407407407,
        "MMLU_high_school_us_history":0.2941176471,
        "MMLU_high_school_world_history":0.2362869198,
        "MMLU_human_aging":0.3004484305,
        "MMLU_human_sexuality":0.2213740458,
        "MMLU_international_law":0.3305785124,
        "MMLU_jurisprudence":0.287037037,
        "MMLU_logical_fallacies":0.282208589,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.1844660194,
        "MMLU_marketing":0.2991452991,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.2860791826,
        "MMLU_moral_disputes":0.2572254335,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2712418301,
        "MMLU_philosophy":0.2861736334,
        "MMLU_prehistory":0.2716049383,
        "MMLU_professional_accounting":0.2730496454,
        "MMLU_professional_law":0.278357236,
        "MMLU_professional_medicine":0.2352941176,
        "MMLU_professional_psychology":0.2777777778,
        "MMLU_public_relations":0.3272727273,
        "MMLU_security_studies":0.2571428571,
        "MMLU_sociology":0.2786069652,
        "MMLU_us_foreign_policy":0.23,
        "MMLU_virology":0.3373493976,
        "MMLU_world_religions":0.269005848
    },
    {
        "Model":"orca_mini_3b",
        "URL":"https:\/\/huggingface.co\/psmathur\/orca_mini_3b",
        "full_model_name":"psmathur\/orca_mini_3b",
        "Parameters":3.0,
        "MMLU_average":0.2679434416,
        "arc:challenge|25":0.3839590444,
        "hellaswag|10":0.4783907588,
        "MMLU_abstract_algebra":0.38,
        "MMLU_anatomy":0.3555555556,
        "MMLU_astronomy":0.2828947368,
        "MMLU_business_ethics":0.27,
        "MMLU_clinical_knowledge":0.2679245283,
        "MMLU_college_biology":0.2847222222,
        "MMLU_college_chemistry":0.15,
        "MMLU_college_computer_science":0.23,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.2485549133,
        "MMLU_college_physics":0.1764705882,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2382978723,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.3034482759,
        "MMLU_elementary_mathematics":0.2513227513,
        "MMLU_formal_logic":0.1746031746,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2193548387,
        "MMLU_high_school_chemistry":0.2463054187,
        "MMLU_high_school_computer_science":0.35,
        "MMLU_high_school_european_history":0.3151515152,
        "MMLU_high_school_geography":0.2878787879,
        "MMLU_high_school_government_and_politics":0.2538860104,
        "MMLU_high_school_macroeconomics":0.2333333333,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.247706422,
        "MMLU_high_school_statistics":0.2175925926,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.2447257384,
        "MMLU_human_aging":0.2825112108,
        "MMLU_human_sexuality":0.2061068702,
        "MMLU_international_law":0.3719008264,
        "MMLU_jurisprudence":0.287037037,
        "MMLU_logical_fallacies":0.2944785276,
        "MMLU_machine_learning":0.2142857143,
        "MMLU_management":0.2524271845,
        "MMLU_marketing":0.3034188034,
        "MMLU_medical_genetics":0.29,
        "MMLU_miscellaneous":0.349936143,
        "MMLU_moral_disputes":0.3092485549,
        "MMLU_moral_scenarios":0.2458100559,
        "MMLU_nutrition":0.2777777778,
        "MMLU_philosophy":0.3569131833,
        "MMLU_prehistory":0.2716049383,
        "MMLU_professional_accounting":0.2482269504,
        "MMLU_professional_law":0.2431551499,
        "MMLU_professional_medicine":0.1801470588,
        "MMLU_professional_psychology":0.2843137255,
        "MMLU_public_relations":0.2272727273,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.3134328358,
        "MMLU_us_foreign_policy":0.33,
        "MMLU_virology":0.3012048193,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"gpt-2-xl_camel-ai-physics",
        "URL":"https:\/\/huggingface.co\/lgaalves\/gpt-2-xl_camel-ai-physics",
        "full_model_name":"lgaalves\/gpt-2-xl_camel-ai-physics",
        "Parameters":null,
        "MMLU_average":0.2678815279,
        "arc:challenge|25":0.2747440273,
        "hellaswag|10":0.3984266082,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.2888888889,
        "MMLU_astronomy":0.2105263158,
        "MMLU_business_ethics":0.19,
        "MMLU_clinical_knowledge":0.320754717,
        "MMLU_college_biology":0.2638888889,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.3352601156,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.36,
        "MMLU_conceptual_physics":0.1957446809,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.3103448276,
        "MMLU_elementary_mathematics":0.2513227513,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.2483870968,
        "MMLU_high_school_chemistry":0.2709359606,
        "MMLU_high_school_computer_science":0.21,
        "MMLU_high_school_european_history":0.3151515152,
        "MMLU_high_school_geography":0.3333333333,
        "MMLU_high_school_government_and_politics":0.2901554404,
        "MMLU_high_school_macroeconomics":0.3487179487,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.218487395,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.3082568807,
        "MMLU_high_school_statistics":0.4444444444,
        "MMLU_high_school_us_history":0.2156862745,
        "MMLU_high_school_world_history":0.1898734177,
        "MMLU_human_aging":0.130044843,
        "MMLU_human_sexuality":0.2671755725,
        "MMLU_international_law":0.2231404959,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.2576687117,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.359223301,
        "MMLU_marketing":0.2777777778,
        "MMLU_medical_genetics":0.24,
        "MMLU_miscellaneous":0.2707535121,
        "MMLU_moral_disputes":0.2572254335,
        "MMLU_moral_scenarios":0.2480446927,
        "MMLU_nutrition":0.2614379085,
        "MMLU_philosophy":0.1800643087,
        "MMLU_prehistory":0.2469135802,
        "MMLU_professional_accounting":0.2340425532,
        "MMLU_professional_law":0.2503259452,
        "MMLU_professional_medicine":0.3419117647,
        "MMLU_professional_psychology":0.2450980392,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.3387755102,
        "MMLU_sociology":0.2388059701,
        "MMLU_us_foreign_policy":0.21,
        "MMLU_virology":0.2710843373,
        "MMLU_world_religions":0.298245614
    },
    {
        "Model":"WizardCoder-Guanaco-15B-V1.0",
        "URL":"https:\/\/huggingface.co\/LoupGarou\/WizardCoder-Guanaco-15B-V1.0",
        "full_model_name":"LoupGarou\/WizardCoder-Guanaco-15B-V1.0",
        "Parameters":15.0,
        "MMLU_average":0.2678680493,
        "arc:challenge|25":0.2883959044,
        "hellaswag|10":0.372734515,
        "MMLU_abstract_algebra":0.19,
        "MMLU_anatomy":0.2444444444,
        "MMLU_astronomy":0.2565789474,
        "MMLU_business_ethics":0.33,
        "MMLU_clinical_knowledge":0.2528301887,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.27,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.3179190751,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.37,
        "MMLU_conceptual_physics":0.3234042553,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.3793103448,
        "MMLU_elementary_mathematics":0.2063492063,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.19,
        "MMLU_high_school_biology":0.2258064516,
        "MMLU_high_school_chemistry":0.2167487685,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.2484848485,
        "MMLU_high_school_geography":0.1919191919,
        "MMLU_high_school_government_and_politics":0.2124352332,
        "MMLU_high_school_macroeconomics":0.2128205128,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.2268907563,
        "MMLU_high_school_physics":0.2317880795,
        "MMLU_high_school_psychology":0.2055045872,
        "MMLU_high_school_statistics":0.162037037,
        "MMLU_high_school_us_history":0.2647058824,
        "MMLU_high_school_world_history":0.2742616034,
        "MMLU_human_aging":0.3273542601,
        "MMLU_human_sexuality":0.2671755725,
        "MMLU_international_law":0.2975206612,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.2524271845,
        "MMLU_marketing":0.3376068376,
        "MMLU_medical_genetics":0.31,
        "MMLU_miscellaneous":0.2413793103,
        "MMLU_moral_disputes":0.2832369942,
        "MMLU_moral_scenarios":0.2346368715,
        "MMLU_nutrition":0.2745098039,
        "MMLU_philosophy":0.3054662379,
        "MMLU_prehistory":0.2592592593,
        "MMLU_professional_accounting":0.2517730496,
        "MMLU_professional_law":0.2737940026,
        "MMLU_professional_medicine":0.1911764706,
        "MMLU_professional_psychology":0.2826797386,
        "MMLU_public_relations":0.3090909091,
        "MMLU_security_studies":0.2163265306,
        "MMLU_sociology":0.328358209,
        "MMLU_us_foreign_policy":0.34,
        "MMLU_virology":0.3072289157,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"Cerebras_1.3b_Quantized",
        "URL":"https:\/\/huggingface.co\/FabbriSimo01\/Cerebras_1.3b_Quantized",
        "full_model_name":"FabbriSimo01\/Cerebras_1.3b_Quantized",
        "Parameters":1.3,
        "MMLU_average":0.2678658762,
        "arc:challenge|25":0.2354948805,
        "hellaswag|10":0.3278231428,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.2222222222,
        "MMLU_astronomy":0.2171052632,
        "MMLU_business_ethics":0.22,
        "MMLU_clinical_knowledge":0.2867924528,
        "MMLU_college_biology":0.2361111111,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.34,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.2601156069,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.21,
        "MMLU_conceptual_physics":0.2765957447,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2551724138,
        "MMLU_elementary_mathematics":0.2301587302,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2064516129,
        "MMLU_high_school_chemistry":0.2315270936,
        "MMLU_high_school_computer_science":0.27,
        "MMLU_high_school_european_history":0.2484848485,
        "MMLU_high_school_geography":0.3333333333,
        "MMLU_high_school_government_and_politics":0.3626943005,
        "MMLU_high_school_macroeconomics":0.3102564103,
        "MMLU_high_school_mathematics":0.237037037,
        "MMLU_high_school_microeconomics":0.2226890756,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.3486238532,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.2911392405,
        "MMLU_human_aging":0.2376681614,
        "MMLU_human_sexuality":0.2671755725,
        "MMLU_international_law":0.2892561983,
        "MMLU_jurisprudence":0.2407407407,
        "MMLU_logical_fallacies":0.3006134969,
        "MMLU_machine_learning":0.2946428571,
        "MMLU_management":0.2233009709,
        "MMLU_marketing":0.2863247863,
        "MMLU_medical_genetics":0.29,
        "MMLU_miscellaneous":0.2477650064,
        "MMLU_moral_disputes":0.2543352601,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2941176471,
        "MMLU_philosophy":0.270096463,
        "MMLU_prehistory":0.262345679,
        "MMLU_professional_accounting":0.2730496454,
        "MMLU_professional_law":0.2385919166,
        "MMLU_professional_medicine":0.3933823529,
        "MMLU_professional_psychology":0.272875817,
        "MMLU_public_relations":0.1818181818,
        "MMLU_security_studies":0.2367346939,
        "MMLU_sociology":0.223880597,
        "MMLU_us_foreign_policy":0.21,
        "MMLU_virology":0.2951807229,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"pythia-2.8b-deduped",
        "URL":"https:\/\/huggingface.co\/EleutherAI\/pythia-2.8b-deduped",
        "full_model_name":"EleutherAI\/pythia-2.8b-deduped",
        "Parameters":2.8,
        "MMLU_average":0.2678463743,
        "arc:challenge|25":0.3259385666,
        "hellaswag|10":0.4516032663,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.2962962963,
        "MMLU_astronomy":0.2039473684,
        "MMLU_business_ethics":0.22,
        "MMLU_clinical_knowledge":0.2679245283,
        "MMLU_college_biology":0.2361111111,
        "MMLU_college_chemistry":0.3,
        "MMLU_college_computer_science":0.24,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.2369942197,
        "MMLU_college_physics":0.1470588235,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2808510638,
        "MMLU_econometrics":0.2192982456,
        "MMLU_electrical_engineering":0.275862069,
        "MMLU_elementary_mathematics":0.2645502646,
        "MMLU_formal_logic":0.2222222222,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.2483870968,
        "MMLU_high_school_chemistry":0.2315270936,
        "MMLU_high_school_computer_science":0.23,
        "MMLU_high_school_european_history":0.2848484848,
        "MMLU_high_school_geography":0.2777777778,
        "MMLU_high_school_government_and_politics":0.2694300518,
        "MMLU_high_school_macroeconomics":0.3205128205,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.2478991597,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.2330275229,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.2156862745,
        "MMLU_high_school_world_history":0.2320675105,
        "MMLU_human_aging":0.3497757848,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.347107438,
        "MMLU_jurisprudence":0.2314814815,
        "MMLU_logical_fallacies":0.2699386503,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.2330097087,
        "MMLU_marketing":0.2435897436,
        "MMLU_medical_genetics":0.24,
        "MMLU_miscellaneous":0.2809706258,
        "MMLU_moral_disputes":0.2861271676,
        "MMLU_moral_scenarios":0.225698324,
        "MMLU_nutrition":0.2647058824,
        "MMLU_philosophy":0.2765273312,
        "MMLU_prehistory":0.2777777778,
        "MMLU_professional_accounting":0.2553191489,
        "MMLU_professional_law":0.258148631,
        "MMLU_professional_medicine":0.4338235294,
        "MMLU_professional_psychology":0.2745098039,
        "MMLU_public_relations":0.3545454545,
        "MMLU_security_studies":0.1959183673,
        "MMLU_sociology":0.2537313433,
        "MMLU_us_foreign_policy":0.2,
        "MMLU_virology":0.3072289157,
        "MMLU_world_religions":0.298245614
    },
    {
        "Model":"gpt-j-6B",
        "URL":"https:\/\/huggingface.co\/EleutherAI\/gpt-j-6B",
        "full_model_name":"EleutherAI\/gpt-j-6B",
        "Parameters":6.0,
        "MMLU_average":0.2678399936,
        "arc:challenge|25":0.3686006826,
        "hellaswag|10":0.4945230034,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.2666666667,
        "MMLU_astronomy":0.2697368421,
        "MMLU_business_ethics":0.28,
        "MMLU_clinical_knowledge":0.2603773585,
        "MMLU_college_biology":0.2430555556,
        "MMLU_college_chemistry":0.17,
        "MMLU_college_computer_science":0.23,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.2832369942,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.41,
        "MMLU_conceptual_physics":0.3404255319,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.2896551724,
        "MMLU_elementary_mathematics":0.2301587302,
        "MMLU_formal_logic":0.1825396825,
        "MMLU_global_facts":0.21,
        "MMLU_high_school_biology":0.2032258065,
        "MMLU_high_school_chemistry":0.2463054187,
        "MMLU_high_school_computer_science":0.15,
        "MMLU_high_school_european_history":0.2848484848,
        "MMLU_high_school_geography":0.2323232323,
        "MMLU_high_school_government_and_politics":0.2227979275,
        "MMLU_high_school_macroeconomics":0.2230769231,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.256302521,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.2256880734,
        "MMLU_high_school_statistics":0.162037037,
        "MMLU_high_school_us_history":0.2892156863,
        "MMLU_high_school_world_history":0.2869198312,
        "MMLU_human_aging":0.33632287,
        "MMLU_human_sexuality":0.213740458,
        "MMLU_international_law":0.2479338843,
        "MMLU_jurisprudence":0.287037037,
        "MMLU_logical_fallacies":0.2515337423,
        "MMLU_machine_learning":0.3839285714,
        "MMLU_management":0.213592233,
        "MMLU_marketing":0.264957265,
        "MMLU_medical_genetics":0.29,
        "MMLU_miscellaneous":0.3141762452,
        "MMLU_moral_disputes":0.2716763006,
        "MMLU_moral_scenarios":0.243575419,
        "MMLU_nutrition":0.2647058824,
        "MMLU_philosophy":0.2604501608,
        "MMLU_prehistory":0.3117283951,
        "MMLU_professional_accounting":0.2836879433,
        "MMLU_professional_law":0.2894393742,
        "MMLU_professional_medicine":0.2426470588,
        "MMLU_professional_psychology":0.2794117647,
        "MMLU_public_relations":0.3454545455,
        "MMLU_security_studies":0.3591836735,
        "MMLU_sociology":0.2786069652,
        "MMLU_us_foreign_policy":0.29,
        "MMLU_virology":0.3373493976,
        "MMLU_world_religions":0.3450292398
    },
    {
        "Model":"openllama-3b-350bt",
        "URL":"https:\/\/huggingface.co\/klosax\/openllama-3b-350bt",
        "full_model_name":"klosax\/openllama-3b-350bt",
        "Parameters":3.0,
        "MMLU_average":0.2677610703,
        "arc:challenge|25":0.3421501706,
        "hellaswag|10":0.4563831906,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.2740740741,
        "MMLU_astronomy":0.2302631579,
        "MMLU_business_ethics":0.24,
        "MMLU_clinical_knowledge":0.3056603774,
        "MMLU_college_biology":0.2291666667,
        "MMLU_college_chemistry":0.27,
        "MMLU_college_computer_science":0.22,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.2427745665,
        "MMLU_college_physics":0.1470588235,
        "MMLU_computer_security":0.32,
        "MMLU_conceptual_physics":0.3319148936,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.253968254,
        "MMLU_formal_logic":0.1428571429,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.2741935484,
        "MMLU_high_school_chemistry":0.2955665025,
        "MMLU_high_school_computer_science":0.26,
        "MMLU_high_school_european_history":0.2303030303,
        "MMLU_high_school_geography":0.3383838384,
        "MMLU_high_school_government_and_politics":0.2797927461,
        "MMLU_high_school_macroeconomics":0.2615384615,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.2857142857,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.2880733945,
        "MMLU_high_school_statistics":0.4027777778,
        "MMLU_high_school_us_history":0.2254901961,
        "MMLU_high_school_world_history":0.2827004219,
        "MMLU_human_aging":0.2780269058,
        "MMLU_human_sexuality":0.1984732824,
        "MMLU_international_law":0.3636363636,
        "MMLU_jurisprudence":0.2962962963,
        "MMLU_logical_fallacies":0.2760736196,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.2524271845,
        "MMLU_marketing":0.2564102564,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.2835249042,
        "MMLU_moral_disputes":0.2832369942,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2581699346,
        "MMLU_philosophy":0.270096463,
        "MMLU_prehistory":0.2314814815,
        "MMLU_professional_accounting":0.2695035461,
        "MMLU_professional_law":0.2281616688,
        "MMLU_professional_medicine":0.3161764706,
        "MMLU_professional_psychology":0.2712418301,
        "MMLU_public_relations":0.3545454545,
        "MMLU_security_studies":0.3020408163,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.21,
        "MMLU_virology":0.2469879518,
        "MMLU_world_religions":0.2807017544
    },
    {
        "Model":"open_llama_3b_350bt_preview",
        "URL":"https:\/\/huggingface.co\/klosax\/open_llama_3b_350bt_preview",
        "full_model_name":"klosax\/open_llama_3b_350bt_preview",
        "Parameters":3.0,
        "MMLU_average":0.2677610703,
        "arc:challenge|25":0.3421501706,
        "hellaswag|10":0.4563831906,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.2740740741,
        "MMLU_astronomy":0.2302631579,
        "MMLU_business_ethics":0.24,
        "MMLU_clinical_knowledge":0.3056603774,
        "MMLU_college_biology":0.2291666667,
        "MMLU_college_chemistry":0.27,
        "MMLU_college_computer_science":0.22,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.2427745665,
        "MMLU_college_physics":0.1470588235,
        "MMLU_computer_security":0.32,
        "MMLU_conceptual_physics":0.3319148936,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.253968254,
        "MMLU_formal_logic":0.1428571429,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.2741935484,
        "MMLU_high_school_chemistry":0.2955665025,
        "MMLU_high_school_computer_science":0.26,
        "MMLU_high_school_european_history":0.2303030303,
        "MMLU_high_school_geography":0.3383838384,
        "MMLU_high_school_government_and_politics":0.2797927461,
        "MMLU_high_school_macroeconomics":0.2615384615,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.2857142857,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.2880733945,
        "MMLU_high_school_statistics":0.4027777778,
        "MMLU_high_school_us_history":0.2254901961,
        "MMLU_high_school_world_history":0.2827004219,
        "MMLU_human_aging":0.2780269058,
        "MMLU_human_sexuality":0.1984732824,
        "MMLU_international_law":0.3636363636,
        "MMLU_jurisprudence":0.2962962963,
        "MMLU_logical_fallacies":0.2760736196,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.2524271845,
        "MMLU_marketing":0.2564102564,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.2835249042,
        "MMLU_moral_disputes":0.2832369942,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2581699346,
        "MMLU_philosophy":0.270096463,
        "MMLU_prehistory":0.2314814815,
        "MMLU_professional_accounting":0.2695035461,
        "MMLU_professional_law":0.2281616688,
        "MMLU_professional_medicine":0.3161764706,
        "MMLU_professional_psychology":0.2712418301,
        "MMLU_public_relations":0.3545454545,
        "MMLU_security_studies":0.3020408163,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.21,
        "MMLU_virology":0.2469879518,
        "MMLU_world_religions":0.2807017544
    },
    {
        "Model":"dlite-v2-355m",
        "URL":"https:\/\/huggingface.co\/aisquared\/dlite-v2-355m",
        "full_model_name":"aisquared\/dlite-v2-355m",
        "Parameters":0.355,
        "MMLU_average":0.2677302666,
        "arc:challenge|25":0.2491467577,
        "hellaswag|10":0.3370842462,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.2740740741,
        "MMLU_astronomy":0.3223684211,
        "MMLU_business_ethics":0.19,
        "MMLU_clinical_knowledge":0.3018867925,
        "MMLU_college_biology":0.2777777778,
        "MMLU_college_chemistry":0.23,
        "MMLU_college_computer_science":0.35,
        "MMLU_college_mathematics":0.2,
        "MMLU_college_medicine":0.2658959538,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.27,
        "MMLU_conceptual_physics":0.2212765957,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.2551724138,
        "MMLU_elementary_mathematics":0.2830687831,
        "MMLU_formal_logic":0.2301587302,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.2580645161,
        "MMLU_high_school_chemistry":0.2955665025,
        "MMLU_high_school_computer_science":0.19,
        "MMLU_high_school_european_history":0.2484848485,
        "MMLU_high_school_geography":0.3383838384,
        "MMLU_high_school_government_and_politics":0.3419689119,
        "MMLU_high_school_macroeconomics":0.3743589744,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.3319327731,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.3431192661,
        "MMLU_high_school_statistics":0.4398148148,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2362869198,
        "MMLU_human_aging":0.1659192825,
        "MMLU_human_sexuality":0.2977099237,
        "MMLU_international_law":0.132231405,
        "MMLU_jurisprudence":0.25,
        "MMLU_logical_fallacies":0.282208589,
        "MMLU_machine_learning":0.1696428571,
        "MMLU_management":0.3300970874,
        "MMLU_marketing":0.2222222222,
        "MMLU_medical_genetics":0.18,
        "MMLU_miscellaneous":0.2554278416,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2391061453,
        "MMLU_nutrition":0.2941176471,
        "MMLU_philosophy":0.2540192926,
        "MMLU_prehistory":0.237654321,
        "MMLU_professional_accounting":0.2517730496,
        "MMLU_professional_law":0.2359843546,
        "MMLU_professional_medicine":0.4595588235,
        "MMLU_professional_psychology":0.2385620915,
        "MMLU_public_relations":0.1636363636,
        "MMLU_security_studies":0.3836734694,
        "MMLU_sociology":0.2089552239,
        "MMLU_us_foreign_policy":0.24,
        "MMLU_virology":0.2168674699,
        "MMLU_world_religions":0.3274853801
    },
    {
        "Model":"GPT-NeoX-20B-Erebus",
        "URL":"https:\/\/huggingface.co\/KoboldAI\/GPT-NeoX-20B-Erebus",
        "full_model_name":"KoboldAI\/GPT-NeoX-20B-Erebus",
        "Parameters":20.0,
        "MMLU_average":0.2677278598,
        "arc:challenge|25":0.4215017065,
        "hellaswag|10":0.537243577,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.2888888889,
        "MMLU_astronomy":0.2368421053,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2603773585,
        "MMLU_college_biology":0.3125,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.2,
        "MMLU_college_mathematics":0.23,
        "MMLU_college_medicine":0.2138728324,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.35,
        "MMLU_conceptual_physics":0.3319148936,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.2592592593,
        "MMLU_formal_logic":0.2222222222,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.235483871,
        "MMLU_high_school_chemistry":0.2019704433,
        "MMLU_high_school_computer_science":0.24,
        "MMLU_high_school_european_history":0.296969697,
        "MMLU_high_school_geography":0.2222222222,
        "MMLU_high_school_government_and_politics":0.2590673575,
        "MMLU_high_school_macroeconomics":0.2384615385,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.231092437,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.2348623853,
        "MMLU_high_school_statistics":0.1527777778,
        "MMLU_high_school_us_history":0.3088235294,
        "MMLU_high_school_world_history":0.2911392405,
        "MMLU_human_aging":0.3004484305,
        "MMLU_human_sexuality":0.2900763359,
        "MMLU_international_law":0.5041322314,
        "MMLU_jurisprudence":0.25,
        "MMLU_logical_fallacies":0.3190184049,
        "MMLU_machine_learning":0.1696428571,
        "MMLU_management":0.3106796117,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.29,
        "MMLU_miscellaneous":0.2924648787,
        "MMLU_moral_disputes":0.3063583815,
        "MMLU_moral_scenarios":0.2502793296,
        "MMLU_nutrition":0.2941176471,
        "MMLU_philosophy":0.2926045016,
        "MMLU_prehistory":0.2932098765,
        "MMLU_professional_accounting":0.2659574468,
        "MMLU_professional_law":0.2444589309,
        "MMLU_professional_medicine":0.1433823529,
        "MMLU_professional_psychology":0.2859477124,
        "MMLU_public_relations":0.2545454545,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.2736318408,
        "MMLU_us_foreign_policy":0.3,
        "MMLU_virology":0.3313253012,
        "MMLU_world_religions":0.269005848
    },
    {
        "Model":"1.3b",
        "URL":"https:\/\/huggingface.co\/Corianas\/1.3b",
        "full_model_name":"Corianas\/1.3b",
        "Parameters":1.3,
        "MMLU_average":0.2676524347,
        "arc:challenge|25":0.2431740614,
        "hellaswag|10":0.3312089225,
        "MMLU_abstract_algebra":0.13,
        "MMLU_anatomy":0.2296296296,
        "MMLU_astronomy":0.1907894737,
        "MMLU_business_ethics":0.22,
        "MMLU_clinical_knowledge":0.2830188679,
        "MMLU_college_biology":0.2708333333,
        "MMLU_college_chemistry":0.26,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.2543352601,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.19,
        "MMLU_conceptual_physics":0.2170212766,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.275862069,
        "MMLU_elementary_mathematics":0.2804232804,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.27,
        "MMLU_high_school_biology":0.2,
        "MMLU_high_school_chemistry":0.315270936,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.3151515152,
        "MMLU_high_school_geography":0.3333333333,
        "MMLU_high_school_government_and_politics":0.3212435233,
        "MMLU_high_school_macroeconomics":0.3641025641,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.218487395,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.3596330275,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2784810127,
        "MMLU_human_aging":0.1569506726,
        "MMLU_human_sexuality":0.2824427481,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.3067484663,
        "MMLU_machine_learning":0.2232142857,
        "MMLU_management":0.3009708738,
        "MMLU_marketing":0.3034188034,
        "MMLU_medical_genetics":0.25,
        "MMLU_miscellaneous":0.2107279693,
        "MMLU_moral_disputes":0.2687861272,
        "MMLU_moral_scenarios":0.2357541899,
        "MMLU_nutrition":0.2483660131,
        "MMLU_philosophy":0.2572347267,
        "MMLU_prehistory":0.2191358025,
        "MMLU_professional_accounting":0.2588652482,
        "MMLU_professional_law":0.240547588,
        "MMLU_professional_medicine":0.3823529412,
        "MMLU_professional_psychology":0.2352941176,
        "MMLU_public_relations":0.1818181818,
        "MMLU_security_studies":0.3142857143,
        "MMLU_sociology":0.2885572139,
        "MMLU_us_foreign_policy":0.23,
        "MMLU_virology":0.2951807229,
        "MMLU_world_religions":0.2865497076
    },
    {
        "Model":"pythia-12b",
        "URL":"https:\/\/huggingface.co\/EleutherAI\/pythia-12b",
        "full_model_name":"EleutherAI\/pythia-12b",
        "Parameters":12.0,
        "MMLU_average":0.267560666,
        "arc:challenge|25":0.3737201365,
        "hellaswag|10":0.5026887074,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.3333333333,
        "MMLU_astronomy":0.2368421053,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.2603773585,
        "MMLU_college_biology":0.2638888889,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.2312138728,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.38,
        "MMLU_conceptual_physics":0.229787234,
        "MMLU_econometrics":0.201754386,
        "MMLU_electrical_engineering":0.275862069,
        "MMLU_elementary_mathematics":0.2698412698,
        "MMLU_formal_logic":0.1984126984,
        "MMLU_global_facts":0.28,
        "MMLU_high_school_biology":0.3161290323,
        "MMLU_high_school_chemistry":0.2709359606,
        "MMLU_high_school_computer_science":0.29,
        "MMLU_high_school_european_history":0.1818181818,
        "MMLU_high_school_geography":0.3282828283,
        "MMLU_high_school_government_and_politics":0.2849740933,
        "MMLU_high_school_macroeconomics":0.2641025641,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.2352941176,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.2678899083,
        "MMLU_high_school_statistics":0.3657407407,
        "MMLU_high_school_us_history":0.2745098039,
        "MMLU_high_school_world_history":0.253164557,
        "MMLU_human_aging":0.2869955157,
        "MMLU_human_sexuality":0.2442748092,
        "MMLU_international_law":0.2975206612,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.2944785276,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.213592233,
        "MMLU_marketing":0.264957265,
        "MMLU_medical_genetics":0.19,
        "MMLU_miscellaneous":0.2541507024,
        "MMLU_moral_disputes":0.289017341,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2679738562,
        "MMLU_philosophy":0.3022508039,
        "MMLU_prehistory":0.2654320988,
        "MMLU_professional_accounting":0.2659574468,
        "MMLU_professional_law":0.2555410691,
        "MMLU_professional_medicine":0.2022058824,
        "MMLU_professional_psychology":0.2598039216,
        "MMLU_public_relations":0.2363636364,
        "MMLU_security_studies":0.2571428571,
        "MMLU_sociology":0.3034825871,
        "MMLU_us_foreign_policy":0.29,
        "MMLU_virology":0.3192771084,
        "MMLU_world_religions":0.3040935673
    },
    {
        "Model":"Orca-Platypus-3B",
        "URL":"https:\/\/huggingface.co\/RobbeD\/Orca-Platypus-3B",
        "full_model_name":"RobbeD\/Orca-Platypus-3B",
        "Parameters":3.0,
        "MMLU_average":0.2675497361,
        "arc:challenge|25":0.3993174061,
        "hellaswag|10":0.496713802,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.3407407407,
        "MMLU_astronomy":0.2828947368,
        "MMLU_business_ethics":0.27,
        "MMLU_clinical_knowledge":0.279245283,
        "MMLU_college_biology":0.3055555556,
        "MMLU_college_chemistry":0.15,
        "MMLU_college_computer_science":0.22,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.2369942197,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.29,
        "MMLU_conceptual_physics":0.2595744681,
        "MMLU_econometrics":0.2192982456,
        "MMLU_electrical_engineering":0.2689655172,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.1904761905,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2225806452,
        "MMLU_high_school_chemistry":0.2315270936,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.303030303,
        "MMLU_high_school_geography":0.2727272727,
        "MMLU_high_school_government_and_politics":0.2590673575,
        "MMLU_high_school_macroeconomics":0.2384615385,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.2226890756,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.2440366972,
        "MMLU_high_school_statistics":0.2083333333,
        "MMLU_high_school_us_history":0.2401960784,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.2600896861,
        "MMLU_human_sexuality":0.2061068702,
        "MMLU_international_law":0.4049586777,
        "MMLU_jurisprudence":0.3055555556,
        "MMLU_logical_fallacies":0.3128834356,
        "MMLU_machine_learning":0.1964285714,
        "MMLU_management":0.2524271845,
        "MMLU_marketing":0.3076923077,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.3473818646,
        "MMLU_moral_disputes":0.3034682081,
        "MMLU_moral_scenarios":0.2446927374,
        "MMLU_nutrition":0.2810457516,
        "MMLU_philosophy":0.3633440514,
        "MMLU_prehistory":0.2716049383,
        "MMLU_professional_accounting":0.2553191489,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.1617647059,
        "MMLU_professional_psychology":0.2941176471,
        "MMLU_public_relations":0.1909090909,
        "MMLU_security_studies":0.1836734694,
        "MMLU_sociology":0.3184079602,
        "MMLU_us_foreign_policy":0.35,
        "MMLU_virology":0.3012048193,
        "MMLU_world_religions":0.3391812865
    },
    {
        "Model":"lamini-neo-125m",
        "URL":"https:\/\/huggingface.co\/MBZUAI\/lamini-neo-125m",
        "full_model_name":"MBZUAI\/lamini-neo-125m",
        "Parameters":0.125,
        "MMLU_average":0.2673582761,
        "arc:challenge|25":0.2098976109,
        "hellaswag|10":0.2838080064,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.3037037037,
        "MMLU_astronomy":0.3092105263,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.2113207547,
        "MMLU_college_biology":0.3125,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.18,
        "MMLU_conceptual_physics":0.2085106383,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.2827586207,
        "MMLU_elementary_mathematics":0.2407407407,
        "MMLU_formal_logic":0.3333333333,
        "MMLU_global_facts":0.15,
        "MMLU_high_school_biology":0.3129032258,
        "MMLU_high_school_chemistry":0.2906403941,
        "MMLU_high_school_computer_science":0.19,
        "MMLU_high_school_european_history":0.2484848485,
        "MMLU_high_school_geography":0.3686868687,
        "MMLU_high_school_government_and_politics":0.3575129534,
        "MMLU_high_school_macroeconomics":0.2564102564,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.2016806723,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.3394495413,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.2362869198,
        "MMLU_human_aging":0.3183856502,
        "MMLU_human_sexuality":0.2671755725,
        "MMLU_international_law":0.3223140496,
        "MMLU_jurisprudence":0.1944444444,
        "MMLU_logical_fallacies":0.2392638037,
        "MMLU_machine_learning":0.1428571429,
        "MMLU_management":0.2427184466,
        "MMLU_marketing":0.2008547009,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.2375478927,
        "MMLU_moral_disputes":0.2225433526,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.3039215686,
        "MMLU_philosophy":0.2250803859,
        "MMLU_prehistory":0.2098765432,
        "MMLU_professional_accounting":0.2304964539,
        "MMLU_professional_law":0.2444589309,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2336601307,
        "MMLU_public_relations":0.2454545455,
        "MMLU_security_studies":0.4,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.1987951807,
        "MMLU_world_religions":0.1988304094
    },
    {
        "Model":"fiction_story_generator",
        "URL":"https:\/\/huggingface.co\/Tincando\/fiction_story_generator",
        "full_model_name":"Tincando\/fiction_story_generator",
        "Parameters":null,
        "MMLU_average":0.2671722,
        "arc:challenge|25":0.1825938567,
        "hellaswag|10":0.2787293368,
        "MMLU_abstract_algebra":0.19,
        "MMLU_anatomy":0.2296296296,
        "MMLU_astronomy":0.2105263158,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.2981132075,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.2774566474,
        "MMLU_college_physics":0.3529411765,
        "MMLU_computer_security":0.18,
        "MMLU_conceptual_physics":0.2382978723,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.2827586207,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.17,
        "MMLU_high_school_biology":0.3161290323,
        "MMLU_high_school_chemistry":0.275862069,
        "MMLU_high_school_computer_science":0.21,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.3535353535,
        "MMLU_high_school_government_and_politics":0.3626943005,
        "MMLU_high_school_macroeconomics":0.3615384615,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.3487394958,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.3486238532,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2067510549,
        "MMLU_human_aging":0.1031390135,
        "MMLU_human_sexuality":0.2748091603,
        "MMLU_international_law":0.1487603306,
        "MMLU_jurisprudence":0.2222222222,
        "MMLU_logical_fallacies":0.2331288344,
        "MMLU_machine_learning":0.1607142857,
        "MMLU_management":0.3786407767,
        "MMLU_marketing":0.2008547009,
        "MMLU_medical_genetics":0.27,
        "MMLU_miscellaneous":0.2081736909,
        "MMLU_moral_disputes":0.2052023121,
        "MMLU_moral_scenarios":0.2648044693,
        "MMLU_nutrition":0.2941176471,
        "MMLU_philosophy":0.2475884244,
        "MMLU_prehistory":0.225308642,
        "MMLU_professional_accounting":0.2375886525,
        "MMLU_professional_law":0.2483702738,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2173202614,
        "MMLU_public_relations":0.2545454545,
        "MMLU_security_studies":0.4,
        "MMLU_sociology":0.2388059701,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.1927710843,
        "MMLU_world_religions":0.1754385965
    },
    {
        "Model":"weblab-10b-instruction-sft",
        "URL":"https:\/\/huggingface.co\/matsuo-lab\/weblab-10b-instruction-sft",
        "full_model_name":"matsuo-lab\/weblab-10b-instruction-sft",
        "Parameters":10.0,
        "MMLU_average":0.2666314297,
        "arc:challenge|25":0.3686006826,
        "hellaswag|10":0.4872535352,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.2666666667,
        "MMLU_astronomy":0.2828947368,
        "MMLU_business_ethics":0.34,
        "MMLU_clinical_knowledge":0.3094339623,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.24,
        "MMLU_college_mathematics":0.17,
        "MMLU_college_medicine":0.225433526,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.26,
        "MMLU_conceptual_physics":0.3446808511,
        "MMLU_econometrics":0.2105263158,
        "MMLU_electrical_engineering":0.2482758621,
        "MMLU_elementary_mathematics":0.2486772487,
        "MMLU_formal_logic":0.2142857143,
        "MMLU_global_facts":0.23,
        "MMLU_high_school_biology":0.2225806452,
        "MMLU_high_school_chemistry":0.157635468,
        "MMLU_high_school_computer_science":0.31,
        "MMLU_high_school_european_history":0.3090909091,
        "MMLU_high_school_geography":0.2222222222,
        "MMLU_high_school_government_and_politics":0.2487046632,
        "MMLU_high_school_macroeconomics":0.2153846154,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.256302521,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.2697247706,
        "MMLU_high_school_statistics":0.2083333333,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.3164556962,
        "MMLU_human_aging":0.33632287,
        "MMLU_human_sexuality":0.213740458,
        "MMLU_international_law":0.3305785124,
        "MMLU_jurisprudence":0.3425925926,
        "MMLU_logical_fallacies":0.2944785276,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.2233009709,
        "MMLU_marketing":0.3760683761,
        "MMLU_medical_genetics":0.27,
        "MMLU_miscellaneous":0.2899106003,
        "MMLU_moral_disputes":0.2658959538,
        "MMLU_moral_scenarios":0.251396648,
        "MMLU_nutrition":0.2483660131,
        "MMLU_philosophy":0.2572347267,
        "MMLU_prehistory":0.3086419753,
        "MMLU_professional_accounting":0.280141844,
        "MMLU_professional_law":0.2757496741,
        "MMLU_professional_medicine":0.1985294118,
        "MMLU_professional_psychology":0.2843137255,
        "MMLU_public_relations":0.3090909091,
        "MMLU_security_studies":0.2734693878,
        "MMLU_sociology":0.2587064677,
        "MMLU_us_foreign_policy":0.29,
        "MMLU_virology":0.2951807229,
        "MMLU_world_religions":0.3391812865
    },
    {
        "Model":"lamini-cerebras-256m",
        "URL":"https:\/\/huggingface.co\/MBZUAI\/lamini-cerebras-256m",
        "full_model_name":"MBZUAI\/lamini-cerebras-256m",
        "Parameters":0.256,
        "MMLU_average":0.2666268002,
        "arc:challenge|25":0.1723549488,
        "hellaswag|10":0.2776339375,
        "MMLU_abstract_algebra":0.21,
        "MMLU_anatomy":0.2592592593,
        "MMLU_astronomy":0.2960526316,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.2226415094,
        "MMLU_college_biology":0.2916666667,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.16,
        "MMLU_college_mathematics":0.23,
        "MMLU_college_medicine":0.1965317919,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.22,
        "MMLU_conceptual_physics":0.3234042553,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.2206896552,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.2698412698,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.3129032258,
        "MMLU_high_school_chemistry":0.2660098522,
        "MMLU_high_school_computer_science":0.36,
        "MMLU_high_school_european_history":0.2484848485,
        "MMLU_high_school_geography":0.3383838384,
        "MMLU_high_school_government_and_politics":0.3264248705,
        "MMLU_high_school_macroeconomics":0.3512820513,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.2647058824,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.3229357798,
        "MMLU_high_school_statistics":0.4537037037,
        "MMLU_high_school_us_history":0.2696078431,
        "MMLU_high_school_world_history":0.253164557,
        "MMLU_human_aging":0.2780269058,
        "MMLU_human_sexuality":0.213740458,
        "MMLU_international_law":0.2809917355,
        "MMLU_jurisprudence":0.2407407407,
        "MMLU_logical_fallacies":0.2699386503,
        "MMLU_machine_learning":0.2321428571,
        "MMLU_management":0.3300970874,
        "MMLU_marketing":0.2008547009,
        "MMLU_medical_genetics":0.25,
        "MMLU_miscellaneous":0.2503192848,
        "MMLU_moral_disputes":0.2630057803,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2712418301,
        "MMLU_philosophy":0.2861736334,
        "MMLU_prehistory":0.2469135802,
        "MMLU_professional_accounting":0.2659574468,
        "MMLU_professional_law":0.2398956975,
        "MMLU_professional_medicine":0.4448529412,
        "MMLU_professional_psychology":0.2287581699,
        "MMLU_public_relations":0.2272727273,
        "MMLU_security_studies":0.1795918367,
        "MMLU_sociology":0.2388059701,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.1626506024,
        "MMLU_world_religions":0.1988304094
    },
    {
        "Model":"opt-30b",
        "URL":"https:\/\/huggingface.co\/facebook\/opt-30b",
        "full_model_name":"facebook\/opt-30b",
        "Parameters":30.0,
        "MMLU_average":0.2665873004,
        "arc:challenge|25":0.3941979522,
        "hellaswag|10":0.5491933878,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.2,
        "MMLU_astronomy":0.2236842105,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2452830189,
        "MMLU_college_biology":0.2847222222,
        "MMLU_college_chemistry":0.19,
        "MMLU_college_computer_science":0.35,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.2601156069,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.3,
        "MMLU_conceptual_physics":0.2340425532,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2344827586,
        "MMLU_elementary_mathematics":0.2513227513,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.2580645161,
        "MMLU_high_school_chemistry":0.2413793103,
        "MMLU_high_school_computer_science":0.15,
        "MMLU_high_school_european_history":0.2787878788,
        "MMLU_high_school_geography":0.196969697,
        "MMLU_high_school_government_and_politics":0.3316062176,
        "MMLU_high_school_macroeconomics":0.3256410256,
        "MMLU_high_school_mathematics":0.2296296296,
        "MMLU_high_school_microeconomics":0.2478991597,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.2990825688,
        "MMLU_high_school_statistics":0.4212962963,
        "MMLU_high_school_us_history":0.3088235294,
        "MMLU_high_school_world_history":0.253164557,
        "MMLU_human_aging":0.3183856502,
        "MMLU_human_sexuality":0.2061068702,
        "MMLU_international_law":0.2975206612,
        "MMLU_jurisprudence":0.2777777778,
        "MMLU_logical_fallacies":0.2515337423,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.1941747573,
        "MMLU_marketing":0.264957265,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.2554278416,
        "MMLU_moral_disputes":0.2630057803,
        "MMLU_moral_scenarios":0.2759776536,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.270096463,
        "MMLU_prehistory":0.299382716,
        "MMLU_professional_accounting":0.2375886525,
        "MMLU_professional_law":0.2757496741,
        "MMLU_professional_medicine":0.3272058824,
        "MMLU_professional_psychology":0.2581699346,
        "MMLU_public_relations":0.3272727273,
        "MMLU_security_studies":0.3632653061,
        "MMLU_sociology":0.223880597,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.3192771084,
        "MMLU_world_religions":0.3567251462
    },
    {
        "Model":"Quokka_1.3b",
        "URL":"https:\/\/huggingface.co\/Corianas\/Quokka_1.3b",
        "full_model_name":"Corianas\/Quokka_1.3b",
        "Parameters":1.3,
        "MMLU_average":0.2665817163,
        "arc:challenge|25":0.2457337884,
        "hellaswag|10":0.3285202151,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.2222222222,
        "MMLU_astronomy":0.2039473684,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.2943396226,
        "MMLU_college_biology":0.2777777778,
        "MMLU_college_chemistry":0.22,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.2427745665,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.2,
        "MMLU_conceptual_physics":0.2,
        "MMLU_econometrics":0.201754386,
        "MMLU_electrical_engineering":0.2827586207,
        "MMLU_elementary_mathematics":0.2354497354,
        "MMLU_formal_logic":0.2698412698,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2032258065,
        "MMLU_high_school_chemistry":0.236453202,
        "MMLU_high_school_computer_science":0.31,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.3737373737,
        "MMLU_high_school_government_and_politics":0.3730569948,
        "MMLU_high_school_macroeconomics":0.2948717949,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.2058823529,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.3559633028,
        "MMLU_high_school_statistics":0.4675925926,
        "MMLU_high_school_us_history":0.2647058824,
        "MMLU_high_school_world_history":0.2616033755,
        "MMLU_human_aging":0.2421524664,
        "MMLU_human_sexuality":0.2671755725,
        "MMLU_international_law":0.2975206612,
        "MMLU_jurisprudence":0.25,
        "MMLU_logical_fallacies":0.2883435583,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.3883495146,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.2043422733,
        "MMLU_moral_disputes":0.2774566474,
        "MMLU_moral_scenarios":0.2413407821,
        "MMLU_nutrition":0.2549019608,
        "MMLU_philosophy":0.2572347267,
        "MMLU_prehistory":0.237654321,
        "MMLU_professional_accounting":0.2907801418,
        "MMLU_professional_law":0.239243807,
        "MMLU_professional_medicine":0.2536764706,
        "MMLU_professional_psychology":0.2565359477,
        "MMLU_public_relations":0.1818181818,
        "MMLU_security_studies":0.1836734694,
        "MMLU_sociology":0.2189054726,
        "MMLU_us_foreign_policy":0.21,
        "MMLU_virology":0.2951807229,
        "MMLU_world_religions":0.3274853801
    },
    {
        "Model":"polyglot-ko-12.8b",
        "URL":"https:\/\/huggingface.co\/EleutherAI\/polyglot-ko-12.8b",
        "full_model_name":"EleutherAI\/polyglot-ko-12.8b",
        "Parameters":12.8,
        "MMLU_average":0.2664125677,
        "arc:challenge|25":0.2448805461,
        "hellaswag|10":0.3951404103,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.2444444444,
        "MMLU_astronomy":0.2236842105,
        "MMLU_business_ethics":0.31,
        "MMLU_clinical_knowledge":0.2075471698,
        "MMLU_college_biology":0.2708333333,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.2774566474,
        "MMLU_college_physics":0.3431372549,
        "MMLU_computer_security":0.29,
        "MMLU_conceptual_physics":0.2085106383,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2689655172,
        "MMLU_elementary_mathematics":0.2936507937,
        "MMLU_formal_logic":0.1428571429,
        "MMLU_global_facts":0.14,
        "MMLU_high_school_biology":0.2967741935,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.3,
        "MMLU_high_school_european_history":0.2484848485,
        "MMLU_high_school_geography":0.2525252525,
        "MMLU_high_school_government_and_politics":0.2694300518,
        "MMLU_high_school_macroeconomics":0.2230769231,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.2330275229,
        "MMLU_high_school_statistics":0.4166666667,
        "MMLU_high_school_us_history":0.2745098039,
        "MMLU_high_school_world_history":0.2573839662,
        "MMLU_human_aging":0.201793722,
        "MMLU_human_sexuality":0.2442748092,
        "MMLU_international_law":0.347107438,
        "MMLU_jurisprudence":0.2777777778,
        "MMLU_logical_fallacies":0.3006134969,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.1844660194,
        "MMLU_marketing":0.2820512821,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2681992337,
        "MMLU_moral_disputes":0.2398843931,
        "MMLU_moral_scenarios":0.261452514,
        "MMLU_nutrition":0.2516339869,
        "MMLU_philosophy":0.231511254,
        "MMLU_prehistory":0.2777777778,
        "MMLU_professional_accounting":0.195035461,
        "MMLU_professional_law":0.2301173403,
        "MMLU_professional_medicine":0.4558823529,
        "MMLU_professional_psychology":0.2630718954,
        "MMLU_public_relations":0.1818181818,
        "MMLU_security_studies":0.2367346939,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.25,
        "MMLU_virology":0.234939759,
        "MMLU_world_religions":0.2923976608
    },
    {
        "Model":"oasst-pythia-12b-flash-attn-5000-steps",
        "URL":"https:\/\/huggingface.co\/dvruette\/oasst-pythia-12b-flash-attn-5000-steps",
        "full_model_name":"dvruette\/oasst-pythia-12b-flash-attn-5000-steps",
        "Parameters":12.0,
        "MMLU_average":0.2663644656,
        "arc:challenge|25":0.4223549488,
        "hellaswag|10":0.5270862378,
        "MMLU_abstract_algebra":0.34,
        "MMLU_anatomy":0.3037037037,
        "MMLU_astronomy":0.3092105263,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.3169811321,
        "MMLU_college_biology":0.2708333333,
        "MMLU_college_chemistry":0.23,
        "MMLU_college_computer_science":0.25,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2485549133,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.25,
        "MMLU_conceptual_physics":0.2468085106,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.2551724138,
        "MMLU_elementary_mathematics":0.2724867725,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.264516129,
        "MMLU_high_school_chemistry":0.2561576355,
        "MMLU_high_school_computer_science":0.26,
        "MMLU_high_school_european_history":0.2727272727,
        "MMLU_high_school_geography":0.3181818182,
        "MMLU_high_school_government_and_politics":0.3523316062,
        "MMLU_high_school_macroeconomics":0.3,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.2268907563,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.2587155963,
        "MMLU_high_school_statistics":0.2592592593,
        "MMLU_high_school_us_history":0.2941176471,
        "MMLU_high_school_world_history":0.2109704641,
        "MMLU_human_aging":0.197309417,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.3719008264,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.3006134969,
        "MMLU_machine_learning":0.2053571429,
        "MMLU_management":0.2233009709,
        "MMLU_marketing":0.264957265,
        "MMLU_medical_genetics":0.21,
        "MMLU_miscellaneous":0.251596424,
        "MMLU_moral_disputes":0.2861271676,
        "MMLU_moral_scenarios":0.2547486034,
        "MMLU_nutrition":0.2941176471,
        "MMLU_philosophy":0.2893890675,
        "MMLU_prehistory":0.2777777778,
        "MMLU_professional_accounting":0.2659574468,
        "MMLU_professional_law":0.2614080834,
        "MMLU_professional_medicine":0.3308823529,
        "MMLU_professional_psychology":0.2712418301,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.3469387755,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.2,
        "MMLU_virology":0.2168674699,
        "MMLU_world_religions":0.2514619883
    },
    {
        "Model":"256_5epoch",
        "URL":"https:\/\/huggingface.co\/Corianas\/256_5epoch",
        "full_model_name":"Corianas\/256_5epoch",
        "Parameters":null,
        "MMLU_average":0.2662219772,
        "arc:challenge|25":0.1800341297,
        "hellaswag|10":0.2773351922,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.237037037,
        "MMLU_astronomy":0.1973684211,
        "MMLU_business_ethics":0.27,
        "MMLU_clinical_knowledge":0.2830188679,
        "MMLU_college_biology":0.2291666667,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.28,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.2138728324,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.23,
        "MMLU_conceptual_physics":0.2978723404,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.275862069,
        "MMLU_elementary_mathematics":0.2407407407,
        "MMLU_formal_logic":0.1666666667,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.3032258065,
        "MMLU_high_school_chemistry":0.2266009852,
        "MMLU_high_school_computer_science":0.29,
        "MMLU_high_school_european_history":0.303030303,
        "MMLU_high_school_geography":0.3434343434,
        "MMLU_high_school_government_and_politics":0.3575129534,
        "MMLU_high_school_macroeconomics":0.3615384615,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.2142857143,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.3467889908,
        "MMLU_high_school_statistics":0.4537037037,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.1856540084,
        "MMLU_human_aging":0.1165919283,
        "MMLU_human_sexuality":0.1603053435,
        "MMLU_international_law":0.2479338843,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2883435583,
        "MMLU_machine_learning":0.2232142857,
        "MMLU_management":0.359223301,
        "MMLU_marketing":0.2777777778,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.2349936143,
        "MMLU_moral_disputes":0.2427745665,
        "MMLU_moral_scenarios":0.251396648,
        "MMLU_nutrition":0.2777777778,
        "MMLU_philosophy":0.270096463,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2730496454,
        "MMLU_professional_law":0.2483702738,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2336601307,
        "MMLU_public_relations":0.2363636364,
        "MMLU_security_studies":0.4,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2048192771,
        "MMLU_world_religions":0.2807017544
    },
    {
        "Model":"results_2023-07-24T09-55-17.325605.json",
        "URL":"https:\/\/huggingface.co\/gpt2-medium\/results_2023-07-24T09-55-17.325605.json",
        "full_model_name":"gpt2-medium\/results_2023-07-24T09-55-17.325605.json",
        "Parameters":null,
        "MMLU_average":0.2660221176,
        "arc:challenge|25":0.2209897611,
        "hellaswag|10":0.330611432,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.2444444444,
        "MMLU_astronomy":0.3026315789,
        "MMLU_business_ethics":0.15,
        "MMLU_clinical_knowledge":0.3018867925,
        "MMLU_college_biology":0.2708333333,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.36,
        "MMLU_college_mathematics":0.22,
        "MMLU_college_medicine":0.2658959538,
        "MMLU_college_physics":0.1568627451,
        "MMLU_computer_security":0.23,
        "MMLU_conceptual_physics":0.2936170213,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.2482758621,
        "MMLU_elementary_mathematics":0.2592592593,
        "MMLU_formal_logic":0.1587301587,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2419354839,
        "MMLU_high_school_chemistry":0.3054187192,
        "MMLU_high_school_computer_science":0.28,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.3585858586,
        "MMLU_high_school_government_and_politics":0.3160621762,
        "MMLU_high_school_macroeconomics":0.3076923077,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.3025210084,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.3357798165,
        "MMLU_high_school_statistics":0.4074074074,
        "MMLU_high_school_us_history":0.2254901961,
        "MMLU_high_school_world_history":0.2109704641,
        "MMLU_human_aging":0.2062780269,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.1652892562,
        "MMLU_jurisprudence":0.2037037037,
        "MMLU_logical_fallacies":0.263803681,
        "MMLU_machine_learning":0.1964285714,
        "MMLU_management":0.359223301,
        "MMLU_marketing":0.2051282051,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.2401021711,
        "MMLU_moral_disputes":0.2427745665,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2679738562,
        "MMLU_philosophy":0.2411575563,
        "MMLU_prehistory":0.2191358025,
        "MMLU_professional_accounting":0.2588652482,
        "MMLU_professional_law":0.2451108214,
        "MMLU_professional_medicine":0.4522058824,
        "MMLU_professional_psychology":0.2352941176,
        "MMLU_public_relations":0.2636363636,
        "MMLU_security_studies":0.3346938776,
        "MMLU_sociology":0.2288557214,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2048192771,
        "MMLU_world_religions":0.2514619883
    },
    {
        "Model":"Cerebras-GPT-1.3B",
        "URL":"https:\/\/huggingface.co\/cerebras\/Cerebras-GPT-1.3B",
        "full_model_name":"cerebras\/Cerebras-GPT-1.3B",
        "Parameters":1.3,
        "MMLU_average":0.2659293346,
        "arc:challenge|25":0.2372013652,
        "hellaswag|10":0.3290181239,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.2074074074,
        "MMLU_astronomy":0.2105263158,
        "MMLU_business_ethics":0.22,
        "MMLU_clinical_knowledge":0.2679245283,
        "MMLU_college_biology":0.2430555556,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.34,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.2658959538,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.21,
        "MMLU_conceptual_physics":0.2765957447,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2344827586,
        "MMLU_elementary_mathematics":0.2407407407,
        "MMLU_formal_logic":0.2619047619,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2032258065,
        "MMLU_high_school_chemistry":0.2266009852,
        "MMLU_high_school_computer_science":0.29,
        "MMLU_high_school_european_history":0.2606060606,
        "MMLU_high_school_geography":0.3131313131,
        "MMLU_high_school_government_and_politics":0.3471502591,
        "MMLU_high_school_macroeconomics":0.3230769231,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.2226890756,
        "MMLU_high_school_physics":0.238410596,
        "MMLU_high_school_psychology":0.3431192661,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2205882353,
        "MMLU_high_school_world_history":0.2742616034,
        "MMLU_human_aging":0.2331838565,
        "MMLU_human_sexuality":0.2366412214,
        "MMLU_international_law":0.2809917355,
        "MMLU_jurisprudence":0.2407407407,
        "MMLU_logical_fallacies":0.3006134969,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.1941747573,
        "MMLU_marketing":0.2948717949,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.2464878672,
        "MMLU_moral_disputes":0.2630057803,
        "MMLU_moral_scenarios":0.2368715084,
        "MMLU_nutrition":0.2941176471,
        "MMLU_philosophy":0.2636655949,
        "MMLU_prehistory":0.2654320988,
        "MMLU_professional_accounting":0.280141844,
        "MMLU_professional_law":0.2379400261,
        "MMLU_professional_medicine":0.4191176471,
        "MMLU_professional_psychology":0.2777777778,
        "MMLU_public_relations":0.1636363636,
        "MMLU_security_studies":0.2367346939,
        "MMLU_sociology":0.2288557214,
        "MMLU_us_foreign_policy":0.19,
        "MMLU_virology":0.3313253012,
        "MMLU_world_religions":0.3040935673
    },
    {
        "Model":"pythia-12b-pre-v8-12.5k-steps",
        "URL":"https:\/\/huggingface.co\/OpenAssistant\/pythia-12b-pre-v8-12.5k-steps",
        "full_model_name":"OpenAssistant\/pythia-12b-pre-v8-12.5k-steps",
        "Parameters":12.0,
        "MMLU_average":0.2658025533,
        "arc:challenge|25":0.385665529,
        "hellaswag|10":0.5109539932,
        "MMLU_abstract_algebra":0.36,
        "MMLU_anatomy":0.2148148148,
        "MMLU_astronomy":0.2960526316,
        "MMLU_business_ethics":0.24,
        "MMLU_clinical_knowledge":0.2566037736,
        "MMLU_college_biology":0.2777777778,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.22,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.3005780347,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.36,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2192982456,
        "MMLU_electrical_engineering":0.275862069,
        "MMLU_elementary_mathematics":0.2169312169,
        "MMLU_formal_logic":0.2063492063,
        "MMLU_global_facts":0.21,
        "MMLU_high_school_biology":0.2129032258,
        "MMLU_high_school_chemistry":0.2512315271,
        "MMLU_high_school_computer_science":0.26,
        "MMLU_high_school_european_history":0.2121212121,
        "MMLU_high_school_geography":0.2525252525,
        "MMLU_high_school_government_and_politics":0.2694300518,
        "MMLU_high_school_macroeconomics":0.2487179487,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.2478991597,
        "MMLU_high_school_physics":0.2185430464,
        "MMLU_high_school_psychology":0.2165137615,
        "MMLU_high_school_statistics":0.2037037037,
        "MMLU_high_school_us_history":0.2647058824,
        "MMLU_high_school_world_history":0.2320675105,
        "MMLU_human_aging":0.2959641256,
        "MMLU_human_sexuality":0.3053435115,
        "MMLU_international_law":0.3140495868,
        "MMLU_jurisprudence":0.2962962963,
        "MMLU_logical_fallacies":0.2576687117,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.2330097087,
        "MMLU_marketing":0.3162393162,
        "MMLU_medical_genetics":0.36,
        "MMLU_miscellaneous":0.2745849298,
        "MMLU_moral_disputes":0.3063583815,
        "MMLU_moral_scenarios":0.2413407821,
        "MMLU_nutrition":0.2973856209,
        "MMLU_philosophy":0.3247588424,
        "MMLU_prehistory":0.2932098765,
        "MMLU_professional_accounting":0.2304964539,
        "MMLU_professional_law":0.2920469361,
        "MMLU_professional_medicine":0.2977941176,
        "MMLU_professional_psychology":0.2663398693,
        "MMLU_public_relations":0.2818181818,
        "MMLU_security_studies":0.2367346939,
        "MMLU_sociology":0.263681592,
        "MMLU_us_foreign_policy":0.32,
        "MMLU_virology":0.2710843373,
        "MMLU_world_religions":0.3391812865
    },
    {
        "Model":"pythia-160m-hq-emails",
        "URL":"https:\/\/huggingface.co\/postbot\/pythia-160m-hq-emails",
        "full_model_name":"postbot\/pythia-160m-hq-emails",
        "Parameters":0.16,
        "MMLU_average":0.2657624681,
        "arc:challenge|25":0.1988054608,
        "hellaswag|10":0.2813184625,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.3481481481,
        "MMLU_astronomy":0.2105263158,
        "MMLU_business_ethics":0.2,
        "MMLU_clinical_knowledge":0.2603773585,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.19,
        "MMLU_college_computer_science":0.3,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.22,
        "MMLU_conceptual_physics":0.2255319149,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.3492063492,
        "MMLU_global_facts":0.26,
        "MMLU_high_school_biology":0.3161290323,
        "MMLU_high_school_chemistry":0.275862069,
        "MMLU_high_school_computer_science":0.24,
        "MMLU_high_school_european_history":0.2242424242,
        "MMLU_high_school_geography":0.3181818182,
        "MMLU_high_school_government_and_politics":0.3471502591,
        "MMLU_high_school_macroeconomics":0.3128205128,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.3151260504,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.2513761468,
        "MMLU_high_school_statistics":0.3842592593,
        "MMLU_high_school_us_history":0.2401960784,
        "MMLU_high_school_world_history":0.1898734177,
        "MMLU_human_aging":0.2421524664,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.3140495868,
        "MMLU_jurisprudence":0.2407407407,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.1607142857,
        "MMLU_management":0.213592233,
        "MMLU_marketing":0.2222222222,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.2745849298,
        "MMLU_moral_disputes":0.2080924855,
        "MMLU_moral_scenarios":0.2491620112,
        "MMLU_nutrition":0.2777777778,
        "MMLU_philosophy":0.2990353698,
        "MMLU_prehistory":0.2469135802,
        "MMLU_professional_accounting":0.2375886525,
        "MMLU_professional_law":0.2333767927,
        "MMLU_professional_medicine":0.4338235294,
        "MMLU_professional_psychology":0.2385620915,
        "MMLU_public_relations":0.2636363636,
        "MMLU_security_studies":0.3918367347,
        "MMLU_sociology":0.2338308458,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.2530120482,
        "MMLU_world_religions":0.2280701754
    },
    {
        "Model":"jerma985",
        "URL":"https:\/\/huggingface.co\/huggingtweets\/jerma985",
        "full_model_name":"huggingtweets\/jerma985",
        "Parameters":null,
        "MMLU_average":0.2656934275,
        "arc:challenge|25":0.20221843,
        "hellaswag|10":0.2885879307,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.2222222222,
        "MMLU_astronomy":0.1710526316,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.3018867925,
        "MMLU_college_biology":0.2152777778,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.1907514451,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.22,
        "MMLU_conceptual_physics":0.3319148936,
        "MMLU_econometrics":0.2192982456,
        "MMLU_electrical_engineering":0.2275862069,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.23,
        "MMLU_high_school_biology":0.3,
        "MMLU_high_school_chemistry":0.275862069,
        "MMLU_high_school_computer_science":0.2,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.3535353535,
        "MMLU_high_school_government_and_politics":0.3678756477,
        "MMLU_high_school_macroeconomics":0.3564102564,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.3277310924,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.3486238532,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2109704641,
        "MMLU_human_aging":0.1076233184,
        "MMLU_human_sexuality":0.213740458,
        "MMLU_international_law":0.2231404959,
        "MMLU_jurisprudence":0.2222222222,
        "MMLU_logical_fallacies":0.2331288344,
        "MMLU_machine_learning":0.1517857143,
        "MMLU_management":0.3786407767,
        "MMLU_marketing":0.1794871795,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.2081736909,
        "MMLU_moral_disputes":0.2080924855,
        "MMLU_moral_scenarios":0.243575419,
        "MMLU_nutrition":0.2745098039,
        "MMLU_philosophy":0.2733118971,
        "MMLU_prehistory":0.225308642,
        "MMLU_professional_accounting":0.2588652482,
        "MMLU_professional_law":0.2398956975,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2598039216,
        "MMLU_public_relations":0.2636363636,
        "MMLU_security_studies":0.4,
        "MMLU_sociology":0.2338308458,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.1987951807,
        "MMLU_world_religions":0.216374269
    },
    {
        "Model":"pile-7b",
        "URL":"https:\/\/huggingface.co\/Kunhao\/pile-7b",
        "full_model_name":"Kunhao\/pile-7b",
        "Parameters":7.0,
        "MMLU_average":0.2654970847,
        "arc:challenge|25":0.2380546075,
        "hellaswag|10":0.326926907,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.237037037,
        "MMLU_astronomy":0.2631578947,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.2113207547,
        "MMLU_college_biology":0.2083333333,
        "MMLU_college_chemistry":0.25,
        "MMLU_college_computer_science":0.35,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.2427745665,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.22,
        "MMLU_conceptual_physics":0.2340425532,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2551724138,
        "MMLU_elementary_mathematics":0.2513227513,
        "MMLU_formal_logic":0.2063492063,
        "MMLU_global_facts":0.25,
        "MMLU_high_school_biology":0.2774193548,
        "MMLU_high_school_chemistry":0.2463054187,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.2545454545,
        "MMLU_high_school_geography":0.3585858586,
        "MMLU_high_school_government_and_politics":0.3678756477,
        "MMLU_high_school_macroeconomics":0.3102564103,
        "MMLU_high_school_mathematics":0.3333333333,
        "MMLU_high_school_microeconomics":0.218487395,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.2587155963,
        "MMLU_high_school_statistics":0.462962963,
        "MMLU_high_school_us_history":0.2794117647,
        "MMLU_high_school_world_history":0.2109704641,
        "MMLU_human_aging":0.2869955157,
        "MMLU_human_sexuality":0.2519083969,
        "MMLU_international_law":0.2561983471,
        "MMLU_jurisprudence":0.2222222222,
        "MMLU_logical_fallacies":0.226993865,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.1794871795,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2464878672,
        "MMLU_moral_disputes":0.2572254335,
        "MMLU_moral_scenarios":0.2368715084,
        "MMLU_nutrition":0.2549019608,
        "MMLU_philosophy":0.2411575563,
        "MMLU_prehistory":0.2407407407,
        "MMLU_professional_accounting":0.2482269504,
        "MMLU_professional_law":0.2490221643,
        "MMLU_professional_medicine":0.4227941176,
        "MMLU_professional_psychology":0.2467320261,
        "MMLU_public_relations":0.1909090909,
        "MMLU_security_studies":0.387755102,
        "MMLU_sociology":0.2587064677,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.2891566265,
        "MMLU_world_religions":0.2573099415
    },
    {
        "Model":"open_llama_7b_hf",
        "URL":"https:\/\/huggingface.co\/quantumaikr\/open_llama_7b_hf",
        "full_model_name":"quantumaikr\/open_llama_7b_hf",
        "Parameters":7.0,
        "MMLU_average":0.2654370755,
        "arc:challenge|25":0.2329351536,
        "hellaswag|10":0.2619996017,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.3185185185,
        "MMLU_astronomy":0.2894736842,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.2490566038,
        "MMLU_college_biology":0.2708333333,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.27,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.2369942197,
        "MMLU_college_physics":0.4019607843,
        "MMLU_computer_security":0.16,
        "MMLU_conceptual_physics":0.2425531915,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.2344827586,
        "MMLU_elementary_mathematics":0.2513227513,
        "MMLU_formal_logic":0.2222222222,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.3161290323,
        "MMLU_high_school_chemistry":0.315270936,
        "MMLU_high_school_computer_science":0.23,
        "MMLU_high_school_european_history":0.2484848485,
        "MMLU_high_school_geography":0.3131313131,
        "MMLU_high_school_government_and_politics":0.3005181347,
        "MMLU_high_school_macroeconomics":0.2128205128,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.3277310924,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.2844036697,
        "MMLU_high_school_statistics":0.4074074074,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.1898734177,
        "MMLU_human_aging":0.1883408072,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.1900826446,
        "MMLU_jurisprudence":0.212962963,
        "MMLU_logical_fallacies":0.226993865,
        "MMLU_machine_learning":0.2321428571,
        "MMLU_management":0.359223301,
        "MMLU_marketing":0.2008547009,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.2656449553,
        "MMLU_moral_disputes":0.2138728324,
        "MMLU_moral_scenarios":0.2491620112,
        "MMLU_nutrition":0.2745098039,
        "MMLU_philosophy":0.2861736334,
        "MMLU_prehistory":0.262345679,
        "MMLU_professional_accounting":0.2588652482,
        "MMLU_professional_law":0.2438070404,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2467320261,
        "MMLU_public_relations":0.2454545455,
        "MMLU_security_studies":0.293877551,
        "MMLU_sociology":0.2338308458,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.1506024096,
        "MMLU_world_religions":0.2397660819
    },
    {
        "Model":"Quokka_256m",
        "URL":"https:\/\/huggingface.co\/Corianas\/Quokka_256m",
        "full_model_name":"Corianas\/Quokka_256m",
        "Parameters":0.256,
        "MMLU_average":0.2648105944,
        "arc:challenge|25":0.1825938567,
        "hellaswag|10":0.2768372834,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.3185185185,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.2,
        "MMLU_clinical_knowledge":0.2264150943,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.27,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.1965317919,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.24,
        "MMLU_conceptual_physics":0.2510638298,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2827586207,
        "MMLU_elementary_mathematics":0.2433862434,
        "MMLU_formal_logic":0.1428571429,
        "MMLU_global_facts":0.27,
        "MMLU_high_school_biology":0.2806451613,
        "MMLU_high_school_chemistry":0.2906403941,
        "MMLU_high_school_computer_science":0.35,
        "MMLU_high_school_european_history":0.3454545455,
        "MMLU_high_school_geography":0.3333333333,
        "MMLU_high_school_government_and_politics":0.274611399,
        "MMLU_high_school_macroeconomics":0.3641025641,
        "MMLU_high_school_mathematics":0.2925925926,
        "MMLU_high_school_microeconomics":0.2058823529,
        "MMLU_high_school_physics":0.238410596,
        "MMLU_high_school_psychology":0.3211009174,
        "MMLU_high_school_statistics":0.2962962963,
        "MMLU_high_school_us_history":0.2990196078,
        "MMLU_high_school_world_history":0.2194092827,
        "MMLU_human_aging":0.2825112108,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2479338843,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.2944785276,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.3300970874,
        "MMLU_marketing":0.2222222222,
        "MMLU_medical_genetics":0.31,
        "MMLU_miscellaneous":0.2771392082,
        "MMLU_moral_disputes":0.2572254335,
        "MMLU_moral_scenarios":0.2502793296,
        "MMLU_nutrition":0.2549019608,
        "MMLU_philosophy":0.2347266881,
        "MMLU_prehistory":0.225308642,
        "MMLU_professional_accounting":0.2304964539,
        "MMLU_professional_law":0.2385919166,
        "MMLU_professional_medicine":0.4301470588,
        "MMLU_professional_psychology":0.2761437908,
        "MMLU_public_relations":0.2363636364,
        "MMLU_security_studies":0.2693877551,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.32,
        "MMLU_virology":0.186746988,
        "MMLU_world_religions":0.2456140351
    },
    {
        "Model":"pythia-6.9b-deduped",
        "URL":"https:\/\/huggingface.co\/EleutherAI\/pythia-6.9b-deduped",
        "full_model_name":"EleutherAI\/pythia-6.9b-deduped",
        "Parameters":6.9,
        "MMLU_average":0.2648094407,
        "arc:challenge|25":0.3728668942,
        "hellaswag|10":0.4925313683,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.237037037,
        "MMLU_astronomy":0.2368421053,
        "MMLU_business_ethics":0.2,
        "MMLU_clinical_knowledge":0.2528301887,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.18,
        "MMLU_college_computer_science":0.35,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.1965317919,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.24,
        "MMLU_conceptual_physics":0.2936170213,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.3034482759,
        "MMLU_elementary_mathematics":0.2407407407,
        "MMLU_formal_logic":0.2142857143,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.2387096774,
        "MMLU_high_school_chemistry":0.3054187192,
        "MMLU_high_school_computer_science":0.19,
        "MMLU_high_school_european_history":0.2363636364,
        "MMLU_high_school_geography":0.2373737374,
        "MMLU_high_school_government_and_politics":0.2538860104,
        "MMLU_high_school_macroeconomics":0.2538461538,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.218487395,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.2183486239,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2278481013,
        "MMLU_human_aging":0.3766816143,
        "MMLU_human_sexuality":0.2366412214,
        "MMLU_international_law":0.3305785124,
        "MMLU_jurisprudence":0.287037037,
        "MMLU_logical_fallacies":0.3128834356,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.2815533981,
        "MMLU_marketing":0.2478632479,
        "MMLU_medical_genetics":0.24,
        "MMLU_miscellaneous":0.2899106003,
        "MMLU_moral_disputes":0.2138728324,
        "MMLU_moral_scenarios":0.243575419,
        "MMLU_nutrition":0.2516339869,
        "MMLU_philosophy":0.2861736334,
        "MMLU_prehistory":0.2654320988,
        "MMLU_professional_accounting":0.280141844,
        "MMLU_professional_law":0.2555410691,
        "MMLU_professional_medicine":0.2132352941,
        "MMLU_professional_psychology":0.2777777778,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.3102040816,
        "MMLU_sociology":0.2388059701,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.3493975904,
        "MMLU_world_religions":0.298245614
    },
    {
        "Model":"TinyLlama-1.1B-intermediate-step-955k-token-2T",
        "URL":"https:\/\/huggingface.co\/TinyLlama\/TinyLlama-1.1B-intermediate-step-955k-token-2T",
        "full_model_name":"TinyLlama\/TinyLlama-1.1B-intermediate-step-955k-token-2T",
        "Parameters":1.1,
        "MMLU_average":0.2646738451,
        "arc:challenge|25":0.2755972696,
        "hellaswag|10":0.4186417048,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.3333333333,
        "MMLU_astronomy":0.2434210526,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.2113207547,
        "MMLU_college_biology":0.2638888889,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.37,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.2658959538,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.27,
        "MMLU_conceptual_physics":0.2042553191,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.275862069,
        "MMLU_elementary_mathematics":0.2857142857,
        "MMLU_formal_logic":0.1428571429,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2612903226,
        "MMLU_high_school_chemistry":0.3103448276,
        "MMLU_high_school_computer_science":0.31,
        "MMLU_high_school_european_history":0.2787878788,
        "MMLU_high_school_geography":0.2575757576,
        "MMLU_high_school_government_and_politics":0.2279792746,
        "MMLU_high_school_macroeconomics":0.2384615385,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.2268907563,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.223853211,
        "MMLU_high_school_statistics":0.3842592593,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.253164557,
        "MMLU_human_aging":0.2376681614,
        "MMLU_human_sexuality":0.1984732824,
        "MMLU_international_law":0.3223140496,
        "MMLU_jurisprudence":0.2407407407,
        "MMLU_logical_fallacies":0.3006134969,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.2427184466,
        "MMLU_marketing":0.2564102564,
        "MMLU_medical_genetics":0.2,
        "MMLU_miscellaneous":0.2771392082,
        "MMLU_moral_disputes":0.2427745665,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.2516339869,
        "MMLU_philosophy":0.2958199357,
        "MMLU_prehistory":0.3055555556,
        "MMLU_professional_accounting":0.2659574468,
        "MMLU_professional_law":0.222946545,
        "MMLU_professional_medicine":0.4448529412,
        "MMLU_professional_psychology":0.2287581699,
        "MMLU_public_relations":0.2090909091,
        "MMLU_security_studies":0.2693877551,
        "MMLU_sociology":0.2587064677,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.2168674699,
        "MMLU_world_religions":0.2865497076
    },
    {
        "Model":"gpt-neo-2.7B",
        "URL":"https:\/\/huggingface.co\/EleutherAI\/gpt-neo-2.7B",
        "full_model_name":"EleutherAI\/gpt-neo-2.7B",
        "Parameters":2.7,
        "MMLU_average":0.2645397884,
        "arc:challenge|25":0.3105802048,
        "hellaswag|10":0.4221270663,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.2074074074,
        "MMLU_astronomy":0.1907894737,
        "MMLU_business_ethics":0.29,
        "MMLU_clinical_knowledge":0.2641509434,
        "MMLU_college_biology":0.2638888889,
        "MMLU_college_chemistry":0.23,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.2543352601,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2765957447,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.2551724138,
        "MMLU_elementary_mathematics":0.291005291,
        "MMLU_formal_logic":0.1746031746,
        "MMLU_global_facts":0.22,
        "MMLU_high_school_biology":0.235483871,
        "MMLU_high_school_chemistry":0.2463054187,
        "MMLU_high_school_computer_science":0.35,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.3333333333,
        "MMLU_high_school_government_and_politics":0.2590673575,
        "MMLU_high_school_macroeconomics":0.3435897436,
        "MMLU_high_school_mathematics":0.237037037,
        "MMLU_high_school_microeconomics":0.256302521,
        "MMLU_high_school_physics":0.2317880795,
        "MMLU_high_school_psychology":0.3100917431,
        "MMLU_high_school_statistics":0.3935185185,
        "MMLU_high_school_us_history":0.1960784314,
        "MMLU_high_school_world_history":0.223628692,
        "MMLU_human_aging":0.1838565022,
        "MMLU_human_sexuality":0.2824427481,
        "MMLU_international_law":0.2314049587,
        "MMLU_jurisprudence":0.287037037,
        "MMLU_logical_fallacies":0.2576687117,
        "MMLU_machine_learning":0.1785714286,
        "MMLU_management":0.2621359223,
        "MMLU_marketing":0.2692307692,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.2388250319,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2648044693,
        "MMLU_nutrition":0.3169934641,
        "MMLU_philosophy":0.3151125402,
        "MMLU_prehistory":0.3086419753,
        "MMLU_professional_accounting":0.2553191489,
        "MMLU_professional_law":0.2438070404,
        "MMLU_professional_medicine":0.4301470588,
        "MMLU_professional_psychology":0.2745098039,
        "MMLU_public_relations":0.1909090909,
        "MMLU_security_studies":0.2857142857,
        "MMLU_sociology":0.223880597,
        "MMLU_us_foreign_policy":0.3,
        "MMLU_virology":0.313253012,
        "MMLU_world_religions":0.2807017544
    },
    {
        "Model":"OPT-2.7B-Nerys-v2",
        "URL":"https:\/\/huggingface.co\/KoboldAI\/OPT-2.7B-Nerys-v2",
        "full_model_name":"KoboldAI\/OPT-2.7B-Nerys-v2",
        "Parameters":2.7,
        "MMLU_average":0.2643749817,
        "arc:challenge|25":0.3114334471,
        "hellaswag|10":0.4593706433,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.2518518519,
        "MMLU_astronomy":0.2236842105,
        "MMLU_business_ethics":0.15,
        "MMLU_clinical_knowledge":0.3132075472,
        "MMLU_college_biology":0.2777777778,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.3063583815,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.25,
        "MMLU_conceptual_physics":0.2468085106,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.2380952381,
        "MMLU_formal_logic":0.1825396825,
        "MMLU_global_facts":0.35,
        "MMLU_high_school_biology":0.2548387097,
        "MMLU_high_school_chemistry":0.3103448276,
        "MMLU_high_school_computer_science":0.31,
        "MMLU_high_school_european_history":0.2606060606,
        "MMLU_high_school_geography":0.2474747475,
        "MMLU_high_school_government_and_politics":0.3419689119,
        "MMLU_high_school_macroeconomics":0.3538461538,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.218487395,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.3449541284,
        "MMLU_high_school_statistics":0.4444444444,
        "MMLU_high_school_us_history":0.2352941176,
        "MMLU_high_school_world_history":0.253164557,
        "MMLU_human_aging":0.134529148,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.2561983471,
        "MMLU_jurisprudence":0.25,
        "MMLU_logical_fallacies":0.2331288344,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.4077669903,
        "MMLU_marketing":0.2136752137,
        "MMLU_medical_genetics":0.27,
        "MMLU_miscellaneous":0.2234993614,
        "MMLU_moral_disputes":0.2572254335,
        "MMLU_moral_scenarios":0.2324022346,
        "MMLU_nutrition":0.2385620915,
        "MMLU_philosophy":0.2797427653,
        "MMLU_prehistory":0.2438271605,
        "MMLU_professional_accounting":0.2375886525,
        "MMLU_professional_law":0.2685788787,
        "MMLU_professional_medicine":0.4154411765,
        "MMLU_professional_psychology":0.2663398693,
        "MMLU_public_relations":0.1909090909,
        "MMLU_security_studies":0.2653061224,
        "MMLU_sociology":0.2786069652,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.186746988,
        "MMLU_world_religions":0.1871345029
    },
    {
        "Model":"LLongMA-3b-LIMA",
        "URL":"https:\/\/huggingface.co\/KnutJaegersberg\/LLongMA-3b-LIMA",
        "full_model_name":"KnutJaegersberg\/LLongMA-3b-LIMA",
        "Parameters":3.0,
        "MMLU_average":0.2643344571,
        "arc:challenge|25":0.364334471,
        "hellaswag|10":0.4890460068,
        "MMLU_abstract_algebra":0.35,
        "MMLU_anatomy":0.3407407407,
        "MMLU_astronomy":0.25,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.279245283,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.25,
        "MMLU_college_computer_science":0.25,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.289017341,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.3,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.3172413793,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.1507936508,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.264516129,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.3,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.2727272727,
        "MMLU_high_school_government_and_politics":0.2227979275,
        "MMLU_high_school_macroeconomics":0.258974359,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.2058823529,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.2513761468,
        "MMLU_high_school_statistics":0.2083333333,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.2511210762,
        "MMLU_human_sexuality":0.1908396947,
        "MMLU_international_law":0.3553719008,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.282208589,
        "MMLU_machine_learning":0.2053571429,
        "MMLU_management":0.2330097087,
        "MMLU_marketing":0.2521367521,
        "MMLU_medical_genetics":0.25,
        "MMLU_miscellaneous":0.2924648787,
        "MMLU_moral_disputes":0.2803468208,
        "MMLU_moral_scenarios":0.2357541899,
        "MMLU_nutrition":0.2843137255,
        "MMLU_philosophy":0.3279742765,
        "MMLU_prehistory":0.287037037,
        "MMLU_professional_accounting":0.2765957447,
        "MMLU_professional_law":0.2646675359,
        "MMLU_professional_medicine":0.2683823529,
        "MMLU_professional_psychology":0.2647058824,
        "MMLU_public_relations":0.2636363636,
        "MMLU_security_studies":0.2163265306,
        "MMLU_sociology":0.2139303483,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.2469879518,
        "MMLU_world_religions":0.3157894737
    },
    {
        "Model":"results_2023-07-24T14-46-19.898757.json",
        "URL":"https:\/\/huggingface.co\/gpt2-xl\/results_2023-07-24T14-46-19.898757.json",
        "full_model_name":"gpt2-xl\/results_2023-07-24T14-46-19.898757.json",
        "Parameters":null,
        "MMLU_average":0.2643322732,
        "arc:challenge|25":0.2593856655,
        "hellaswag|10":0.398127863,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.2074074074,
        "MMLU_astronomy":0.1907894737,
        "MMLU_business_ethics":0.27,
        "MMLU_clinical_knowledge":0.2905660377,
        "MMLU_college_biology":0.2847222222,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.3,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.3063583815,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.29,
        "MMLU_conceptual_physics":0.2978723404,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.2407407407,
        "MMLU_formal_logic":0.3412698413,
        "MMLU_global_facts":0.25,
        "MMLU_high_school_biology":0.2483870968,
        "MMLU_high_school_chemistry":0.2118226601,
        "MMLU_high_school_computer_science":0.29,
        "MMLU_high_school_european_history":0.3090909091,
        "MMLU_high_school_geography":0.2777777778,
        "MMLU_high_school_government_and_politics":0.2176165803,
        "MMLU_high_school_macroeconomics":0.3435897436,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.2226890756,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.328440367,
        "MMLU_high_school_statistics":0.3935185185,
        "MMLU_high_school_us_history":0.2254901961,
        "MMLU_high_school_world_history":0.2489451477,
        "MMLU_human_aging":0.1883408072,
        "MMLU_human_sexuality":0.2519083969,
        "MMLU_international_law":0.2809917355,
        "MMLU_jurisprudence":0.2962962963,
        "MMLU_logical_fallacies":0.2392638037,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.2621359223,
        "MMLU_marketing":0.264957265,
        "MMLU_medical_genetics":0.25,
        "MMLU_miscellaneous":0.2681992337,
        "MMLU_moral_disputes":0.2398843931,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2810457516,
        "MMLU_philosophy":0.2122186495,
        "MMLU_prehistory":0.2438271605,
        "MMLU_professional_accounting":0.2340425532,
        "MMLU_professional_law":0.259452412,
        "MMLU_professional_medicine":0.2095588235,
        "MMLU_professional_psychology":0.2401960784,
        "MMLU_public_relations":0.3181818182,
        "MMLU_security_studies":0.2897959184,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.22,
        "MMLU_virology":0.3373493976,
        "MMLU_world_religions":0.3274853801
    },
    {
        "Model":"Bean-3B",
        "URL":"https:\/\/huggingface.co\/acrastt\/Bean-3B",
        "full_model_name":"acrastt\/Bean-3B",
        "Parameters":3.0,
        "MMLU_average":0.2642876421,
        "arc:challenge|25":0.3686006826,
        "hellaswag|10":0.5371439952,
        "MMLU_abstract_algebra":0.21,
        "MMLU_anatomy":0.237037037,
        "MMLU_astronomy":0.2763157895,
        "MMLU_business_ethics":0.26,
        "MMLU_clinical_knowledge":0.2905660377,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.19,
        "MMLU_college_computer_science":0.22,
        "MMLU_college_mathematics":0.37,
        "MMLU_college_medicine":0.1965317919,
        "MMLU_college_physics":0.1764705882,
        "MMLU_computer_security":0.31,
        "MMLU_conceptual_physics":0.3106382979,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.2275862069,
        "MMLU_elementary_mathematics":0.2698412698,
        "MMLU_formal_logic":0.2380952381,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2709677419,
        "MMLU_high_school_chemistry":0.2906403941,
        "MMLU_high_school_computer_science":0.23,
        "MMLU_high_school_european_history":0.2363636364,
        "MMLU_high_school_geography":0.2373737374,
        "MMLU_high_school_government_and_politics":0.2279792746,
        "MMLU_high_school_macroeconomics":0.2358974359,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.2647058824,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.2623853211,
        "MMLU_high_school_statistics":0.1898148148,
        "MMLU_high_school_us_history":0.2205882353,
        "MMLU_high_school_world_history":0.2447257384,
        "MMLU_human_aging":0.4080717489,
        "MMLU_human_sexuality":0.2519083969,
        "MMLU_international_law":0.3553719008,
        "MMLU_jurisprudence":0.3055555556,
        "MMLU_logical_fallacies":0.2392638037,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.3009708738,
        "MMLU_marketing":0.2692307692,
        "MMLU_medical_genetics":0.22,
        "MMLU_miscellaneous":0.3052362708,
        "MMLU_moral_disputes":0.2861271676,
        "MMLU_moral_scenarios":0.2368715084,
        "MMLU_nutrition":0.2647058824,
        "MMLU_philosophy":0.2926045016,
        "MMLU_prehistory":0.2685185185,
        "MMLU_professional_accounting":0.2730496454,
        "MMLU_professional_law":0.2425032595,
        "MMLU_professional_medicine":0.1948529412,
        "MMLU_professional_psychology":0.272875817,
        "MMLU_public_relations":0.3363636364,
        "MMLU_security_studies":0.3142857143,
        "MMLU_sociology":0.2736318408,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.3192771084,
        "MMLU_world_religions":0.2865497076
    },
    {
        "Model":"wangchanglm-7.5B-sft-en-sharded",
        "URL":"https:\/\/huggingface.co\/pythainlp\/wangchanglm-7.5B-sft-en-sharded",
        "full_model_name":"pythainlp\/wangchanglm-7.5B-sft-en-sharded",
        "Parameters":7.5,
        "MMLU_average":0.2637223298,
        "arc:challenge|25":0.3259385666,
        "hellaswag|10":0.4523999203,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.2444444444,
        "MMLU_astronomy":0.1710526316,
        "MMLU_business_ethics":0.26,
        "MMLU_clinical_knowledge":0.2339622642,
        "MMLU_college_biology":0.2361111111,
        "MMLU_college_chemistry":0.32,
        "MMLU_college_computer_science":0.41,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.2832369942,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.22,
        "MMLU_conceptual_physics":0.2,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2137931034,
        "MMLU_elementary_mathematics":0.2619047619,
        "MMLU_formal_logic":0.2222222222,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.3161290323,
        "MMLU_high_school_chemistry":0.2315270936,
        "MMLU_high_school_computer_science":0.19,
        "MMLU_high_school_european_history":0.2242424242,
        "MMLU_high_school_geography":0.3181818182,
        "MMLU_high_school_government_and_politics":0.3678756477,
        "MMLU_high_school_macroeconomics":0.358974359,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.2941176471,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.2605504587,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.2953586498,
        "MMLU_human_aging":0.2869955157,
        "MMLU_human_sexuality":0.2671755725,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.2760736196,
        "MMLU_machine_learning":0.2232142857,
        "MMLU_management":0.1844660194,
        "MMLU_marketing":0.2136752137,
        "MMLU_medical_genetics":0.31,
        "MMLU_miscellaneous":0.1966794381,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2908496732,
        "MMLU_philosophy":0.2250803859,
        "MMLU_prehistory":0.2592592593,
        "MMLU_professional_accounting":0.2234042553,
        "MMLU_professional_law":0.2346805737,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2401960784,
        "MMLU_public_relations":0.2272727273,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.2587064677,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.3012048193,
        "MMLU_world_religions":0.3333333333
    },
    {
        "Model":"orca_mini_3b_juniper",
        "URL":"https:\/\/huggingface.co\/frank098\/orca_mini_3b_juniper",
        "full_model_name":"frank098\/orca_mini_3b_juniper",
        "Parameters":3.0,
        "MMLU_average":0.2636692626,
        "arc:challenge|25":0.3848122867,
        "hellaswag|10":0.4781915953,
        "MMLU_abstract_algebra":0.37,
        "MMLU_anatomy":0.3111111111,
        "MMLU_astronomy":0.2763157895,
        "MMLU_business_ethics":0.32,
        "MMLU_clinical_knowledge":0.241509434,
        "MMLU_college_biology":0.2916666667,
        "MMLU_college_chemistry":0.16,
        "MMLU_college_computer_science":0.28,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.2485549133,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.27,
        "MMLU_conceptual_physics":0.2765957447,
        "MMLU_econometrics":0.2192982456,
        "MMLU_electrical_engineering":0.3379310345,
        "MMLU_elementary_mathematics":0.2380952381,
        "MMLU_formal_logic":0.2142857143,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2451612903,
        "MMLU_high_school_chemistry":0.2512315271,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.2606060606,
        "MMLU_high_school_geography":0.297979798,
        "MMLU_high_school_government_and_politics":0.2849740933,
        "MMLU_high_school_macroeconomics":0.2461538462,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.2142857143,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.2403669725,
        "MMLU_high_school_statistics":0.2083333333,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.3183856502,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.3636363636,
        "MMLU_jurisprudence":0.2314814815,
        "MMLU_logical_fallacies":0.1840490798,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.2912621359,
        "MMLU_marketing":0.3076923077,
        "MMLU_medical_genetics":0.22,
        "MMLU_miscellaneous":0.2950191571,
        "MMLU_moral_disputes":0.2919075145,
        "MMLU_moral_scenarios":0.2648044693,
        "MMLU_nutrition":0.2679738562,
        "MMLU_philosophy":0.2990353698,
        "MMLU_prehistory":0.2592592593,
        "MMLU_professional_accounting":0.2446808511,
        "MMLU_professional_law":0.2496740548,
        "MMLU_professional_medicine":0.1764705882,
        "MMLU_professional_psychology":0.2598039216,
        "MMLU_public_relations":0.2454545455,
        "MMLU_security_studies":0.2612244898,
        "MMLU_sociology":0.223880597,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2409638554,
        "MMLU_world_religions":0.2514619883
    },
    {
        "Model":"Griffin-3B",
        "URL":"https:\/\/huggingface.co\/acrastt\/Griffin-3B",
        "full_model_name":"acrastt\/Griffin-3B",
        "Parameters":3.0,
        "MMLU_average":0.2636238039,
        "arc:challenge|25":0.3805460751,
        "hellaswag|10":0.5424218283,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.2222222222,
        "MMLU_astronomy":0.2960526316,
        "MMLU_business_ethics":0.37,
        "MMLU_clinical_knowledge":0.2490566038,
        "MMLU_college_biology":0.2430555556,
        "MMLU_college_chemistry":0.25,
        "MMLU_college_computer_science":0.21,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.2196531792,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.35,
        "MMLU_conceptual_physics":0.3361702128,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.2068965517,
        "MMLU_elementary_mathematics":0.2830687831,
        "MMLU_formal_logic":0.2063492063,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.2161290323,
        "MMLU_high_school_chemistry":0.2315270936,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2242424242,
        "MMLU_high_school_geography":0.196969697,
        "MMLU_high_school_government_and_politics":0.2331606218,
        "MMLU_high_school_macroeconomics":0.2358974359,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.2899159664,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.2422018349,
        "MMLU_high_school_statistics":0.1666666667,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2995780591,
        "MMLU_human_aging":0.4125560538,
        "MMLU_human_sexuality":0.2442748092,
        "MMLU_international_law":0.305785124,
        "MMLU_jurisprudence":0.3055555556,
        "MMLU_logical_fallacies":0.226993865,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.2815533981,
        "MMLU_marketing":0.2820512821,
        "MMLU_medical_genetics":0.22,
        "MMLU_miscellaneous":0.2911877395,
        "MMLU_moral_disputes":0.2658959538,
        "MMLU_moral_scenarios":0.2413407821,
        "MMLU_nutrition":0.2516339869,
        "MMLU_philosophy":0.2733118971,
        "MMLU_prehistory":0.262345679,
        "MMLU_professional_accounting":0.2872340426,
        "MMLU_professional_law":0.2425032595,
        "MMLU_professional_medicine":0.2095588235,
        "MMLU_professional_psychology":0.2630718954,
        "MMLU_public_relations":0.3181818182,
        "MMLU_security_studies":0.2448979592,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.3,
        "MMLU_virology":0.3192771084,
        "MMLU_world_religions":0.3274853801
    },
    {
        "Model":"pyg-instruct-wizardlm",
        "URL":"https:\/\/huggingface.co\/Lazycuber\/pyg-instruct-wizardlm",
        "full_model_name":"Lazycuber\/pyg-instruct-wizardlm",
        "Parameters":null,
        "MMLU_average":0.2633225678,
        "arc:challenge|25":0.3703071672,
        "hellaswag|10":0.4907388966,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.2444444444,
        "MMLU_astronomy":0.2631578947,
        "MMLU_business_ethics":0.31,
        "MMLU_clinical_knowledge":0.2641509434,
        "MMLU_college_biology":0.2638888889,
        "MMLU_college_chemistry":0.19,
        "MMLU_college_computer_science":0.18,
        "MMLU_college_mathematics":0.23,
        "MMLU_college_medicine":0.2312138728,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.29,
        "MMLU_conceptual_physics":0.3234042553,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.2551724138,
        "MMLU_elementary_mathematics":0.2486772487,
        "MMLU_formal_logic":0.2222222222,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.2612903226,
        "MMLU_high_school_chemistry":0.2610837438,
        "MMLU_high_school_computer_science":0.26,
        "MMLU_high_school_european_history":0.2909090909,
        "MMLU_high_school_geography":0.2171717172,
        "MMLU_high_school_government_and_politics":0.2227979275,
        "MMLU_high_school_macroeconomics":0.2512820513,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.218487395,
        "MMLU_high_school_physics":0.2052980132,
        "MMLU_high_school_psychology":0.2587155963,
        "MMLU_high_school_statistics":0.1666666667,
        "MMLU_high_school_us_history":0.2794117647,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.3408071749,
        "MMLU_human_sexuality":0.1984732824,
        "MMLU_international_law":0.3553719008,
        "MMLU_jurisprudence":0.3981481481,
        "MMLU_logical_fallacies":0.263803681,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.213592233,
        "MMLU_marketing":0.2820512821,
        "MMLU_medical_genetics":0.25,
        "MMLU_miscellaneous":0.3077905492,
        "MMLU_moral_disputes":0.2630057803,
        "MMLU_moral_scenarios":0.2703910615,
        "MMLU_nutrition":0.2843137255,
        "MMLU_philosophy":0.2893890675,
        "MMLU_prehistory":0.3024691358,
        "MMLU_professional_accounting":0.2730496454,
        "MMLU_professional_law":0.2724902216,
        "MMLU_professional_medicine":0.1801470588,
        "MMLU_professional_psychology":0.272875817,
        "MMLU_public_relations":0.2909090909,
        "MMLU_security_studies":0.2367346939,
        "MMLU_sociology":0.2686567164,
        "MMLU_us_foreign_policy":0.25,
        "MMLU_virology":0.3313253012,
        "MMLU_world_religions":0.3157894737
    },
    {
        "Model":"gpt2-large-conversational",
        "URL":"https:\/\/huggingface.co\/Locutusque\/gpt2-large-conversational",
        "full_model_name":"Locutusque\/gpt2-large-conversational",
        "Parameters":null,
        "MMLU_average":0.2632907193,
        "arc:challenge|25":0.2380546075,
        "hellaswag|10":0.3577972515,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.3259259259,
        "MMLU_astronomy":0.2302631579,
        "MMLU_business_ethics":0.25,
        "MMLU_clinical_knowledge":0.2603773585,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.15,
        "MMLU_college_computer_science":0.27,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.2543352601,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.32,
        "MMLU_conceptual_physics":0.3191489362,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.3103448276,
        "MMLU_elementary_mathematics":0.2698412698,
        "MMLU_formal_logic":0.1666666667,
        "MMLU_global_facts":0.36,
        "MMLU_high_school_biology":0.2580645161,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.31,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.2626262626,
        "MMLU_high_school_government_and_politics":0.2124352332,
        "MMLU_high_school_macroeconomics":0.2615384615,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.2142857143,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.2458715596,
        "MMLU_high_school_statistics":0.4537037037,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.253164557,
        "MMLU_human_aging":0.1928251121,
        "MMLU_human_sexuality":0.1908396947,
        "MMLU_international_law":0.3140495868,
        "MMLU_jurisprudence":0.2962962963,
        "MMLU_logical_fallacies":0.2760736196,
        "MMLU_machine_learning":0.2410714286,
        "MMLU_management":0.3786407767,
        "MMLU_marketing":0.2606837607,
        "MMLU_medical_genetics":0.17,
        "MMLU_miscellaneous":0.2477650064,
        "MMLU_moral_disputes":0.2687861272,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2450980392,
        "MMLU_philosophy":0.2668810289,
        "MMLU_prehistory":0.2901234568,
        "MMLU_professional_accounting":0.2730496454,
        "MMLU_professional_law":0.2327249022,
        "MMLU_professional_medicine":0.2647058824,
        "MMLU_professional_psychology":0.2385620915,
        "MMLU_public_relations":0.2090909091,
        "MMLU_security_studies":0.2163265306,
        "MMLU_sociology":0.2587064677,
        "MMLU_us_foreign_policy":0.31,
        "MMLU_virology":0.313253012,
        "MMLU_world_religions":0.2923976608
    },
    {
        "Model":"gpt2-xl-alpaca",
        "URL":"https:\/\/huggingface.co\/Rachneet\/gpt2-xl-alpaca",
        "full_model_name":"Rachneet\/gpt2-xl-alpaca",
        "Parameters":null,
        "MMLU_average":0.263065388,
        "arc:challenge|25":0.2312286689,
        "hellaswag|10":0.3598884684,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.2296296296,
        "MMLU_astronomy":0.2434210526,
        "MMLU_business_ethics":0.22,
        "MMLU_clinical_knowledge":0.3056603774,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.27,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.3410404624,
        "MMLU_college_physics":0.3333333333,
        "MMLU_computer_security":0.16,
        "MMLU_conceptual_physics":0.2382978723,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2068965517,
        "MMLU_elementary_mathematics":0.2513227513,
        "MMLU_formal_logic":0.3650793651,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.2838709677,
        "MMLU_high_school_chemistry":0.2413793103,
        "MMLU_high_school_computer_science":0.2,
        "MMLU_high_school_european_history":0.2787878788,
        "MMLU_high_school_geography":0.3535353535,
        "MMLU_high_school_government_and_politics":0.3056994819,
        "MMLU_high_school_macroeconomics":0.3615384615,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.281512605,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.3394495413,
        "MMLU_high_school_statistics":0.4675925926,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.194092827,
        "MMLU_human_aging":0.1076233184,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.1570247934,
        "MMLU_jurisprudence":0.2222222222,
        "MMLU_logical_fallacies":0.3006134969,
        "MMLU_machine_learning":0.1875,
        "MMLU_management":0.3009708738,
        "MMLU_marketing":0.2307692308,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.2464878672,
        "MMLU_moral_disputes":0.2427745665,
        "MMLU_moral_scenarios":0.2592178771,
        "MMLU_nutrition":0.2581699346,
        "MMLU_philosophy":0.231511254,
        "MMLU_prehistory":0.225308642,
        "MMLU_professional_accounting":0.2553191489,
        "MMLU_professional_law":0.2464146023,
        "MMLU_professional_medicine":0.4301470588,
        "MMLU_professional_psychology":0.2467320261,
        "MMLU_public_relations":0.2454545455,
        "MMLU_security_studies":0.3795918367,
        "MMLU_sociology":0.2288557214,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.2710843373,
        "MMLU_world_religions":0.2631578947
    },
    {
        "Model":"LaMini-GPT-774M",
        "URL":"https:\/\/huggingface.co\/MBZUAI\/LaMini-GPT-774M",
        "full_model_name":"MBZUAI\/LaMini-GPT-774M",
        "Parameters":0.774,
        "MMLU_average":0.2629991205,
        "arc:challenge|25":0.2534129693,
        "hellaswag|10":0.3617805218,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.2296296296,
        "MMLU_astronomy":0.1842105263,
        "MMLU_business_ethics":0.25,
        "MMLU_clinical_knowledge":0.2490566038,
        "MMLU_college_biology":0.2291666667,
        "MMLU_college_chemistry":0.23,
        "MMLU_college_computer_science":0.2,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.2312138728,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.32,
        "MMLU_conceptual_physics":0.3319148936,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.2344827586,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.2301587302,
        "MMLU_global_facts":0.22,
        "MMLU_high_school_biology":0.3129032258,
        "MMLU_high_school_chemistry":0.2463054187,
        "MMLU_high_school_computer_science":0.35,
        "MMLU_high_school_european_history":0.2909090909,
        "MMLU_high_school_geography":0.3131313131,
        "MMLU_high_school_government_and_politics":0.3834196891,
        "MMLU_high_school_macroeconomics":0.2820512821,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.3025210084,
        "MMLU_high_school_physics":0.238410596,
        "MMLU_high_school_psychology":0.2623853211,
        "MMLU_high_school_statistics":0.3935185185,
        "MMLU_high_school_us_history":0.2647058824,
        "MMLU_high_school_world_history":0.3291139241,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2366412214,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.3006134969,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.2621359223,
        "MMLU_marketing":0.2094017094,
        "MMLU_medical_genetics":0.23,
        "MMLU_miscellaneous":0.2873563218,
        "MMLU_moral_disputes":0.225433526,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2450980392,
        "MMLU_philosophy":0.2926045016,
        "MMLU_prehistory":0.2962962963,
        "MMLU_professional_accounting":0.2375886525,
        "MMLU_professional_law":0.2444589309,
        "MMLU_professional_medicine":0.2022058824,
        "MMLU_professional_psychology":0.2450980392,
        "MMLU_public_relations":0.1727272727,
        "MMLU_security_studies":0.2081632653,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.32,
        "MMLU_virology":0.3373493976,
        "MMLU_world_religions":0.2280701754
    },
    {
        "Model":"Bloom_1b_Quantized",
        "URL":"https:\/\/huggingface.co\/FabbriSimo01\/Bloom_1b_Quantized",
        "full_model_name":"FabbriSimo01\/Bloom_1b_Quantized",
        "Parameters":1.0,
        "MMLU_average":0.2628241421,
        "arc:challenge|25":0.2542662116,
        "hellaswag|10":0.3483369847,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.3555555556,
        "MMLU_astronomy":0.2236842105,
        "MMLU_business_ethics":0.2,
        "MMLU_clinical_knowledge":0.2566037736,
        "MMLU_college_biology":0.2638888889,
        "MMLU_college_chemistry":0.18,
        "MMLU_college_computer_science":0.35,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.2369942197,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.23,
        "MMLU_conceptual_physics":0.2042553191,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.3034482759,
        "MMLU_elementary_mathematics":0.2645502646,
        "MMLU_formal_logic":0.1507936508,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.2548387097,
        "MMLU_high_school_chemistry":0.2561576355,
        "MMLU_high_school_computer_science":0.29,
        "MMLU_high_school_european_history":0.2909090909,
        "MMLU_high_school_geography":0.3333333333,
        "MMLU_high_school_government_and_politics":0.3626943005,
        "MMLU_high_school_macroeconomics":0.2666666667,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.2142857143,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.3064220183,
        "MMLU_high_school_statistics":0.4444444444,
        "MMLU_high_school_us_history":0.2647058824,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.2466367713,
        "MMLU_human_sexuality":0.1984732824,
        "MMLU_international_law":0.3801652893,
        "MMLU_jurisprudence":0.1944444444,
        "MMLU_logical_fallacies":0.3312883436,
        "MMLU_machine_learning":0.1785714286,
        "MMLU_management":0.4077669903,
        "MMLU_marketing":0.2051282051,
        "MMLU_medical_genetics":0.21,
        "MMLU_miscellaneous":0.2567049808,
        "MMLU_moral_disputes":0.2196531792,
        "MMLU_moral_scenarios":0.2312849162,
        "MMLU_nutrition":0.2450980392,
        "MMLU_philosophy":0.2926045016,
        "MMLU_prehistory":0.25,
        "MMLU_professional_accounting":0.2659574468,
        "MMLU_professional_law":0.2646675359,
        "MMLU_professional_medicine":0.3602941176,
        "MMLU_professional_psychology":0.2663398693,
        "MMLU_public_relations":0.1818181818,
        "MMLU_security_studies":0.2571428571,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.2228915663,
        "MMLU_world_religions":0.2807017544
    },
    {
        "Model":"oasst-pythia-6.9b-4000-steps",
        "URL":"https:\/\/huggingface.co\/dvruette\/oasst-pythia-6.9b-4000-steps",
        "full_model_name":"dvruette\/oasst-pythia-6.9b-4000-steps",
        "Parameters":6.9,
        "MMLU_average":0.26259679,
        "arc:challenge|25":0.3694539249,
        "hellaswag|10":0.4859589723,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.3333333333,
        "MMLU_astronomy":0.2894736842,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.3245283019,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.27,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.2427745665,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.31,
        "MMLU_conceptual_physics":0.2042553191,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.3172413793,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.1746031746,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.2580645161,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.32,
        "MMLU_high_school_european_history":0.2727272727,
        "MMLU_high_school_geography":0.2929292929,
        "MMLU_high_school_government_and_politics":0.2901554404,
        "MMLU_high_school_macroeconomics":0.2717948718,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.2935779817,
        "MMLU_high_school_statistics":0.25,
        "MMLU_high_school_us_history":0.2696078431,
        "MMLU_high_school_world_history":0.2573839662,
        "MMLU_human_aging":0.2107623318,
        "MMLU_human_sexuality":0.1984732824,
        "MMLU_international_law":0.3801652893,
        "MMLU_jurisprudence":0.2314814815,
        "MMLU_logical_fallacies":0.2883435583,
        "MMLU_machine_learning":0.1964285714,
        "MMLU_management":0.2233009709,
        "MMLU_marketing":0.2564102564,
        "MMLU_medical_genetics":0.2,
        "MMLU_miscellaneous":0.2503192848,
        "MMLU_moral_disputes":0.2919075145,
        "MMLU_moral_scenarios":0.2446927374,
        "MMLU_nutrition":0.2679738562,
        "MMLU_philosophy":0.3022508039,
        "MMLU_prehistory":0.3209876543,
        "MMLU_professional_accounting":0.2695035461,
        "MMLU_professional_law":0.2718383312,
        "MMLU_professional_medicine":0.1911764706,
        "MMLU_professional_psychology":0.2761437908,
        "MMLU_public_relations":0.2090909091,
        "MMLU_security_studies":0.2489795918,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.2289156627,
        "MMLU_world_religions":0.2923976608
    },
    {
        "Model":"TinyLlama-1.1B-intermediate-step-240k-503b",
        "URL":"https:\/\/huggingface.co\/PY007\/TinyLlama-1.1B-intermediate-step-240k-503b",
        "full_model_name":"PY007\/TinyLlama-1.1B-intermediate-step-240k-503b",
        "Parameters":1.1,
        "MMLU_average":0.2625778417,
        "arc:challenge|25":0.2593856655,
        "hellaswag|10":0.3857797252,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.1925925926,
        "MMLU_astronomy":0.2039473684,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2566037736,
        "MMLU_college_biology":0.3472222222,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.35,
        "MMLU_college_mathematics":0.23,
        "MMLU_college_medicine":0.2485549133,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.31,
        "MMLU_conceptual_physics":0.2680851064,
        "MMLU_econometrics":0.2192982456,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.2777777778,
        "MMLU_formal_logic":0.2301587302,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.1935483871,
        "MMLU_high_school_chemistry":0.197044335,
        "MMLU_high_school_computer_science":0.28,
        "MMLU_high_school_european_history":0.1939393939,
        "MMLU_high_school_geography":0.2070707071,
        "MMLU_high_school_government_and_politics":0.3316062176,
        "MMLU_high_school_macroeconomics":0.2794871795,
        "MMLU_high_school_mathematics":0.3333333333,
        "MMLU_high_school_microeconomics":0.2268907563,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.2348623853,
        "MMLU_high_school_statistics":0.3518518519,
        "MMLU_high_school_us_history":0.2745098039,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.269058296,
        "MMLU_human_sexuality":0.213740458,
        "MMLU_international_law":0.2727272727,
        "MMLU_jurisprudence":0.2407407407,
        "MMLU_logical_fallacies":0.2515337423,
        "MMLU_machine_learning":0.2232142857,
        "MMLU_management":0.3106796117,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.24,
        "MMLU_miscellaneous":0.2643678161,
        "MMLU_moral_disputes":0.2427745665,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2418300654,
        "MMLU_philosophy":0.2508038585,
        "MMLU_prehistory":0.2654320988,
        "MMLU_professional_accounting":0.2624113475,
        "MMLU_professional_law":0.2568448501,
        "MMLU_professional_medicine":0.2757352941,
        "MMLU_professional_psychology":0.2761437908,
        "MMLU_public_relations":0.3,
        "MMLU_security_studies":0.306122449,
        "MMLU_sociology":0.2935323383,
        "MMLU_us_foreign_policy":0.23,
        "MMLU_virology":0.2710843373,
        "MMLU_world_religions":0.298245614
    },
    {
        "Model":"h2ogpt-oasst1-512-20b",
        "URL":"https:\/\/huggingface.co\/h2oai\/h2ogpt-oasst1-512-20b",
        "full_model_name":"h2oai\/h2ogpt-oasst1-512-20b",
        "Parameters":20.0,
        "MMLU_average":0.2624989059,
        "arc:challenge|25":0.430887372,
        "hellaswag|10":0.537243577,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.3481481481,
        "MMLU_astronomy":0.3223684211,
        "MMLU_business_ethics":0.37,
        "MMLU_clinical_knowledge":0.2566037736,
        "MMLU_college_biology":0.2708333333,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.23,
        "MMLU_college_medicine":0.2138728324,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.37,
        "MMLU_conceptual_physics":0.2468085106,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.2689655172,
        "MMLU_elementary_mathematics":0.2804232804,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.28,
        "MMLU_high_school_biology":0.2096774194,
        "MMLU_high_school_chemistry":0.1822660099,
        "MMLU_high_school_computer_science":0.27,
        "MMLU_high_school_european_history":0.2484848485,
        "MMLU_high_school_geography":0.2323232323,
        "MMLU_high_school_government_and_politics":0.2642487047,
        "MMLU_high_school_macroeconomics":0.2051282051,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.2142857143,
        "MMLU_high_school_physics":0.2251655629,
        "MMLU_high_school_psychology":0.2165137615,
        "MMLU_high_school_statistics":0.1574074074,
        "MMLU_high_school_us_history":0.2794117647,
        "MMLU_high_school_world_history":0.2784810127,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2366412214,
        "MMLU_international_law":0.2975206612,
        "MMLU_jurisprudence":0.3240740741,
        "MMLU_logical_fallacies":0.3067484663,
        "MMLU_machine_learning":0.2053571429,
        "MMLU_management":0.2330097087,
        "MMLU_marketing":0.2606837607,
        "MMLU_medical_genetics":0.32,
        "MMLU_miscellaneous":0.2796934866,
        "MMLU_moral_disputes":0.3005780347,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.2810457516,
        "MMLU_philosophy":0.3118971061,
        "MMLU_prehistory":0.2685185185,
        "MMLU_professional_accounting":0.2588652482,
        "MMLU_professional_law":0.2438070404,
        "MMLU_professional_medicine":0.2022058824,
        "MMLU_professional_psychology":0.2663398693,
        "MMLU_public_relations":0.2272727273,
        "MMLU_security_studies":0.2204081633,
        "MMLU_sociology":0.263681592,
        "MMLU_us_foreign_policy":0.31,
        "MMLU_virology":0.2590361446,
        "MMLU_world_religions":0.3450292398
    },
    {
        "Model":"Guanaco-3B-Uncensored-v2-GPTQ",
        "URL":"https:\/\/huggingface.co\/TheBloke\/Guanaco-3B-Uncensored-v2-GPTQ",
        "full_model_name":"TheBloke\/Guanaco-3B-Uncensored-v2-GPTQ",
        "Parameters":3.0,
        "MMLU_average":0.2624938991,
        "arc:challenge|25":0.3703071672,
        "hellaswag|10":0.4772953595,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.2444444444,
        "MMLU_astronomy":0.3486842105,
        "MMLU_business_ethics":0.24,
        "MMLU_clinical_knowledge":0.2943396226,
        "MMLU_college_biology":0.2430555556,
        "MMLU_college_chemistry":0.27,
        "MMLU_college_computer_science":0.23,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.2543352601,
        "MMLU_college_physics":0.137254902,
        "MMLU_computer_security":0.29,
        "MMLU_conceptual_physics":0.1957446809,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.2619047619,
        "MMLU_formal_logic":0.2698412698,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.264516129,
        "MMLU_high_school_chemistry":0.2610837438,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.3383838384,
        "MMLU_high_school_government_and_politics":0.2331606218,
        "MMLU_high_school_macroeconomics":0.2794871795,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.2478991597,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.2678899083,
        "MMLU_high_school_statistics":0.2083333333,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.2616033755,
        "MMLU_human_aging":0.1883408072,
        "MMLU_human_sexuality":0.2366412214,
        "MMLU_international_law":0.3801652893,
        "MMLU_jurisprudence":0.2777777778,
        "MMLU_logical_fallacies":0.2883435583,
        "MMLU_machine_learning":0.2321428571,
        "MMLU_management":0.3203883495,
        "MMLU_marketing":0.2777777778,
        "MMLU_medical_genetics":0.23,
        "MMLU_miscellaneous":0.2503192848,
        "MMLU_moral_disputes":0.2803468208,
        "MMLU_moral_scenarios":0.2480446927,
        "MMLU_nutrition":0.2549019608,
        "MMLU_philosophy":0.2797427653,
        "MMLU_prehistory":0.2530864198,
        "MMLU_professional_accounting":0.2695035461,
        "MMLU_professional_law":0.2711864407,
        "MMLU_professional_medicine":0.1691176471,
        "MMLU_professional_psychology":0.2320261438,
        "MMLU_public_relations":0.3090909091,
        "MMLU_security_studies":0.2571428571,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.23,
        "MMLU_virology":0.2710843373,
        "MMLU_world_religions":0.3099415205
    },
    {
        "Model":"bloom-7b1",
        "URL":"https:\/\/huggingface.co\/bigscience\/bloom-7b1",
        "full_model_name":"bigscience\/bloom-7b1",
        "Parameters":7.0,
        "MMLU_average":0.2624622238,
        "arc:challenge|25":0.364334471,
        "hellaswag|10":0.4622585142,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.2444444444,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.26,
        "MMLU_clinical_knowledge":0.2754716981,
        "MMLU_college_biology":0.1944444444,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.23,
        "MMLU_conceptual_physics":0.3234042553,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2482758621,
        "MMLU_elementary_mathematics":0.2751322751,
        "MMLU_formal_logic":0.1666666667,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.2548387097,
        "MMLU_high_school_chemistry":0.2709359606,
        "MMLU_high_school_computer_science":0.35,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.2272727273,
        "MMLU_high_school_government_and_politics":0.207253886,
        "MMLU_high_school_macroeconomics":0.2307692308,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.2268907563,
        "MMLU_high_school_physics":0.238410596,
        "MMLU_high_school_psychology":0.247706422,
        "MMLU_high_school_statistics":0.3842592593,
        "MMLU_high_school_us_history":0.2647058824,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.3632286996,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.3636363636,
        "MMLU_jurisprudence":0.287037037,
        "MMLU_logical_fallacies":0.2576687117,
        "MMLU_machine_learning":0.2946428571,
        "MMLU_management":0.2524271845,
        "MMLU_marketing":0.2521367521,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.288633461,
        "MMLU_moral_disputes":0.2456647399,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2320261438,
        "MMLU_philosophy":0.2733118971,
        "MMLU_prehistory":0.262345679,
        "MMLU_professional_accounting":0.2659574468,
        "MMLU_professional_law":0.2529335072,
        "MMLU_professional_medicine":0.2095588235,
        "MMLU_professional_psychology":0.2614379085,
        "MMLU_public_relations":0.3272727273,
        "MMLU_security_studies":0.3020408163,
        "MMLU_sociology":0.2388059701,
        "MMLU_us_foreign_policy":0.21,
        "MMLU_virology":0.3072289157,
        "MMLU_world_religions":0.2923976608
    },
    {
        "Model":"gpt-YA-1-1_160M",
        "URL":"https:\/\/huggingface.co\/BreadAi\/gpt-YA-1-1_160M",
        "full_model_name":"BreadAi\/gpt-YA-1-1_160M",
        "Parameters":0.16,
        "MMLU_average":0.262450425,
        "arc:challenge|25":0.1919795222,
        "hellaswag|10":0.2629954192,
        "MMLU_abstract_algebra":0.2,
        "MMLU_anatomy":0.237037037,
        "MMLU_astronomy":0.2960526316,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.1962264151,
        "MMLU_college_biology":0.2430555556,
        "MMLU_college_chemistry":0.32,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.2369942197,
        "MMLU_college_physics":0.362745098,
        "MMLU_computer_security":0.22,
        "MMLU_conceptual_physics":0.3063829787,
        "MMLU_econometrics":0.1666666667,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.2830687831,
        "MMLU_formal_logic":0.3253968254,
        "MMLU_global_facts":0.19,
        "MMLU_high_school_biology":0.3161290323,
        "MMLU_high_school_chemistry":0.275862069,
        "MMLU_high_school_computer_science":0.2,
        "MMLU_high_school_european_history":0.2848484848,
        "MMLU_high_school_geography":0.3131313131,
        "MMLU_high_school_government_and_politics":0.3626943005,
        "MMLU_high_school_macroeconomics":0.3384615385,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.3403361345,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.2733944954,
        "MMLU_high_school_statistics":0.4675925926,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.2067510549,
        "MMLU_human_aging":0.1704035874,
        "MMLU_human_sexuality":0.2442748092,
        "MMLU_international_law":0.132231405,
        "MMLU_jurisprudence":0.212962963,
        "MMLU_logical_fallacies":0.2331288344,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.2621359223,
        "MMLU_marketing":0.1965811966,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.2605363985,
        "MMLU_moral_disputes":0.2167630058,
        "MMLU_moral_scenarios":0.2726256983,
        "MMLU_nutrition":0.2679738562,
        "MMLU_philosophy":0.2411575563,
        "MMLU_prehistory":0.212962963,
        "MMLU_professional_accounting":0.2446808511,
        "MMLU_professional_law":0.2411994785,
        "MMLU_professional_medicine":0.3676470588,
        "MMLU_professional_psychology":0.2205882353,
        "MMLU_public_relations":0.2272727273,
        "MMLU_security_studies":0.3673469388,
        "MMLU_sociology":0.2985074627,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.1927710843,
        "MMLU_world_religions":0.1812865497
    },
    {
        "Model":"gpt2_open-platypus",
        "URL":"https:\/\/huggingface.co\/lgaalves\/gpt2_open-platypus",
        "full_model_name":"lgaalves\/gpt2_open-platypus",
        "Parameters":null,
        "MMLU_average":0.2619429444,
        "arc:challenge|25":0.1877133106,
        "hellaswag|10":0.293766182,
        "MMLU_abstract_algebra":0.2,
        "MMLU_anatomy":0.2962962963,
        "MMLU_astronomy":0.25,
        "MMLU_business_ethics":0.15,
        "MMLU_clinical_knowledge":0.2981132075,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.33,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.2427745665,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.18,
        "MMLU_conceptual_physics":0.2510638298,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2965517241,
        "MMLU_elementary_mathematics":0.2513227513,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.14,
        "MMLU_high_school_biology":0.2451612903,
        "MMLU_high_school_chemistry":0.2660098522,
        "MMLU_high_school_computer_science":0.21,
        "MMLU_high_school_european_history":0.2303030303,
        "MMLU_high_school_geography":0.3535353535,
        "MMLU_high_school_government_and_politics":0.3678756477,
        "MMLU_high_school_macroeconomics":0.3615384615,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.218487395,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.3486238532,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.1076233184,
        "MMLU_human_sexuality":0.2977099237,
        "MMLU_international_law":0.2561983471,
        "MMLU_jurisprudence":0.1851851852,
        "MMLU_logical_fallacies":0.2883435583,
        "MMLU_machine_learning":0.1517857143,
        "MMLU_management":0.3689320388,
        "MMLU_marketing":0.2222222222,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.2081736909,
        "MMLU_moral_disputes":0.2514450867,
        "MMLU_moral_scenarios":0.2446927374,
        "MMLU_nutrition":0.2581699346,
        "MMLU_philosophy":0.2411575563,
        "MMLU_prehistory":0.225308642,
        "MMLU_professional_accounting":0.2624113475,
        "MMLU_professional_law":0.2568448501,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2254901961,
        "MMLU_public_relations":0.2363636364,
        "MMLU_security_studies":0.4,
        "MMLU_sociology":0.1890547264,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.1927710843,
        "MMLU_world_religions":0.3040935673
    },
    {
        "Model":"gpt-sw3-1.3b-instruct",
        "URL":"https:\/\/huggingface.co\/AI-Sweden-Models\/gpt-sw3-1.3b-instruct",
        "full_model_name":"AI-Sweden-Models\/gpt-sw3-1.3b-instruct",
        "Parameters":1.3,
        "MMLU_average":0.2616521772,
        "arc:challenge|25":0.271331058,
        "hellaswag|10":0.4013144792,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.2888888889,
        "MMLU_astronomy":0.2960526316,
        "MMLU_business_ethics":0.31,
        "MMLU_clinical_knowledge":0.2301886792,
        "MMLU_college_biology":0.2847222222,
        "MMLU_college_chemistry":0.19,
        "MMLU_college_computer_science":0.24,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.2774566474,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.29,
        "MMLU_conceptual_physics":0.2936170213,
        "MMLU_econometrics":0.1929824561,
        "MMLU_electrical_engineering":0.3379310345,
        "MMLU_elementary_mathematics":0.2724867725,
        "MMLU_formal_logic":0.1904761905,
        "MMLU_global_facts":0.27,
        "MMLU_high_school_biology":0.2483870968,
        "MMLU_high_school_chemistry":0.2561576355,
        "MMLU_high_school_computer_science":0.36,
        "MMLU_high_school_european_history":0.2606060606,
        "MMLU_high_school_geography":0.2070707071,
        "MMLU_high_school_government_and_politics":0.2124352332,
        "MMLU_high_school_macroeconomics":0.241025641,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.1806722689,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.2146788991,
        "MMLU_high_school_statistics":0.1574074074,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2869198312,
        "MMLU_human_aging":0.331838565,
        "MMLU_human_sexuality":0.2213740458,
        "MMLU_international_law":0.3305785124,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.282208589,
        "MMLU_machine_learning":0.2946428571,
        "MMLU_management":0.2330097087,
        "MMLU_marketing":0.2863247863,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.2975734355,
        "MMLU_moral_disputes":0.2861271676,
        "MMLU_moral_scenarios":0.2480446927,
        "MMLU_nutrition":0.2287581699,
        "MMLU_philosophy":0.270096463,
        "MMLU_prehistory":0.2654320988,
        "MMLU_professional_accounting":0.2624113475,
        "MMLU_professional_law":0.260756193,
        "MMLU_professional_medicine":0.2683823529,
        "MMLU_professional_psychology":0.2990196078,
        "MMLU_public_relations":0.3272727273,
        "MMLU_security_studies":0.1918367347,
        "MMLU_sociology":0.2537313433,
        "MMLU_us_foreign_policy":0.24,
        "MMLU_virology":0.2771084337,
        "MMLU_world_religions":0.298245614
    },
    {
        "Model":"PM_modelV2",
        "URL":"https:\/\/huggingface.co\/BreadAi\/PM_modelV2",
        "full_model_name":"BreadAi\/PM_modelV2",
        "Parameters":null,
        "MMLU_average":0.2614250538,
        "arc:challenge|25":0.2116040956,
        "hellaswag|10":0.2625970922,
        "MMLU_abstract_algebra":0.21,
        "MMLU_anatomy":0.362962963,
        "MMLU_astronomy":0.3421052632,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.241509434,
        "MMLU_college_biology":0.2361111111,
        "MMLU_college_chemistry":0.41,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.23,
        "MMLU_college_medicine":0.3294797688,
        "MMLU_college_physics":0.3529411765,
        "MMLU_computer_security":0.17,
        "MMLU_conceptual_physics":0.2,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2068965517,
        "MMLU_elementary_mathematics":0.2830687831,
        "MMLU_formal_logic":0.1825396825,
        "MMLU_global_facts":0.2,
        "MMLU_high_school_biology":0.3161290323,
        "MMLU_high_school_chemistry":0.2463054187,
        "MMLU_high_school_computer_science":0.21,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.3585858586,
        "MMLU_high_school_government_and_politics":0.2642487047,
        "MMLU_high_school_macroeconomics":0.2666666667,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.3277310924,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.2917431193,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2352941176,
        "MMLU_high_school_world_history":0.2151898734,
        "MMLU_human_aging":0.130044843,
        "MMLU_human_sexuality":0.2977099237,
        "MMLU_international_law":0.1239669421,
        "MMLU_jurisprudence":0.212962963,
        "MMLU_logical_fallacies":0.245398773,
        "MMLU_machine_learning":0.1785714286,
        "MMLU_management":0.3203883495,
        "MMLU_marketing":0.1923076923,
        "MMLU_medical_genetics":0.24,
        "MMLU_miscellaneous":0.2375478927,
        "MMLU_moral_disputes":0.2196531792,
        "MMLU_moral_scenarios":0.2458100559,
        "MMLU_nutrition":0.2679738562,
        "MMLU_philosophy":0.2218649518,
        "MMLU_prehistory":0.2469135802,
        "MMLU_professional_accounting":0.2588652482,
        "MMLU_professional_law":0.2496740548,
        "MMLU_professional_medicine":0.4154411765,
        "MMLU_professional_psychology":0.2173202614,
        "MMLU_public_relations":0.2272727273,
        "MMLU_security_studies":0.3714285714,
        "MMLU_sociology":0.2885572139,
        "MMLU_us_foreign_policy":0.31,
        "MMLU_virology":0.1987951807,
        "MMLU_world_religions":0.1871345029
    },
    {
        "Model":"gpt-sw3-1.3b",
        "URL":"https:\/\/huggingface.co\/AI-Sweden-Models\/gpt-sw3-1.3b",
        "full_model_name":"AI-Sweden-Models\/gpt-sw3-1.3b",
        "Parameters":1.3,
        "MMLU_average":0.2614077323,
        "arc:challenge|25":0.2730375427,
        "hellaswag|10":0.3951404103,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.2740740741,
        "MMLU_astronomy":0.1973684211,
        "MMLU_business_ethics":0.32,
        "MMLU_clinical_knowledge":0.2566037736,
        "MMLU_college_biology":0.3125,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.35,
        "MMLU_college_mathematics":0.22,
        "MMLU_college_medicine":0.2832369942,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.31,
        "MMLU_conceptual_physics":0.3106382979,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.2551724138,
        "MMLU_elementary_mathematics":0.2804232804,
        "MMLU_formal_logic":0.2777777778,
        "MMLU_global_facts":0.29,
        "MMLU_high_school_biology":0.2483870968,
        "MMLU_high_school_chemistry":0.2463054187,
        "MMLU_high_school_computer_science":0.2,
        "MMLU_high_school_european_history":0.2121212121,
        "MMLU_high_school_geography":0.2929292929,
        "MMLU_high_school_government_and_politics":0.2279792746,
        "MMLU_high_school_macroeconomics":0.3102564103,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.218487395,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.2201834862,
        "MMLU_high_school_statistics":0.3935185185,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.3587443946,
        "MMLU_human_sexuality":0.2748091603,
        "MMLU_international_law":0.2975206612,
        "MMLU_jurisprudence":0.287037037,
        "MMLU_logical_fallacies":0.263803681,
        "MMLU_machine_learning":0.1696428571,
        "MMLU_management":0.2524271845,
        "MMLU_marketing":0.235042735,
        "MMLU_medical_genetics":0.23,
        "MMLU_miscellaneous":0.275862069,
        "MMLU_moral_disputes":0.210982659,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2124183007,
        "MMLU_philosophy":0.2604501608,
        "MMLU_prehistory":0.2222222222,
        "MMLU_professional_accounting":0.2553191489,
        "MMLU_professional_law":0.2496740548,
        "MMLU_professional_medicine":0.3492647059,
        "MMLU_professional_psychology":0.2565359477,
        "MMLU_public_relations":0.2545454545,
        "MMLU_security_studies":0.2326530612,
        "MMLU_sociology":0.1940298507,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.2710843373,
        "MMLU_world_religions":0.3099415205
    },
    {
        "Model":"RedPajama-INCITE-Chat-3B-ShareGPT-11K",
        "URL":"https:\/\/huggingface.co\/Fredithefish\/RedPajama-INCITE-Chat-3B-ShareGPT-11K",
        "full_model_name":"Fredithefish\/RedPajama-INCITE-Chat-3B-ShareGPT-11K",
        "Parameters":3.0,
        "MMLU_average":0.2613213155,
        "arc:challenge|25":0.3660409556,
        "hellaswag|10":0.4818761203,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.2740740741,
        "MMLU_astronomy":0.1907894737,
        "MMLU_business_ethics":0.25,
        "MMLU_clinical_knowledge":0.3245283019,
        "MMLU_college_biology":0.2291666667,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.28,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.2716763006,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.2,
        "MMLU_conceptual_physics":0.3361702128,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2592592593,
        "MMLU_formal_logic":0.1984126984,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.2483870968,
        "MMLU_high_school_chemistry":0.2266009852,
        "MMLU_high_school_computer_science":0.22,
        "MMLU_high_school_european_history":0.2727272727,
        "MMLU_high_school_geography":0.3383838384,
        "MMLU_high_school_government_and_politics":0.2279792746,
        "MMLU_high_school_macroeconomics":0.2333333333,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.2731092437,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.3376146789,
        "MMLU_high_school_statistics":0.1898148148,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.2827004219,
        "MMLU_human_aging":0.2556053812,
        "MMLU_human_sexuality":0.2442748092,
        "MMLU_international_law":0.347107438,
        "MMLU_jurisprudence":0.2407407407,
        "MMLU_logical_fallacies":0.2331288344,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.3106796117,
        "MMLU_marketing":0.2051282051,
        "MMLU_medical_genetics":0.24,
        "MMLU_miscellaneous":0.2490421456,
        "MMLU_moral_disputes":0.2514450867,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2647058824,
        "MMLU_philosophy":0.2765273312,
        "MMLU_prehistory":0.262345679,
        "MMLU_professional_accounting":0.2836879433,
        "MMLU_professional_law":0.240547588,
        "MMLU_professional_medicine":0.2205882353,
        "MMLU_professional_psychology":0.2532679739,
        "MMLU_public_relations":0.3454545455,
        "MMLU_security_studies":0.3346938776,
        "MMLU_sociology":0.2338308458,
        "MMLU_us_foreign_policy":0.21,
        "MMLU_virology":0.3012048193,
        "MMLU_world_religions":0.3157894737
    },
    {
        "Model":"rwkv-4-14b-pile",
        "URL":"https:\/\/huggingface.co\/RWKV\/rwkv-4-14b-pile",
        "full_model_name":"RWKV\/rwkv-4-14b-pile",
        "Parameters":14.0,
        "MMLU_average":0.2612436353,
        "arc:challenge|25":0.3907849829,
        "hellaswag|10":0.5224058952,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.2444444444,
        "MMLU_astronomy":0.2171052632,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2641509434,
        "MMLU_college_biology":0.2361111111,
        "MMLU_college_chemistry":0.26,
        "MMLU_college_computer_science":0.23,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.289017341,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.37,
        "MMLU_conceptual_physics":0.3276595745,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.2896551724,
        "MMLU_elementary_mathematics":0.2619047619,
        "MMLU_formal_logic":0.2222222222,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.2032258065,
        "MMLU_high_school_chemistry":0.2413793103,
        "MMLU_high_school_computer_science":0.24,
        "MMLU_high_school_european_history":0.2484848485,
        "MMLU_high_school_geography":0.2222222222,
        "MMLU_high_school_government_and_politics":0.207253886,
        "MMLU_high_school_macroeconomics":0.2179487179,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.231092437,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.3119266055,
        "MMLU_high_school_statistics":0.1712962963,
        "MMLU_high_school_us_history":0.2303921569,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.2376681614,
        "MMLU_human_sexuality":0.2900763359,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.25,
        "MMLU_logical_fallacies":0.226993865,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.3009708738,
        "MMLU_marketing":0.2435897436,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.2528735632,
        "MMLU_moral_disputes":0.2398843931,
        "MMLU_moral_scenarios":0.2413407821,
        "MMLU_nutrition":0.3169934641,
        "MMLU_philosophy":0.2636655949,
        "MMLU_prehistory":0.287037037,
        "MMLU_professional_accounting":0.2482269504,
        "MMLU_professional_law":0.258148631,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.2663398693,
        "MMLU_public_relations":0.3363636364,
        "MMLU_security_studies":0.2816326531,
        "MMLU_sociology":0.263681592,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.3253012048,
        "MMLU_world_religions":0.2748538012
    },
    {
        "Model":"pythia-12b-sft-v8-rlhf-2k-steps",
        "URL":"https:\/\/huggingface.co\/OpenAssistant\/pythia-12b-sft-v8-rlhf-2k-steps",
        "full_model_name":"OpenAssistant\/pythia-12b-sft-v8-rlhf-2k-steps",
        "Parameters":12.0,
        "MMLU_average":0.2612215167,
        "arc:challenge|25":0.4044368601,
        "hellaswag|10":0.5209121689,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.1259259259,
        "MMLU_astronomy":0.2236842105,
        "MMLU_business_ethics":0.22,
        "MMLU_clinical_knowledge":0.2905660377,
        "MMLU_college_biology":0.2777777778,
        "MMLU_college_chemistry":0.25,
        "MMLU_college_computer_science":0.3,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.2196531792,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.3,
        "MMLU_conceptual_physics":0.2936170213,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2206896552,
        "MMLU_elementary_mathematics":0.2248677249,
        "MMLU_formal_logic":0.1904761905,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.2161290323,
        "MMLU_high_school_chemistry":0.2266009852,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2363636364,
        "MMLU_high_school_geography":0.3232323232,
        "MMLU_high_school_government_and_politics":0.274611399,
        "MMLU_high_school_macroeconomics":0.2666666667,
        "MMLU_high_school_mathematics":0.2185185185,
        "MMLU_high_school_microeconomics":0.256302521,
        "MMLU_high_school_physics":0.2251655629,
        "MMLU_high_school_psychology":0.2073394495,
        "MMLU_high_school_statistics":0.2824074074,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2489451477,
        "MMLU_human_aging":0.3273542601,
        "MMLU_human_sexuality":0.3282442748,
        "MMLU_international_law":0.2727272727,
        "MMLU_jurisprudence":0.3055555556,
        "MMLU_logical_fallacies":0.245398773,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.2233009709,
        "MMLU_marketing":0.311965812,
        "MMLU_medical_genetics":0.35,
        "MMLU_miscellaneous":0.2643678161,
        "MMLU_moral_disputes":0.2687861272,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2679738562,
        "MMLU_philosophy":0.2379421222,
        "MMLU_prehistory":0.2839506173,
        "MMLU_professional_accounting":0.2092198582,
        "MMLU_professional_law":0.2555410691,
        "MMLU_professional_medicine":0.3529411765,
        "MMLU_professional_psychology":0.272875817,
        "MMLU_public_relations":0.2363636364,
        "MMLU_security_studies":0.3306122449,
        "MMLU_sociology":0.2139303483,
        "MMLU_us_foreign_policy":0.3,
        "MMLU_virology":0.265060241,
        "MMLU_world_religions":0.350877193
    },
    {
        "Model":"RedPajama-INCITE-Chat-3B-v1-RL-LoRA-8bit-test1",
        "URL":"https:\/\/huggingface.co\/DanielSc4\/RedPajama-INCITE-Chat-3B-v1-RL-LoRA-8bit-test1",
        "full_model_name":"DanielSc4\/RedPajama-INCITE-Chat-3B-v1-RL-LoRA-8bit-test1",
        "Parameters":3.0,
        "MMLU_average":0.2609526397,
        "arc:challenge|25":0.3728668942,
        "hellaswag|10":0.4942242581,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.2518518519,
        "MMLU_astronomy":0.2894736842,
        "MMLU_business_ethics":0.2,
        "MMLU_clinical_knowledge":0.2830188679,
        "MMLU_college_biology":0.2708333333,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.23,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.2601156069,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2170212766,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2275132275,
        "MMLU_formal_logic":0.2380952381,
        "MMLU_global_facts":0.28,
        "MMLU_high_school_biology":0.264516129,
        "MMLU_high_school_chemistry":0.2315270936,
        "MMLU_high_school_computer_science":0.26,
        "MMLU_high_school_european_history":0.2545454545,
        "MMLU_high_school_geography":0.3636363636,
        "MMLU_high_school_government_and_politics":0.2849740933,
        "MMLU_high_school_macroeconomics":0.2692307692,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.243697479,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.3321100917,
        "MMLU_high_school_statistics":0.2824074074,
        "MMLU_high_school_us_history":0.318627451,
        "MMLU_high_school_world_history":0.2616033755,
        "MMLU_human_aging":0.1524663677,
        "MMLU_human_sexuality":0.213740458,
        "MMLU_international_law":0.4214876033,
        "MMLU_jurisprudence":0.2037037037,
        "MMLU_logical_fallacies":0.1717791411,
        "MMLU_machine_learning":0.2232142857,
        "MMLU_management":0.2815533981,
        "MMLU_marketing":0.2435897436,
        "MMLU_medical_genetics":0.24,
        "MMLU_miscellaneous":0.2490421456,
        "MMLU_moral_disputes":0.2427745665,
        "MMLU_moral_scenarios":0.2592178771,
        "MMLU_nutrition":0.2875816993,
        "MMLU_philosophy":0.2636655949,
        "MMLU_prehistory":0.2839506173,
        "MMLU_professional_accounting":0.2659574468,
        "MMLU_professional_law":0.2614080834,
        "MMLU_professional_medicine":0.2279411765,
        "MMLU_professional_psychology":0.2565359477,
        "MMLU_public_relations":0.3545454545,
        "MMLU_security_studies":0.2857142857,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.2048192771,
        "MMLU_world_religions":0.2807017544
    },
    {
        "Model":"ReasonixPajama-3B-HF",
        "URL":"https:\/\/huggingface.co\/Fredithefish\/ReasonixPajama-3B-HF",
        "full_model_name":"Fredithefish\/ReasonixPajama-3B-HF",
        "Parameters":3.0,
        "MMLU_average":0.2608883665,
        "arc:challenge|25":0.3540955631,
        "hellaswag|10":0.4782911771,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.3333333333,
        "MMLU_astronomy":0.2236842105,
        "MMLU_business_ethics":0.22,
        "MMLU_clinical_knowledge":0.2603773585,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.2485549133,
        "MMLU_college_physics":0.3137254902,
        "MMLU_computer_security":0.23,
        "MMLU_conceptual_physics":0.2382978723,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.3034482759,
        "MMLU_elementary_mathematics":0.2010582011,
        "MMLU_formal_logic":0.2063492063,
        "MMLU_global_facts":0.19,
        "MMLU_high_school_biology":0.264516129,
        "MMLU_high_school_chemistry":0.2167487685,
        "MMLU_high_school_computer_science":0.19,
        "MMLU_high_school_european_history":0.2909090909,
        "MMLU_high_school_geography":0.3333333333,
        "MMLU_high_school_government_and_politics":0.2797927461,
        "MMLU_high_school_macroeconomics":0.2974358974,
        "MMLU_high_school_mathematics":0.237037037,
        "MMLU_high_school_microeconomics":0.2226890756,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.2660550459,
        "MMLU_high_school_statistics":0.2962962963,
        "MMLU_high_school_us_history":0.3137254902,
        "MMLU_high_school_world_history":0.3206751055,
        "MMLU_human_aging":0.2556053812,
        "MMLU_human_sexuality":0.2366412214,
        "MMLU_international_law":0.3305785124,
        "MMLU_jurisprudence":0.287037037,
        "MMLU_logical_fallacies":0.2392638037,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.213592233,
        "MMLU_marketing":0.235042735,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.2554278416,
        "MMLU_moral_disputes":0.2398843931,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.2581699346,
        "MMLU_philosophy":0.270096463,
        "MMLU_prehistory":0.2469135802,
        "MMLU_professional_accounting":0.2907801418,
        "MMLU_professional_law":0.240547588,
        "MMLU_professional_medicine":0.3566176471,
        "MMLU_professional_psychology":0.2598039216,
        "MMLU_public_relations":0.2545454545,
        "MMLU_security_studies":0.2816326531,
        "MMLU_sociology":0.263681592,
        "MMLU_us_foreign_policy":0.24,
        "MMLU_virology":0.1927710843,
        "MMLU_world_religions":0.298245614
    },
    {
        "Model":"results_2023-07-24T10-16-22.897961.json",
        "URL":"https:\/\/huggingface.co\/gpt2-large\/results_2023-07-24T10-16-22.897961.json",
        "full_model_name":"gpt2-large\/results_2023-07-24T10-16-22.897961.json",
        "Parameters":null,
        "MMLU_average":0.2608442991,
        "arc:challenge|25":0.2354948805,
        "hellaswag|10":0.3616809401,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.3185185185,
        "MMLU_astronomy":0.1710526316,
        "MMLU_business_ethics":0.25,
        "MMLU_clinical_knowledge":0.2716981132,
        "MMLU_college_biology":0.2638888889,
        "MMLU_college_chemistry":0.18,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.2601156069,
        "MMLU_college_physics":0.1470588235,
        "MMLU_computer_security":0.33,
        "MMLU_conceptual_physics":0.3234042553,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.2206896552,
        "MMLU_elementary_mathematics":0.2407407407,
        "MMLU_formal_logic":0.1507936508,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.2516129032,
        "MMLU_high_school_chemistry":0.2906403941,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.2242424242,
        "MMLU_high_school_geography":0.2676767677,
        "MMLU_high_school_government_and_politics":0.2279792746,
        "MMLU_high_school_macroeconomics":0.2666666667,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.218487395,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.2440366972,
        "MMLU_high_school_statistics":0.3611111111,
        "MMLU_high_school_us_history":0.2794117647,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.2959641256,
        "MMLU_human_sexuality":0.1984732824,
        "MMLU_international_law":0.2479338843,
        "MMLU_jurisprudence":0.2962962963,
        "MMLU_logical_fallacies":0.3006134969,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.3883495146,
        "MMLU_marketing":0.2179487179,
        "MMLU_medical_genetics":0.21,
        "MMLU_miscellaneous":0.2567049808,
        "MMLU_moral_disputes":0.2543352601,
        "MMLU_moral_scenarios":0.2413407821,
        "MMLU_nutrition":0.2516339869,
        "MMLU_philosophy":0.2893890675,
        "MMLU_prehistory":0.2654320988,
        "MMLU_professional_accounting":0.2695035461,
        "MMLU_professional_law":0.2451108214,
        "MMLU_professional_medicine":0.1985294118,
        "MMLU_professional_psychology":0.2516339869,
        "MMLU_public_relations":0.2090909091,
        "MMLU_security_studies":0.1918367347,
        "MMLU_sociology":0.2587064677,
        "MMLU_us_foreign_policy":0.3,
        "MMLU_virology":0.2771084337,
        "MMLU_world_religions":0.2923976608
    },
    {
        "Model":"camel-5b-hf",
        "URL":"https:\/\/huggingface.co\/Writer\/camel-5b-hf",
        "full_model_name":"Writer\/camel-5b-hf",
        "Parameters":5.0,
        "MMLU_average":0.2606537304,
        "arc:challenge|25":0.3139931741,
        "hellaswag|10":0.4350726947,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.2888888889,
        "MMLU_astronomy":0.2105263158,
        "MMLU_business_ethics":0.27,
        "MMLU_clinical_knowledge":0.2452830189,
        "MMLU_college_biology":0.2291666667,
        "MMLU_college_chemistry":0.17,
        "MMLU_college_computer_science":0.2,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.225433526,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.33,
        "MMLU_conceptual_physics":0.3191489362,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2275862069,
        "MMLU_elementary_mathematics":0.2433862434,
        "MMLU_formal_logic":0.1666666667,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.2516129032,
        "MMLU_high_school_chemistry":0.2906403941,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.2727272727,
        "MMLU_high_school_geography":0.2424242424,
        "MMLU_high_school_government_and_politics":0.2331606218,
        "MMLU_high_school_macroeconomics":0.2461538462,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.2521008403,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.2348623853,
        "MMLU_high_school_statistics":0.1805555556,
        "MMLU_high_school_us_history":0.2843137255,
        "MMLU_high_school_world_history":0.3333333333,
        "MMLU_human_aging":0.3632286996,
        "MMLU_human_sexuality":0.2366412214,
        "MMLU_international_law":0.3140495868,
        "MMLU_jurisprudence":0.2777777778,
        "MMLU_logical_fallacies":0.3251533742,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.2427184466,
        "MMLU_marketing":0.2948717949,
        "MMLU_medical_genetics":0.19,
        "MMLU_miscellaneous":0.288633461,
        "MMLU_moral_disputes":0.2572254335,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.2516339869,
        "MMLU_philosophy":0.3118971061,
        "MMLU_prehistory":0.2777777778,
        "MMLU_professional_accounting":0.2411347518,
        "MMLU_professional_law":0.278357236,
        "MMLU_professional_medicine":0.2095588235,
        "MMLU_professional_psychology":0.2761437908,
        "MMLU_public_relations":0.3272727273,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.2985074627,
        "MMLU_us_foreign_policy":0.22,
        "MMLU_virology":0.3313253012,
        "MMLU_world_religions":0.2748538012
    },
    {
        "Model":"pythia-70m-deduped-step44k-92bt",
        "URL":"https:\/\/huggingface.co\/klosax\/pythia-70m-deduped-step44k-92bt",
        "full_model_name":"klosax\/pythia-70m-deduped-step44k-92bt",
        "Parameters":0.07,
        "MMLU_average":0.2602702394,
        "arc:challenge|25":0.1877133106,
        "hellaswag|10":0.2720573591,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.237037037,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.2339622642,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.29,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.17,
        "MMLU_conceptual_physics":0.285106383,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2827586207,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.1507936508,
        "MMLU_global_facts":0.17,
        "MMLU_high_school_biology":0.3161290323,
        "MMLU_high_school_chemistry":0.275862069,
        "MMLU_high_school_computer_science":0.29,
        "MMLU_high_school_european_history":0.2484848485,
        "MMLU_high_school_geography":0.3585858586,
        "MMLU_high_school_government_and_politics":0.3523316062,
        "MMLU_high_school_macroeconomics":0.2666666667,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.2142857143,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.2990825688,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2696078431,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3004484305,
        "MMLU_human_sexuality":0.2519083969,
        "MMLU_international_law":0.2809917355,
        "MMLU_jurisprudence":0.2222222222,
        "MMLU_logical_fallacies":0.2699386503,
        "MMLU_machine_learning":0.2053571429,
        "MMLU_management":0.3300970874,
        "MMLU_marketing":0.1965811966,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2464878672,
        "MMLU_moral_disputes":0.2543352601,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.2647058824,
        "MMLU_philosophy":0.2347266881,
        "MMLU_prehistory":0.2191358025,
        "MMLU_professional_accounting":0.2163120567,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.4448529412,
        "MMLU_professional_psychology":0.2450980392,
        "MMLU_public_relations":0.2272727273,
        "MMLU_security_studies":0.3551020408,
        "MMLU_sociology":0.263681592,
        "MMLU_us_foreign_policy":0.29,
        "MMLU_virology":0.1686746988,
        "MMLU_world_religions":0.1988304094
    },
    {
        "Model":"opt-350m",
        "URL":"https:\/\/huggingface.co\/facebook\/opt-350m",
        "full_model_name":"facebook\/opt-350m",
        "Parameters":0.35,
        "MMLU_average":0.2602309674,
        "arc:challenge|25":0.2056313993,
        "hellaswag|10":0.3212507469,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.2740740741,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.2679245283,
        "MMLU_college_biology":0.2291666667,
        "MMLU_college_chemistry":0.35,
        "MMLU_college_computer_science":0.3,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.2543352601,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.18,
        "MMLU_conceptual_physics":0.2723404255,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2965517241,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.2380952381,
        "MMLU_global_facts":0.19,
        "MMLU_high_school_biology":0.3,
        "MMLU_high_school_chemistry":0.2906403941,
        "MMLU_high_school_computer_science":0.19,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.3585858586,
        "MMLU_high_school_government_and_politics":0.3471502591,
        "MMLU_high_school_macroeconomics":0.2743589744,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.2731092437,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.3266055046,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.2067510549,
        "MMLU_human_aging":0.134529148,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.3719008264,
        "MMLU_jurisprudence":0.2222222222,
        "MMLU_logical_fallacies":0.245398773,
        "MMLU_machine_learning":0.1785714286,
        "MMLU_management":0.2233009709,
        "MMLU_marketing":0.2136752137,
        "MMLU_medical_genetics":0.29,
        "MMLU_miscellaneous":0.2056194125,
        "MMLU_moral_disputes":0.2630057803,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.2614379085,
        "MMLU_philosophy":0.1961414791,
        "MMLU_prehistory":0.2716049383,
        "MMLU_professional_accounting":0.2482269504,
        "MMLU_professional_law":0.2411994785,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2418300654,
        "MMLU_public_relations":0.1909090909,
        "MMLU_security_studies":0.3959183673,
        "MMLU_sociology":0.2189054726,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.1927710843,
        "MMLU_world_religions":0.1871345029
    },
    {
        "Model":"opt-125m",
        "URL":"https:\/\/huggingface.co\/facebook\/opt-125m",
        "full_model_name":"facebook\/opt-125m",
        "Parameters":0.125,
        "MMLU_average":0.2601558756,
        "arc:challenge|25":0.2039249147,
        "hellaswag|10":0.2918741287,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.2444444444,
        "MMLU_astronomy":0.2697368421,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.28,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.2023121387,
        "MMLU_college_physics":0.3725490196,
        "MMLU_computer_security":0.18,
        "MMLU_conceptual_physics":0.3191489362,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.2482758621,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.1428571429,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.3161290323,
        "MMLU_high_school_chemistry":0.2955665025,
        "MMLU_high_school_computer_science":0.19,
        "MMLU_high_school_european_history":0.2121212121,
        "MMLU_high_school_geography":0.2727272727,
        "MMLU_high_school_government_and_politics":0.3678756477,
        "MMLU_high_school_macroeconomics":0.3435897436,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.3529411765,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.2275229358,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.2573839662,
        "MMLU_human_aging":0.201793722,
        "MMLU_human_sexuality":0.2366412214,
        "MMLU_international_law":0.3801652893,
        "MMLU_jurisprudence":0.212962963,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.1517857143,
        "MMLU_management":0.1844660194,
        "MMLU_marketing":0.1965811966,
        "MMLU_medical_genetics":0.34,
        "MMLU_miscellaneous":0.2541507024,
        "MMLU_moral_disputes":0.2312138728,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2712418301,
        "MMLU_philosophy":0.2379421222,
        "MMLU_prehistory":0.2932098765,
        "MMLU_professional_accounting":0.2659574468,
        "MMLU_professional_law":0.2529335072,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2222222222,
        "MMLU_public_relations":0.2272727273,
        "MMLU_security_studies":0.2489795918,
        "MMLU_sociology":0.2338308458,
        "MMLU_us_foreign_policy":0.32,
        "MMLU_virology":0.2048192771,
        "MMLU_world_religions":0.1754385965
    },
    {
        "Model":"h2ogpt-oasst1-512-12b",
        "URL":"https:\/\/huggingface.co\/h2oai\/h2ogpt-oasst1-512-12b",
        "full_model_name":"h2oai\/h2ogpt-oasst1-512-12b",
        "Parameters":12.0,
        "MMLU_average":0.2601394692,
        "arc:challenge|25":0.4027303754,
        "hellaswag|10":0.5226050588,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.3407407407,
        "MMLU_astronomy":0.3157894737,
        "MMLU_business_ethics":0.2,
        "MMLU_clinical_knowledge":0.2679245283,
        "MMLU_college_biology":0.3055555556,
        "MMLU_college_chemistry":0.17,
        "MMLU_college_computer_science":0.25,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.2312138728,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.27,
        "MMLU_conceptual_physics":0.2425531915,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2698412698,
        "MMLU_formal_logic":0.1666666667,
        "MMLU_global_facts":0.29,
        "MMLU_high_school_biology":0.2451612903,
        "MMLU_high_school_chemistry":0.3004926108,
        "MMLU_high_school_computer_science":0.32,
        "MMLU_high_school_european_history":0.2363636364,
        "MMLU_high_school_geography":0.2070707071,
        "MMLU_high_school_government_and_politics":0.2227979275,
        "MMLU_high_school_macroeconomics":0.2564102564,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.2605042017,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.2201834862,
        "MMLU_high_school_statistics":0.2175925926,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.2616033755,
        "MMLU_human_aging":0.3811659193,
        "MMLU_human_sexuality":0.2366412214,
        "MMLU_international_law":0.4049586777,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.2427184466,
        "MMLU_marketing":0.2564102564,
        "MMLU_medical_genetics":0.21,
        "MMLU_miscellaneous":0.3116219668,
        "MMLU_moral_disputes":0.2919075145,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2908496732,
        "MMLU_philosophy":0.2990353698,
        "MMLU_prehistory":0.287037037,
        "MMLU_professional_accounting":0.2553191489,
        "MMLU_professional_law":0.2464146023,
        "MMLU_professional_medicine":0.1985294118,
        "MMLU_professional_psychology":0.2859477124,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.1836734694,
        "MMLU_sociology":0.263681592,
        "MMLU_us_foreign_policy":0.23,
        "MMLU_virology":0.313253012,
        "MMLU_world_religions":0.3274853801
    },
    {
        "Model":"pythia-13b-deduped-green_devil",
        "URL":"https:\/\/huggingface.co\/Pirr\/pythia-13b-deduped-green_devil",
        "full_model_name":"Pirr\/pythia-13b-deduped-green_devil",
        "Parameters":13.0,
        "MMLU_average":0.2600786907,
        "arc:challenge|25":0.3873720137,
        "hellaswag|10":0.5157339175,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.3111111111,
        "MMLU_astronomy":0.2763157895,
        "MMLU_business_ethics":0.28,
        "MMLU_clinical_knowledge":0.2981132075,
        "MMLU_college_biology":0.2638888889,
        "MMLU_college_chemistry":0.18,
        "MMLU_college_computer_science":0.27,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.2543352601,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.27,
        "MMLU_conceptual_physics":0.3106382979,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2883597884,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.28,
        "MMLU_high_school_biology":0.2129032258,
        "MMLU_high_school_chemistry":0.1822660099,
        "MMLU_high_school_computer_science":0.23,
        "MMLU_high_school_european_history":0.2060606061,
        "MMLU_high_school_geography":0.2222222222,
        "MMLU_high_school_government_and_politics":0.2227979275,
        "MMLU_high_school_macroeconomics":0.2282051282,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.268907563,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.223853211,
        "MMLU_high_school_statistics":0.2314814815,
        "MMLU_high_school_us_history":0.2892156863,
        "MMLU_high_school_world_history":0.253164557,
        "MMLU_human_aging":0.33632287,
        "MMLU_human_sexuality":0.2671755725,
        "MMLU_international_law":0.305785124,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2147239264,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.1941747573,
        "MMLU_marketing":0.2692307692,
        "MMLU_medical_genetics":0.29,
        "MMLU_miscellaneous":0.2720306513,
        "MMLU_moral_disputes":0.2658959538,
        "MMLU_moral_scenarios":0.2502793296,
        "MMLU_nutrition":0.2287581699,
        "MMLU_philosophy":0.3022508039,
        "MMLU_prehistory":0.2685185185,
        "MMLU_professional_accounting":0.2765957447,
        "MMLU_professional_law":0.2724902216,
        "MMLU_professional_medicine":0.1875,
        "MMLU_professional_psychology":0.2679738562,
        "MMLU_public_relations":0.2545454545,
        "MMLU_security_studies":0.1918367347,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.3012048193,
        "MMLU_world_religions":0.3450292398
    },
    {
        "Model":"gpt-sw3-6.7b",
        "URL":"https:\/\/huggingface.co\/AI-Sweden-Models\/gpt-sw3-6.7b",
        "full_model_name":"AI-Sweden-Models\/gpt-sw3-6.7b",
        "Parameters":6.7,
        "MMLU_average":0.2599824538,
        "arc:challenge|25":0.3208191126,
        "hellaswag|10":0.4570802629,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.3407407407,
        "MMLU_astronomy":0.3026315789,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.2339622642,
        "MMLU_college_biology":0.2638888889,
        "MMLU_college_chemistry":0.18,
        "MMLU_college_computer_science":0.28,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.2485549133,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.26,
        "MMLU_conceptual_physics":0.2127659574,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.275862069,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.2142857143,
        "MMLU_global_facts":0.38,
        "MMLU_high_school_biology":0.264516129,
        "MMLU_high_school_chemistry":0.2610837438,
        "MMLU_high_school_computer_science":0.34,
        "MMLU_high_school_european_history":0.296969697,
        "MMLU_high_school_geography":0.297979798,
        "MMLU_high_school_government_and_politics":0.2279792746,
        "MMLU_high_school_macroeconomics":0.2153846154,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.2422018349,
        "MMLU_high_school_statistics":0.2175925926,
        "MMLU_high_school_us_history":0.2745098039,
        "MMLU_high_school_world_history":0.2742616034,
        "MMLU_human_aging":0.269058296,
        "MMLU_human_sexuality":0.2519083969,
        "MMLU_international_law":0.3719008264,
        "MMLU_jurisprudence":0.2407407407,
        "MMLU_logical_fallacies":0.3006134969,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.1941747573,
        "MMLU_marketing":0.2521367521,
        "MMLU_medical_genetics":0.16,
        "MMLU_miscellaneous":0.2796934866,
        "MMLU_moral_disputes":0.2861271676,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.2516339869,
        "MMLU_philosophy":0.3054662379,
        "MMLU_prehistory":0.2808641975,
        "MMLU_professional_accounting":0.2659574468,
        "MMLU_professional_law":0.2744458931,
        "MMLU_professional_medicine":0.1617647059,
        "MMLU_professional_psychology":0.2794117647,
        "MMLU_public_relations":0.2090909091,
        "MMLU_security_studies":0.2612244898,
        "MMLU_sociology":0.2537313433,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2409638554,
        "MMLU_world_religions":0.2923976608
    },
    {
        "Model":"RedPajama-INCITE-Chat-Instruct-3B-V1",
        "URL":"https:\/\/huggingface.co\/acrastt\/RedPajama-INCITE-Chat-Instruct-3B-V1",
        "full_model_name":"acrastt\/RedPajama-INCITE-Chat-Instruct-3B-V1",
        "Parameters":3.0,
        "MMLU_average":0.2599345663,
        "arc:challenge|25":0.3873720137,
        "hellaswag|10":0.5,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.2148148148,
        "MMLU_astronomy":0.25,
        "MMLU_business_ethics":0.22,
        "MMLU_clinical_knowledge":0.279245283,
        "MMLU_college_biology":0.1875,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.29,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.2427745665,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.27,
        "MMLU_conceptual_physics":0.2510638298,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2551724138,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.27,
        "MMLU_high_school_biology":0.2741935484,
        "MMLU_high_school_chemistry":0.2216748768,
        "MMLU_high_school_computer_science":0.28,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.3484848485,
        "MMLU_high_school_government_and_politics":0.2331606218,
        "MMLU_high_school_macroeconomics":0.1974358974,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.2226890756,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.271559633,
        "MMLU_high_school_statistics":0.2222222222,
        "MMLU_high_school_us_history":0.2647058824,
        "MMLU_high_school_world_history":0.2616033755,
        "MMLU_human_aging":0.1748878924,
        "MMLU_human_sexuality":0.2519083969,
        "MMLU_international_law":0.3719008264,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.2576687117,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.3203883495,
        "MMLU_marketing":0.3290598291,
        "MMLU_medical_genetics":0.19,
        "MMLU_miscellaneous":0.2605363985,
        "MMLU_moral_disputes":0.2861271676,
        "MMLU_moral_scenarios":0.2569832402,
        "MMLU_nutrition":0.2581699346,
        "MMLU_philosophy":0.2765273312,
        "MMLU_prehistory":0.262345679,
        "MMLU_professional_accounting":0.2659574468,
        "MMLU_professional_law":0.2659713168,
        "MMLU_professional_medicine":0.2205882353,
        "MMLU_professional_psychology":0.2565359477,
        "MMLU_public_relations":0.3181818182,
        "MMLU_security_studies":0.2204081633,
        "MMLU_sociology":0.2736318408,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.2469879518,
        "MMLU_world_religions":0.3157894737
    },
    {
        "Model":"Llama-2-13b-sf",
        "URL":"https:\/\/huggingface.co\/porkorbeef\/Llama-2-13b-sf",
        "full_model_name":"porkorbeef\/Llama-2-13b-sf",
        "Parameters":13.0,
        "MMLU_average":0.2598386584,
        "arc:challenge|25":0.2218430034,
        "hellaswag|10":0.2595100578,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.2222222222,
        "MMLU_astronomy":0.2434210526,
        "MMLU_business_ethics":0.2,
        "MMLU_clinical_knowledge":0.3132075472,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.29,
        "MMLU_college_computer_science":0.2,
        "MMLU_college_mathematics":0.16,
        "MMLU_college_medicine":0.2774566474,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.25,
        "MMLU_conceptual_physics":0.2808510638,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.1931034483,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.2142857143,
        "MMLU_global_facts":0.29,
        "MMLU_high_school_biology":0.2709677419,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.35,
        "MMLU_high_school_european_history":0.2242424242,
        "MMLU_high_school_geography":0.2929292929,
        "MMLU_high_school_government_and_politics":0.2227979275,
        "MMLU_high_school_macroeconomics":0.2948717949,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.231092437,
        "MMLU_high_school_physics":0.2317880795,
        "MMLU_high_school_psychology":0.3100917431,
        "MMLU_high_school_statistics":0.2314814815,
        "MMLU_high_school_us_history":0.2009803922,
        "MMLU_high_school_world_history":0.2784810127,
        "MMLU_human_aging":0.3094170404,
        "MMLU_human_sexuality":0.3129770992,
        "MMLU_international_law":0.2644628099,
        "MMLU_jurisprudence":0.25,
        "MMLU_logical_fallacies":0.2576687117,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.3106796117,
        "MMLU_marketing":0.2564102564,
        "MMLU_medical_genetics":0.29,
        "MMLU_miscellaneous":0.2541507024,
        "MMLU_moral_disputes":0.2341040462,
        "MMLU_moral_scenarios":0.2346368715,
        "MMLU_nutrition":0.2156862745,
        "MMLU_philosophy":0.2990353698,
        "MMLU_prehistory":0.2901234568,
        "MMLU_professional_accounting":0.2624113475,
        "MMLU_professional_law":0.2411994785,
        "MMLU_professional_medicine":0.2095588235,
        "MMLU_professional_psychology":0.2777777778,
        "MMLU_public_relations":0.2818181818,
        "MMLU_security_studies":0.2081632653,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.33,
        "MMLU_virology":0.2168674699,
        "MMLU_world_religions":0.2456140351
    },
    {
        "Model":"TinyLlama-1.1B-Chat-v0.6",
        "URL":"https:\/\/huggingface.co\/TinyLlama\/TinyLlama-1.1B-Chat-v0.6",
        "full_model_name":"TinyLlama\/TinyLlama-1.1B-Chat-v0.6",
        "Parameters":1.1,
        "MMLU_average":0.259832638,
        "arc:challenge|25":0.2866894198,
        "hellaswag|10":0.4250149373,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.3333333333,
        "MMLU_astronomy":0.2105263158,
        "MMLU_business_ethics":0.26,
        "MMLU_clinical_knowledge":0.2188679245,
        "MMLU_college_biology":0.2638888889,
        "MMLU_college_chemistry":0.26,
        "MMLU_college_computer_science":0.36,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.2543352601,
        "MMLU_college_physics":0.1666666667,
        "MMLU_computer_security":0.26,
        "MMLU_conceptual_physics":0.2085106383,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.2751322751,
        "MMLU_formal_logic":0.1507936508,
        "MMLU_global_facts":0.29,
        "MMLU_high_school_biology":0.2580645161,
        "MMLU_high_school_chemistry":0.3004926108,
        "MMLU_high_school_computer_science":0.32,
        "MMLU_high_school_european_history":0.2848484848,
        "MMLU_high_school_geography":0.2272727273,
        "MMLU_high_school_government_and_politics":0.2279792746,
        "MMLU_high_school_macroeconomics":0.2256410256,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.2226890756,
        "MMLU_high_school_physics":0.238410596,
        "MMLU_high_school_psychology":0.2330275229,
        "MMLU_high_school_statistics":0.3611111111,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.2784810127,
        "MMLU_human_aging":0.2869955157,
        "MMLU_human_sexuality":0.2519083969,
        "MMLU_international_law":0.2727272727,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.2944785276,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.2233009709,
        "MMLU_marketing":0.2521367521,
        "MMLU_medical_genetics":0.22,
        "MMLU_miscellaneous":0.2707535121,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.2418300654,
        "MMLU_philosophy":0.2990353698,
        "MMLU_prehistory":0.299382716,
        "MMLU_professional_accounting":0.2304964539,
        "MMLU_professional_law":0.2346805737,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2189542484,
        "MMLU_public_relations":0.2090909091,
        "MMLU_security_studies":0.2530612245,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.234939759,
        "MMLU_world_religions":0.2222222222
    },
    {
        "Model":"oasst-pythia-12b-6000-steps",
        "URL":"https:\/\/huggingface.co\/dvruette\/oasst-pythia-12b-6000-steps",
        "full_model_name":"dvruette\/oasst-pythia-12b-6000-steps",
        "Parameters":12.0,
        "MMLU_average":0.259719583,
        "arc:challenge|25":0.4146757679,
        "hellaswag|10":0.5239992033,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.3259259259,
        "MMLU_astronomy":0.3092105263,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.2830188679,
        "MMLU_college_biology":0.2847222222,
        "MMLU_college_chemistry":0.22,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.2427745665,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.26,
        "MMLU_conceptual_physics":0.1957446809,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.2896551724,
        "MMLU_elementary_mathematics":0.2724867725,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.2451612903,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.2363636364,
        "MMLU_high_school_geography":0.3282828283,
        "MMLU_high_school_government_and_politics":0.2797927461,
        "MMLU_high_school_macroeconomics":0.2666666667,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.2352941176,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.2513761468,
        "MMLU_high_school_statistics":0.2962962963,
        "MMLU_high_school_us_history":0.2892156863,
        "MMLU_high_school_world_history":0.2025316456,
        "MMLU_human_aging":0.1928251121,
        "MMLU_human_sexuality":0.2061068702,
        "MMLU_international_law":0.3719008264,
        "MMLU_jurisprudence":0.2407407407,
        "MMLU_logical_fallacies":0.2760736196,
        "MMLU_machine_learning":0.1517857143,
        "MMLU_management":0.2524271845,
        "MMLU_marketing":0.2564102564,
        "MMLU_medical_genetics":0.19,
        "MMLU_miscellaneous":0.2503192848,
        "MMLU_moral_disputes":0.2832369942,
        "MMLU_moral_scenarios":0.2413407821,
        "MMLU_nutrition":0.2647058824,
        "MMLU_philosophy":0.2926045016,
        "MMLU_prehistory":0.25,
        "MMLU_professional_accounting":0.2553191489,
        "MMLU_professional_law":0.2692307692,
        "MMLU_professional_medicine":0.3198529412,
        "MMLU_professional_psychology":0.2581699346,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.2448979592,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.18,
        "MMLU_virology":0.2469879518,
        "MMLU_world_religions":0.269005848
    },
    {
        "Model":"distilgpt2-emailgen",
        "URL":"https:\/\/huggingface.co\/postbot\/distilgpt2-emailgen",
        "full_model_name":"postbot\/distilgpt2-emailgen",
        "Parameters":null,
        "MMLU_average":0.2597114167,
        "arc:challenge|25":0.1860068259,
        "hellaswag|10":0.2687711611,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.3259259259,
        "MMLU_astronomy":0.1710526316,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.2188679245,
        "MMLU_college_biology":0.2152777778,
        "MMLU_college_chemistry":0.25,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.23,
        "MMLU_college_medicine":0.2369942197,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.26,
        "MMLU_conceptual_physics":0.2255319149,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2896551724,
        "MMLU_elementary_mathematics":0.253968254,
        "MMLU_formal_logic":0.1507936508,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.3161290323,
        "MMLU_high_school_chemistry":0.3004926108,
        "MMLU_high_school_computer_science":0.23,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.2525252525,
        "MMLU_high_school_government_and_politics":0.2383419689,
        "MMLU_high_school_macroeconomics":0.2256410256,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.3529411765,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.2165137615,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.253164557,
        "MMLU_human_aging":0.2600896861,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2561983471,
        "MMLU_jurisprudence":0.25,
        "MMLU_logical_fallacies":0.263803681,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.188034188,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2554278416,
        "MMLU_moral_disputes":0.2456647399,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2549019608,
        "MMLU_philosophy":0.2893890675,
        "MMLU_prehistory":0.287037037,
        "MMLU_professional_accounting":0.1985815603,
        "MMLU_professional_law":0.2483702738,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2483660131,
        "MMLU_public_relations":0.2454545455,
        "MMLU_security_studies":0.2653061224,
        "MMLU_sociology":0.2537313433,
        "MMLU_us_foreign_policy":0.3,
        "MMLU_virology":0.1987951807,
        "MMLU_world_religions":0.2105263158
    },
    {
        "Model":"gpt-neo-125m",
        "URL":"https:\/\/huggingface.co\/EleutherAI\/gpt-neo-125m",
        "full_model_name":"EleutherAI\/gpt-neo-125m",
        "Parameters":0.125,
        "MMLU_average":0.2597039138,
        "arc:challenge|25":0.1911262799,
        "hellaswag|10":0.2834096793,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.2666666667,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.2566037736,
        "MMLU_college_biology":0.2777777778,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.34,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.1907514451,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.19,
        "MMLU_conceptual_physics":0.285106383,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2344827586,
        "MMLU_elementary_mathematics":0.2407407407,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.2483870968,
        "MMLU_high_school_chemistry":0.2807881773,
        "MMLU_high_school_computer_science":0.21,
        "MMLU_high_school_european_history":0.2484848485,
        "MMLU_high_school_geography":0.3585858586,
        "MMLU_high_school_government_and_politics":0.3575129534,
        "MMLU_high_school_macroeconomics":0.2538461538,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.231092437,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.2788990826,
        "MMLU_high_school_statistics":0.4583333333,
        "MMLU_high_school_us_history":0.2990196078,
        "MMLU_high_school_world_history":0.2194092827,
        "MMLU_human_aging":0.2780269058,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2231404959,
        "MMLU_jurisprudence":0.2222222222,
        "MMLU_logical_fallacies":0.245398773,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.2427184466,
        "MMLU_marketing":0.2692307692,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2401021711,
        "MMLU_moral_disputes":0.2456647399,
        "MMLU_moral_scenarios":0.2268156425,
        "MMLU_nutrition":0.2647058824,
        "MMLU_philosophy":0.1832797428,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2482269504,
        "MMLU_professional_law":0.2470664928,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2696078431,
        "MMLU_public_relations":0.2545454545,
        "MMLU_security_studies":0.4,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.1987951807,
        "MMLU_world_religions":0.2046783626
    },
    {
        "Model":"proofGPT-v0.1",
        "URL":"https:\/\/huggingface.co\/hoskinson-center\/proofGPT-v0.1",
        "full_model_name":"hoskinson-center\/proofGPT-v0.1",
        "Parameters":null,
        "MMLU_average":0.2595930867,
        "arc:challenge|25":0.2081911263,
        "hellaswag|10":0.2696673969,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.3481481481,
        "MMLU_astronomy":0.1973684211,
        "MMLU_business_ethics":0.25,
        "MMLU_clinical_knowledge":0.2943396226,
        "MMLU_college_biology":0.25,
        "MMLU_college_chemistry":0.25,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.2543352601,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.26,
        "MMLU_conceptual_physics":0.229787234,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2275862069,
        "MMLU_elementary_mathematics":0.246031746,
        "MMLU_formal_logic":0.1507936508,
        "MMLU_global_facts":0.16,
        "MMLU_high_school_biology":0.3032258065,
        "MMLU_high_school_chemistry":0.3103448276,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.2848484848,
        "MMLU_high_school_geography":0.3484848485,
        "MMLU_high_school_government_and_politics":0.3316062176,
        "MMLU_high_school_macroeconomics":0.2384615385,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.2016806723,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.3412844037,
        "MMLU_high_school_statistics":0.4444444444,
        "MMLU_high_school_us_history":0.2990196078,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.197309417,
        "MMLU_human_sexuality":0.1374045802,
        "MMLU_international_law":0.2892561983,
        "MMLU_jurisprudence":0.1759259259,
        "MMLU_logical_fallacies":0.263803681,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.2524271845,
        "MMLU_marketing":0.1965811966,
        "MMLU_medical_genetics":0.29,
        "MMLU_miscellaneous":0.2171136654,
        "MMLU_moral_disputes":0.2138728324,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.2352941176,
        "MMLU_philosophy":0.2733118971,
        "MMLU_prehistory":0.25,
        "MMLU_professional_accounting":0.2517730496,
        "MMLU_professional_law":0.2470664928,
        "MMLU_professional_medicine":0.4558823529,
        "MMLU_professional_psychology":0.2565359477,
        "MMLU_public_relations":0.2272727273,
        "MMLU_security_studies":0.2163265306,
        "MMLU_sociology":0.2537313433,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.1506024096,
        "MMLU_world_religions":0.269005848
    },
    {
        "Model":"galactica-orca-wizardlm-1.3b",
        "URL":"https:\/\/huggingface.co\/KnutJaegersberg\/galactica-orca-wizardlm-1.3b",
        "full_model_name":"KnutJaegersberg\/galactica-orca-wizardlm-1.3b",
        "Parameters":1.3,
        "MMLU_average":0.2594491121,
        "arc:challenge|25":0.2730375427,
        "hellaswag|10":0.3206532563,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.2814814815,
        "MMLU_astronomy":0.2434210526,
        "MMLU_business_ethics":0.31,
        "MMLU_clinical_knowledge":0.2603773585,
        "MMLU_college_biology":0.3333333333,
        "MMLU_college_chemistry":0.17,
        "MMLU_college_computer_science":0.19,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.2774566474,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.27,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.3310344828,
        "MMLU_elementary_mathematics":0.2830687831,
        "MMLU_formal_logic":0.1904761905,
        "MMLU_global_facts":0.29,
        "MMLU_high_school_biology":0.2161290323,
        "MMLU_high_school_chemistry":0.236453202,
        "MMLU_high_school_computer_science":0.3,
        "MMLU_high_school_european_history":0.2303030303,
        "MMLU_high_school_geography":0.2070707071,
        "MMLU_high_school_government_and_politics":0.2227979275,
        "MMLU_high_school_macroeconomics":0.3179487179,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.2352941176,
        "MMLU_high_school_physics":0.2450331126,
        "MMLU_high_school_psychology":0.2513761468,
        "MMLU_high_school_statistics":0.3009259259,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3228699552,
        "MMLU_human_sexuality":0.1832061069,
        "MMLU_international_law":0.2479338843,
        "MMLU_jurisprudence":0.25,
        "MMLU_logical_fallacies":0.2147239264,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.3203883495,
        "MMLU_marketing":0.2777777778,
        "MMLU_medical_genetics":0.27,
        "MMLU_miscellaneous":0.2439335888,
        "MMLU_moral_disputes":0.3005780347,
        "MMLU_moral_scenarios":0.2368715084,
        "MMLU_nutrition":0.2483660131,
        "MMLU_philosophy":0.2057877814,
        "MMLU_prehistory":0.2839506173,
        "MMLU_professional_accounting":0.2553191489,
        "MMLU_professional_law":0.2490221643,
        "MMLU_professional_medicine":0.2757352941,
        "MMLU_professional_psychology":0.2794117647,
        "MMLU_public_relations":0.2363636364,
        "MMLU_security_studies":0.1959183673,
        "MMLU_sociology":0.263681592,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.3040935673
    },
    {
        "Model":"CodeLlama-13b-Python-hf",
        "URL":"https:\/\/huggingface.co\/codellama\/CodeLlama-13b-Python-hf",
        "full_model_name":"codellama\/CodeLlama-13b-Python-hf",
        "Parameters":13.0,
        "MMLU_average":0.2594321442,
        "arc:challenge|25":0.2960750853,
        "hellaswag|10":0.3576976698,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.3333333333,
        "MMLU_astronomy":0.2039473684,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.2188679245,
        "MMLU_college_biology":0.2916666667,
        "MMLU_college_chemistry":0.18,
        "MMLU_college_computer_science":0.29,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.2601156069,
        "MMLU_college_physics":0.1666666667,
        "MMLU_computer_security":0.21,
        "MMLU_conceptual_physics":0.2680851064,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.2689655172,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.1746031746,
        "MMLU_global_facts":0.24,
        "MMLU_high_school_biology":0.3129032258,
        "MMLU_high_school_chemistry":0.2709359606,
        "MMLU_high_school_computer_science":0.31,
        "MMLU_high_school_european_history":0.2606060606,
        "MMLU_high_school_geography":0.2828282828,
        "MMLU_high_school_government_and_politics":0.3523316062,
        "MMLU_high_school_macroeconomics":0.3282051282,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.2605042017,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.2899082569,
        "MMLU_high_school_statistics":0.3981481481,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.2995780591,
        "MMLU_human_aging":0.2421524664,
        "MMLU_human_sexuality":0.2977099237,
        "MMLU_international_law":0.2561983471,
        "MMLU_jurisprudence":0.2407407407,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.2321428571,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.1752136752,
        "MMLU_medical_genetics":0.25,
        "MMLU_miscellaneous":0.3180076628,
        "MMLU_moral_disputes":0.2080924855,
        "MMLU_moral_scenarios":0.2715083799,
        "MMLU_nutrition":0.2581699346,
        "MMLU_philosophy":0.2475884244,
        "MMLU_prehistory":0.2283950617,
        "MMLU_professional_accounting":0.2836879433,
        "MMLU_professional_law":0.2438070404,
        "MMLU_professional_medicine":0.2169117647,
        "MMLU_professional_psychology":0.2467320261,
        "MMLU_public_relations":0.2090909091,
        "MMLU_security_studies":0.387755102,
        "MMLU_sociology":0.2139303483,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.2168674699,
        "MMLU_world_religions":0.3157894737
    },
    {
        "Model":"Cerebras-GPT-6.7B",
        "URL":"https:\/\/huggingface.co\/cerebras\/Cerebras-GPT-6.7B",
        "full_model_name":"cerebras\/Cerebras-GPT-6.7B",
        "Parameters":6.7,
        "MMLU_average":0.2592839463,
        "arc:challenge|25":0.3088737201,
        "hellaswag|10":0.4451304521,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.237037037,
        "MMLU_astronomy":0.2434210526,
        "MMLU_business_ethics":0.13,
        "MMLU_clinical_knowledge":0.2226415094,
        "MMLU_college_biology":0.2152777778,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.4,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.2427745665,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.22,
        "MMLU_conceptual_physics":0.3021276596,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2551724138,
        "MMLU_elementary_mathematics":0.2433862434,
        "MMLU_formal_logic":0.2380952381,
        "MMLU_global_facts":0.19,
        "MMLU_high_school_biology":0.2193548387,
        "MMLU_high_school_chemistry":0.2068965517,
        "MMLU_high_school_computer_science":0.37,
        "MMLU_high_school_european_history":0.2848484848,
        "MMLU_high_school_geography":0.1868686869,
        "MMLU_high_school_government_and_politics":0.2176165803,
        "MMLU_high_school_macroeconomics":0.2487179487,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.2478991597,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.304587156,
        "MMLU_high_school_statistics":0.3425925926,
        "MMLU_high_school_us_history":0.2401960784,
        "MMLU_high_school_world_history":0.223628692,
        "MMLU_human_aging":0.2959641256,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2644628099,
        "MMLU_jurisprudence":0.2222222222,
        "MMLU_logical_fallacies":0.2883435583,
        "MMLU_machine_learning":0.2321428571,
        "MMLU_management":0.2718446602,
        "MMLU_marketing":0.2393162393,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.2605363985,
        "MMLU_moral_disputes":0.2832369942,
        "MMLU_moral_scenarios":0.2726256983,
        "MMLU_nutrition":0.2352941176,
        "MMLU_philosophy":0.2604501608,
        "MMLU_prehistory":0.2407407407,
        "MMLU_professional_accounting":0.2482269504,
        "MMLU_professional_law":0.2698826597,
        "MMLU_professional_medicine":0.4117647059,
        "MMLU_professional_psychology":0.2483660131,
        "MMLU_public_relations":0.2454545455,
        "MMLU_security_studies":0.2,
        "MMLU_sociology":0.2537313433,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.265060241,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"dolly-v2-12b",
        "URL":"https:\/\/huggingface.co\/databricks\/dolly-v2-12b",
        "full_model_name":"databricks\/dolly-v2-12b",
        "Parameters":12.0,
        "MMLU_average":0.2591684565,
        "arc:challenge|25":0.3813993174,
        "hellaswag|10":0.5463055168,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.3333333333,
        "MMLU_astronomy":0.2631578947,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.2226415094,
        "MMLU_college_biology":0.2638888889,
        "MMLU_college_chemistry":0.18,
        "MMLU_college_computer_science":0.2,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.2369942197,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.3,
        "MMLU_conceptual_physics":0.2382978723,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2827586207,
        "MMLU_elementary_mathematics":0.2698412698,
        "MMLU_formal_logic":0.1984126984,
        "MMLU_global_facts":0.39,
        "MMLU_high_school_biology":0.2612903226,
        "MMLU_high_school_chemistry":0.2512315271,
        "MMLU_high_school_computer_science":0.36,
        "MMLU_high_school_european_history":0.2060606061,
        "MMLU_high_school_geography":0.2070707071,
        "MMLU_high_school_government_and_politics":0.2331606218,
        "MMLU_high_school_macroeconomics":0.2307692308,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.2268907563,
        "MMLU_high_school_physics":0.2317880795,
        "MMLU_high_school_psychology":0.2660550459,
        "MMLU_high_school_statistics":0.2083333333,
        "MMLU_high_school_us_history":0.2696078431,
        "MMLU_high_school_world_history":0.3248945148,
        "MMLU_human_aging":0.2959641256,
        "MMLU_human_sexuality":0.2213740458,
        "MMLU_international_law":0.3801652893,
        "MMLU_jurisprudence":0.2222222222,
        "MMLU_logical_fallacies":0.2392638037,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.2427184466,
        "MMLU_marketing":0.2735042735,
        "MMLU_medical_genetics":0.22,
        "MMLU_miscellaneous":0.2924648787,
        "MMLU_moral_disputes":0.2630057803,
        "MMLU_moral_scenarios":0.2413407821,
        "MMLU_nutrition":0.2450980392,
        "MMLU_philosophy":0.3054662379,
        "MMLU_prehistory":0.25,
        "MMLU_professional_accounting":0.2695035461,
        "MMLU_professional_law":0.2692307692,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.2647058824,
        "MMLU_public_relations":0.2818181818,
        "MMLU_security_studies":0.1755102041,
        "MMLU_sociology":0.2935323383,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.265060241,
        "MMLU_world_religions":0.3157894737
    },
    {
        "Model":"Medical-ChatBot",
        "URL":"https:\/\/huggingface.co\/Mohammed-Altaf\/Medical-ChatBot",
        "full_model_name":"Mohammed-Altaf\/Medical-ChatBot",
        "Parameters":null,
        "MMLU_average":0.2591303805,
        "arc:challenge|25":0.2781569966,
        "hellaswag|10":0.3319059948,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.3037037037,
        "MMLU_astronomy":0.2236842105,
        "MMLU_business_ethics":0.28,
        "MMLU_clinical_knowledge":0.3018867925,
        "MMLU_college_biology":0.25,
        "MMLU_college_chemistry":0.18,
        "MMLU_college_computer_science":0.25,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.2716763006,
        "MMLU_college_physics":0.3431372549,
        "MMLU_computer_security":0.23,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2551724138,
        "MMLU_elementary_mathematics":0.253968254,
        "MMLU_formal_logic":0.1904761905,
        "MMLU_global_facts":0.2,
        "MMLU_high_school_biology":0.2935483871,
        "MMLU_high_school_chemistry":0.2807881773,
        "MMLU_high_school_computer_science":0.24,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.3585858586,
        "MMLU_high_school_government_and_politics":0.3264248705,
        "MMLU_high_school_macroeconomics":0.2948717949,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.2521008403,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.3266055046,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.2401960784,
        "MMLU_high_school_world_history":0.2447257384,
        "MMLU_human_aging":0.1434977578,
        "MMLU_human_sexuality":0.2671755725,
        "MMLU_international_law":0.3305785124,
        "MMLU_jurisprudence":0.2314814815,
        "MMLU_logical_fallacies":0.282208589,
        "MMLU_machine_learning":0.1785714286,
        "MMLU_management":0.3786407767,
        "MMLU_marketing":0.2136752137,
        "MMLU_medical_genetics":0.29,
        "MMLU_miscellaneous":0.2247765006,
        "MMLU_moral_disputes":0.2572254335,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.2352941176,
        "MMLU_philosophy":0.192926045,
        "MMLU_prehistory":0.225308642,
        "MMLU_professional_accounting":0.2375886525,
        "MMLU_professional_law":0.2470664928,
        "MMLU_professional_medicine":0.1654411765,
        "MMLU_professional_psychology":0.2418300654,
        "MMLU_public_relations":0.1909090909,
        "MMLU_security_studies":0.3265306122,
        "MMLU_sociology":0.223880597,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.2228915663,
        "MMLU_world_religions":0.2865497076
    },
    {
        "Model":"dlite-v1-774m",
        "URL":"https:\/\/huggingface.co\/aisquared\/dlite-v1-774m",
        "full_model_name":"aisquared\/dlite-v1-774m",
        "Parameters":0.774,
        "MMLU_average":0.2590675772,
        "arc:challenge|25":0.2534129693,
        "hellaswag|10":0.365365465,
        "MMLU_abstract_algebra":0.18,
        "MMLU_anatomy":0.2222222222,
        "MMLU_astronomy":0.2171052632,
        "MMLU_business_ethics":0.24,
        "MMLU_clinical_knowledge":0.320754717,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.18,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.2427745665,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.17,
        "MMLU_conceptual_physics":0.2085106383,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.3172413793,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.1587301587,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.2548387097,
        "MMLU_high_school_chemistry":0.3004926108,
        "MMLU_high_school_computer_science":0.3,
        "MMLU_high_school_european_history":0.2848484848,
        "MMLU_high_school_geography":0.3636363636,
        "MMLU_high_school_government_and_politics":0.3212435233,
        "MMLU_high_school_macroeconomics":0.3512820513,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.218487395,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.3174311927,
        "MMLU_high_school_statistics":0.3240740741,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.1076233184,
        "MMLU_human_sexuality":0.2977099237,
        "MMLU_international_law":0.3801652893,
        "MMLU_jurisprudence":0.1666666667,
        "MMLU_logical_fallacies":0.3190184049,
        "MMLU_machine_learning":0.2053571429,
        "MMLU_management":0.3689320388,
        "MMLU_marketing":0.2136752137,
        "MMLU_medical_genetics":0.17,
        "MMLU_miscellaneous":0.1979565773,
        "MMLU_moral_disputes":0.2630057803,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.2483660131,
        "MMLU_philosophy":0.2443729904,
        "MMLU_prehistory":0.2561728395,
        "MMLU_professional_accounting":0.2659574468,
        "MMLU_professional_law":0.2327249022,
        "MMLU_professional_medicine":0.2463235294,
        "MMLU_professional_psychology":0.2287581699,
        "MMLU_public_relations":0.1909090909,
        "MMLU_security_studies":0.3673469388,
        "MMLU_sociology":0.2935323383,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.265060241,
        "MMLU_world_religions":0.2923976608
    },
    {
        "Model":"Project-Baize-v2-13B-GPTQ",
        "URL":"https:\/\/huggingface.co\/TheBloke\/Project-Baize-v2-13B-GPTQ",
        "full_model_name":"TheBloke\/Project-Baize-v2-13B-GPTQ",
        "Parameters":13.0,
        "MMLU_average":0.2590555389,
        "arc:challenge|25":0.2457337884,
        "hellaswag|10":0.2552280422,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.3333333333,
        "MMLU_astronomy":0.2434210526,
        "MMLU_business_ethics":0.25,
        "MMLU_clinical_knowledge":0.2075471698,
        "MMLU_college_biology":0.2638888889,
        "MMLU_college_chemistry":0.28,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.225433526,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.3,
        "MMLU_conceptual_physics":0.2765957447,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.2689655172,
        "MMLU_elementary_mathematics":0.2407407407,
        "MMLU_formal_logic":0.2380952381,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.2322580645,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.27,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.2575757576,
        "MMLU_high_school_government_and_politics":0.2020725389,
        "MMLU_high_school_macroeconomics":0.2128205128,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.2142857143,
        "MMLU_high_school_physics":0.238410596,
        "MMLU_high_school_psychology":0.2256880734,
        "MMLU_high_school_statistics":0.2916666667,
        "MMLU_high_school_us_history":0.2647058824,
        "MMLU_high_school_world_history":0.2194092827,
        "MMLU_human_aging":0.2600896861,
        "MMLU_human_sexuality":0.1603053435,
        "MMLU_international_law":0.2727272727,
        "MMLU_jurisprudence":0.2777777778,
        "MMLU_logical_fallacies":0.263803681,
        "MMLU_machine_learning":0.2053571429,
        "MMLU_management":0.1844660194,
        "MMLU_marketing":0.2478632479,
        "MMLU_medical_genetics":0.19,
        "MMLU_miscellaneous":0.2720306513,
        "MMLU_moral_disputes":0.2630057803,
        "MMLU_moral_scenarios":0.2480446927,
        "MMLU_nutrition":0.2777777778,
        "MMLU_philosophy":0.2861736334,
        "MMLU_prehistory":0.3179012346,
        "MMLU_professional_accounting":0.2836879433,
        "MMLU_professional_law":0.2614080834,
        "MMLU_professional_medicine":0.3051470588,
        "MMLU_professional_psychology":0.2647058824,
        "MMLU_public_relations":0.3090909091,
        "MMLU_security_studies":0.3224489796,
        "MMLU_sociology":0.2835820896,
        "MMLU_us_foreign_policy":0.22,
        "MMLU_virology":0.2710843373,
        "MMLU_world_religions":0.216374269
    },
    {
        "Model":"pythia-70m",
        "URL":"https:\/\/huggingface.co\/EleutherAI\/pythia-70m",
        "full_model_name":"EleutherAI\/pythia-70m",
        "Parameters":0.07,
        "MMLU_average":0.2590069583,
        "arc:challenge|25":0.180887372,
        "hellaswag|10":0.2658832902,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.2740740741,
        "MMLU_astronomy":0.2368421053,
        "MMLU_business_ethics":0.25,
        "MMLU_clinical_knowledge":0.241509434,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.2947976879,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.25,
        "MMLU_conceptual_physics":0.2680851064,
        "MMLU_econometrics":0.201754386,
        "MMLU_electrical_engineering":0.2689655172,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.1666666667,
        "MMLU_global_facts":0.14,
        "MMLU_high_school_biology":0.2677419355,
        "MMLU_high_school_chemistry":0.2807881773,
        "MMLU_high_school_computer_science":0.29,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.3131313131,
        "MMLU_high_school_government_and_politics":0.3471502591,
        "MMLU_high_school_macroeconomics":0.3307692308,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.268907563,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.2587155963,
        "MMLU_high_school_statistics":0.4675925926,
        "MMLU_high_school_us_history":0.2352941176,
        "MMLU_high_school_world_history":0.2573839662,
        "MMLU_human_aging":0.2735426009,
        "MMLU_human_sexuality":0.2748091603,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2314814815,
        "MMLU_logical_fallacies":0.2699386503,
        "MMLU_machine_learning":0.2142857143,
        "MMLU_management":0.1844660194,
        "MMLU_marketing":0.1965811966,
        "MMLU_medical_genetics":0.29,
        "MMLU_miscellaneous":0.2171136654,
        "MMLU_moral_disputes":0.2456647399,
        "MMLU_moral_scenarios":0.2324022346,
        "MMLU_nutrition":0.2483660131,
        "MMLU_philosophy":0.2186495177,
        "MMLU_prehistory":0.2407407407,
        "MMLU_professional_accounting":0.2163120567,
        "MMLU_professional_law":0.2346805737,
        "MMLU_professional_medicine":0.3419117647,
        "MMLU_professional_psychology":0.2173202614,
        "MMLU_public_relations":0.2636363636,
        "MMLU_security_studies":0.2734693878,
        "MMLU_sociology":0.2786069652,
        "MMLU_us_foreign_policy":0.22,
        "MMLU_virology":0.2409638554,
        "MMLU_world_religions":0.2222222222
    },
    {
        "Model":"santacoder",
        "URL":"https:\/\/huggingface.co\/bigcode\/santacoder",
        "full_model_name":"bigcode\/santacoder",
        "Parameters":null,
        "MMLU_average":0.2589252902,
        "arc:challenge|25":0.2372013652,
        "hellaswag|10":0.255725951,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.2518518519,
        "MMLU_astronomy":0.3026315789,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.2716981132,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.23,
        "MMLU_college_computer_science":0.23,
        "MMLU_college_mathematics":0.2,
        "MMLU_college_medicine":0.2543352601,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.26,
        "MMLU_conceptual_physics":0.3234042553,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2206896552,
        "MMLU_elementary_mathematics":0.2407407407,
        "MMLU_formal_logic":0.2777777778,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.2677419355,
        "MMLU_high_school_chemistry":0.3251231527,
        "MMLU_high_school_computer_science":0.23,
        "MMLU_high_school_european_history":0.2727272727,
        "MMLU_high_school_geography":0.2171717172,
        "MMLU_high_school_government_and_politics":0.2487046632,
        "MMLU_high_school_macroeconomics":0.2205128205,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.231092437,
        "MMLU_high_school_physics":0.238410596,
        "MMLU_high_school_psychology":0.2440366972,
        "MMLU_high_school_statistics":0.2592592593,
        "MMLU_high_school_us_history":0.2794117647,
        "MMLU_high_school_world_history":0.2067510549,
        "MMLU_human_aging":0.3766816143,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.2644628099,
        "MMLU_jurisprudence":0.2962962963,
        "MMLU_logical_fallacies":0.245398773,
        "MMLU_machine_learning":0.2232142857,
        "MMLU_management":0.2524271845,
        "MMLU_marketing":0.2564102564,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.288633461,
        "MMLU_moral_disputes":0.274566474,
        "MMLU_moral_scenarios":0.2636871508,
        "MMLU_nutrition":0.2483660131,
        "MMLU_philosophy":0.2733118971,
        "MMLU_prehistory":0.2685185185,
        "MMLU_professional_accounting":0.2482269504,
        "MMLU_professional_law":0.2451108214,
        "MMLU_professional_medicine":0.3014705882,
        "MMLU_professional_psychology":0.2696078431,
        "MMLU_public_relations":0.3363636364,
        "MMLU_security_studies":0.3510204082,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.21,
        "MMLU_virology":0.3192771084,
        "MMLU_world_religions":0.2105263158
    },
    {
        "Model":"WizardCoder-Guanaco-15B-V1.1",
        "URL":"https:\/\/huggingface.co\/LoupGarou\/WizardCoder-Guanaco-15B-V1.1",
        "full_model_name":"LoupGarou\/WizardCoder-Guanaco-15B-V1.1",
        "Parameters":15.0,
        "MMLU_average":0.25881783,
        "arc:challenge|25":0.2883959044,
        "hellaswag|10":0.3720374427,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.2444444444,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2339622642,
        "MMLU_college_biology":0.25,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.3,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.39,
        "MMLU_conceptual_physics":0.3021276596,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.3448275862,
        "MMLU_elementary_mathematics":0.2275132275,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.23,
        "MMLU_high_school_biology":0.2161290323,
        "MMLU_high_school_chemistry":0.1724137931,
        "MMLU_high_school_computer_science":0.32,
        "MMLU_high_school_european_history":0.2121212121,
        "MMLU_high_school_geography":0.202020202,
        "MMLU_high_school_government_and_politics":0.207253886,
        "MMLU_high_school_macroeconomics":0.2153846154,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.243697479,
        "MMLU_high_school_physics":0.2450331126,
        "MMLU_high_school_psychology":0.2036697248,
        "MMLU_high_school_statistics":0.1759259259,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.2827004219,
        "MMLU_human_aging":0.33632287,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2892561983,
        "MMLU_jurisprudence":0.2407407407,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.2233009709,
        "MMLU_marketing":0.3418803419,
        "MMLU_medical_genetics":0.36,
        "MMLU_miscellaneous":0.2988505747,
        "MMLU_moral_disputes":0.2630057803,
        "MMLU_moral_scenarios":0.251396648,
        "MMLU_nutrition":0.2483660131,
        "MMLU_philosophy":0.2475884244,
        "MMLU_prehistory":0.2407407407,
        "MMLU_professional_accounting":0.2730496454,
        "MMLU_professional_law":0.2516297262,
        "MMLU_professional_medicine":0.2022058824,
        "MMLU_professional_psychology":0.272875817,
        "MMLU_public_relations":0.3,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.2686567164,
        "MMLU_us_foreign_policy":0.29,
        "MMLU_virology":0.3012048193,
        "MMLU_world_religions":0.3157894737
    },
    {
        "Model":"gpt2_platypus-camel_physics",
        "URL":"https:\/\/huggingface.co\/behnamsh\/gpt2_platypus-camel_physics",
        "full_model_name":"behnamsh\/gpt2_platypus-camel_physics",
        "Parameters":null,
        "MMLU_average":0.2586768503,
        "arc:challenge|25":0.1911262799,
        "hellaswag|10":0.2919737104,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.2962962963,
        "MMLU_astronomy":0.1973684211,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.2452830189,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.29,
        "MMLU_college_computer_science":0.27,
        "MMLU_college_mathematics":0.23,
        "MMLU_college_medicine":0.2369942197,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.19,
        "MMLU_conceptual_physics":0.2553191489,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.2063492063,
        "MMLU_global_facts":0.17,
        "MMLU_high_school_biology":0.3032258065,
        "MMLU_high_school_chemistry":0.2610837438,
        "MMLU_high_school_computer_science":0.19,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.3535353535,
        "MMLU_high_school_government_and_politics":0.3678756477,
        "MMLU_high_school_macroeconomics":0.3025641026,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.3151260504,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.3504587156,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2320675105,
        "MMLU_human_aging":0.1883408072,
        "MMLU_human_sexuality":0.2366412214,
        "MMLU_international_law":0.3636363636,
        "MMLU_jurisprudence":0.212962963,
        "MMLU_logical_fallacies":0.2515337423,
        "MMLU_machine_learning":0.1607142857,
        "MMLU_management":0.3786407767,
        "MMLU_marketing":0.2051282051,
        "MMLU_medical_genetics":0.29,
        "MMLU_miscellaneous":0.2043422733,
        "MMLU_moral_disputes":0.2080924855,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2483660131,
        "MMLU_philosophy":0.1961414791,
        "MMLU_prehistory":0.225308642,
        "MMLU_professional_accounting":0.2765957447,
        "MMLU_professional_law":0.2516297262,
        "MMLU_professional_medicine":0.4411764706,
        "MMLU_professional_psychology":0.2598039216,
        "MMLU_public_relations":0.2,
        "MMLU_security_studies":0.4,
        "MMLU_sociology":0.223880597,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.1927710843,
        "MMLU_world_religions":0.2105263158
    },
    {
        "Model":"finetuned-gpt2-tiny",
        "URL":"https:\/\/huggingface.co\/dpv\/finetuned-gpt2-tiny",
        "full_model_name":"dpv\/finetuned-gpt2-tiny",
        "Parameters":null,
        "MMLU_average":0.2585851394,
        "arc:challenge|25":0.1996587031,
        "hellaswag|10":0.2922724557,
        "MMLU_abstract_algebra":0.21,
        "MMLU_anatomy":0.2444444444,
        "MMLU_astronomy":0.1644736842,
        "MMLU_business_ethics":0.17,
        "MMLU_clinical_knowledge":0.2301886792,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.3,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.2369942197,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.17,
        "MMLU_conceptual_physics":0.2680851064,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.253968254,
        "MMLU_formal_logic":0.1428571429,
        "MMLU_global_facts":0.15,
        "MMLU_high_school_biology":0.3,
        "MMLU_high_school_chemistry":0.275862069,
        "MMLU_high_school_computer_science":0.26,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.3535353535,
        "MMLU_high_school_government_and_politics":0.3678756477,
        "MMLU_high_school_macroeconomics":0.2743589744,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.2899159664,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.3467889908,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2489451477,
        "MMLU_human_aging":0.2914798206,
        "MMLU_human_sexuality":0.2671755725,
        "MMLU_international_law":0.3305785124,
        "MMLU_jurisprudence":0.212962963,
        "MMLU_logical_fallacies":0.2576687117,
        "MMLU_machine_learning":0.2410714286,
        "MMLU_management":0.3398058252,
        "MMLU_marketing":0.1794871795,
        "MMLU_medical_genetics":0.27,
        "MMLU_miscellaneous":0.2183908046,
        "MMLU_moral_disputes":0.2398843931,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2189542484,
        "MMLU_philosophy":0.2508038585,
        "MMLU_prehistory":0.225308642,
        "MMLU_professional_accounting":0.2695035461,
        "MMLU_professional_law":0.2477183833,
        "MMLU_professional_medicine":0.4411764706,
        "MMLU_professional_psychology":0.2647058824,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.4,
        "MMLU_sociology":0.2288557214,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.1927710843,
        "MMLU_world_religions":0.2105263158
    },
    {
        "Model":"wizard-vicuna-13B-GPTQ",
        "URL":"https:\/\/huggingface.co\/TheBloke\/wizard-vicuna-13B-GPTQ",
        "full_model_name":"TheBloke\/wizard-vicuna-13B-GPTQ",
        "Parameters":13.0,
        "MMLU_average":0.2584284621,
        "arc:challenge|25":0.2346416382,
        "hellaswag|10":0.2555267875,
        "MMLU_abstract_algebra":0.21,
        "MMLU_anatomy":0.2296296296,
        "MMLU_astronomy":0.2697368421,
        "MMLU_business_ethics":0.18,
        "MMLU_clinical_knowledge":0.2981132075,
        "MMLU_college_biology":0.2013888889,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.3,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.3121387283,
        "MMLU_college_physics":0.3431372549,
        "MMLU_computer_security":0.19,
        "MMLU_conceptual_physics":0.2085106383,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2592592593,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.17,
        "MMLU_high_school_biology":0.2741935484,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.24,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.3484848485,
        "MMLU_high_school_government_and_politics":0.3367875648,
        "MMLU_high_school_macroeconomics":0.3615384615,
        "MMLU_high_school_mathematics":0.237037037,
        "MMLU_high_school_microeconomics":0.3529411765,
        "MMLU_high_school_physics":0.2251655629,
        "MMLU_high_school_psychology":0.2862385321,
        "MMLU_high_school_statistics":0.375,
        "MMLU_high_school_us_history":0.2794117647,
        "MMLU_high_school_world_history":0.2489451477,
        "MMLU_human_aging":0.1076233184,
        "MMLU_human_sexuality":0.2824427481,
        "MMLU_international_law":0.2231404959,
        "MMLU_jurisprudence":0.2222222222,
        "MMLU_logical_fallacies":0.2331288344,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.3786407767,
        "MMLU_marketing":0.1965811966,
        "MMLU_medical_genetics":0.24,
        "MMLU_miscellaneous":0.2043422733,
        "MMLU_moral_disputes":0.2398843931,
        "MMLU_moral_scenarios":0.2960893855,
        "MMLU_nutrition":0.2875816993,
        "MMLU_philosophy":0.2443729904,
        "MMLU_prehistory":0.2283950617,
        "MMLU_professional_accounting":0.2553191489,
        "MMLU_professional_law":0.2627118644,
        "MMLU_professional_medicine":0.2536764706,
        "MMLU_professional_psychology":0.2467320261,
        "MMLU_public_relations":0.2454545455,
        "MMLU_security_studies":0.2653061224,
        "MMLU_sociology":0.263681592,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.1987951807,
        "MMLU_world_religions":0.1754385965
    },
    {
        "Model":"WizardLM-33B-V1.0-Uncensored-GPTQ",
        "URL":"https:\/\/huggingface.co\/TheBloke\/WizardLM-33B-V1.0-Uncensored-GPTQ",
        "full_model_name":"TheBloke\/WizardLM-33B-V1.0-Uncensored-GPTQ",
        "Parameters":33.0,
        "MMLU_average":0.2581008553,
        "arc:challenge|25":0.2363481229,
        "hellaswag|10":0.2528380801,
        "MMLU_abstract_algebra":0.21,
        "MMLU_anatomy":0.2296296296,
        "MMLU_astronomy":0.2960526316,
        "MMLU_business_ethics":0.26,
        "MMLU_clinical_knowledge":0.2981132075,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.32,
        "MMLU_college_computer_science":0.25,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.3352601156,
        "MMLU_college_physics":0.3529411765,
        "MMLU_computer_security":0.18,
        "MMLU_conceptual_physics":0.2085106383,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2936507937,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.2935483871,
        "MMLU_high_school_chemistry":0.2512315271,
        "MMLU_high_school_computer_science":0.23,
        "MMLU_high_school_european_history":0.2545454545,
        "MMLU_high_school_geography":0.3535353535,
        "MMLU_high_school_government_and_politics":0.3523316062,
        "MMLU_high_school_macroeconomics":0.3615384615,
        "MMLU_high_school_mathematics":0.2185185185,
        "MMLU_high_school_microeconomics":0.3487394958,
        "MMLU_high_school_physics":0.3774834437,
        "MMLU_high_school_psychology":0.3137614679,
        "MMLU_high_school_statistics":0.3101851852,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.2573839662,
        "MMLU_human_aging":0.1076233184,
        "MMLU_human_sexuality":0.2824427481,
        "MMLU_international_law":0.1652892562,
        "MMLU_jurisprudence":0.212962963,
        "MMLU_logical_fallacies":0.2331288344,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.3786407767,
        "MMLU_marketing":0.1965811966,
        "MMLU_medical_genetics":0.24,
        "MMLU_miscellaneous":0.2056194125,
        "MMLU_moral_disputes":0.2456647399,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2614379085,
        "MMLU_philosophy":0.2411575563,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2056737589,
        "MMLU_professional_law":0.2398956975,
        "MMLU_professional_medicine":0.3051470588,
        "MMLU_professional_psychology":0.2058823529,
        "MMLU_public_relations":0.2272727273,
        "MMLU_security_studies":0.2,
        "MMLU_sociology":0.2885572139,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.1927710843,
        "MMLU_world_religions":0.1754385965
    },
    {
        "Model":"ShortKingv0.1",
        "URL":"https:\/\/huggingface.co\/AtAndDev\/ShortKingv0.1",
        "full_model_name":"AtAndDev\/ShortKingv0.1",
        "Parameters":null,
        "MMLU_average":0.2577993885,
        "arc:challenge|25":0.3088737201,
        "hellaswag|10":0.4230233021,
        "MMLU_abstract_algebra":0.2,
        "MMLU_anatomy":0.3555555556,
        "MMLU_astronomy":0.1710526316,
        "MMLU_business_ethics":0.31,
        "MMLU_clinical_knowledge":0.2641509434,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.18,
        "MMLU_college_computer_science":0.35,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.2138728324,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.32,
        "MMLU_conceptual_physics":0.2595744681,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.3103448276,
        "MMLU_elementary_mathematics":0.2619047619,
        "MMLU_formal_logic":0.2142857143,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.2677419355,
        "MMLU_high_school_chemistry":0.2463054187,
        "MMLU_high_school_computer_science":0.3,
        "MMLU_high_school_european_history":0.2848484848,
        "MMLU_high_school_geography":0.2323232323,
        "MMLU_high_school_government_and_politics":0.2279792746,
        "MMLU_high_school_macroeconomics":0.2128205128,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.2330275229,
        "MMLU_high_school_statistics":0.4259259259,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2447257384,
        "MMLU_human_aging":0.2197309417,
        "MMLU_human_sexuality":0.2213740458,
        "MMLU_international_law":0.2479338843,
        "MMLU_jurisprudence":0.287037037,
        "MMLU_logical_fallacies":0.3006134969,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2435897436,
        "MMLU_medical_genetics":0.27,
        "MMLU_miscellaneous":0.2567049808,
        "MMLU_moral_disputes":0.2658959538,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2516339869,
        "MMLU_philosophy":0.2572347267,
        "MMLU_prehistory":0.2808641975,
        "MMLU_professional_accounting":0.2411347518,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.2205882353,
        "MMLU_professional_psychology":0.2581699346,
        "MMLU_public_relations":0.2454545455,
        "MMLU_security_studies":0.1714285714,
        "MMLU_sociology":0.2786069652,
        "MMLU_us_foreign_policy":0.3,
        "MMLU_virology":0.2469879518,
        "MMLU_world_religions":0.2807017544
    },
    {
        "Model":"rwkv-4-1b5-pile",
        "URL":"https:\/\/huggingface.co\/RWKV\/rwkv-4-1b5-pile",
        "full_model_name":"RWKV\/rwkv-4-1b5-pile",
        "Parameters":1.0,
        "MMLU_average":0.2577446961,
        "arc:challenge|25":0.2764505119,
        "hellaswag|10":0.4033061143,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.2962962963,
        "MMLU_astronomy":0.2763157895,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.2301886792,
        "MMLU_college_biology":0.2430555556,
        "MMLU_college_chemistry":0.18,
        "MMLU_college_computer_science":0.16,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.2601156069,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.3,
        "MMLU_conceptual_physics":0.285106383,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2275862069,
        "MMLU_elementary_mathematics":0.2486772487,
        "MMLU_formal_logic":0.1666666667,
        "MMLU_global_facts":0.25,
        "MMLU_high_school_biology":0.2774193548,
        "MMLU_high_school_chemistry":0.2955665025,
        "MMLU_high_school_computer_science":0.35,
        "MMLU_high_school_european_history":0.2848484848,
        "MMLU_high_school_geography":0.2070707071,
        "MMLU_high_school_government_and_politics":0.2331606218,
        "MMLU_high_school_macroeconomics":0.241025641,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.178807947,
        "MMLU_high_school_psychology":0.2311926606,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.2742616034,
        "MMLU_human_aging":0.33632287,
        "MMLU_human_sexuality":0.2442748092,
        "MMLU_international_law":0.3305785124,
        "MMLU_jurisprudence":0.2222222222,
        "MMLU_logical_fallacies":0.226993865,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.2524271845,
        "MMLU_marketing":0.2948717949,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2618135377,
        "MMLU_moral_disputes":0.2947976879,
        "MMLU_moral_scenarios":0.2726256983,
        "MMLU_nutrition":0.2287581699,
        "MMLU_philosophy":0.2540192926,
        "MMLU_prehistory":0.25,
        "MMLU_professional_accounting":0.2695035461,
        "MMLU_professional_law":0.2659713168,
        "MMLU_professional_medicine":0.2573529412,
        "MMLU_professional_psychology":0.2794117647,
        "MMLU_public_relations":0.3,
        "MMLU_security_studies":0.2285714286,
        "MMLU_sociology":0.2139303483,
        "MMLU_us_foreign_policy":0.23,
        "MMLU_virology":0.3012048193,
        "MMLU_world_religions":0.2105263158
    },
    {
        "Model":"GPTNeo350M-Instruct-SFT",
        "URL":"https:\/\/huggingface.co\/SummerSigh\/GPTNeo350M-Instruct-SFT",
        "full_model_name":"SummerSigh\/GPTNeo350M-Instruct-SFT",
        "Parameters":0.35,
        "MMLU_average":0.2576247009,
        "arc:challenge|25":0.226109215,
        "hellaswag|10":0.3304122685,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.1925925926,
        "MMLU_astronomy":0.1710526316,
        "MMLU_business_ethics":0.28,
        "MMLU_clinical_knowledge":0.2339622642,
        "MMLU_college_biology":0.2430555556,
        "MMLU_college_chemistry":0.22,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.2369942197,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.22,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2105263158,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2248677249,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.19,
        "MMLU_high_school_biology":0.2580645161,
        "MMLU_high_school_chemistry":0.2709359606,
        "MMLU_high_school_computer_science":0.27,
        "MMLU_high_school_european_history":0.2484848485,
        "MMLU_high_school_geography":0.202020202,
        "MMLU_high_school_government_and_politics":0.3005181347,
        "MMLU_high_school_macroeconomics":0.3564102564,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.2352941176,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.2825688073,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.2254901961,
        "MMLU_high_school_world_history":0.2320675105,
        "MMLU_human_aging":0.1614349776,
        "MMLU_human_sexuality":0.2824427481,
        "MMLU_international_law":0.2479338843,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.226993865,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.359223301,
        "MMLU_marketing":0.1965811966,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.245210728,
        "MMLU_moral_disputes":0.2427745665,
        "MMLU_moral_scenarios":0.2558659218,
        "MMLU_nutrition":0.2287581699,
        "MMLU_philosophy":0.2797427653,
        "MMLU_prehistory":0.212962963,
        "MMLU_professional_accounting":0.2446808511,
        "MMLU_professional_law":0.2372881356,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2254901961,
        "MMLU_public_relations":0.2363636364,
        "MMLU_security_studies":0.2775510204,
        "MMLU_sociology":0.2537313433,
        "MMLU_us_foreign_policy":0.31,
        "MMLU_virology":0.2710843373,
        "MMLU_world_religions":0.2339181287
    },
    {
        "Model":"gpt2-alpaca-gpt4",
        "URL":"https:\/\/huggingface.co\/vicgalle\/gpt2-alpaca-gpt4",
        "full_model_name":"vicgalle\/gpt2-alpaca-gpt4",
        "Parameters":null,
        "MMLU_average":0.257590005,
        "arc:challenge|25":0.20221843,
        "hellaswag|10":0.2907787293,
        "MMLU_abstract_algebra":0.17,
        "MMLU_anatomy":0.2740740741,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.2,
        "MMLU_clinical_knowledge":0.2377358491,
        "MMLU_college_biology":0.2361111111,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.29,
        "MMLU_college_mathematics":0.19,
        "MMLU_college_medicine":0.2485549133,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.19,
        "MMLU_conceptual_physics":0.2723404255,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.2513227513,
        "MMLU_formal_logic":0.1507936508,
        "MMLU_global_facts":0.15,
        "MMLU_high_school_biology":0.2677419355,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.31,
        "MMLU_high_school_european_history":0.2,
        "MMLU_high_school_geography":0.3535353535,
        "MMLU_high_school_government_and_politics":0.3626943005,
        "MMLU_high_school_macroeconomics":0.2820512821,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.2941176471,
        "MMLU_high_school_physics":0.2119205298,
        "MMLU_high_school_psychology":0.2844036697,
        "MMLU_high_school_statistics":0.4583333333,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.253164557,
        "MMLU_human_aging":0.3497757848,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.3553719008,
        "MMLU_jurisprudence":0.1851851852,
        "MMLU_logical_fallacies":0.3251533742,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.1965811966,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2950191571,
        "MMLU_moral_disputes":0.2225433526,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2549019608,
        "MMLU_philosophy":0.2154340836,
        "MMLU_prehistory":0.225308642,
        "MMLU_professional_accounting":0.2730496454,
        "MMLU_professional_law":0.2574967405,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2532679739,
        "MMLU_public_relations":0.2,
        "MMLU_security_studies":0.387755102,
        "MMLU_sociology":0.223880597,
        "MMLU_us_foreign_policy":0.29,
        "MMLU_virology":0.2108433735,
        "MMLU_world_religions":0.2046783626
    },
    {
        "Model":"black_goo_recipe_a",
        "URL":"https:\/\/huggingface.co\/KnutJaegersberg\/black_goo_recipe_a",
        "full_model_name":"KnutJaegersberg\/black_goo_recipe_a",
        "Parameters":null,
        "MMLU_average":0.2575080713,
        "arc:challenge|25":0.3549488055,
        "hellaswag|10":0.4876518622,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.2592592593,
        "MMLU_astronomy":0.1842105263,
        "MMLU_business_ethics":0.25,
        "MMLU_clinical_knowledge":0.279245283,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.27,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.2658959538,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.25,
        "MMLU_conceptual_physics":0.3234042553,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2068965517,
        "MMLU_elementary_mathematics":0.2486772487,
        "MMLU_formal_logic":0.1904761905,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.2741935484,
        "MMLU_high_school_chemistry":0.2660098522,
        "MMLU_high_school_computer_science":0.24,
        "MMLU_high_school_european_history":0.2606060606,
        "MMLU_high_school_geography":0.2525252525,
        "MMLU_high_school_government_and_politics":0.2124352332,
        "MMLU_high_school_macroeconomics":0.2307692308,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.243697479,
        "MMLU_high_school_physics":0.2185430464,
        "MMLU_high_school_psychology":0.2403669725,
        "MMLU_high_school_statistics":0.2962962963,
        "MMLU_high_school_us_history":0.2352941176,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3856502242,
        "MMLU_human_sexuality":0.2366412214,
        "MMLU_international_law":0.2479338843,
        "MMLU_jurisprudence":0.2962962963,
        "MMLU_logical_fallacies":0.2392638037,
        "MMLU_machine_learning":0.2410714286,
        "MMLU_management":0.2524271845,
        "MMLU_marketing":0.264957265,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.2860791826,
        "MMLU_moral_disputes":0.2601156069,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2549019608,
        "MMLU_philosophy":0.2733118971,
        "MMLU_prehistory":0.2685185185,
        "MMLU_professional_accounting":0.2588652482,
        "MMLU_professional_law":0.2411994785,
        "MMLU_professional_medicine":0.3933823529,
        "MMLU_professional_psychology":0.2483660131,
        "MMLU_public_relations":0.2727272727,
        "MMLU_security_studies":0.2040816327,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.21,
        "MMLU_virology":0.3192771084,
        "MMLU_world_religions":0.2514619883
    },
    {
        "Model":"test-2048-1500ck",
        "URL":"https:\/\/huggingface.co\/NoIdeaLand\/test-2048-1500ck",
        "full_model_name":"NoIdeaLand\/test-2048-1500ck",
        "Parameters":null,
        "MMLU_average":0.2572316794,
        "arc:challenge|25":0.3353242321,
        "hellaswag|10":0.4581756622,
        "MMLU_abstract_algebra":0.21,
        "MMLU_anatomy":0.2444444444,
        "MMLU_astronomy":0.1907894737,
        "MMLU_business_ethics":0.35,
        "MMLU_clinical_knowledge":0.241509434,
        "MMLU_college_biology":0.2986111111,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.18,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.2023121387,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.3191489362,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.2896551724,
        "MMLU_elementary_mathematics":0.2328042328,
        "MMLU_formal_logic":0.2301587302,
        "MMLU_global_facts":0.19,
        "MMLU_high_school_biology":0.2225806452,
        "MMLU_high_school_chemistry":0.236453202,
        "MMLU_high_school_computer_science":0.19,
        "MMLU_high_school_european_history":0.3151515152,
        "MMLU_high_school_geography":0.196969697,
        "MMLU_high_school_government_and_politics":0.3056994819,
        "MMLU_high_school_macroeconomics":0.2692307692,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.2226890756,
        "MMLU_high_school_physics":0.2119205298,
        "MMLU_high_school_psychology":0.2201834862,
        "MMLU_high_school_statistics":0.2361111111,
        "MMLU_high_school_us_history":0.2843137255,
        "MMLU_high_school_world_history":0.2869198312,
        "MMLU_human_aging":0.3946188341,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2479338843,
        "MMLU_jurisprudence":0.2777777778,
        "MMLU_logical_fallacies":0.2515337423,
        "MMLU_machine_learning":0.2946428571,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.3076923077,
        "MMLU_medical_genetics":0.32,
        "MMLU_miscellaneous":0.2835249042,
        "MMLU_moral_disputes":0.274566474,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2745098039,
        "MMLU_philosophy":0.2347266881,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2553191489,
        "MMLU_professional_law":0.2627118644,
        "MMLU_professional_medicine":0.2169117647,
        "MMLU_professional_psychology":0.2745098039,
        "MMLU_public_relations":0.2818181818,
        "MMLU_security_studies":0.1836734694,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.37,
        "MMLU_virology":0.2951807229,
        "MMLU_world_religions":0.3157894737
    },
    {
        "Model":"Sheared-LLaMA-1.3B",
        "URL":"https:\/\/huggingface.co\/princeton-nlp\/Sheared-LLaMA-1.3B",
        "full_model_name":"princeton-nlp\/Sheared-LLaMA-1.3B",
        "Parameters":1.3,
        "MMLU_average":0.2570506222,
        "arc:challenge|25":0.2909556314,
        "hellaswag|10":0.4552877913,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.2888888889,
        "MMLU_astronomy":0.1842105263,
        "MMLU_business_ethics":0.31,
        "MMLU_clinical_knowledge":0.279245283,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.29,
        "MMLU_college_mathematics":0.22,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.31,
        "MMLU_conceptual_physics":0.3319148936,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.2344827586,
        "MMLU_elementary_mathematics":0.2433862434,
        "MMLU_formal_logic":0.1666666667,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.2483870968,
        "MMLU_high_school_chemistry":0.3004926108,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.2121212121,
        "MMLU_high_school_government_and_politics":0.207253886,
        "MMLU_high_school_macroeconomics":0.2615384615,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.2352941176,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.223853211,
        "MMLU_high_school_statistics":0.212962963,
        "MMLU_high_school_us_history":0.2794117647,
        "MMLU_high_school_world_history":0.2784810127,
        "MMLU_human_aging":0.3497757848,
        "MMLU_human_sexuality":0.2442748092,
        "MMLU_international_law":0.3388429752,
        "MMLU_jurisprudence":0.287037037,
        "MMLU_logical_fallacies":0.2392638037,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.2330097087,
        "MMLU_marketing":0.311965812,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.2720306513,
        "MMLU_moral_disputes":0.2398843931,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2614379085,
        "MMLU_philosophy":0.270096463,
        "MMLU_prehistory":0.2530864198,
        "MMLU_professional_accounting":0.280141844,
        "MMLU_professional_law":0.2614080834,
        "MMLU_professional_medicine":0.1801470588,
        "MMLU_professional_psychology":0.2794117647,
        "MMLU_public_relations":0.3090909091,
        "MMLU_security_studies":0.2367346939,
        "MMLU_sociology":0.2189054726,
        "MMLU_us_foreign_policy":0.24,
        "MMLU_virology":0.2530120482,
        "MMLU_world_religions":0.269005848
    },
    {
        "Model":"WizardVicuna-Uncensored-3B-instruct-PL-lora_unload",
        "URL":"https:\/\/huggingface.co\/Aspik101\/WizardVicuna-Uncensored-3B-instruct-PL-lora_unload",
        "full_model_name":"Aspik101\/WizardVicuna-Uncensored-3B-instruct-PL-lora_unload",
        "Parameters":3.0,
        "MMLU_average":0.2568805093,
        "arc:challenge|25":0.3865187713,
        "hellaswag|10":0.5086636128,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.2962962963,
        "MMLU_astronomy":0.1710526316,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2716981132,
        "MMLU_college_biology":0.2708333333,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.27,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.2716763006,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.32,
        "MMLU_conceptual_physics":0.3106382979,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2827586207,
        "MMLU_elementary_mathematics":0.2275132275,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.25,
        "MMLU_high_school_biology":0.2290322581,
        "MMLU_high_school_chemistry":0.2660098522,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2,
        "MMLU_high_school_geography":0.1616161616,
        "MMLU_high_school_government_and_politics":0.2279792746,
        "MMLU_high_school_macroeconomics":0.2358974359,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.2016806723,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.1926605505,
        "MMLU_high_school_statistics":0.2083333333,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.2616033755,
        "MMLU_human_aging":0.3408071749,
        "MMLU_human_sexuality":0.1984732824,
        "MMLU_international_law":0.2231404959,
        "MMLU_jurisprudence":0.2962962963,
        "MMLU_logical_fallacies":0.2331288344,
        "MMLU_machine_learning":0.2142857143,
        "MMLU_management":0.2524271845,
        "MMLU_marketing":0.2692307692,
        "MMLU_medical_genetics":0.25,
        "MMLU_miscellaneous":0.288633461,
        "MMLU_moral_disputes":0.2976878613,
        "MMLU_moral_scenarios":0.2458100559,
        "MMLU_nutrition":0.2581699346,
        "MMLU_philosophy":0.2926045016,
        "MMLU_prehistory":0.2469135802,
        "MMLU_professional_accounting":0.2375886525,
        "MMLU_professional_law":0.2398956975,
        "MMLU_professional_medicine":0.375,
        "MMLU_professional_psychology":0.2565359477,
        "MMLU_public_relations":0.3090909091,
        "MMLU_security_studies":0.1918367347,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.2891566265,
        "MMLU_world_religions":0.3333333333
    },
    {
        "Model":"black_goo_recipe_b",
        "URL":"https:\/\/huggingface.co\/KnutJaegersberg\/black_goo_recipe_b",
        "full_model_name":"KnutJaegersberg\/black_goo_recipe_b",
        "Parameters":null,
        "MMLU_average":0.2567501296,
        "arc:challenge|25":0.3549488055,
        "hellaswag|10":0.4896434973,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.2148148148,
        "MMLU_astronomy":0.2302631579,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.2754716981,
        "MMLU_college_biology":0.2430555556,
        "MMLU_college_chemistry":0.22,
        "MMLU_college_computer_science":0.22,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.2601156069,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.25,
        "MMLU_conceptual_physics":0.3276595745,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2344827586,
        "MMLU_elementary_mathematics":0.2645502646,
        "MMLU_formal_logic":0.2063492063,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.2548387097,
        "MMLU_high_school_chemistry":0.236453202,
        "MMLU_high_school_computer_science":0.2,
        "MMLU_high_school_european_history":0.2545454545,
        "MMLU_high_school_geography":0.2878787879,
        "MMLU_high_school_government_and_politics":0.2435233161,
        "MMLU_high_school_macroeconomics":0.2615384615,
        "MMLU_high_school_mathematics":0.2333333333,
        "MMLU_high_school_microeconomics":0.2478991597,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.2825688073,
        "MMLU_high_school_statistics":0.1851851852,
        "MMLU_high_school_us_history":0.2696078431,
        "MMLU_high_school_world_history":0.2827004219,
        "MMLU_human_aging":0.3004484305,
        "MMLU_human_sexuality":0.2213740458,
        "MMLU_international_law":0.347107438,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.1840490798,
        "MMLU_machine_learning":0.2053571429,
        "MMLU_management":0.2621359223,
        "MMLU_marketing":0.2606837607,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.2873563218,
        "MMLU_moral_disputes":0.274566474,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2352941176,
        "MMLU_philosophy":0.270096463,
        "MMLU_prehistory":0.2685185185,
        "MMLU_professional_accounting":0.2553191489,
        "MMLU_professional_law":0.2301173403,
        "MMLU_professional_medicine":0.3272058824,
        "MMLU_professional_psychology":0.2565359477,
        "MMLU_public_relations":0.3,
        "MMLU_security_studies":0.3102040816,
        "MMLU_sociology":0.2338308458,
        "MMLU_us_foreign_policy":0.22,
        "MMLU_virology":0.343373494,
        "MMLU_world_religions":0.2631578947
    },
    {
        "Model":"test_llama2_ko_7b",
        "URL":"https:\/\/huggingface.co\/yeen214\/test_llama2_ko_7b",
        "full_model_name":"yeen214\/test_llama2_ko_7b",
        "Parameters":7.0,
        "MMLU_average":0.2562136479,
        "arc:challenge|25":0.2252559727,
        "hellaswag|10":0.2563234415,
        "MMLU_abstract_algebra":0.21,
        "MMLU_anatomy":0.237037037,
        "MMLU_astronomy":0.3355263158,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.2188679245,
        "MMLU_college_biology":0.2083333333,
        "MMLU_college_chemistry":0.26,
        "MMLU_college_computer_science":0.28,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.2774566474,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.24,
        "MMLU_conceptual_physics":0.2936170213,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.3034482759,
        "MMLU_elementary_mathematics":0.2830687831,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.27,
        "MMLU_high_school_biology":0.2806451613,
        "MMLU_high_school_chemistry":0.3054187192,
        "MMLU_high_school_computer_science":0.19,
        "MMLU_high_school_european_history":0.2545454545,
        "MMLU_high_school_geography":0.2777777778,
        "MMLU_high_school_government_and_politics":0.378238342,
        "MMLU_high_school_macroeconomics":0.2794871795,
        "MMLU_high_school_mathematics":0.237037037,
        "MMLU_high_school_microeconomics":0.2605042017,
        "MMLU_high_school_physics":0.238410596,
        "MMLU_high_school_psychology":0.2128440367,
        "MMLU_high_school_statistics":0.287037037,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2194092827,
        "MMLU_human_aging":0.1704035874,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.2561983471,
        "MMLU_jurisprudence":0.2314814815,
        "MMLU_logical_fallacies":0.2331288344,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.3009708738,
        "MMLU_marketing":0.2051282051,
        "MMLU_medical_genetics":0.27,
        "MMLU_miscellaneous":0.2311621967,
        "MMLU_moral_disputes":0.2312138728,
        "MMLU_moral_scenarios":0.2782122905,
        "MMLU_nutrition":0.2483660131,
        "MMLU_philosophy":0.2733118971,
        "MMLU_prehistory":0.2685185185,
        "MMLU_professional_accounting":0.2765957447,
        "MMLU_professional_law":0.2679269883,
        "MMLU_professional_medicine":0.2977941176,
        "MMLU_professional_psychology":0.2434640523,
        "MMLU_public_relations":0.2,
        "MMLU_security_studies":0.2775510204,
        "MMLU_sociology":0.2686567164,
        "MMLU_us_foreign_policy":0.2,
        "MMLU_virology":0.2951807229,
        "MMLU_world_religions":0.2456140351
    },
    {
        "Model":"dlite-v1-1_5b",
        "URL":"https:\/\/huggingface.co\/aisquared\/dlite-v1-1_5b",
        "full_model_name":"aisquared\/dlite-v1-1_5b",
        "Parameters":5.0,
        "MMLU_average":0.2561721011,
        "arc:challenge|25":0.2849829352,
        "hellaswag|10":0.3949412468,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.2518518519,
        "MMLU_astronomy":0.2039473684,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.2830188679,
        "MMLU_college_biology":0.2986111111,
        "MMLU_college_chemistry":0.22,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.3526011561,
        "MMLU_college_physics":0.1764705882,
        "MMLU_computer_security":0.26,
        "MMLU_conceptual_physics":0.2510638298,
        "MMLU_econometrics":0.2105263158,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.253968254,
        "MMLU_formal_logic":0.3571428571,
        "MMLU_global_facts":0.15,
        "MMLU_high_school_biology":0.2548387097,
        "MMLU_high_school_chemistry":0.1871921182,
        "MMLU_high_school_computer_science":0.24,
        "MMLU_high_school_european_history":0.2848484848,
        "MMLU_high_school_geography":0.3636363636,
        "MMLU_high_school_government_and_politics":0.2227979275,
        "MMLU_high_school_macroeconomics":0.341025641,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.2352941176,
        "MMLU_high_school_physics":0.2251655629,
        "MMLU_high_school_psychology":0.3321100917,
        "MMLU_high_school_statistics":0.3518518519,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.253164557,
        "MMLU_human_aging":0.1704035874,
        "MMLU_human_sexuality":0.2671755725,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.25,
        "MMLU_logical_fallacies":0.2515337423,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.2718446602,
        "MMLU_marketing":0.2692307692,
        "MMLU_medical_genetics":0.24,
        "MMLU_miscellaneous":0.2720306513,
        "MMLU_moral_disputes":0.2456647399,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.1897106109,
        "MMLU_prehistory":0.2561728395,
        "MMLU_professional_accounting":0.2375886525,
        "MMLU_professional_law":0.2411994785,
        "MMLU_professional_medicine":0.2536764706,
        "MMLU_professional_psychology":0.2450980392,
        "MMLU_public_relations":0.3363636364,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.2537313433,
        "MMLU_us_foreign_policy":0.19,
        "MMLU_virology":0.3313253012,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"SparseOPT-1.3B",
        "URL":"https:\/\/huggingface.co\/shaohang\/SparseOPT-1.3B",
        "full_model_name":"shaohang\/SparseOPT-1.3B",
        "Parameters":1.3,
        "MMLU_average":0.2559628529,
        "arc:challenge|25":0.2406143345,
        "hellaswag|10":0.3836885083,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.2148148148,
        "MMLU_astronomy":0.1578947368,
        "MMLU_business_ethics":0.2,
        "MMLU_clinical_knowledge":0.2113207547,
        "MMLU_college_biology":0.2083333333,
        "MMLU_college_chemistry":0.19,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.2601156069,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.3,
        "MMLU_conceptual_physics":0.2127659574,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.3333333333,
        "MMLU_global_facts":0.2,
        "MMLU_high_school_biology":0.2322580645,
        "MMLU_high_school_chemistry":0.2315270936,
        "MMLU_high_school_computer_science":0.23,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.2474747475,
        "MMLU_high_school_government_and_politics":0.3005181347,
        "MMLU_high_school_macroeconomics":0.3487179487,
        "MMLU_high_school_mathematics":0.2148148148,
        "MMLU_high_school_microeconomics":0.3445378151,
        "MMLU_high_school_physics":0.2450331126,
        "MMLU_high_school_psychology":0.2495412844,
        "MMLU_high_school_statistics":0.4305555556,
        "MMLU_high_school_us_history":0.1960784314,
        "MMLU_high_school_world_history":0.2573839662,
        "MMLU_human_aging":0.3094170404,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2148760331,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2392638037,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.3689320388,
        "MMLU_marketing":0.264957265,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.2286079183,
        "MMLU_moral_disputes":0.2514450867,
        "MMLU_moral_scenarios":0.2357541899,
        "MMLU_nutrition":0.2908496732,
        "MMLU_philosophy":0.2057877814,
        "MMLU_prehistory":0.25,
        "MMLU_professional_accounting":0.2624113475,
        "MMLU_professional_law":0.2385919166,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2549019608,
        "MMLU_public_relations":0.2363636364,
        "MMLU_security_studies":0.1714285714,
        "MMLU_sociology":0.2288557214,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.216374269
    },
    {
        "Model":"LLmRa-2.7B",
        "URL":"https:\/\/huggingface.co\/L-R\/LLmRa-2.7B",
        "full_model_name":"L-R\/LLmRa-2.7B",
        "Parameters":2.7,
        "MMLU_average":0.2558131245,
        "arc:challenge|25":0.3208191126,
        "hellaswag|10":0.4561840271,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.3333333333,
        "MMLU_astronomy":0.2105263158,
        "MMLU_business_ethics":0.25,
        "MMLU_clinical_knowledge":0.2226415094,
        "MMLU_college_biology":0.2638888889,
        "MMLU_college_chemistry":0.18,
        "MMLU_college_computer_science":0.28,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.2369942197,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.33,
        "MMLU_conceptual_physics":0.2425531915,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2965517241,
        "MMLU_elementary_mathematics":0.2301587302,
        "MMLU_formal_logic":0.1507936508,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.1903225806,
        "MMLU_high_school_chemistry":0.3054187192,
        "MMLU_high_school_computer_science":0.31,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.2121212121,
        "MMLU_high_school_government_and_politics":0.2176165803,
        "MMLU_high_school_macroeconomics":0.2358974359,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.231092437,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.2330275229,
        "MMLU_high_school_statistics":0.2175925926,
        "MMLU_high_school_us_history":0.2303921569,
        "MMLU_high_school_world_history":0.2953586498,
        "MMLU_human_aging":0.2107623318,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.2479338843,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.282208589,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.3398058252,
        "MMLU_marketing":0.2307692308,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.2630906769,
        "MMLU_moral_disputes":0.2456647399,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2516339869,
        "MMLU_philosophy":0.3440514469,
        "MMLU_prehistory":0.2098765432,
        "MMLU_professional_accounting":0.2695035461,
        "MMLU_professional_law":0.2444589309,
        "MMLU_professional_medicine":0.1911764706,
        "MMLU_professional_psychology":0.2614379085,
        "MMLU_public_relations":0.3636363636,
        "MMLU_security_studies":0.1959183673,
        "MMLU_sociology":0.2985074627,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.2530120482,
        "MMLU_world_religions":0.2865497076
    },
    {
        "Model":"pythia-1.4b-deduped",
        "URL":"https:\/\/huggingface.co\/EleutherAI\/pythia-1.4b-deduped",
        "full_model_name":"EleutherAI\/pythia-1.4b-deduped",
        "Parameters":1.4,
        "MMLU_average":0.2555549665,
        "arc:challenge|25":0.295221843,
        "hellaswag|10":0.4178450508,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.1925925926,
        "MMLU_astronomy":0.1578947368,
        "MMLU_business_ethics":0.34,
        "MMLU_clinical_knowledge":0.2490566038,
        "MMLU_college_biology":0.2986111111,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.35,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.27,
        "MMLU_conceptual_physics":0.2765957447,
        "MMLU_econometrics":0.2192982456,
        "MMLU_electrical_engineering":0.324137931,
        "MMLU_elementary_mathematics":0.2592592593,
        "MMLU_formal_logic":0.2380952381,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.2419354839,
        "MMLU_high_school_chemistry":0.236453202,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.1666666667,
        "MMLU_high_school_government_and_politics":0.207253886,
        "MMLU_high_school_macroeconomics":0.2384615385,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.2058823529,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.2055045872,
        "MMLU_high_school_statistics":0.4166666667,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.2489451477,
        "MMLU_human_aging":0.3183856502,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.2561983471,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.245398773,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2692307692,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.2349936143,
        "MMLU_moral_disputes":0.2687861272,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2614379085,
        "MMLU_philosophy":0.2411575563,
        "MMLU_prehistory":0.2407407407,
        "MMLU_professional_accounting":0.2872340426,
        "MMLU_professional_law":0.2496740548,
        "MMLU_professional_medicine":0.2205882353,
        "MMLU_professional_psychology":0.2549019608,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.2081632653,
        "MMLU_sociology":0.2388059701,
        "MMLU_us_foreign_policy":0.3,
        "MMLU_virology":0.2590361446,
        "MMLU_world_religions":0.3157894737
    },
    {
        "Model":"Guanaco-3B-Uncensored",
        "URL":"https:\/\/huggingface.co\/Fredithefish\/Guanaco-3B-Uncensored",
        "full_model_name":"Fredithefish\/Guanaco-3B-Uncensored",
        "Parameters":3.0,
        "MMLU_average":0.2555153895,
        "arc:challenge|25":0.3976109215,
        "hellaswag|10":0.4988050189,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.2814814815,
        "MMLU_astronomy":0.25,
        "MMLU_business_ethics":0.18,
        "MMLU_clinical_knowledge":0.2490566038,
        "MMLU_college_biology":0.2291666667,
        "MMLU_college_chemistry":0.25,
        "MMLU_college_computer_science":0.24,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.2427745665,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.32,
        "MMLU_conceptual_physics":0.2978723404,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.2827586207,
        "MMLU_elementary_mathematics":0.2619047619,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.2451612903,
        "MMLU_high_school_chemistry":0.2561576355,
        "MMLU_high_school_computer_science":0.24,
        "MMLU_high_school_european_history":0.2606060606,
        "MMLU_high_school_geography":0.3131313131,
        "MMLU_high_school_government_and_politics":0.2694300518,
        "MMLU_high_school_macroeconomics":0.2256410256,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.2016806723,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.2440366972,
        "MMLU_high_school_statistics":0.2407407407,
        "MMLU_high_school_us_history":0.2647058824,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.2152466368,
        "MMLU_human_sexuality":0.1832061069,
        "MMLU_international_law":0.347107438,
        "MMLU_jurisprudence":0.2407407407,
        "MMLU_logical_fallacies":0.2576687117,
        "MMLU_machine_learning":0.1964285714,
        "MMLU_management":0.2815533981,
        "MMLU_marketing":0.2564102564,
        "MMLU_medical_genetics":0.18,
        "MMLU_miscellaneous":0.2528735632,
        "MMLU_moral_disputes":0.274566474,
        "MMLU_moral_scenarios":0.2301675978,
        "MMLU_nutrition":0.2712418301,
        "MMLU_philosophy":0.2990353698,
        "MMLU_prehistory":0.2685185185,
        "MMLU_professional_accounting":0.2624113475,
        "MMLU_professional_law":0.2724902216,
        "MMLU_professional_medicine":0.1617647059,
        "MMLU_professional_psychology":0.2614379085,
        "MMLU_public_relations":0.3090909091,
        "MMLU_security_studies":0.3224489796,
        "MMLU_sociology":0.2537313433,
        "MMLU_us_foreign_policy":0.24,
        "MMLU_virology":0.234939759,
        "MMLU_world_religions":0.2748538012
    },
    {
        "Model":"StellarX-4B-V0.2",
        "URL":"https:\/\/huggingface.co\/Dampish\/StellarX-4B-V0.2",
        "full_model_name":"Dampish\/StellarX-4B-V0.2",
        "Parameters":4.0,
        "MMLU_average":0.2554755107,
        "arc:challenge|25":0.3174061433,
        "hellaswag|10":0.4158534157,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.3037037037,
        "MMLU_astronomy":0.3026315789,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.2,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.18,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.2485549133,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.29,
        "MMLU_conceptual_physics":0.2,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.2965517241,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.1507936508,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.2516129032,
        "MMLU_high_school_chemistry":0.2709359606,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.2303030303,
        "MMLU_high_school_geography":0.2474747475,
        "MMLU_high_school_government_and_politics":0.2849740933,
        "MMLU_high_school_macroeconomics":0.2282051282,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.2513761468,
        "MMLU_high_school_statistics":0.2222222222,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.1883408072,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.3719008264,
        "MMLU_jurisprudence":0.2314814815,
        "MMLU_logical_fallacies":0.3006134969,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.2524271845,
        "MMLU_marketing":0.2606837607,
        "MMLU_medical_genetics":0.2,
        "MMLU_miscellaneous":0.2643678161,
        "MMLU_moral_disputes":0.2919075145,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.2516339869,
        "MMLU_philosophy":0.3215434084,
        "MMLU_prehistory":0.262345679,
        "MMLU_professional_accounting":0.2695035461,
        "MMLU_professional_law":0.2705345502,
        "MMLU_professional_medicine":0.1691176471,
        "MMLU_professional_psychology":0.2745098039,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.2571428571,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.2409638554,
        "MMLU_world_religions":0.2923976608
    },
    {
        "Model":"megachat",
        "URL":"https:\/\/huggingface.co\/w95\/megachat",
        "full_model_name":"w95\/megachat",
        "Parameters":null,
        "MMLU_average":0.2554526006,
        "arc:challenge|25":0.2704778157,
        "hellaswag|10":0.4100776738,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.3333333333,
        "MMLU_astronomy":0.3092105263,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.2113207547,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.17,
        "MMLU_college_computer_science":0.24,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.2485549133,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.29,
        "MMLU_conceptual_physics":0.2042553191,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.2965517241,
        "MMLU_elementary_mathematics":0.2698412698,
        "MMLU_formal_logic":0.1507936508,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.2516129032,
        "MMLU_high_school_chemistry":0.2660098522,
        "MMLU_high_school_computer_science":0.35,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.202020202,
        "MMLU_high_school_government_and_politics":0.2538860104,
        "MMLU_high_school_macroeconomics":0.2230769231,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.1816513761,
        "MMLU_high_school_statistics":0.212962963,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2573839662,
        "MMLU_human_aging":0.201793722,
        "MMLU_human_sexuality":0.2442748092,
        "MMLU_international_law":0.3636363636,
        "MMLU_jurisprudence":0.2407407407,
        "MMLU_logical_fallacies":0.3006134969,
        "MMLU_machine_learning":0.2232142857,
        "MMLU_management":0.1844660194,
        "MMLU_marketing":0.2606837607,
        "MMLU_medical_genetics":0.21,
        "MMLU_miscellaneous":0.2528735632,
        "MMLU_moral_disputes":0.2919075145,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.2483660131,
        "MMLU_philosophy":0.2990353698,
        "MMLU_prehistory":0.3179012346,
        "MMLU_professional_accounting":0.2659574468,
        "MMLU_professional_law":0.2718383312,
        "MMLU_professional_medicine":0.1727941176,
        "MMLU_professional_psychology":0.2696078431,
        "MMLU_public_relations":0.2272727273,
        "MMLU_security_studies":0.2489795918,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.2469879518,
        "MMLU_world_religions":0.3742690058
    },
    {
        "Model":"TinyLlama-1.1B-Chat-v0.3",
        "URL":"https:\/\/huggingface.co\/PY007\/TinyLlama-1.1B-Chat-v0.3",
        "full_model_name":"PY007\/TinyLlama-1.1B-Chat-v0.3",
        "Parameters":1.1,
        "MMLU_average":0.2553454405,
        "arc:challenge|25":0.3139931741,
        "hellaswag|10":0.4373630751,
        "MMLU_abstract_algebra":0.32,
        "MMLU_anatomy":0.3333333333,
        "MMLU_astronomy":0.3026315789,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.18,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.2543352601,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.34,
        "MMLU_conceptual_physics":0.2085106383,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2827586207,
        "MMLU_elementary_mathematics":0.2592592593,
        "MMLU_formal_logic":0.1587301587,
        "MMLU_global_facts":0.28,
        "MMLU_high_school_biology":0.2483870968,
        "MMLU_high_school_chemistry":0.2512315271,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.2272727273,
        "MMLU_high_school_government_and_politics":0.2435233161,
        "MMLU_high_school_macroeconomics":0.2384615385,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.2058823529,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.2458715596,
        "MMLU_high_school_statistics":0.2453703704,
        "MMLU_high_school_us_history":0.2401960784,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.2107623318,
        "MMLU_human_sexuality":0.2366412214,
        "MMLU_international_law":0.3719008264,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2883435583,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.213592233,
        "MMLU_marketing":0.2735042735,
        "MMLU_medical_genetics":0.16,
        "MMLU_miscellaneous":0.2605363985,
        "MMLU_moral_disputes":0.2687861272,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.2549019608,
        "MMLU_philosophy":0.2926045016,
        "MMLU_prehistory":0.3055555556,
        "MMLU_professional_accounting":0.2730496454,
        "MMLU_professional_law":0.2614080834,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.2712418301,
        "MMLU_public_relations":0.1909090909,
        "MMLU_security_studies":0.2204081633,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.25,
        "MMLU_virology":0.265060241,
        "MMLU_world_religions":0.2631578947
    },
    {
        "Model":"gpt2-large-lora-stf4",
        "URL":"https:\/\/huggingface.co\/Mikivis\/gpt2-large-lora-stf4",
        "full_model_name":"Mikivis\/gpt2-large-lora-stf4",
        "Parameters":null,
        "MMLU_average":0.2552925949,
        "arc:challenge|25":0.2201365188,
        "hellaswag|10":0.356403107,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.2962962963,
        "MMLU_astronomy":0.2039473684,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2361111111,
        "MMLU_college_chemistry":0.25,
        "MMLU_college_computer_science":0.28,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.225433526,
        "MMLU_college_physics":0.3431372549,
        "MMLU_computer_security":0.19,
        "MMLU_conceptual_physics":0.3191489362,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2344827586,
        "MMLU_elementary_mathematics":0.2777777778,
        "MMLU_formal_logic":0.2063492063,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.3096774194,
        "MMLU_high_school_chemistry":0.3054187192,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2787878788,
        "MMLU_high_school_geography":0.2828282828,
        "MMLU_high_school_government_and_politics":0.2797927461,
        "MMLU_high_school_macroeconomics":0.2128205128,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.2857142857,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.2752293578,
        "MMLU_high_school_statistics":0.3101851852,
        "MMLU_high_school_us_history":0.2794117647,
        "MMLU_high_school_world_history":0.2405063291,
        "MMLU_human_aging":0.2107623318,
        "MMLU_human_sexuality":0.1908396947,
        "MMLU_international_law":0.4049586777,
        "MMLU_jurisprudence":0.212962963,
        "MMLU_logical_fallacies":0.245398773,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.3786407767,
        "MMLU_marketing":0.1923076923,
        "MMLU_medical_genetics":0.24,
        "MMLU_miscellaneous":0.2401021711,
        "MMLU_moral_disputes":0.2167630058,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2320261438,
        "MMLU_philosophy":0.2475884244,
        "MMLU_prehistory":0.2901234568,
        "MMLU_professional_accounting":0.2553191489,
        "MMLU_professional_law":0.2542372881,
        "MMLU_professional_medicine":0.2095588235,
        "MMLU_professional_psychology":0.2156862745,
        "MMLU_public_relations":0.2090909091,
        "MMLU_security_studies":0.2489795918,
        "MMLU_sociology":0.3134328358,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.234939759,
        "MMLU_world_religions":0.2105263158
    },
    {
        "Model":"koala-7B-HF",
        "URL":"https:\/\/huggingface.co\/TheBloke\/koala-7B-HF",
        "full_model_name":"TheBloke\/koala-7B-HF",
        "Parameters":7.0,
        "MMLU_average":0.2552916587,
        "arc:challenge|25":0.430887372,
        "hellaswag|10":0.5472017526,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.2592592593,
        "MMLU_astronomy":0.2171052632,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2716981132,
        "MMLU_college_biology":0.2777777778,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.2427745665,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.33,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.2275862069,
        "MMLU_elementary_mathematics":0.2248677249,
        "MMLU_formal_logic":0.2698412698,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.2032258065,
        "MMLU_high_school_chemistry":0.1822660099,
        "MMLU_high_school_computer_science":0.26,
        "MMLU_high_school_european_history":0.2242424242,
        "MMLU_high_school_geography":0.2626262626,
        "MMLU_high_school_government_and_politics":0.2279792746,
        "MMLU_high_school_macroeconomics":0.2205128205,
        "MMLU_high_school_mathematics":0.237037037,
        "MMLU_high_school_microeconomics":0.2142857143,
        "MMLU_high_school_physics":0.2119205298,
        "MMLU_high_school_psychology":0.2220183486,
        "MMLU_high_school_statistics":0.1851851852,
        "MMLU_high_school_us_history":0.2696078431,
        "MMLU_high_school_world_history":0.2784810127,
        "MMLU_human_aging":0.3183856502,
        "MMLU_human_sexuality":0.2671755725,
        "MMLU_international_law":0.3719008264,
        "MMLU_jurisprudence":0.287037037,
        "MMLU_logical_fallacies":0.245398773,
        "MMLU_machine_learning":0.2321428571,
        "MMLU_management":0.1941747573,
        "MMLU_marketing":0.3376068376,
        "MMLU_medical_genetics":0.31,
        "MMLU_miscellaneous":0.2911877395,
        "MMLU_moral_disputes":0.2658959538,
        "MMLU_moral_scenarios":0.251396648,
        "MMLU_nutrition":0.2320261438,
        "MMLU_philosophy":0.2218649518,
        "MMLU_prehistory":0.2283950617,
        "MMLU_professional_accounting":0.2340425532,
        "MMLU_professional_law":0.2601043025,
        "MMLU_professional_medicine":0.2132352941,
        "MMLU_professional_psychology":0.2777777778,
        "MMLU_public_relations":0.2636363636,
        "MMLU_security_studies":0.2530612245,
        "MMLU_sociology":0.3184079602,
        "MMLU_us_foreign_policy":0.34,
        "MMLU_virology":0.2951807229,
        "MMLU_world_religions":0.3333333333
    },
    {
        "Model":"distilgpt2-emailgen-V2",
        "URL":"https:\/\/huggingface.co\/postbot\/distilgpt2-emailgen-V2",
        "full_model_name":"postbot\/distilgpt2-emailgen-V2",
        "Parameters":null,
        "MMLU_average":0.2552904329,
        "arc:challenge|25":0.1689419795,
        "hellaswag|10":0.2659828719,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.3259259259,
        "MMLU_astronomy":0.1644736842,
        "MMLU_business_ethics":0.25,
        "MMLU_clinical_knowledge":0.2264150943,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.25,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.22,
        "MMLU_college_medicine":0.2312138728,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.15,
        "MMLU_conceptual_physics":0.2382978723,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2206896552,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.1587301587,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.3161290323,
        "MMLU_high_school_chemistry":0.2955665025,
        "MMLU_high_school_computer_science":0.32,
        "MMLU_high_school_european_history":0.2060606061,
        "MMLU_high_school_geography":0.3585858586,
        "MMLU_high_school_government_and_politics":0.2279792746,
        "MMLU_high_school_macroeconomics":0.2128205128,
        "MMLU_high_school_mathematics":0.2925925926,
        "MMLU_high_school_microeconomics":0.2142857143,
        "MMLU_high_school_physics":0.2052980132,
        "MMLU_high_school_psychology":0.2146788991,
        "MMLU_high_school_statistics":0.4675925926,
        "MMLU_high_school_us_history":0.2401960784,
        "MMLU_high_school_world_history":0.2616033755,
        "MMLU_human_aging":0.2062780269,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2561983471,
        "MMLU_jurisprudence":0.2777777778,
        "MMLU_logical_fallacies":0.3006134969,
        "MMLU_machine_learning":0.2410714286,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2051282051,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.2656449553,
        "MMLU_moral_disputes":0.2138728324,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2385620915,
        "MMLU_philosophy":0.2990353698,
        "MMLU_prehistory":0.2962962963,
        "MMLU_professional_accounting":0.2340425532,
        "MMLU_professional_law":0.239243807,
        "MMLU_professional_medicine":0.4264705882,
        "MMLU_professional_psychology":0.2663398693,
        "MMLU_public_relations":0.2090909091,
        "MMLU_security_studies":0.2163265306,
        "MMLU_sociology":0.2388059701,
        "MMLU_us_foreign_policy":0.25,
        "MMLU_virology":0.2951807229,
        "MMLU_world_religions":0.2105263158
    },
    {
        "Model":"42dot_LLM-SFT-1.3B",
        "URL":"https:\/\/huggingface.co\/42dot\/42dot_LLM-SFT-1.3B",
        "full_model_name":"42dot\/42dot_LLM-SFT-1.3B",
        "Parameters":1.3,
        "MMLU_average":0.2551227918,
        "arc:challenge|25":0.3361774744,
        "hellaswag|10":0.4421429994,
        "MMLU_abstract_algebra":0.18,
        "MMLU_anatomy":0.2074074074,
        "MMLU_astronomy":0.2105263158,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2113207547,
        "MMLU_college_biology":0.25,
        "MMLU_college_chemistry":0.23,
        "MMLU_college_computer_science":0.34,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.2601156069,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.31,
        "MMLU_conceptual_physics":0.2936170213,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.2551724138,
        "MMLU_elementary_mathematics":0.2301587302,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.1806451613,
        "MMLU_high_school_chemistry":0.236453202,
        "MMLU_high_school_computer_science":0.22,
        "MMLU_high_school_european_history":0.2484848485,
        "MMLU_high_school_geography":0.1818181818,
        "MMLU_high_school_government_and_politics":0.2331606218,
        "MMLU_high_school_macroeconomics":0.2461538462,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.2352941176,
        "MMLU_high_school_physics":0.3774834437,
        "MMLU_high_school_psychology":0.2091743119,
        "MMLU_high_school_statistics":0.3148148148,
        "MMLU_high_school_us_history":0.2107843137,
        "MMLU_high_school_world_history":0.2911392405,
        "MMLU_human_aging":0.2869955157,
        "MMLU_human_sexuality":0.2442748092,
        "MMLU_international_law":0.2561983471,
        "MMLU_jurisprudence":0.25,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2948717949,
        "MMLU_medical_genetics":0.33,
        "MMLU_miscellaneous":0.2349936143,
        "MMLU_moral_disputes":0.2514450867,
        "MMLU_moral_scenarios":0.2726256983,
        "MMLU_nutrition":0.2352941176,
        "MMLU_philosophy":0.2733118971,
        "MMLU_prehistory":0.2345679012,
        "MMLU_professional_accounting":0.219858156,
        "MMLU_professional_law":0.2477183833,
        "MMLU_professional_medicine":0.2904411765,
        "MMLU_professional_psychology":0.2630718954,
        "MMLU_public_relations":0.2272727273,
        "MMLU_security_studies":0.2081632653,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.29,
        "MMLU_virology":0.2710843373,
        "MMLU_world_religions":0.3099415205
    },
    {
        "Model":"TinyLlama-1.1B-intermediate-step-715k-1.5T-lr-5-2.2epochs-oasst1-top1-instruct-V1",
        "URL":"https:\/\/huggingface.co\/habanoz\/TinyLlama-1.1B-intermediate-step-715k-1.5T-lr-5-2.2epochs-oasst1-top1-instruct-V1",
        "full_model_name":"habanoz\/TinyLlama-1.1B-intermediate-step-715k-1.5T-lr-5-2.2epochs-oasst1-top1-instruct-V1",
        "Parameters":1.1,
        "MMLU_average":0.2546617943,
        "arc:challenge|25":0.2849829352,
        "hellaswag|10":0.4180442143,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.3111111111,
        "MMLU_astronomy":0.2171052632,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.2754716981,
        "MMLU_college_biology":0.2291666667,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.2369942197,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.2,
        "MMLU_conceptual_physics":0.229787234,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2482758621,
        "MMLU_elementary_mathematics":0.2883597884,
        "MMLU_formal_logic":0.1507936508,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.2419354839,
        "MMLU_high_school_chemistry":0.275862069,
        "MMLU_high_school_computer_science":0.35,
        "MMLU_high_school_european_history":0.2848484848,
        "MMLU_high_school_geography":0.2222222222,
        "MMLU_high_school_government_and_politics":0.2279792746,
        "MMLU_high_school_macroeconomics":0.2179487179,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.231092437,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.2311926606,
        "MMLU_high_school_statistics":0.2268518519,
        "MMLU_high_school_us_history":0.2205882353,
        "MMLU_high_school_world_history":0.2911392405,
        "MMLU_human_aging":0.3766816143,
        "MMLU_human_sexuality":0.2061068702,
        "MMLU_international_law":0.2561983471,
        "MMLU_jurisprudence":0.2962962963,
        "MMLU_logical_fallacies":0.2760736196,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.2038834951,
        "MMLU_marketing":0.2606837607,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.2567049808,
        "MMLU_moral_disputes":0.2341040462,
        "MMLU_moral_scenarios":0.2547486034,
        "MMLU_nutrition":0.2614379085,
        "MMLU_philosophy":0.2958199357,
        "MMLU_prehistory":0.3086419753,
        "MMLU_professional_accounting":0.2269503546,
        "MMLU_professional_law":0.2444589309,
        "MMLU_professional_medicine":0.2573529412,
        "MMLU_professional_psychology":0.2418300654,
        "MMLU_public_relations":0.3,
        "MMLU_security_studies":0.1836734694,
        "MMLU_sociology":0.2089552239,
        "MMLU_us_foreign_policy":0.31,
        "MMLU_virology":0.3012048193,
        "MMLU_world_religions":0.2280701754
    },
    {
        "Model":"open_llama_7b_700bt_preview",
        "URL":"https:\/\/huggingface.co\/openlm-research\/open_llama_7b_700bt_preview",
        "full_model_name":"openlm-research\/open_llama_7b_700bt_preview",
        "Parameters":7.0,
        "MMLU_average":0.2544589805,
        "arc:challenge|25":0.3540955631,
        "hellaswag|10":0.4969129655,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.1851851852,
        "MMLU_astronomy":0.2171052632,
        "MMLU_business_ethics":0.31,
        "MMLU_clinical_knowledge":0.3018867925,
        "MMLU_college_biology":0.2777777778,
        "MMLU_college_chemistry":0.15,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.22,
        "MMLU_college_medicine":0.225433526,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.32,
        "MMLU_conceptual_physics":0.2978723404,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.2482758621,
        "MMLU_elementary_mathematics":0.2328042328,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.28,
        "MMLU_high_school_biology":0.2322580645,
        "MMLU_high_school_chemistry":0.1871921182,
        "MMLU_high_school_computer_science":0.19,
        "MMLU_high_school_european_history":0.2303030303,
        "MMLU_high_school_geography":0.2727272727,
        "MMLU_high_school_government_and_politics":0.2020725389,
        "MMLU_high_school_macroeconomics":0.2282051282,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.2185430464,
        "MMLU_high_school_psychology":0.2146788991,
        "MMLU_high_school_statistics":0.2268518519,
        "MMLU_high_school_us_history":0.2794117647,
        "MMLU_high_school_world_history":0.2616033755,
        "MMLU_human_aging":0.331838565,
        "MMLU_human_sexuality":0.2671755725,
        "MMLU_international_law":0.2314049587,
        "MMLU_jurisprudence":0.287037037,
        "MMLU_logical_fallacies":0.1963190184,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.2524271845,
        "MMLU_marketing":0.2393162393,
        "MMLU_medical_genetics":0.31,
        "MMLU_miscellaneous":0.3090676884,
        "MMLU_moral_disputes":0.2543352601,
        "MMLU_moral_scenarios":0.2413407821,
        "MMLU_nutrition":0.2549019608,
        "MMLU_philosophy":0.2636655949,
        "MMLU_prehistory":0.2191358025,
        "MMLU_professional_accounting":0.280141844,
        "MMLU_professional_law":0.2359843546,
        "MMLU_professional_medicine":0.2573529412,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.3909090909,
        "MMLU_security_studies":0.1918367347,
        "MMLU_sociology":0.2288557214,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.343373494,
        "MMLU_world_religions":0.2748538012
    },
    {
        "Model":"chopt-2_7b",
        "URL":"https:\/\/huggingface.co\/aisquared\/chopt-2_7b",
        "full_model_name":"aisquared\/chopt-2_7b",
        "Parameters":7.0,
        "MMLU_average":0.2544251286,
        "arc:challenge|25":0.3464163823,
        "hellaswag|10":0.4769966142,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.2888888889,
        "MMLU_astronomy":0.1842105263,
        "MMLU_business_ethics":0.28,
        "MMLU_clinical_knowledge":0.2905660377,
        "MMLU_college_biology":0.2430555556,
        "MMLU_college_chemistry":0.22,
        "MMLU_college_computer_science":0.3,
        "MMLU_college_mathematics":0.17,
        "MMLU_college_medicine":0.225433526,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.23,
        "MMLU_conceptual_physics":0.2468085106,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2137931034,
        "MMLU_elementary_mathematics":0.2513227513,
        "MMLU_formal_logic":0.3174603175,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.2419354839,
        "MMLU_high_school_chemistry":0.275862069,
        "MMLU_high_school_computer_science":0.3,
        "MMLU_high_school_european_history":0.2363636364,
        "MMLU_high_school_geography":0.1464646465,
        "MMLU_high_school_government_and_politics":0.2020725389,
        "MMLU_high_school_macroeconomics":0.258974359,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.2226890756,
        "MMLU_high_school_physics":0.238410596,
        "MMLU_high_school_psychology":0.2660550459,
        "MMLU_high_school_statistics":0.2314814815,
        "MMLU_high_school_us_history":0.2205882353,
        "MMLU_high_school_world_history":0.2489451477,
        "MMLU_human_aging":0.331838565,
        "MMLU_human_sexuality":0.2519083969,
        "MMLU_international_law":0.2314049587,
        "MMLU_jurisprudence":0.25,
        "MMLU_logical_fallacies":0.2576687117,
        "MMLU_machine_learning":0.375,
        "MMLU_management":0.3009708738,
        "MMLU_marketing":0.2435897436,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.3077905492,
        "MMLU_moral_disputes":0.2456647399,
        "MMLU_moral_scenarios":0.2145251397,
        "MMLU_nutrition":0.2450980392,
        "MMLU_philosophy":0.2765273312,
        "MMLU_prehistory":0.2469135802,
        "MMLU_professional_accounting":0.2517730496,
        "MMLU_professional_law":0.2340286832,
        "MMLU_professional_medicine":0.25,
        "MMLU_professional_psychology":0.2598039216,
        "MMLU_public_relations":0.3090909091,
        "MMLU_security_studies":0.212244898,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.29,
        "MMLU_virology":0.3313253012,
        "MMLU_world_religions":0.2514619883
    },
    {
        "Model":"opt-2.7b",
        "URL":"https:\/\/huggingface.co\/facebook\/opt-2.7b",
        "full_model_name":"facebook\/opt-2.7b",
        "Parameters":2.7,
        "MMLU_average":0.2543197295,
        "arc:challenge|25":0.3097269625,
        "hellaswag|10":0.4600677156,
        "MMLU_abstract_algebra":0.2,
        "MMLU_anatomy":0.2518518519,
        "MMLU_astronomy":0.1710526316,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.2226415094,
        "MMLU_college_biology":0.2083333333,
        "MMLU_college_chemistry":0.19,
        "MMLU_college_computer_science":0.29,
        "MMLU_college_mathematics":0.23,
        "MMLU_college_medicine":0.2485549133,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.26,
        "MMLU_conceptual_physics":0.2255319149,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2827586207,
        "MMLU_elementary_mathematics":0.2248677249,
        "MMLU_formal_logic":0.1904761905,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2161290323,
        "MMLU_high_school_chemistry":0.2512315271,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.2363636364,
        "MMLU_high_school_geography":0.2171717172,
        "MMLU_high_school_government_and_politics":0.3367875648,
        "MMLU_high_school_macroeconomics":0.3487179487,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.2142857143,
        "MMLU_high_school_physics":0.3178807947,
        "MMLU_high_school_psychology":0.3266055046,
        "MMLU_high_school_statistics":0.4074074074,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2362869198,
        "MMLU_human_aging":0.201793722,
        "MMLU_human_sexuality":0.1984732824,
        "MMLU_international_law":0.2479338843,
        "MMLU_jurisprudence":0.287037037,
        "MMLU_logical_fallacies":0.226993865,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.3689320388,
        "MMLU_marketing":0.2393162393,
        "MMLU_medical_genetics":0.35,
        "MMLU_miscellaneous":0.2464878672,
        "MMLU_moral_disputes":0.2427745665,
        "MMLU_moral_scenarios":0.2357541899,
        "MMLU_nutrition":0.1928104575,
        "MMLU_philosophy":0.3151125402,
        "MMLU_prehistory":0.2407407407,
        "MMLU_professional_accounting":0.2978723404,
        "MMLU_professional_law":0.2522816167,
        "MMLU_professional_medicine":0.3198529412,
        "MMLU_professional_psychology":0.2581699346,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.2081632653,
        "MMLU_sociology":0.2537313433,
        "MMLU_us_foreign_policy":0.25,
        "MMLU_virology":0.234939759,
        "MMLU_world_religions":0.1754385965
    },
    {
        "Model":"gpt2-xl_lima",
        "URL":"https:\/\/huggingface.co\/lgaalves\/gpt2-xl_lima",
        "full_model_name":"lgaalves\/gpt2-xl_lima",
        "Parameters":null,
        "MMLU_average":0.2542601492,
        "arc:challenge|25":0.2645051195,
        "hellaswag|10":0.3984266082,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.2444444444,
        "MMLU_astronomy":0.2236842105,
        "MMLU_business_ethics":0.22,
        "MMLU_clinical_knowledge":0.3056603774,
        "MMLU_college_biology":0.2777777778,
        "MMLU_college_chemistry":0.22,
        "MMLU_college_computer_science":0.3,
        "MMLU_college_mathematics":0.32,
        "MMLU_college_medicine":0.3352601156,
        "MMLU_college_physics":0.137254902,
        "MMLU_computer_security":0.26,
        "MMLU_conceptual_physics":0.2510638298,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2827586207,
        "MMLU_elementary_mathematics":0.2010582011,
        "MMLU_formal_logic":0.3015873016,
        "MMLU_global_facts":0.17,
        "MMLU_high_school_biology":0.2096774194,
        "MMLU_high_school_chemistry":0.157635468,
        "MMLU_high_school_computer_science":0.29,
        "MMLU_high_school_european_history":0.3151515152,
        "MMLU_high_school_geography":0.3383838384,
        "MMLU_high_school_government_and_politics":0.2279792746,
        "MMLU_high_school_macroeconomics":0.3564102564,
        "MMLU_high_school_mathematics":0.2111111111,
        "MMLU_high_school_microeconomics":0.2268907563,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.352293578,
        "MMLU_high_school_statistics":0.4537037037,
        "MMLU_high_school_us_history":0.2009803922,
        "MMLU_high_school_world_history":0.2067510549,
        "MMLU_human_aging":0.1434977578,
        "MMLU_human_sexuality":0.2442748092,
        "MMLU_international_law":0.1570247934,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2760736196,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.2718446602,
        "MMLU_marketing":0.2393162393,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2618135377,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.2057877814,
        "MMLU_prehistory":0.2561728395,
        "MMLU_professional_accounting":0.2304964539,
        "MMLU_professional_law":0.2411994785,
        "MMLU_professional_medicine":0.1985294118,
        "MMLU_professional_psychology":0.2434640523,
        "MMLU_public_relations":0.2727272727,
        "MMLU_security_studies":0.2163265306,
        "MMLU_sociology":0.2537313433,
        "MMLU_us_foreign_policy":0.24,
        "MMLU_virology":0.265060241,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"3B-redpajama-conditional-alpha",
        "URL":"https:\/\/huggingface.co\/Rallio67\/3B-redpajama-conditional-alpha",
        "full_model_name":"Rallio67\/3B-redpajama-conditional-alpha",
        "Parameters":3.0,
        "MMLU_average":0.2541888266,
        "arc:challenge|25":0.319112628,
        "hellaswag|10":0.4528978291,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.3407407407,
        "MMLU_astronomy":0.2763157895,
        "MMLU_business_ethics":0.2,
        "MMLU_clinical_knowledge":0.241509434,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.22,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.2427745665,
        "MMLU_college_physics":0.1666666667,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2085106383,
        "MMLU_econometrics":0.2105263158,
        "MMLU_electrical_engineering":0.275862069,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.1746031746,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.2322580645,
        "MMLU_high_school_chemistry":0.236453202,
        "MMLU_high_school_computer_science":0.32,
        "MMLU_high_school_european_history":0.2727272727,
        "MMLU_high_school_geography":0.3080808081,
        "MMLU_high_school_government_and_politics":0.2279792746,
        "MMLU_high_school_macroeconomics":0.1974358974,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.2268907563,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.2311926606,
        "MMLU_high_school_statistics":0.2407407407,
        "MMLU_high_school_us_history":0.2647058824,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.2735426009,
        "MMLU_human_sexuality":0.1984732824,
        "MMLU_international_law":0.3966942149,
        "MMLU_jurisprudence":0.2777777778,
        "MMLU_logical_fallacies":0.2883435583,
        "MMLU_machine_learning":0.2321428571,
        "MMLU_management":0.1941747573,
        "MMLU_marketing":0.2264957265,
        "MMLU_medical_genetics":0.21,
        "MMLU_miscellaneous":0.2592592593,
        "MMLU_moral_disputes":0.3121387283,
        "MMLU_moral_scenarios":0.2446927374,
        "MMLU_nutrition":0.2516339869,
        "MMLU_philosophy":0.3151125402,
        "MMLU_prehistory":0.2716049383,
        "MMLU_professional_accounting":0.2624113475,
        "MMLU_professional_law":0.2555410691,
        "MMLU_professional_medicine":0.1948529412,
        "MMLU_professional_psychology":0.2581699346,
        "MMLU_public_relations":0.3,
        "MMLU_security_studies":0.2244897959,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.22,
        "MMLU_virology":0.2048192771,
        "MMLU_world_religions":0.2923976608
    },
    {
        "Model":"opt-1.3b-rlhf",
        "URL":"https:\/\/huggingface.co\/jzjiao\/opt-1.3b-rlhf",
        "full_model_name":"jzjiao\/opt-1.3b-rlhf",
        "Parameters":1.3,
        "MMLU_average":0.253941339,
        "arc:challenge|25":0.2645051195,
        "hellaswag|10":0.4102768373,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.2814814815,
        "MMLU_astronomy":0.1513157895,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.2603773585,
        "MMLU_college_biology":0.25,
        "MMLU_college_chemistry":0.15,
        "MMLU_college_computer_science":0.3,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.3005780347,
        "MMLU_college_physics":0.3431372549,
        "MMLU_computer_security":0.2,
        "MMLU_conceptual_physics":0.1787234043,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.2689655172,
        "MMLU_elementary_mathematics":0.2301587302,
        "MMLU_formal_logic":0.2142857143,
        "MMLU_global_facts":0.25,
        "MMLU_high_school_biology":0.2483870968,
        "MMLU_high_school_chemistry":0.2955665025,
        "MMLU_high_school_computer_science":0.3,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.2575757576,
        "MMLU_high_school_government_and_politics":0.3212435233,
        "MMLU_high_school_macroeconomics":0.3282051282,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.243697479,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.2587155963,
        "MMLU_high_school_statistics":0.3611111111,
        "MMLU_high_school_us_history":0.2843137255,
        "MMLU_high_school_world_history":0.2109704641,
        "MMLU_human_aging":0.1883408072,
        "MMLU_human_sexuality":0.1832061069,
        "MMLU_international_law":0.3223140496,
        "MMLU_jurisprudence":0.25,
        "MMLU_logical_fallacies":0.2760736196,
        "MMLU_machine_learning":0.1696428571,
        "MMLU_management":0.2524271845,
        "MMLU_marketing":0.2435897436,
        "MMLU_medical_genetics":0.29,
        "MMLU_miscellaneous":0.2490421456,
        "MMLU_moral_disputes":0.2543352601,
        "MMLU_moral_scenarios":0.2324022346,
        "MMLU_nutrition":0.2450980392,
        "MMLU_philosophy":0.2250803859,
        "MMLU_prehistory":0.2592592593,
        "MMLU_professional_accounting":0.2588652482,
        "MMLU_professional_law":0.2385919166,
        "MMLU_professional_medicine":0.4301470588,
        "MMLU_professional_psychology":0.2516339869,
        "MMLU_public_relations":0.2454545455,
        "MMLU_security_studies":0.2857142857,
        "MMLU_sociology":0.2089552239,
        "MMLU_us_foreign_policy":0.17,
        "MMLU_virology":0.2168674699,
        "MMLU_world_religions":0.2514619883
    },
    {
        "Model":"test-3k-mx",
        "URL":"https:\/\/huggingface.co\/NoIdeaLand\/test-3k-mx",
        "full_model_name":"NoIdeaLand\/test-3k-mx",
        "Parameters":null,
        "MMLU_average":0.2539304219,
        "arc:challenge|25":0.3438566553,
        "hellaswag|10":0.4851623183,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.1925925926,
        "MMLU_astronomy":0.1842105263,
        "MMLU_business_ethics":0.34,
        "MMLU_clinical_knowledge":0.2528301887,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.23,
        "MMLU_college_computer_science":0.18,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2138728324,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.26,
        "MMLU_conceptual_physics":0.2893617021,
        "MMLU_econometrics":0.1666666667,
        "MMLU_electrical_engineering":0.2827586207,
        "MMLU_elementary_mathematics":0.2142857143,
        "MMLU_formal_logic":0.1984126984,
        "MMLU_global_facts":0.23,
        "MMLU_high_school_biology":0.2032258065,
        "MMLU_high_school_chemistry":0.2118226601,
        "MMLU_high_school_computer_science":0.26,
        "MMLU_high_school_european_history":0.2606060606,
        "MMLU_high_school_geography":0.1868686869,
        "MMLU_high_school_government_and_politics":0.2642487047,
        "MMLU_high_school_macroeconomics":0.2871794872,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.2268907563,
        "MMLU_high_school_physics":0.2450331126,
        "MMLU_high_school_psychology":0.1981651376,
        "MMLU_high_school_statistics":0.1712962963,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2911392405,
        "MMLU_human_aging":0.3408071749,
        "MMLU_human_sexuality":0.2824427481,
        "MMLU_international_law":0.2727272727,
        "MMLU_jurisprudence":0.2962962963,
        "MMLU_logical_fallacies":0.2699386503,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.1844660194,
        "MMLU_marketing":0.3504273504,
        "MMLU_medical_genetics":0.33,
        "MMLU_miscellaneous":0.2605363985,
        "MMLU_moral_disputes":0.2572254335,
        "MMLU_moral_scenarios":0.2413407821,
        "MMLU_nutrition":0.2875816993,
        "MMLU_philosophy":0.2282958199,
        "MMLU_prehistory":0.2345679012,
        "MMLU_professional_accounting":0.2517730496,
        "MMLU_professional_law":0.277053455,
        "MMLU_professional_medicine":0.1985294118,
        "MMLU_professional_psychology":0.2549019608,
        "MMLU_public_relations":0.3,
        "MMLU_security_studies":0.2530612245,
        "MMLU_sociology":0.2388059701,
        "MMLU_us_foreign_policy":0.41,
        "MMLU_virology":0.3012048193,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"mistral-7b-v0.1-layla-v2",
        "URL":"https:\/\/huggingface.co\/l3utterfly\/mistral-7b-v0.1-layla-v2",
        "full_model_name":"l3utterfly\/mistral-7b-v0.1-layla-v2",
        "Parameters":7.0,
        "MMLU_average":0.253831312,
        "arc:challenge|25":0.2406143345,
        "hellaswag|10":0.2571200956,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.3333333333,
        "MMLU_astronomy":0.3026315789,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.2188679245,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.18,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.2485549133,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.3,
        "MMLU_conceptual_physics":0.2042553191,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2965517241,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.1507936508,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.2516129032,
        "MMLU_high_school_chemistry":0.2955665025,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.2848484848,
        "MMLU_high_school_geography":0.2525252525,
        "MMLU_high_school_government_and_politics":0.2279792746,
        "MMLU_high_school_macroeconomics":0.2128205128,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.2220183486,
        "MMLU_high_school_statistics":0.212962963,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.201793722,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.3719008264,
        "MMLU_jurisprudence":0.2314814815,
        "MMLU_logical_fallacies":0.3006134969,
        "MMLU_machine_learning":0.2410714286,
        "MMLU_management":0.1941747573,
        "MMLU_marketing":0.2564102564,
        "MMLU_medical_genetics":0.2,
        "MMLU_miscellaneous":0.2707535121,
        "MMLU_moral_disputes":0.2919075145,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.2516339869,
        "MMLU_philosophy":0.2990353698,
        "MMLU_prehistory":0.2932098765,
        "MMLU_professional_accounting":0.2695035461,
        "MMLU_professional_law":0.2705345502,
        "MMLU_professional_medicine":0.1654411765,
        "MMLU_professional_psychology":0.2761437908,
        "MMLU_public_relations":0.2090909091,
        "MMLU_security_studies":0.2408163265,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.25,
        "MMLU_virology":0.2048192771,
        "MMLU_world_religions":0.2923976608
    },
    {
        "Model":"gpt-YA-1-1_70M",
        "URL":"https:\/\/huggingface.co\/BreadAi\/gpt-YA-1-1_70M",
        "full_model_name":"BreadAi\/gpt-YA-1-1_70M",
        "Parameters":0.07,
        "MMLU_average":0.2537574898,
        "arc:challenge|25":0.1749146758,
        "hellaswag|10":0.266381199,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.2814814815,
        "MMLU_astronomy":0.1907894737,
        "MMLU_business_ethics":0.18,
        "MMLU_clinical_knowledge":0.2301886792,
        "MMLU_college_biology":0.2291666667,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.2312138728,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.23,
        "MMLU_conceptual_physics":0.2553191489,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.1825396825,
        "MMLU_global_facts":0.17,
        "MMLU_high_school_biology":0.3,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.22,
        "MMLU_high_school_european_history":0.2848484848,
        "MMLU_high_school_geography":0.2373737374,
        "MMLU_high_school_government_and_politics":0.3005181347,
        "MMLU_high_school_macroeconomics":0.3051282051,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.3193277311,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.2733944954,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2352941176,
        "MMLU_high_school_world_history":0.253164557,
        "MMLU_human_aging":0.3408071749,
        "MMLU_human_sexuality":0.2748091603,
        "MMLU_international_law":0.2314049587,
        "MMLU_jurisprudence":0.25,
        "MMLU_logical_fallacies":0.226993865,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.1844660194,
        "MMLU_marketing":0.1837606838,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.275862069,
        "MMLU_moral_disputes":0.2312138728,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2647058824,
        "MMLU_philosophy":0.2025723473,
        "MMLU_prehistory":0.2808641975,
        "MMLU_professional_accounting":0.2553191489,
        "MMLU_professional_law":0.2438070404,
        "MMLU_professional_medicine":0.3345588235,
        "MMLU_professional_psychology":0.2516339869,
        "MMLU_public_relations":0.1818181818,
        "MMLU_security_studies":0.293877551,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.1686746988,
        "MMLU_world_religions":0.2923976608
    },
    {
        "Model":"dolphinette",
        "URL":"https:\/\/huggingface.co\/player1537\/dolphinette",
        "full_model_name":"player1537\/dolphinette",
        "Parameters":null,
        "MMLU_average":0.253744702,
        "arc:challenge|25":0.2235494881,
        "hellaswag|10":0.3140808604,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.2518518519,
        "MMLU_astronomy":0.2565789474,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.2528301887,
        "MMLU_college_biology":0.2291666667,
        "MMLU_college_chemistry":0.4,
        "MMLU_college_computer_science":0.28,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.2427745665,
        "MMLU_college_physics":0.3431372549,
        "MMLU_computer_security":0.24,
        "MMLU_conceptual_physics":0.2425531915,
        "MMLU_econometrics":0.2192982456,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.2486772487,
        "MMLU_formal_logic":0.2301587302,
        "MMLU_global_facts":0.2,
        "MMLU_high_school_biology":0.3096774194,
        "MMLU_high_school_chemistry":0.3004926108,
        "MMLU_high_school_computer_science":0.28,
        "MMLU_high_school_european_history":0.2727272727,
        "MMLU_high_school_geography":0.2222222222,
        "MMLU_high_school_government_and_politics":0.3367875648,
        "MMLU_high_school_macroeconomics":0.2538461538,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.3487394958,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.2550458716,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.2647058824,
        "MMLU_high_school_world_history":0.253164557,
        "MMLU_human_aging":0.1793721973,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.2479338843,
        "MMLU_jurisprudence":0.2407407407,
        "MMLU_logical_fallacies":0.2392638037,
        "MMLU_machine_learning":0.1785714286,
        "MMLU_management":0.1844660194,
        "MMLU_marketing":0.188034188,
        "MMLU_medical_genetics":0.23,
        "MMLU_miscellaneous":0.2477650064,
        "MMLU_moral_disputes":0.2196531792,
        "MMLU_moral_scenarios":0.2536312849,
        "MMLU_nutrition":0.2777777778,
        "MMLU_philosophy":0.2347266881,
        "MMLU_prehistory":0.2530864198,
        "MMLU_professional_accounting":0.2340425532,
        "MMLU_professional_law":0.239243807,
        "MMLU_professional_medicine":0.2389705882,
        "MMLU_professional_psychology":0.227124183,
        "MMLU_public_relations":0.2090909091,
        "MMLU_security_studies":0.2408163265,
        "MMLU_sociology":0.2786069652,
        "MMLU_us_foreign_policy":0.23,
        "MMLU_virology":0.1807228916,
        "MMLU_world_religions":0.2339181287
    },
    {
        "Model":"Skegma-GPTJ",
        "URL":"https:\/\/huggingface.co\/digitous\/Skegma-GPTJ",
        "full_model_name":"digitous\/Skegma-GPTJ",
        "Parameters":null,
        "MMLU_average":0.2537288097,
        "arc:challenge|25":0.3950511945,
        "hellaswag|10":0.5075682135,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.2740740741,
        "MMLU_astronomy":0.2368421053,
        "MMLU_business_ethics":0.28,
        "MMLU_clinical_knowledge":0.2754716981,
        "MMLU_college_biology":0.2291666667,
        "MMLU_college_chemistry":0.22,
        "MMLU_college_computer_science":0.15,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.2138728324,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.3234042553,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.2137931034,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.2063492063,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2548387097,
        "MMLU_high_school_chemistry":0.275862069,
        "MMLU_high_school_computer_science":0.23,
        "MMLU_high_school_european_history":0.2606060606,
        "MMLU_high_school_geography":0.2272727273,
        "MMLU_high_school_government_and_politics":0.2124352332,
        "MMLU_high_school_macroeconomics":0.2333333333,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.2394957983,
        "MMLU_high_school_physics":0.2185430464,
        "MMLU_high_school_psychology":0.2366972477,
        "MMLU_high_school_statistics":0.1481481481,
        "MMLU_high_school_us_history":0.2745098039,
        "MMLU_high_school_world_history":0.2447257384,
        "MMLU_human_aging":0.3632286996,
        "MMLU_human_sexuality":0.2213740458,
        "MMLU_international_law":0.2809917355,
        "MMLU_jurisprudence":0.2962962963,
        "MMLU_logical_fallacies":0.245398773,
        "MMLU_machine_learning":0.2946428571,
        "MMLU_management":0.2233009709,
        "MMLU_marketing":0.2735042735,
        "MMLU_medical_genetics":0.27,
        "MMLU_miscellaneous":0.3103448276,
        "MMLU_moral_disputes":0.2312138728,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2712418301,
        "MMLU_philosophy":0.2958199357,
        "MMLU_prehistory":0.2777777778,
        "MMLU_professional_accounting":0.2765957447,
        "MMLU_professional_law":0.2529335072,
        "MMLU_professional_medicine":0.2022058824,
        "MMLU_professional_psychology":0.2614379085,
        "MMLU_public_relations":0.3181818182,
        "MMLU_security_studies":0.2367346939,
        "MMLU_sociology":0.2537313433,
        "MMLU_us_foreign_policy":0.25,
        "MMLU_virology":0.3253012048,
        "MMLU_world_religions":0.2456140351
    },
    {
        "Model":"test5",
        "URL":"https:\/\/huggingface.co\/doas\/test5",
        "full_model_name":"doas\/test5",
        "Parameters":null,
        "MMLU_average":0.2535780205,
        "arc:challenge|25":0.2320819113,
        "hellaswag|10":0.2572196774,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.2,
        "MMLU_astronomy":0.2763157895,
        "MMLU_business_ethics":0.31,
        "MMLU_clinical_knowledge":0.2905660377,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.29,
        "MMLU_college_computer_science":0.24,
        "MMLU_college_mathematics":0.16,
        "MMLU_college_medicine":0.387283237,
        "MMLU_college_physics":0.3039215686,
        "MMLU_computer_security":0.22,
        "MMLU_conceptual_physics":0.1914893617,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.2689655172,
        "MMLU_elementary_mathematics":0.2962962963,
        "MMLU_formal_logic":0.2380952381,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.264516129,
        "MMLU_high_school_chemistry":0.2807881773,
        "MMLU_high_school_computer_science":0.26,
        "MMLU_high_school_european_history":0.2606060606,
        "MMLU_high_school_geography":0.2878787879,
        "MMLU_high_school_government_and_politics":0.274611399,
        "MMLU_high_school_macroeconomics":0.2794871795,
        "MMLU_high_school_mathematics":0.2296296296,
        "MMLU_high_school_microeconomics":0.2857142857,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.2458715596,
        "MMLU_high_school_statistics":0.2037037037,
        "MMLU_high_school_us_history":0.2843137255,
        "MMLU_high_school_world_history":0.2869198312,
        "MMLU_human_aging":0.1659192825,
        "MMLU_human_sexuality":0.2900763359,
        "MMLU_international_law":0.347107438,
        "MMLU_jurisprudence":0.2037037037,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.4174757282,
        "MMLU_marketing":0.2606837607,
        "MMLU_medical_genetics":0.22,
        "MMLU_miscellaneous":0.2081736909,
        "MMLU_moral_disputes":0.2658959538,
        "MMLU_moral_scenarios":0.2748603352,
        "MMLU_nutrition":0.2450980392,
        "MMLU_philosophy":0.2540192926,
        "MMLU_prehistory":0.2438271605,
        "MMLU_professional_accounting":0.2304964539,
        "MMLU_professional_law":0.2385919166,
        "MMLU_professional_medicine":0.2352941176,
        "MMLU_professional_psychology":0.2532679739,
        "MMLU_public_relations":0.1818181818,
        "MMLU_security_studies":0.1591836735,
        "MMLU_sociology":0.263681592,
        "MMLU_us_foreign_policy":0.25,
        "MMLU_virology":0.2048192771,
        "MMLU_world_religions":0.2280701754
    },
    {
        "Model":"OPT-13B-Nerys-v2",
        "URL":"https:\/\/huggingface.co\/KoboldAI\/OPT-13B-Nerys-v2",
        "full_model_name":"KoboldAI\/OPT-13B-Nerys-v2",
        "Parameters":13.0,
        "MMLU_average":0.25356955,
        "arc:challenge|25":0.3660409556,
        "hellaswag|10":0.5228042223,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.2888888889,
        "MMLU_astronomy":0.1973684211,
        "MMLU_business_ethics":0.15,
        "MMLU_clinical_knowledge":0.2,
        "MMLU_college_biology":0.2361111111,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.21,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.2,
        "MMLU_conceptual_physics":0.2553191489,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.3310344828,
        "MMLU_elementary_mathematics":0.2619047619,
        "MMLU_formal_logic":0.2777777778,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.2580645161,
        "MMLU_high_school_chemistry":0.2413793103,
        "MMLU_high_school_computer_science":0.27,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.2575757576,
        "MMLU_high_school_government_and_politics":0.1917098446,
        "MMLU_high_school_macroeconomics":0.2641025641,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.2268907563,
        "MMLU_high_school_physics":0.2450331126,
        "MMLU_high_school_psychology":0.2293577982,
        "MMLU_high_school_statistics":0.3009259259,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2405063291,
        "MMLU_human_aging":0.2600896861,
        "MMLU_human_sexuality":0.2442748092,
        "MMLU_international_law":0.2561983471,
        "MMLU_jurisprudence":0.2314814815,
        "MMLU_logical_fallacies":0.2699386503,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.2718446602,
        "MMLU_marketing":0.2606837607,
        "MMLU_medical_genetics":0.31,
        "MMLU_miscellaneous":0.3001277139,
        "MMLU_moral_disputes":0.2630057803,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2483660131,
        "MMLU_philosophy":0.2668810289,
        "MMLU_prehistory":0.2592592593,
        "MMLU_professional_accounting":0.2588652482,
        "MMLU_professional_law":0.2509778357,
        "MMLU_professional_medicine":0.2242647059,
        "MMLU_professional_psychology":0.2679738562,
        "MMLU_public_relations":0.3272727273,
        "MMLU_security_studies":0.2326530612,
        "MMLU_sociology":0.2288557214,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.2710843373,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"Quokka_590m",
        "URL":"https:\/\/huggingface.co\/Corianas\/Quokka_590m",
        "full_model_name":"Corianas\/Quokka_590m",
        "Parameters":0.59,
        "MMLU_average":0.253567533,
        "arc:challenge|25":0.1868600683,
        "hellaswag|10":0.2878908584,
        "MMLU_abstract_algebra":0.21,
        "MMLU_anatomy":0.2518518519,
        "MMLU_astronomy":0.1973684211,
        "MMLU_business_ethics":0.22,
        "MMLU_clinical_knowledge":0.2339622642,
        "MMLU_college_biology":0.2708333333,
        "MMLU_college_chemistry":0.22,
        "MMLU_college_computer_science":0.34,
        "MMLU_college_mathematics":0.22,
        "MMLU_college_medicine":0.2312138728,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.26,
        "MMLU_conceptual_physics":0.285106383,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2896551724,
        "MMLU_elementary_mathematics":0.2142857143,
        "MMLU_formal_logic":0.1825396825,
        "MMLU_global_facts":0.28,
        "MMLU_high_school_biology":0.2,
        "MMLU_high_school_chemistry":0.2413793103,
        "MMLU_high_school_computer_science":0.32,
        "MMLU_high_school_european_history":0.3212121212,
        "MMLU_high_school_geography":0.2878787879,
        "MMLU_high_school_government_and_politics":0.3005181347,
        "MMLU_high_school_macroeconomics":0.2282051282,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.1722689076,
        "MMLU_high_school_physics":0.1920529801,
        "MMLU_high_school_psychology":0.2880733945,
        "MMLU_high_school_statistics":0.4583333333,
        "MMLU_high_school_us_history":0.2401960784,
        "MMLU_high_school_world_history":0.2194092827,
        "MMLU_human_aging":0.331838565,
        "MMLU_human_sexuality":0.2366412214,
        "MMLU_international_law":0.2809917355,
        "MMLU_jurisprudence":0.2407407407,
        "MMLU_logical_fallacies":0.2883435583,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.213592233,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.2605363985,
        "MMLU_moral_disputes":0.225433526,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2091503268,
        "MMLU_philosophy":0.2379421222,
        "MMLU_prehistory":0.237654321,
        "MMLU_professional_accounting":0.2375886525,
        "MMLU_professional_law":0.2242503259,
        "MMLU_professional_medicine":0.3786764706,
        "MMLU_professional_psychology":0.2598039216,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.1673469388,
        "MMLU_sociology":0.2189054726,
        "MMLU_us_foreign_policy":0.3,
        "MMLU_virology":0.1746987952,
        "MMLU_world_religions":0.3274853801
    },
    {
        "Model":"chopt-1_3b",
        "URL":"https:\/\/huggingface.co\/aisquared\/chopt-1_3b",
        "full_model_name":"aisquared\/chopt-1_3b",
        "Parameters":3.0,
        "MMLU_average":0.2535045394,
        "arc:challenge|25":0.2909556314,
        "hellaswag|10":0.4298944433,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.3111111111,
        "MMLU_astronomy":0.2105263158,
        "MMLU_business_ethics":0.25,
        "MMLU_clinical_knowledge":0.2490566038,
        "MMLU_college_biology":0.2638888889,
        "MMLU_college_chemistry":0.19,
        "MMLU_college_computer_science":0.36,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2312138728,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.25,
        "MMLU_conceptual_physics":0.2,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.2698412698,
        "MMLU_formal_logic":0.2380952381,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2451612903,
        "MMLU_high_school_chemistry":0.2315270936,
        "MMLU_high_school_computer_science":0.29,
        "MMLU_high_school_european_history":0.2909090909,
        "MMLU_high_school_geography":0.2222222222,
        "MMLU_high_school_government_and_politics":0.2279792746,
        "MMLU_high_school_macroeconomics":0.258974359,
        "MMLU_high_school_mathematics":0.2148148148,
        "MMLU_high_school_microeconomics":0.1890756303,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.3211009174,
        "MMLU_high_school_statistics":0.2777777778,
        "MMLU_high_school_us_history":0.2205882353,
        "MMLU_high_school_world_history":0.2405063291,
        "MMLU_human_aging":0.2376681614,
        "MMLU_human_sexuality":0.2748091603,
        "MMLU_international_law":0.3140495868,
        "MMLU_jurisprudence":0.2037037037,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.1964285714,
        "MMLU_management":0.3786407767,
        "MMLU_marketing":0.2478632479,
        "MMLU_medical_genetics":0.27,
        "MMLU_miscellaneous":0.2528735632,
        "MMLU_moral_disputes":0.2716763006,
        "MMLU_moral_scenarios":0.2681564246,
        "MMLU_nutrition":0.2320261438,
        "MMLU_philosophy":0.2347266881,
        "MMLU_prehistory":0.2592592593,
        "MMLU_professional_accounting":0.2695035461,
        "MMLU_professional_law":0.2757496741,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.2679738562,
        "MMLU_public_relations":0.3090909091,
        "MMLU_security_studies":0.1795918367,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.35,
        "MMLU_virology":0.2469879518,
        "MMLU_world_religions":0.2222222222
    },
    {
        "Model":"mamba-gpt-3b",
        "URL":"https:\/\/huggingface.co\/CobraMamba\/mamba-gpt-3b",
        "full_model_name":"CobraMamba\/mamba-gpt-3b",
        "Parameters":3.0,
        "MMLU_average":0.2534652589,
        "arc:challenge|25":0.3745733788,
        "hellaswag|10":0.4819757021,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.2222222222,
        "MMLU_astronomy":0.1842105263,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2679245283,
        "MMLU_college_biology":0.2361111111,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.23,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.2369942197,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.25,
        "MMLU_conceptual_physics":0.3191489362,
        "MMLU_econometrics":0.2192982456,
        "MMLU_electrical_engineering":0.2551724138,
        "MMLU_elementary_mathematics":0.2248677249,
        "MMLU_formal_logic":0.2380952381,
        "MMLU_global_facts":0.28,
        "MMLU_high_school_biology":0.2193548387,
        "MMLU_high_school_chemistry":0.2561576355,
        "MMLU_high_school_computer_science":0.2,
        "MMLU_high_school_european_history":0.2545454545,
        "MMLU_high_school_geography":0.2222222222,
        "MMLU_high_school_government_and_politics":0.2383419689,
        "MMLU_high_school_macroeconomics":0.2358974359,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.218487395,
        "MMLU_high_school_physics":0.2450331126,
        "MMLU_high_school_psychology":0.2458715596,
        "MMLU_high_school_statistics":0.2546296296,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2447257384,
        "MMLU_human_aging":0.3452914798,
        "MMLU_human_sexuality":0.1984732824,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.2147239264,
        "MMLU_machine_learning":0.2321428571,
        "MMLU_management":0.2524271845,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.2796934866,
        "MMLU_moral_disputes":0.2832369942,
        "MMLU_moral_scenarios":0.2446927374,
        "MMLU_nutrition":0.2549019608,
        "MMLU_philosophy":0.2604501608,
        "MMLU_prehistory":0.2530864198,
        "MMLU_professional_accounting":0.2765957447,
        "MMLU_professional_law":0.239243807,
        "MMLU_professional_medicine":0.3823529412,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.3181818182,
        "MMLU_security_studies":0.1795918367,
        "MMLU_sociology":0.2587064677,
        "MMLU_us_foreign_policy":0.25,
        "MMLU_virology":0.3373493976,
        "MMLU_world_religions":0.3333333333
    },
    {
        "Model":"OPT-13B-Erebus",
        "URL":"https:\/\/huggingface.co\/KoboldAI\/OPT-13B-Erebus",
        "full_model_name":"KoboldAI\/OPT-13B-Erebus",
        "Parameters":13.0,
        "MMLU_average":0.2532314691,
        "arc:challenge|25":0.3686006826,
        "hellaswag|10":0.5167297351,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.2444444444,
        "MMLU_astronomy":0.1842105263,
        "MMLU_business_ethics":0.17,
        "MMLU_clinical_knowledge":0.2528301887,
        "MMLU_college_biology":0.25,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.28,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.1791907514,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.26,
        "MMLU_conceptual_physics":0.2808510638,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.2965517241,
        "MMLU_elementary_mathematics":0.2883597884,
        "MMLU_formal_logic":0.2619047619,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.2483870968,
        "MMLU_high_school_chemistry":0.2413793103,
        "MMLU_high_school_computer_science":0.31,
        "MMLU_high_school_european_history":0.2606060606,
        "MMLU_high_school_geography":0.202020202,
        "MMLU_high_school_government_and_politics":0.2227979275,
        "MMLU_high_school_macroeconomics":0.2384615385,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.2268907563,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.2348623853,
        "MMLU_high_school_statistics":0.1851851852,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.2784810127,
        "MMLU_human_aging":0.264573991,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2760736196,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.2621359223,
        "MMLU_marketing":0.2606837607,
        "MMLU_medical_genetics":0.33,
        "MMLU_miscellaneous":0.2899106003,
        "MMLU_moral_disputes":0.2572254335,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2483660131,
        "MMLU_philosophy":0.2668810289,
        "MMLU_prehistory":0.262345679,
        "MMLU_professional_accounting":0.2695035461,
        "MMLU_professional_law":0.2464146023,
        "MMLU_professional_medicine":0.2132352941,
        "MMLU_professional_psychology":0.2549019608,
        "MMLU_public_relations":0.3181818182,
        "MMLU_security_studies":0.1632653061,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.25,
        "MMLU_virology":0.2951807229,
        "MMLU_world_religions":0.3333333333
    },
    {
        "Model":"smartyplats-3b-v1",
        "URL":"https:\/\/huggingface.co\/vihangd\/smartyplats-3b-v1",
        "full_model_name":"vihangd\/smartyplats-3b-v1",
        "Parameters":3.0,
        "MMLU_average":0.2531230266,
        "arc:challenge|25":0.3779863481,
        "hellaswag|10":0.5236008763,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.1777777778,
        "MMLU_astronomy":0.25,
        "MMLU_business_ethics":0.31,
        "MMLU_clinical_knowledge":0.2339622642,
        "MMLU_college_biology":0.2430555556,
        "MMLU_college_chemistry":0.16,
        "MMLU_college_computer_science":0.29,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.2369942197,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.33,
        "MMLU_conceptual_physics":0.3234042553,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.2551724138,
        "MMLU_elementary_mathematics":0.2116402116,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.2064516129,
        "MMLU_high_school_chemistry":0.1773399015,
        "MMLU_high_school_computer_science":0.26,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.1919191919,
        "MMLU_high_school_government_and_politics":0.207253886,
        "MMLU_high_school_macroeconomics":0.2205128205,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.2142857143,
        "MMLU_high_school_physics":0.238410596,
        "MMLU_high_school_psychology":0.2385321101,
        "MMLU_high_school_statistics":0.1805555556,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.33632287,
        "MMLU_human_sexuality":0.2366412214,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.25,
        "MMLU_logical_fallacies":0.2392638037,
        "MMLU_machine_learning":0.2410714286,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.3034188034,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2643678161,
        "MMLU_moral_disputes":0.2514450867,
        "MMLU_moral_scenarios":0.2547486034,
        "MMLU_nutrition":0.2320261438,
        "MMLU_philosophy":0.2861736334,
        "MMLU_prehistory":0.299382716,
        "MMLU_professional_accounting":0.2375886525,
        "MMLU_professional_law":0.2398956975,
        "MMLU_professional_medicine":0.1985294118,
        "MMLU_professional_psychology":0.2647058824,
        "MMLU_public_relations":0.2818181818,
        "MMLU_security_studies":0.2040816327,
        "MMLU_sociology":0.2935323383,
        "MMLU_us_foreign_policy":0.3,
        "MMLU_virology":0.3192771084,
        "MMLU_world_religions":0.3157894737
    },
    {
        "Model":"gpt-neox-122m-minipile-digits",
        "URL":"https:\/\/huggingface.co\/euclaise\/gpt-neox-122m-minipile-digits",
        "full_model_name":"euclaise\/gpt-neox-122m-minipile-digits",
        "Parameters":0.122,
        "MMLU_average":0.2531167309,
        "arc:challenge|25":0.1817406143,
        "hellaswag|10":0.2637920733,
        "MMLU_abstract_algebra":0.19,
        "MMLU_anatomy":0.3333333333,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.2,
        "MMLU_clinical_knowledge":0.2188679245,
        "MMLU_college_biology":0.2291666667,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.2196531792,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.24,
        "MMLU_conceptual_physics":0.2468085106,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.2551724138,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.1507936508,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.2677419355,
        "MMLU_high_school_chemistry":0.2955665025,
        "MMLU_high_school_computer_science":0.36,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.2474747475,
        "MMLU_high_school_government_and_politics":0.2176165803,
        "MMLU_high_school_macroeconomics":0.2128205128,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.231092437,
        "MMLU_high_school_physics":0.2450331126,
        "MMLU_high_school_psychology":0.2201834862,
        "MMLU_high_school_statistics":0.4583333333,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.2062780269,
        "MMLU_human_sexuality":0.2671755725,
        "MMLU_international_law":0.2809917355,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.3006134969,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.1965811966,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2707535121,
        "MMLU_moral_disputes":0.2456647399,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.2647058824,
        "MMLU_philosophy":0.1832797428,
        "MMLU_prehistory":0.2592592593,
        "MMLU_professional_accounting":0.2659574468,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.3235294118,
        "MMLU_professional_psychology":0.2532679739,
        "MMLU_public_relations":0.2454545455,
        "MMLU_security_studies":0.2204081633,
        "MMLU_sociology":0.223880597,
        "MMLU_us_foreign_policy":0.36,
        "MMLU_virology":0.313253012,
        "MMLU_world_religions":0.2046783626
    },
    {
        "Model":"gogpt-560m",
        "URL":"https:\/\/huggingface.co\/golaxy\/gogpt-560m",
        "full_model_name":"golaxy\/gogpt-560m",
        "Parameters":0.56,
        "MMLU_average":0.2529124185,
        "arc:challenge|25":0.2218430034,
        "hellaswag|10":0.2976498705,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.3333333333,
        "MMLU_astronomy":0.2960526316,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.2188679245,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.18,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.2485549133,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.3,
        "MMLU_conceptual_physics":0.2042553191,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2965517241,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.1507936508,
        "MMLU_global_facts":0.37,
        "MMLU_high_school_biology":0.264516129,
        "MMLU_high_school_chemistry":0.2955665025,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.2727272727,
        "MMLU_high_school_geography":0.2525252525,
        "MMLU_high_school_government_and_politics":0.2279792746,
        "MMLU_high_school_macroeconomics":0.2128205128,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.2220183486,
        "MMLU_high_school_statistics":0.212962963,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.2616033755,
        "MMLU_human_aging":0.201793722,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.3719008264,
        "MMLU_jurisprudence":0.2222222222,
        "MMLU_logical_fallacies":0.2944785276,
        "MMLU_machine_learning":0.2410714286,
        "MMLU_management":0.1941747573,
        "MMLU_marketing":0.2179487179,
        "MMLU_medical_genetics":0.2,
        "MMLU_miscellaneous":0.2707535121,
        "MMLU_moral_disputes":0.289017341,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.2516339869,
        "MMLU_philosophy":0.2990353698,
        "MMLU_prehistory":0.2932098765,
        "MMLU_professional_accounting":0.2730496454,
        "MMLU_professional_law":0.2620599739,
        "MMLU_professional_medicine":0.1654411765,
        "MMLU_professional_psychology":0.2630718954,
        "MMLU_public_relations":0.2090909091,
        "MMLU_security_studies":0.2448979592,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.25,
        "MMLU_virology":0.2048192771,
        "MMLU_world_religions":0.2807017544
    },
    {
        "Model":"Aira-124M",
        "URL":"https:\/\/huggingface.co\/nicholasKluge\/Aira-124M",
        "full_model_name":"nicholasKluge\/Aira-124M",
        "Parameters":0.124,
        "MMLU_average":0.2529048444,
        "arc:challenge|25":0.1988054608,
        "hellaswag|10":0.2921728739,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.2074074074,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.2,
        "MMLU_clinical_knowledge":0.2113207547,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.19,
        "MMLU_college_computer_science":0.28,
        "MMLU_college_mathematics":0.19,
        "MMLU_college_medicine":0.2427745665,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.25,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.2486772487,
        "MMLU_formal_logic":0.1666666667,
        "MMLU_global_facts":0.13,
        "MMLU_high_school_biology":0.2193548387,
        "MMLU_high_school_chemistry":0.197044335,
        "MMLU_high_school_computer_science":0.32,
        "MMLU_high_school_european_history":0.2121212121,
        "MMLU_high_school_geography":0.3535353535,
        "MMLU_high_school_government_and_politics":0.3678756477,
        "MMLU_high_school_macroeconomics":0.3025641026,
        "MMLU_high_school_mathematics":0.2296296296,
        "MMLU_high_school_microeconomics":0.231092437,
        "MMLU_high_school_physics":0.2185430464,
        "MMLU_high_school_psychology":0.3467889908,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2794117647,
        "MMLU_high_school_world_history":0.2827004219,
        "MMLU_human_aging":0.1210762332,
        "MMLU_human_sexuality":0.2519083969,
        "MMLU_international_law":0.2727272727,
        "MMLU_jurisprudence":0.25,
        "MMLU_logical_fallacies":0.2699386503,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.1941747573,
        "MMLU_marketing":0.235042735,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.245210728,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2516339869,
        "MMLU_philosophy":0.1800643087,
        "MMLU_prehistory":0.2530864198,
        "MMLU_professional_accounting":0.2730496454,
        "MMLU_professional_law":0.2555410691,
        "MMLU_professional_medicine":0.4448529412,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.1818181818,
        "MMLU_security_studies":0.4,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.2228915663,
        "MMLU_world_religions":0.2807017544
    },
    {
        "Model":"verysmol_llama-v11-KIx2",
        "URL":"https:\/\/huggingface.co\/BEE-spoke-data\/verysmol_llama-v11-KIx2",
        "full_model_name":"BEE-spoke-data\/verysmol_llama-v11-KIx2",
        "Parameters":null,
        "MMLU_average":0.2528402254,
        "arc:challenge|25":0.1979522184,
        "hellaswag|10":0.2698665604,
        "MMLU_abstract_algebra":0.17,
        "MMLU_anatomy":0.2666666667,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.29,
        "MMLU_clinical_knowledge":0.2188679245,
        "MMLU_college_biology":0.2361111111,
        "MMLU_college_chemistry":0.25,
        "MMLU_college_computer_science":0.23,
        "MMLU_college_mathematics":0.22,
        "MMLU_college_medicine":0.2023121387,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.21,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.2137931034,
        "MMLU_elementary_mathematics":0.2513227513,
        "MMLU_formal_logic":0.2142857143,
        "MMLU_global_facts":0.14,
        "MMLU_high_school_biology":0.3096774194,
        "MMLU_high_school_chemistry":0.2955665025,
        "MMLU_high_school_computer_science":0.24,
        "MMLU_high_school_european_history":0.2606060606,
        "MMLU_high_school_geography":0.3383838384,
        "MMLU_high_school_government_and_politics":0.274611399,
        "MMLU_high_school_macroeconomics":0.3307692308,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.2941176471,
        "MMLU_high_school_physics":0.2185430464,
        "MMLU_high_school_psychology":0.2256880734,
        "MMLU_high_school_statistics":0.462962963,
        "MMLU_high_school_us_history":0.2401960784,
        "MMLU_high_school_world_history":0.253164557,
        "MMLU_human_aging":0.2780269058,
        "MMLU_human_sexuality":0.2519083969,
        "MMLU_international_law":0.2479338843,
        "MMLU_jurisprudence":0.2314814815,
        "MMLU_logical_fallacies":0.226993865,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2435897436,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2630906769,
        "MMLU_moral_disputes":0.2543352601,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2777777778,
        "MMLU_philosophy":0.1993569132,
        "MMLU_prehistory":0.2191358025,
        "MMLU_professional_accounting":0.2482269504,
        "MMLU_professional_law":0.2470664928,
        "MMLU_professional_medicine":0.4007352941,
        "MMLU_professional_psychology":0.2516339869,
        "MMLU_public_relations":0.2363636364,
        "MMLU_security_studies":0.2163265306,
        "MMLU_sociology":0.263681592,
        "MMLU_us_foreign_policy":0.3,
        "MMLU_virology":0.2590361446,
        "MMLU_world_religions":0.2046783626
    },
    {
        "Model":"testmodel",
        "URL":"https:\/\/huggingface.co\/huashiyiqike\/testmodel",
        "full_model_name":"huashiyiqike\/testmodel",
        "Parameters":null,
        "MMLU_average":0.2527554147,
        "arc:challenge|25":0.1757679181,
        "hellaswag|10":0.2674765983,
        "MMLU_abstract_algebra":0.2,
        "MMLU_anatomy":0.2666666667,
        "MMLU_astronomy":0.1644736842,
        "MMLU_business_ethics":0.24,
        "MMLU_clinical_knowledge":0.2452830189,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.18,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.2485549133,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.25,
        "MMLU_conceptual_physics":0.2680851064,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2804232804,
        "MMLU_formal_logic":0.1587301587,
        "MMLU_global_facts":0.14,
        "MMLU_high_school_biology":0.3064516129,
        "MMLU_high_school_chemistry":0.2906403941,
        "MMLU_high_school_computer_science":0.28,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.2676767677,
        "MMLU_high_school_government_and_politics":0.2331606218,
        "MMLU_high_school_macroeconomics":0.2564102564,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.231092437,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.3155963303,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.253164557,
        "MMLU_human_aging":0.2242152466,
        "MMLU_human_sexuality":0.2748091603,
        "MMLU_international_law":0.3553719008,
        "MMLU_jurisprudence":0.2407407407,
        "MMLU_logical_fallacies":0.3006134969,
        "MMLU_machine_learning":0.2142857143,
        "MMLU_management":0.1650485437,
        "MMLU_marketing":0.2051282051,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.2733077905,
        "MMLU_moral_disputes":0.2427745665,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2516339869,
        "MMLU_philosophy":0.270096463,
        "MMLU_prehistory":0.225308642,
        "MMLU_professional_accounting":0.2695035461,
        "MMLU_professional_law":0.240547588,
        "MMLU_professional_medicine":0.3676470588,
        "MMLU_professional_psychology":0.272875817,
        "MMLU_public_relations":0.1909090909,
        "MMLU_security_studies":0.2979591837,
        "MMLU_sociology":0.2388059701,
        "MMLU_us_foreign_policy":0.18,
        "MMLU_virology":0.1927710843,
        "MMLU_world_religions":0.2807017544
    },
    {
        "Model":"111m",
        "URL":"https:\/\/huggingface.co\/Corianas\/111m",
        "full_model_name":"Corianas\/111m",
        "Parameters":0.111,
        "MMLU_average":0.2527554147,
        "arc:challenge|25":0.1757679181,
        "hellaswag|10":0.2674765983,
        "MMLU_abstract_algebra":0.2,
        "MMLU_anatomy":0.2666666667,
        "MMLU_astronomy":0.1644736842,
        "MMLU_business_ethics":0.24,
        "MMLU_clinical_knowledge":0.2452830189,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.18,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.2485549133,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.25,
        "MMLU_conceptual_physics":0.2680851064,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2804232804,
        "MMLU_formal_logic":0.1587301587,
        "MMLU_global_facts":0.14,
        "MMLU_high_school_biology":0.3064516129,
        "MMLU_high_school_chemistry":0.2906403941,
        "MMLU_high_school_computer_science":0.28,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.2676767677,
        "MMLU_high_school_government_and_politics":0.2331606218,
        "MMLU_high_school_macroeconomics":0.2564102564,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.231092437,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.3155963303,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.253164557,
        "MMLU_human_aging":0.2242152466,
        "MMLU_human_sexuality":0.2748091603,
        "MMLU_international_law":0.3553719008,
        "MMLU_jurisprudence":0.2407407407,
        "MMLU_logical_fallacies":0.3006134969,
        "MMLU_machine_learning":0.2142857143,
        "MMLU_management":0.1650485437,
        "MMLU_marketing":0.2051282051,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.2733077905,
        "MMLU_moral_disputes":0.2427745665,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2516339869,
        "MMLU_philosophy":0.270096463,
        "MMLU_prehistory":0.225308642,
        "MMLU_professional_accounting":0.2695035461,
        "MMLU_professional_law":0.240547588,
        "MMLU_professional_medicine":0.3676470588,
        "MMLU_professional_psychology":0.272875817,
        "MMLU_public_relations":0.1909090909,
        "MMLU_security_studies":0.2979591837,
        "MMLU_sociology":0.2388059701,
        "MMLU_us_foreign_policy":0.18,
        "MMLU_virology":0.1927710843,
        "MMLU_world_religions":0.2807017544
    },
    {
        "Model":"pythia-70m-deduped",
        "URL":"https:\/\/huggingface.co\/EleutherAI\/pythia-70m-deduped",
        "full_model_name":"EleutherAI\/pythia-70m-deduped",
        "Parameters":0.07,
        "MMLU_average":0.2526050704,
        "arc:challenge|25":0.183447099,
        "hellaswag|10":0.2679745071,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.3555555556,
        "MMLU_astronomy":0.1842105263,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.2339622642,
        "MMLU_college_biology":0.2291666667,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.28,
        "MMLU_college_mathematics":0.22,
        "MMLU_college_medicine":0.2023121387,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.27,
        "MMLU_conceptual_physics":0.2893617021,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2068965517,
        "MMLU_elementary_mathematics":0.253968254,
        "MMLU_formal_logic":0.2063492063,
        "MMLU_global_facts":0.17,
        "MMLU_high_school_biology":0.3258064516,
        "MMLU_high_school_chemistry":0.2807881773,
        "MMLU_high_school_computer_science":0.22,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.2222222222,
        "MMLU_high_school_government_and_politics":0.3523316062,
        "MMLU_high_school_macroeconomics":0.2846153846,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.231092437,
        "MMLU_high_school_physics":0.2052980132,
        "MMLU_high_school_psychology":0.2403669725,
        "MMLU_high_school_statistics":0.4583333333,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.2827004219,
        "MMLU_human_aging":0.2556053812,
        "MMLU_human_sexuality":0.2900763359,
        "MMLU_international_law":0.1983471074,
        "MMLU_jurisprudence":0.2037037037,
        "MMLU_logical_fallacies":0.2515337423,
        "MMLU_machine_learning":0.1785714286,
        "MMLU_management":0.1844660194,
        "MMLU_marketing":0.2008547009,
        "MMLU_medical_genetics":0.34,
        "MMLU_miscellaneous":0.2375478927,
        "MMLU_moral_disputes":0.2658959538,
        "MMLU_moral_scenarios":0.2670391061,
        "MMLU_nutrition":0.2647058824,
        "MMLU_philosophy":0.1961414791,
        "MMLU_prehistory":0.2345679012,
        "MMLU_professional_accounting":0.2588652482,
        "MMLU_professional_law":0.239243807,
        "MMLU_professional_medicine":0.4154411765,
        "MMLU_professional_psychology":0.2483660131,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.2285714286,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.32,
        "MMLU_virology":0.1927710843,
        "MMLU_world_religions":0.3099415205
    },
    {
        "Model":"metharme-1.3b-finetuned",
        "URL":"https:\/\/huggingface.co\/uberkie\/metharme-1.3b-finetuned",
        "full_model_name":"uberkie\/metharme-1.3b-finetuned",
        "Parameters":1.3,
        "MMLU_average":0.2525982968,
        "arc:challenge|25":0.1766211604,
        "hellaswag|10":0.2710615415,
        "MMLU_abstract_algebra":0.17,
        "MMLU_anatomy":0.3037037037,
        "MMLU_astronomy":0.2631578947,
        "MMLU_business_ethics":0.26,
        "MMLU_clinical_knowledge":0.2037735849,
        "MMLU_college_biology":0.25,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.27,
        "MMLU_college_mathematics":0.22,
        "MMLU_college_medicine":0.3468208092,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.23,
        "MMLU_conceptual_physics":0.2680851064,
        "MMLU_econometrics":0.2105263158,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.1507936508,
        "MMLU_global_facts":0.2,
        "MMLU_high_school_biology":0.3064516129,
        "MMLU_high_school_chemistry":0.2906403941,
        "MMLU_high_school_computer_science":0.26,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.2878787879,
        "MMLU_high_school_government_and_politics":0.2642487047,
        "MMLU_high_school_macroeconomics":0.2205128205,
        "MMLU_high_school_mathematics":0.2851851852,
        "MMLU_high_school_microeconomics":0.3109243697,
        "MMLU_high_school_physics":0.2317880795,
        "MMLU_high_school_psychology":0.247706422,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.2278481013,
        "MMLU_human_aging":0.264573991,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.2561983471,
        "MMLU_jurisprudence":0.2222222222,
        "MMLU_logical_fallacies":0.2576687117,
        "MMLU_machine_learning":0.2142857143,
        "MMLU_management":0.1941747573,
        "MMLU_marketing":0.1965811966,
        "MMLU_medical_genetics":0.29,
        "MMLU_miscellaneous":0.220945083,
        "MMLU_moral_disputes":0.2514450867,
        "MMLU_moral_scenarios":0.2324022346,
        "MMLU_nutrition":0.2843137255,
        "MMLU_philosophy":0.2604501608,
        "MMLU_prehistory":0.2407407407,
        "MMLU_professional_accounting":0.2092198582,
        "MMLU_professional_law":0.2438070404,
        "MMLU_professional_medicine":0.3860294118,
        "MMLU_professional_psychology":0.2238562092,
        "MMLU_public_relations":0.2636363636,
        "MMLU_security_studies":0.1959183673,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.23,
        "MMLU_virology":0.265060241,
        "MMLU_world_religions":0.1988304094
    },
    {
        "Model":"pile-7b-250b-tokens",
        "URL":"https:\/\/huggingface.co\/Kunhao\/pile-7b-250b-tokens",
        "full_model_name":"Kunhao\/pile-7b-250b-tokens",
        "Parameters":7.0,
        "MMLU_average":0.2524986401,
        "arc:challenge|25":0.2593856655,
        "hellaswag|10":0.3720374427,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.2666666667,
        "MMLU_astronomy":0.1973684211,
        "MMLU_business_ethics":0.22,
        "MMLU_clinical_knowledge":0.241509434,
        "MMLU_college_biology":0.2638888889,
        "MMLU_college_chemistry":0.3,
        "MMLU_college_computer_science":0.38,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.2485549133,
        "MMLU_college_physics":0.1764705882,
        "MMLU_computer_security":0.27,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2192982456,
        "MMLU_electrical_engineering":0.2068965517,
        "MMLU_elementary_mathematics":0.2486772487,
        "MMLU_formal_logic":0.1984126984,
        "MMLU_global_facts":0.29,
        "MMLU_high_school_biology":0.2193548387,
        "MMLU_high_school_chemistry":0.236453202,
        "MMLU_high_school_computer_science":0.28,
        "MMLU_high_school_european_history":0.2121212121,
        "MMLU_high_school_geography":0.2828282828,
        "MMLU_high_school_government_and_politics":0.2538860104,
        "MMLU_high_school_macroeconomics":0.2333333333,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.1932773109,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.2403669725,
        "MMLU_high_school_statistics":0.2546296296,
        "MMLU_high_school_us_history":0.2696078431,
        "MMLU_high_school_world_history":0.2362869198,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.305785124,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.282208589,
        "MMLU_machine_learning":0.2232142857,
        "MMLU_management":0.1844660194,
        "MMLU_marketing":0.2435897436,
        "MMLU_medical_genetics":0.23,
        "MMLU_miscellaneous":0.282247765,
        "MMLU_moral_disputes":0.2514450867,
        "MMLU_moral_scenarios":0.243575419,
        "MMLU_nutrition":0.2450980392,
        "MMLU_philosophy":0.2893890675,
        "MMLU_prehistory":0.2345679012,
        "MMLU_professional_accounting":0.2659574468,
        "MMLU_professional_law":0.2398956975,
        "MMLU_professional_medicine":0.1948529412,
        "MMLU_professional_psychology":0.2777777778,
        "MMLU_public_relations":0.2090909091,
        "MMLU_security_studies":0.1836734694,
        "MMLU_sociology":0.2388059701,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.3313253012,
        "MMLU_world_religions":0.3099415205
    },
    {
        "Model":"pythia-70m-deduped-cleansharegpt-en",
        "URL":"https:\/\/huggingface.co\/HWERI\/pythia-70m-deduped-cleansharegpt-en",
        "full_model_name":"HWERI\/pythia-70m-deduped-cleansharegpt-en",
        "Parameters":0.07,
        "MMLU_average":0.2524486843,
        "arc:challenge|25":0.1945392491,
        "hellaswag|10":0.2659828719,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.3259259259,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.2377358491,
        "MMLU_college_biology":0.2291666667,
        "MMLU_college_chemistry":0.19,
        "MMLU_college_computer_science":0.27,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.2369942197,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.36,
        "MMLU_conceptual_physics":0.2893617021,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2698412698,
        "MMLU_formal_logic":0.2222222222,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.2419354839,
        "MMLU_high_school_chemistry":0.236453202,
        "MMLU_high_school_computer_science":0.23,
        "MMLU_high_school_european_history":0.2060606061,
        "MMLU_high_school_geography":0.196969697,
        "MMLU_high_school_government_and_politics":0.3316062176,
        "MMLU_high_school_macroeconomics":0.3384615385,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.2352941176,
        "MMLU_high_school_physics":0.2450331126,
        "MMLU_high_school_psychology":0.2146788991,
        "MMLU_high_school_statistics":0.4768518519,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.2742616034,
        "MMLU_human_aging":0.3004484305,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2231404959,
        "MMLU_jurisprudence":0.2222222222,
        "MMLU_logical_fallacies":0.2392638037,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2606837607,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2337164751,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2418300654,
        "MMLU_philosophy":0.1768488746,
        "MMLU_prehistory":0.2098765432,
        "MMLU_professional_accounting":0.2624113475,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2647058824,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.2040816327,
        "MMLU_sociology":0.2288557214,
        "MMLU_us_foreign_policy":0.23,
        "MMLU_virology":0.2168674699,
        "MMLU_world_religions":0.2631578947
    },
    {
        "Model":"firefly-bloom-2b6-v2",
        "URL":"https:\/\/huggingface.co\/YeungNLP\/firefly-bloom-2b6-v2",
        "full_model_name":"YeungNLP\/firefly-bloom-2b6-v2",
        "Parameters":2.0,
        "MMLU_average":0.2524180716,
        "arc:challenge|25":0.2116040956,
        "hellaswag|10":0.331408086,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.2,
        "MMLU_astronomy":0.1842105263,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.2603773585,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.23,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.2312138728,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.32,
        "MMLU_conceptual_physics":0.3106382979,
        "MMLU_econometrics":0.2192982456,
        "MMLU_electrical_engineering":0.2689655172,
        "MMLU_elementary_mathematics":0.2407407407,
        "MMLU_formal_logic":0.2301587302,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.235483871,
        "MMLU_high_school_chemistry":0.2019704433,
        "MMLU_high_school_computer_science":0.38,
        "MMLU_high_school_european_history":0.2545454545,
        "MMLU_high_school_geography":0.1767676768,
        "MMLU_high_school_government_and_politics":0.2590673575,
        "MMLU_high_school_macroeconomics":0.341025641,
        "MMLU_high_school_mathematics":0.2111111111,
        "MMLU_high_school_microeconomics":0.2226890756,
        "MMLU_high_school_physics":0.2052980132,
        "MMLU_high_school_psychology":0.2330275229,
        "MMLU_high_school_statistics":0.412037037,
        "MMLU_high_school_us_history":0.2352941176,
        "MMLU_high_school_world_history":0.2573839662,
        "MMLU_human_aging":0.3408071749,
        "MMLU_human_sexuality":0.2900763359,
        "MMLU_international_law":0.2231404959,
        "MMLU_jurisprudence":0.2314814815,
        "MMLU_logical_fallacies":0.2699386503,
        "MMLU_machine_learning":0.2410714286,
        "MMLU_management":0.2330097087,
        "MMLU_marketing":0.2435897436,
        "MMLU_medical_genetics":0.32,
        "MMLU_miscellaneous":0.2464878672,
        "MMLU_moral_disputes":0.2456647399,
        "MMLU_moral_scenarios":0.2659217877,
        "MMLU_nutrition":0.2516339869,
        "MMLU_philosophy":0.231511254,
        "MMLU_prehistory":0.2283950617,
        "MMLU_professional_accounting":0.2624113475,
        "MMLU_professional_law":0.2425032595,
        "MMLU_professional_medicine":0.1911764706,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.1818181818,
        "MMLU_security_studies":0.2816326531,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2108433735,
        "MMLU_world_religions":0.2865497076
    },
    {
        "Model":"open-calm-large",
        "URL":"https:\/\/huggingface.co\/cyberagent\/open-calm-large",
        "full_model_name":"cyberagent\/open-calm-large",
        "Parameters":null,
        "MMLU_average":0.2523101414,
        "arc:challenge|25":0.1740614334,
        "hellaswag|10":0.2795259908,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.237037037,
        "MMLU_astronomy":0.1842105263,
        "MMLU_business_ethics":0.24,
        "MMLU_clinical_knowledge":0.2566037736,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.37,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.2658959538,
        "MMLU_college_physics":0.1078431373,
        "MMLU_computer_security":0.22,
        "MMLU_conceptual_physics":0.2,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.2068965517,
        "MMLU_elementary_mathematics":0.2380952381,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.27,
        "MMLU_high_school_biology":0.2225806452,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.27,
        "MMLU_high_school_european_history":0.1939393939,
        "MMLU_high_school_geography":0.2272727273,
        "MMLU_high_school_government_and_politics":0.2849740933,
        "MMLU_high_school_macroeconomics":0.258974359,
        "MMLU_high_school_mathematics":0.2296296296,
        "MMLU_high_school_microeconomics":0.231092437,
        "MMLU_high_school_physics":0.3112582781,
        "MMLU_high_school_psychology":0.2660550459,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.2401960784,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.1928251121,
        "MMLU_human_sexuality":0.2519083969,
        "MMLU_international_law":0.3884297521,
        "MMLU_jurisprudence":0.212962963,
        "MMLU_logical_fallacies":0.2699386503,
        "MMLU_machine_learning":0.2232142857,
        "MMLU_management":0.2427184466,
        "MMLU_marketing":0.2094017094,
        "MMLU_medical_genetics":0.13,
        "MMLU_miscellaneous":0.2694763729,
        "MMLU_moral_disputes":0.2716763006,
        "MMLU_moral_scenarios":0.2525139665,
        "MMLU_nutrition":0.2385620915,
        "MMLU_philosophy":0.2636655949,
        "MMLU_prehistory":0.287037037,
        "MMLU_professional_accounting":0.2659574468,
        "MMLU_professional_law":0.2235984355,
        "MMLU_professional_medicine":0.3455882353,
        "MMLU_professional_psychology":0.2549019608,
        "MMLU_public_relations":0.1727272727,
        "MMLU_security_studies":0.2244897959,
        "MMLU_sociology":0.2338308458,
        "MMLU_us_foreign_policy":0.24,
        "MMLU_virology":0.3253012048,
        "MMLU_world_religions":0.216374269
    },
    {
        "Model":"MusePy-1-2",
        "URL":"https:\/\/huggingface.co\/BreadAi\/MusePy-1-2",
        "full_model_name":"BreadAi\/MusePy-1-2",
        "Parameters":null,
        "MMLU_average":0.2522276129,
        "arc:challenge|25":0.2039249147,
        "hellaswag|10":0.2567217686,
        "MMLU_abstract_algebra":0.33,
        "MMLU_anatomy":0.3259259259,
        "MMLU_astronomy":0.1710526316,
        "MMLU_business_ethics":0.19,
        "MMLU_clinical_knowledge":0.2188679245,
        "MMLU_college_biology":0.2291666667,
        "MMLU_college_chemistry":0.19,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.23,
        "MMLU_college_medicine":0.2196531792,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.33,
        "MMLU_conceptual_physics":0.229787234,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.253968254,
        "MMLU_formal_logic":0.1666666667,
        "MMLU_global_facts":0.28,
        "MMLU_high_school_biology":0.2677419355,
        "MMLU_high_school_chemistry":0.2955665025,
        "MMLU_high_school_computer_science":0.26,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.2929292929,
        "MMLU_high_school_government_and_politics":0.2279792746,
        "MMLU_high_school_macroeconomics":0.2333333333,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.256302521,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.2311926606,
        "MMLU_high_school_statistics":0.4305555556,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.2242152466,
        "MMLU_human_sexuality":0.2442748092,
        "MMLU_international_law":0.3388429752,
        "MMLU_jurisprudence":0.2314814815,
        "MMLU_logical_fallacies":0.2699386503,
        "MMLU_machine_learning":0.2321428571,
        "MMLU_management":0.1844660194,
        "MMLU_marketing":0.1923076923,
        "MMLU_medical_genetics":0.31,
        "MMLU_miscellaneous":0.2796934866,
        "MMLU_moral_disputes":0.2341040462,
        "MMLU_moral_scenarios":0.2268156425,
        "MMLU_nutrition":0.2549019608,
        "MMLU_philosophy":0.2829581994,
        "MMLU_prehistory":0.2561728395,
        "MMLU_professional_accounting":0.2730496454,
        "MMLU_professional_law":0.2294654498,
        "MMLU_professional_medicine":0.2610294118,
        "MMLU_professional_psychology":0.2352941176,
        "MMLU_public_relations":0.1909090909,
        "MMLU_security_studies":0.2367346939,
        "MMLU_sociology":0.2338308458,
        "MMLU_us_foreign_policy":0.33,
        "MMLU_virology":0.1987951807,
        "MMLU_world_religions":0.2222222222
    },
    {
        "Model":"bloom-560m-4bit-alpaca",
        "URL":"https:\/\/huggingface.co\/TFLai\/bloom-560m-4bit-alpaca",
        "full_model_name":"TFLai\/bloom-560m-4bit-alpaca",
        "Parameters":0.56,
        "MMLU_average":0.2520182306,
        "arc:challenge|25":0.1902730375,
        "hellaswag|10":0.2761402111,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.2666666667,
        "MMLU_astronomy":0.1710526316,
        "MMLU_business_ethics":0.24,
        "MMLU_clinical_knowledge":0.2566037736,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.32,
        "MMLU_college_computer_science":0.27,
        "MMLU_college_mathematics":0.2,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.18,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2,
        "MMLU_elementary_mathematics":0.253968254,
        "MMLU_formal_logic":0.1587301587,
        "MMLU_global_facts":0.22,
        "MMLU_high_school_biology":0.3,
        "MMLU_high_school_chemistry":0.2561576355,
        "MMLU_high_school_computer_science":0.34,
        "MMLU_high_school_european_history":0.2060606061,
        "MMLU_high_school_geography":0.2171717172,
        "MMLU_high_school_government_and_politics":0.2279792746,
        "MMLU_high_school_macroeconomics":0.2153846154,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.2521008403,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.2495412844,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2254901961,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3587443946,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.1983471074,
        "MMLU_jurisprudence":0.2222222222,
        "MMLU_logical_fallacies":0.282208589,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.2912621359,
        "MMLU_marketing":0.2136752137,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2541507024,
        "MMLU_moral_disputes":0.2369942197,
        "MMLU_moral_scenarios":0.2446927374,
        "MMLU_nutrition":0.2222222222,
        "MMLU_philosophy":0.2282958199,
        "MMLU_prehistory":0.2283950617,
        "MMLU_professional_accounting":0.2588652482,
        "MMLU_professional_law":0.2451108214,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2352941176,
        "MMLU_public_relations":0.2909090909,
        "MMLU_security_studies":0.2,
        "MMLU_sociology":0.2388059701,
        "MMLU_us_foreign_policy":0.24,
        "MMLU_virology":0.2108433735,
        "MMLU_world_religions":0.2046783626
    },
    {
        "Model":"gogpt-3b-bloom",
        "URL":"https:\/\/huggingface.co\/golaxy\/gogpt-3b-bloom",
        "full_model_name":"golaxy\/gogpt-3b-bloom",
        "Parameters":3.0,
        "MMLU_average":0.2519778666,
        "arc:challenge|25":0.2901023891,
        "hellaswag|10":0.3998207528,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.2592592593,
        "MMLU_astronomy":0.3092105263,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.2113207547,
        "MMLU_college_biology":0.25,
        "MMLU_college_chemistry":0.18,
        "MMLU_college_computer_science":0.38,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.2138728324,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.29,
        "MMLU_conceptual_physics":0.2170212766,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2965517241,
        "MMLU_elementary_mathematics":0.2751322751,
        "MMLU_formal_logic":0.1984126984,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.3,
        "MMLU_high_school_chemistry":0.3201970443,
        "MMLU_high_school_computer_science":0.3,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.2575757576,
        "MMLU_high_school_government_and_politics":0.2383419689,
        "MMLU_high_school_macroeconomics":0.2282051282,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.2226890756,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.2385321101,
        "MMLU_high_school_statistics":0.2453703704,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2573839662,
        "MMLU_human_aging":0.2286995516,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.347107438,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.2515337423,
        "MMLU_machine_learning":0.2321428571,
        "MMLU_management":0.1844660194,
        "MMLU_marketing":0.1965811966,
        "MMLU_medical_genetics":0.17,
        "MMLU_miscellaneous":0.2707535121,
        "MMLU_moral_disputes":0.2514450867,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.2679738562,
        "MMLU_philosophy":0.2765273312,
        "MMLU_prehistory":0.274691358,
        "MMLU_professional_accounting":0.2695035461,
        "MMLU_professional_law":0.2470664928,
        "MMLU_professional_medicine":0.1654411765,
        "MMLU_professional_psychology":0.2630718954,
        "MMLU_public_relations":0.1727272727,
        "MMLU_security_studies":0.2408163265,
        "MMLU_sociology":0.263681592,
        "MMLU_us_foreign_policy":0.31,
        "MMLU_virology":0.2168674699,
        "MMLU_world_religions":0.2748538012
    },
    {
        "Model":"gpt-neo-1.3B-4bit-alpaca",
        "URL":"https:\/\/huggingface.co\/TFLai\/gpt-neo-1.3B-4bit-alpaca",
        "full_model_name":"TFLai\/gpt-neo-1.3B-4bit-alpaca",
        "Parameters":1.3,
        "MMLU_average":0.2518979278,
        "arc:challenge|25":0.2482935154,
        "hellaswag|10":0.3634734117,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.1703703704,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.31,
        "MMLU_clinical_knowledge":0.2528301887,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.27,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.2169312169,
        "MMLU_formal_logic":0.2698412698,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.2129032258,
        "MMLU_high_school_chemistry":0.2216748768,
        "MMLU_high_school_computer_science":0.22,
        "MMLU_high_school_european_history":0.2303030303,
        "MMLU_high_school_geography":0.1666666667,
        "MMLU_high_school_government_and_politics":0.2383419689,
        "MMLU_high_school_macroeconomics":0.2974358974,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.2847682119,
        "MMLU_high_school_psychology":0.3082568807,
        "MMLU_high_school_statistics":0.4398148148,
        "MMLU_high_school_us_history":0.2696078431,
        "MMLU_high_school_world_history":0.3080168776,
        "MMLU_human_aging":0.2376681614,
        "MMLU_human_sexuality":0.2824427481,
        "MMLU_international_law":0.2727272727,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2120051086,
        "MMLU_moral_disputes":0.2398843931,
        "MMLU_moral_scenarios":0.2502793296,
        "MMLU_nutrition":0.2287581699,
        "MMLU_philosophy":0.1800643087,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2375886525,
        "MMLU_professional_law":0.2451108214,
        "MMLU_professional_medicine":0.2573529412,
        "MMLU_professional_psychology":0.2565359477,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.2775510204,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.29,
        "MMLU_virology":0.313253012,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"zephyr-smol_llama-100m-dpo-full",
        "URL":"https:\/\/huggingface.co\/amazingvince\/zephyr-smol_llama-100m-dpo-full",
        "full_model_name":"amazingvince\/zephyr-smol_llama-100m-dpo-full",
        "Parameters":0.1,
        "MMLU_average":0.2518063794,
        "arc:challenge|25":0.1979522184,
        "hellaswag|10":0.2779326827,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.3407407407,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.24,
        "MMLU_clinical_knowledge":0.2264150943,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.18,
        "MMLU_college_computer_science":0.21,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.27,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2328042328,
        "MMLU_formal_logic":0.1507936508,
        "MMLU_global_facts":0.17,
        "MMLU_high_school_biology":0.3032258065,
        "MMLU_high_school_chemistry":0.2807881773,
        "MMLU_high_school_computer_science":0.3,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.2373737374,
        "MMLU_high_school_government_and_politics":0.2642487047,
        "MMLU_high_school_macroeconomics":0.2897435897,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.2773109244,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.2073394495,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.3039215686,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.3049327354,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2231404959,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.2944785276,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.213592233,
        "MMLU_marketing":0.2820512821,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2401021711,
        "MMLU_moral_disputes":0.2456647399,
        "MMLU_moral_scenarios":0.2480446927,
        "MMLU_nutrition":0.1764705882,
        "MMLU_philosophy":0.2025723473,
        "MMLU_prehistory":0.2098765432,
        "MMLU_professional_accounting":0.2269503546,
        "MMLU_professional_law":0.241851369,
        "MMLU_professional_medicine":0.3198529412,
        "MMLU_professional_psychology":0.2549019608,
        "MMLU_public_relations":0.1818181818,
        "MMLU_security_studies":0.2489795918,
        "MMLU_sociology":0.2587064677,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.2590361446,
        "MMLU_world_religions":0.2105263158
    },
    {
        "Model":"guanaco-65b-merged",
        "URL":"https:\/\/huggingface.co\/timdettmers\/guanaco-65b-merged",
        "full_model_name":"timdettmers\/guanaco-65b-merged",
        "Parameters":65.0,
        "MMLU_average":0.2517020428,
        "arc:challenge|25":0.2030716724,
        "hellaswag|10":0.2615016929,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.1851851852,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.208994709,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.1774193548,
        "MMLU_high_school_chemistry":0.1527093596,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.7696969697,
        "MMLU_high_school_geography":0.1767676768,
        "MMLU_high_school_government_and_politics":0.1968911917,
        "MMLU_high_school_macroeconomics":0.2025641026,
        "MMLU_high_school_mathematics":0.2111111111,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.1926605505,
        "MMLU_high_school_statistics":0.1527777778,
        "MMLU_high_school_us_history":0.8382352941,
        "MMLU_high_school_world_history":0.2742616034,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2948717949,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2375478927,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.1864951768,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2340425532,
        "MMLU_professional_law":0.2679269883,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"Cerebras-GPT-2.7B",
        "URL":"https:\/\/huggingface.co\/cerebras\/Cerebras-GPT-2.7B",
        "full_model_name":"cerebras\/Cerebras-GPT-2.7B",
        "Parameters":2.7,
        "MMLU_average":0.2516691582,
        "arc:challenge|25":0.2696245734,
        "hellaswag|10":0.3854809799,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.2666666667,
        "MMLU_astronomy":0.1907894737,
        "MMLU_business_ethics":0.22,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2708333333,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.37,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.2312138728,
        "MMLU_college_physics":0.1764705882,
        "MMLU_computer_security":0.26,
        "MMLU_conceptual_physics":0.2553191489,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2010582011,
        "MMLU_formal_logic":0.2222222222,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.2451612903,
        "MMLU_high_school_chemistry":0.2709359606,
        "MMLU_high_school_computer_science":0.39,
        "MMLU_high_school_european_history":0.2303030303,
        "MMLU_high_school_geography":0.2222222222,
        "MMLU_high_school_government_and_politics":0.2176165803,
        "MMLU_high_school_macroeconomics":0.2384615385,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.231092437,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.2330275229,
        "MMLU_high_school_statistics":0.287037037,
        "MMLU_high_school_us_history":0.2696078431,
        "MMLU_high_school_world_history":0.2573839662,
        "MMLU_human_aging":0.2421524664,
        "MMLU_human_sexuality":0.2366412214,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.1851851852,
        "MMLU_logical_fallacies":0.3190184049,
        "MMLU_machine_learning":0.3392857143,
        "MMLU_management":0.213592233,
        "MMLU_marketing":0.2564102564,
        "MMLU_medical_genetics":0.22,
        "MMLU_miscellaneous":0.2669220945,
        "MMLU_moral_disputes":0.2658959538,
        "MMLU_moral_scenarios":0.2715083799,
        "MMLU_nutrition":0.2712418301,
        "MMLU_philosophy":0.2668810289,
        "MMLU_prehistory":0.2314814815,
        "MMLU_professional_accounting":0.2659574468,
        "MMLU_professional_law":0.2640156454,
        "MMLU_professional_medicine":0.2536764706,
        "MMLU_professional_psychology":0.2630718954,
        "MMLU_public_relations":0.1727272727,
        "MMLU_security_studies":0.3183673469,
        "MMLU_sociology":0.2537313433,
        "MMLU_us_foreign_policy":0.29,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.269005848
    },
    {
        "Model":"RedPajama-INCITE-Chat-3B-v1-FT-LoRA-8bit-test1",
        "URL":"https:\/\/huggingface.co\/DanielSc4\/RedPajama-INCITE-Chat-3B-v1-FT-LoRA-8bit-test1",
        "full_model_name":"DanielSc4\/RedPajama-INCITE-Chat-3B-v1-FT-LoRA-8bit-test1",
        "Parameters":3.0,
        "MMLU_average":0.2516405686,
        "arc:challenge|25":0.364334471,
        "hellaswag|10":0.4704242183,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.2592592593,
        "MMLU_astronomy":0.2631578947,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2708333333,
        "MMLU_college_chemistry":0.25,
        "MMLU_college_computer_science":0.22,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.2369942197,
        "MMLU_college_physics":0.3235294118,
        "MMLU_computer_security":0.26,
        "MMLU_conceptual_physics":0.2340425532,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2827586207,
        "MMLU_elementary_mathematics":0.253968254,
        "MMLU_formal_logic":0.1904761905,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2387096774,
        "MMLU_high_school_chemistry":0.2610837438,
        "MMLU_high_school_computer_science":0.26,
        "MMLU_high_school_european_history":0.2484848485,
        "MMLU_high_school_geography":0.2373737374,
        "MMLU_high_school_government_and_politics":0.1968911917,
        "MMLU_high_school_macroeconomics":0.2076923077,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.218487395,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.2568807339,
        "MMLU_high_school_statistics":0.2083333333,
        "MMLU_high_school_us_history":0.2794117647,
        "MMLU_high_school_world_history":0.2616033755,
        "MMLU_human_aging":0.264573991,
        "MMLU_human_sexuality":0.1832061069,
        "MMLU_international_law":0.347107438,
        "MMLU_jurisprudence":0.2407407407,
        "MMLU_logical_fallacies":0.2699386503,
        "MMLU_machine_learning":0.2946428571,
        "MMLU_management":0.2233009709,
        "MMLU_marketing":0.2564102564,
        "MMLU_medical_genetics":0.24,
        "MMLU_miscellaneous":0.2579821201,
        "MMLU_moral_disputes":0.2283236994,
        "MMLU_moral_scenarios":0.2413407821,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.2990353698,
        "MMLU_prehistory":0.2808641975,
        "MMLU_professional_accounting":0.2553191489,
        "MMLU_professional_law":0.2777053455,
        "MMLU_professional_medicine":0.1727941176,
        "MMLU_professional_psychology":0.2483660131,
        "MMLU_public_relations":0.2545454545,
        "MMLU_security_studies":0.212244898,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.25,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.298245614
    },
    {
        "Model":"YetAnother_Open-Llama-3B-LoRA-OpenOrca",
        "URL":"https:\/\/huggingface.co\/Andron00e\/YetAnother_Open-Llama-3B-LoRA-OpenOrca",
        "full_model_name":"Andron00e\/YetAnother_Open-Llama-3B-LoRA-OpenOrca",
        "Parameters":3.0,
        "MMLU_average":0.2515824057,
        "arc:challenge|25":0.226109215,
        "hellaswag|10":0.2597092213,
        "MMLU_abstract_algebra":0.18,
        "MMLU_anatomy":0.2444444444,
        "MMLU_astronomy":0.2763157895,
        "MMLU_business_ethics":0.26,
        "MMLU_clinical_knowledge":0.3056603774,
        "MMLU_college_biology":0.2361111111,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.21,
        "MMLU_college_mathematics":0.19,
        "MMLU_college_medicine":0.2832369942,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.26,
        "MMLU_conceptual_physics":0.2723404255,
        "MMLU_econometrics":0.3157894737,
        "MMLU_electrical_engineering":0.2965517241,
        "MMLU_elementary_mathematics":0.2275132275,
        "MMLU_formal_logic":0.1904761905,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.2935483871,
        "MMLU_high_school_chemistry":0.2463054187,
        "MMLU_high_school_computer_science":0.24,
        "MMLU_high_school_european_history":0.2606060606,
        "MMLU_high_school_geography":0.2373737374,
        "MMLU_high_school_government_and_politics":0.2383419689,
        "MMLU_high_school_macroeconomics":0.2717948718,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.2394957983,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.2311926606,
        "MMLU_high_school_statistics":0.2222222222,
        "MMLU_high_school_us_history":0.2794117647,
        "MMLU_high_school_world_history":0.2362869198,
        "MMLU_human_aging":0.2511210762,
        "MMLU_human_sexuality":0.2748091603,
        "MMLU_international_law":0.2644628099,
        "MMLU_jurisprudence":0.2222222222,
        "MMLU_logical_fallacies":0.2944785276,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.213592233,
        "MMLU_marketing":0.2222222222,
        "MMLU_medical_genetics":0.18,
        "MMLU_miscellaneous":0.2541507024,
        "MMLU_moral_disputes":0.2861271676,
        "MMLU_moral_scenarios":0.225698324,
        "MMLU_nutrition":0.2352941176,
        "MMLU_philosophy":0.2090032154,
        "MMLU_prehistory":0.237654321,
        "MMLU_professional_accounting":0.2730496454,
        "MMLU_professional_law":0.2483702738,
        "MMLU_professional_medicine":0.2463235294,
        "MMLU_professional_psychology":0.2696078431,
        "MMLU_public_relations":0.2363636364,
        "MMLU_security_studies":0.2489795918,
        "MMLU_sociology":0.2985074627,
        "MMLU_us_foreign_policy":0.23,
        "MMLU_virology":0.3192771084,
        "MMLU_world_religions":0.3099415205
    },
    {
        "Model":"gpt-neo-125m-neurallinguisticpioneers",
        "URL":"https:\/\/huggingface.co\/ogimgio\/gpt-neo-125m-neurallinguisticpioneers",
        "full_model_name":"ogimgio\/gpt-neo-125m-neurallinguisticpioneers",
        "Parameters":0.125,
        "MMLU_average":0.2513786599,
        "arc:challenge|25":0.1953924915,
        "hellaswag|10":0.285202151,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.3037037037,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.2264150943,
        "MMLU_college_biology":0.2152777778,
        "MMLU_college_chemistry":0.23,
        "MMLU_college_computer_science":0.28,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.2023121387,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.21,
        "MMLU_conceptual_physics":0.2936170213,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2482758621,
        "MMLU_elementary_mathematics":0.2328042328,
        "MMLU_formal_logic":0.1666666667,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.2548387097,
        "MMLU_high_school_chemistry":0.2709359606,
        "MMLU_high_school_computer_science":0.3,
        "MMLU_high_school_european_history":0.2606060606,
        "MMLU_high_school_geography":0.3282828283,
        "MMLU_high_school_government_and_politics":0.3005181347,
        "MMLU_high_school_macroeconomics":0.258974359,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.2052980132,
        "MMLU_high_school_psychology":0.2201834862,
        "MMLU_high_school_statistics":0.4212962963,
        "MMLU_high_school_us_history":0.2696078431,
        "MMLU_high_school_world_history":0.2067510549,
        "MMLU_human_aging":0.2376681614,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2809917355,
        "MMLU_jurisprudence":0.2314814815,
        "MMLU_logical_fallacies":0.3067484663,
        "MMLU_machine_learning":0.2232142857,
        "MMLU_management":0.1553398058,
        "MMLU_marketing":0.2521367521,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2554278416,
        "MMLU_moral_disputes":0.2427745665,
        "MMLU_moral_scenarios":0.2446927374,
        "MMLU_nutrition":0.2647058824,
        "MMLU_philosophy":0.1864951768,
        "MMLU_prehistory":0.2407407407,
        "MMLU_professional_accounting":0.2304964539,
        "MMLU_professional_law":0.2425032595,
        "MMLU_professional_medicine":0.4117647059,
        "MMLU_professional_psychology":0.2679738562,
        "MMLU_public_relations":0.2454545455,
        "MMLU_security_studies":0.387755102,
        "MMLU_sociology":0.223880597,
        "MMLU_us_foreign_policy":0.23,
        "MMLU_virology":0.2590361446,
        "MMLU_world_religions":0.2105263158
    },
    {
        "Model":"RWKV-pileplus-1B5-evol_instruct_v2",
        "URL":"https:\/\/huggingface.co\/KnutJaegersberg\/RWKV-pileplus-1B5-evol_instruct_v2",
        "full_model_name":"KnutJaegersberg\/RWKV-pileplus-1B5-evol_instruct_v2",
        "Parameters":1.0,
        "MMLU_average":0.251336191,
        "arc:challenge|25":0.2935153584,
        "hellaswag|10":0.4223262298,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.3111111111,
        "MMLU_astronomy":0.2368421053,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.1962264151,
        "MMLU_college_biology":0.2430555556,
        "MMLU_college_chemistry":0.15,
        "MMLU_college_computer_science":0.2,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.2485549133,
        "MMLU_college_physics":0.1274509804,
        "MMLU_computer_security":0.23,
        "MMLU_conceptual_physics":0.3063829787,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.1904761905,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2709677419,
        "MMLU_high_school_chemistry":0.2512315271,
        "MMLU_high_school_computer_science":0.34,
        "MMLU_high_school_european_history":0.2727272727,
        "MMLU_high_school_geography":0.1565656566,
        "MMLU_high_school_government_and_politics":0.2487046632,
        "MMLU_high_school_macroeconomics":0.2230769231,
        "MMLU_high_school_mathematics":0.2888888889,
        "MMLU_high_school_microeconomics":0.2058823529,
        "MMLU_high_school_physics":0.1920529801,
        "MMLU_high_school_psychology":0.2201834862,
        "MMLU_high_school_statistics":0.3009259259,
        "MMLU_high_school_us_history":0.2794117647,
        "MMLU_high_school_world_history":0.2784810127,
        "MMLU_human_aging":0.2556053812,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.3801652893,
        "MMLU_jurisprudence":0.1759259259,
        "MMLU_logical_fallacies":0.2576687117,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.2524271845,
        "MMLU_marketing":0.2820512821,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.251596424,
        "MMLU_moral_disputes":0.3005780347,
        "MMLU_moral_scenarios":0.2223463687,
        "MMLU_nutrition":0.2483660131,
        "MMLU_philosophy":0.2797427653,
        "MMLU_prehistory":0.262345679,
        "MMLU_professional_accounting":0.2907801418,
        "MMLU_professional_law":0.2718383312,
        "MMLU_professional_medicine":0.1985294118,
        "MMLU_professional_psychology":0.2777777778,
        "MMLU_public_relations":0.2636363636,
        "MMLU_security_studies":0.2285714286,
        "MMLU_sociology":0.2338308458,
        "MMLU_us_foreign_policy":0.19,
        "MMLU_virology":0.313253012,
        "MMLU_world_religions":0.2807017544
    },
    {
        "Model":"opt-flan-iml-6.7b",
        "URL":"https:\/\/huggingface.co\/MayaPH\/opt-flan-iml-6.7b",
        "full_model_name":"MayaPH\/opt-flan-iml-6.7b",
        "Parameters":6.7,
        "MMLU_average":0.251205989,
        "arc:challenge|25":0.2372013652,
        "hellaswag|10":0.4442342163,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.2518518519,
        "MMLU_astronomy":0.1710526316,
        "MMLU_business_ethics":0.26,
        "MMLU_clinical_knowledge":0.2679245283,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.22,
        "MMLU_college_computer_science":0.16,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.2023121387,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.27,
        "MMLU_conceptual_physics":0.3234042553,
        "MMLU_econometrics":0.3070175439,
        "MMLU_electrical_engineering":0.2275862069,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.1904761905,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2838709677,
        "MMLU_high_school_chemistry":0.275862069,
        "MMLU_high_school_computer_science":0.24,
        "MMLU_high_school_european_history":0.2545454545,
        "MMLU_high_school_geography":0.2171717172,
        "MMLU_high_school_government_and_politics":0.1917098446,
        "MMLU_high_school_macroeconomics":0.3358974359,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.231092437,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.2385321101,
        "MMLU_high_school_statistics":0.3101851852,
        "MMLU_high_school_us_history":0.2352941176,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.3677130045,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.2561983471,
        "MMLU_jurisprudence":0.2407407407,
        "MMLU_logical_fallacies":0.245398773,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.1844660194,
        "MMLU_marketing":0.2521367521,
        "MMLU_medical_genetics":0.27,
        "MMLU_miscellaneous":0.2860791826,
        "MMLU_moral_disputes":0.2369942197,
        "MMLU_moral_scenarios":0.243575419,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.2797427653,
        "MMLU_prehistory":0.2839506173,
        "MMLU_professional_accounting":0.2588652482,
        "MMLU_professional_law":0.2366362451,
        "MMLU_professional_medicine":0.3382352941,
        "MMLU_professional_psychology":0.2075163399,
        "MMLU_public_relations":0.3272727273,
        "MMLU_security_studies":0.1673469388,
        "MMLU_sociology":0.2388059701,
        "MMLU_us_foreign_policy":0.25,
        "MMLU_virology":0.3192771084,
        "MMLU_world_religions":0.2105263158
    },
    {
        "Model":"b1ade-1b",
        "URL":"https:\/\/huggingface.co\/w601sxs\/b1ade-1b",
        "full_model_name":"w601sxs\/b1ade-1b",
        "Parameters":1.0,
        "MMLU_average":0.2511429621,
        "arc:challenge|25":0.2406143345,
        "hellaswag|10":0.3672575184,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.2518518519,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.26,
        "MMLU_clinical_knowledge":0.2603773585,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.15,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.2312138728,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.24,
        "MMLU_conceptual_physics":0.2595744681,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2896551724,
        "MMLU_elementary_mathematics":0.253968254,
        "MMLU_formal_logic":0.2142857143,
        "MMLU_global_facts":0.24,
        "MMLU_high_school_biology":0.264516129,
        "MMLU_high_school_chemistry":0.2709359606,
        "MMLU_high_school_computer_science":0.26,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.2070707071,
        "MMLU_high_school_government_and_politics":0.2331606218,
        "MMLU_high_school_macroeconomics":0.2128205128,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.231092437,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.2256880734,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2352941176,
        "MMLU_high_school_world_history":0.2616033755,
        "MMLU_human_aging":0.3228699552,
        "MMLU_human_sexuality":0.2671755725,
        "MMLU_international_law":0.2314049587,
        "MMLU_jurisprudence":0.212962963,
        "MMLU_logical_fallacies":0.2392638037,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.213592233,
        "MMLU_marketing":0.2735042735,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2733077905,
        "MMLU_moral_disputes":0.2283236994,
        "MMLU_moral_scenarios":0.2357541899,
        "MMLU_nutrition":0.2287581699,
        "MMLU_philosophy":0.1897106109,
        "MMLU_prehistory":0.2901234568,
        "MMLU_professional_accounting":0.2482269504,
        "MMLU_professional_law":0.2425032595,
        "MMLU_professional_medicine":0.3933823529,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2545454545,
        "MMLU_security_studies":0.1795918367,
        "MMLU_sociology":0.2338308458,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.3253012048,
        "MMLU_world_religions":0.2222222222
    },
    {
        "Model":"falcon_1b_stage3",
        "URL":"https:\/\/huggingface.co\/euclaise\/falcon_1b_stage3",
        "full_model_name":"euclaise\/falcon_1b_stage3",
        "Parameters":1.0,
        "MMLU_average":0.2510525429,
        "arc:challenge|25":0.3054607509,
        "hellaswag|10":0.4156542521,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.3185185185,
        "MMLU_astronomy":0.1578947368,
        "MMLU_business_ethics":0.14,
        "MMLU_clinical_knowledge":0.2452830189,
        "MMLU_college_biology":0.2777777778,
        "MMLU_college_chemistry":0.18,
        "MMLU_college_computer_science":0.2,
        "MMLU_college_mathematics":0.23,
        "MMLU_college_medicine":0.2716763006,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.24,
        "MMLU_conceptual_physics":0.2468085106,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.2896551724,
        "MMLU_elementary_mathematics":0.253968254,
        "MMLU_formal_logic":0.1587301587,
        "MMLU_global_facts":0.29,
        "MMLU_high_school_biology":0.2677419355,
        "MMLU_high_school_chemistry":0.2512315271,
        "MMLU_high_school_computer_science":0.26,
        "MMLU_high_school_european_history":0.3090909091,
        "MMLU_high_school_geography":0.2121212121,
        "MMLU_high_school_government_and_politics":0.2797927461,
        "MMLU_high_school_macroeconomics":0.2564102564,
        "MMLU_high_school_mathematics":0.2185185185,
        "MMLU_high_school_microeconomics":0.2941176471,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.2073394495,
        "MMLU_high_school_statistics":0.2037037037,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.2489451477,
        "MMLU_human_aging":0.2152466368,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2561983471,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.3006134969,
        "MMLU_machine_learning":0.3571428571,
        "MMLU_management":0.2233009709,
        "MMLU_marketing":0.2051282051,
        "MMLU_medical_genetics":0.23,
        "MMLU_miscellaneous":0.2745849298,
        "MMLU_moral_disputes":0.274566474,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2156862745,
        "MMLU_philosophy":0.3376205788,
        "MMLU_prehistory":0.274691358,
        "MMLU_professional_accounting":0.2588652482,
        "MMLU_professional_law":0.2496740548,
        "MMLU_professional_medicine":0.2536764706,
        "MMLU_professional_psychology":0.2369281046,
        "MMLU_public_relations":0.2363636364,
        "MMLU_security_studies":0.212244898,
        "MMLU_sociology":0.3084577114,
        "MMLU_us_foreign_policy":0.34,
        "MMLU_virology":0.2409638554,
        "MMLU_world_religions":0.2923976608
    },
    {
        "Model":"Aira-2-774M",
        "URL":"https:\/\/huggingface.co\/nicholasKluge\/Aira-2-774M",
        "full_model_name":"nicholasKluge\/Aira-2-774M",
        "Parameters":0.774,
        "MMLU_average":0.251044515,
        "arc:challenge|25":0.2610921502,
        "hellaswag|10":0.3414658435,
        "MMLU_abstract_algebra":0.19,
        "MMLU_anatomy":0.3111111111,
        "MMLU_astronomy":0.1644736842,
        "MMLU_business_ethics":0.27,
        "MMLU_clinical_knowledge":0.2943396226,
        "MMLU_college_biology":0.2638888889,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.27,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.3410404624,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.32,
        "MMLU_conceptual_physics":0.3063829787,
        "MMLU_econometrics":0.201754386,
        "MMLU_electrical_engineering":0.2344827586,
        "MMLU_elementary_mathematics":0.1931216931,
        "MMLU_formal_logic":0.1984126984,
        "MMLU_global_facts":0.24,
        "MMLU_high_school_biology":0.2451612903,
        "MMLU_high_school_chemistry":0.1625615764,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.3131313131,
        "MMLU_high_school_government_and_politics":0.2642487047,
        "MMLU_high_school_macroeconomics":0.2307692308,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.2142857143,
        "MMLU_high_school_physics":0.2450331126,
        "MMLU_high_school_psychology":0.2605504587,
        "MMLU_high_school_statistics":0.1990740741,
        "MMLU_high_school_us_history":0.2696078431,
        "MMLU_high_school_world_history":0.2911392405,
        "MMLU_human_aging":0.1255605381,
        "MMLU_human_sexuality":0.2366412214,
        "MMLU_international_law":0.2644628099,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2944785276,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.3786407767,
        "MMLU_marketing":0.2820512821,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.1992337165,
        "MMLU_moral_disputes":0.2658959538,
        "MMLU_moral_scenarios":0.2368715084,
        "MMLU_nutrition":0.2712418301,
        "MMLU_philosophy":0.3279742765,
        "MMLU_prehistory":0.225308642,
        "MMLU_professional_accounting":0.2588652482,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.1654411765,
        "MMLU_professional_psychology":0.2401960784,
        "MMLU_public_relations":0.1909090909,
        "MMLU_security_studies":0.2204081633,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.2590361446,
        "MMLU_world_religions":0.2456140351
    },
    {
        "Model":"bloomz-560m",
        "URL":"https:\/\/huggingface.co\/bigscience\/bloomz-560m",
        "full_model_name":"bigscience\/bloomz-560m",
        "Parameters":0.56,
        "MMLU_average":0.2510043779,
        "arc:challenge|25":0.2081911263,
        "hellaswag|10":0.3127862976,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.2,
        "MMLU_astronomy":0.1842105263,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2430555556,
        "MMLU_college_chemistry":0.22,
        "MMLU_college_computer_science":0.29,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.2658959538,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.2,
        "MMLU_conceptual_physics":0.2042553191,
        "MMLU_econometrics":0.2105263158,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.2619047619,
        "MMLU_formal_logic":0.2063492063,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.2580645161,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2363636364,
        "MMLU_high_school_geography":0.2373737374,
        "MMLU_high_school_government_and_politics":0.2020725389,
        "MMLU_high_school_macroeconomics":0.2615384615,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.2142857143,
        "MMLU_high_school_physics":0.2582781457,
        "MMLU_high_school_psychology":0.319266055,
        "MMLU_high_school_statistics":0.2175925926,
        "MMLU_high_school_us_history":0.2107843137,
        "MMLU_high_school_world_history":0.2827004219,
        "MMLU_human_aging":0.2331838565,
        "MMLU_human_sexuality":0.2671755725,
        "MMLU_international_law":0.3140495868,
        "MMLU_jurisprudence":0.3055555556,
        "MMLU_logical_fallacies":0.3251533742,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2948717949,
        "MMLU_medical_genetics":0.29,
        "MMLU_miscellaneous":0.2784163474,
        "MMLU_moral_disputes":0.2572254335,
        "MMLU_moral_scenarios":0.2312849162,
        "MMLU_nutrition":0.2189542484,
        "MMLU_philosophy":0.1897106109,
        "MMLU_prehistory":0.237654321,
        "MMLU_professional_accounting":0.2907801418,
        "MMLU_professional_law":0.2477183833,
        "MMLU_professional_medicine":0.3676470588,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2363636364,
        "MMLU_security_studies":0.2,
        "MMLU_sociology":0.2736318408,
        "MMLU_us_foreign_policy":0.31,
        "MMLU_virology":0.2289156627,
        "MMLU_world_religions":0.3274853801
    },
    {
        "Model":"xglm-1.7B",
        "URL":"https:\/\/huggingface.co\/facebook\/xglm-1.7B",
        "full_model_name":"facebook\/xglm-1.7B",
        "Parameters":1.7,
        "MMLU_average":0.2510041646,
        "arc:challenge|25":0.2209897611,
        "hellaswag|10":0.3633738299,
        "MMLU_abstract_algebra":0.3,
        "MMLU_anatomy":0.3259259259,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.2,
        "MMLU_clinical_knowledge":0.2188679245,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.18,
        "MMLU_college_computer_science":0.24,
        "MMLU_college_mathematics":0.2,
        "MMLU_college_medicine":0.2138728324,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.22,
        "MMLU_conceptual_physics":0.2468085106,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2275862069,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.1904761905,
        "MMLU_global_facts":0.2,
        "MMLU_high_school_biology":0.2838709677,
        "MMLU_high_school_chemistry":0.3103448276,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.2242424242,
        "MMLU_high_school_geography":0.2676767677,
        "MMLU_high_school_government_and_politics":0.2279792746,
        "MMLU_high_school_macroeconomics":0.2102564103,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.2142857143,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.2256880734,
        "MMLU_high_school_statistics":0.4768518519,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.253164557,
        "MMLU_human_aging":0.2376681614,
        "MMLU_human_sexuality":0.1679389313,
        "MMLU_international_law":0.2479338843,
        "MMLU_jurisprudence":0.2314814815,
        "MMLU_logical_fallacies":0.2392638037,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.1941747573,
        "MMLU_marketing":0.2393162393,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2669220945,
        "MMLU_moral_disputes":0.2456647399,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2287581699,
        "MMLU_philosophy":0.2508038585,
        "MMLU_prehistory":0.3209876543,
        "MMLU_professional_accounting":0.2695035461,
        "MMLU_professional_law":0.239243807,
        "MMLU_professional_medicine":0.4522058824,
        "MMLU_professional_psychology":0.2614379085,
        "MMLU_public_relations":0.2272727273,
        "MMLU_security_studies":0.2204081633,
        "MMLU_sociology":0.2388059701,
        "MMLU_us_foreign_policy":0.33,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.216374269
    },
    {
        "Model":"dlite-v1-124m",
        "URL":"https:\/\/huggingface.co\/aisquared\/dlite-v1-124m",
        "full_model_name":"aisquared\/dlite-v1-124m",
        "Parameters":0.124,
        "MMLU_average":0.2508005008,
        "arc:challenge|25":0.2167235495,
        "hellaswag|10":0.292571201,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.2666666667,
        "MMLU_astronomy":0.1842105263,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.2037735849,
        "MMLU_college_biology":0.2847222222,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.35,
        "MMLU_college_mathematics":0.23,
        "MMLU_college_medicine":0.2543352601,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.22,
        "MMLU_conceptual_physics":0.2212765957,
        "MMLU_econometrics":0.2105263158,
        "MMLU_electrical_engineering":0.2137931034,
        "MMLU_elementary_mathematics":0.2328042328,
        "MMLU_formal_logic":0.1825396825,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.2225806452,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.32,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.297979798,
        "MMLU_high_school_government_and_politics":0.310880829,
        "MMLU_high_school_macroeconomics":0.2897435897,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.2226890756,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.3100917431,
        "MMLU_high_school_statistics":0.3425925926,
        "MMLU_high_school_us_history":0.2401960784,
        "MMLU_high_school_world_history":0.2827004219,
        "MMLU_human_aging":0.1569506726,
        "MMLU_human_sexuality":0.2442748092,
        "MMLU_international_law":0.305785124,
        "MMLU_jurisprudence":0.2037037037,
        "MMLU_logical_fallacies":0.282208589,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.213592233,
        "MMLU_marketing":0.2777777778,
        "MMLU_medical_genetics":0.2,
        "MMLU_miscellaneous":0.2630906769,
        "MMLU_moral_disputes":0.2398843931,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2581699346,
        "MMLU_philosophy":0.2861736334,
        "MMLU_prehistory":0.2561728395,
        "MMLU_professional_accounting":0.2517730496,
        "MMLU_professional_law":0.239243807,
        "MMLU_professional_medicine":0.3786764706,
        "MMLU_professional_psychology":0.2565359477,
        "MMLU_public_relations":0.2636363636,
        "MMLU_security_studies":0.2326530612,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.25,
        "MMLU_virology":0.1927710843,
        "MMLU_world_religions":0.3040935673
    },
    {
        "Model":"gpt-neox-20b-4bit-alpaca",
        "URL":"https:\/\/huggingface.co\/TFLai\/gpt-neox-20b-4bit-alpaca",
        "full_model_name":"TFLai\/gpt-neox-20b-4bit-alpaca",
        "Parameters":20.0,
        "MMLU_average":0.2505802321,
        "arc:challenge|25":0.4146757679,
        "hellaswag|10":0.5090619399,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.237037037,
        "MMLU_astronomy":0.2105263158,
        "MMLU_business_ethics":0.33,
        "MMLU_clinical_knowledge":0.2377358491,
        "MMLU_college_biology":0.2708333333,
        "MMLU_college_chemistry":0.22,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.23,
        "MMLU_college_medicine":0.2138728324,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.32,
        "MMLU_conceptual_physics":0.2765957447,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.2689655172,
        "MMLU_elementary_mathematics":0.2380952381,
        "MMLU_formal_logic":0.2301587302,
        "MMLU_global_facts":0.29,
        "MMLU_high_school_biology":0.1967741935,
        "MMLU_high_school_chemistry":0.1477832512,
        "MMLU_high_school_computer_science":0.18,
        "MMLU_high_school_european_history":0.2242424242,
        "MMLU_high_school_geography":0.1717171717,
        "MMLU_high_school_government_and_politics":0.207253886,
        "MMLU_high_school_macroeconomics":0.258974359,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.218487395,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.2146788991,
        "MMLU_high_school_statistics":0.1851851852,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.2573839662,
        "MMLU_human_aging":0.3497757848,
        "MMLU_human_sexuality":0.3129770992,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2222222222,
        "MMLU_logical_fallacies":0.2331288344,
        "MMLU_machine_learning":0.1785714286,
        "MMLU_management":0.2718446602,
        "MMLU_marketing":0.235042735,
        "MMLU_medical_genetics":0.35,
        "MMLU_miscellaneous":0.2899106003,
        "MMLU_moral_disputes":0.3005780347,
        "MMLU_moral_scenarios":0.2458100559,
        "MMLU_nutrition":0.2483660131,
        "MMLU_philosophy":0.3279742765,
        "MMLU_prehistory":0.237654321,
        "MMLU_professional_accounting":0.2446808511,
        "MMLU_professional_law":0.2444589309,
        "MMLU_professional_medicine":0.1727941176,
        "MMLU_professional_psychology":0.2352941176,
        "MMLU_public_relations":0.3,
        "MMLU_security_studies":0.1795918367,
        "MMLU_sociology":0.2885572139,
        "MMLU_us_foreign_policy":0.29,
        "MMLU_virology":0.2891566265,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"CodeGPT-small-py",
        "URL":"https:\/\/huggingface.co\/microsoft\/CodeGPT-small-py",
        "full_model_name":"microsoft\/CodeGPT-small-py",
        "Parameters":null,
        "MMLU_average":0.2504909027,
        "arc:challenge|25":0.1979522184,
        "hellaswag|10":0.263095001,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.3259259259,
        "MMLU_astronomy":0.2368421053,
        "MMLU_business_ethics":0.27,
        "MMLU_clinical_knowledge":0.2188679245,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.27,
        "MMLU_college_computer_science":0.25,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.2543352601,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.22,
        "MMLU_conceptual_physics":0.2127659574,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.275862069,
        "MMLU_elementary_mathematics":0.2407407407,
        "MMLU_formal_logic":0.1904761905,
        "MMLU_global_facts":0.16,
        "MMLU_high_school_biology":0.3064516129,
        "MMLU_high_school_chemistry":0.2315270936,
        "MMLU_high_school_computer_science":0.21,
        "MMLU_high_school_european_history":0.2303030303,
        "MMLU_high_school_geography":0.2575757576,
        "MMLU_high_school_government_and_politics":0.2487046632,
        "MMLU_high_school_macroeconomics":0.2179487179,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.256302521,
        "MMLU_high_school_physics":0.3443708609,
        "MMLU_high_school_psychology":0.247706422,
        "MMLU_high_school_statistics":0.4490740741,
        "MMLU_high_school_us_history":0.2254901961,
        "MMLU_high_school_world_history":0.2447257384,
        "MMLU_human_aging":0.2600896861,
        "MMLU_human_sexuality":0.2442748092,
        "MMLU_international_law":0.1983471074,
        "MMLU_jurisprudence":0.25,
        "MMLU_logical_fallacies":0.2331288344,
        "MMLU_machine_learning":0.2410714286,
        "MMLU_management":0.2233009709,
        "MMLU_marketing":0.2008547009,
        "MMLU_medical_genetics":0.33,
        "MMLU_miscellaneous":0.2796934866,
        "MMLU_moral_disputes":0.1907514451,
        "MMLU_moral_scenarios":0.2670391061,
        "MMLU_nutrition":0.2843137255,
        "MMLU_philosophy":0.2379421222,
        "MMLU_prehistory":0.2314814815,
        "MMLU_professional_accounting":0.2234042553,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.3308823529,
        "MMLU_professional_psychology":0.2516339869,
        "MMLU_public_relations":0.2818181818,
        "MMLU_security_studies":0.2816326531,
        "MMLU_sociology":0.2288557214,
        "MMLU_us_foreign_policy":0.31,
        "MMLU_virology":0.186746988,
        "MMLU_world_religions":0.2105263158
    },
    {
        "Model":"gpt2023",
        "URL":"https:\/\/huggingface.co\/crumb\/gpt2023",
        "full_model_name":"crumb\/gpt2023",
        "Parameters":null,
        "MMLU_average":0.2504810429,
        "arc:challenge|25":0.1988054608,
        "hellaswag|10":0.2895837483,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.237037037,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.2,
        "MMLU_clinical_knowledge":0.2188679245,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.2485549133,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.19,
        "MMLU_conceptual_physics":0.2680851064,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.119047619,
        "MMLU_global_facts":0.15,
        "MMLU_high_school_biology":0.2838709677,
        "MMLU_high_school_chemistry":0.2561576355,
        "MMLU_high_school_computer_science":0.17,
        "MMLU_high_school_european_history":0.2121212121,
        "MMLU_high_school_geography":0.3535353535,
        "MMLU_high_school_government_and_politics":0.3678756477,
        "MMLU_high_school_macroeconomics":0.2179487179,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.2352941176,
        "MMLU_high_school_physics":0.2450331126,
        "MMLU_high_school_psychology":0.3467889908,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.3004484305,
        "MMLU_human_sexuality":0.2519083969,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2037037037,
        "MMLU_logical_fallacies":0.2515337423,
        "MMLU_machine_learning":0.1696428571,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.1923076923,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2541507024,
        "MMLU_moral_disputes":0.2398843931,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2549019608,
        "MMLU_philosophy":0.2186495177,
        "MMLU_prehistory":0.2222222222,
        "MMLU_professional_accounting":0.2943262411,
        "MMLU_professional_law":0.2496740548,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2565359477,
        "MMLU_public_relations":0.1818181818,
        "MMLU_security_studies":0.3959183673,
        "MMLU_sociology":0.2388059701,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2289156627,
        "MMLU_world_religions":0.2105263158
    },
    {
        "Model":"dolly-v2-3b",
        "URL":"https:\/\/huggingface.co\/databricks\/dolly-v2-3b",
        "full_model_name":"databricks\/dolly-v2-3b",
        "Parameters":3.0,
        "MMLU_average":0.2504632801,
        "arc:challenge|25":0.3558020478,
        "hellaswag|10":0.4891455885,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.2592592593,
        "MMLU_astronomy":0.1973684211,
        "MMLU_business_ethics":0.2,
        "MMLU_clinical_knowledge":0.2679245283,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.18,
        "MMLU_college_mathematics":0.22,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.29,
        "MMLU_conceptual_physics":0.2978723404,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.3034482759,
        "MMLU_elementary_mathematics":0.2645502646,
        "MMLU_formal_logic":0.1825396825,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.2774193548,
        "MMLU_high_school_chemistry":0.2463054187,
        "MMLU_high_school_computer_science":0.22,
        "MMLU_high_school_european_history":0.2363636364,
        "MMLU_high_school_geography":0.2171717172,
        "MMLU_high_school_government_and_politics":0.2124352332,
        "MMLU_high_school_macroeconomics":0.2256410256,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.2226890756,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.2550458716,
        "MMLU_high_school_statistics":0.2407407407,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.2827004219,
        "MMLU_human_aging":0.3228699552,
        "MMLU_human_sexuality":0.2213740458,
        "MMLU_international_law":0.2644628099,
        "MMLU_jurisprudence":0.2777777778,
        "MMLU_logical_fallacies":0.263803681,
        "MMLU_machine_learning":0.2946428571,
        "MMLU_management":0.3106796117,
        "MMLU_marketing":0.264957265,
        "MMLU_medical_genetics":0.22,
        "MMLU_miscellaneous":0.2643678161,
        "MMLU_moral_disputes":0.274566474,
        "MMLU_moral_scenarios":0.2346368715,
        "MMLU_nutrition":0.2418300654,
        "MMLU_philosophy":0.2733118971,
        "MMLU_prehistory":0.2654320988,
        "MMLU_professional_accounting":0.2517730496,
        "MMLU_professional_law":0.2555410691,
        "MMLU_professional_medicine":0.1801470588,
        "MMLU_professional_psychology":0.2598039216,
        "MMLU_public_relations":0.3090909091,
        "MMLU_security_studies":0.1714285714,
        "MMLU_sociology":0.2338308458,
        "MMLU_us_foreign_policy":0.21,
        "MMLU_virology":0.3072289157,
        "MMLU_world_religions":0.298245614
    },
    {
        "Model":"RedPajama-INCITE-Instruct-3B-v1",
        "URL":"https:\/\/huggingface.co\/togethercomputer\/RedPajama-INCITE-Instruct-3B-v1",
        "full_model_name":"togethercomputer\/RedPajama-INCITE-Instruct-3B-v1",
        "Parameters":3.0,
        "MMLU_average":0.2503221402,
        "arc:challenge|25":0.383105802,
        "hellaswag|10":0.4831706831,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.2296296296,
        "MMLU_astronomy":0.2039473684,
        "MMLU_business_ethics":0.31,
        "MMLU_clinical_knowledge":0.241509434,
        "MMLU_college_biology":0.2291666667,
        "MMLU_college_chemistry":0.28,
        "MMLU_college_computer_science":0.27,
        "MMLU_college_mathematics":0.28,
        "MMLU_college_medicine":0.1965317919,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.21,
        "MMLU_conceptual_physics":0.3276595745,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.1931034483,
        "MMLU_elementary_mathematics":0.2433862434,
        "MMLU_formal_logic":0.2301587302,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.2225806452,
        "MMLU_high_school_chemistry":0.2118226601,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2363636364,
        "MMLU_high_school_geography":0.1919191919,
        "MMLU_high_school_government_and_politics":0.2331606218,
        "MMLU_high_school_macroeconomics":0.1794871795,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.2226890756,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.2183486239,
        "MMLU_high_school_statistics":0.1851851852,
        "MMLU_high_school_us_history":0.1960784314,
        "MMLU_high_school_world_history":0.2742616034,
        "MMLU_human_aging":0.3004484305,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2975206612,
        "MMLU_jurisprudence":0.287037037,
        "MMLU_logical_fallacies":0.2515337423,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.2427184466,
        "MMLU_marketing":0.2863247863,
        "MMLU_medical_genetics":0.25,
        "MMLU_miscellaneous":0.2426564496,
        "MMLU_moral_disputes":0.2774566474,
        "MMLU_moral_scenarios":0.2413407821,
        "MMLU_nutrition":0.2516339869,
        "MMLU_philosophy":0.270096463,
        "MMLU_prehistory":0.2345679012,
        "MMLU_professional_accounting":0.2624113475,
        "MMLU_professional_law":0.2718383312,
        "MMLU_professional_medicine":0.2205882353,
        "MMLU_professional_psychology":0.2532679739,
        "MMLU_public_relations":0.3545454545,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.2338308458,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.3313253012,
        "MMLU_world_religions":0.2923976608
    },
    {
        "Model":"h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt-v2",
        "URL":"https:\/\/huggingface.co\/h2oai\/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt-v2",
        "full_model_name":"h2oai\/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt-v2",
        "Parameters":7.0,
        "MMLU_average":0.2501465983,
        "arc:challenge|25":0.3370307167,
        "hellaswag|10":0.4679346744,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.2148148148,
        "MMLU_astronomy":0.1842105263,
        "MMLU_business_ethics":0.33,
        "MMLU_clinical_knowledge":0.2452830189,
        "MMLU_college_biology":0.2361111111,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.23,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.2023121387,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.26,
        "MMLU_conceptual_physics":0.3276595745,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.1862068966,
        "MMLU_elementary_mathematics":0.2619047619,
        "MMLU_formal_logic":0.2222222222,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.2193548387,
        "MMLU_high_school_chemistry":0.2660098522,
        "MMLU_high_school_computer_science":0.24,
        "MMLU_high_school_european_history":0.1757575758,
        "MMLU_high_school_geography":0.2121212121,
        "MMLU_high_school_government_and_politics":0.1968911917,
        "MMLU_high_school_macroeconomics":0.2128205128,
        "MMLU_high_school_mathematics":0.2259259259,
        "MMLU_high_school_microeconomics":0.218487395,
        "MMLU_high_school_physics":0.2317880795,
        "MMLU_high_school_psychology":0.2165137615,
        "MMLU_high_school_statistics":0.1666666667,
        "MMLU_high_school_us_history":0.2794117647,
        "MMLU_high_school_world_history":0.2489451477,
        "MMLU_human_aging":0.3632286996,
        "MMLU_human_sexuality":0.2442748092,
        "MMLU_international_law":0.3305785124,
        "MMLU_jurisprudence":0.287037037,
        "MMLU_logical_fallacies":0.2024539877,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.2330097087,
        "MMLU_marketing":0.2863247863,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.251596424,
        "MMLU_moral_disputes":0.2514450867,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.2450980392,
        "MMLU_philosophy":0.2765273312,
        "MMLU_prehistory":0.287037037,
        "MMLU_professional_accounting":0.2375886525,
        "MMLU_professional_law":0.2483702738,
        "MMLU_professional_medicine":0.2169117647,
        "MMLU_professional_psychology":0.2679738562,
        "MMLU_public_relations":0.3181818182,
        "MMLU_security_studies":0.1836734694,
        "MMLU_sociology":0.2537313433,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.313253012,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"stablelm-7b-sft-v7-epoch-3",
        "URL":"https:\/\/huggingface.co\/OpenAssistant\/stablelm-7b-sft-v7-epoch-3",
        "full_model_name":"OpenAssistant\/stablelm-7b-sft-v7-epoch-3",
        "Parameters":7.0,
        "MMLU_average":0.2500614413,
        "arc:challenge|25":0.3216723549,
        "hellaswag|10":0.4294961163,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.2296296296,
        "MMLU_astronomy":0.2697368421,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.2452830189,
        "MMLU_college_biology":0.1736111111,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.1965317919,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.32,
        "MMLU_conceptual_physics":0.2765957447,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.3172413793,
        "MMLU_elementary_mathematics":0.2804232804,
        "MMLU_formal_logic":0.2698412698,
        "MMLU_global_facts":0.27,
        "MMLU_high_school_biology":0.2419354839,
        "MMLU_high_school_chemistry":0.2413793103,
        "MMLU_high_school_computer_science":0.15,
        "MMLU_high_school_european_history":0.2727272727,
        "MMLU_high_school_geography":0.2323232323,
        "MMLU_high_school_government_and_politics":0.2176165803,
        "MMLU_high_school_macroeconomics":0.258974359,
        "MMLU_high_school_mathematics":0.237037037,
        "MMLU_high_school_microeconomics":0.2394957983,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.2183486239,
        "MMLU_high_school_statistics":0.2916666667,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.2827004219,
        "MMLU_human_aging":0.2556053812,
        "MMLU_human_sexuality":0.2366412214,
        "MMLU_international_law":0.2479338843,
        "MMLU_jurisprudence":0.3148148148,
        "MMLU_logical_fallacies":0.245398773,
        "MMLU_machine_learning":0.2321428571,
        "MMLU_management":0.2912621359,
        "MMLU_marketing":0.2264957265,
        "MMLU_medical_genetics":0.39,
        "MMLU_miscellaneous":0.2630906769,
        "MMLU_moral_disputes":0.2543352601,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2810457516,
        "MMLU_philosophy":0.2508038585,
        "MMLU_prehistory":0.2592592593,
        "MMLU_professional_accounting":0.2127659574,
        "MMLU_professional_law":0.2451108214,
        "MMLU_professional_medicine":0.2205882353,
        "MMLU_professional_psychology":0.2434640523,
        "MMLU_public_relations":0.2818181818,
        "MMLU_security_studies":0.1836734694,
        "MMLU_sociology":0.1940298507,
        "MMLU_us_foreign_policy":0.25,
        "MMLU_virology":0.2530120482,
        "MMLU_world_religions":0.2807017544
    },
    {
        "Model":"gpt-neox-20b",
        "URL":"https:\/\/huggingface.co\/EleutherAI\/gpt-neox-20b",
        "full_model_name":"EleutherAI\/gpt-neox-20b",
        "Parameters":20.0,
        "MMLU_average":0.250025953,
        "arc:challenge|25":0.4266211604,
        "hellaswag|10":0.5416251743,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.237037037,
        "MMLU_astronomy":0.2302631579,
        "MMLU_business_ethics":0.33,
        "MMLU_clinical_knowledge":0.2226415094,
        "MMLU_college_biology":0.25,
        "MMLU_college_chemistry":0.22,
        "MMLU_college_computer_science":0.32,
        "MMLU_college_mathematics":0.22,
        "MMLU_college_medicine":0.2196531792,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.22,
        "MMLU_conceptual_physics":0.3617021277,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.2068965517,
        "MMLU_elementary_mathematics":0.2592592593,
        "MMLU_formal_logic":0.2301587302,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.2064516129,
        "MMLU_high_school_chemistry":0.1477832512,
        "MMLU_high_school_computer_science":0.27,
        "MMLU_high_school_european_history":0.2121212121,
        "MMLU_high_school_geography":0.1868686869,
        "MMLU_high_school_government_and_politics":0.2279792746,
        "MMLU_high_school_macroeconomics":0.2,
        "MMLU_high_school_mathematics":0.2296296296,
        "MMLU_high_school_microeconomics":0.218487395,
        "MMLU_high_school_physics":0.2450331126,
        "MMLU_high_school_psychology":0.2146788991,
        "MMLU_high_school_statistics":0.1759259259,
        "MMLU_high_school_us_history":0.2941176471,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.3497757848,
        "MMLU_human_sexuality":0.2519083969,
        "MMLU_international_law":0.2727272727,
        "MMLU_jurisprudence":0.2777777778,
        "MMLU_logical_fallacies":0.2392638037,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.2427184466,
        "MMLU_marketing":0.2863247863,
        "MMLU_medical_genetics":0.31,
        "MMLU_miscellaneous":0.2656449553,
        "MMLU_moral_disputes":0.2630057803,
        "MMLU_moral_scenarios":0.2446927374,
        "MMLU_nutrition":0.2745098039,
        "MMLU_philosophy":0.2765273312,
        "MMLU_prehistory":0.2592592593,
        "MMLU_professional_accounting":0.2588652482,
        "MMLU_professional_law":0.2385919166,
        "MMLU_professional_medicine":0.1911764706,
        "MMLU_professional_psychology":0.2598039216,
        "MMLU_public_relations":0.2545454545,
        "MMLU_security_studies":0.2040816327,
        "MMLU_sociology":0.2885572139,
        "MMLU_us_foreign_policy":0.22,
        "MMLU_virology":0.3072289157,
        "MMLU_world_religions":0.3333333333
    },
    {
        "Model":"TinyLlama-1.1B-intermediate-step-480k-1T",
        "URL":"https:\/\/huggingface.co\/PY007\/TinyLlama-1.1B-intermediate-step-480k-1T",
        "full_model_name":"PY007\/TinyLlama-1.1B-intermediate-step-480k-1T",
        "Parameters":1.1,
        "MMLU_average":0.2499546906,
        "arc:challenge|25":0.2636518771,
        "hellaswag|10":0.4008165704,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.3111111111,
        "MMLU_astronomy":0.2236842105,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.2452830189,
        "MMLU_college_biology":0.2291666667,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.25,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.32,
        "MMLU_conceptual_physics":0.2553191489,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2068965517,
        "MMLU_elementary_mathematics":0.2063492063,
        "MMLU_formal_logic":0.1825396825,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.2290322581,
        "MMLU_high_school_chemistry":0.2463054187,
        "MMLU_high_school_computer_science":0.31,
        "MMLU_high_school_european_history":0.2787878788,
        "MMLU_high_school_geography":0.196969697,
        "MMLU_high_school_government_and_politics":0.2331606218,
        "MMLU_high_school_macroeconomics":0.2102564103,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.218487395,
        "MMLU_high_school_physics":0.3046357616,
        "MMLU_high_school_psychology":0.2165137615,
        "MMLU_high_school_statistics":0.2222222222,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3049327354,
        "MMLU_human_sexuality":0.2748091603,
        "MMLU_international_law":0.2561983471,
        "MMLU_jurisprudence":0.3148148148,
        "MMLU_logical_fallacies":0.2883435583,
        "MMLU_machine_learning":0.2946428571,
        "MMLU_management":0.1650485437,
        "MMLU_marketing":0.2863247863,
        "MMLU_medical_genetics":0.22,
        "MMLU_miscellaneous":0.2528735632,
        "MMLU_moral_disputes":0.2716763006,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.2385620915,
        "MMLU_philosophy":0.3022508039,
        "MMLU_prehistory":0.2685185185,
        "MMLU_professional_accounting":0.2375886525,
        "MMLU_professional_law":0.2385919166,
        "MMLU_professional_medicine":0.1985294118,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2545454545,
        "MMLU_security_studies":0.2081632653,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.25,
        "MMLU_virology":0.265060241,
        "MMLU_world_religions":0.3099415205
    },
    {
        "Model":"TinyStories-3M",
        "URL":"https:\/\/huggingface.co\/roneneldan\/TinyStories-3M",
        "full_model_name":"roneneldan\/TinyStories-3M",
        "Parameters":0.003,
        "MMLU_average":0.249900435,
        "arc:challenge|25":0.1919795222,
        "hellaswag|10":0.2575184226,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.3555555556,
        "MMLU_astronomy":0.2105263158,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.2641509434,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.16,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.2312138728,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.26,
        "MMLU_conceptual_physics":0.314893617,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.324137931,
        "MMLU_elementary_mathematics":0.2592592593,
        "MMLU_formal_logic":0.2142857143,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.264516129,
        "MMLU_high_school_chemistry":0.2561576355,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.2121212121,
        "MMLU_high_school_government_and_politics":0.1968911917,
        "MMLU_high_school_macroeconomics":0.2205128205,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.2268907563,
        "MMLU_high_school_physics":0.2119205298,
        "MMLU_high_school_psychology":0.2183486239,
        "MMLU_high_school_statistics":0.1527777778,
        "MMLU_high_school_us_history":0.2745098039,
        "MMLU_high_school_world_history":0.2405063291,
        "MMLU_human_aging":0.3183856502,
        "MMLU_human_sexuality":0.2519083969,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.212962963,
        "MMLU_logical_fallacies":0.2392638037,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.2815533981,
        "MMLU_marketing":0.2777777778,
        "MMLU_medical_genetics":0.27,
        "MMLU_miscellaneous":0.2784163474,
        "MMLU_moral_disputes":0.225433526,
        "MMLU_moral_scenarios":0.2324022346,
        "MMLU_nutrition":0.2124183007,
        "MMLU_philosophy":0.2572347267,
        "MMLU_prehistory":0.2469135802,
        "MMLU_professional_accounting":0.2978723404,
        "MMLU_professional_law":0.2366362451,
        "MMLU_professional_medicine":0.2022058824,
        "MMLU_professional_psychology":0.2532679739,
        "MMLU_public_relations":0.3363636364,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.2189054726,
        "MMLU_us_foreign_policy":0.22,
        "MMLU_virology":0.2710843373,
        "MMLU_world_religions":0.2105263158
    },
    {
        "Model":"pythia-31m-simplepile-lite-2048-scratch-2e",
        "URL":"https:\/\/huggingface.co\/pszemraj\/pythia-31m-simplepile-lite-2048-scratch-2e",
        "full_model_name":"pszemraj\/pythia-31m-simplepile-lite-2048-scratch-2e",
        "Parameters":0.031,
        "MMLU_average":0.2498655754,
        "arc:challenge|25":0.183447099,
        "hellaswag|10":0.2564230233,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.2148148148,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.2188679245,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.22,
        "MMLU_college_computer_science":0.29,
        "MMLU_college_mathematics":0.33,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.18,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2137931034,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.1507936508,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.3161290323,
        "MMLU_high_school_chemistry":0.2955665025,
        "MMLU_high_school_computer_science":0.23,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.2222222222,
        "MMLU_high_school_government_and_politics":0.3212435233,
        "MMLU_high_school_macroeconomics":0.241025641,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.3487394958,
        "MMLU_high_school_physics":0.3311258278,
        "MMLU_high_school_psychology":0.2422018349,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2352941176,
        "MMLU_high_school_world_history":0.2616033755,
        "MMLU_human_aging":0.3273542601,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2809917355,
        "MMLU_jurisprudence":0.212962963,
        "MMLU_logical_fallacies":0.2331288344,
        "MMLU_machine_learning":0.2321428571,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.1965811966,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2554278416,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2516339869,
        "MMLU_philosophy":0.2090032154,
        "MMLU_prehistory":0.2067901235,
        "MMLU_professional_accounting":0.2163120567,
        "MMLU_professional_law":0.2464146023,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2189542484,
        "MMLU_public_relations":0.2272727273,
        "MMLU_security_studies":0.2244897959,
        "MMLU_sociology":0.1990049751,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.313253012,
        "MMLU_world_religions":0.2105263158
    },
    {
        "Model":"rwkv-4-7b-pile",
        "URL":"https:\/\/huggingface.co\/RWKV\/rwkv-4-7b-pile",
        "full_model_name":"RWKV\/rwkv-4-7b-pile",
        "Parameters":7.0,
        "MMLU_average":0.2495936335,
        "arc:challenge|25":0.3540955631,
        "hellaswag|10":0.488946425,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.1851851852,
        "MMLU_astronomy":0.2302631579,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2188679245,
        "MMLU_college_biology":0.2430555556,
        "MMLU_college_chemistry":0.23,
        "MMLU_college_computer_science":0.25,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2369942197,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2468085106,
        "MMLU_econometrics":0.2192982456,
        "MMLU_electrical_engineering":0.2482758621,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.34,
        "MMLU_high_school_biology":0.2322580645,
        "MMLU_high_school_chemistry":0.2512315271,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2363636364,
        "MMLU_high_school_geography":0.1717171717,
        "MMLU_high_school_government_and_politics":0.2797927461,
        "MMLU_high_school_macroeconomics":0.2128205128,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.2521008403,
        "MMLU_high_school_physics":0.2119205298,
        "MMLU_high_school_psychology":0.2018348624,
        "MMLU_high_school_statistics":0.1759259259,
        "MMLU_high_school_us_history":0.2647058824,
        "MMLU_high_school_world_history":0.2362869198,
        "MMLU_human_aging":0.264573991,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2644628099,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.2515337423,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.2038834951,
        "MMLU_marketing":0.2393162393,
        "MMLU_medical_genetics":0.37,
        "MMLU_miscellaneous":0.2464878672,
        "MMLU_moral_disputes":0.2861271676,
        "MMLU_moral_scenarios":0.225698324,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.2218649518,
        "MMLU_prehistory":0.2561728395,
        "MMLU_professional_accounting":0.2553191489,
        "MMLU_professional_law":0.2750977836,
        "MMLU_professional_medicine":0.2904411765,
        "MMLU_professional_psychology":0.2696078431,
        "MMLU_public_relations":0.2363636364,
        "MMLU_security_studies":0.2857142857,
        "MMLU_sociology":0.1890547264,
        "MMLU_us_foreign_policy":0.32,
        "MMLU_virology":0.3012048193,
        "MMLU_world_religions":0.2865497076
    },
    {
        "Model":"pythia-160m",
        "URL":"https:\/\/huggingface.co\/EleutherAI\/pythia-160m",
        "full_model_name":"EleutherAI\/pythia-160m",
        "Parameters":0.16,
        "MMLU_average":0.2495059339,
        "arc:challenge|25":0.1902730375,
        "hellaswag|10":0.2874925314,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.2518518519,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.31,
        "MMLU_clinical_knowledge":0.2037735849,
        "MMLU_college_biology":0.2430555556,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.3,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.2369942197,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.18,
        "MMLU_conceptual_physics":0.2680851064,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2482758621,
        "MMLU_elementary_mathematics":0.2513227513,
        "MMLU_formal_logic":0.1984126984,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.3129032258,
        "MMLU_high_school_chemistry":0.2709359606,
        "MMLU_high_school_computer_science":0.2,
        "MMLU_high_school_european_history":0.2484848485,
        "MMLU_high_school_geography":0.2323232323,
        "MMLU_high_school_government_and_politics":0.2435233161,
        "MMLU_high_school_macroeconomics":0.2461538462,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.2731092437,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.223853211,
        "MMLU_high_school_statistics":0.4768518519,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.2953586498,
        "MMLU_human_aging":0.3094170404,
        "MMLU_human_sexuality":0.2366412214,
        "MMLU_international_law":0.2231404959,
        "MMLU_jurisprudence":0.2777777778,
        "MMLU_logical_fallacies":0.2085889571,
        "MMLU_machine_learning":0.1875,
        "MMLU_management":0.1941747573,
        "MMLU_marketing":0.1837606838,
        "MMLU_medical_genetics":0.32,
        "MMLU_miscellaneous":0.2528735632,
        "MMLU_moral_disputes":0.2630057803,
        "MMLU_moral_scenarios":0.2770949721,
        "MMLU_nutrition":0.2352941176,
        "MMLU_philosophy":0.1897106109,
        "MMLU_prehistory":0.2592592593,
        "MMLU_professional_accounting":0.280141844,
        "MMLU_professional_law":0.2372881356,
        "MMLU_professional_medicine":0.4485294118,
        "MMLU_professional_psychology":0.2516339869,
        "MMLU_public_relations":0.2545454545,
        "MMLU_security_studies":0.2448979592,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.1807228916,
        "MMLU_world_religions":0.2046783626
    },
    {
        "Model":"Open-LLongMA-3b",
        "URL":"https:\/\/huggingface.co\/conceptofmind\/Open-LLongMA-3b",
        "full_model_name":"conceptofmind\/Open-LLongMA-3b",
        "Parameters":3.0,
        "MMLU_average":0.2494629262,
        "arc:challenge|25":0.3805460751,
        "hellaswag|10":0.4915355507,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.2222222222,
        "MMLU_astronomy":0.1973684211,
        "MMLU_business_ethics":0.27,
        "MMLU_clinical_knowledge":0.2754716981,
        "MMLU_college_biology":0.2013888889,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.19,
        "MMLU_college_mathematics":0.22,
        "MMLU_college_medicine":0.2023121387,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.27,
        "MMLU_conceptual_physics":0.3276595745,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.2206896552,
        "MMLU_elementary_mathematics":0.2645502646,
        "MMLU_formal_logic":0.1904761905,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2580645161,
        "MMLU_high_school_chemistry":0.2709359606,
        "MMLU_high_school_computer_science":0.23,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.2272727273,
        "MMLU_high_school_government_and_politics":0.2176165803,
        "MMLU_high_school_macroeconomics":0.2256410256,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.231092437,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.2,
        "MMLU_high_school_statistics":0.1990740741,
        "MMLU_high_school_us_history":0.2303921569,
        "MMLU_high_school_world_history":0.2784810127,
        "MMLU_human_aging":0.3721973094,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.2231404959,
        "MMLU_jurisprudence":0.2962962963,
        "MMLU_logical_fallacies":0.2024539877,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.2524271845,
        "MMLU_marketing":0.2777777778,
        "MMLU_medical_genetics":0.25,
        "MMLU_miscellaneous":0.2899106003,
        "MMLU_moral_disputes":0.2572254335,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2450980392,
        "MMLU_philosophy":0.2797427653,
        "MMLU_prehistory":0.2685185185,
        "MMLU_professional_accounting":0.2730496454,
        "MMLU_professional_law":0.239243807,
        "MMLU_professional_medicine":0.25,
        "MMLU_professional_psychology":0.2450980392,
        "MMLU_public_relations":0.3363636364,
        "MMLU_security_studies":0.1959183673,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.23,
        "MMLU_virology":0.3192771084,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"GPlatty-30B-SuperHOT-8K-fp16",
        "URL":"https:\/\/huggingface.co\/TheBloke\/GPlatty-30B-SuperHOT-8K-fp16",
        "full_model_name":"TheBloke\/GPlatty-30B-SuperHOT-8K-fp16",
        "Parameters":30.0,
        "MMLU_average":0.2491954008,
        "arc:challenge|25":0.2269624573,
        "hellaswag|10":0.2845050787,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.2518518519,
        "MMLU_astronomy":0.1973684211,
        "MMLU_business_ethics":0.31,
        "MMLU_clinical_knowledge":0.2264150943,
        "MMLU_college_biology":0.2361111111,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.29,
        "MMLU_college_mathematics":0.19,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.31,
        "MMLU_conceptual_physics":0.285106383,
        "MMLU_econometrics":0.2105263158,
        "MMLU_electrical_engineering":0.275862069,
        "MMLU_elementary_mathematics":0.2142857143,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.264516129,
        "MMLU_high_school_chemistry":0.2216748768,
        "MMLU_high_school_computer_science":0.29,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.2171717172,
        "MMLU_high_school_government_and_politics":0.1813471503,
        "MMLU_high_school_macroeconomics":0.2717948718,
        "MMLU_high_school_mathematics":0.2037037037,
        "MMLU_high_school_microeconomics":0.2226890756,
        "MMLU_high_school_physics":0.1920529801,
        "MMLU_high_school_psychology":0.2256880734,
        "MMLU_high_school_statistics":0.2685185185,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.2869198312,
        "MMLU_human_aging":0.2914798206,
        "MMLU_human_sexuality":0.2748091603,
        "MMLU_international_law":0.2727272727,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.2699386503,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2863247863,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.30651341,
        "MMLU_moral_disputes":0.2312138728,
        "MMLU_moral_scenarios":0.2458100559,
        "MMLU_nutrition":0.3104575163,
        "MMLU_philosophy":0.270096463,
        "MMLU_prehistory":0.2345679012,
        "MMLU_professional_accounting":0.2517730496,
        "MMLU_professional_law":0.2659713168,
        "MMLU_professional_medicine":0.1764705882,
        "MMLU_professional_psychology":0.2745098039,
        "MMLU_public_relations":0.2,
        "MMLU_security_studies":0.2081632653,
        "MMLU_sociology":0.2686567164,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.2710843373,
        "MMLU_world_religions":0.2923976608
    },
    {
        "Model":"h2ogpt-oig-oasst1-512-6_9b",
        "URL":"https:\/\/huggingface.co\/h2oai\/h2ogpt-oig-oasst1-512-6_9b",
        "full_model_name":"h2oai\/h2ogpt-oig-oasst1-512-6_9b",
        "Parameters":9.0,
        "MMLU_average":0.2490209663,
        "arc:challenge|25":0.3728668942,
        "hellaswag|10":0.4839673372,
        "MMLU_abstract_algebra":0.21,
        "MMLU_anatomy":0.3555555556,
        "MMLU_astronomy":0.2171052632,
        "MMLU_business_ethics":0.2,
        "MMLU_clinical_knowledge":0.2716981132,
        "MMLU_college_biology":0.2361111111,
        "MMLU_college_chemistry":0.17,
        "MMLU_college_computer_science":0.3,
        "MMLU_college_mathematics":0.22,
        "MMLU_college_medicine":0.1676300578,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.21,
        "MMLU_conceptual_physics":0.3063829787,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2827586207,
        "MMLU_elementary_mathematics":0.2407407407,
        "MMLU_formal_logic":0.2142857143,
        "MMLU_global_facts":0.19,
        "MMLU_high_school_biology":0.2580645161,
        "MMLU_high_school_chemistry":0.197044335,
        "MMLU_high_school_computer_science":0.28,
        "MMLU_high_school_european_history":0.2303030303,
        "MMLU_high_school_geography":0.2171717172,
        "MMLU_high_school_government_and_politics":0.2331606218,
        "MMLU_high_school_macroeconomics":0.2076923077,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.2478991597,
        "MMLU_high_school_physics":0.2913907285,
        "MMLU_high_school_psychology":0.2165137615,
        "MMLU_high_school_statistics":0.3564814815,
        "MMLU_high_school_us_history":0.318627451,
        "MMLU_high_school_world_history":0.2320675105,
        "MMLU_human_aging":0.2825112108,
        "MMLU_human_sexuality":0.1755725191,
        "MMLU_international_law":0.3305785124,
        "MMLU_jurisprudence":0.2777777778,
        "MMLU_logical_fallacies":0.2699386503,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.2718446602,
        "MMLU_marketing":0.2222222222,
        "MMLU_medical_genetics":0.22,
        "MMLU_miscellaneous":0.2733077905,
        "MMLU_moral_disputes":0.2687861272,
        "MMLU_moral_scenarios":0.2636871508,
        "MMLU_nutrition":0.2614379085,
        "MMLU_philosophy":0.2475884244,
        "MMLU_prehistory":0.2345679012,
        "MMLU_professional_accounting":0.2482269504,
        "MMLU_professional_law":0.2398956975,
        "MMLU_professional_medicine":0.2279411765,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.3545454545,
        "MMLU_security_studies":0.2367346939,
        "MMLU_sociology":0.2388059701,
        "MMLU_us_foreign_policy":0.21,
        "MMLU_virology":0.2409638554,
        "MMLU_world_religions":0.298245614
    },
    {
        "Model":"OPT-13B-Nerybus-Mix",
        "URL":"https:\/\/huggingface.co\/KoboldAI\/OPT-13B-Nerybus-Mix",
        "full_model_name":"KoboldAI\/OPT-13B-Nerybus-Mix",
        "Parameters":13.0,
        "MMLU_average":0.2490068738,
        "arc:challenge|25":0.3660409556,
        "hellaswag|10":0.5227046405,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.2444444444,
        "MMLU_astronomy":0.1842105263,
        "MMLU_business_ethics":0.17,
        "MMLU_clinical_knowledge":0.2377358491,
        "MMLU_college_biology":0.2291666667,
        "MMLU_college_chemistry":0.22,
        "MMLU_college_computer_science":0.23,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.1907514451,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.25,
        "MMLU_conceptual_physics":0.2595744681,
        "MMLU_econometrics":0.2105263158,
        "MMLU_electrical_engineering":0.3172413793,
        "MMLU_elementary_mathematics":0.2830687831,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.2548387097,
        "MMLU_high_school_chemistry":0.2610837438,
        "MMLU_high_school_computer_science":0.27,
        "MMLU_high_school_european_history":0.2606060606,
        "MMLU_high_school_geography":0.2323232323,
        "MMLU_high_school_government_and_politics":0.207253886,
        "MMLU_high_school_macroeconomics":0.241025641,
        "MMLU_high_school_mathematics":0.2222222222,
        "MMLU_high_school_microeconomics":0.218487395,
        "MMLU_high_school_physics":0.2251655629,
        "MMLU_high_school_psychology":0.2330275229,
        "MMLU_high_school_statistics":0.2083333333,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.2556053812,
        "MMLU_human_sexuality":0.2366412214,
        "MMLU_international_law":0.2314049587,
        "MMLU_jurisprudence":0.25,
        "MMLU_logical_fallacies":0.282208589,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.2524271845,
        "MMLU_marketing":0.2692307692,
        "MMLU_medical_genetics":0.32,
        "MMLU_miscellaneous":0.2911877395,
        "MMLU_moral_disputes":0.2716763006,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2516339869,
        "MMLU_philosophy":0.270096463,
        "MMLU_prehistory":0.2685185185,
        "MMLU_professional_accounting":0.2553191489,
        "MMLU_professional_law":0.2490221643,
        "MMLU_professional_medicine":0.2132352941,
        "MMLU_professional_psychology":0.2483660131,
        "MMLU_public_relations":0.3,
        "MMLU_security_studies":0.1755102041,
        "MMLU_sociology":0.2537313433,
        "MMLU_us_foreign_policy":0.25,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.3450292398
    },
    {
        "Model":"rugpt3large_based_on_gpt2",
        "URL":"https:\/\/huggingface.co\/ai-forever\/rugpt3large_based_on_gpt2",
        "full_model_name":"ai-forever\/rugpt3large_based_on_gpt2",
        "Parameters":null,
        "MMLU_average":0.2489738111,
        "arc:challenge|25":0.1970989761,
        "hellaswag|10":0.2951603266,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.2518518519,
        "MMLU_astronomy":0.1710526316,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.2264150943,
        "MMLU_college_biology":0.2152777778,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.15,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.225433526,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.18,
        "MMLU_conceptual_physics":0.3234042553,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2344827586,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.1507936508,
        "MMLU_global_facts":0.29,
        "MMLU_high_school_biology":0.2935483871,
        "MMLU_high_school_chemistry":0.3103448276,
        "MMLU_high_school_computer_science":0.21,
        "MMLU_high_school_european_history":0.2121212121,
        "MMLU_high_school_geography":0.2171717172,
        "MMLU_high_school_government_and_politics":0.274611399,
        "MMLU_high_school_macroeconomics":0.2153846154,
        "MMLU_high_school_mathematics":0.2888888889,
        "MMLU_high_school_microeconomics":0.231092437,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.2697247706,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2303921569,
        "MMLU_high_school_world_history":0.2573839662,
        "MMLU_human_aging":0.3766816143,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.2479338843,
        "MMLU_jurisprudence":0.1944444444,
        "MMLU_logical_fallacies":0.2515337423,
        "MMLU_machine_learning":0.2410714286,
        "MMLU_management":0.2038834951,
        "MMLU_marketing":0.1923076923,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2362707535,
        "MMLU_moral_disputes":0.2225433526,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2287581699,
        "MMLU_philosophy":0.2475884244,
        "MMLU_prehistory":0.2777777778,
        "MMLU_professional_accounting":0.2553191489,
        "MMLU_professional_law":0.2398956975,
        "MMLU_professional_medicine":0.4522058824,
        "MMLU_professional_psychology":0.2549019608,
        "MMLU_public_relations":0.2636363636,
        "MMLU_security_studies":0.2897959184,
        "MMLU_sociology":0.2388059701,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.234939759,
        "MMLU_world_religions":0.2105263158
    },
    {
        "Model":"GodziLLa-30B-instruct",
        "URL":"https:\/\/huggingface.co\/MayaPH\/GodziLLa-30B-instruct",
        "full_model_name":"MayaPH\/GodziLLa-30B-instruct",
        "Parameters":30.0,
        "MMLU_average":0.2489545254,
        "arc:challenge|25":0.2337883959,
        "hellaswag|10":0.2566221868,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.2814814815,
        "MMLU_astronomy":0.2039473684,
        "MMLU_business_ethics":0.34,
        "MMLU_clinical_knowledge":0.2867924528,
        "MMLU_college_biology":0.1527777778,
        "MMLU_college_chemistry":0.15,
        "MMLU_college_computer_science":0.24,
        "MMLU_college_mathematics":0.17,
        "MMLU_college_medicine":0.2543352601,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.27,
        "MMLU_conceptual_physics":0.2553191489,
        "MMLU_econometrics":0.3333333333,
        "MMLU_electrical_engineering":0.2,
        "MMLU_elementary_mathematics":0.2380952381,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2451612903,
        "MMLU_high_school_chemistry":0.2906403941,
        "MMLU_high_school_computer_science":0.22,
        "MMLU_high_school_european_history":0.2606060606,
        "MMLU_high_school_geography":0.202020202,
        "MMLU_high_school_government_and_politics":0.2538860104,
        "MMLU_high_school_macroeconomics":0.2282051282,
        "MMLU_high_school_mathematics":0.3,
        "MMLU_high_school_microeconomics":0.2352941176,
        "MMLU_high_school_physics":0.2251655629,
        "MMLU_high_school_psychology":0.280733945,
        "MMLU_high_school_statistics":0.2083333333,
        "MMLU_high_school_us_history":0.2156862745,
        "MMLU_high_school_world_history":0.1983122363,
        "MMLU_human_aging":0.3408071749,
        "MMLU_human_sexuality":0.1297709924,
        "MMLU_international_law":0.3719008264,
        "MMLU_jurisprudence":0.287037037,
        "MMLU_logical_fallacies":0.1779141104,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.2427184466,
        "MMLU_marketing":0.2735042735,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.2733077905,
        "MMLU_moral_disputes":0.2167630058,
        "MMLU_moral_scenarios":0.2413407821,
        "MMLU_nutrition":0.2222222222,
        "MMLU_philosophy":0.2636655949,
        "MMLU_prehistory":0.2469135802,
        "MMLU_professional_accounting":0.2234042553,
        "MMLU_professional_law":0.2737940026,
        "MMLU_professional_medicine":0.2022058824,
        "MMLU_professional_psychology":0.2794117647,
        "MMLU_public_relations":0.3363636364,
        "MMLU_security_studies":0.2367346939,
        "MMLU_sociology":0.2338308458,
        "MMLU_us_foreign_policy":0.22,
        "MMLU_virology":0.3072289157,
        "MMLU_world_religions":0.216374269
    },
    {
        "Model":"Echidna-30B",
        "URL":"https:\/\/huggingface.co\/jaspercatapang\/Echidna-30B",
        "full_model_name":"jaspercatapang\/Echidna-30B",
        "Parameters":30.0,
        "MMLU_average":0.2488963342,
        "arc:challenge|25":0.2414675768,
        "hellaswag|10":0.2530372436,
        "MMLU_abstract_algebra":0.21,
        "MMLU_anatomy":0.2962962963,
        "MMLU_astronomy":0.2631578947,
        "MMLU_business_ethics":0.26,
        "MMLU_clinical_knowledge":0.2226415094,
        "MMLU_college_biology":0.2777777778,
        "MMLU_college_chemistry":0.34,
        "MMLU_college_computer_science":0.28,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.2601156069,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.26,
        "MMLU_conceptual_physics":0.2425531915,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.2206896552,
        "MMLU_elementary_mathematics":0.2433862434,
        "MMLU_formal_logic":0.2619047619,
        "MMLU_global_facts":0.23,
        "MMLU_high_school_biology":0.2580645161,
        "MMLU_high_school_chemistry":0.2709359606,
        "MMLU_high_school_computer_science":0.14,
        "MMLU_high_school_european_history":0.1878787879,
        "MMLU_high_school_geography":0.1515151515,
        "MMLU_high_school_government_and_politics":0.3419689119,
        "MMLU_high_school_macroeconomics":0.2076923077,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.2226890756,
        "MMLU_high_school_physics":0.2317880795,
        "MMLU_high_school_psychology":0.2311926606,
        "MMLU_high_school_statistics":0.2175925926,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.2573839662,
        "MMLU_human_aging":0.2511210762,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2231404959,
        "MMLU_jurisprudence":0.2777777778,
        "MMLU_logical_fallacies":0.282208589,
        "MMLU_machine_learning":0.2410714286,
        "MMLU_management":0.1650485437,
        "MMLU_marketing":0.2991452991,
        "MMLU_medical_genetics":0.25,
        "MMLU_miscellaneous":0.2311621967,
        "MMLU_moral_disputes":0.2283236994,
        "MMLU_moral_scenarios":0.2491620112,
        "MMLU_nutrition":0.2647058824,
        "MMLU_philosophy":0.2411575563,
        "MMLU_prehistory":0.2561728395,
        "MMLU_professional_accounting":0.2765957447,
        "MMLU_professional_law":0.2366362451,
        "MMLU_professional_medicine":0.3419117647,
        "MMLU_professional_psychology":0.2140522876,
        "MMLU_public_relations":0.1909090909,
        "MMLU_security_studies":0.3102040816,
        "MMLU_sociology":0.223880597,
        "MMLU_us_foreign_policy":0.25,
        "MMLU_virology":0.2409638554,
        "MMLU_world_religions":0.3099415205
    },
    {
        "Model":"Nape-0",
        "URL":"https:\/\/huggingface.co\/nnpy\/Nape-0",
        "full_model_name":"nnpy\/Nape-0",
        "Parameters":null,
        "MMLU_average":0.2487870205,
        "arc:challenge|25":0.3114334471,
        "hellaswag|10":0.4470225055,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.3407407407,
        "MMLU_astronomy":0.2368421053,
        "MMLU_business_ethics":0.2,
        "MMLU_clinical_knowledge":0.2603773585,
        "MMLU_college_biology":0.2152777778,
        "MMLU_college_chemistry":0.18,
        "MMLU_college_computer_science":0.16,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.2312138728,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.24,
        "MMLU_conceptual_physics":0.2510638298,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.2896551724,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.1428571429,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2483870968,
        "MMLU_high_school_chemistry":0.2561576355,
        "MMLU_high_school_computer_science":0.35,
        "MMLU_high_school_european_history":0.296969697,
        "MMLU_high_school_geography":0.2525252525,
        "MMLU_high_school_government_and_politics":0.2227979275,
        "MMLU_high_school_macroeconomics":0.2307692308,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.2142857143,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.2311926606,
        "MMLU_high_school_statistics":0.1944444444,
        "MMLU_high_school_us_history":0.2009803922,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.331838565,
        "MMLU_human_sexuality":0.1984732824,
        "MMLU_international_law":0.2809917355,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2883435583,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.2427184466,
        "MMLU_marketing":0.2307692308,
        "MMLU_medical_genetics":0.22,
        "MMLU_miscellaneous":0.2669220945,
        "MMLU_moral_disputes":0.2658959538,
        "MMLU_moral_scenarios":0.2592178771,
        "MMLU_nutrition":0.2843137255,
        "MMLU_philosophy":0.3022508039,
        "MMLU_prehistory":0.2592592593,
        "MMLU_professional_accounting":0.2340425532,
        "MMLU_professional_law":0.2379400261,
        "MMLU_professional_medicine":0.2279411765,
        "MMLU_professional_psychology":0.2532679739,
        "MMLU_public_relations":0.2545454545,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.2039800995,
        "MMLU_us_foreign_policy":0.25,
        "MMLU_virology":0.313253012,
        "MMLU_world_religions":0.2514619883
    },
    {
        "Model":"GPT-J-6B-Skein",
        "URL":"https:\/\/huggingface.co\/KoboldAI\/GPT-J-6B-Skein",
        "full_model_name":"KoboldAI\/GPT-J-6B-Skein",
        "Parameters":6.0,
        "MMLU_average":0.2487618409,
        "arc:challenge|25":0.385665529,
        "hellaswag|10":0.507767377,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.237037037,
        "MMLU_astronomy":0.2105263158,
        "MMLU_business_ethics":0.27,
        "MMLU_clinical_knowledge":0.2716981132,
        "MMLU_college_biology":0.2291666667,
        "MMLU_college_chemistry":0.22,
        "MMLU_college_computer_science":0.15,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.1907514451,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.23,
        "MMLU_conceptual_physics":0.3191489362,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2137931034,
        "MMLU_elementary_mathematics":0.253968254,
        "MMLU_formal_logic":0.1984126984,
        "MMLU_global_facts":0.28,
        "MMLU_high_school_biology":0.2580645161,
        "MMLU_high_school_chemistry":0.275862069,
        "MMLU_high_school_computer_science":0.24,
        "MMLU_high_school_european_history":0.2606060606,
        "MMLU_high_school_geography":0.2121212121,
        "MMLU_high_school_government_and_politics":0.2124352332,
        "MMLU_high_school_macroeconomics":0.2256410256,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.243697479,
        "MMLU_high_school_physics":0.2119205298,
        "MMLU_high_school_psychology":0.2348623853,
        "MMLU_high_school_statistics":0.162037037,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.2742616034,
        "MMLU_human_aging":0.3632286996,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.2727272727,
        "MMLU_jurisprudence":0.3148148148,
        "MMLU_logical_fallacies":0.2331288344,
        "MMLU_machine_learning":0.2946428571,
        "MMLU_management":0.2233009709,
        "MMLU_marketing":0.2606837607,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.3039591315,
        "MMLU_moral_disputes":0.2427745665,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2679738562,
        "MMLU_philosophy":0.2765273312,
        "MMLU_prehistory":0.2654320988,
        "MMLU_professional_accounting":0.2553191489,
        "MMLU_professional_law":0.259452412,
        "MMLU_professional_medicine":0.2022058824,
        "MMLU_professional_psychology":0.2581699346,
        "MMLU_public_relations":0.3454545455,
        "MMLU_security_studies":0.2285714286,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.23,
        "MMLU_virology":0.3192771084,
        "MMLU_world_religions":0.2397660819
    },
    {
        "Model":"Aira-2-1B1",
        "URL":"https:\/\/huggingface.co\/nicholasKluge\/Aira-2-1B1",
        "full_model_name":"nicholasKluge\/Aira-2-1B1",
        "Parameters":1.0,
        "MMLU_average":0.2486397022,
        "arc:challenge|25":0.20221843,
        "hellaswag|10":0.266381199,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.2814814815,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.22,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.1736111111,
        "MMLU_college_chemistry":0.16,
        "MMLU_college_computer_science":0.17,
        "MMLU_college_mathematics":0.19,
        "MMLU_college_medicine":0.1965317919,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.3,
        "MMLU_conceptual_physics":0.2680851064,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.2830687831,
        "MMLU_formal_logic":0.2142857143,
        "MMLU_global_facts":0.23,
        "MMLU_high_school_biology":0.2387096774,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.2787878788,
        "MMLU_high_school_geography":0.196969697,
        "MMLU_high_school_government_and_politics":0.2331606218,
        "MMLU_high_school_macroeconomics":0.2128205128,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.2268907563,
        "MMLU_high_school_physics":0.2317880795,
        "MMLU_high_school_psychology":0.1908256881,
        "MMLU_high_school_statistics":0.25,
        "MMLU_high_school_us_history":0.2892156863,
        "MMLU_high_school_world_history":0.2573839662,
        "MMLU_human_aging":0.2556053812,
        "MMLU_human_sexuality":0.2442748092,
        "MMLU_international_law":0.2727272727,
        "MMLU_jurisprudence":0.3148148148,
        "MMLU_logical_fallacies":0.282208589,
        "MMLU_machine_learning":0.1964285714,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2991452991,
        "MMLU_medical_genetics":0.31,
        "MMLU_miscellaneous":0.2630906769,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2480446927,
        "MMLU_nutrition":0.2614379085,
        "MMLU_philosophy":0.2540192926,
        "MMLU_prehistory":0.2592592593,
        "MMLU_professional_accounting":0.2978723404,
        "MMLU_professional_law":0.2653194263,
        "MMLU_professional_medicine":0.4191176471,
        "MMLU_professional_psychology":0.2549019608,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.2326530612,
        "MMLU_sociology":0.2537313433,
        "MMLU_us_foreign_policy":0.25,
        "MMLU_virology":0.3012048193,
        "MMLU_world_religions":0.2631578947
    },
    {
        "Model":"pythia-160m-deduped",
        "URL":"https:\/\/huggingface.co\/EleutherAI\/pythia-160m-deduped",
        "full_model_name":"EleutherAI\/pythia-160m-deduped",
        "Parameters":0.16,
        "MMLU_average":0.24860575,
        "arc:challenge|25":0.2056313993,
        "hellaswag|10":0.2864967138,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.2222222222,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.17,
        "MMLU_clinical_knowledge":0.2075471698,
        "MMLU_college_biology":0.2291666667,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.28,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2340425532,
        "MMLU_econometrics":0.2192982456,
        "MMLU_electrical_engineering":0.2827586207,
        "MMLU_elementary_mathematics":0.253968254,
        "MMLU_formal_logic":0.2063492063,
        "MMLU_global_facts":0.16,
        "MMLU_high_school_biology":0.2548387097,
        "MMLU_high_school_chemistry":0.3103448276,
        "MMLU_high_school_computer_science":0.24,
        "MMLU_high_school_european_history":0.2121212121,
        "MMLU_high_school_geography":0.2474747475,
        "MMLU_high_school_government_and_politics":0.2590673575,
        "MMLU_high_school_macroeconomics":0.2153846154,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.2268907563,
        "MMLU_high_school_physics":0.3245033113,
        "MMLU_high_school_psychology":0.2422018349,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2009803922,
        "MMLU_high_school_world_history":0.2784810127,
        "MMLU_human_aging":0.3677130045,
        "MMLU_human_sexuality":0.2671755725,
        "MMLU_international_law":0.305785124,
        "MMLU_jurisprudence":0.212962963,
        "MMLU_logical_fallacies":0.2944785276,
        "MMLU_machine_learning":0.1785714286,
        "MMLU_management":0.1553398058,
        "MMLU_marketing":0.1752136752,
        "MMLU_medical_genetics":0.32,
        "MMLU_miscellaneous":0.2413793103,
        "MMLU_moral_disputes":0.2456647399,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2483660131,
        "MMLU_philosophy":0.2057877814,
        "MMLU_prehistory":0.2407407407,
        "MMLU_professional_accounting":0.2340425532,
        "MMLU_professional_law":0.2372881356,
        "MMLU_professional_medicine":0.4375,
        "MMLU_professional_psychology":0.2532679739,
        "MMLU_public_relations":0.2,
        "MMLU_security_studies":0.306122449,
        "MMLU_sociology":0.2388059701,
        "MMLU_us_foreign_policy":0.29,
        "MMLU_virology":0.2469879518,
        "MMLU_world_religions":0.1988304094
    },
    {
        "Model":"WizardLM-7B-uncensored-GPTQ",
        "URL":"https:\/\/huggingface.co\/TheBloke\/WizardLM-7B-uncensored-GPTQ",
        "full_model_name":"TheBloke\/WizardLM-7B-uncensored-GPTQ",
        "Parameters":7.0,
        "MMLU_average":0.2485184456,
        "arc:challenge|25":0.2312286689,
        "hellaswag|10":0.2552280422,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.1703703704,
        "MMLU_astronomy":0.2368421053,
        "MMLU_business_ethics":0.24,
        "MMLU_clinical_knowledge":0.2754716981,
        "MMLU_college_biology":0.2430555556,
        "MMLU_college_chemistry":0.27,
        "MMLU_college_computer_science":0.23,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.2427745665,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.26,
        "MMLU_conceptual_physics":0.3063829787,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.2513227513,
        "MMLU_formal_logic":0.3015873016,
        "MMLU_global_facts":0.21,
        "MMLU_high_school_biology":0.2451612903,
        "MMLU_high_school_chemistry":0.2019704433,
        "MMLU_high_school_computer_science":0.23,
        "MMLU_high_school_european_history":0.2909090909,
        "MMLU_high_school_geography":0.2121212121,
        "MMLU_high_school_government_and_politics":0.1865284974,
        "MMLU_high_school_macroeconomics":0.2153846154,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.218487395,
        "MMLU_high_school_physics":0.1655629139,
        "MMLU_high_school_psychology":0.2201834862,
        "MMLU_high_school_statistics":0.2962962963,
        "MMLU_high_school_us_history":0.2647058824,
        "MMLU_high_school_world_history":0.2194092827,
        "MMLU_human_aging":0.4035874439,
        "MMLU_human_sexuality":0.2824427481,
        "MMLU_international_law":0.2314049587,
        "MMLU_jurisprudence":0.2962962963,
        "MMLU_logical_fallacies":0.2085889571,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.2815533981,
        "MMLU_marketing":0.2777777778,
        "MMLU_medical_genetics":0.29,
        "MMLU_miscellaneous":0.2771392082,
        "MMLU_moral_disputes":0.2283236994,
        "MMLU_moral_scenarios":0.2670391061,
        "MMLU_nutrition":0.1928104575,
        "MMLU_philosophy":0.2347266881,
        "MMLU_prehistory":0.212962963,
        "MMLU_professional_accounting":0.2446808511,
        "MMLU_professional_law":0.2777053455,
        "MMLU_professional_medicine":0.2316176471,
        "MMLU_professional_psychology":0.2336601307,
        "MMLU_public_relations":0.2545454545,
        "MMLU_security_studies":0.2163265306,
        "MMLU_sociology":0.2338308458,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.2530120482,
        "MMLU_world_religions":0.2807017544
    },
    {
        "Model":"Quokka_2.7b",
        "URL":"https:\/\/huggingface.co\/Corianas\/Quokka_2.7b",
        "full_model_name":"Corianas\/Quokka_2.7b",
        "Parameters":2.7,
        "MMLU_average":0.248026588,
        "arc:challenge|25":0.2815699659,
        "hellaswag|10":0.3811989643,
        "MMLU_abstract_algebra":0.19,
        "MMLU_anatomy":0.2296296296,
        "MMLU_astronomy":0.2236842105,
        "MMLU_business_ethics":0.24,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2638888889,
        "MMLU_college_chemistry":0.18,
        "MMLU_college_computer_science":0.38,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.2023121387,
        "MMLU_college_physics":0.1568627451,
        "MMLU_computer_security":0.3,
        "MMLU_conceptual_physics":0.2765957447,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2275862069,
        "MMLU_elementary_mathematics":0.208994709,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.17,
        "MMLU_high_school_biology":0.2096774194,
        "MMLU_high_school_chemistry":0.2167487685,
        "MMLU_high_school_computer_science":0.34,
        "MMLU_high_school_european_history":0.2242424242,
        "MMLU_high_school_geography":0.1717171717,
        "MMLU_high_school_government_and_politics":0.207253886,
        "MMLU_high_school_macroeconomics":0.2051282051,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.2058823529,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.2256880734,
        "MMLU_high_school_statistics":0.3657407407,
        "MMLU_high_school_us_history":0.2745098039,
        "MMLU_high_school_world_history":0.2489451477,
        "MMLU_human_aging":0.3273542601,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.3006134969,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.1941747573,
        "MMLU_marketing":0.2179487179,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.2490421456,
        "MMLU_moral_disputes":0.2283236994,
        "MMLU_moral_scenarios":0.2625698324,
        "MMLU_nutrition":0.2189542484,
        "MMLU_philosophy":0.2347266881,
        "MMLU_prehistory":0.212962963,
        "MMLU_professional_accounting":0.2553191489,
        "MMLU_professional_law":0.2522816167,
        "MMLU_professional_medicine":0.3639705882,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.1636363636,
        "MMLU_security_studies":0.3306122449,
        "MMLU_sociology":0.223880597,
        "MMLU_us_foreign_policy":0.32,
        "MMLU_virology":0.2891566265,
        "MMLU_world_religions":0.2865497076
    },
    {
        "Model":"OPT-30B-Erebus",
        "URL":"https:\/\/huggingface.co\/KoboldAI\/OPT-30B-Erebus",
        "full_model_name":"KoboldAI\/OPT-30B-Erebus",
        "Parameters":30.0,
        "MMLU_average":0.2479660779,
        "arc:challenge|25":0.3293515358,
        "hellaswag|10":0.4858593906,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.3185185185,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.28,
        "MMLU_clinical_knowledge":0.2301886792,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.19,
        "MMLU_college_computer_science":0.15,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.2601156069,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.285106383,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.2551724138,
        "MMLU_elementary_mathematics":0.2513227513,
        "MMLU_formal_logic":0.3095238095,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.2935483871,
        "MMLU_high_school_chemistry":0.2068965517,
        "MMLU_high_school_computer_science":0.22,
        "MMLU_high_school_european_history":0.2303030303,
        "MMLU_high_school_geography":0.2272727273,
        "MMLU_high_school_government_and_politics":0.2642487047,
        "MMLU_high_school_macroeconomics":0.2102564103,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.2268907563,
        "MMLU_high_school_physics":0.2450331126,
        "MMLU_high_school_psychology":0.2256880734,
        "MMLU_high_school_statistics":0.3194444444,
        "MMLU_high_school_us_history":0.2401960784,
        "MMLU_high_school_world_history":0.2953586498,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2748091603,
        "MMLU_international_law":0.1983471074,
        "MMLU_jurisprudence":0.3055555556,
        "MMLU_logical_fallacies":0.1840490798,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.1844660194,
        "MMLU_marketing":0.2179487179,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.275862069,
        "MMLU_moral_disputes":0.2312138728,
        "MMLU_moral_scenarios":0.2391061453,
        "MMLU_nutrition":0.2777777778,
        "MMLU_philosophy":0.1864951768,
        "MMLU_prehistory":0.2592592593,
        "MMLU_professional_accounting":0.2234042553,
        "MMLU_professional_law":0.2425032595,
        "MMLU_professional_medicine":0.1875,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.3,
        "MMLU_security_studies":0.2489795918,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.24,
        "MMLU_virology":0.2951807229,
        "MMLU_world_religions":0.269005848
    },
    {
        "Model":"PT_GPTNEO350_ATG",
        "URL":"https:\/\/huggingface.co\/xhyi\/PT_GPTNEO350_ATG",
        "full_model_name":"xhyi\/PT_GPTNEO350_ATG",
        "Parameters":null,
        "MMLU_average":0.2479080195,
        "arc:challenge|25":0.2124573379,
        "hellaswag|10":0.3212507469,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.2518518519,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.22,
        "MMLU_clinical_knowledge":0.2,
        "MMLU_college_biology":0.2291666667,
        "MMLU_college_chemistry":0.25,
        "MMLU_college_computer_science":0.33,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.2312138728,
        "MMLU_college_physics":0.1764705882,
        "MMLU_computer_security":0.24,
        "MMLU_conceptual_physics":0.2680851064,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2206896552,
        "MMLU_elementary_mathematics":0.2486772487,
        "MMLU_formal_logic":0.1507936508,
        "MMLU_global_facts":0.21,
        "MMLU_high_school_biology":0.2741935484,
        "MMLU_high_school_chemistry":0.2955665025,
        "MMLU_high_school_computer_science":0.33,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.2323232323,
        "MMLU_high_school_government_and_politics":0.2227979275,
        "MMLU_high_school_macroeconomics":0.2128205128,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.2073394495,
        "MMLU_high_school_statistics":0.4398148148,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.2742616034,
        "MMLU_human_aging":0.3049327354,
        "MMLU_human_sexuality":0.1908396947,
        "MMLU_international_law":0.2314049587,
        "MMLU_jurisprudence":0.2407407407,
        "MMLU_logical_fallacies":0.3128834356,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2136752137,
        "MMLU_medical_genetics":0.29,
        "MMLU_miscellaneous":0.2477650064,
        "MMLU_moral_disputes":0.2803468208,
        "MMLU_moral_scenarios":0.2413407821,
        "MMLU_nutrition":0.2124183007,
        "MMLU_philosophy":0.2636655949,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2907801418,
        "MMLU_professional_law":0.2385919166,
        "MMLU_professional_medicine":0.3382352941,
        "MMLU_professional_psychology":0.2712418301,
        "MMLU_public_relations":0.1545454545,
        "MMLU_security_studies":0.2367346939,
        "MMLU_sociology":0.2189054726,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.2409638554,
        "MMLU_world_religions":0.2456140351
    },
    {
        "Model":"Pythia-70M-ChatSalad",
        "URL":"https:\/\/huggingface.co\/concedo\/Pythia-70M-ChatSalad",
        "full_model_name":"concedo\/Pythia-70M-ChatSalad",
        "Parameters":0.07,
        "MMLU_average":0.2478483817,
        "arc:challenge|25":0.1774744027,
        "hellaswag|10":0.2660824537,
        "MMLU_abstract_algebra":0.2,
        "MMLU_anatomy":0.3333333333,
        "MMLU_astronomy":0.2105263158,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.2113207547,
        "MMLU_college_biology":0.1597222222,
        "MMLU_college_chemistry":0.26,
        "MMLU_college_computer_science":0.27,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.2543352601,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.3,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2344827586,
        "MMLU_elementary_mathematics":0.2592592593,
        "MMLU_formal_logic":0.1984126984,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.3161290323,
        "MMLU_high_school_chemistry":0.3004926108,
        "MMLU_high_school_computer_science":0.3,
        "MMLU_high_school_european_history":0.2484848485,
        "MMLU_high_school_geography":0.2777777778,
        "MMLU_high_school_government_and_politics":0.2435233161,
        "MMLU_high_school_macroeconomics":0.2076923077,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.218487395,
        "MMLU_high_school_physics":0.3377483444,
        "MMLU_high_school_psychology":0.2146788991,
        "MMLU_high_school_statistics":0.2962962963,
        "MMLU_high_school_us_history":0.2745098039,
        "MMLU_high_school_world_history":0.253164557,
        "MMLU_human_aging":0.264573991,
        "MMLU_human_sexuality":0.1984732824,
        "MMLU_international_law":0.3636363636,
        "MMLU_jurisprudence":0.2314814815,
        "MMLU_logical_fallacies":0.3190184049,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.1923076923,
        "MMLU_medical_genetics":0.31,
        "MMLU_miscellaneous":0.2630906769,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2290502793,
        "MMLU_nutrition":0.2418300654,
        "MMLU_philosophy":0.2765273312,
        "MMLU_prehistory":0.2685185185,
        "MMLU_professional_accounting":0.2304964539,
        "MMLU_professional_law":0.2451108214,
        "MMLU_professional_medicine":0.1875,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.1909090909,
        "MMLU_security_studies":0.2448979592,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.23,
        "MMLU_virology":0.2048192771,
        "MMLU_world_religions":0.1637426901
    },
    {
        "Model":"shearedplats-1.3b-v1",
        "URL":"https:\/\/huggingface.co\/vihangd\/shearedplats-1.3b-v1",
        "full_model_name":"vihangd\/shearedplats-1.3b-v1",
        "Parameters":1.3,
        "MMLU_average":0.2474845604,
        "arc:challenge|25":0.3174061433,
        "hellaswag|10":0.4705238,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.2222222222,
        "MMLU_astronomy":0.3223684211,
        "MMLU_business_ethics":0.28,
        "MMLU_clinical_knowledge":0.2528301887,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.15,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.2485549133,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.27,
        "MMLU_conceptual_physics":0.3276595745,
        "MMLU_econometrics":0.2192982456,
        "MMLU_electrical_engineering":0.2344827586,
        "MMLU_elementary_mathematics":0.2592592593,
        "MMLU_formal_logic":0.1587301587,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.2741935484,
        "MMLU_high_school_chemistry":0.2709359606,
        "MMLU_high_school_computer_science":0.22,
        "MMLU_high_school_european_history":0.2606060606,
        "MMLU_high_school_geography":0.2171717172,
        "MMLU_high_school_government_and_politics":0.2124352332,
        "MMLU_high_school_macroeconomics":0.2461538462,
        "MMLU_high_school_mathematics":0.3037037037,
        "MMLU_high_school_microeconomics":0.231092437,
        "MMLU_high_school_physics":0.2251655629,
        "MMLU_high_school_psychology":0.2366972477,
        "MMLU_high_school_statistics":0.3703703704,
        "MMLU_high_school_us_history":0.2107843137,
        "MMLU_high_school_world_history":0.2784810127,
        "MMLU_human_aging":0.33632287,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.3801652893,
        "MMLU_jurisprudence":0.2314814815,
        "MMLU_logical_fallacies":0.226993865,
        "MMLU_machine_learning":0.2321428571,
        "MMLU_management":0.2330097087,
        "MMLU_marketing":0.2307692308,
        "MMLU_medical_genetics":0.22,
        "MMLU_miscellaneous":0.2694763729,
        "MMLU_moral_disputes":0.2167630058,
        "MMLU_moral_scenarios":0.2525139665,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.2443729904,
        "MMLU_prehistory":0.2962962963,
        "MMLU_professional_accounting":0.2730496454,
        "MMLU_professional_law":0.2451108214,
        "MMLU_professional_medicine":0.1948529412,
        "MMLU_professional_psychology":0.2467320261,
        "MMLU_public_relations":0.2363636364,
        "MMLU_security_studies":0.2081632653,
        "MMLU_sociology":0.2288557214,
        "MMLU_us_foreign_policy":0.19,
        "MMLU_virology":0.1987951807,
        "MMLU_world_religions":0.2397660819
    },
    {
        "Model":"PULI-GPTrio",
        "URL":"https:\/\/huggingface.co\/NYTK\/PULI-GPTrio",
        "full_model_name":"NYTK\/PULI-GPTrio",
        "Parameters":null,
        "MMLU_average":0.2472639998,
        "arc:challenge|25":0.2866894198,
        "hellaswag|10":0.4078868751,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.2518518519,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.2301886792,
        "MMLU_college_biology":0.2430555556,
        "MMLU_college_chemistry":0.23,
        "MMLU_college_computer_science":0.25,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.1965317919,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2255319149,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2724867725,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.1870967742,
        "MMLU_high_school_chemistry":0.1822660099,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2787878788,
        "MMLU_high_school_geography":0.202020202,
        "MMLU_high_school_government_and_politics":0.2279792746,
        "MMLU_high_school_macroeconomics":0.2461538462,
        "MMLU_high_school_mathematics":0.2148148148,
        "MMLU_high_school_microeconomics":0.2142857143,
        "MMLU_high_school_physics":0.2251655629,
        "MMLU_high_school_psychology":0.2220183486,
        "MMLU_high_school_statistics":0.3333333333,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.3037974684,
        "MMLU_human_aging":0.3183856502,
        "MMLU_human_sexuality":0.2442748092,
        "MMLU_international_law":0.2644628099,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.3128834356,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.2038834951,
        "MMLU_marketing":0.2606837607,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.2669220945,
        "MMLU_moral_disputes":0.2341040462,
        "MMLU_moral_scenarios":0.2480446927,
        "MMLU_nutrition":0.2352941176,
        "MMLU_philosophy":0.308681672,
        "MMLU_prehistory":0.25,
        "MMLU_professional_accounting":0.2163120567,
        "MMLU_professional_law":0.259452412,
        "MMLU_professional_medicine":0.2463235294,
        "MMLU_professional_psychology":0.2924836601,
        "MMLU_public_relations":0.2818181818,
        "MMLU_security_studies":0.2163265306,
        "MMLU_sociology":0.1890547264,
        "MMLU_us_foreign_policy":0.25,
        "MMLU_virology":0.3072289157,
        "MMLU_world_religions":0.3742690058
    },
    {
        "Model":"gpt2-large-lora-sft2",
        "URL":"https:\/\/huggingface.co\/Mikivis\/gpt2-large-lora-sft2",
        "full_model_name":"Mikivis\/gpt2-large-lora-sft2",
        "Parameters":null,
        "MMLU_average":0.24722226,
        "arc:challenge|25":0.2346416382,
        "hellaswag|10":0.3583947421,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.3185185185,
        "MMLU_astronomy":0.2828947368,
        "MMLU_business_ethics":0.18,
        "MMLU_clinical_knowledge":0.2188679245,
        "MMLU_college_biology":0.2430555556,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.29,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.2369942197,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2510638298,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.2965517241,
        "MMLU_elementary_mathematics":0.246031746,
        "MMLU_formal_logic":0.1746031746,
        "MMLU_global_facts":0.22,
        "MMLU_high_school_biology":0.2483870968,
        "MMLU_high_school_chemistry":0.2955665025,
        "MMLU_high_school_computer_science":0.24,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.2171717172,
        "MMLU_high_school_government_and_politics":0.2383419689,
        "MMLU_high_school_macroeconomics":0.2179487179,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.218487395,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.2311926606,
        "MMLU_high_school_statistics":0.2314814815,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.2062780269,
        "MMLU_human_sexuality":0.2061068702,
        "MMLU_international_law":0.3719008264,
        "MMLU_jurisprudence":0.2314814815,
        "MMLU_logical_fallacies":0.2576687117,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.2718446602,
        "MMLU_marketing":0.235042735,
        "MMLU_medical_genetics":0.21,
        "MMLU_miscellaneous":0.2733077905,
        "MMLU_moral_disputes":0.2919075145,
        "MMLU_moral_scenarios":0.2502793296,
        "MMLU_nutrition":0.2483660131,
        "MMLU_philosophy":0.2990353698,
        "MMLU_prehistory":0.2901234568,
        "MMLU_professional_accounting":0.2765957447,
        "MMLU_professional_law":0.2724902216,
        "MMLU_professional_medicine":0.1654411765,
        "MMLU_professional_psychology":0.2336601307,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.2081632653,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.25,
        "MMLU_virology":0.2168674699,
        "MMLU_world_religions":0.2105263158
    },
    {
        "Model":"WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GPTQ",
        "URL":"https:\/\/huggingface.co\/TheBloke\/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GPTQ",
        "full_model_name":"TheBloke\/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GPTQ",
        "Parameters":30.0,
        "MMLU_average":0.2470841559,
        "arc:challenge|25":0.2278156997,
        "hellaswag|10":0.2582154949,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.2666666667,
        "MMLU_astronomy":0.3026315789,
        "MMLU_business_ethics":0.2,
        "MMLU_clinical_knowledge":0.2188679245,
        "MMLU_college_biology":0.25,
        "MMLU_college_chemistry":0.29,
        "MMLU_college_computer_science":0.27,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.2658959538,
        "MMLU_college_physics":0.1764705882,
        "MMLU_computer_security":0.27,
        "MMLU_conceptual_physics":0.2085106383,
        "MMLU_econometrics":0.1929824561,
        "MMLU_electrical_engineering":0.2275862069,
        "MMLU_elementary_mathematics":0.2301587302,
        "MMLU_formal_logic":0.2777777778,
        "MMLU_global_facts":0.26,
        "MMLU_high_school_biology":0.264516129,
        "MMLU_high_school_chemistry":0.2709359606,
        "MMLU_high_school_computer_science":0.22,
        "MMLU_high_school_european_history":0.2727272727,
        "MMLU_high_school_geography":0.1818181818,
        "MMLU_high_school_government_and_politics":0.3316062176,
        "MMLU_high_school_macroeconomics":0.2076923077,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.2142857143,
        "MMLU_high_school_physics":0.178807947,
        "MMLU_high_school_psychology":0.2110091743,
        "MMLU_high_school_statistics":0.25,
        "MMLU_high_school_us_history":0.2205882353,
        "MMLU_high_school_world_history":0.2194092827,
        "MMLU_human_aging":0.2286995516,
        "MMLU_human_sexuality":0.2748091603,
        "MMLU_international_law":0.2892561983,
        "MMLU_jurisprudence":0.2314814815,
        "MMLU_logical_fallacies":0.2944785276,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.1553398058,
        "MMLU_marketing":0.2777777778,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.245210728,
        "MMLU_moral_disputes":0.2369942197,
        "MMLU_moral_scenarios":0.2659217877,
        "MMLU_nutrition":0.2549019608,
        "MMLU_philosophy":0.2443729904,
        "MMLU_prehistory":0.2685185185,
        "MMLU_professional_accounting":0.280141844,
        "MMLU_professional_law":0.2620599739,
        "MMLU_professional_medicine":0.2610294118,
        "MMLU_professional_psychology":0.2565359477,
        "MMLU_public_relations":0.2,
        "MMLU_security_studies":0.2775510204,
        "MMLU_sociology":0.1990049751,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.3274853801
    },
    {
        "Model":"Llama-2-7b-Chat-AWQ",
        "URL":"https:\/\/huggingface.co\/TheBloke\/Llama-2-7b-Chat-AWQ",
        "full_model_name":"TheBloke\/Llama-2-7b-Chat-AWQ",
        "Parameters":7.0,
        "MMLU_average":0.2466599618,
        "arc:challenge|25":0.228668942,
        "hellaswag|10":0.2551284605,
        "MMLU_abstract_algebra":0.28,
        "MMLU_anatomy":0.2518518519,
        "MMLU_astronomy":0.2631578947,
        "MMLU_business_ethics":0.25,
        "MMLU_clinical_knowledge":0.2075471698,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.19,
        "MMLU_college_computer_science":0.19,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.2601156069,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.31,
        "MMLU_conceptual_physics":0.2595744681,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2689655172,
        "MMLU_elementary_mathematics":0.2301587302,
        "MMLU_formal_logic":0.1904761905,
        "MMLU_global_facts":0.26,
        "MMLU_high_school_biology":0.2225806452,
        "MMLU_high_school_chemistry":0.2068965517,
        "MMLU_high_school_computer_science":0.31,
        "MMLU_high_school_european_history":0.303030303,
        "MMLU_high_school_geography":0.2474747475,
        "MMLU_high_school_government_and_politics":0.1813471503,
        "MMLU_high_school_macroeconomics":0.2179487179,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.2058823529,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.2275229358,
        "MMLU_high_school_statistics":0.2407407407,
        "MMLU_high_school_us_history":0.2009803922,
        "MMLU_high_school_world_history":0.2405063291,
        "MMLU_human_aging":0.2869955157,
        "MMLU_human_sexuality":0.213740458,
        "MMLU_international_law":0.2727272727,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2576687117,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.213592233,
        "MMLU_marketing":0.2692307692,
        "MMLU_medical_genetics":0.22,
        "MMLU_miscellaneous":0.2618135377,
        "MMLU_moral_disputes":0.2947976879,
        "MMLU_moral_scenarios":0.2547486034,
        "MMLU_nutrition":0.2614379085,
        "MMLU_philosophy":0.2411575563,
        "MMLU_prehistory":0.25,
        "MMLU_professional_accounting":0.2588652482,
        "MMLU_professional_law":0.2627118644,
        "MMLU_professional_medicine":0.2095588235,
        "MMLU_professional_psychology":0.2565359477,
        "MMLU_public_relations":0.2454545455,
        "MMLU_security_studies":0.2367346939,
        "MMLU_sociology":0.2885572139,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.2108433735,
        "MMLU_world_religions":0.2865497076
    },
    {
        "Model":"pythia-6.7b",
        "URL":"https:\/\/huggingface.co\/EleutherAI\/pythia-6.7b",
        "full_model_name":"EleutherAI\/pythia-6.7b",
        "Parameters":6.7,
        "MMLU_average":0.2463660185,
        "arc:challenge|25":0.3686006826,
        "hellaswag|10":0.4802828122,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.1851851852,
        "MMLU_astronomy":0.2368421053,
        "MMLU_business_ethics":0.34,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2708333333,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.16,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2947976879,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.27,
        "MMLU_conceptual_physics":0.3361702128,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.2068965517,
        "MMLU_elementary_mathematics":0.2777777778,
        "MMLU_formal_logic":0.2063492063,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2129032258,
        "MMLU_high_school_chemistry":0.197044335,
        "MMLU_high_school_computer_science":0.2,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.1767676768,
        "MMLU_high_school_government_and_politics":0.2020725389,
        "MMLU_high_school_macroeconomics":0.2102564103,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.2521008403,
        "MMLU_high_school_physics":0.238410596,
        "MMLU_high_school_psychology":0.2495412844,
        "MMLU_high_school_statistics":0.2268518519,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.2780269058,
        "MMLU_human_sexuality":0.2671755725,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.287037037,
        "MMLU_logical_fallacies":0.2576687117,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.2524271845,
        "MMLU_marketing":0.2435897436,
        "MMLU_medical_genetics":0.21,
        "MMLU_miscellaneous":0.2171136654,
        "MMLU_moral_disputes":0.2514450867,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2745098039,
        "MMLU_philosophy":0.2218649518,
        "MMLU_prehistory":0.2654320988,
        "MMLU_professional_accounting":0.2375886525,
        "MMLU_professional_law":0.2659713168,
        "MMLU_professional_medicine":0.1948529412,
        "MMLU_professional_psychology":0.2794117647,
        "MMLU_public_relations":0.2909090909,
        "MMLU_security_studies":0.2040816327,
        "MMLU_sociology":0.2736318408,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2891566265,
        "MMLU_world_religions":0.3099415205
    },
    {
        "Model":"WizardLM-30B-GPTQ",
        "URL":"https:\/\/huggingface.co\/TheBloke\/WizardLM-30B-GPTQ",
        "full_model_name":"TheBloke\/WizardLM-30B-GPTQ",
        "Parameters":30.0,
        "MMLU_average":0.2461812961,
        "arc:challenge|25":0.2252559727,
        "hellaswag|10":0.2559251145,
        "MMLU_abstract_algebra":0.31,
        "MMLU_anatomy":0.2518518519,
        "MMLU_astronomy":0.2763157895,
        "MMLU_business_ethics":0.24,
        "MMLU_clinical_knowledge":0.2075471698,
        "MMLU_college_biology":0.2847222222,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.25,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.2658959538,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.31,
        "MMLU_conceptual_physics":0.2595744681,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.3172413793,
        "MMLU_elementary_mathematics":0.2671957672,
        "MMLU_formal_logic":0.1825396825,
        "MMLU_global_facts":0.25,
        "MMLU_high_school_biology":0.2225806452,
        "MMLU_high_school_chemistry":0.1773399015,
        "MMLU_high_school_computer_science":0.26,
        "MMLU_high_school_european_history":0.2242424242,
        "MMLU_high_school_geography":0.2525252525,
        "MMLU_high_school_government_and_politics":0.1554404145,
        "MMLU_high_school_macroeconomics":0.2051282051,
        "MMLU_high_school_mathematics":0.2296296296,
        "MMLU_high_school_microeconomics":0.2016806723,
        "MMLU_high_school_physics":0.2052980132,
        "MMLU_high_school_psychology":0.2293577982,
        "MMLU_high_school_statistics":0.2638888889,
        "MMLU_high_school_us_history":0.2254901961,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.2869955157,
        "MMLU_human_sexuality":0.213740458,
        "MMLU_international_law":0.2644628099,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.2515337423,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.213592233,
        "MMLU_marketing":0.2692307692,
        "MMLU_medical_genetics":0.22,
        "MMLU_miscellaneous":0.2618135377,
        "MMLU_moral_disputes":0.2658959538,
        "MMLU_moral_scenarios":0.2324022346,
        "MMLU_nutrition":0.2320261438,
        "MMLU_philosophy":0.2443729904,
        "MMLU_prehistory":0.2283950617,
        "MMLU_professional_accounting":0.2659574468,
        "MMLU_professional_law":0.2666232073,
        "MMLU_professional_medicine":0.2426470588,
        "MMLU_professional_psychology":0.2418300654,
        "MMLU_public_relations":0.2454545455,
        "MMLU_security_studies":0.1918367347,
        "MMLU_sociology":0.3034825871,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.2048192771,
        "MMLU_world_religions":0.2865497076
    },
    {
        "Model":"proofGPT-v0.1-6.7B",
        "URL":"https:\/\/huggingface.co\/hoskinson-center\/proofGPT-v0.1-6.7B",
        "full_model_name":"hoskinson-center\/proofGPT-v0.1-6.7B",
        "Parameters":6.7,
        "MMLU_average":0.2457196396,
        "arc:challenge|25":0.2133105802,
        "hellaswag|10":0.2721569408,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.2814814815,
        "MMLU_astronomy":0.1710526316,
        "MMLU_business_ethics":0.36,
        "MMLU_clinical_knowledge":0.1962264151,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.25,
        "MMLU_college_computer_science":0.3,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.2138728324,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.26,
        "MMLU_conceptual_physics":0.2595744681,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2206896552,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.1587301587,
        "MMLU_global_facts":0.17,
        "MMLU_high_school_biology":0.2451612903,
        "MMLU_high_school_chemistry":0.2413793103,
        "MMLU_high_school_computer_science":0.3,
        "MMLU_high_school_european_history":0.1939393939,
        "MMLU_high_school_geography":0.2575757576,
        "MMLU_high_school_government_and_politics":0.2020725389,
        "MMLU_high_school_macroeconomics":0.2615384615,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.2226890756,
        "MMLU_high_school_physics":0.2450331126,
        "MMLU_high_school_psychology":0.280733945,
        "MMLU_high_school_statistics":0.2731481481,
        "MMLU_high_school_us_history":0.2254901961,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.2914798206,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.2809917355,
        "MMLU_jurisprudence":0.2314814815,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.2946428571,
        "MMLU_management":0.1650485437,
        "MMLU_marketing":0.1923076923,
        "MMLU_medical_genetics":0.31,
        "MMLU_miscellaneous":0.2247765006,
        "MMLU_moral_disputes":0.2514450867,
        "MMLU_moral_scenarios":0.2391061453,
        "MMLU_nutrition":0.2189542484,
        "MMLU_philosophy":0.1800643087,
        "MMLU_prehistory":0.2314814815,
        "MMLU_professional_accounting":0.2056737589,
        "MMLU_professional_law":0.2294654498,
        "MMLU_professional_medicine":0.4191176471,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2454545455,
        "MMLU_security_studies":0.2285714286,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.29,
        "MMLU_virology":0.3012048193,
        "MMLU_world_religions":0.2456140351
    },
    {
        "Model":"GodziLLa-30B-plus",
        "URL":"https:\/\/huggingface.co\/MayaPH\/GodziLLa-30B-plus",
        "full_model_name":"MayaPH\/GodziLLa-30B-plus",
        "Parameters":30.0,
        "MMLU_average":0.2455455711,
        "arc:challenge|25":0.2380546075,
        "hellaswag|10":0.2568213503,
        "MMLU_abstract_algebra":0.21,
        "MMLU_anatomy":0.2518518519,
        "MMLU_astronomy":0.1907894737,
        "MMLU_business_ethics":0.26,
        "MMLU_clinical_knowledge":0.2377358491,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.19,
        "MMLU_college_computer_science":0.24,
        "MMLU_college_mathematics":0.22,
        "MMLU_college_medicine":0.2658959538,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.3,
        "MMLU_conceptual_physics":0.2893617021,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2551724138,
        "MMLU_elementary_mathematics":0.2195767196,
        "MMLU_formal_logic":0.1825396825,
        "MMLU_global_facts":0.16,
        "MMLU_high_school_biology":0.2161290323,
        "MMLU_high_school_chemistry":0.2068965517,
        "MMLU_high_school_computer_science":0.28,
        "MMLU_high_school_european_history":0.2727272727,
        "MMLU_high_school_geography":0.1919191919,
        "MMLU_high_school_government_and_politics":0.2901554404,
        "MMLU_high_school_macroeconomics":0.2769230769,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.268907563,
        "MMLU_high_school_physics":0.2185430464,
        "MMLU_high_school_psychology":0.2128440367,
        "MMLU_high_school_statistics":0.162037037,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2278481013,
        "MMLU_human_aging":0.3183856502,
        "MMLU_human_sexuality":0.2900763359,
        "MMLU_international_law":0.2066115702,
        "MMLU_jurisprudence":0.2777777778,
        "MMLU_logical_fallacies":0.2883435583,
        "MMLU_machine_learning":0.2321428571,
        "MMLU_management":0.2524271845,
        "MMLU_marketing":0.3461538462,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2426564496,
        "MMLU_moral_disputes":0.2312138728,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.2124183007,
        "MMLU_philosophy":0.2057877814,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2163120567,
        "MMLU_professional_law":0.2535853977,
        "MMLU_professional_medicine":0.1985294118,
        "MMLU_professional_psychology":0.2598039216,
        "MMLU_public_relations":0.2454545455,
        "MMLU_security_studies":0.2367346939,
        "MMLU_sociology":0.263681592,
        "MMLU_us_foreign_policy":0.3,
        "MMLU_virology":0.313253012,
        "MMLU_world_religions":0.298245614
    },
    {
        "Model":"lotus-12B",
        "URL":"https:\/\/huggingface.co\/hakurei\/lotus-12B",
        "full_model_name":"hakurei\/lotus-12B",
        "Parameters":12.0,
        "MMLU_average":0.2454879135,
        "arc:challenge|25":0.2653583618,
        "hellaswag|10":0.405496913,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.2592592593,
        "MMLU_astronomy":0.1973684211,
        "MMLU_business_ethics":0.28,
        "MMLU_clinical_knowledge":0.2716981132,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.23,
        "MMLU_college_computer_science":0.17,
        "MMLU_college_mathematics":0.2,
        "MMLU_college_medicine":0.2196531792,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.17,
        "MMLU_conceptual_physics":0.3021276596,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.2344827586,
        "MMLU_elementary_mathematics":0.2486772487,
        "MMLU_formal_logic":0.1825396825,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.2580645161,
        "MMLU_high_school_chemistry":0.2906403941,
        "MMLU_high_school_computer_science":0.22,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.2424242424,
        "MMLU_high_school_government_and_politics":0.2176165803,
        "MMLU_high_school_macroeconomics":0.2256410256,
        "MMLU_high_school_mathematics":0.237037037,
        "MMLU_high_school_microeconomics":0.1890756303,
        "MMLU_high_school_physics":0.1920529801,
        "MMLU_high_school_psychology":0.2440366972,
        "MMLU_high_school_statistics":0.2962962963,
        "MMLU_high_school_us_history":0.2647058824,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3452914798,
        "MMLU_human_sexuality":0.2366412214,
        "MMLU_international_law":0.2644628099,
        "MMLU_jurisprudence":0.25,
        "MMLU_logical_fallacies":0.2699386503,
        "MMLU_machine_learning":0.2678571429,
        "MMLU_management":0.3300970874,
        "MMLU_marketing":0.2264957265,
        "MMLU_medical_genetics":0.35,
        "MMLU_miscellaneous":0.2873563218,
        "MMLU_moral_disputes":0.225433526,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2352941176,
        "MMLU_philosophy":0.2540192926,
        "MMLU_prehistory":0.2808641975,
        "MMLU_professional_accounting":0.2765957447,
        "MMLU_professional_law":0.2385919166,
        "MMLU_professional_medicine":0.2058823529,
        "MMLU_professional_psychology":0.2614379085,
        "MMLU_public_relations":0.2545454545,
        "MMLU_security_studies":0.1755102041,
        "MMLU_sociology":0.2338308458,
        "MMLU_us_foreign_policy":0.22,
        "MMLU_virology":0.2710843373,
        "MMLU_world_religions":0.2222222222
    },
    {
        "Model":"pythia-160m-deduped-step92k-193bt",
        "URL":"https:\/\/huggingface.co\/klosax\/pythia-160m-deduped-step92k-193bt",
        "full_model_name":"klosax\/pythia-160m-deduped-step92k-193bt",
        "Parameters":0.16,
        "MMLU_average":0.2454342199,
        "arc:challenge|25":0.20221843,
        "hellaswag|10":0.2965544712,
        "MMLU_abstract_algebra":0.2,
        "MMLU_anatomy":0.2592592593,
        "MMLU_astronomy":0.1842105263,
        "MMLU_business_ethics":0.22,
        "MMLU_clinical_knowledge":0.2679245283,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.14,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.1965317919,
        "MMLU_college_physics":0.2745098039,
        "MMLU_computer_security":0.2,
        "MMLU_conceptual_physics":0.2680851064,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2354497354,
        "MMLU_formal_logic":0.1984126984,
        "MMLU_global_facts":0.17,
        "MMLU_high_school_biology":0.264516129,
        "MMLU_high_school_chemistry":0.2068965517,
        "MMLU_high_school_computer_science":0.18,
        "MMLU_high_school_european_history":0.2363636364,
        "MMLU_high_school_geography":0.2474747475,
        "MMLU_high_school_government_and_politics":0.2642487047,
        "MMLU_high_school_macroeconomics":0.2153846154,
        "MMLU_high_school_mathematics":0.2888888889,
        "MMLU_high_school_microeconomics":0.231092437,
        "MMLU_high_school_physics":0.2119205298,
        "MMLU_high_school_psychology":0.2440366972,
        "MMLU_high_school_statistics":0.412037037,
        "MMLU_high_school_us_history":0.2696078431,
        "MMLU_high_school_world_history":0.2742616034,
        "MMLU_human_aging":0.3497757848,
        "MMLU_human_sexuality":0.2671755725,
        "MMLU_international_law":0.2479338843,
        "MMLU_jurisprudence":0.212962963,
        "MMLU_logical_fallacies":0.2699386503,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.1965811966,
        "MMLU_medical_genetics":0.31,
        "MMLU_miscellaneous":0.2464878672,
        "MMLU_moral_disputes":0.2456647399,
        "MMLU_moral_scenarios":0.2402234637,
        "MMLU_nutrition":0.2352941176,
        "MMLU_philosophy":0.1800643087,
        "MMLU_prehistory":0.2530864198,
        "MMLU_professional_accounting":0.2517730496,
        "MMLU_professional_law":0.2235984355,
        "MMLU_professional_medicine":0.4448529412,
        "MMLU_professional_psychology":0.2581699346,
        "MMLU_public_relations":0.2363636364,
        "MMLU_security_studies":0.2,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.3,
        "MMLU_virology":0.2771084337,
        "MMLU_world_religions":0.2105263158
    },
    {
        "Model":"Dante-2.8B",
        "URL":"https:\/\/huggingface.co\/Dampish\/Dante-2.8B",
        "full_model_name":"Dampish\/Dante-2.8B",
        "Parameters":2.8,
        "MMLU_average":0.2450792498,
        "arc:challenge|25":0.2346416382,
        "hellaswag|10":0.2544313882,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.2814814815,
        "MMLU_astronomy":0.25,
        "MMLU_business_ethics":0.22,
        "MMLU_clinical_knowledge":0.2037735849,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.31,
        "MMLU_college_computer_science":0.23,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.2543352601,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.17,
        "MMLU_conceptual_physics":0.2510638298,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.1862068966,
        "MMLU_elementary_mathematics":0.2486772487,
        "MMLU_formal_logic":0.2619047619,
        "MMLU_global_facts":0.2,
        "MMLU_high_school_biology":0.2193548387,
        "MMLU_high_school_chemistry":0.2807881773,
        "MMLU_high_school_computer_science":0.28,
        "MMLU_high_school_european_history":0.2121212121,
        "MMLU_high_school_geography":0.2575757576,
        "MMLU_high_school_government_and_politics":0.2383419689,
        "MMLU_high_school_macroeconomics":0.2384615385,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.2394957983,
        "MMLU_high_school_physics":0.178807947,
        "MMLU_high_school_psychology":0.2550458716,
        "MMLU_high_school_statistics":0.2222222222,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2784810127,
        "MMLU_human_aging":0.2735426009,
        "MMLU_human_sexuality":0.2824427481,
        "MMLU_international_law":0.2148760331,
        "MMLU_jurisprudence":0.25,
        "MMLU_logical_fallacies":0.2760736196,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.2330097087,
        "MMLU_marketing":0.2307692308,
        "MMLU_medical_genetics":0.22,
        "MMLU_miscellaneous":0.2579821201,
        "MMLU_moral_disputes":0.2514450867,
        "MMLU_moral_scenarios":0.2715083799,
        "MMLU_nutrition":0.2549019608,
        "MMLU_philosophy":0.2540192926,
        "MMLU_prehistory":0.2314814815,
        "MMLU_professional_accounting":0.2163120567,
        "MMLU_professional_law":0.2516297262,
        "MMLU_professional_medicine":0.2389705882,
        "MMLU_professional_psychology":0.2385620915,
        "MMLU_public_relations":0.1909090909,
        "MMLU_security_studies":0.2204081633,
        "MMLU_sociology":0.263681592,
        "MMLU_us_foreign_policy":0.31,
        "MMLU_virology":0.2530120482,
        "MMLU_world_religions":0.2865497076
    },
    {
        "Model":"wizard-orca-3b",
        "URL":"https:\/\/huggingface.co\/harborwater\/wizard-orca-3b",
        "full_model_name":"harborwater\/wizard-orca-3b",
        "Parameters":3.0,
        "MMLU_average":0.2449111213,
        "arc:challenge|25":0.3848122867,
        "hellaswag|10":0.5467038439,
        "MMLU_abstract_algebra":0.16,
        "MMLU_anatomy":0.1777777778,
        "MMLU_astronomy":0.2171052632,
        "MMLU_business_ethics":0.33,
        "MMLU_clinical_knowledge":0.2339622642,
        "MMLU_college_biology":0.25,
        "MMLU_college_chemistry":0.22,
        "MMLU_college_computer_science":0.31,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.2023121387,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.34,
        "MMLU_conceptual_physics":0.3106382979,
        "MMLU_econometrics":0.2105263158,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.2169312169,
        "MMLU_formal_logic":0.2777777778,
        "MMLU_global_facts":0.28,
        "MMLU_high_school_biology":0.1870967742,
        "MMLU_high_school_chemistry":0.1527093596,
        "MMLU_high_school_computer_science":0.3,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.1666666667,
        "MMLU_high_school_government_and_politics":0.2020725389,
        "MMLU_high_school_macroeconomics":0.2205128205,
        "MMLU_high_school_mathematics":0.2111111111,
        "MMLU_high_school_microeconomics":0.2478991597,
        "MMLU_high_school_physics":0.2450331126,
        "MMLU_high_school_psychology":0.2183486239,
        "MMLU_high_school_statistics":0.1805555556,
        "MMLU_high_school_us_history":0.1911764706,
        "MMLU_high_school_world_history":0.2911392405,
        "MMLU_human_aging":0.33632287,
        "MMLU_human_sexuality":0.2824427481,
        "MMLU_international_law":0.2314049587,
        "MMLU_jurisprudence":0.2407407407,
        "MMLU_logical_fallacies":0.2147239264,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.1359223301,
        "MMLU_marketing":0.3076923077,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.2426564496,
        "MMLU_moral_disputes":0.2572254335,
        "MMLU_moral_scenarios":0.2368715084,
        "MMLU_nutrition":0.2156862745,
        "MMLU_philosophy":0.270096463,
        "MMLU_prehistory":0.2407407407,
        "MMLU_professional_accounting":0.2730496454,
        "MMLU_professional_law":0.239243807,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.2516339869,
        "MMLU_public_relations":0.2636363636,
        "MMLU_security_studies":0.212244898,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.3,
        "MMLU_virology":0.313253012,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"megatron-GPT-2-345m-EvolInstruct",
        "URL":"https:\/\/huggingface.co\/KnutJaegersberg\/megatron-GPT-2-345m-EvolInstruct",
        "full_model_name":"KnutJaegersberg\/megatron-GPT-2-345m-EvolInstruct",
        "Parameters":0.345,
        "MMLU_average":0.2448251951,
        "arc:challenge|25":0.2107508532,
        "hellaswag|10":0.3143796057,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.2148148148,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.2641509434,
        "MMLU_college_biology":0.2361111111,
        "MMLU_college_chemistry":0.14,
        "MMLU_college_computer_science":0.23,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.1734104046,
        "MMLU_college_physics":0.1176470588,
        "MMLU_computer_security":0.24,
        "MMLU_conceptual_physics":0.2893617021,
        "MMLU_econometrics":0.201754386,
        "MMLU_electrical_engineering":0.2482758621,
        "MMLU_elementary_mathematics":0.2301587302,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.23,
        "MMLU_high_school_biology":0.2612903226,
        "MMLU_high_school_chemistry":0.275862069,
        "MMLU_high_school_computer_science":0.24,
        "MMLU_high_school_european_history":0.2727272727,
        "MMLU_high_school_geography":0.2121212121,
        "MMLU_high_school_government_and_politics":0.3160621762,
        "MMLU_high_school_macroeconomics":0.2717948718,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.256302521,
        "MMLU_high_school_physics":0.2185430464,
        "MMLU_high_school_psychology":0.2146788991,
        "MMLU_high_school_statistics":0.4166666667,
        "MMLU_high_school_us_history":0.2156862745,
        "MMLU_high_school_world_history":0.2194092827,
        "MMLU_human_aging":0.2780269058,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2962962963,
        "MMLU_logical_fallacies":0.2576687117,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.1650485437,
        "MMLU_marketing":0.2820512821,
        "MMLU_medical_genetics":0.24,
        "MMLU_miscellaneous":0.2860791826,
        "MMLU_moral_disputes":0.2341040462,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2058823529,
        "MMLU_philosophy":0.2443729904,
        "MMLU_prehistory":0.212962963,
        "MMLU_professional_accounting":0.2234042553,
        "MMLU_professional_law":0.2516297262,
        "MMLU_professional_medicine":0.3382352941,
        "MMLU_professional_psychology":0.2516339869,
        "MMLU_public_relations":0.2909090909,
        "MMLU_security_studies":0.2367346939,
        "MMLU_sociology":0.263681592,
        "MMLU_us_foreign_policy":0.21,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.2807017544
    },
    {
        "Model":"gpt3-finnish-small",
        "URL":"https:\/\/huggingface.co\/TurkuNLP\/gpt3-finnish-small",
        "full_model_name":"TurkuNLP\/gpt3-finnish-small",
        "Parameters":null,
        "MMLU_average":0.2446739715,
        "arc:challenge|25":0.1646757679,
        "hellaswag|10":0.2695678152,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.3259259259,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.26,
        "MMLU_clinical_knowledge":0.2452830189,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.15,
        "MMLU_college_mathematics":0.26,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.22,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.1904761905,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.2967741935,
        "MMLU_high_school_chemistry":0.1773399015,
        "MMLU_high_school_computer_science":0.31,
        "MMLU_high_school_european_history":0.303030303,
        "MMLU_high_school_geography":0.2373737374,
        "MMLU_high_school_government_and_politics":0.2279792746,
        "MMLU_high_school_macroeconomics":0.2282051282,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.2394957983,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.1944954128,
        "MMLU_high_school_statistics":0.3842592593,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2489451477,
        "MMLU_human_aging":0.3049327354,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2314049587,
        "MMLU_jurisprudence":0.2037037037,
        "MMLU_logical_fallacies":0.282208589,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.1965811966,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2311621967,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.1832797428,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2269503546,
        "MMLU_professional_law":0.2542372881,
        "MMLU_professional_medicine":0.4264705882,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2090909091,
        "MMLU_security_studies":0.1673469388,
        "MMLU_sociology":0.2288557214,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2891566265,
        "MMLU_world_religions":0.3274853801
    },
    {
        "Model":"Marcoroni-7B-LaMini-80K",
        "URL":"https:\/\/huggingface.co\/marcchew\/Marcoroni-7B-LaMini-80K",
        "full_model_name":"marcchew\/Marcoroni-7B-LaMini-80K",
        "Parameters":7.0,
        "MMLU_average":0.2446201882,
        "arc:challenge|25":0.2209897611,
        "hellaswag|10":0.2551284605,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.2518518519,
        "MMLU_astronomy":0.1842105263,
        "MMLU_business_ethics":0.26,
        "MMLU_clinical_knowledge":0.2679245283,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.15,
        "MMLU_college_mathematics":0.23,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.24,
        "MMLU_conceptual_physics":0.3234042553,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.2206896552,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.1984126984,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2548387097,
        "MMLU_high_school_chemistry":0.2709359606,
        "MMLU_high_school_computer_science":0.23,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.2171717172,
        "MMLU_high_school_government_and_politics":0.207253886,
        "MMLU_high_school_macroeconomics":0.2205128205,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.231092437,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.2366972477,
        "MMLU_high_school_statistics":0.162037037,
        "MMLU_high_school_us_history":0.2352941176,
        "MMLU_high_school_world_history":0.2616033755,
        "MMLU_human_aging":0.3766816143,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.2479338843,
        "MMLU_jurisprudence":0.2962962963,
        "MMLU_logical_fallacies":0.245398773,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.2524271845,
        "MMLU_marketing":0.2564102564,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.2873563218,
        "MMLU_moral_disputes":0.2456647399,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2287581699,
        "MMLU_philosophy":0.2733118971,
        "MMLU_prehistory":0.2654320988,
        "MMLU_professional_accounting":0.2553191489,
        "MMLU_professional_law":0.239243807,
        "MMLU_professional_medicine":0.2022058824,
        "MMLU_professional_psychology":0.2565359477,
        "MMLU_public_relations":0.3454545455,
        "MMLU_security_studies":0.1714285714,
        "MMLU_sociology":0.2388059701,
        "MMLU_us_foreign_policy":0.21,
        "MMLU_virology":0.3192771084,
        "MMLU_world_religions":0.2105263158
    },
    {
        "Model":"gpt-j-tiny-random",
        "URL":"https:\/\/huggingface.co\/anton-l\/gpt-j-tiny-random",
        "full_model_name":"anton-l\/gpt-j-tiny-random",
        "Parameters":null,
        "MMLU_average":0.2446201882,
        "arc:challenge|25":0.2303754266,
        "hellaswag|10":0.2559251145,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.2518518519,
        "MMLU_astronomy":0.1842105263,
        "MMLU_business_ethics":0.26,
        "MMLU_clinical_knowledge":0.2679245283,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.15,
        "MMLU_college_mathematics":0.23,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.24,
        "MMLU_conceptual_physics":0.3234042553,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.2206896552,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.1984126984,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2548387097,
        "MMLU_high_school_chemistry":0.2709359606,
        "MMLU_high_school_computer_science":0.23,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.2171717172,
        "MMLU_high_school_government_and_politics":0.207253886,
        "MMLU_high_school_macroeconomics":0.2205128205,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.231092437,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.2366972477,
        "MMLU_high_school_statistics":0.162037037,
        "MMLU_high_school_us_history":0.2352941176,
        "MMLU_high_school_world_history":0.2616033755,
        "MMLU_human_aging":0.3766816143,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.2479338843,
        "MMLU_jurisprudence":0.2962962963,
        "MMLU_logical_fallacies":0.245398773,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.2524271845,
        "MMLU_marketing":0.2564102564,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.2873563218,
        "MMLU_moral_disputes":0.2456647399,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2287581699,
        "MMLU_philosophy":0.2733118971,
        "MMLU_prehistory":0.2654320988,
        "MMLU_professional_accounting":0.2553191489,
        "MMLU_professional_law":0.239243807,
        "MMLU_professional_medicine":0.2022058824,
        "MMLU_professional_psychology":0.2565359477,
        "MMLU_public_relations":0.3454545455,
        "MMLU_security_studies":0.1714285714,
        "MMLU_sociology":0.2388059701,
        "MMLU_us_foreign_policy":0.21,
        "MMLU_virology":0.3192771084,
        "MMLU_world_religions":0.2105263158
    },
    {
        "Model":"bloom-560m-guanaco",
        "URL":"https:\/\/huggingface.co\/rishiraj\/bloom-560m-guanaco",
        "full_model_name":"rishiraj\/bloom-560m-guanaco",
        "Parameters":0.56,
        "MMLU_average":0.2446201882,
        "arc:challenge|25":0.2167235495,
        "hellaswag|10":0.2572196774,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.2518518519,
        "MMLU_astronomy":0.1842105263,
        "MMLU_business_ethics":0.26,
        "MMLU_clinical_knowledge":0.2679245283,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.15,
        "MMLU_college_mathematics":0.23,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.24,
        "MMLU_conceptual_physics":0.3234042553,
        "MMLU_econometrics":0.2807017544,
        "MMLU_electrical_engineering":0.2206896552,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.1984126984,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2548387097,
        "MMLU_high_school_chemistry":0.2709359606,
        "MMLU_high_school_computer_science":0.23,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.2171717172,
        "MMLU_high_school_government_and_politics":0.207253886,
        "MMLU_high_school_macroeconomics":0.2205128205,
        "MMLU_high_school_mathematics":0.262962963,
        "MMLU_high_school_microeconomics":0.231092437,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.2366972477,
        "MMLU_high_school_statistics":0.162037037,
        "MMLU_high_school_us_history":0.2352941176,
        "MMLU_high_school_world_history":0.2616033755,
        "MMLU_human_aging":0.3766816143,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.2479338843,
        "MMLU_jurisprudence":0.2962962963,
        "MMLU_logical_fallacies":0.245398773,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.2524271845,
        "MMLU_marketing":0.2564102564,
        "MMLU_medical_genetics":0.26,
        "MMLU_miscellaneous":0.2873563218,
        "MMLU_moral_disputes":0.2456647399,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2287581699,
        "MMLU_philosophy":0.2733118971,
        "MMLU_prehistory":0.2654320988,
        "MMLU_professional_accounting":0.2553191489,
        "MMLU_professional_law":0.239243807,
        "MMLU_professional_medicine":0.2022058824,
        "MMLU_professional_psychology":0.2565359477,
        "MMLU_public_relations":0.3454545455,
        "MMLU_security_studies":0.1714285714,
        "MMLU_sociology":0.2388059701,
        "MMLU_us_foreign_policy":0.21,
        "MMLU_virology":0.3192771084,
        "MMLU_world_religions":0.2105263158
    },
    {
        "Model":"RWKV-4-PilePlus-430M-20230520-6162-1018Gtokens-ctx4098",
        "URL":"https:\/\/huggingface.co\/KnutJaegersberg\/RWKV-4-PilePlus-430M-20230520-6162-1018Gtokens-ctx4098",
        "full_model_name":"KnutJaegersberg\/RWKV-4-PilePlus-430M-20230520-6162-1018Gtokens-ctx4098",
        "Parameters":0.43,
        "MMLU_average":0.2444916893,
        "arc:challenge|25":0.2337883959,
        "hellaswag|10":0.3362875921,
        "MMLU_abstract_algebra":0.17,
        "MMLU_anatomy":0.3185185185,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.21,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.19,
        "MMLU_college_computer_science":0.18,
        "MMLU_college_mathematics":0.34,
        "MMLU_college_medicine":0.2427745665,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.29,
        "MMLU_conceptual_physics":0.2936170213,
        "MMLU_econometrics":0.2192982456,
        "MMLU_electrical_engineering":0.2137931034,
        "MMLU_elementary_mathematics":0.2619047619,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.2,
        "MMLU_high_school_biology":0.2516129032,
        "MMLU_high_school_chemistry":0.2118226601,
        "MMLU_high_school_computer_science":0.24,
        "MMLU_high_school_european_history":0.2848484848,
        "MMLU_high_school_geography":0.202020202,
        "MMLU_high_school_government_and_politics":0.2383419689,
        "MMLU_high_school_macroeconomics":0.2487179487,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.2478991597,
        "MMLU_high_school_physics":0.2317880795,
        "MMLU_high_school_psychology":0.2018348624,
        "MMLU_high_school_statistics":0.2685185185,
        "MMLU_high_school_us_history":0.2156862745,
        "MMLU_high_school_world_history":0.2573839662,
        "MMLU_human_aging":0.264573991,
        "MMLU_human_sexuality":0.1526717557,
        "MMLU_international_law":0.2892561983,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.2331288344,
        "MMLU_machine_learning":0.2857142857,
        "MMLU_management":0.2330097087,
        "MMLU_marketing":0.2094017094,
        "MMLU_medical_genetics":0.34,
        "MMLU_miscellaneous":0.2567049808,
        "MMLU_moral_disputes":0.2543352601,
        "MMLU_moral_scenarios":0.2558659218,
        "MMLU_nutrition":0.2222222222,
        "MMLU_philosophy":0.231511254,
        "MMLU_prehistory":0.2654320988,
        "MMLU_professional_accounting":0.2517730496,
        "MMLU_professional_law":0.241851369,
        "MMLU_professional_medicine":0.2904411765,
        "MMLU_professional_psychology":0.2516339869,
        "MMLU_public_relations":0.3272727273,
        "MMLU_security_studies":0.2979591837,
        "MMLU_sociology":0.2388059701,
        "MMLU_us_foreign_policy":0.22,
        "MMLU_virology":0.2590361446,
        "MMLU_world_religions":0.2280701754
    },
    {
        "Model":"stablelm-tuned-alpha-7b",
        "URL":"https:\/\/huggingface.co\/stabilityai\/stablelm-tuned-alpha-7b",
        "full_model_name":"stabilityai\/stablelm-tuned-alpha-7b",
        "Parameters":7.0,
        "MMLU_average":0.2441246159,
        "arc:challenge|25":0.3003412969,
        "hellaswag|10":0.412865963,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.2666666667,
        "MMLU_astronomy":0.1710526316,
        "MMLU_business_ethics":0.24,
        "MMLU_clinical_knowledge":0.2754716981,
        "MMLU_college_biology":0.2361111111,
        "MMLU_college_chemistry":0.28,
        "MMLU_college_computer_science":0.22,
        "MMLU_college_mathematics":0.3,
        "MMLU_college_medicine":0.2312138728,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.314893617,
        "MMLU_econometrics":0.2105263158,
        "MMLU_electrical_engineering":0.275862069,
        "MMLU_elementary_mathematics":0.2433862434,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.19,
        "MMLU_high_school_biology":0.2129032258,
        "MMLU_high_school_chemistry":0.1822660099,
        "MMLU_high_school_computer_science":0.22,
        "MMLU_high_school_european_history":0.2242424242,
        "MMLU_high_school_geography":0.1868686869,
        "MMLU_high_school_government_and_politics":0.2124352332,
        "MMLU_high_school_macroeconomics":0.2743589744,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.1974789916,
        "MMLU_high_school_physics":0.2185430464,
        "MMLU_high_school_psychology":0.2293577982,
        "MMLU_high_school_statistics":0.3518518519,
        "MMLU_high_school_us_history":0.2401960784,
        "MMLU_high_school_world_history":0.2869198312,
        "MMLU_human_aging":0.3542600897,
        "MMLU_human_sexuality":0.2900763359,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.287037037,
        "MMLU_logical_fallacies":0.2760736196,
        "MMLU_machine_learning":0.2410714286,
        "MMLU_management":0.1650485437,
        "MMLU_marketing":0.2478632479,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2349936143,
        "MMLU_moral_disputes":0.225433526,
        "MMLU_moral_scenarios":0.2413407821,
        "MMLU_nutrition":0.2516339869,
        "MMLU_philosophy":0.2186495177,
        "MMLU_prehistory":0.2222222222,
        "MMLU_professional_accounting":0.2482269504,
        "MMLU_professional_law":0.2294654498,
        "MMLU_professional_medicine":0.1654411765,
        "MMLU_professional_psychology":0.2516339869,
        "MMLU_public_relations":0.2363636364,
        "MMLU_security_studies":0.1673469388,
        "MMLU_sociology":0.263681592,
        "MMLU_us_foreign_policy":0.24,
        "MMLU_virology":0.2891566265,
        "MMLU_world_religions":0.2456140351
    },
    {
        "Model":"DiffMerge-DollyGPT-Pygmalion",
        "URL":"https:\/\/huggingface.co\/TehVenom\/DiffMerge-DollyGPT-Pygmalion",
        "full_model_name":"TehVenom\/DiffMerge-DollyGPT-Pygmalion",
        "Parameters":null,
        "MMLU_average":0.2441047911,
        "arc:challenge|25":0.2107508532,
        "hellaswag|10":0.2993427604,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.2148148148,
        "MMLU_astronomy":0.1907894737,
        "MMLU_business_ethics":0.25,
        "MMLU_clinical_knowledge":0.2301886792,
        "MMLU_college_biology":0.2152777778,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.22,
        "MMLU_college_mathematics":0.2,
        "MMLU_college_medicine":0.2312138728,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.24,
        "MMLU_conceptual_physics":0.2808510638,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2344827586,
        "MMLU_elementary_mathematics":0.2116402116,
        "MMLU_formal_logic":0.2142857143,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.2064516129,
        "MMLU_high_school_chemistry":0.2807881773,
        "MMLU_high_school_computer_science":0.27,
        "MMLU_high_school_european_history":0.2303030303,
        "MMLU_high_school_geography":0.2424242424,
        "MMLU_high_school_government_and_politics":0.2124352332,
        "MMLU_high_school_macroeconomics":0.2179487179,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.2478991597,
        "MMLU_high_school_physics":0.2119205298,
        "MMLU_high_school_psychology":0.2311926606,
        "MMLU_high_school_statistics":0.4537037037,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.3273542601,
        "MMLU_human_sexuality":0.2519083969,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.2024539877,
        "MMLU_machine_learning":0.2321428571,
        "MMLU_management":0.1844660194,
        "MMLU_marketing":0.1837606838,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.2349936143,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.243575419,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.2347266881,
        "MMLU_prehistory":0.2283950617,
        "MMLU_professional_accounting":0.2553191489,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.3308823529,
        "MMLU_professional_psychology":0.2287581699,
        "MMLU_public_relations":0.2727272727,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.2,
        "MMLU_virology":0.3072289157,
        "MMLU_world_religions":0.2865497076
    },
    {
        "Model":"smol_llama-101M-GQA",
        "URL":"https:\/\/huggingface.co\/BEE-spoke-data\/smol_llama-101M-GQA",
        "full_model_name":"BEE-spoke-data\/smol_llama-101M-GQA",
        "Parameters":0.101,
        "MMLU_average":0.243452276,
        "arc:challenge|25":0.180887372,
        "hellaswag|10":0.2789285003,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.2740740741,
        "MMLU_astronomy":0.2039473684,
        "MMLU_business_ethics":0.19,
        "MMLU_clinical_knowledge":0.2264150943,
        "MMLU_college_biology":0.2708333333,
        "MMLU_college_chemistry":0.15,
        "MMLU_college_computer_science":0.18,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.1907514451,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.26,
        "MMLU_conceptual_physics":0.2382978723,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.1798941799,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.23,
        "MMLU_high_school_biology":0.3064516129,
        "MMLU_high_school_chemistry":0.2807881773,
        "MMLU_high_school_computer_science":0.16,
        "MMLU_high_school_european_history":0.2242424242,
        "MMLU_high_school_geography":0.2626262626,
        "MMLU_high_school_government_and_politics":0.3419689119,
        "MMLU_high_school_macroeconomics":0.3025641026,
        "MMLU_high_school_mathematics":0.2740740741,
        "MMLU_high_school_microeconomics":0.281512605,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.2275229358,
        "MMLU_high_school_statistics":0.4074074074,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.2489451477,
        "MMLU_human_aging":0.2197309417,
        "MMLU_human_sexuality":0.2061068702,
        "MMLU_international_law":0.2892561983,
        "MMLU_jurisprudence":0.2407407407,
        "MMLU_logical_fallacies":0.245398773,
        "MMLU_machine_learning":0.2053571429,
        "MMLU_management":0.213592233,
        "MMLU_marketing":0.1965811966,
        "MMLU_medical_genetics":0.24,
        "MMLU_miscellaneous":0.2643678161,
        "MMLU_moral_disputes":0.2543352601,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.2222222222,
        "MMLU_philosophy":0.2893890675,
        "MMLU_prehistory":0.2407407407,
        "MMLU_professional_accounting":0.2695035461,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.2794117647,
        "MMLU_professional_psychology":0.2663398693,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.3387755102,
        "MMLU_sociology":0.2139303483,
        "MMLU_us_foreign_policy":0.24,
        "MMLU_virology":0.2108433735,
        "MMLU_world_religions":0.2046783626
    },
    {
        "Model":"DeciCoder-1b",
        "URL":"https:\/\/huggingface.co\/Deci\/DeciCoder-1b",
        "full_model_name":"Deci\/DeciCoder-1b",
        "Parameters":1.0,
        "MMLU_average":0.2434460288,
        "arc:challenge|25":0.1604095563,
        "hellaswag|10":0.2826130253,
        "MMLU_abstract_algebra":0.29,
        "MMLU_anatomy":0.2592592593,
        "MMLU_astronomy":0.2631578947,
        "MMLU_business_ethics":0.33,
        "MMLU_clinical_knowledge":0.2,
        "MMLU_college_biology":0.2638888889,
        "MMLU_college_chemistry":0.26,
        "MMLU_college_computer_science":0.19,
        "MMLU_college_mathematics":0.22,
        "MMLU_college_medicine":0.225433526,
        "MMLU_college_physics":0.2549019608,
        "MMLU_computer_security":0.27,
        "MMLU_conceptual_physics":0.2808510638,
        "MMLU_econometrics":0.201754386,
        "MMLU_electrical_engineering":0.2275862069,
        "MMLU_elementary_mathematics":0.2328042328,
        "MMLU_formal_logic":0.2698412698,
        "MMLU_global_facts":0.3,
        "MMLU_high_school_biology":0.1741935484,
        "MMLU_high_school_chemistry":0.1871921182,
        "MMLU_high_school_computer_science":0.3,
        "MMLU_high_school_european_history":0.2303030303,
        "MMLU_high_school_geography":0.2070707071,
        "MMLU_high_school_government_and_politics":0.207253886,
        "MMLU_high_school_macroeconomics":0.1948717949,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.1932773109,
        "MMLU_high_school_physics":0.2649006623,
        "MMLU_high_school_psychology":0.1908256881,
        "MMLU_high_school_statistics":0.1712962963,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3094170404,
        "MMLU_human_sexuality":0.2824427481,
        "MMLU_international_law":0.2561983471,
        "MMLU_jurisprudence":0.2314814815,
        "MMLU_logical_fallacies":0.263803681,
        "MMLU_machine_learning":0.2589285714,
        "MMLU_management":0.1844660194,
        "MMLU_marketing":0.2820512821,
        "MMLU_medical_genetics":0.2,
        "MMLU_miscellaneous":0.2796934866,
        "MMLU_moral_disputes":0.2398843931,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.2189542484,
        "MMLU_philosophy":0.2829581994,
        "MMLU_prehistory":0.2438271605,
        "MMLU_professional_accounting":0.2517730496,
        "MMLU_professional_law":0.241851369,
        "MMLU_professional_medicine":0.1727941176,
        "MMLU_professional_psychology":0.2826797386,
        "MMLU_public_relations":0.2454545455,
        "MMLU_security_studies":0.1959183673,
        "MMLU_sociology":0.2587064677,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.2530120482,
        "MMLU_world_religions":0.298245614
    },
    {
        "Model":"test2",
        "URL":"https:\/\/huggingface.co\/doas\/test2",
        "full_model_name":"doas\/test2",
        "Parameters":null,
        "MMLU_average":0.2434279328,
        "arc:challenge|25":0.2363481229,
        "hellaswag|10":0.256124278,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.237037037,
        "MMLU_astronomy":0.1513157895,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.241509434,
        "MMLU_college_biology":0.2291666667,
        "MMLU_college_chemistry":0.25,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.31,
        "MMLU_college_medicine":0.1676300578,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.21,
        "MMLU_conceptual_physics":0.3063829787,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2137931034,
        "MMLU_elementary_mathematics":0.2275132275,
        "MMLU_formal_logic":0.2698412698,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2451612903,
        "MMLU_high_school_chemistry":0.2167487685,
        "MMLU_high_school_computer_science":0.17,
        "MMLU_high_school_european_history":0.2,
        "MMLU_high_school_geography":0.2222222222,
        "MMLU_high_school_government_and_politics":0.2487046632,
        "MMLU_high_school_macroeconomics":0.1846153846,
        "MMLU_high_school_mathematics":0.2296296296,
        "MMLU_high_school_microeconomics":0.2058823529,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.2330275229,
        "MMLU_high_school_statistics":0.3101851852,
        "MMLU_high_school_us_history":0.1911764706,
        "MMLU_high_school_world_history":0.1983122363,
        "MMLU_human_aging":0.3452914798,
        "MMLU_human_sexuality":0.2366412214,
        "MMLU_international_law":0.2727272727,
        "MMLU_jurisprudence":0.2222222222,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.2410714286,
        "MMLU_management":0.2718446602,
        "MMLU_marketing":0.2307692308,
        "MMLU_medical_genetics":0.21,
        "MMLU_miscellaneous":0.2873563218,
        "MMLU_moral_disputes":0.2369942197,
        "MMLU_moral_scenarios":0.2715083799,
        "MMLU_nutrition":0.2745098039,
        "MMLU_philosophy":0.2765273312,
        "MMLU_prehistory":0.2685185185,
        "MMLU_professional_accounting":0.2482269504,
        "MMLU_professional_law":0.2425032595,
        "MMLU_professional_medicine":0.2573529412,
        "MMLU_professional_psychology":0.2254901961,
        "MMLU_public_relations":0.2909090909,
        "MMLU_security_studies":0.2367346939,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.24,
        "MMLU_virology":0.3192771084,
        "MMLU_world_religions":0.216374269
    },
    {
        "Model":"OPT-6B-nerys-v2",
        "URL":"https:\/\/huggingface.co\/KoboldAI\/OPT-6B-nerys-v2",
        "full_model_name":"KoboldAI\/OPT-6B-nerys-v2",
        "Parameters":6.0,
        "MMLU_average":0.2433747556,
        "arc:challenge|25":0.3421501706,
        "hellaswag|10":0.506970723,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.3407407407,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.17,
        "MMLU_clinical_knowledge":0.2490566038,
        "MMLU_college_biology":0.2152777778,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.2138728324,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.23,
        "MMLU_conceptual_physics":0.2680851064,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2137931034,
        "MMLU_elementary_mathematics":0.2328042328,
        "MMLU_formal_logic":0.1984126984,
        "MMLU_global_facts":0.28,
        "MMLU_high_school_biology":0.235483871,
        "MMLU_high_school_chemistry":0.2512315271,
        "MMLU_high_school_computer_science":0.19,
        "MMLU_high_school_european_history":0.2484848485,
        "MMLU_high_school_geography":0.2171717172,
        "MMLU_high_school_government_and_politics":0.2383419689,
        "MMLU_high_school_macroeconomics":0.241025641,
        "MMLU_high_school_mathematics":0.2814814815,
        "MMLU_high_school_microeconomics":0.2058823529,
        "MMLU_high_school_physics":0.2119205298,
        "MMLU_high_school_psychology":0.2605504587,
        "MMLU_high_school_statistics":0.1990740741,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.2573839662,
        "MMLU_human_aging":0.33632287,
        "MMLU_human_sexuality":0.2366412214,
        "MMLU_international_law":0.2314049587,
        "MMLU_jurisprudence":0.2777777778,
        "MMLU_logical_fallacies":0.2331288344,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.2330097087,
        "MMLU_marketing":0.3076923077,
        "MMLU_medical_genetics":0.25,
        "MMLU_miscellaneous":0.275862069,
        "MMLU_moral_disputes":0.2312138728,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2124183007,
        "MMLU_philosophy":0.2572347267,
        "MMLU_prehistory":0.274691358,
        "MMLU_professional_accounting":0.2872340426,
        "MMLU_professional_law":0.2431551499,
        "MMLU_professional_medicine":0.25,
        "MMLU_professional_psychology":0.2516339869,
        "MMLU_public_relations":0.2818181818,
        "MMLU_security_studies":0.1591836735,
        "MMLU_sociology":0.2338308458,
        "MMLU_us_foreign_policy":0.21,
        "MMLU_virology":0.3072289157,
        "MMLU_world_religions":0.2339181287
    },
    {
        "Model":"megatron-gpt2-345m",
        "URL":"https:\/\/huggingface.co\/robowaifudev\/megatron-gpt2-345m",
        "full_model_name":"robowaifudev\/megatron-gpt2-345m",
        "Parameters":0.345,
        "MMLU_average":0.2431970035,
        "arc:challenge|25":0.2175767918,
        "hellaswag|10":0.3299143597,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.2814814815,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.31,
        "MMLU_clinical_knowledge":0.2264150943,
        "MMLU_college_biology":0.2291666667,
        "MMLU_college_chemistry":0.22,
        "MMLU_college_computer_science":0.14,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.2023121387,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.32,
        "MMLU_conceptual_physics":0.2595744681,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.2551724138,
        "MMLU_elementary_mathematics":0.1878306878,
        "MMLU_formal_logic":0.126984127,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.2677419355,
        "MMLU_high_school_chemistry":0.2857142857,
        "MMLU_high_school_computer_science":0.23,
        "MMLU_high_school_european_history":0.2303030303,
        "MMLU_high_school_geography":0.2575757576,
        "MMLU_high_school_government_and_politics":0.2538860104,
        "MMLU_high_school_macroeconomics":0.2205128205,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.218487395,
        "MMLU_high_school_physics":0.1920529801,
        "MMLU_high_school_psychology":0.2128440367,
        "MMLU_high_school_statistics":0.3472222222,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.1928251121,
        "MMLU_human_sexuality":0.2519083969,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2024539877,
        "MMLU_machine_learning":0.3660714286,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2307692308,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.245210728,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2222222222,
        "MMLU_philosophy":0.1897106109,
        "MMLU_prehistory":0.2191358025,
        "MMLU_professional_accounting":0.2340425532,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.4338235294,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2090909091,
        "MMLU_security_studies":0.2408163265,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.25,
        "MMLU_virology":0.2530120482,
        "MMLU_world_religions":0.3333333333
    },
    {
        "Model":"orca_mini_v3_7B-GPTQ",
        "URL":"https:\/\/huggingface.co\/TheBloke\/orca_mini_v3_7B-GPTQ",
        "full_model_name":"TheBloke\/orca_mini_v3_7B-GPTQ",
        "Parameters":7.0,
        "MMLU_average":0.2431209048,
        "arc:challenge|25":0.2354948805,
        "hellaswag|10":0.254929297,
        "MMLU_abstract_algebra":0.26,
        "MMLU_anatomy":0.2666666667,
        "MMLU_astronomy":0.1710526316,
        "MMLU_business_ethics":0.27,
        "MMLU_clinical_knowledge":0.2679245283,
        "MMLU_college_biology":0.2430555556,
        "MMLU_college_chemistry":0.17,
        "MMLU_college_computer_science":0.24,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2138728324,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.25,
        "MMLU_conceptual_physics":0.2808510638,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.2206896552,
        "MMLU_elementary_mathematics":0.2619047619,
        "MMLU_formal_logic":0.1984126984,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.2483870968,
        "MMLU_high_school_chemistry":0.2807881773,
        "MMLU_high_school_computer_science":0.24,
        "MMLU_high_school_european_history":0.2545454545,
        "MMLU_high_school_geography":0.1868686869,
        "MMLU_high_school_government_and_politics":0.2435233161,
        "MMLU_high_school_macroeconomics":0.2333333333,
        "MMLU_high_school_mathematics":0.2703703704,
        "MMLU_high_school_microeconomics":0.231092437,
        "MMLU_high_school_physics":0.238410596,
        "MMLU_high_school_psychology":0.2513761468,
        "MMLU_high_school_statistics":0.1435185185,
        "MMLU_high_school_us_history":0.2941176471,
        "MMLU_high_school_world_history":0.2320675105,
        "MMLU_human_aging":0.2869955157,
        "MMLU_human_sexuality":0.2061068702,
        "MMLU_international_law":0.3305785124,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.226993865,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.145631068,
        "MMLU_marketing":0.2435897436,
        "MMLU_medical_genetics":0.27,
        "MMLU_miscellaneous":0.2477650064,
        "MMLU_moral_disputes":0.2225433526,
        "MMLU_moral_scenarios":0.2413407821,
        "MMLU_nutrition":0.2189542484,
        "MMLU_philosophy":0.2379421222,
        "MMLU_prehistory":0.2469135802,
        "MMLU_professional_accounting":0.2446808511,
        "MMLU_professional_law":0.2535853977,
        "MMLU_professional_medicine":0.1580882353,
        "MMLU_professional_psychology":0.2614379085,
        "MMLU_public_relations":0.3363636364,
        "MMLU_security_studies":0.1714285714,
        "MMLU_sociology":0.2338308458,
        "MMLU_us_foreign_policy":0.24,
        "MMLU_virology":0.3493975904,
        "MMLU_world_religions":0.2514619883
    },
    {
        "Model":"TinyLlama-1.1bee",
        "URL":"https:\/\/huggingface.co\/BEE-spoke-data\/TinyLlama-1.1bee",
        "full_model_name":"BEE-spoke-data\/TinyLlama-1.1bee",
        "Parameters":1.1,
        "MMLU_average":0.2425356195,
        "arc:challenge|25":0.2610921502,
        "hellaswag|10":0.3949412468,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.1703703704,
        "MMLU_astronomy":0.1973684211,
        "MMLU_business_ethics":0.31,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.25,
        "MMLU_college_chemistry":0.26,
        "MMLU_college_computer_science":0.29,
        "MMLU_college_mathematics":0.23,
        "MMLU_college_medicine":0.2196531792,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.27,
        "MMLU_conceptual_physics":0.2680851064,
        "MMLU_econometrics":0.2105263158,
        "MMLU_electrical_engineering":0.2482758621,
        "MMLU_elementary_mathematics":0.246031746,
        "MMLU_formal_logic":0.2142857143,
        "MMLU_global_facts":0.31,
        "MMLU_high_school_biology":0.1838709677,
        "MMLU_high_school_chemistry":0.1921182266,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.1717171717,
        "MMLU_high_school_government_and_politics":0.2279792746,
        "MMLU_high_school_macroeconomics":0.1948717949,
        "MMLU_high_school_mathematics":0.2518518519,
        "MMLU_high_school_microeconomics":0.218487395,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.2165137615,
        "MMLU_high_school_statistics":0.1666666667,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2748091603,
        "MMLU_international_law":0.2727272727,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.2883435583,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.145631068,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.21,
        "MMLU_miscellaneous":0.2273307791,
        "MMLU_moral_disputes":0.2138728324,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2222222222,
        "MMLU_philosophy":0.2250803859,
        "MMLU_prehistory":0.2067901235,
        "MMLU_professional_accounting":0.2624113475,
        "MMLU_professional_law":0.2698826597,
        "MMLU_professional_medicine":0.1801470588,
        "MMLU_professional_psychology":0.3071895425,
        "MMLU_public_relations":0.2545454545,
        "MMLU_security_studies":0.2857142857,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2710843373,
        "MMLU_world_religions":0.3333333333
    },
    {
        "Model":"mpt-7b-storywriter",
        "URL":"https:\/\/huggingface.co\/mosaicml\/mpt-7b-storywriter",
        "full_model_name":"mosaicml\/mpt-7b-storywriter",
        "Parameters":7.0,
        "MMLU_average":0.2424982815,
        "arc:challenge|25":0.1732081911,
        "hellaswag|10":0.2633937463,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.3259259259,
        "MMLU_astronomy":0.1842105263,
        "MMLU_business_ethics":0.27,
        "MMLU_clinical_knowledge":0.2226415094,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.28,
        "MMLU_college_computer_science":0.15,
        "MMLU_college_mathematics":0.2,
        "MMLU_college_medicine":0.2138728324,
        "MMLU_college_physics":0.2843137255,
        "MMLU_computer_security":0.23,
        "MMLU_conceptual_physics":0.229787234,
        "MMLU_econometrics":0.2280701754,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.2592592593,
        "MMLU_formal_logic":0.2063492063,
        "MMLU_global_facts":0.21,
        "MMLU_high_school_biology":0.3096774194,
        "MMLU_high_school_chemistry":0.2512315271,
        "MMLU_high_school_computer_science":0.2,
        "MMLU_high_school_european_history":0.2363636364,
        "MMLU_high_school_geography":0.2474747475,
        "MMLU_high_school_government_and_politics":0.2279792746,
        "MMLU_high_school_macroeconomics":0.2051282051,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.231092437,
        "MMLU_high_school_physics":0.2052980132,
        "MMLU_high_school_psychology":0.2091743119,
        "MMLU_high_school_statistics":0.4398148148,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.2362869198,
        "MMLU_human_aging":0.2242152466,
        "MMLU_human_sexuality":0.2519083969,
        "MMLU_international_law":0.2479338843,
        "MMLU_jurisprudence":0.212962963,
        "MMLU_logical_fallacies":0.2515337423,
        "MMLU_machine_learning":0.1964285714,
        "MMLU_management":0.1941747573,
        "MMLU_marketing":0.235042735,
        "MMLU_medical_genetics":0.27,
        "MMLU_miscellaneous":0.2592592593,
        "MMLU_moral_disputes":0.2167630058,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.2250803859,
        "MMLU_prehistory":0.25,
        "MMLU_professional_accounting":0.2375886525,
        "MMLU_professional_law":0.2496740548,
        "MMLU_professional_medicine":0.2463235294,
        "MMLU_professional_psychology":0.2598039216,
        "MMLU_public_relations":0.2545454545,
        "MMLU_security_studies":0.2653061224,
        "MMLU_sociology":0.2786069652,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.2469879518,
        "MMLU_world_religions":0.2807017544
    },
    {
        "Model":"llama-2-4b",
        "URL":"https:\/\/huggingface.co\/winglian\/llama-2-4b",
        "full_model_name":"winglian\/llama-2-4b",
        "Parameters":4.0,
        "MMLU_average":0.2422303793,
        "arc:challenge|25":0.2807167235,
        "hellaswag|10":0.4076877116,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.3555555556,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.28,
        "MMLU_clinical_knowledge":0.2754716981,
        "MMLU_college_biology":0.25,
        "MMLU_college_chemistry":0.22,
        "MMLU_college_computer_science":0.29,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.2023121387,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.27,
        "MMLU_conceptual_physics":0.2978723404,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2068965517,
        "MMLU_elementary_mathematics":0.2486772487,
        "MMLU_formal_logic":0.2222222222,
        "MMLU_global_facts":0.21,
        "MMLU_high_school_biology":0.2129032258,
        "MMLU_high_school_chemistry":0.2463054187,
        "MMLU_high_school_computer_science":0.24,
        "MMLU_high_school_european_history":0.2242424242,
        "MMLU_high_school_geography":0.1919191919,
        "MMLU_high_school_government_and_politics":0.2176165803,
        "MMLU_high_school_macroeconomics":0.2307692308,
        "MMLU_high_school_mathematics":0.2592592593,
        "MMLU_high_school_microeconomics":0.2058823529,
        "MMLU_high_school_physics":0.2052980132,
        "MMLU_high_school_psychology":0.2146788991,
        "MMLU_high_school_statistics":0.2592592593,
        "MMLU_high_school_us_history":0.2352941176,
        "MMLU_high_school_world_history":0.2573839662,
        "MMLU_human_aging":0.3004484305,
        "MMLU_human_sexuality":0.2442748092,
        "MMLU_international_law":0.2561983471,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.263803681,
        "MMLU_machine_learning":0.2321428571,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2777777778,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2477650064,
        "MMLU_moral_disputes":0.2456647399,
        "MMLU_moral_scenarios":0.2402234637,
        "MMLU_nutrition":0.2483660131,
        "MMLU_philosophy":0.2025723473,
        "MMLU_prehistory":0.2561728395,
        "MMLU_professional_accounting":0.2553191489,
        "MMLU_professional_law":0.2464146023,
        "MMLU_professional_medicine":0.1617647059,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2272727273,
        "MMLU_security_studies":0.1469387755,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.3012048193,
        "MMLU_world_religions":0.3450292398
    },
    {
        "Model":"Yi-8B-Llama",
        "URL":"https:\/\/huggingface.co\/ByteWave\/Yi-8B-Llama",
        "full_model_name":"ByteWave\/Yi-8B-Llama",
        "Parameters":8.0,
        "MMLU_average":0.2413926222,
        "arc:challenge|25":0.2295221843,
        "hellaswag|10":0.2571200956,
        "MMLU_abstract_algebra":0.17,
        "MMLU_anatomy":0.2666666667,
        "MMLU_astronomy":0.2039473684,
        "MMLU_business_ethics":0.24,
        "MMLU_clinical_knowledge":0.2528301887,
        "MMLU_college_biology":0.2291666667,
        "MMLU_college_chemistry":0.22,
        "MMLU_college_computer_science":0.16,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.1907514451,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.22,
        "MMLU_conceptual_physics":0.3063829787,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.275862069,
        "MMLU_elementary_mathematics":0.2142857143,
        "MMLU_formal_logic":0.2777777778,
        "MMLU_global_facts":0.2,
        "MMLU_high_school_biology":0.2774193548,
        "MMLU_high_school_chemistry":0.2610837438,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2484848485,
        "MMLU_high_school_geography":0.2272727273,
        "MMLU_high_school_government_and_politics":0.2124352332,
        "MMLU_high_school_macroeconomics":0.1923076923,
        "MMLU_high_school_mathematics":0.2037037037,
        "MMLU_high_school_microeconomics":0.1974789916,
        "MMLU_high_school_physics":0.1920529801,
        "MMLU_high_school_psychology":0.2366972477,
        "MMLU_high_school_statistics":0.1296296296,
        "MMLU_high_school_us_history":0.2303921569,
        "MMLU_high_school_world_history":0.2573839662,
        "MMLU_human_aging":0.33632287,
        "MMLU_human_sexuality":0.2366412214,
        "MMLU_international_law":0.2561983471,
        "MMLU_jurisprudence":0.2407407407,
        "MMLU_logical_fallacies":0.2392638037,
        "MMLU_machine_learning":0.2946428571,
        "MMLU_management":0.2233009709,
        "MMLU_marketing":0.2307692308,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2656449553,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.270096463,
        "MMLU_prehistory":0.2561728395,
        "MMLU_professional_accounting":0.2375886525,
        "MMLU_professional_law":0.2398956975,
        "MMLU_professional_medicine":0.4301470588,
        "MMLU_professional_psychology":0.2549019608,
        "MMLU_public_relations":0.2818181818,
        "MMLU_security_studies":0.1714285714,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.23,
        "MMLU_virology":0.3192771084,
        "MMLU_world_religions":0.2222222222
    },
    {
        "Model":"pygmalion-1.3b",
        "URL":"https:\/\/huggingface.co\/PygmalionAI\/pygmalion-1.3b",
        "full_model_name":"PygmalionAI\/pygmalion-1.3b",
        "Parameters":1.3,
        "MMLU_average":0.2412022278,
        "arc:challenge|25":0.252559727,
        "hellaswag|10":0.3845847441,
        "MMLU_abstract_algebra":0.19,
        "MMLU_anatomy":0.2222222222,
        "MMLU_astronomy":0.2171052632,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2226415094,
        "MMLU_college_biology":0.25,
        "MMLU_college_chemistry":0.27,
        "MMLU_college_computer_science":0.25,
        "MMLU_college_mathematics":0.29,
        "MMLU_college_medicine":0.2658959538,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.24,
        "MMLU_conceptual_physics":0.2510638298,
        "MMLU_econometrics":0.201754386,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.2433862434,
        "MMLU_formal_logic":0.2380952381,
        "MMLU_global_facts":0.24,
        "MMLU_high_school_biology":0.1935483871,
        "MMLU_high_school_chemistry":0.1773399015,
        "MMLU_high_school_computer_science":0.19,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.1767676768,
        "MMLU_high_school_government_and_politics":0.1813471503,
        "MMLU_high_school_macroeconomics":0.2205128205,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.256302521,
        "MMLU_high_school_physics":0.357615894,
        "MMLU_high_school_psychology":0.1981651376,
        "MMLU_high_school_statistics":0.1712962963,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2827004219,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2644628099,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.1963190184,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.1844660194,
        "MMLU_marketing":0.2863247863,
        "MMLU_medical_genetics":0.31,
        "MMLU_miscellaneous":0.2375478927,
        "MMLU_moral_disputes":0.2398843931,
        "MMLU_moral_scenarios":0.2368715084,
        "MMLU_nutrition":0.2287581699,
        "MMLU_philosophy":0.1993569132,
        "MMLU_prehistory":0.212962963,
        "MMLU_professional_accounting":0.219858156,
        "MMLU_professional_law":0.2503259452,
        "MMLU_professional_medicine":0.1911764706,
        "MMLU_professional_psychology":0.227124183,
        "MMLU_public_relations":0.2545454545,
        "MMLU_security_studies":0.2448979592,
        "MMLU_sociology":0.2189054726,
        "MMLU_us_foreign_policy":0.31,
        "MMLU_virology":0.265060241,
        "MMLU_world_religions":0.3040935673
    },
    {
        "Model":"gpt2-medium-emailgen",
        "URL":"https:\/\/huggingface.co\/postbot\/gpt2-medium-emailgen",
        "full_model_name":"postbot\/gpt2-medium-emailgen",
        "Parameters":null,
        "MMLU_average":0.2410286878,
        "arc:challenge|25":0.2218430034,
        "hellaswag|10":0.3054172476,
        "MMLU_abstract_algebra":0.27,
        "MMLU_anatomy":0.2592592593,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.2377358491,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.23,
        "MMLU_college_computer_science":0.22,
        "MMLU_college_mathematics":0.22,
        "MMLU_college_medicine":0.2196531792,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.22,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.2068965517,
        "MMLU_elementary_mathematics":0.2407407407,
        "MMLU_formal_logic":0.2063492063,
        "MMLU_global_facts":0.19,
        "MMLU_high_school_biology":0.2677419355,
        "MMLU_high_school_chemistry":0.1625615764,
        "MMLU_high_school_computer_science":0.24,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.2171717172,
        "MMLU_high_school_government_and_politics":0.1968911917,
        "MMLU_high_school_macroeconomics":0.2179487179,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.2226890756,
        "MMLU_high_school_physics":0.298013245,
        "MMLU_high_school_psychology":0.2568807339,
        "MMLU_high_school_statistics":0.4583333333,
        "MMLU_high_school_us_history":0.2401960784,
        "MMLU_high_school_world_history":0.2278481013,
        "MMLU_human_aging":0.2556053812,
        "MMLU_human_sexuality":0.2519083969,
        "MMLU_international_law":0.2231404959,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2331288344,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.213592233,
        "MMLU_marketing":0.1794871795,
        "MMLU_medical_genetics":0.23,
        "MMLU_miscellaneous":0.2349936143,
        "MMLU_moral_disputes":0.2543352601,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2287581699,
        "MMLU_philosophy":0.2122186495,
        "MMLU_prehistory":0.2067901235,
        "MMLU_professional_accounting":0.2730496454,
        "MMLU_professional_law":0.2431551499,
        "MMLU_professional_medicine":0.4154411765,
        "MMLU_professional_psychology":0.2647058824,
        "MMLU_public_relations":0.2545454545,
        "MMLU_security_studies":0.2,
        "MMLU_sociology":0.2338308458,
        "MMLU_us_foreign_policy":0.3,
        "MMLU_virology":0.1987951807,
        "MMLU_world_religions":0.2397660819
    },
    {
        "Model":"Chinese-Alpaca-33B-SuperHOT-8K-fp16",
        "URL":"https:\/\/huggingface.co\/TheBloke\/Chinese-Alpaca-33B-SuperHOT-8K-fp16",
        "full_model_name":"TheBloke\/Chinese-Alpaca-33B-SuperHOT-8K-fp16",
        "Parameters":33.0,
        "MMLU_average":0.2406987768,
        "arc:challenge|25":0.2175767918,
        "hellaswag|10":0.2692690699,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.2222222222,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.31,
        "MMLU_clinical_knowledge":0.2188679245,
        "MMLU_college_biology":0.2222222222,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.15,
        "MMLU_college_mathematics":0.23,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.24,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2275132275,
        "MMLU_formal_logic":0.2222222222,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.2516129032,
        "MMLU_high_school_chemistry":0.236453202,
        "MMLU_high_school_computer_science":0.19,
        "MMLU_high_school_european_history":0.2727272727,
        "MMLU_high_school_geography":0.2121212121,
        "MMLU_high_school_government_and_politics":0.2176165803,
        "MMLU_high_school_macroeconomics":0.2128205128,
        "MMLU_high_school_mathematics":0.2666666667,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.1926605505,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2827004219,
        "MMLU_human_aging":0.2869955157,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.25,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2991452991,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2694763729,
        "MMLU_moral_disputes":0.2687861272,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2156862745,
        "MMLU_philosophy":0.1864951768,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2234042553,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.2058823529,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.2040816327,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.32,
        "MMLU_virology":0.265060241,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"smol_llama-81M-tied",
        "URL":"https:\/\/huggingface.co\/BEE-spoke-data\/smol_llama-81M-tied",
        "full_model_name":"BEE-spoke-data\/smol_llama-81M-tied",
        "Parameters":0.081,
        "MMLU_average":0.2406386477,
        "arc:challenge|25":0.1655290102,
        "hellaswag|10":0.2765385381,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.2962962963,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.19,
        "MMLU_clinical_knowledge":0.2264150943,
        "MMLU_college_biology":0.2291666667,
        "MMLU_college_chemistry":0.19,
        "MMLU_college_computer_science":0.23,
        "MMLU_college_mathematics":0.2,
        "MMLU_college_medicine":0.2023121387,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.22,
        "MMLU_conceptual_physics":0.2595744681,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2248677249,
        "MMLU_formal_logic":0.1587301587,
        "MMLU_global_facts":0.19,
        "MMLU_high_school_biology":0.2967741935,
        "MMLU_high_school_chemistry":0.2906403941,
        "MMLU_high_school_computer_science":0.27,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.2676767677,
        "MMLU_high_school_government_and_politics":0.2383419689,
        "MMLU_high_school_macroeconomics":0.2230769231,
        "MMLU_high_school_mathematics":0.2185185185,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.1920529801,
        "MMLU_high_school_psychology":0.2055045872,
        "MMLU_high_school_statistics":0.3981481481,
        "MMLU_high_school_us_history":0.2941176471,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.2466367713,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2809917355,
        "MMLU_jurisprudence":0.2314814815,
        "MMLU_logical_fallacies":0.2699386503,
        "MMLU_machine_learning":0.2321428571,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2222222222,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2503192848,
        "MMLU_moral_disputes":0.2456647399,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2222222222,
        "MMLU_philosophy":0.2025723473,
        "MMLU_prehistory":0.2407407407,
        "MMLU_professional_accounting":0.2234042553,
        "MMLU_professional_law":0.2477183833,
        "MMLU_professional_medicine":0.4044117647,
        "MMLU_professional_psychology":0.2614379085,
        "MMLU_public_relations":0.2090909091,
        "MMLU_security_studies":0.2367346939,
        "MMLU_sociology":0.2338308458,
        "MMLU_us_foreign_policy":0.23,
        "MMLU_virology":0.2891566265,
        "MMLU_world_religions":0.2105263158
    },
    {
        "Model":"open-llama-3b-claude-30k",
        "URL":"https:\/\/huggingface.co\/harborwater\/open-llama-3b-claude-30k",
        "full_model_name":"harborwater\/open-llama-3b-claude-30k",
        "Parameters":3.0,
        "MMLU_average":0.240284031,
        "arc:challenge|25":0.3984641638,
        "hellaswag|10":0.5435172276,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.1851851852,
        "MMLU_astronomy":0.2105263158,
        "MMLU_business_ethics":0.31,
        "MMLU_clinical_knowledge":0.2301886792,
        "MMLU_college_biology":0.25,
        "MMLU_college_chemistry":0.19,
        "MMLU_college_computer_science":0.28,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2196531792,
        "MMLU_college_physics":0.2450980392,
        "MMLU_computer_security":0.29,
        "MMLU_conceptual_physics":0.3106382979,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2068965517,
        "MMLU_elementary_mathematics":0.208994709,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.32,
        "MMLU_high_school_biology":0.1774193548,
        "MMLU_high_school_chemistry":0.157635468,
        "MMLU_high_school_computer_science":0.3,
        "MMLU_high_school_european_history":0.2424242424,
        "MMLU_high_school_geography":0.1767676768,
        "MMLU_high_school_government_and_politics":0.2020725389,
        "MMLU_high_school_macroeconomics":0.2025641026,
        "MMLU_high_school_mathematics":0.2185185185,
        "MMLU_high_school_microeconomics":0.2268907563,
        "MMLU_high_school_physics":0.2119205298,
        "MMLU_high_school_psychology":0.2055045872,
        "MMLU_high_school_statistics":0.1527777778,
        "MMLU_high_school_us_history":0.2254901961,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.331838565,
        "MMLU_human_sexuality":0.2748091603,
        "MMLU_international_law":0.2231404959,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.2767857143,
        "MMLU_management":0.145631068,
        "MMLU_marketing":0.3076923077,
        "MMLU_medical_genetics":0.31,
        "MMLU_miscellaneous":0.245210728,
        "MMLU_moral_disputes":0.2803468208,
        "MMLU_moral_scenarios":0.2245810056,
        "MMLU_nutrition":0.2156862745,
        "MMLU_philosophy":0.231511254,
        "MMLU_prehistory":0.2561728395,
        "MMLU_professional_accounting":0.2482269504,
        "MMLU_professional_law":0.2281616688,
        "MMLU_professional_medicine":0.1875,
        "MMLU_professional_psychology":0.2467320261,
        "MMLU_public_relations":0.2272727273,
        "MMLU_security_studies":0.2163265306,
        "MMLU_sociology":0.2338308458,
        "MMLU_us_foreign_policy":0.29,
        "MMLU_virology":0.2951807229,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"pythia-1.4b-deduped-sharegpt",
        "URL":"https:\/\/huggingface.co\/HWERI\/pythia-1.4b-deduped-sharegpt",
        "full_model_name":"HWERI\/pythia-1.4b-deduped-sharegpt",
        "Parameters":1.4,
        "MMLU_average":0.2399605543,
        "arc:challenge|25":0.2994880546,
        "hellaswag|10":0.4133638717,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.1851851852,
        "MMLU_astronomy":0.1447368421,
        "MMLU_business_ethics":0.31,
        "MMLU_clinical_knowledge":0.2830188679,
        "MMLU_college_biology":0.2361111111,
        "MMLU_college_chemistry":0.18,
        "MMLU_college_computer_science":0.28,
        "MMLU_college_mathematics":0.19,
        "MMLU_college_medicine":0.2312138728,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.3,
        "MMLU_conceptual_physics":0.2680851064,
        "MMLU_econometrics":0.201754386,
        "MMLU_electrical_engineering":0.2206896552,
        "MMLU_elementary_mathematics":0.2248677249,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.19,
        "MMLU_high_school_biology":0.2322580645,
        "MMLU_high_school_chemistry":0.1674876847,
        "MMLU_high_school_computer_science":0.18,
        "MMLU_high_school_european_history":0.2,
        "MMLU_high_school_geography":0.1717171717,
        "MMLU_high_school_government_and_politics":0.2227979275,
        "MMLU_high_school_macroeconomics":0.241025641,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.2394957983,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.2128440367,
        "MMLU_high_school_statistics":0.4212962963,
        "MMLU_high_school_us_history":0.2401960784,
        "MMLU_high_school_world_history":0.2489451477,
        "MMLU_human_aging":0.2780269058,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.2561983471,
        "MMLU_jurisprudence":0.25,
        "MMLU_logical_fallacies":0.2147239264,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.1844660194,
        "MMLU_marketing":0.2435897436,
        "MMLU_medical_genetics":0.33,
        "MMLU_miscellaneous":0.2413793103,
        "MMLU_moral_disputes":0.2716763006,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2450980392,
        "MMLU_philosophy":0.2090032154,
        "MMLU_prehistory":0.2592592593,
        "MMLU_professional_accounting":0.2092198582,
        "MMLU_professional_law":0.2451108214,
        "MMLU_professional_medicine":0.1875,
        "MMLU_professional_psychology":0.2647058824,
        "MMLU_public_relations":0.2090909091,
        "MMLU_security_studies":0.2530612245,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.24,
        "MMLU_virology":0.3012048193,
        "MMLU_world_religions":0.2807017544
    },
    {
        "Model":"pythia-1.4b-deduped-sharegpt",
        "URL":"https:\/\/huggingface.co\/beaugogh\/pythia-1.4b-deduped-sharegpt",
        "full_model_name":"beaugogh\/pythia-1.4b-deduped-sharegpt",
        "Parameters":1.4,
        "MMLU_average":0.2399605543,
        "arc:challenge|25":0.2994880546,
        "hellaswag|10":0.4133638717,
        "MMLU_abstract_algebra":0.24,
        "MMLU_anatomy":0.1851851852,
        "MMLU_astronomy":0.1447368421,
        "MMLU_business_ethics":0.31,
        "MMLU_clinical_knowledge":0.2830188679,
        "MMLU_college_biology":0.2361111111,
        "MMLU_college_chemistry":0.18,
        "MMLU_college_computer_science":0.28,
        "MMLU_college_mathematics":0.19,
        "MMLU_college_medicine":0.2312138728,
        "MMLU_college_physics":0.2058823529,
        "MMLU_computer_security":0.3,
        "MMLU_conceptual_physics":0.2680851064,
        "MMLU_econometrics":0.201754386,
        "MMLU_electrical_engineering":0.2206896552,
        "MMLU_elementary_mathematics":0.2248677249,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.19,
        "MMLU_high_school_biology":0.2322580645,
        "MMLU_high_school_chemistry":0.1674876847,
        "MMLU_high_school_computer_science":0.18,
        "MMLU_high_school_european_history":0.2,
        "MMLU_high_school_geography":0.1717171717,
        "MMLU_high_school_government_and_politics":0.2227979275,
        "MMLU_high_school_macroeconomics":0.241025641,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.2394957983,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.2128440367,
        "MMLU_high_school_statistics":0.4212962963,
        "MMLU_high_school_us_history":0.2401960784,
        "MMLU_high_school_world_history":0.2489451477,
        "MMLU_human_aging":0.2780269058,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.2561983471,
        "MMLU_jurisprudence":0.25,
        "MMLU_logical_fallacies":0.2147239264,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.1844660194,
        "MMLU_marketing":0.2435897436,
        "MMLU_medical_genetics":0.33,
        "MMLU_miscellaneous":0.2413793103,
        "MMLU_moral_disputes":0.2716763006,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2450980392,
        "MMLU_philosophy":0.2090032154,
        "MMLU_prehistory":0.2592592593,
        "MMLU_professional_accounting":0.2092198582,
        "MMLU_professional_law":0.2451108214,
        "MMLU_professional_medicine":0.1875,
        "MMLU_professional_psychology":0.2647058824,
        "MMLU_public_relations":0.2090909091,
        "MMLU_security_studies":0.2530612245,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.24,
        "MMLU_virology":0.3012048193,
        "MMLU_world_religions":0.2807017544
    },
    {
        "Model":"bloom-560m-RLHF-v2",
        "URL":"https:\/\/huggingface.co\/TheTravellingEngineer\/bloom-560m-RLHF-v2",
        "full_model_name":"TheTravellingEngineer\/bloom-560m-RLHF-v2",
        "Parameters":0.56,
        "MMLU_average":0.2394738105,
        "arc:challenge|25":0.2372013652,
        "hellaswag|10":0.3217486556,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.2296296296,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.19,
        "MMLU_clinical_knowledge":0.2339622642,
        "MMLU_college_biology":0.25,
        "MMLU_college_chemistry":0.19,
        "MMLU_college_computer_science":0.35,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.27,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2620689655,
        "MMLU_elementary_mathematics":0.2380952381,
        "MMLU_formal_logic":0.2142857143,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.1967741935,
        "MMLU_high_school_chemistry":0.1822660099,
        "MMLU_high_school_computer_science":0.27,
        "MMLU_high_school_european_history":0.2363636364,
        "MMLU_high_school_geography":0.1818181818,
        "MMLU_high_school_government_and_politics":0.2020725389,
        "MMLU_high_school_macroeconomics":0.2128205128,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.1974789916,
        "MMLU_high_school_physics":0.2052980132,
        "MMLU_high_school_psychology":0.2,
        "MMLU_high_school_statistics":0.4027777778,
        "MMLU_high_school_us_history":0.1911764706,
        "MMLU_high_school_world_history":0.2320675105,
        "MMLU_human_aging":0.2421524664,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2314049587,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.2392638037,
        "MMLU_machine_learning":0.3928571429,
        "MMLU_management":0.1844660194,
        "MMLU_marketing":0.2521367521,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2426564496,
        "MMLU_moral_disputes":0.2543352601,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2483660131,
        "MMLU_philosophy":0.1993569132,
        "MMLU_prehistory":0.2067901235,
        "MMLU_professional_accounting":0.2375886525,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.3676470588,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2363636364,
        "MMLU_security_studies":0.2163265306,
        "MMLU_sociology":0.2587064677,
        "MMLU_us_foreign_policy":0.19,
        "MMLU_virology":0.2951807229,
        "MMLU_world_religions":0.2397660819
    },
    {
        "Model":"openbuddy-openllama-3b-v10-bf16",
        "URL":"https:\/\/huggingface.co\/OpenBuddy\/openbuddy-openllama-3b-v10-bf16",
        "full_model_name":"OpenBuddy\/openbuddy-openllama-3b-v10-bf16",
        "Parameters":3.0,
        "MMLU_average":0.2388552986,
        "arc:challenge|25":0.3344709898,
        "hellaswag|10":0.4537940649,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.1925925926,
        "MMLU_astronomy":0.1973684211,
        "MMLU_business_ethics":0.32,
        "MMLU_clinical_knowledge":0.241509434,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.25,
        "MMLU_college_mathematics":0.27,
        "MMLU_college_medicine":0.225433526,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.3,
        "MMLU_conceptual_physics":0.2680851064,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2116402116,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.1935483871,
        "MMLU_high_school_chemistry":0.1625615764,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2242424242,
        "MMLU_high_school_geography":0.2424242424,
        "MMLU_high_school_government_and_politics":0.1865284974,
        "MMLU_high_school_macroeconomics":0.2102564103,
        "MMLU_high_school_mathematics":0.2259259259,
        "MMLU_high_school_microeconomics":0.2142857143,
        "MMLU_high_school_physics":0.238410596,
        "MMLU_high_school_psychology":0.1981651376,
        "MMLU_high_school_statistics":0.1666666667,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2827004219,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2671755725,
        "MMLU_international_law":0.2231404959,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2948717949,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2375478927,
        "MMLU_moral_disputes":0.2572254335,
        "MMLU_moral_scenarios":0.2536312849,
        "MMLU_nutrition":0.2189542484,
        "MMLU_philosophy":0.1864951768,
        "MMLU_prehistory":0.225308642,
        "MMLU_professional_accounting":0.2304964539,
        "MMLU_professional_law":0.240547588,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2272727273,
        "MMLU_security_studies":0.212244898,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.3,
        "MMLU_virology":0.2891566265,
        "MMLU_world_religions":0.3157894737
    },
    {
        "Model":"medical_transcription_generator",
        "URL":"https:\/\/huggingface.co\/alibidaran\/medical_transcription_generator",
        "full_model_name":"alibidaran\/medical_transcription_generator",
        "Parameters":null,
        "MMLU_average":0.2383650104,
        "arc:challenge|25":0.2030716724,
        "hellaswag|10":0.2909778929,
        "MMLU_abstract_algebra":0.19,
        "MMLU_anatomy":0.2518518519,
        "MMLU_astronomy":0.2236842105,
        "MMLU_business_ethics":0.38,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2291666667,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.27,
        "MMLU_college_mathematics":0.22,
        "MMLU_college_medicine":0.2023121387,
        "MMLU_college_physics":0.1862745098,
        "MMLU_computer_security":0.25,
        "MMLU_conceptual_physics":0.314893617,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.1862068966,
        "MMLU_elementary_mathematics":0.2195767196,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.21,
        "MMLU_high_school_biology":0.2387096774,
        "MMLU_high_school_chemistry":0.1231527094,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2242424242,
        "MMLU_high_school_geography":0.1818181818,
        "MMLU_high_school_government_and_politics":0.2797927461,
        "MMLU_high_school_macroeconomics":0.2025641026,
        "MMLU_high_school_mathematics":0.2555555556,
        "MMLU_high_school_microeconomics":0.2142857143,
        "MMLU_high_school_physics":0.2715231788,
        "MMLU_high_school_psychology":0.2862385321,
        "MMLU_high_school_statistics":0.1481481481,
        "MMLU_high_school_us_history":0.2303921569,
        "MMLU_high_school_world_history":0.223628692,
        "MMLU_human_aging":0.3452914798,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2148760331,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.2760736196,
        "MMLU_machine_learning":0.25,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2692307692,
        "MMLU_medical_genetics":0.29,
        "MMLU_miscellaneous":0.2426564496,
        "MMLU_moral_disputes":0.2514450867,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2385620915,
        "MMLU_philosophy":0.2475884244,
        "MMLU_prehistory":0.212962963,
        "MMLU_professional_accounting":0.2482269504,
        "MMLU_professional_law":0.2379400261,
        "MMLU_professional_medicine":0.1617647059,
        "MMLU_professional_psychology":0.2630718954,
        "MMLU_public_relations":0.1909090909,
        "MMLU_security_studies":0.2163265306,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.24,
        "MMLU_virology":0.2590361446,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"DialoGPT-large",
        "URL":"https:\/\/huggingface.co\/microsoft\/DialoGPT-large",
        "full_model_name":"microsoft\/DialoGPT-large",
        "Parameters":null,
        "MMLU_average":0.2381366301,
        "arc:challenge|25":0.2090443686,
        "hellaswag|10":0.2567217686,
        "MMLU_abstract_algebra":0.21,
        "MMLU_anatomy":0.2518518519,
        "MMLU_astronomy":0.3355263158,
        "MMLU_business_ethics":0.26,
        "MMLU_clinical_knowledge":0.2452830189,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.22,
        "MMLU_college_computer_science":0.21,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2138728324,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.21,
        "MMLU_conceptual_physics":0.3234042553,
        "MMLU_econometrics":0.2894736842,
        "MMLU_electrical_engineering":0.2275862069,
        "MMLU_elementary_mathematics":0.2566137566,
        "MMLU_formal_logic":0.253968254,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.2451612903,
        "MMLU_high_school_chemistry":0.2709359606,
        "MMLU_high_school_computer_science":0.2,
        "MMLU_high_school_european_history":0.2121212121,
        "MMLU_high_school_geography":0.2171717172,
        "MMLU_high_school_government_and_politics":0.1917098446,
        "MMLU_high_school_macroeconomics":0.2333333333,
        "MMLU_high_school_mathematics":0.2037037037,
        "MMLU_high_school_microeconomics":0.2016806723,
        "MMLU_high_school_physics":0.2052980132,
        "MMLU_high_school_psychology":0.2403669725,
        "MMLU_high_school_statistics":0.1527777778,
        "MMLU_high_school_us_history":0.2401960784,
        "MMLU_high_school_world_history":0.2362869198,
        "MMLU_human_aging":0.3766816143,
        "MMLU_human_sexuality":0.2366412214,
        "MMLU_international_law":0.2479338843,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.245398773,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.2524271845,
        "MMLU_marketing":0.264957265,
        "MMLU_medical_genetics":0.25,
        "MMLU_miscellaneous":0.2669220945,
        "MMLU_moral_disputes":0.2341040462,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2287581699,
        "MMLU_philosophy":0.1864951768,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2517730496,
        "MMLU_professional_law":0.2438070404,
        "MMLU_professional_medicine":0.1911764706,
        "MMLU_professional_psychology":0.2565359477,
        "MMLU_public_relations":0.1818181818,
        "MMLU_security_studies":0.2775510204,
        "MMLU_sociology":0.2388059701,
        "MMLU_us_foreign_policy":0.21,
        "MMLU_virology":0.313253012,
        "MMLU_world_religions":0.1871345029
    },
    {
        "Model":"DialoGPT-sarcastic-medium",
        "URL":"https:\/\/huggingface.co\/abhiramtirumala\/DialoGPT-sarcastic-medium",
        "full_model_name":"abhiramtirumala\/DialoGPT-sarcastic-medium",
        "Parameters":null,
        "MMLU_average":0.2376449428,
        "arc:challenge|25":0.2039249147,
        "hellaswag|10":0.2560246963,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.2888888889,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.28,
        "MMLU_clinical_knowledge":0.2188679245,
        "MMLU_college_biology":0.3263888889,
        "MMLU_college_chemistry":0.18,
        "MMLU_college_computer_science":0.25,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2427745665,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.25,
        "MMLU_conceptual_physics":0.2808510638,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2513227513,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.23,
        "MMLU_high_school_biology":0.1935483871,
        "MMLU_high_school_chemistry":0.3054187192,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.2424242424,
        "MMLU_high_school_government_and_politics":0.1968911917,
        "MMLU_high_school_macroeconomics":0.2051282051,
        "MMLU_high_school_mathematics":0.2111111111,
        "MMLU_high_school_microeconomics":0.218487395,
        "MMLU_high_school_physics":0.2119205298,
        "MMLU_high_school_psychology":0.2348623853,
        "MMLU_high_school_statistics":0.1527777778,
        "MMLU_high_school_us_history":0.2647058824,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.2062780269,
        "MMLU_human_sexuality":0.2671755725,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1844660194,
        "MMLU_marketing":0.3076923077,
        "MMLU_medical_genetics":0.28,
        "MMLU_miscellaneous":0.2490421456,
        "MMLU_moral_disputes":0.2687861272,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2222222222,
        "MMLU_philosophy":0.2090032154,
        "MMLU_prehistory":0.2530864198,
        "MMLU_professional_accounting":0.2340425532,
        "MMLU_professional_law":0.2529335072,
        "MMLU_professional_medicine":0.1764705882,
        "MMLU_professional_psychology":0.2614379085,
        "MMLU_public_relations":0.2,
        "MMLU_security_studies":0.1959183673,
        "MMLU_sociology":0.2388059701,
        "MMLU_us_foreign_policy":0.29,
        "MMLU_virology":0.2409638554,
        "MMLU_world_religions":0.2105263158
    },
    {
        "Model":"Llama-2-7b-longlora-16k-ft",
        "URL":"https:\/\/huggingface.co\/Yukang\/Llama-2-7b-longlora-16k-ft",
        "full_model_name":"Yukang\/Llama-2-7b-longlora-16k-ft",
        "Parameters":7.0,
        "MMLU_average":0.2374748767,
        "arc:challenge|25":0.2005119454,
        "hellaswag|10":0.2573192591,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.2,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2075471698,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.208994709,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.1774193548,
        "MMLU_high_school_chemistry":0.1527093596,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.1767676768,
        "MMLU_high_school_government_and_politics":0.1968911917,
        "MMLU_high_school_macroeconomics":0.2076923077,
        "MMLU_high_school_mathematics":0.2111111111,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.1926605505,
        "MMLU_high_school_statistics":0.4722222222,
        "MMLU_high_school_us_history":0.2352941176,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3094170404,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2991452991,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2349936143,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.1864951768,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2340425532,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.2285714286,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"CAMEL-33B-Combined-Data-SuperHOT-8K-fp16",
        "URL":"https:\/\/huggingface.co\/TheBloke\/CAMEL-33B-Combined-Data-SuperHOT-8K-fp16",
        "full_model_name":"TheBloke\/CAMEL-33B-Combined-Data-SuperHOT-8K-fp16",
        "Parameters":33.0,
        "MMLU_average":0.2368632057,
        "arc:challenge|25":0.2278156997,
        "hellaswag|10":0.2765385381,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.2148148148,
        "MMLU_astronomy":0.1973684211,
        "MMLU_business_ethics":0.33,
        "MMLU_clinical_knowledge":0.2037735849,
        "MMLU_college_biology":0.25,
        "MMLU_college_chemistry":0.22,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2138728324,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.33,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.208994709,
        "MMLU_formal_logic":0.2777777778,
        "MMLU_global_facts":0.22,
        "MMLU_high_school_biology":0.1870967742,
        "MMLU_high_school_chemistry":0.1527093596,
        "MMLU_high_school_computer_science":0.27,
        "MMLU_high_school_european_history":0.2242424242,
        "MMLU_high_school_geography":0.1767676768,
        "MMLU_high_school_government_and_politics":0.2020725389,
        "MMLU_high_school_macroeconomics":0.2076923077,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.1932773109,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.1908256881,
        "MMLU_high_school_statistics":0.162037037,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.2658227848,
        "MMLU_human_aging":0.3049327354,
        "MMLU_human_sexuality":0.2519083969,
        "MMLU_international_law":0.2561983471,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1844660194,
        "MMLU_marketing":0.2735042735,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2528735632,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2480446927,
        "MMLU_nutrition":0.2156862745,
        "MMLU_philosophy":0.1832797428,
        "MMLU_prehistory":0.212962963,
        "MMLU_professional_accounting":0.2872340426,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.2467320261,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.2571428571,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.265060241,
        "MMLU_world_religions":0.3040935673
    },
    {
        "Model":"bloom-560m-RLHF",
        "URL":"https:\/\/huggingface.co\/TheTravellingEngineer\/bloom-560m-RLHF",
        "full_model_name":"TheTravellingEngineer\/bloom-560m-RLHF",
        "Parameters":0.56,
        "MMLU_average":0.2362667305,
        "arc:challenge|25":0.2116040956,
        "hellaswag|10":0.3173670584,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.1851851852,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.27,
        "MMLU_clinical_knowledge":0.2452830189,
        "MMLU_college_biology":0.2361111111,
        "MMLU_college_chemistry":0.19,
        "MMLU_college_computer_science":0.29,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.27,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.201754386,
        "MMLU_electrical_engineering":0.2689655172,
        "MMLU_elementary_mathematics":0.246031746,
        "MMLU_formal_logic":0.1825396825,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.1774193548,
        "MMLU_high_school_chemistry":0.157635468,
        "MMLU_high_school_computer_science":0.26,
        "MMLU_high_school_european_history":0.2121212121,
        "MMLU_high_school_geography":0.1767676768,
        "MMLU_high_school_government_and_politics":0.1968911917,
        "MMLU_high_school_macroeconomics":0.2205128205,
        "MMLU_high_school_mathematics":0.2333333333,
        "MMLU_high_school_microeconomics":0.1722689076,
        "MMLU_high_school_physics":0.2251655629,
        "MMLU_high_school_psychology":0.1963302752,
        "MMLU_high_school_statistics":0.3240740741,
        "MMLU_high_school_us_history":0.1960784314,
        "MMLU_high_school_world_history":0.2616033755,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2231404959,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2147239264,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2528735632,
        "MMLU_moral_disputes":0.2543352601,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2450980392,
        "MMLU_philosophy":0.192926045,
        "MMLU_prehistory":0.2098765432,
        "MMLU_professional_accounting":0.2304964539,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.3897058824,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.1836734694,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.29,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.2573099415
    },
    {
        "Model":"Vicuna-33B-1-3-SuperHOT-8K-fp16",
        "URL":"https:\/\/huggingface.co\/TheBloke\/Vicuna-33B-1-3-SuperHOT-8K-fp16",
        "full_model_name":"TheBloke\/Vicuna-33B-1-3-SuperHOT-8K-fp16",
        "Parameters":33.0,
        "MMLU_average":0.2362206285,
        "arc:challenge|25":0.2133105802,
        "hellaswag|10":0.2882891854,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.2444444444,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.34,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.25,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.24,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.3,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2275132275,
        "MMLU_formal_logic":0.2063492063,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.2838709677,
        "MMLU_high_school_chemistry":0.2463054187,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2727272727,
        "MMLU_high_school_geography":0.1818181818,
        "MMLU_high_school_government_and_politics":0.1968911917,
        "MMLU_high_school_macroeconomics":0.2051282051,
        "MMLU_high_school_mathematics":0.237037037,
        "MMLU_high_school_microeconomics":0.2352941176,
        "MMLU_high_school_physics":0.1920529801,
        "MMLU_high_school_psychology":0.2146788991,
        "MMLU_high_school_statistics":0.2638888889,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.2421524664,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.226993865,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2393162393,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2554278416,
        "MMLU_moral_disputes":0.2052023121,
        "MMLU_moral_scenarios":0.2391061453,
        "MMLU_nutrition":0.2156862745,
        "MMLU_philosophy":0.1864951768,
        "MMLU_prehistory":0.2191358025,
        "MMLU_professional_accounting":0.2482269504,
        "MMLU_professional_law":0.2477183833,
        "MMLU_professional_medicine":0.2132352941,
        "MMLU_professional_psychology":0.2549019608,
        "MMLU_public_relations":0.2,
        "MMLU_security_studies":0.1918367347,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.23,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.2222222222
    },
    {
        "Model":"Platypus-30B-SuperHOT-8K-fp16",
        "URL":"https:\/\/huggingface.co\/TheBloke\/Platypus-30B-SuperHOT-8K-fp16",
        "full_model_name":"TheBloke\/Platypus-30B-SuperHOT-8K-fp16",
        "Parameters":30.0,
        "MMLU_average":0.2361479932,
        "arc:challenge|25":0.2184300341,
        "hellaswag|10":0.2731527584,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.2074074074,
        "MMLU_astronomy":0.2039473684,
        "MMLU_business_ethics":0.32,
        "MMLU_clinical_knowledge":0.2264150943,
        "MMLU_college_biology":0.2361111111,
        "MMLU_college_chemistry":0.17,
        "MMLU_college_computer_science":0.24,
        "MMLU_college_mathematics":0.2,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2352941176,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2595744681,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2482758621,
        "MMLU_elementary_mathematics":0.208994709,
        "MMLU_formal_logic":0.2777777778,
        "MMLU_global_facts":0.16,
        "MMLU_high_school_biology":0.2548387097,
        "MMLU_high_school_chemistry":0.1822660099,
        "MMLU_high_school_computer_science":0.28,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.1868686869,
        "MMLU_high_school_government_and_politics":0.2124352332,
        "MMLU_high_school_macroeconomics":0.2333333333,
        "MMLU_high_school_mathematics":0.2037037037,
        "MMLU_high_school_microeconomics":0.2142857143,
        "MMLU_high_school_physics":0.1721854305,
        "MMLU_high_school_psychology":0.1981651376,
        "MMLU_high_school_statistics":0.2037037037,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2827004219,
        "MMLU_human_aging":0.3094170404,
        "MMLU_human_sexuality":0.2977099237,
        "MMLU_international_law":0.2479338843,
        "MMLU_jurisprudence":0.25,
        "MMLU_logical_fallacies":0.245398773,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.251596424,
        "MMLU_moral_disputes":0.2427745665,
        "MMLU_moral_scenarios":0.2391061453,
        "MMLU_nutrition":0.2287581699,
        "MMLU_philosophy":0.2347266881,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2304964539,
        "MMLU_professional_law":0.2496740548,
        "MMLU_professional_medicine":0.1948529412,
        "MMLU_professional_psychology":0.2630718954,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.1795918367,
        "MMLU_sociology":0.2388059701,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.2771084337,
        "MMLU_world_religions":0.2573099415
    },
    {
        "Model":"airoboros-33B-gpt4-1-4-SuperHOT-8K-fp16",
        "URL":"https:\/\/huggingface.co\/TheBloke\/airoboros-33B-gpt4-1-4-SuperHOT-8K-fp16",
        "full_model_name":"TheBloke\/airoboros-33B-gpt4-1-4-SuperHOT-8K-fp16",
        "Parameters":33.0,
        "MMLU_average":0.2357333938,
        "arc:challenge|25":0.2303754266,
        "hellaswag|10":0.2709619598,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.2592592593,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.32,
        "MMLU_clinical_knowledge":0.2226415094,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.27,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.208994709,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.2677419355,
        "MMLU_high_school_chemistry":0.157635468,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2242424242,
        "MMLU_high_school_geography":0.1868686869,
        "MMLU_high_school_government_and_politics":0.1865284974,
        "MMLU_high_school_macroeconomics":0.2076923077,
        "MMLU_high_school_mathematics":0.2407407407,
        "MMLU_high_school_microeconomics":0.2521008403,
        "MMLU_high_school_physics":0.2119205298,
        "MMLU_high_school_psychology":0.1944954128,
        "MMLU_high_school_statistics":0.1527777778,
        "MMLU_high_school_us_history":0.2401960784,
        "MMLU_high_school_world_history":0.2742616034,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2777777778,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2991452991,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2337164751,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2469273743,
        "MMLU_nutrition":0.1830065359,
        "MMLU_philosophy":0.1864951768,
        "MMLU_prehistory":0.2191358025,
        "MMLU_professional_accounting":0.2269503546,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.2532679739,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.1795918367,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.3157894737
    },
    {
        "Model":"gpt-Youtube",
        "URL":"https:\/\/huggingface.co\/BreadAi\/gpt-Youtube",
        "full_model_name":"BreadAi\/gpt-Youtube",
        "Parameters":null,
        "MMLU_average":0.2353783133,
        "arc:challenge|25":0.2278156997,
        "hellaswag|10":0.2568213503,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.2814814815,
        "MMLU_astronomy":0.1842105263,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2188679245,
        "MMLU_college_biology":0.25,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.23,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.3,
        "MMLU_conceptual_physics":0.2680851064,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2551724138,
        "MMLU_elementary_mathematics":0.2380952381,
        "MMLU_formal_logic":0.2698412698,
        "MMLU_global_facts":0.19,
        "MMLU_high_school_biology":0.1838709677,
        "MMLU_high_school_chemistry":0.157635468,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.1868686869,
        "MMLU_high_school_government_and_politics":0.1865284974,
        "MMLU_high_school_macroeconomics":0.2076923077,
        "MMLU_high_school_mathematics":0.2296296296,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.2185430464,
        "MMLU_high_school_psychology":0.1944954128,
        "MMLU_high_school_statistics":0.1805555556,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3183856502,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2231404959,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2147239264,
        "MMLU_machine_learning":0.3214285714,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2349936143,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2156862745,
        "MMLU_philosophy":0.1961414791,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2730496454,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.1801470588,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.2537313433,
        "MMLU_us_foreign_policy":0.24,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.3391812865
    },
    {
        "Model":"Platypus2-70B-instruct-4bit-gptq",
        "URL":"https:\/\/huggingface.co\/malhajar\/Platypus2-70B-instruct-4bit-gptq",
        "full_model_name":"malhajar\/Platypus2-70B-instruct-4bit-gptq",
        "Parameters":70.0,
        "MMLU_average":0.2353148003,
        "arc:challenge|25":0.2363481229,
        "hellaswag|10":0.2560246963,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.1851851852,
        "MMLU_astronomy":0.2171052632,
        "MMLU_business_ethics":0.29,
        "MMLU_clinical_knowledge":0.2113207547,
        "MMLU_college_biology":0.2430555556,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.25,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2196531792,
        "MMLU_college_physics":0.2647058824,
        "MMLU_computer_security":0.29,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2719298246,
        "MMLU_electrical_engineering":0.2482758621,
        "MMLU_elementary_mathematics":0.2380952381,
        "MMLU_formal_logic":0.1904761905,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.1903225806,
        "MMLU_high_school_chemistry":0.1822660099,
        "MMLU_high_school_computer_science":0.28,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.1767676768,
        "MMLU_high_school_government_and_politics":0.2020725389,
        "MMLU_high_school_macroeconomics":0.2,
        "MMLU_high_school_mathematics":0.2444444444,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.1981651376,
        "MMLU_high_school_statistics":0.1203703704,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2742616034,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2644628099,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2331288344,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2388250319,
        "MMLU_moral_disputes":0.2543352601,
        "MMLU_moral_scenarios":0.2491620112,
        "MMLU_nutrition":0.2320261438,
        "MMLU_philosophy":0.1864951768,
        "MMLU_prehistory":0.2654320988,
        "MMLU_professional_accounting":0.2092198582,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.1691176471,
        "MMLU_professional_psychology":0.2565359477,
        "MMLU_public_relations":0.2090909091,
        "MMLU_security_studies":0.2081632653,
        "MMLU_sociology":0.2487562189,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"llama-30b-supercot-SuperHOT-8K-fp16",
        "URL":"https:\/\/huggingface.co\/TheBloke\/llama-30b-supercot-SuperHOT-8K-fp16",
        "full_model_name":"TheBloke\/llama-30b-supercot-SuperHOT-8K-fp16",
        "Parameters":30.0,
        "MMLU_average":0.2350036905,
        "arc:challenge|25":0.228668942,
        "hellaswag|10":0.2740489942,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.1851851852,
        "MMLU_astronomy":0.1842105263,
        "MMLU_business_ethics":0.32,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.21,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2275862069,
        "MMLU_elementary_mathematics":0.2142857143,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.2483870968,
        "MMLU_high_school_chemistry":0.1822660099,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.1818181818,
        "MMLU_high_school_government_and_politics":0.2435233161,
        "MMLU_high_school_macroeconomics":0.2205128205,
        "MMLU_high_school_mathematics":0.2222222222,
        "MMLU_high_school_microeconomics":0.2226890756,
        "MMLU_high_school_physics":0.178807947,
        "MMLU_high_school_psychology":0.176146789,
        "MMLU_high_school_statistics":0.1898148148,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3228699552,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2085889571,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2311621967,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2516339869,
        "MMLU_philosophy":0.1864951768,
        "MMLU_prehistory":0.2191358025,
        "MMLU_professional_accounting":0.2304964539,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"Llama-2-7b-longlora-100k-ft",
        "URL":"https:\/\/huggingface.co\/Yukang\/Llama-2-7b-longlora-100k-ft",
        "full_model_name":"Yukang\/Llama-2-7b-longlora-100k-ft",
        "Parameters":7.0,
        "MMLU_average":0.234765982,
        "arc:challenge|25":0.2005119454,
        "hellaswag|10":0.2568213503,
        "MMLU_abstract_algebra":0.25,
        "MMLU_anatomy":0.1777777778,
        "MMLU_astronomy":0.1842105263,
        "MMLU_business_ethics":0.28,
        "MMLU_clinical_knowledge":0.2226415094,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.16,
        "MMLU_college_computer_science":0.22,
        "MMLU_college_mathematics":0.23,
        "MMLU_college_medicine":0.2023121387,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2344827586,
        "MMLU_elementary_mathematics":0.208994709,
        "MMLU_formal_logic":0.2698412698,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.1709677419,
        "MMLU_high_school_chemistry":0.1871921182,
        "MMLU_high_school_computer_science":0.31,
        "MMLU_high_school_european_history":0.2666666667,
        "MMLU_high_school_geography":0.2272727273,
        "MMLU_high_school_government_and_politics":0.207253886,
        "MMLU_high_school_macroeconomics":0.2461538462,
        "MMLU_high_school_mathematics":0.2185185185,
        "MMLU_high_school_microeconomics":0.1764705882,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.1926605505,
        "MMLU_high_school_statistics":0.162037037,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.2573839662,
        "MMLU_human_aging":0.201793722,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.3719008264,
        "MMLU_jurisprudence":0.2407407407,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2948717949,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2375478927,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.243575419,
        "MMLU_nutrition":0.2516339869,
        "MMLU_philosophy":0.1993569132,
        "MMLU_prehistory":0.200617284,
        "MMLU_professional_accounting":0.2340425532,
        "MMLU_professional_law":0.2568448501,
        "MMLU_professional_medicine":0.1654411765,
        "MMLU_professional_psychology":0.2434640523,
        "MMLU_public_relations":0.2363636364,
        "MMLU_security_studies":0.2040816327,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"Wizard-Vicuna-30B-Superhot-8K-fp16",
        "URL":"https:\/\/huggingface.co\/TheBloke\/Wizard-Vicuna-30B-Superhot-8K-fp16",
        "full_model_name":"TheBloke\/Wizard-Vicuna-30B-Superhot-8K-fp16",
        "Parameters":30.0,
        "MMLU_average":0.2345755863,
        "arc:challenge|25":0.2252559727,
        "hellaswag|10":0.2804222266,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.1925925926,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.1924528302,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2486772487,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.1806451613,
        "MMLU_high_school_chemistry":0.1527093596,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2606060606,
        "MMLU_high_school_geography":0.1767676768,
        "MMLU_high_school_government_and_politics":0.1865284974,
        "MMLU_high_school_macroeconomics":0.2128205128,
        "MMLU_high_school_mathematics":0.2481481481,
        "MMLU_high_school_microeconomics":0.2394957983,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.1926605505,
        "MMLU_high_school_statistics":0.2361111111,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2742616034,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2735042735,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2375478927,
        "MMLU_moral_disputes":0.2427745665,
        "MMLU_moral_scenarios":0.2458100559,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.1832797428,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2340425532,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2090909091,
        "MMLU_security_studies":0.2204081633,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.2865497076
    },
    {
        "Model":"RWKV-4-PilePlus-169M-20230520-done-ctx4096",
        "URL":"https:\/\/huggingface.co\/KnutJaegersberg\/RWKV-4-PilePlus-169M-20230520-done-ctx4096",
        "full_model_name":"KnutJaegersberg\/RWKV-4-PilePlus-169M-20230520-done-ctx4096",
        "Parameters":0.169,
        "MMLU_average":0.233683581,
        "arc:challenge|25":0.1936860068,
        "hellaswag|10":0.2915753834,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.1925925926,
        "MMLU_astronomy":0.1973684211,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2777777778,
        "MMLU_college_chemistry":0.19,
        "MMLU_college_computer_science":0.24,
        "MMLU_college_mathematics":0.22,
        "MMLU_college_medicine":0.2196531792,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.32,
        "MMLU_conceptual_physics":0.2595744681,
        "MMLU_econometrics":0.2105263158,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.2380952381,
        "MMLU_formal_logic":0.2619047619,
        "MMLU_global_facts":0.2,
        "MMLU_high_school_biology":0.1774193548,
        "MMLU_high_school_chemistry":0.1527093596,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2242424242,
        "MMLU_high_school_geography":0.1868686869,
        "MMLU_high_school_government_and_politics":0.1968911917,
        "MMLU_high_school_macroeconomics":0.2076923077,
        "MMLU_high_school_mathematics":0.2333333333,
        "MMLU_high_school_microeconomics":0.1848739496,
        "MMLU_high_school_physics":0.178807947,
        "MMLU_high_school_psychology":0.1944954128,
        "MMLU_high_school_statistics":0.1666666667,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2489451477,
        "MMLU_human_aging":0.3049327354,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2644628099,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2147239264,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.1941747573,
        "MMLU_marketing":0.2564102564,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2503192848,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2391061453,
        "MMLU_nutrition":0.2647058824,
        "MMLU_philosophy":0.1864951768,
        "MMLU_prehistory":0.225308642,
        "MMLU_professional_accounting":0.2375886525,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.1875,
        "MMLU_professional_psychology":0.2565359477,
        "MMLU_public_relations":0.2363636364,
        "MMLU_security_studies":0.1959183673,
        "MMLU_sociology":0.223880597,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.313253012,
        "MMLU_world_religions":0.3040935673
    },
    {
        "Model":"TinyStories-Alpaca",
        "URL":"https:\/\/huggingface.co\/blueapple8259\/TinyStories-Alpaca",
        "full_model_name":"blueapple8259\/TinyStories-Alpaca",
        "Parameters":null,
        "MMLU_average":0.2335496521,
        "arc:challenge|25":0.2039249147,
        "hellaswag|10":0.2578171679,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.2962962963,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.29,
        "MMLU_clinical_knowledge":0.2226415094,
        "MMLU_college_biology":0.2152777778,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.17,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.2543352601,
        "MMLU_college_physics":0.2254901961,
        "MMLU_computer_security":0.26,
        "MMLU_conceptual_physics":0.2553191489,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2275862069,
        "MMLU_elementary_mathematics":0.2698412698,
        "MMLU_formal_logic":0.2380952381,
        "MMLU_global_facts":0.26,
        "MMLU_high_school_biology":0.1903225806,
        "MMLU_high_school_chemistry":0.2709359606,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.1565656566,
        "MMLU_high_school_government_and_politics":0.1554404145,
        "MMLU_high_school_macroeconomics":0.2076923077,
        "MMLU_high_school_mathematics":0.2296296296,
        "MMLU_high_school_microeconomics":0.2605042017,
        "MMLU_high_school_physics":0.1854304636,
        "MMLU_high_school_psychology":0.2018348624,
        "MMLU_high_school_statistics":0.125,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2573839662,
        "MMLU_human_aging":0.197309417,
        "MMLU_human_sexuality":0.2977099237,
        "MMLU_international_law":0.3305785124,
        "MMLU_jurisprudence":0.2685185185,
        "MMLU_logical_fallacies":0.2883435583,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2393162393,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2490421456,
        "MMLU_moral_disputes":0.2427745665,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2026143791,
        "MMLU_philosophy":0.2025723473,
        "MMLU_prehistory":0.2345679012,
        "MMLU_professional_accounting":0.2163120567,
        "MMLU_professional_law":0.2464146023,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.2483660131,
        "MMLU_public_relations":0.2,
        "MMLU_security_studies":0.1959183673,
        "MMLU_sociology":0.2189054726,
        "MMLU_us_foreign_policy":0.26,
        "MMLU_virology":0.2108433735,
        "MMLU_world_religions":0.298245614
    },
    {
        "Model":"Llama-2-3b-hf",
        "URL":"https:\/\/huggingface.co\/winglian\/Llama-2-3b-hf",
        "full_model_name":"winglian\/Llama-2-3b-hf",
        "Parameters":3.0,
        "MMLU_average":0.2333223627,
        "arc:challenge|25":0.2226962457,
        "hellaswag|10":0.2604062936,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.1851851852,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.29,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.1965317919,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.29,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2543859649,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.208994709,
        "MMLU_formal_logic":0.2380952381,
        "MMLU_global_facts":0.17,
        "MMLU_high_school_biology":0.1774193548,
        "MMLU_high_school_chemistry":0.157635468,
        "MMLU_high_school_computer_science":0.26,
        "MMLU_high_school_european_history":0.2727272727,
        "MMLU_high_school_geography":0.1767676768,
        "MMLU_high_school_government_and_politics":0.1968911917,
        "MMLU_high_school_macroeconomics":0.2025641026,
        "MMLU_high_school_mathematics":0.2111111111,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.1920529801,
        "MMLU_high_school_psychology":0.1871559633,
        "MMLU_high_school_statistics":0.1481481481,
        "MMLU_high_school_us_history":0.2598039216,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2231404959,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2392638037,
        "MMLU_machine_learning":0.3482142857,
        "MMLU_management":0.1844660194,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.32,
        "MMLU_miscellaneous":0.2349936143,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.2347266881,
        "MMLU_prehistory":0.212962963,
        "MMLU_professional_accounting":0.2411347518,
        "MMLU_professional_law":0.2431551499,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.1795918367,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.2771084337,
        "MMLU_world_religions":0.3333333333
    },
    {
        "Model":"LLmRa-1.3B",
        "URL":"https:\/\/huggingface.co\/L-R\/LLmRa-1.3B",
        "full_model_name":"L-R\/LLmRa-1.3B",
        "Parameters":1.3,
        "MMLU_average":0.2323367891,
        "arc:challenge|25":0.3029010239,
        "hellaswag|10":0.4481179048,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.2592592593,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.29,
        "MMLU_clinical_knowledge":0.2037735849,
        "MMLU_college_biology":0.2152777778,
        "MMLU_college_chemistry":0.16,
        "MMLU_college_computer_science":0.15,
        "MMLU_college_mathematics":0.25,
        "MMLU_college_medicine":0.2138728324,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.24,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2631578947,
        "MMLU_electrical_engineering":0.2137931034,
        "MMLU_elementary_mathematics":0.2354497354,
        "MMLU_formal_logic":0.246031746,
        "MMLU_global_facts":0.19,
        "MMLU_high_school_biology":0.2290322581,
        "MMLU_high_school_chemistry":0.2512315271,
        "MMLU_high_school_computer_science":0.26,
        "MMLU_high_school_european_history":0.2,
        "MMLU_high_school_geography":0.1919191919,
        "MMLU_high_school_government_and_politics":0.2176165803,
        "MMLU_high_school_macroeconomics":0.2153846154,
        "MMLU_high_school_mathematics":0.2777777778,
        "MMLU_high_school_microeconomics":0.1974789916,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.2,
        "MMLU_high_school_statistics":0.3240740741,
        "MMLU_high_school_us_history":0.2352941176,
        "MMLU_high_school_world_history":0.2616033755,
        "MMLU_human_aging":0.2600896861,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2561983471,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2392638037,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.213592233,
        "MMLU_marketing":0.2863247863,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2413793103,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2335195531,
        "MMLU_nutrition":0.2156862745,
        "MMLU_philosophy":0.1961414791,
        "MMLU_prehistory":0.2191358025,
        "MMLU_professional_accounting":0.2127659574,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2,
        "MMLU_security_studies":0.1673469388,
        "MMLU_sociology":0.223880597,
        "MMLU_us_foreign_policy":0.25,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.216374269
    },
    {
        "Model":"Mistral-7B-model_45k6e2e4",
        "URL":"https:\/\/huggingface.co\/pankajmathur\/Mistral-7B-model_45k6e2e4",
        "full_model_name":"pankajmathur\/Mistral-7B-model_45k6e2e4",
        "Parameters":7.0,
        "MMLU_average":0.2319077249,
        "arc:challenge|25":0.20221843,
        "hellaswag|10":0.2577175861,
        "MMLU_abstract_algebra":0.17,
        "MMLU_anatomy":0.1851851852,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.24,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.18,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.29,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2344827586,
        "MMLU_elementary_mathematics":0.2486772487,
        "MMLU_formal_logic":0.3015873016,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.1774193548,
        "MMLU_high_school_chemistry":0.1527093596,
        "MMLU_high_school_computer_science":0.27,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.1767676768,
        "MMLU_high_school_government_and_politics":0.1968911917,
        "MMLU_high_school_macroeconomics":0.2051282051,
        "MMLU_high_school_mathematics":0.2074074074,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.1854304636,
        "MMLU_high_school_psychology":0.1926605505,
        "MMLU_high_school_statistics":0.1527777778,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3303571429,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2375478927,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.1864951768,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2340425532,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"gladosystem",
        "URL":"https:\/\/huggingface.co\/huggingtweets\/gladosystem",
        "full_model_name":"huggingtweets\/gladosystem",
        "Parameters":null,
        "MMLU_average":0.2318475407,
        "arc:challenge|25":0.2175767918,
        "hellaswag|10":0.2834096793,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.1851851852,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.208994709,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.2,
        "MMLU_high_school_biology":0.1774193548,
        "MMLU_high_school_chemistry":0.157635468,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.1818181818,
        "MMLU_high_school_government_and_politics":0.1917098446,
        "MMLU_high_school_macroeconomics":0.2128205128,
        "MMLU_high_school_mathematics":0.2222222222,
        "MMLU_high_school_microeconomics":0.2142857143,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.1926605505,
        "MMLU_high_school_statistics":0.1527777778,
        "MMLU_high_school_us_history":0.2549019608,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2362707535,
        "MMLU_moral_disputes":0.2427745665,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.1864951768,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2340425532,
        "MMLU_professional_law":0.2490221643,
        "MMLU_professional_medicine":0.1801470588,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2090909091,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"llama2-13b-platypus-ckpt-1000",
        "URL":"https:\/\/huggingface.co\/bsp-albz\/llama2-13b-platypus-ckpt-1000",
        "full_model_name":"bsp-albz\/llama2-13b-platypus-ckpt-1000",
        "Parameters":13.0,
        "MMLU_average":0.2317495403,
        "arc:challenge|25":0.226109215,
        "hellaswag|10":0.2580163314,
        "MMLU_abstract_algebra":0.2,
        "MMLU_anatomy":0.2444444444,
        "MMLU_astronomy":0.2368421053,
        "MMLU_business_ethics":0.22,
        "MMLU_clinical_knowledge":0.1924528302,
        "MMLU_college_biology":0.25,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.17,
        "MMLU_college_medicine":0.2427745665,
        "MMLU_college_physics":0.2941176471,
        "MMLU_computer_security":0.18,
        "MMLU_conceptual_physics":0.2170212766,
        "MMLU_econometrics":0.298245614,
        "MMLU_electrical_engineering":0.2137931034,
        "MMLU_elementary_mathematics":0.2275132275,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.23,
        "MMLU_high_school_biology":0.2483870968,
        "MMLU_high_school_chemistry":0.354679803,
        "MMLU_high_school_computer_science":0.24,
        "MMLU_high_school_european_history":0.1696969697,
        "MMLU_high_school_geography":0.2272727273,
        "MMLU_high_school_government_and_politics":0.3005181347,
        "MMLU_high_school_macroeconomics":0.2564102564,
        "MMLU_high_school_mathematics":0.2962962963,
        "MMLU_high_school_microeconomics":0.3109243697,
        "MMLU_high_school_physics":0.2781456954,
        "MMLU_high_school_psychology":0.2256880734,
        "MMLU_high_school_statistics":0.2546296296,
        "MMLU_high_school_us_history":0.2450980392,
        "MMLU_high_school_world_history":0.2067510549,
        "MMLU_human_aging":0.1569506726,
        "MMLU_human_sexuality":0.1908396947,
        "MMLU_international_law":0.2644628099,
        "MMLU_jurisprudence":0.1759259259,
        "MMLU_logical_fallacies":0.1963190184,
        "MMLU_machine_learning":0.1607142857,
        "MMLU_management":0.1844660194,
        "MMLU_marketing":0.1794871795,
        "MMLU_medical_genetics":0.18,
        "MMLU_miscellaneous":0.190293742,
        "MMLU_moral_disputes":0.2167630058,
        "MMLU_moral_scenarios":0.2625698324,
        "MMLU_nutrition":0.2581699346,
        "MMLU_philosophy":0.2186495177,
        "MMLU_prehistory":0.2592592593,
        "MMLU_professional_accounting":0.219858156,
        "MMLU_professional_law":0.2490221643,
        "MMLU_professional_medicine":0.3125,
        "MMLU_professional_psychology":0.2401960784,
        "MMLU_public_relations":0.1818181818,
        "MMLU_security_studies":0.2897959184,
        "MMLU_sociology":0.2089552239,
        "MMLU_us_foreign_policy":0.19,
        "MMLU_virology":0.2289156627,
        "MMLU_world_religions":0.216374269
    },
    {
        "Model":"panda-coder-13B",
        "URL":"https:\/\/huggingface.co\/aiplanet\/panda-coder-13B",
        "full_model_name":"aiplanet\/panda-coder-13B",
        "Parameters":13.0,
        "MMLU_average":0.2311685756,
        "arc:challenge|25":0.2269624573,
        "hellaswag|10":0.2504481179,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.1851851852,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.208994709,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.1774193548,
        "MMLU_high_school_chemistry":0.1527093596,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.1767676768,
        "MMLU_high_school_government_and_politics":0.1968911917,
        "MMLU_high_school_macroeconomics":0.2025641026,
        "MMLU_high_school_mathematics":0.2111111111,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.1926605505,
        "MMLU_high_school_statistics":0.1527777778,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2375478927,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.1864951768,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2340425532,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"Llama-2-13b-chat-longlora-32k-sft",
        "URL":"https:\/\/huggingface.co\/Yukang\/Llama-2-13b-chat-longlora-32k-sft",
        "full_model_name":"Yukang\/Llama-2-13b-chat-longlora-32k-sft",
        "Parameters":13.0,
        "MMLU_average":0.2311685756,
        "arc:challenge|25":0.2064846416,
        "hellaswag|10":0.2574188409,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.1851851852,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.208994709,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.1774193548,
        "MMLU_high_school_chemistry":0.1527093596,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.1767676768,
        "MMLU_high_school_government_and_politics":0.1968911917,
        "MMLU_high_school_macroeconomics":0.2025641026,
        "MMLU_high_school_mathematics":0.2111111111,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.1926605505,
        "MMLU_high_school_statistics":0.1527777778,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2375478927,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.1864951768,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2340425532,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"X1-large",
        "URL":"https:\/\/huggingface.co\/GigaML\/X1-large",
        "full_model_name":"GigaML\/X1-large",
        "Parameters":null,
        "MMLU_average":0.2311685756,
        "arc:challenge|25":0.2269624573,
        "hellaswag|10":0.2504481179,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.1851851852,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.208994709,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.1774193548,
        "MMLU_high_school_chemistry":0.1527093596,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.1767676768,
        "MMLU_high_school_government_and_politics":0.1968911917,
        "MMLU_high_school_macroeconomics":0.2025641026,
        "MMLU_high_school_mathematics":0.2111111111,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.1926605505,
        "MMLU_high_school_statistics":0.1527777778,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2375478927,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.1864951768,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2340425532,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"DataLinguistic-34B-V1.0",
        "URL":"https:\/\/huggingface.co\/DataLinguistic\/DataLinguistic-34B-V1.0",
        "full_model_name":"DataLinguistic\/DataLinguistic-34B-V1.0",
        "Parameters":34.0,
        "MMLU_average":0.2311685756,
        "arc:challenge|25":0.2329351536,
        "hellaswag|10":0.281915953,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.1851851852,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.208994709,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.1774193548,
        "MMLU_high_school_chemistry":0.1527093596,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.1767676768,
        "MMLU_high_school_government_and_politics":0.1968911917,
        "MMLU_high_school_macroeconomics":0.2025641026,
        "MMLU_high_school_mathematics":0.2111111111,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.1926605505,
        "MMLU_high_school_statistics":0.1527777778,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2375478927,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.1864951768,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2340425532,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"pythia-70m-deduped-cleansharegpt",
        "URL":"https:\/\/huggingface.co\/HWERI\/pythia-70m-deduped-cleansharegpt",
        "full_model_name":"HWERI\/pythia-70m-deduped-cleansharegpt",
        "Parameters":0.07,
        "MMLU_average":0.2311685756,
        "arc:challenge|25":0.2081911263,
        "hellaswag|10":0.2576180044,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.1851851852,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.208994709,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.1774193548,
        "MMLU_high_school_chemistry":0.1527093596,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.1767676768,
        "MMLU_high_school_government_and_politics":0.1968911917,
        "MMLU_high_school_macroeconomics":0.2025641026,
        "MMLU_high_school_mathematics":0.2111111111,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.1926605505,
        "MMLU_high_school_statistics":0.1527777778,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2375478927,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.1864951768,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2340425532,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"bloom-1b1-RLHF-v2",
        "URL":"https:\/\/huggingface.co\/TheTravellingEngineer\/bloom-1b1-RLHF-v2",
        "full_model_name":"TheTravellingEngineer\/bloom-1b1-RLHF-v2",
        "Parameters":1.0,
        "MMLU_average":0.2311685756,
        "arc:challenge|25":0.2269624573,
        "hellaswag|10":0.2504481179,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.1851851852,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.208994709,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.1774193548,
        "MMLU_high_school_chemistry":0.1527093596,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.1767676768,
        "MMLU_high_school_government_and_politics":0.1968911917,
        "MMLU_high_school_macroeconomics":0.2025641026,
        "MMLU_high_school_mathematics":0.2111111111,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.1926605505,
        "MMLU_high_school_statistics":0.1527777778,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2375478927,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.1864951768,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2340425532,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"easyTermsSummerizer",
        "URL":"https:\/\/huggingface.co\/Quake24\/easyTermsSummerizer",
        "full_model_name":"Quake24\/easyTermsSummerizer",
        "Parameters":null,
        "MMLU_average":0.2311685756,
        "arc:challenge|25":0.1996587031,
        "hellaswag|10":0.2565226051,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.1851851852,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.208994709,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.1774193548,
        "MMLU_high_school_chemistry":0.1527093596,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.1767676768,
        "MMLU_high_school_government_and_politics":0.1968911917,
        "MMLU_high_school_macroeconomics":0.2025641026,
        "MMLU_high_school_mathematics":0.2111111111,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.1926605505,
        "MMLU_high_school_statistics":0.1527777778,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2375478927,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.1864951768,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2340425532,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"Cerebras-GPT-590M-finetuned-DND",
        "URL":"https:\/\/huggingface.co\/grantprice\/Cerebras-GPT-590M-finetuned-DND",
        "full_model_name":"grantprice\/Cerebras-GPT-590M-finetuned-DND",
        "Parameters":0.59,
        "MMLU_average":0.2311685756,
        "arc:challenge|25":0.2124573379,
        "hellaswag|10":0.2609042024,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.1851851852,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.208994709,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.1774193548,
        "MMLU_high_school_chemistry":0.1527093596,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.1767676768,
        "MMLU_high_school_government_and_politics":0.1968911917,
        "MMLU_high_school_macroeconomics":0.2025641026,
        "MMLU_high_school_mathematics":0.2111111111,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.1926605505,
        "MMLU_high_school_statistics":0.1527777778,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2375478927,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.1864951768,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2340425532,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"bilingual-gpt-neox-4b-instruction-sft",
        "URL":"https:\/\/huggingface.co\/rinna\/bilingual-gpt-neox-4b-instruction-sft",
        "full_model_name":"rinna\/bilingual-gpt-neox-4b-instruction-sft",
        "Parameters":4.0,
        "MMLU_average":0.2311685756,
        "arc:challenge|25":0.2482935154,
        "hellaswag|10":0.3824935272,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.1851851852,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.208994709,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.1774193548,
        "MMLU_high_school_chemistry":0.1527093596,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.1767676768,
        "MMLU_high_school_government_and_politics":0.1968911917,
        "MMLU_high_school_macroeconomics":0.2025641026,
        "MMLU_high_school_mathematics":0.2111111111,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.1926605505,
        "MMLU_high_school_statistics":0.1527777778,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2375478927,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.1864951768,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2340425532,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"model_007_13b",
        "URL":"https:\/\/huggingface.co\/psmathur\/model_007_13b",
        "full_model_name":"psmathur\/model_007_13b",
        "Parameters":13.0,
        "MMLU_average":0.2311685756,
        "arc:challenge|25":0.2269624573,
        "hellaswag|10":0.2504481179,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.1851851852,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.208994709,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.1774193548,
        "MMLU_high_school_chemistry":0.1527093596,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.1767676768,
        "MMLU_high_school_government_and_politics":0.1968911917,
        "MMLU_high_school_macroeconomics":0.2025641026,
        "MMLU_high_school_mathematics":0.2111111111,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.1926605505,
        "MMLU_high_school_statistics":0.1527777778,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2375478927,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.1864951768,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2340425532,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"FinOPT-Franklin",
        "URL":"https:\/\/huggingface.co\/MayaPH\/FinOPT-Franklin",
        "full_model_name":"MayaPH\/FinOPT-Franklin",
        "Parameters":null,
        "MMLU_average":0.2311685756,
        "arc:challenge|25":0.2278156997,
        "hellaswag|10":0.2539334794,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.1851851852,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.208994709,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.1774193548,
        "MMLU_high_school_chemistry":0.1527093596,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.1767676768,
        "MMLU_high_school_government_and_politics":0.1968911917,
        "MMLU_high_school_macroeconomics":0.2025641026,
        "MMLU_high_school_mathematics":0.2111111111,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.1926605505,
        "MMLU_high_school_statistics":0.1527777778,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2375478927,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.1864951768,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2340425532,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"UltraLM-13b",
        "URL":"https:\/\/huggingface.co\/openbmb\/UltraLM-13b",
        "full_model_name":"openbmb\/UltraLM-13b",
        "Parameters":13.0,
        "MMLU_average":0.2311685756,
        "arc:challenge|25":0.2397610922,
        "hellaswag|10":0.2566221868,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.1851851852,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.208994709,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.1774193548,
        "MMLU_high_school_chemistry":0.1527093596,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.1767676768,
        "MMLU_high_school_government_and_politics":0.1968911917,
        "MMLU_high_school_macroeconomics":0.2025641026,
        "MMLU_high_school_mathematics":0.2111111111,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.1926605505,
        "MMLU_high_school_statistics":0.1527777778,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2375478927,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.1864951768,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2340425532,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"WizardLM-13B-1.0",
        "URL":"https:\/\/huggingface.co\/victor123\/WizardLM-13B-1.0",
        "full_model_name":"victor123\/WizardLM-13B-1.0",
        "Parameters":13.0,
        "MMLU_average":0.2311685756,
        "arc:challenge|25":0.2192832765,
        "hellaswag|10":0.2545309699,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.1851851852,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.208994709,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.1774193548,
        "MMLU_high_school_chemistry":0.1527093596,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.1767676768,
        "MMLU_high_school_government_and_politics":0.1968911917,
        "MMLU_high_school_macroeconomics":0.2025641026,
        "MMLU_high_school_mathematics":0.2111111111,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.1926605505,
        "MMLU_high_school_statistics":0.1527777778,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2375478927,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.1864951768,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2340425532,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"zsc-text",
        "URL":"https:\/\/huggingface.co\/deepnight-research\/zsc-text",
        "full_model_name":"deepnight-research\/zsc-text",
        "Parameters":null,
        "MMLU_average":0.2311685756,
        "arc:challenge|25":0.2201365188,
        "hellaswag|10":0.2547301334,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.1851851852,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.208994709,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.1774193548,
        "MMLU_high_school_chemistry":0.1527093596,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.1767676768,
        "MMLU_high_school_government_and_politics":0.1968911917,
        "MMLU_high_school_macroeconomics":0.2025641026,
        "MMLU_high_school_mathematics":0.2111111111,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.1926605505,
        "MMLU_high_school_statistics":0.1527777778,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2375478927,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.1864951768,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2340425532,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"pythia-31m-goodwiki-deduped-2048-scratch",
        "URL":"https:\/\/huggingface.co\/pszemraj\/pythia-31m-goodwiki-deduped-2048-scratch",
        "full_model_name":"pszemraj\/pythia-31m-goodwiki-deduped-2048-scratch",
        "Parameters":0.031,
        "MMLU_average":0.2311461696,
        "arc:challenge|25":0.1749146758,
        "hellaswag|10":0.2592113125,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.1851851852,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.208994709,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.1774193548,
        "MMLU_high_school_chemistry":0.1527093596,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.1767676768,
        "MMLU_high_school_government_and_politics":0.1968911917,
        "MMLU_high_school_macroeconomics":0.2025641026,
        "MMLU_high_school_mathematics":0.2111111111,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.1926605505,
        "MMLU_high_school_statistics":0.1527777778,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2362707535,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.1864951768,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2340425532,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"TinyMistral-248m",
        "URL":"https:\/\/huggingface.co\/Locutusque\/TinyMistral-248m",
        "full_model_name":"Locutusque\/TinyMistral-248m",
        "Parameters":0.248,
        "MMLU_average":0.2310555007,
        "arc:challenge|25":0.1766211604,
        "hellaswag|10":0.2653853814,
        "MMLU_abstract_algebra":0.21,
        "MMLU_anatomy":0.1925925926,
        "MMLU_astronomy":0.1710526316,
        "MMLU_business_ethics":0.31,
        "MMLU_clinical_knowledge":0.2188679245,
        "MMLU_college_biology":0.25,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.24,
        "MMLU_college_mathematics":0.21,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.29,
        "MMLU_conceptual_physics":0.2680851064,
        "MMLU_econometrics":0.2105263158,
        "MMLU_electrical_engineering":0.2344827586,
        "MMLU_elementary_mathematics":0.208994709,
        "MMLU_formal_logic":0.2936507937,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.1903225806,
        "MMLU_high_school_chemistry":0.1674876847,
        "MMLU_high_school_computer_science":0.28,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.1868686869,
        "MMLU_high_school_government_and_politics":0.1968911917,
        "MMLU_high_school_macroeconomics":0.2102564103,
        "MMLU_high_school_mathematics":0.2148148148,
        "MMLU_high_school_microeconomics":0.2142857143,
        "MMLU_high_school_physics":0.2052980132,
        "MMLU_high_school_psychology":0.1853211009,
        "MMLU_high_school_statistics":0.1435185185,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2742616034,
        "MMLU_human_aging":0.2959641256,
        "MMLU_human_sexuality":0.2290076336,
        "MMLU_international_law":0.2314049587,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.226993865,
        "MMLU_machine_learning":0.3035714286,
        "MMLU_management":0.1844660194,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2388250319,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.1864951768,
        "MMLU_prehistory":0.2191358025,
        "MMLU_professional_accounting":0.2269503546,
        "MMLU_professional_law":0.2411994785,
        "MMLU_professional_medicine":0.1911764706,
        "MMLU_professional_psychology":0.2549019608,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.3157894737
    },
    {
        "Model":"bilingual-gpt-neox-4b",
        "URL":"https:\/\/huggingface.co\/rinna\/bilingual-gpt-neox-4b",
        "full_model_name":"rinna\/bilingual-gpt-neox-4b",
        "Parameters":4.0,
        "MMLU_average":0.230993137,
        "arc:challenge|25":0.2389078498,
        "hellaswag|10":0.3646683928,
        "MMLU_abstract_algebra":0.22,
        "MMLU_anatomy":0.1851851852,
        "MMLU_astronomy":0.1776315789,
        "MMLU_business_ethics":0.3,
        "MMLU_clinical_knowledge":0.2150943396,
        "MMLU_college_biology":0.2569444444,
        "MMLU_college_chemistry":0.2,
        "MMLU_college_computer_science":0.26,
        "MMLU_college_mathematics":0.2,
        "MMLU_college_medicine":0.2080924855,
        "MMLU_college_physics":0.2156862745,
        "MMLU_computer_security":0.28,
        "MMLU_conceptual_physics":0.2638297872,
        "MMLU_econometrics":0.2368421053,
        "MMLU_electrical_engineering":0.2413793103,
        "MMLU_elementary_mathematics":0.208994709,
        "MMLU_formal_logic":0.2857142857,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.1774193548,
        "MMLU_high_school_chemistry":0.1527093596,
        "MMLU_high_school_computer_science":0.25,
        "MMLU_high_school_european_history":0.2181818182,
        "MMLU_high_school_geography":0.1767676768,
        "MMLU_high_school_government_and_politics":0.1968911917,
        "MMLU_high_school_macroeconomics":0.2025641026,
        "MMLU_high_school_mathematics":0.2111111111,
        "MMLU_high_school_microeconomics":0.2100840336,
        "MMLU_high_school_physics":0.1986754967,
        "MMLU_high_school_psychology":0.1926605505,
        "MMLU_high_school_statistics":0.1527777778,
        "MMLU_high_school_us_history":0.25,
        "MMLU_high_school_world_history":0.2700421941,
        "MMLU_human_aging":0.3139013453,
        "MMLU_human_sexuality":0.2595419847,
        "MMLU_international_law":0.2396694215,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1747572816,
        "MMLU_marketing":0.2905982906,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2375478927,
        "MMLU_moral_disputes":0.2485549133,
        "MMLU_moral_scenarios":0.2379888268,
        "MMLU_nutrition":0.2254901961,
        "MMLU_philosophy":0.1864951768,
        "MMLU_prehistory":0.2160493827,
        "MMLU_professional_accounting":0.2340425532,
        "MMLU_professional_law":0.2457627119,
        "MMLU_professional_medicine":0.1838235294,
        "MMLU_professional_psychology":0.25,
        "MMLU_public_relations":0.2181818182,
        "MMLU_security_studies":0.187755102,
        "MMLU_sociology":0.2437810945,
        "MMLU_us_foreign_policy":0.28,
        "MMLU_virology":0.2831325301,
        "MMLU_world_religions":0.3216374269
    },
    {
        "Model":"stablelm-tuned-alpha-3b",
        "URL":"https:\/\/huggingface.co\/stabilityai\/stablelm-tuned-alpha-3b",
        "full_model_name":"stabilityai\/stablelm-tuned-alpha-3b",
        "Parameters":3.0,
        "MMLU_average":0.2308013536,
        "arc:challenge|25":0.2465870307,
        "hellaswag|10":0.360485959,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.2518518519,
        "MMLU_astronomy":0.1973684211,
        "MMLU_business_ethics":0.35,
        "MMLU_clinical_knowledge":0.2301886792,
        "MMLU_college_biology":0.2638888889,
        "MMLU_college_chemistry":0.14,
        "MMLU_college_computer_science":0.27,
        "MMLU_college_mathematics":0.19,
        "MMLU_college_medicine":0.225433526,
        "MMLU_college_physics":0.1764705882,
        "MMLU_computer_security":0.29,
        "MMLU_conceptual_physics":0.2723404255,
        "MMLU_econometrics":0.201754386,
        "MMLU_electrical_engineering":0.2344827586,
        "MMLU_elementary_mathematics":0.2248677249,
        "MMLU_formal_logic":0.2619047619,
        "MMLU_global_facts":0.18,
        "MMLU_high_school_biology":0.1838709677,
        "MMLU_high_school_chemistry":0.1428571429,
        "MMLU_high_school_computer_science":0.24,
        "MMLU_high_school_european_history":0.2121212121,
        "MMLU_high_school_geography":0.1767676768,
        "MMLU_high_school_government_and_politics":0.2124352332,
        "MMLU_high_school_macroeconomics":0.2205128205,
        "MMLU_high_school_mathematics":0.2074074074,
        "MMLU_high_school_microeconomics":0.231092437,
        "MMLU_high_school_physics":0.2119205298,
        "MMLU_high_school_psychology":0.1981651376,
        "MMLU_high_school_statistics":0.1527777778,
        "MMLU_high_school_us_history":0.2254901961,
        "MMLU_high_school_world_history":0.2827004219,
        "MMLU_human_aging":0.3004484305,
        "MMLU_human_sexuality":0.2213740458,
        "MMLU_international_law":0.2314049587,
        "MMLU_jurisprudence":0.2592592593,
        "MMLU_logical_fallacies":0.2208588957,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.1650485437,
        "MMLU_marketing":0.2777777778,
        "MMLU_medical_genetics":0.3,
        "MMLU_miscellaneous":0.2541507024,
        "MMLU_moral_disputes":0.225433526,
        "MMLU_moral_scenarios":0.2424581006,
        "MMLU_nutrition":0.2320261438,
        "MMLU_philosophy":0.1800643087,
        "MMLU_prehistory":0.2098765432,
        "MMLU_professional_accounting":0.1843971631,
        "MMLU_professional_law":0.239243807,
        "MMLU_professional_medicine":0.2426470588,
        "MMLU_professional_psychology":0.2549019608,
        "MMLU_public_relations":0.2909090909,
        "MMLU_security_studies":0.1755102041,
        "MMLU_sociology":0.2189054726,
        "MMLU_us_foreign_policy":0.27,
        "MMLU_virology":0.2771084337,
        "MMLU_world_religions":0.2807017544
    },
    {
        "Model":"Llama-2-13b-public",
        "URL":"https:\/\/huggingface.co\/porkorbeef\/Llama-2-13b-public",
        "full_model_name":"porkorbeef\/Llama-2-13b-public",
        "Parameters":13.0,
        "MMLU_average":0.2273586062,
        "arc:challenge|25":0.2465870307,
        "hellaswag|10":0.2584146584,
        "MMLU_abstract_algebra":0.23,
        "MMLU_anatomy":0.237037037,
        "MMLU_astronomy":0.25,
        "MMLU_business_ethics":0.23,
        "MMLU_clinical_knowledge":0.1924528302,
        "MMLU_college_biology":0.2291666667,
        "MMLU_college_chemistry":0.3,
        "MMLU_college_computer_science":0.17,
        "MMLU_college_mathematics":0.17,
        "MMLU_college_medicine":0.1849710983,
        "MMLU_college_physics":0.1960784314,
        "MMLU_computer_security":0.25,
        "MMLU_conceptual_physics":0.2723404255,
        "MMLU_econometrics":0.2456140351,
        "MMLU_electrical_engineering":0.2896551724,
        "MMLU_elementary_mathematics":0.2407407407,
        "MMLU_formal_logic":0.2142857143,
        "MMLU_global_facts":0.33,
        "MMLU_high_school_biology":0.2,
        "MMLU_high_school_chemistry":0.236453202,
        "MMLU_high_school_computer_science":0.27,
        "MMLU_high_school_european_history":0.2242424242,
        "MMLU_high_school_geography":0.2323232323,
        "MMLU_high_school_government_and_politics":0.1554404145,
        "MMLU_high_school_macroeconomics":0.1743589744,
        "MMLU_high_school_mathematics":0.3074074074,
        "MMLU_high_school_microeconomics":0.1764705882,
        "MMLU_high_school_physics":0.2516556291,
        "MMLU_high_school_psychology":0.2128440367,
        "MMLU_high_school_statistics":0.1759259259,
        "MMLU_high_school_us_history":0.2205882353,
        "MMLU_high_school_world_history":0.2784810127,
        "MMLU_human_aging":0.269058296,
        "MMLU_human_sexuality":0.1374045802,
        "MMLU_international_law":0.2066115702,
        "MMLU_jurisprudence":0.2222222222,
        "MMLU_logical_fallacies":0.2024539877,
        "MMLU_machine_learning":0.3125,
        "MMLU_management":0.2427184466,
        "MMLU_marketing":0.188034188,
        "MMLU_medical_genetics":0.17,
        "MMLU_miscellaneous":0.2503192848,
        "MMLU_moral_disputes":0.2312138728,
        "MMLU_moral_scenarios":0.2558659218,
        "MMLU_nutrition":0.2320261438,
        "MMLU_philosophy":0.2154340836,
        "MMLU_prehistory":0.2283950617,
        "MMLU_professional_accounting":0.2482269504,
        "MMLU_professional_law":0.2451108214,
        "MMLU_professional_medicine":0.1875,
        "MMLU_professional_psychology":0.2565359477,
        "MMLU_public_relations":0.1909090909,
        "MMLU_security_studies":0.2653061224,
        "MMLU_sociology":0.2587064677,
        "MMLU_us_foreign_policy":0.19,
        "MMLU_virology":0.2289156627,
        "MMLU_world_religions":0.1754385965
    }
]