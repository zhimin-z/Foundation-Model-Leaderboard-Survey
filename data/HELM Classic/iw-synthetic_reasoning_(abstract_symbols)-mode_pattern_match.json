[
    {
        "Model":"J1-Jumbo v1 (178B)",
        "EM":0.437,
        "Denoised inference time (s)":"0.55",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":233.611,
        "# output tokens":5.617,
        "# trials":3
    },
    {
        "Model":"J1-Large v1 (7.5B)",
        "EM":0.31,
        "Denoised inference time (s)":"0.37",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":233.611,
        "# output tokens":5.627,
        "# trials":3
    },
    {
        "Model":"J1-Grande v1 (17B)",
        "EM":0.379,
        "Denoised inference time (s)":"0.571",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":233.611,
        "# output tokens":5.5,
        "# trials":3
    },
    {
        "Model":"J1-Grande v2 beta (17B)",
        "EM":0.351,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":233.611,
        "# output tokens":5.46,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "EM":0.423,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":233.611,
        "# output tokens":5.469,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "EM":0.426,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":233.611,
        "# output tokens":5.483,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Large (7.5B)",
        "EM":0.254,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":233.611,
        "# output tokens":5.498,
        "# trials":3
    },
    {
        "Model":"Luminous Base (13B)",
        "EM":0.35,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":247.872,
        "# output tokens":4.187,
        "# trials":3
    },
    {
        "Model":"Luminous Extended (30B)",
        "EM":0.329,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":247.872,
        "# output tokens":4.249,
        "# trials":3
    },
    {
        "Model":"Luminous Supreme (70B)",
        "EM":0.504,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":247.872,
        "# output tokens":4.524,
        "# trials":3
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B)",
        "EM":0.495,
        "Denoised inference time (s)":"0.817",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":239.216,
        "# output tokens":4.578,
        "# trials":3
    },
    {
        "Model":"BLOOM (176B)",
        "EM":0.426,
        "Denoised inference time (s)":"0.296",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":235.649,
        "# output tokens":10.16,
        "# trials":3
    },
    {
        "Model":"T0pp (11B)",
        "EM":0.0,
        "Denoised inference time (s)":"0.356",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":321.994,
        "# output tokens":50.0,
        "# trials":3
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B)",
        "EM":0.289,
        "Denoised inference time (s)":"0.587",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":247.521,
        "# output tokens":4.66,
        "# trials":3
    },
    {
        "Model":"Cohere large v20220720 (13.1B)",
        "EM":0.203,
        "Denoised inference time (s)":"0.339",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":247.521,
        "# output tokens":4.119,
        "# trials":3
    },
    {
        "Model":"Cohere medium v20220720 (6.1B)",
        "EM":0.19,
        "Denoised inference time (s)":"0.278",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":247.521,
        "# output tokens":4.489,
        "# trials":3
    },
    {
        "Model":"Cohere small v20220720 (410M)",
        "EM":0.193,
        "Denoised inference time (s)":"0.266",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":247.521,
        "# output tokens":4.917,
        "# trials":3
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B)",
        "EM":0.354,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":247.521,
        "# output tokens":4.728,
        "# trials":3
    },
    {
        "Model":"Cohere medium v20221108 (6.1B)",
        "EM":0.107,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":247.521,
        "# output tokens":4.506,
        "# trials":3
    },
    {
        "Model":"Cohere Command beta (6.1B)",
        "EM":0.169,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":247.521,
        "# output tokens":4.728,
        "# trials":3
    },
    {
        "Model":"Cohere Command beta (52.4B)",
        "EM":0.355,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":247.521,
        "# output tokens":4.436,
        "# trials":3
    },
    {
        "Model":"GPT-J (6B)",
        "EM":0.23,
        "Denoised inference time (s)":"0.223",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":239.216,
        "# output tokens":33.379,
        "# trials":3
    },
    {
        "Model":"GPT-NeoX (20B)",
        "EM":0.278,
        "Denoised inference time (s)":"0.207",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":241.061,
        "# output tokens":34.729,
        "# trials":3
    },
    {
        "Model":"Pythia (6.9B)",
        "EM":0.334,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":234.728,
        "# output tokens":50.0,
        "# trials":1
    },
    {
        "Model":"Pythia (12B)",
        "EM":0.359,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":234.728,
        "# output tokens":50.0,
        "# trials":1
    },
    {
        "Model":"T5 (11B)",
        "EM":0.396,
        "Denoised inference time (s)":"0.608",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":321.994,
        "# output tokens":50.0,
        "# trials":3
    },
    {
        "Model":"UL2 (20B)",
        "EM":0.425,
        "Denoised inference time (s)":"0.444",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":325.994,
        "# output tokens":50.0,
        "# trials":3
    },
    {
        "Model":"OPT (175B)",
        "EM":0.406,
        "Denoised inference time (s)":"0.317",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":239.216,
        "# output tokens":17.453,
        "# trials":3
    },
    {
        "Model":"OPT (66B)",
        "EM":0.35,
        "Denoised inference time (s)":"0.081",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":239.216,
        "# output tokens":16.628,
        "# trials":3
    },
    {
        "Model":"LLaMA (7B)",
        "EM":0.258,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":253.252,
        "# output tokens":50.0,
        "# trials":1
    },
    {
        "Model":"LLaMA (13B)",
        "EM":0.462,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":253.252,
        "# output tokens":50.0,
        "# trials":1
    },
    {
        "Model":"LLaMA (30B)",
        "EM":0.46,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":253.252,
        "# output tokens":50.0,
        "# trials":1
    },
    {
        "Model":"LLaMA (65B)",
        "EM":0.431,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":253.252,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (7B)",
        "EM":0.45,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":253.252,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (13B)",
        "EM":0.449,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":253.252,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (70B)",
        "EM":0.482,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":253.252,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Alpaca (7B)",
        "EM":0.239,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":253.252,
        "# output tokens":13.573,
        "# trials":1
    },
    {
        "Model":"Vicuna v1.3 (7B)",
        "EM":0.25,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":253.252,
        "# output tokens":49.326,
        "# trials":1
    },
    {
        "Model":"Vicuna v1.3 (13B)",
        "EM":0.47,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":253.252,
        "# output tokens":49.893,
        "# trials":1
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "EM":0.433,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":0.0,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"TNLG v2 (530B)",
        "EM":0.407,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":239.216,
        "# output tokens":4.635,
        "# trials":3
    },
    {
        "Model":"TNLG v2 (6.7B)",
        "EM":0.447,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":239.216,
        "# output tokens":4.636,
        "# trials":3
    },
    {
        "Model":"davinci (175B)",
        "EM":0.334,
        "Denoised inference time (s)":"0.312",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":239.216,
        "# output tokens":4.366,
        "# trials":3
    },
    {
        "Model":"curie (6.7B)",
        "EM":0.419,
        "Denoised inference time (s)":"0.119",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":239.216,
        "# output tokens":4.893,
        "# trials":3
    },
    {
        "Model":"babbage (1.3B)",
        "EM":0.306,
        "Denoised inference time (s)":"0.151",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":239.216,
        "# output tokens":6.656,
        "# trials":3
    },
    {
        "Model":"ada (350M)",
        "EM":0.137,
        "Denoised inference time (s)":"0.177",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":239.216,
        "# output tokens":6.915,
        "# trials":3
    },
    {
        "Model":"text-davinci-003",
        "EM":0.424,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":239.216,
        "# output tokens":4.487,
        "# trials":3
    },
    {
        "Model":"text-davinci-002",
        "EM":0.431,
        "Denoised inference time (s)":"0.297",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":239.216,
        "# output tokens":4.491,
        "# trials":3
    },
    {
        "Model":"text-curie-001",
        "EM":0.337,
        "Denoised inference time (s)":"0.158",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":239.216,
        "# output tokens":4.15,
        "# trials":3
    },
    {
        "Model":"text-babbage-001",
        "EM":0.194,
        "Denoised inference time (s)":"0.149",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":239.216,
        "# output tokens":3.798,
        "# trials":3
    },
    {
        "Model":"text-ada-001",
        "EM":0.081,
        "Denoised inference time (s)":"0.11",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":239.216,
        "# output tokens":5.017,
        "# trials":3
    },
    {
        "Model":"code-davinci-002",
        "EM":0.54,
        "Denoised inference time (s)":"0.308",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":239.216,
        "# output tokens":5.617,
        "# trials":3
    },
    {
        "Model":"code-cushman-001 (12B)",
        "EM":0.426,
        "Denoised inference time (s)":"0.138",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":239.216,
        "# output tokens":5.555,
        "# trials":3
    },
    {
        "Model":"gpt-3.5-turbo-0301",
        "EM":0.359,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":221.872,
        "# output tokens":4.517,
        "# trials":1
    },
    {
        "Model":"gpt-3.5-turbo-0613",
        "EM":0.493,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":221.872,
        "# output tokens":4.955,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Base-v1 (3B)",
        "EM":0.326,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":234.728,
        "# output tokens":50.0,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Instruct-v1 (3B)",
        "EM":0.332,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":234.728,
        "# output tokens":50.0,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Base (7B)",
        "EM":0.332,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":234.728,
        "# output tokens":50.0,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Instruct (7B)",
        "EM":0.402,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":234.728,
        "# output tokens":50.0,
        "# trials":1
    },
    {
        "Model":"MPT (30B)",
        "EM":0.468,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":234.728,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"MPT-Instruct (30B)",
        "EM":0.404,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":234.728,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon (7B)",
        "EM":0.132,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":326.406,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon-Instruct (7B)",
        "EM":0.025,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":326.406,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon (40B)",
        "EM":0.396,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":326.406,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon-Instruct (40B)",
        "EM":0.429,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":326.406,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"GLM (130B)",
        "EM":0.382,
        "Denoised inference time (s)":"0.23",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":233.054,
        "# output tokens":5.837,
        "# trials":3
    },
    {
        "Model":"InstructPalmyra (30B)",
        "EM":0.37,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":239.216,
        "# output tokens":4.399,
        "# trials":3
    },
    {
        "Model":"Palmyra X (43B)",
        "EM":0.558,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":239.216,
        "# output tokens":4.363,
        "# trials":3
    },
    {
        "Model":"YaLM (100B)",
        "EM":0.083,
        "Denoised inference time (s)":"0.387",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":239.419,
        "# output tokens":39.811,
        "# trials":3
    }
]