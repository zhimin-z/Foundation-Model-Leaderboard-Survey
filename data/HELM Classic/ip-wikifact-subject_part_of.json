[
    {
        "Model":"J1-Jumbo v1 (178B)",
        "EM":0.192,
        "Denoised inference time (s)":"1.295",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":70.932,
        "# output tokens":24.991,
        "# trials":3
    },
    {
        "Model":"J1-Large v1 (7.5B)",
        "EM":0.178,
        "Denoised inference time (s)":"0.648",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":70.932,
        "# output tokens":25.544,
        "# trials":3
    },
    {
        "Model":"J1-Grande v1 (17B)",
        "EM":0.211,
        "Denoised inference time (s)":"0.822",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":70.932,
        "# output tokens":24.077,
        "# trials":3
    },
    {
        "Model":"J1-Grande v2 beta (17B)",
        "EM":0.257,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":70.932,
        "# output tokens":24.04,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "EM":0.289,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":70.932,
        "# output tokens":21.933,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "EM":0.257,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":70.932,
        "# output tokens":20.105,
        "# trials":3
    },
    {
        "Model":"Luminous Base (13B)",
        "EM":0.191,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":93.377,
        "# output tokens":22.196,
        "# trials":3
    },
    {
        "Model":"Luminous Extended (30B)",
        "EM":0.223,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":93.377,
        "# output tokens":20.778,
        "# trials":3
    },
    {
        "Model":"Luminous Supreme (70B)",
        "EM":0.245,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":93.377,
        "# output tokens":22.591,
        "# trials":3
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B)",
        "EM":0.246,
        "Denoised inference time (s)":"1.769",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":95.975,
        "# output tokens":23.078,
        "# trials":3
    },
    {
        "Model":"BLOOM (176B)",
        "EM":0.148,
        "Denoised inference time (s)":"0.803",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":91.402,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"T0pp (11B)",
        "EM":0.021,
        "Denoised inference time (s)":"0.158",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":105.019,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B)",
        "EM":0.21,
        "Denoised inference time (s)":"1.234",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":96.603,
        "# output tokens":22.779,
        "# trials":3
    },
    {
        "Model":"Cohere large v20220720 (13.1B)",
        "EM":0.182,
        "Denoised inference time (s)":"0.731",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":96.603,
        "# output tokens":22.916,
        "# trials":3
    },
    {
        "Model":"Cohere medium v20220720 (6.1B)",
        "EM":0.166,
        "Denoised inference time (s)":"0.478",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":96.603,
        "# output tokens":23.695,
        "# trials":3
    },
    {
        "Model":"Cohere small v20220720 (410M)",
        "EM":0.114,
        "Denoised inference time (s)":"0.36",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":96.603,
        "# output tokens":23.647,
        "# trials":3
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B)",
        "EM":0.236,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":96.603,
        "# output tokens":22.857,
        "# trials":3
    },
    {
        "Model":"Cohere medium v20221108 (6.1B)",
        "EM":0.16,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":96.603,
        "# output tokens":22.989,
        "# trials":3
    },
    {
        "Model":"Cohere Command beta (6.1B)",
        "EM":0.206,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":96.603,
        "# output tokens":19.492,
        "# trials":3
    },
    {
        "Model":"Cohere Command beta (52.4B)",
        "EM":0.258,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":96.603,
        "# output tokens":25.81,
        "# trials":3
    },
    {
        "Model":"GPT-J (6B)",
        "EM":0.111,
        "Denoised inference time (s)":"0.096",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":95.975,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"GPT-NeoX (20B)",
        "EM":0.128,
        "Denoised inference time (s)":"0.064",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":98.466,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"Pythia (6.9B)",
        "EM":0.095,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":96.133,
        "# output tokens":6.38,
        "# trials":1
    },
    {
        "Model":"Pythia (12B)",
        "EM":0.115,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":96.133,
        "# output tokens":5.822,
        "# trials":1
    },
    {
        "Model":"T5 (11B)",
        "EM":0.107,
        "Denoised inference time (s)":"0.289",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":105.019,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"UL2 (20B)",
        "EM":0.155,
        "Denoised inference time (s)":"0.263",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":109.019,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"OPT (175B)",
        "EM":0.18,
        "Denoised inference time (s)":"0.596",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":95.975,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"OPT (66B)",
        "EM":0.144,
        "Denoised inference time (s)":"0.144",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":95.975,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"LLaMA (7B)",
        "EM":0.071,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":124.045,
        "# output tokens":6.334,
        "# trials":1
    },
    {
        "Model":"LLaMA (13B)",
        "EM":0.094,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":124.045,
        "# output tokens":6.46,
        "# trials":1
    },
    {
        "Model":"LLaMA (30B)",
        "EM":0.125,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":124.045,
        "# output tokens":6.548,
        "# trials":1
    },
    {
        "Model":"LLaMA (65B)",
        "EM":0.171,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":124.045,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (7B)",
        "EM":0.12,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":124.045,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (13B)",
        "EM":0.145,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":124.045,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (70B)",
        "EM":0.182,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":124.045,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Alpaca (7B)",
        "EM":0.089,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":124.045,
        "# output tokens":6.558,
        "# trials":1
    },
    {
        "Model":"Vicuna v1.3 (7B)",
        "EM":0.084,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":124.045,
        "# output tokens":6.353,
        "# trials":1
    },
    {
        "Model":"Vicuna v1.3 (13B)",
        "EM":0.109,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":124.045,
        "# output tokens":6.502,
        "# trials":1
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "EM":0.135,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":0.0,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"TNLG v2 (530B)",
        "EM":0.233,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":95.975,
        "# output tokens":23.169,
        "# trials":3
    },
    {
        "Model":"TNLG v2 (6.7B)",
        "EM":0.182,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":95.975,
        "# output tokens":24.929,
        "# trials":3
    },
    {
        "Model":"davinci (175B)",
        "EM":0.233,
        "Denoised inference time (s)":"0.873",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":95.975,
        "# output tokens":23.64,
        "# trials":3
    },
    {
        "Model":"curie (6.7B)",
        "EM":0.177,
        "Denoised inference time (s)":"0.245",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":95.975,
        "# output tokens":24.027,
        "# trials":3
    },
    {
        "Model":"babbage (1.3B)",
        "EM":0.136,
        "Denoised inference time (s)":"0.255",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":95.975,
        "# output tokens":24.589,
        "# trials":3
    },
    {
        "Model":"ada (350M)",
        "EM":0.118,
        "Denoised inference time (s)":"0.282",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":95.975,
        "# output tokens":25.015,
        "# trials":3
    },
    {
        "Model":"text-davinci-003",
        "EM":0.306,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":95.975,
        "# output tokens":23.789,
        "# trials":3
    },
    {
        "Model":"text-davinci-002",
        "EM":0.304,
        "Denoised inference time (s)":"0.798",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":95.975,
        "# output tokens":22.591,
        "# trials":3
    },
    {
        "Model":"text-curie-001",
        "EM":0.193,
        "Denoised inference time (s)":"0.286",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":95.975,
        "# output tokens":23.927,
        "# trials":3
    },
    {
        "Model":"text-babbage-001",
        "EM":0.147,
        "Denoised inference time (s)":"0.298",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":95.975,
        "# output tokens":25.682,
        "# trials":3
    },
    {
        "Model":"text-ada-001",
        "EM":0.101,
        "Denoised inference time (s)":"0.23",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":95.975,
        "# output tokens":26.071,
        "# trials":3
    },
    {
        "Model":"gpt-3.5-turbo-0301",
        "EM":0.228,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":96.269,
        "# output tokens":33.208,
        "# trials":1
    },
    {
        "Model":"gpt-3.5-turbo-0613",
        "EM":0.22,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":96.269,
        "# output tokens":34.086,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Base-v1 (3B)",
        "EM":0.099,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":96.133,
        "# output tokens":5.985,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Instruct-v1 (3B)",
        "EM":0.105,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":96.133,
        "# output tokens":5.836,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Base (7B)",
        "EM":0.124,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":96.133,
        "# output tokens":5.842,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Instruct (7B)",
        "EM":0.113,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":96.133,
        "# output tokens":5.552,
        "# trials":1
    },
    {
        "Model":"MPT (30B)",
        "EM":0.286,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":96.133,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"MPT-Instruct (30B)",
        "EM":0.266,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":96.133,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon (7B)",
        "EM":0.188,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":103.586,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon-Instruct (7B)",
        "EM":0.125,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":103.586,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon (40B)",
        "EM":0.227,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":103.586,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon-Instruct (40B)",
        "EM":0.227,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":103.586,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"GLM (130B)",
        "EM":0.09,
        "Denoised inference time (s)":"0.528",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":115.395,
        "# output tokens":28.567,
        "# trials":3
    },
    {
        "Model":"InstructPalmyra (30B)",
        "EM":0.198,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":95.975,
        "# output tokens":20.827,
        "# trials":3
    },
    {
        "Model":"Palmyra X (43B)",
        "EM":0.182,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":95.975,
        "# output tokens":19.191,
        "# trials":3
    },
    {
        "Model":"YaLM (100B)",
        "EM":0.022,
        "Denoised inference time (s)":"0.34",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":95.925,
        "# output tokens":40.0,
        "# trials":3
    }
]