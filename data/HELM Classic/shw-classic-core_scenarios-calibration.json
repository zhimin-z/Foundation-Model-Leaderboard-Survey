[
    {
        "Model":"J1-Jumbo v1 (178B)",
        "Mean win rate":"0.666",
        "MMLU - ECE (10-bin)":"0.131",
        "BoolQ - ECE (10-bin)":"0.215",
        "NarrativeQA - ECE (10-bin)":"0.034",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.035",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.065",
        "QuAC - ECE (10-bin)":"0.043",
        "HellaSwag - ECE (10-bin)":"0.217",
        "OpenbookQA - ECE (10-bin)":"0.25",
        "TruthfulQA - ECE (10-bin)":"0.113",
        "IMDB - ECE (10-bin)":"0.064",
        "CivilComments - ECE (10-bin)":"0.27",
        "RAFT - ECE (10-bin)":"0.228"
    },
    {
        "Model":"J1-Large v1 (7.5B)",
        "Mean win rate":"0.638",
        "MMLU - ECE (10-bin)":"0.123",
        "BoolQ - ECE (10-bin)":"0.106",
        "NarrativeQA - ECE (10-bin)":"0.046",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.015",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.086",
        "QuAC - ECE (10-bin)":"0.024",
        "HellaSwag - ECE (10-bin)":"0.192",
        "OpenbookQA - ECE (10-bin)":"0.25",
        "TruthfulQA - ECE (10-bin)":"0.112",
        "IMDB - ECE (10-bin)":"0.213",
        "CivilComments - ECE (10-bin)":"0.377",
        "RAFT - ECE (10-bin)":"0.269"
    },
    {
        "Model":"J1-Grande v1 (17B)",
        "Mean win rate":"0.622",
        "MMLU - ECE (10-bin)":"0.114",
        "BoolQ - ECE (10-bin)":"0.154",
        "NarrativeQA - ECE (10-bin)":"0.047",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.029",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.081",
        "QuAC - ECE (10-bin)":"0.036",
        "HellaSwag - ECE (10-bin)":"0.213",
        "OpenbookQA - ECE (10-bin)":"0.258",
        "TruthfulQA - ECE (10-bin)":"0.091",
        "IMDB - ECE (10-bin)":"0.158",
        "CivilComments - ECE (10-bin)":"0.408",
        "RAFT - ECE (10-bin)":"0.244"
    },
    {
        "Model":"J1-Grande v2 beta (17B)",
        "Mean win rate":"0.634",
        "MMLU - ECE (10-bin)":"0.139",
        "BoolQ - ECE (10-bin)":"0.167",
        "NarrativeQA - ECE (10-bin)":"0.041",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.036",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.065",
        "QuAC - ECE (10-bin)":"0.04",
        "HellaSwag - ECE (10-bin)":"0.226",
        "OpenbookQA - ECE (10-bin)":"0.215",
        "TruthfulQA - ECE (10-bin)":"0.123",
        "IMDB - ECE (10-bin)":"0.136",
        "CivilComments - ECE (10-bin)":"0.376",
        "RAFT - ECE (10-bin)":"0.234"
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "Mean win rate":"0.66",
        "MMLU - ECE (10-bin)":"0.137",
        "BoolQ - ECE (10-bin)":"0.175",
        "NarrativeQA - ECE (10-bin)":"0.073",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.018",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.073",
        "QuAC - ECE (10-bin)":"0.035",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.068",
        "IMDB - ECE (10-bin)":"0.182",
        "CivilComments - ECE (10-bin)":"0.314",
        "RAFT - ECE (10-bin)":"0.218"
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "Mean win rate":"0.63",
        "MMLU - ECE (10-bin)":"0.134",
        "BoolQ - ECE (10-bin)":"0.209",
        "NarrativeQA - ECE (10-bin)":"0.126",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.018",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.063",
        "QuAC - ECE (10-bin)":"0.035",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.097",
        "IMDB - ECE (10-bin)":"0.111",
        "CivilComments - ECE (10-bin)":"0.381",
        "RAFT - ECE (10-bin)":"0.232"
    },
    {
        "Model":"Jurassic-2 Large (7.5B)",
        "Mean win rate":"0.644",
        "MMLU - ECE (10-bin)":"0.141",
        "BoolQ - ECE (10-bin)":"0.147",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.014",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.084",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.102",
        "IMDB - ECE (10-bin)":"0.178",
        "CivilComments - ECE (10-bin)":"0.19",
        "RAFT - ECE (10-bin)":"0.254"
    },
    {
        "Model":"Luminous Base (13B)",
        "Mean win rate":"0.641",
        "MMLU - ECE (10-bin)":"0.111",
        "BoolQ - ECE (10-bin)":"0.066",
        "NarrativeQA - ECE (10-bin)":"0.048",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.045",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.07",
        "QuAC - ECE (10-bin)":"0.098",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.081",
        "IMDB - ECE (10-bin)":"0.232",
        "CivilComments - ECE (10-bin)":"0.28",
        "RAFT - ECE (10-bin)":"0.29"
    },
    {
        "Model":"Luminous Extended (30B)",
        "Mean win rate":"0.577",
        "MMLU - ECE (10-bin)":"0.135",
        "BoolQ - ECE (10-bin)":"0.129",
        "NarrativeQA - ECE (10-bin)":"0.046",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.022",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.09",
        "QuAC - ECE (10-bin)":"0.096",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.064",
        "IMDB - ECE (10-bin)":"0.204",
        "CivilComments - ECE (10-bin)":"0.359",
        "RAFT - ECE (10-bin)":"0.29"
    },
    {
        "Model":"Luminous Supreme (70B)",
        "Mean win rate":"0.624",
        "MMLU - ECE (10-bin)":"0.154",
        "BoolQ - ECE (10-bin)":"0.083",
        "NarrativeQA - ECE (10-bin)":"0.049",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.041",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.074",
        "QuAC - ECE (10-bin)":"0.058",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.092",
        "IMDB - ECE (10-bin)":"0.173",
        "CivilComments - ECE (10-bin)":"0.272",
        "RAFT - ECE (10-bin)":"0.238"
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "IMDB - ECE (10-bin)":"-",
        "CivilComments - ECE (10-bin)":"-",
        "RAFT - ECE (10-bin)":"-"
    },
    {
        "Model":"BLOOM (176B)",
        "Mean win rate":"0.348",
        "MMLU - ECE (10-bin)":"0.137",
        "BoolQ - ECE (10-bin)":"0.209",
        "NarrativeQA - ECE (10-bin)":"0.237",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.116",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.347",
        "QuAC - ECE (10-bin)":"0.122",
        "HellaSwag - ECE (10-bin)":"0.293",
        "OpenbookQA - ECE (10-bin)":"0.248",
        "TruthfulQA - ECE (10-bin)":"0.096",
        "IMDB - ECE (10-bin)":"0.343",
        "CivilComments - ECE (10-bin)":"0.262",
        "RAFT - ECE (10-bin)":"0.44"
    },
    {
        "Model":"T0pp (11B)",
        "Mean win rate":"0.758",
        "MMLU - ECE (10-bin)":"0.168",
        "BoolQ - ECE (10-bin)":"0.322",
        "NarrativeQA - ECE (10-bin)":"0",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0",
        "QuAC - ECE (10-bin)":"0.001",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.154",
        "IMDB - ECE (10-bin)":"0.291",
        "CivilComments - ECE (10-bin)":"0.308",
        "RAFT - ECE (10-bin)":"0.086"
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B)",
        "Mean win rate":"0.543",
        "MMLU - ECE (10-bin)":"0.149",
        "BoolQ - ECE (10-bin)":"0.04",
        "NarrativeQA - ECE (10-bin)":"0.062",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.068",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.085",
        "QuAC - ECE (10-bin)":"0.067",
        "HellaSwag - ECE (10-bin)":"0.341",
        "OpenbookQA - ECE (10-bin)":"0.235",
        "TruthfulQA - ECE (10-bin)":"0.099",
        "IMDB - ECE (10-bin)":"0.069",
        "CivilComments - ECE (10-bin)":"0.327",
        "RAFT - ECE (10-bin)":"0.274"
    },
    {
        "Model":"Cohere large v20220720 (13.1B)",
        "Mean win rate":"0.652",
        "MMLU - ECE (10-bin)":"0.112",
        "BoolQ - ECE (10-bin)":"0.088",
        "NarrativeQA - ECE (10-bin)":"0.037",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.025",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.143",
        "QuAC - ECE (10-bin)":"0.033",
        "HellaSwag - ECE (10-bin)":"0.288",
        "OpenbookQA - ECE (10-bin)":"0.225",
        "TruthfulQA - ECE (10-bin)":"0.105",
        "IMDB - ECE (10-bin)":"0.132",
        "CivilComments - ECE (10-bin)":"0.384",
        "RAFT - ECE (10-bin)":"0.267"
    },
    {
        "Model":"Cohere medium v20220720 (6.1B)",
        "Mean win rate":"0.51",
        "MMLU - ECE (10-bin)":"0.114",
        "BoolQ - ECE (10-bin)":"0.082",
        "NarrativeQA - ECE (10-bin)":"0.047",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.026",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.142",
        "QuAC - ECE (10-bin)":"0.048",
        "HellaSwag - ECE (10-bin)":"0.271",
        "OpenbookQA - ECE (10-bin)":"0.275",
        "TruthfulQA - ECE (10-bin)":"0.094",
        "IMDB - ECE (10-bin)":"0.36",
        "CivilComments - ECE (10-bin)":"0.459",
        "RAFT - ECE (10-bin)":"0.304"
    },
    {
        "Model":"Cohere small v20220720 (410M)",
        "Mean win rate":"0.609",
        "MMLU - ECE (10-bin)":"0.136",
        "BoolQ - ECE (10-bin)":"0.095",
        "NarrativeQA - ECE (10-bin)":"0.031",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.023",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.198",
        "QuAC - ECE (10-bin)":"0.036",
        "HellaSwag - ECE (10-bin)":"0.083",
        "OpenbookQA - ECE (10-bin)":"0.379",
        "TruthfulQA - ECE (10-bin)":"0.076",
        "IMDB - ECE (10-bin)":"0.134",
        "CivilComments - ECE (10-bin)":"0.486",
        "RAFT - ECE (10-bin)":"0.234"
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B)",
        "Mean win rate":"0.585",
        "MMLU - ECE (10-bin)":"0.143",
        "BoolQ - ECE (10-bin)":"0.051",
        "NarrativeQA - ECE (10-bin)":"0.059",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.054",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.073",
        "QuAC - ECE (10-bin)":"0.063",
        "HellaSwag - ECE (10-bin)":"0.333",
        "OpenbookQA - ECE (10-bin)":"0.207",
        "TruthfulQA - ECE (10-bin)":"0.211",
        "IMDB - ECE (10-bin)":"0.069",
        "CivilComments - ECE (10-bin)":"0.313",
        "RAFT - ECE (10-bin)":"0.25"
    },
    {
        "Model":"Cohere medium v20221108 (6.1B)",
        "Mean win rate":"0.601",
        "MMLU - ECE (10-bin)":"0.113",
        "BoolQ - ECE (10-bin)":"0.095",
        "NarrativeQA - ECE (10-bin)":"0.028",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.015",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.233",
        "QuAC - ECE (10-bin)":"0.041",
        "HellaSwag - ECE (10-bin)":"0.281",
        "OpenbookQA - ECE (10-bin)":"0.23",
        "TruthfulQA - ECE (10-bin)":"0.08",
        "IMDB - ECE (10-bin)":"0.36",
        "CivilComments - ECE (10-bin)":"0.487",
        "RAFT - ECE (10-bin)":"0.253"
    },
    {
        "Model":"Cohere Command beta (6.1B)",
        "Mean win rate":"0.529",
        "MMLU - ECE (10-bin)":"0.155",
        "BoolQ - ECE (10-bin)":"0.059",
        "NarrativeQA - ECE (10-bin)":"0.076",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.042",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.057",
        "QuAC - ECE (10-bin)":"0.062",
        "HellaSwag - ECE (10-bin)":"0.293",
        "OpenbookQA - ECE (10-bin)":"0.25",
        "TruthfulQA - ECE (10-bin)":"0.3",
        "IMDB - ECE (10-bin)":"0.014",
        "CivilComments - ECE (10-bin)":"0.358",
        "RAFT - ECE (10-bin)":"0.274"
    },
    {
        "Model":"Cohere Command beta (52.4B)",
        "Mean win rate":"0.596",
        "MMLU - ECE (10-bin)":"0.183",
        "BoolQ - ECE (10-bin)":"0.023",
        "NarrativeQA - ECE (10-bin)":"0.058",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.084",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.056",
        "QuAC - ECE (10-bin)":"0.06",
        "HellaSwag - ECE (10-bin)":"0.325",
        "OpenbookQA - ECE (10-bin)":"0.231",
        "TruthfulQA - ECE (10-bin)":"0.311",
        "IMDB - ECE (10-bin)":"0.015",
        "CivilComments - ECE (10-bin)":"0.161",
        "RAFT - ECE (10-bin)":"0.262"
    },
    {
        "Model":"GPT-J (6B)",
        "Mean win rate":"0.464",
        "MMLU - ECE (10-bin)":"0.115",
        "BoolQ - ECE (10-bin)":"0.062",
        "NarrativeQA - ECE (10-bin)":"0.199",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.075",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.354",
        "QuAC - ECE (10-bin)":"0.13",
        "HellaSwag - ECE (10-bin)":"0.233",
        "OpenbookQA - ECE (10-bin)":"0.235",
        "TruthfulQA - ECE (10-bin)":"0.078",
        "IMDB - ECE (10-bin)":"0.295",
        "CivilComments - ECE (10-bin)":"0.409",
        "RAFT - ECE (10-bin)":"0.389"
    },
    {
        "Model":"GPT-NeoX (20B)",
        "Mean win rate":"0.422",
        "MMLU - ECE (10-bin)":"0.122",
        "BoolQ - ECE (10-bin)":"0.195",
        "NarrativeQA - ECE (10-bin)":"0.224",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.103",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.373",
        "QuAC - ECE (10-bin)":"0.115",
        "HellaSwag - ECE (10-bin)":"0.277",
        "OpenbookQA - ECE (10-bin)":"0.232",
        "TruthfulQA - ECE (10-bin)":"0.058",
        "IMDB - ECE (10-bin)":"0.23",
        "CivilComments - ECE (10-bin)":"0.444",
        "RAFT - ECE (10-bin)":"0.324"
    },
    {
        "Model":"Pythia (6.9B)",
        "Mean win rate":"0.43",
        "MMLU - ECE (10-bin)":"0.136",
        "BoolQ - ECE (10-bin)":"0.106",
        "NarrativeQA - ECE (10-bin)":"0.217",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.07",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.369",
        "QuAC - ECE (10-bin)":"0.1",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.076",
        "IMDB - ECE (10-bin)":"0.302",
        "CivilComments - ECE (10-bin)":"0.259",
        "RAFT - ECE (10-bin)":"0.502"
    },
    {
        "Model":"Pythia (12B)",
        "Mean win rate":"0.374",
        "MMLU - ECE (10-bin)":"0.111",
        "BoolQ - ECE (10-bin)":"0.14",
        "NarrativeQA - ECE (10-bin)":"0.239",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.094",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.39",
        "QuAC - ECE (10-bin)":"0.138",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.094",
        "IMDB - ECE (10-bin)":"0.342",
        "CivilComments - ECE (10-bin)":"0.297",
        "RAFT - ECE (10-bin)":"0.514"
    },
    {
        "Model":"T5 (11B)",
        "Mean win rate":"0.435",
        "MMLU - ECE (10-bin)":"0.151",
        "BoolQ - ECE (10-bin)":"0.433",
        "NarrativeQA - ECE (10-bin)":"0",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.076",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.239",
        "QuAC - ECE (10-bin)":"0",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.143",
        "IMDB - ECE (10-bin)":"0.236",
        "CivilComments - ECE (10-bin)":"0.38",
        "RAFT - ECE (10-bin)":"0.367"
    },
    {
        "Model":"UL2 (20B)",
        "Mean win rate":"0.464",
        "MMLU - ECE (10-bin)":"0.134",
        "BoolQ - ECE (10-bin)":"0.46",
        "NarrativeQA - ECE (10-bin)":"0",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.092",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.179",
        "QuAC - ECE (10-bin)":"0",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.125",
        "IMDB - ECE (10-bin)":"0.225",
        "CivilComments - ECE (10-bin)":"0.404",
        "RAFT - ECE (10-bin)":"0.401"
    },
    {
        "Model":"OPT (175B)",
        "Mean win rate":"0.338",
        "MMLU - ECE (10-bin)":"0.147",
        "BoolQ - ECE (10-bin)":"0.194",
        "NarrativeQA - ECE (10-bin)":"0.254",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.173",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.372",
        "QuAC - ECE (10-bin)":"0.148",
        "HellaSwag - ECE (10-bin)":"0.325",
        "OpenbookQA - ECE (10-bin)":"0.209",
        "TruthfulQA - ECE (10-bin)":"0.054",
        "IMDB - ECE (10-bin)":"0.19",
        "CivilComments - ECE (10-bin)":"0.462",
        "RAFT - ECE (10-bin)":"0.352"
    },
    {
        "Model":"OPT (66B)",
        "Mean win rate":"0.289",
        "MMLU - ECE (10-bin)":"0.135",
        "BoolQ - ECE (10-bin)":"0.2",
        "NarrativeQA - ECE (10-bin)":"0.245",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.141",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.384",
        "QuAC - ECE (10-bin)":"0.154",
        "HellaSwag - ECE (10-bin)":"0.293",
        "OpenbookQA - ECE (10-bin)":"0.237",
        "TruthfulQA - ECE (10-bin)":"0.073",
        "IMDB - ECE (10-bin)":"0.302",
        "CivilComments - ECE (10-bin)":"0.474",
        "RAFT - ECE (10-bin)":"0.468"
    },
    {
        "Model":"LLaMA (7B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "IMDB - ECE (10-bin)":"-",
        "CivilComments - ECE (10-bin)":"-",
        "RAFT - ECE (10-bin)":"-"
    },
    {
        "Model":"LLaMA (13B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "IMDB - ECE (10-bin)":"-",
        "CivilComments - ECE (10-bin)":"-",
        "RAFT - ECE (10-bin)":"-"
    },
    {
        "Model":"LLaMA (30B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "IMDB - ECE (10-bin)":"-",
        "CivilComments - ECE (10-bin)":"-",
        "RAFT - ECE (10-bin)":"-"
    },
    {
        "Model":"LLaMA (65B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "IMDB - ECE (10-bin)":"-",
        "CivilComments - ECE (10-bin)":"-",
        "RAFT - ECE (10-bin)":"-"
    },
    {
        "Model":"Llama 2 (7B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "IMDB - ECE (10-bin)":"-",
        "CivilComments - ECE (10-bin)":"-",
        "RAFT - ECE (10-bin)":"-"
    },
    {
        "Model":"Llama 2 (13B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "IMDB - ECE (10-bin)":"-",
        "CivilComments - ECE (10-bin)":"-",
        "RAFT - ECE (10-bin)":"-"
    },
    {
        "Model":"Llama 2 (70B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "IMDB - ECE (10-bin)":"-",
        "CivilComments - ECE (10-bin)":"-",
        "RAFT - ECE (10-bin)":"-"
    },
    {
        "Model":"Alpaca (7B)",
        "Mean win rate":"0.334",
        "MMLU - ECE (10-bin)":"0.234",
        "BoolQ - ECE (10-bin)":"0.343",
        "NarrativeQA - ECE (10-bin)":"0.046",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.134",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.238",
        "QuAC - ECE (10-bin)":"0.04",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.375",
        "IMDB - ECE (10-bin)":"0.281",
        "CivilComments - ECE (10-bin)":"0.352",
        "RAFT - ECE (10-bin)":"0.33"
    },
    {
        "Model":"Vicuna v1.3 (7B)",
        "Mean win rate":"0.204",
        "MMLU - ECE (10-bin)":"0.176",
        "BoolQ - ECE (10-bin)":"0.322",
        "NarrativeQA - ECE (10-bin)":"0.084",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.162",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.413",
        "QuAC - ECE (10-bin)":"0.109",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.227",
        "IMDB - ECE (10-bin)":"0.348",
        "CivilComments - ECE (10-bin)":"0.346",
        "RAFT - ECE (10-bin)":"0.601"
    },
    {
        "Model":"Vicuna v1.3 (13B)",
        "Mean win rate":"0.275",
        "MMLU - ECE (10-bin)":"0.194",
        "BoolQ - ECE (10-bin)":"0.159",
        "NarrativeQA - ECE (10-bin)":"0.257",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.202",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.43",
        "QuAC - ECE (10-bin)":"0.103",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.316",
        "IMDB - ECE (10-bin)":"0.183",
        "CivilComments - ECE (10-bin)":"0.253",
        "RAFT - ECE (10-bin)":"0.376"
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "IMDB - ECE (10-bin)":"-",
        "CivilComments - ECE (10-bin)":"-",
        "RAFT - ECE (10-bin)":"-"
    },
    {
        "Model":"TNLG v2 (530B)",
        "Mean win rate":"0.615",
        "MMLU - ECE (10-bin)":"0.127",
        "BoolQ - ECE (10-bin)":"0.048",
        "NarrativeQA - ECE (10-bin)":"0.05",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.04",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.075",
        "QuAC - ECE (10-bin)":"0.08",
        "HellaSwag - ECE (10-bin)":"0.322",
        "OpenbookQA - ECE (10-bin)":"0.243",
        "TruthfulQA - ECE (10-bin)":"0.226",
        "IMDB - ECE (10-bin)":"0.087",
        "CivilComments - ECE (10-bin)":"0.213",
        "RAFT - ECE (10-bin)":"0.244"
    },
    {
        "Model":"TNLG v2 (6.7B)",
        "Mean win rate":"0.602",
        "MMLU - ECE (10-bin)":"0.132",
        "BoolQ - ECE (10-bin)":"0.065",
        "NarrativeQA - ECE (10-bin)":"0.046",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.031",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.089",
        "QuAC - ECE (10-bin)":"0.056",
        "HellaSwag - ECE (10-bin)":"0.268",
        "OpenbookQA - ECE (10-bin)":"0.282",
        "TruthfulQA - ECE (10-bin)":"0.117",
        "IMDB - ECE (10-bin)":"0.118",
        "CivilComments - ECE (10-bin)":"0.248",
        "RAFT - ECE (10-bin)":"0.314"
    },
    {
        "Model":"davinci (175B)",
        "Mean win rate":"0.575",
        "MMLU - ECE (10-bin)":"0.132",
        "BoolQ - ECE (10-bin)":"0.072",
        "NarrativeQA - ECE (10-bin)":"0.067",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.061",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.079",
        "QuAC - ECE (10-bin)":"0.068",
        "HellaSwag - ECE (10-bin)":"0.31",
        "OpenbookQA - ECE (10-bin)":"0.204",
        "TruthfulQA - ECE (10-bin)":"0.211",
        "IMDB - ECE (10-bin)":"0.126",
        "CivilComments - ECE (10-bin)":"0.396",
        "RAFT - ECE (10-bin)":"0.222"
    },
    {
        "Model":"curie (6.7B)",
        "Mean win rate":"0.603",
        "MMLU - ECE (10-bin)":"0.138",
        "BoolQ - ECE (10-bin)":"0.079",
        "NarrativeQA - ECE (10-bin)":"0.045",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.017",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.134",
        "QuAC - ECE (10-bin)":"0.043",
        "HellaSwag - ECE (10-bin)":"0.25",
        "OpenbookQA - ECE (10-bin)":"0.26",
        "TruthfulQA - ECE (10-bin)":"0.062",
        "IMDB - ECE (10-bin)":"0.259",
        "CivilComments - ECE (10-bin)":"0.293",
        "RAFT - ECE (10-bin)":"0.319"
    },
    {
        "Model":"babbage (1.3B)",
        "Mean win rate":"0.588",
        "MMLU - ECE (10-bin)":"0.14",
        "BoolQ - ECE (10-bin)":"0.068",
        "NarrativeQA - ECE (10-bin)":"0.027",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.016",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.147",
        "QuAC - ECE (10-bin)":"0.045",
        "HellaSwag - ECE (10-bin)":"0.144",
        "OpenbookQA - ECE (10-bin)":"0.3",
        "TruthfulQA - ECE (10-bin)":"0.142",
        "IMDB - ECE (10-bin)":"0.212",
        "CivilComments - ECE (10-bin)":"0.31",
        "RAFT - ECE (10-bin)":"0.286"
    },
    {
        "Model":"ada (350M)",
        "Mean win rate":"0.616",
        "MMLU - ECE (10-bin)":"0.128",
        "BoolQ - ECE (10-bin)":"0.067",
        "NarrativeQA - ECE (10-bin)":"0.046",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.028",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.18",
        "QuAC - ECE (10-bin)":"0.039",
        "HellaSwag - ECE (10-bin)":"0.057",
        "OpenbookQA - ECE (10-bin)":"0.346",
        "TruthfulQA - ECE (10-bin)":"0.071",
        "IMDB - ECE (10-bin)":"0.274",
        "CivilComments - ECE (10-bin)":"0.355",
        "RAFT - ECE (10-bin)":"0.268"
    },
    {
        "Model":"text-davinci-003",
        "Mean win rate":"0.407",
        "MMLU - ECE (10-bin)":"0.317",
        "BoolQ - ECE (10-bin)":"0.098",
        "NarrativeQA - ECE (10-bin)":"0.37",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.286",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.323",
        "QuAC - ECE (10-bin)":"0.27",
        "HellaSwag - ECE (10-bin)":"0.278",
        "OpenbookQA - ECE (10-bin)":"0.216",
        "TruthfulQA - ECE (10-bin)":"0.348",
        "IMDB - ECE (10-bin)":"0.113",
        "CivilComments - ECE (10-bin)":"0.292",
        "RAFT - ECE (10-bin)":"0.203"
    },
    {
        "Model":"text-davinci-002",
        "Mean win rate":"0.474",
        "MMLU - ECE (10-bin)":"0.176",
        "BoolQ - ECE (10-bin)":"0.064",
        "NarrativeQA - ECE (10-bin)":"0.239",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.341",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.242",
        "QuAC - ECE (10-bin)":"0.274",
        "HellaSwag - ECE (10-bin)":"0.286",
        "OpenbookQA - ECE (10-bin)":"0.238",
        "TruthfulQA - ECE (10-bin)":"0.199",
        "IMDB - ECE (10-bin)":"0.031",
        "CivilComments - ECE (10-bin)":"0.183",
        "RAFT - ECE (10-bin)":"0.212"
    },
    {
        "Model":"text-curie-001",
        "Mean win rate":"0.335",
        "MMLU - ECE (10-bin)":"0.462",
        "BoolQ - ECE (10-bin)":"0.253",
        "NarrativeQA - ECE (10-bin)":"0.221",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.253",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.216",
        "QuAC - ECE (10-bin)":"0.254",
        "HellaSwag - ECE (10-bin)":"0.153",
        "OpenbookQA - ECE (10-bin)":"0.321",
        "TruthfulQA - ECE (10-bin)":"0.355",
        "IMDB - ECE (10-bin)":"0.031",
        "CivilComments - ECE (10-bin)":"0.262",
        "RAFT - ECE (10-bin)":"0.409"
    },
    {
        "Model":"text-babbage-001",
        "Mean win rate":"0.277",
        "MMLU - ECE (10-bin)":"0.311",
        "BoolQ - ECE (10-bin)":"0.344",
        "NarrativeQA - ECE (10-bin)":"0.186",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.522",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.385",
        "QuAC - ECE (10-bin)":"0.24",
        "HellaSwag - ECE (10-bin)":"0.083",
        "OpenbookQA - ECE (10-bin)":"0.362",
        "TruthfulQA - ECE (10-bin)":"0.251",
        "IMDB - ECE (10-bin)":"0.038",
        "CivilComments - ECE (10-bin)":"0.499",
        "RAFT - ECE (10-bin)":"0.295"
    },
    {
        "Model":"text-ada-001",
        "Mean win rate":"0.171",
        "MMLU - ECE (10-bin)":"0.506",
        "BoolQ - ECE (10-bin)":"0.346",
        "NarrativeQA - ECE (10-bin)":"0.319",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.764",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.691",
        "QuAC - ECE (10-bin)":"0.268",
        "HellaSwag - ECE (10-bin)":"0.103",
        "OpenbookQA - ECE (10-bin)":"0.487",
        "TruthfulQA - ECE (10-bin)":"0.465",
        "IMDB - ECE (10-bin)":"0.09",
        "CivilComments - ECE (10-bin)":"0.479",
        "RAFT - ECE (10-bin)":"0.473"
    },
    {
        "Model":"gpt-3.5-turbo-0301",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "IMDB - ECE (10-bin)":"-",
        "CivilComments - ECE (10-bin)":"-",
        "RAFT - ECE (10-bin)":"-"
    },
    {
        "Model":"gpt-3.5-turbo-0613",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "IMDB - ECE (10-bin)":"-",
        "CivilComments - ECE (10-bin)":"-",
        "RAFT - ECE (10-bin)":"-"
    },
    {
        "Model":"RedPajama-INCITE-Base-v1 (3B)",
        "Mean win rate":"0.439",
        "MMLU - ECE (10-bin)":"0.115",
        "BoolQ - ECE (10-bin)":"0.187",
        "NarrativeQA - ECE (10-bin)":"0.234",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.116",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.345",
        "QuAC - ECE (10-bin)":"0.078",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.048",
        "IMDB - ECE (10-bin)":"0.248",
        "CivilComments - ECE (10-bin)":"0.303",
        "RAFT - ECE (10-bin)":"0.502"
    },
    {
        "Model":"RedPajama-INCITE-Instruct-v1 (3B)",
        "Mean win rate":"0.372",
        "MMLU - ECE (10-bin)":"0.124",
        "BoolQ - ECE (10-bin)":"0.141",
        "NarrativeQA - ECE (10-bin)":"0.254",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.12",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.454",
        "QuAC - ECE (10-bin)":"0.1",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.097",
        "IMDB - ECE (10-bin)":"0.04",
        "CivilComments - ECE (10-bin)":"0.383",
        "RAFT - ECE (10-bin)":"0.661"
    },
    {
        "Model":"RedPajama-INCITE-Base (7B)",
        "Mean win rate":"0.409",
        "MMLU - ECE (10-bin)":"0.098",
        "BoolQ - ECE (10-bin)":"0.127",
        "NarrativeQA - ECE (10-bin)":"0.276",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.127",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.396",
        "QuAC - ECE (10-bin)":"0.131",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.063",
        "IMDB - ECE (10-bin)":"0.206",
        "CivilComments - ECE (10-bin)":"0.305",
        "RAFT - ECE (10-bin)":"0.648"
    },
    {
        "Model":"RedPajama-INCITE-Instruct (7B)",
        "Mean win rate":"0.388",
        "MMLU - ECE (10-bin)":"0.143",
        "BoolQ - ECE (10-bin)":"0.035",
        "NarrativeQA - ECE (10-bin)":"0.247",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.142",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.466",
        "QuAC - ECE (10-bin)":"0.074",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.232",
        "IMDB - ECE (10-bin)":"0.159",
        "CivilComments - ECE (10-bin)":"0.102",
        "RAFT - ECE (10-bin)":"0.695"
    },
    {
        "Model":"MPT (30B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "IMDB - ECE (10-bin)":"-",
        "CivilComments - ECE (10-bin)":"-",
        "RAFT - ECE (10-bin)":"-"
    },
    {
        "Model":"MPT-Instruct (30B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "IMDB - ECE (10-bin)":"-",
        "CivilComments - ECE (10-bin)":"-",
        "RAFT - ECE (10-bin)":"-"
    },
    {
        "Model":"Falcon (7B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "IMDB - ECE (10-bin)":"-",
        "CivilComments - ECE (10-bin)":"-",
        "RAFT - ECE (10-bin)":"-"
    },
    {
        "Model":"Falcon-Instruct (7B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "IMDB - ECE (10-bin)":"-",
        "CivilComments - ECE (10-bin)":"-",
        "RAFT - ECE (10-bin)":"-"
    },
    {
        "Model":"Falcon (40B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "IMDB - ECE (10-bin)":"-",
        "CivilComments - ECE (10-bin)":"-",
        "RAFT - ECE (10-bin)":"-"
    },
    {
        "Model":"Falcon-Instruct (40B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "IMDB - ECE (10-bin)":"-",
        "CivilComments - ECE (10-bin)":"-",
        "RAFT - ECE (10-bin)":"-"
    },
    {
        "Model":"GLM (130B)",
        "Mean win rate":"0.652",
        "MMLU - ECE (10-bin)":"0.128",
        "BoolQ - ECE (10-bin)":"0.171",
        "NarrativeQA - ECE (10-bin)":"0.037",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.022",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.076",
        "QuAC - ECE (10-bin)":"0.027",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.088",
        "IMDB - ECE (10-bin)":"0.18",
        "CivilComments - ECE (10-bin)":"0.486",
        "RAFT - ECE (10-bin)":"0.226"
    },
    {
        "Model":"InstructPalmyra (30B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "IMDB - ECE (10-bin)":"-",
        "CivilComments - ECE (10-bin)":"-",
        "RAFT - ECE (10-bin)":"-"
    },
    {
        "Model":"Palmyra X (43B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "IMDB - ECE (10-bin)":"-",
        "CivilComments - ECE (10-bin)":"-",
        "RAFT - ECE (10-bin)":"-"
    },
    {
        "Model":"YaLM (100B)",
        "Mean win rate":"0.402",
        "MMLU - ECE (10-bin)":"0.708",
        "BoolQ - ECE (10-bin)":"0.147",
        "NarrativeQA - ECE (10-bin)":"0.06",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.02",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.086",
        "QuAC - ECE (10-bin)":"0.029",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.679",
        "IMDB - ECE (10-bin)":"0.418",
        "CivilComments - ECE (10-bin)":"0.437",
        "RAFT - ECE (10-bin)":"0.278"
    }
]