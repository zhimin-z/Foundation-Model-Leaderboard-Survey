[
    {
        "Model":"J1-Jumbo v1 (178B)",
        "EM":"-",
        "Denoised inference time (s)":"0.599",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":318.217,
        "# output tokens":5.166,
        "# trials":3
    },
    {
        "Model":"J1-Large v1 (7.5B)",
        "EM":"-",
        "Denoised inference time (s)":"0.424",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":318.217,
        "# output tokens":5.52,
        "# trials":3
    },
    {
        "Model":"J1-Grande v1 (17B)",
        "EM":"-",
        "Denoised inference time (s)":"0.478",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":318.217,
        "# output tokens":5.458,
        "# trials":3
    },
    {
        "Model":"J1-Grande v2 beta (17B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":318.217,
        "# output tokens":5.397,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":318.217,
        "# output tokens":5.369,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":318.217,
        "# output tokens":5.522,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Large (7.5B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":318.217,
        "# output tokens":3.909,
        "# trials":3
    },
    {
        "Model":"Luminous Base (13B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":412.897,
        "# output tokens":0.0,
        "# trials":3
    },
    {
        "Model":"Luminous Extended (30B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":412.897,
        "# output tokens":0.0,
        "# trials":3
    },
    {
        "Model":"Luminous Supreme (70B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":412.897,
        "# output tokens":0.0,
        "# trials":3
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B)",
        "EM":"-",
        "Denoised inference time (s)":"0.816",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":399.15,
        "# output tokens":4.927,
        "# trials":3
    },
    {
        "Model":"BLOOM (176B)",
        "EM":"-",
        "Denoised inference time (s)":"0.299",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":375.423,
        "# output tokens":9.0,
        "# trials":3
    },
    {
        "Model":"T0pp (11B)",
        "EM":"-",
        "Denoised inference time (s)":"0.276",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":410.755,
        "# output tokens":20.0,
        "# trials":3
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B)",
        "EM":"-",
        "Denoised inference time (s)":"0.484",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":404.107,
        "# output tokens":0.0,
        "# trials":3
    },
    {
        "Model":"Cohere large v20220720 (13.1B)",
        "EM":"-",
        "Denoised inference time (s)":"0.3",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":404.107,
        "# output tokens":0.0,
        "# trials":3
    },
    {
        "Model":"Cohere medium v20220720 (6.1B)",
        "EM":"-",
        "Denoised inference time (s)":"0.271",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":404.107,
        "# output tokens":0.0,
        "# trials":3
    },
    {
        "Model":"Cohere small v20220720 (410M)",
        "EM":"-",
        "Denoised inference time (s)":"0.269",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":404.107,
        "# output tokens":0.0,
        "# trials":3
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":404.107,
        "# output tokens":0.0,
        "# trials":3
    },
    {
        "Model":"Cohere medium v20221108 (6.1B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":404.107,
        "# output tokens":0.0,
        "# trials":3
    },
    {
        "Model":"Cohere Command beta (6.1B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":404.107,
        "# output tokens":5.062,
        "# trials":3
    },
    {
        "Model":"Cohere Command beta (52.4B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":404.107,
        "# output tokens":5.056,
        "# trials":3
    },
    {
        "Model":"GPT-J (6B)",
        "EM":"-",
        "Denoised inference time (s)":"0.098",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":399.15,
        "# output tokens":9.526,
        "# trials":3
    },
    {
        "Model":"GPT-NeoX (20B)",
        "EM":"-",
        "Denoised inference time (s)":"0.108",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":400.173,
        "# output tokens":9.153,
        "# trials":3
    },
    {
        "Model":"Pythia (6.9B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":392.173,
        "# output tokens":20.0,
        "# trials":1
    },
    {
        "Model":"Pythia (12B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":392.173,
        "# output tokens":20.0,
        "# trials":1
    },
    {
        "Model":"T5 (11B)",
        "EM":"-",
        "Denoised inference time (s)":"0.464",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":409.755,
        "# output tokens":20.0,
        "# trials":3
    },
    {
        "Model":"UL2 (20B)",
        "EM":"-",
        "Denoised inference time (s)":"0.342",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":413.755,
        "# output tokens":20.0,
        "# trials":3
    },
    {
        "Model":"OPT (175B)",
        "EM":"-",
        "Denoised inference time (s)":"0.22",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":399.15,
        "# output tokens":9.404,
        "# trials":3
    },
    {
        "Model":"OPT (66B)",
        "EM":"-",
        "Denoised inference time (s)":"0.067",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":399.15,
        "# output tokens":9.052,
        "# trials":3
    },
    {
        "Model":"LLaMA (7B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":408.781,
        "# output tokens":20.0,
        "# trials":1
    },
    {
        "Model":"LLaMA (13B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":408.781,
        "# output tokens":20.0,
        "# trials":1
    },
    {
        "Model":"LLaMA (30B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":408.781,
        "# output tokens":20.0,
        "# trials":1
    },
    {
        "Model":"LLaMA (65B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":408.781,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (7B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":408.781,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (13B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":408.781,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (70B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":408.781,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Alpaca (7B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":408.781,
        "# output tokens":8.959,
        "# trials":1
    },
    {
        "Model":"Vicuna v1.3 (7B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":408.781,
        "# output tokens":19.977,
        "# trials":1
    },
    {
        "Model":"Vicuna v1.3 (13B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":408.781,
        "# output tokens":19.546,
        "# trials":1
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":0.0,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"TNLG v2 (530B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":399.15,
        "# output tokens":4.452,
        "# trials":3
    },
    {
        "Model":"TNLG v2 (6.7B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":399.15,
        "# output tokens":5.019,
        "# trials":3
    },
    {
        "Model":"davinci (175B)",
        "EM":"-",
        "Denoised inference time (s)":"0.337",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":399.15,
        "# output tokens":4.977,
        "# trials":3
    },
    {
        "Model":"curie (6.7B)",
        "EM":"-",
        "Denoised inference time (s)":"0.119",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":399.15,
        "# output tokens":4.863,
        "# trials":3
    },
    {
        "Model":"babbage (1.3B)",
        "EM":"-",
        "Denoised inference time (s)":"0.145",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":399.15,
        "# output tokens":5.273,
        "# trials":3
    },
    {
        "Model":"ada (350M)",
        "EM":"-",
        "Denoised inference time (s)":"0.17",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":399.15,
        "# output tokens":5.871,
        "# trials":3
    },
    {
        "Model":"text-davinci-003",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":399.15,
        "# output tokens":5.037,
        "# trials":3
    },
    {
        "Model":"text-davinci-002",
        "EM":"-",
        "Denoised inference time (s)":"0.329",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":399.15,
        "# output tokens":5.105,
        "# trials":3
    },
    {
        "Model":"text-curie-001",
        "EM":"-",
        "Denoised inference time (s)":"0.159",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":399.15,
        "# output tokens":5.096,
        "# trials":3
    },
    {
        "Model":"text-babbage-001",
        "EM":"-",
        "Denoised inference time (s)":"0.161",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":399.15,
        "# output tokens":5.173,
        "# trials":3
    },
    {
        "Model":"text-ada-001",
        "EM":"-",
        "Denoised inference time (s)":"0.116",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":399.15,
        "# output tokens":5.87,
        "# trials":3
    },
    {
        "Model":"code-davinci-002",
        "EM":"-",
        "Denoised inference time (s)":"0.158",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":399.15,
        "# output tokens":0.0,
        "# trials":3
    },
    {
        "Model":"code-cushman-001 (12B)",
        "EM":"-",
        "Denoised inference time (s)":"0.098",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":399.15,
        "# output tokens":0.0,
        "# trials":3
    },
    {
        "Model":"gpt-3.5-turbo-0301",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":347.711,
        "# output tokens":5.54,
        "# trials":1
    },
    {
        "Model":"gpt-3.5-turbo-0613",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":347.711,
        "# output tokens":5.041,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Base-v1 (3B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":392.173,
        "# output tokens":20.0,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Instruct-v1 (3B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":392.173,
        "# output tokens":20.0,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Base (7B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":392.173,
        "# output tokens":20.0,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Instruct (7B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":392.173,
        "# output tokens":20.0,
        "# trials":1
    },
    {
        "Model":"MPT (30B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":392.173,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"MPT-Instruct (30B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":392.173,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon (7B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":390.398,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon-Instruct (7B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":390.398,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon (40B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":390.398,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon-Instruct (40B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":390.398,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"GLM (130B)",
        "EM":"-",
        "Denoised inference time (s)":"0.331",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":370.468,
        "# output tokens":5.868,
        "# trials":3
    },
    {
        "Model":"InstructPalmyra (30B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":399.15,
        "# output tokens":5.017,
        "# trials":3
    },
    {
        "Model":"Palmyra X (43B)",
        "EM":"-",
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":399.15,
        "# output tokens":5.152,
        "# trials":3
    },
    {
        "Model":"YaLM (100B)",
        "EM":"-",
        "Denoised inference time (s)":"0.208",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":399.944,
        "# output tokens":15.809,
        "# trials":3
    }
]