[
    {
        "Model":"J1-Jumbo v1 (178B)",
        "EM":0.841,
        "Denoised inference time (s)":"0.693",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":895.507,
        "# output tokens":2.0,
        "# trials":3
    },
    {
        "Model":"J1-Large v1 (7.5B)",
        "EM":0.827,
        "Denoised inference time (s)":"0.533",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":895.507,
        "# output tokens":2.0,
        "# trials":3
    },
    {
        "Model":"J1-Grande v1 (17B)",
        "EM":0.831,
        "Denoised inference time (s)":"0.603",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":895.507,
        "# output tokens":2.0,
        "# trials":3
    },
    {
        "Model":"J1-Grande v2 beta (17B)",
        "EM":0.844,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":895.507,
        "# output tokens":2.0,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "EM":0.79,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":895.507,
        "# output tokens":2.0,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "EM":0.849,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":895.507,
        "# output tokens":2.0,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Large (7.5B)",
        "EM":0.836,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":895.507,
        "# output tokens":2.0,
        "# trials":3
    },
    {
        "Model":"Luminous Base (13B)",
        "EM":0.543,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":889.812,
        "# output tokens":1.0,
        "# trials":3
    },
    {
        "Model":"Luminous Extended (30B)",
        "EM":0.635,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":889.812,
        "# output tokens":1.0,
        "# trials":3
    },
    {
        "Model":"Luminous Supreme (70B)",
        "EM":0.707,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":889.812,
        "# output tokens":1.0,
        "# trials":3
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B)",
        "EM":0.71,
        "Denoised inference time (s)":"0.633",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":890.493,
        "# output tokens":1.0,
        "# trials":3
    },
    {
        "Model":"BLOOM (176B)",
        "EM":0.852,
        "Denoised inference time (s)":"0.568",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":850.495,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"T0pp (11B)",
        "EM":0.0,
        "Denoised inference time (s)":"0.168",
        "# eval":231,
        "# train":4.138,
        "truncated":0.0,
        "# prompt tokens":848.446,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B)",
        "EM":0.823,
        "Denoised inference time (s)":"0.587",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":887.65,
        "# output tokens":1.0,
        "# trials":3
    },
    {
        "Model":"Cohere large v20220720 (13.1B)",
        "EM":0.713,
        "Denoised inference time (s)":"0.412",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":887.65,
        "# output tokens":1.0,
        "# trials":3
    },
    {
        "Model":"Cohere medium v20220720 (6.1B)",
        "EM":0.482,
        "Denoised inference time (s)":"0.343",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":887.65,
        "# output tokens":1.0,
        "# trials":3
    },
    {
        "Model":"Cohere small v20220720 (410M)",
        "EM":0.176,
        "Denoised inference time (s)":"0.361",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":887.65,
        "# output tokens":1.0,
        "# trials":3
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B)",
        "EM":0.812,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":887.65,
        "# output tokens":1.0,
        "# trials":3
    },
    {
        "Model":"Cohere medium v20221108 (6.1B)",
        "EM":0.535,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":887.65,
        "# output tokens":1.0,
        "# trials":3
    },
    {
        "Model":"Cohere Command beta (6.1B)",
        "EM":0.532,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":887.65,
        "# output tokens":1.0,
        "# trials":3
    },
    {
        "Model":"Cohere Command beta (52.4B)",
        "EM":0.569,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":887.65,
        "# output tokens":1.0,
        "# trials":3
    },
    {
        "Model":"GPT-J (6B)",
        "EM":0.194,
        "Denoised inference time (s)":"0.197",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":890.493,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"GPT-NeoX (20B)",
        "EM":0.82,
        "Denoised inference time (s)":"0.247",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":906.402,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"Pythia (6.9B)",
        "EM":0.824,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":942.402,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"Pythia (12B)",
        "EM":0.78,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":942.402,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"T5 (11B)",
        "EM":0.652,
        "Denoised inference time (s)":"0.236",
        "# eval":231,
        "# train":1.66,
        "truncated":0.132,
        "# prompt tokens":421.841,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"UL2 (20B)",
        "EM":0.672,
        "Denoised inference time (s)":"0.222",
        "# eval":231,
        "# train":1.631,
        "truncated":0.143,
        "# prompt tokens":419.916,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"OPT (175B)",
        "EM":0.374,
        "Denoised inference time (s)":"0.63",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":890.493,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"OPT (66B)",
        "EM":0.466,
        "Denoised inference time (s)":"0.554",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":890.493,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"LLaMA (7B)",
        "EM":0.836,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":1109.316,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"LLaMA (13B)",
        "EM":0.859,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":1109.316,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"LLaMA (30B)",
        "EM":0.908,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":1109.316,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"LLaMA (65B)",
        "EM":0.861,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":1109.316,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (7B)",
        "EM":0.851,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":1109.316,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (13B)",
        "EM":0.528,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":1109.316,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (70B)",
        "EM":0.732,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":1109.316,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Alpaca (7B)",
        "EM":0.853,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":1109.316,
        "# output tokens":2.301,
        "# trials":1
    },
    {
        "Model":"Vicuna v1.3 (7B)",
        "EM":0.848,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":1109.316,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"Vicuna v1.3 (13B)",
        "EM":0.879,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":1109.316,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "EM":0.938,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":1086.232,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"TNLG v2 (530B)",
        "EM":0.881,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":890.493,
        "# output tokens":1.0,
        "# trials":3
    },
    {
        "Model":"TNLG v2 (6.7B)",
        "EM":0.694,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":890.493,
        "# output tokens":1.0,
        "# trials":3
    },
    {
        "Model":"davinci (175B)",
        "EM":0.603,
        "Denoised inference time (s)":"0.21",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":890.493,
        "# output tokens":1.0,
        "# trials":3
    },
    {
        "Model":"curie (6.7B)",
        "EM":0.453,
        "Denoised inference time (s)":"0.099",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":890.493,
        "# output tokens":1.0,
        "# trials":3
    },
    {
        "Model":"babbage (1.3B)",
        "EM":0.632,
        "Denoised inference time (s)":"0.121",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":890.493,
        "# output tokens":1.0,
        "# trials":3
    },
    {
        "Model":"ada (350M)",
        "EM":0.349,
        "Denoised inference time (s)":"0.14",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":890.493,
        "# output tokens":1.0,
        "# trials":3
    },
    {
        "Model":"text-davinci-003",
        "EM":0.93,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":890.493,
        "# output tokens":1.0,
        "# trials":3
    },
    {
        "Model":"text-davinci-002",
        "EM":0.931,
        "Denoised inference time (s)":"0.19",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":890.493,
        "# output tokens":1.0,
        "# trials":3
    },
    {
        "Model":"text-curie-001",
        "EM":0.852,
        "Denoised inference time (s)":"0.142",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":890.493,
        "# output tokens":1.0,
        "# trials":3
    },
    {
        "Model":"text-babbage-001",
        "EM":0.734,
        "Denoised inference time (s)":"0.142",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":890.493,
        "# output tokens":1.0,
        "# trials":3
    },
    {
        "Model":"text-ada-001",
        "EM":0.808,
        "Denoised inference time (s)":"0.095",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":890.493,
        "# output tokens":1.0,
        "# trials":3
    },
    {
        "Model":"gpt-3.5-turbo-0301",
        "EM":0.943,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":908.418,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"gpt-3.5-turbo-0613",
        "EM":0.918,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":908.418,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Base-v1 (3B)",
        "EM":0.533,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":942.402,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Instruct-v1 (3B)",
        "EM":0.828,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":942.402,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Base (7B)",
        "EM":0.554,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":942.402,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Instruct (7B)",
        "EM":0.843,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":942.402,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"MPT (30B)",
        "EM":0.897,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":942.402,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"MPT-Instruct (30B)",
        "EM":0.895,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":942.402,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon (7B)",
        "EM":0.602,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":1019.896,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon-Instruct (7B)",
        "EM":0.824,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":1019.896,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon (40B)",
        "EM":0.847,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":1019.896,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon-Instruct (40B)",
        "EM":0.872,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":1019.896,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"GLM (130B)",
        "EM":0.472,
        "Denoised inference time (s)":"0.657",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":1041.057,
        "# output tokens":2.0,
        "# trials":3
    },
    {
        "Model":"InstructPalmyra (30B)",
        "EM":0.77,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":890.493,
        "# output tokens":1.0,
        "# trials":3
    },
    {
        "Model":"Palmyra X (43B)",
        "EM":0.966,
        "Denoised inference time (s)":"-",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":890.493,
        "# output tokens":1.0,
        "# trials":3
    },
    {
        "Model":"YaLM (100B)",
        "EM":0.176,
        "Denoised inference time (s)":"0.516",
        "# eval":231,
        "# train":5.0,
        "truncated":0.0,
        "# prompt tokens":848.586,
        "# output tokens":5.0,
        "# trials":3
    }
]