[
    {
        "Model":"J1-Jumbo v1 (178B)",
        "EM":0.098,
        "Denoised inference time (s)":"0.487",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":279.717,
        "# output tokens":3.0,
        "# trials":3
    },
    {
        "Model":"J1-Large v1 (7.5B)",
        "EM":0.073,
        "Denoised inference time (s)":"0.37",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":279.717,
        "# output tokens":3.0,
        "# trials":3
    },
    {
        "Model":"J1-Grande v1 (17B)",
        "EM":0.092,
        "Denoised inference time (s)":"0.413",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":279.717,
        "# output tokens":3.0,
        "# trials":3
    },
    {
        "Model":"J1-Grande v2 beta (17B)",
        "EM":0.097,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":279.717,
        "# output tokens":2.994,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "EM":0.089,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":279.717,
        "# output tokens":3.0,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "EM":0.087,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":279.717,
        "# output tokens":3.0,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Large (7.5B)",
        "EM":0.077,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":279.717,
        "# output tokens":3.0,
        "# trials":3
    },
    {
        "Model":"Luminous Base (13B)",
        "EM":0.074,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":416.728,
        "# output tokens":2.0,
        "# trials":3
    },
    {
        "Model":"Luminous Extended (30B)",
        "EM":0.07,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":416.728,
        "# output tokens":2.0,
        "# trials":3
    },
    {
        "Model":"Luminous Supreme (70B)",
        "EM":0.088,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":416.728,
        "# output tokens":2.0,
        "# trials":3
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B)",
        "EM":0.092,
        "Denoised inference time (s)":"0.651",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":2.0,
        "# trials":3
    },
    {
        "Model":"BLOOM (176B)",
        "EM":0.09,
        "Denoised inference time (s)":"0.286",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":370.728,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"T0pp (11B)",
        "EM":0.0,
        "Denoised inference time (s)":"0.167",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":368.0,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B)",
        "EM":0.065,
        "Denoised inference time (s)":"0.524",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":2.0,
        "# trials":3
    },
    {
        "Model":"Cohere large v20220720 (13.1B)",
        "EM":0.07,
        "Denoised inference time (s)":"0.323",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":2.0,
        "# trials":3
    },
    {
        "Model":"Cohere medium v20220720 (6.1B)",
        "EM":0.08,
        "Denoised inference time (s)":"0.283",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":2.0,
        "# trials":3
    },
    {
        "Model":"Cohere small v20220720 (410M)",
        "EM":0.1,
        "Denoised inference time (s)":"0.275",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":2.0,
        "# trials":3
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B)",
        "EM":0.085,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":2.0,
        "# trials":3
    },
    {
        "Model":"Cohere medium v20221108 (6.1B)",
        "EM":0.087,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":2.0,
        "# trials":3
    },
    {
        "Model":"Cohere Command beta (6.1B)",
        "EM":0.081,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":2.0,
        "# trials":3
    },
    {
        "Model":"Cohere Command beta (52.4B)",
        "EM":0.094,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":1.981,
        "# trials":3
    },
    {
        "Model":"GPT-J (6B)",
        "EM":0.076,
        "Denoised inference time (s)":"0.346",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"GPT-NeoX (20B)",
        "EM":0.085,
        "Denoised inference time (s)":"0.347",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"Pythia (1B)",
        "EM":0.075,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"Pythia (6.9B)",
        "EM":0.076,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"Pythia (12B)",
        "EM":0.061,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"T5 (11B)",
        "EM":0.063,
        "Denoised inference time (s)":"0.229",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":368.0,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"UL2 (20B)",
        "EM":0.077,
        "Denoised inference time (s)":"0.17",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":372.0,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"OPT (175B)",
        "EM":0.073,
        "Denoised inference time (s)":"1.011",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"OPT (66B)",
        "EM":0.078,
        "Denoised inference time (s)":"0.209",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"LLaMA (7B)",
        "EM":0.059,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":443.04,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"LLaMA (13B)",
        "EM":0.116,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":443.04,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"LLaMA (30B)",
        "EM":0.129,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":443.04,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"LLaMA (65B)",
        "EM":0.147,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":443.04,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (7B)",
        "EM":0.066,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":443.04,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (13B)",
        "EM":0.102,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":443.04,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (70B)",
        "EM":0.204,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":443.04,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Alpaca (7B)",
        "EM":0.061,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":443.04,
        "# output tokens":3.0,
        "# trials":1
    },
    {
        "Model":"Vicuna v1.3 (7B)",
        "EM":0.048,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":443.04,
        "# output tokens":4.985,
        "# trials":1
    },
    {
        "Model":"Vicuna v1.3 (13B)",
        "EM":0.079,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":443.04,
        "# output tokens":4.934,
        "# trials":1
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "EM":0.1,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":410.0,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"TNLG v2 (530B)",
        "EM":0.096,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":2.0,
        "# trials":3
    },
    {
        "Model":"TNLG v2 (6.7B)",
        "EM":0.094,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":2.0,
        "# trials":3
    },
    {
        "Model":"davinci (175B)",
        "EM":0.061,
        "Denoised inference time (s)":"0.249",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":2.0,
        "# trials":3
    },
    {
        "Model":"curie (6.7B)",
        "EM":0.073,
        "Denoised inference time (s)":"0.099",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":2.0,
        "# trials":3
    },
    {
        "Model":"babbage (1.3B)",
        "EM":0.074,
        "Denoised inference time (s)":"0.126",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":2.0,
        "# trials":3
    },
    {
        "Model":"ada (350M)",
        "EM":0.092,
        "Denoised inference time (s)":"0.147",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":2.0,
        "# trials":3
    },
    {
        "Model":"text-davinci-003",
        "EM":0.185,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":2.005,
        "# trials":3
    },
    {
        "Model":"text-davinci-002",
        "EM":0.173,
        "Denoised inference time (s)":"0.239",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":1.992,
        "# trials":3
    },
    {
        "Model":"text-curie-001",
        "EM":0.061,
        "Denoised inference time (s)":"0.137",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":1.997,
        "# trials":3
    },
    {
        "Model":"text-babbage-001",
        "EM":0.057,
        "Denoised inference time (s)":"0.138",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":1.996,
        "# trials":3
    },
    {
        "Model":"text-ada-001",
        "EM":0.094,
        "Denoised inference time (s)":"0.093",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":1.958,
        "# trials":3
    },
    {
        "Model":"code-davinci-002",
        "EM":0.211,
        "Denoised inference time (s)":"0.209",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":3.01,
        "# trials":3
    },
    {
        "Model":"code-cushman-001 (12B)",
        "EM":0.086,
        "Denoised inference time (s)":"0.115",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":3.0,
        "# trials":3
    },
    {
        "Model":"gpt-3.5-turbo-0301",
        "EM":0.213,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":363.0,
        "# output tokens":1.932,
        "# trials":1
    },
    {
        "Model":"gpt-3.5-turbo-0613",
        "EM":0.246,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":363.0,
        "# output tokens":2.004,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Base-v1 (3B)",
        "EM":0.059,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Instruct-v1 (3B)",
        "EM":0.066,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Base (7B)",
        "EM":0.085,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Instruct (7B)",
        "EM":0.036,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"MPT (30B)",
        "EM":0.09,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"MPT-Instruct (30B)",
        "EM":0.09,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon (7B)",
        "EM":0.074,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon-Instruct (7B)",
        "EM":0.086,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon (40B)",
        "EM":0.112,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon-Instruct (40B)",
        "EM":0.108,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"GLM (130B)",
        "EM":0.092,
        "Denoised inference time (s)":"0.906",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":362.0,
        "# output tokens":3.0,
        "# trials":3
    },
    {
        "Model":"InstructPalmyra (30B)",
        "EM":0.054,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":2.0,
        "# trials":3
    },
    {
        "Model":"Palmyra X (43B)",
        "EM":0.223,
        "Denoised inference time (s)":"-",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":1.913,
        "# trials":3
    },
    {
        "Model":"YaLM (100B)",
        "EM":0.088,
        "Denoised inference time (s)":"0.096",
        "# eval":909,
        "# train":5,
        "truncated":0,
        "# prompt tokens":404.0,
        "# output tokens":5.0,
        "# trials":3
    }
]