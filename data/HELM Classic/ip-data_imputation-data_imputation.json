[
    {
        "Model":"J1-Jumbo v1 (178B)",
        "EM":0.735,
        "Denoised inference time (s)":"0.466",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":297.311,
        "# output tokens":2.549,
        "# trials":3
    },
    {
        "Model":"J1-Large v1 (7.5B)",
        "EM":0.729,
        "Denoised inference time (s)":"0.355",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":297.311,
        "# output tokens":2.494,
        "# trials":3
    },
    {
        "Model":"J1-Grande v1 (17B)",
        "EM":0.729,
        "Denoised inference time (s)":"0.47",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":297.311,
        "# output tokens":2.503,
        "# trials":3
    },
    {
        "Model":"J1-Grande v2 beta (17B)",
        "EM":0.8,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":297.311,
        "# output tokens":2.483,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "EM":0.841,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":297.311,
        "# output tokens":2.378,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "EM":0.829,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":297.311,
        "# output tokens":2.387,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Large (7.5B)",
        "EM":0.731,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":297.311,
        "# output tokens":2.563,
        "# trials":3
    },
    {
        "Model":"Luminous Base (13B)",
        "EM":0.725,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":306.525,
        "# output tokens":1.995,
        "# trials":3
    },
    {
        "Model":"Luminous Extended (30B)",
        "EM":0.722,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":306.525,
        "# output tokens":1.962,
        "# trials":3
    },
    {
        "Model":"Luminous Supreme (70B)",
        "EM":0.758,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":306.525,
        "# output tokens":1.923,
        "# trials":3
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B)",
        "EM":0.733,
        "Denoised inference time (s)":"0.686",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":307.878,
        "# output tokens":2.575,
        "# trials":3
    },
    {
        "Model":"BLOOM (176B)",
        "EM":0.677,
        "Denoised inference time (s)":"0.461",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":274.468,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"T0pp (11B)",
        "EM":0.004,
        "Denoised inference time (s)":"0.167",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":339.538,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B)",
        "EM":0.785,
        "Denoised inference time (s)":"0.501",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":312.664,
        "# output tokens":2.101,
        "# trials":3
    },
    {
        "Model":"Cohere large v20220720 (13.1B)",
        "EM":0.71,
        "Denoised inference time (s)":"0.308",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":312.664,
        "# output tokens":2.188,
        "# trials":3
    },
    {
        "Model":"Cohere medium v20220720 (6.1B)",
        "EM":0.721,
        "Denoised inference time (s)":"0.265",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":312.664,
        "# output tokens":2.207,
        "# trials":3
    },
    {
        "Model":"Cohere small v20220720 (410M)",
        "EM":0.47,
        "Denoised inference time (s)":"0.261",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":312.664,
        "# output tokens":2.139,
        "# trials":3
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B)",
        "EM":0.803,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":312.664,
        "# output tokens":2.111,
        "# trials":3
    },
    {
        "Model":"Cohere medium v20221108 (6.1B)",
        "EM":0.72,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":312.664,
        "# output tokens":2.214,
        "# trials":3
    },
    {
        "Model":"Cohere Command beta (6.1B)",
        "EM":0.696,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":312.664,
        "# output tokens":2.192,
        "# trials":3
    },
    {
        "Model":"Cohere Command beta (52.4B)",
        "EM":0.752,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":312.664,
        "# output tokens":2.184,
        "# trials":3
    },
    {
        "Model":"GPT-J (6B)",
        "EM":0.673,
        "Denoised inference time (s)":"0.152",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":307.878,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"GPT-NeoX (20B)",
        "EM":0.705,
        "Denoised inference time (s)":"0.305",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":314.872,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"Pythia (6.9B)",
        "EM":0.644,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":291.539,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"Pythia (12B)",
        "EM":0.683,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":291.539,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"T5 (11B)",
        "EM":0.624,
        "Denoised inference time (s)":"0.231",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":339.538,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"UL2 (20B)",
        "EM":0.611,
        "Denoised inference time (s)":"0.222",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":343.538,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"OPT (175B)",
        "EM":0.722,
        "Denoised inference time (s)":"0.451",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":307.878,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"OPT (66B)",
        "EM":0.718,
        "Denoised inference time (s)":"0.057",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":307.878,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"LLaMA (7B)",
        "EM":0.834,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":370.63,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"LLaMA (13B)",
        "EM":0.824,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":370.63,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"LLaMA (30B)",
        "EM":0.844,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":370.63,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"LLaMA (65B)",
        "EM":0.822,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":370.63,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (7B)",
        "EM":0.801,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":370.63,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (13B)",
        "EM":0.844,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":370.63,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (70B)",
        "EM":0.838,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":370.63,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Alpaca (7B)",
        "EM":0.735,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":370.63,
        "# output tokens":4.898,
        "# trials":1
    },
    {
        "Model":"Vicuna v1.3 (7B)",
        "EM":0.708,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":370.63,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"Vicuna v1.3 (13B)",
        "EM":0.714,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":370.63,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "EM":0.795,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":370.568,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"TNLG v2 (530B)",
        "EM":0.811,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":307.878,
        "# output tokens":2.262,
        "# trials":3
    },
    {
        "Model":"TNLG v2 (6.7B)",
        "EM":0.826,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":307.878,
        "# output tokens":2.208,
        "# trials":3
    },
    {
        "Model":"davinci (175B)",
        "EM":0.836,
        "Denoised inference time (s)":"0.25",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":307.878,
        "# output tokens":2.21,
        "# trials":3
    },
    {
        "Model":"curie (6.7B)",
        "EM":0.811,
        "Denoised inference time (s)":"0.1",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":307.878,
        "# output tokens":2.254,
        "# trials":3
    },
    {
        "Model":"babbage (1.3B)",
        "EM":0.599,
        "Denoised inference time (s)":"0.127",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":307.878,
        "# output tokens":2.473,
        "# trials":3
    },
    {
        "Model":"ada (350M)",
        "EM":0.582,
        "Denoised inference time (s)":"0.148",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":307.878,
        "# output tokens":2.2,
        "# trials":3
    },
    {
        "Model":"text-davinci-003",
        "EM":0.839,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":307.878,
        "# output tokens":2.218,
        "# trials":3
    },
    {
        "Model":"text-davinci-002",
        "EM":0.842,
        "Denoised inference time (s)":"0.236",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":307.878,
        "# output tokens":2.222,
        "# trials":3
    },
    {
        "Model":"text-curie-001",
        "EM":0.791,
        "Denoised inference time (s)":"0.141",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":307.878,
        "# output tokens":2.237,
        "# trials":3
    },
    {
        "Model":"text-babbage-001",
        "EM":0.704,
        "Denoised inference time (s)":"0.137",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":307.878,
        "# output tokens":2.033,
        "# trials":3
    },
    {
        "Model":"text-ada-001",
        "EM":0.597,
        "Denoised inference time (s)":"0.092",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":307.878,
        "# output tokens":1.96,
        "# trials":3
    },
    {
        "Model":"gpt-3.5-turbo-0301",
        "EM":0.778,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":277.848,
        "# output tokens":2.193,
        "# trials":1
    },
    {
        "Model":"gpt-3.5-turbo-0613",
        "EM":0.836,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":277.848,
        "# output tokens":2.042,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Base-v1 (3B)",
        "EM":0.828,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":291.539,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Instruct-v1 (3B)",
        "EM":0.752,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":291.539,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Base (7B)",
        "EM":0.706,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":291.539,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Instruct (7B)",
        "EM":0.772,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":291.539,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"MPT (30B)",
        "EM":0.855,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":291.539,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"MPT-Instruct (30B)",
        "EM":0.826,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":291.539,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon (7B)",
        "EM":0.725,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":304.048,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon-Instruct (7B)",
        "EM":0.836,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":304.048,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon (40B)",
        "EM":0.83,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":304.048,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon-Instruct (40B)",
        "EM":0.727,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":304.048,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"GLM (130B)",
        "EM":0.66,
        "Denoised inference time (s)":"0.762",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":368.375,
        "# output tokens":3.457,
        "# trials":3
    },
    {
        "Model":"InstructPalmyra (30B)",
        "EM":0.833,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":307.878,
        "# output tokens":2.205,
        "# trials":3
    },
    {
        "Model":"Palmyra X (43B)",
        "EM":0.824,
        "Denoised inference time (s)":"-",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":307.878,
        "# output tokens":2.334,
        "# trials":3
    },
    {
        "Model":"YaLM (100B)",
        "EM":0.419,
        "Denoised inference time (s)":"0.378",
        "# eval":75.5,
        "# train":5,
        "truncated":0,
        "# prompt tokens":287.091,
        "# output tokens":5.0,
        "# trials":3
    }
]