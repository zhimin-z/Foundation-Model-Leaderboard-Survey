[
    {
        "Model":"J1-Jumbo v1 (178B)",
        "EM":0.877,
        "Denoised inference time (s)":"0.496",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":350.877,
        "# output tokens":2.308,
        "# trials":3
    },
    {
        "Model":"J1-Large v1 (7.5B)",
        "EM":0.841,
        "Denoised inference time (s)":"0.383",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":350.877,
        "# output tokens":2.256,
        "# trials":3
    },
    {
        "Model":"J1-Grande v1 (17B)",
        "EM":0.831,
        "Denoised inference time (s)":"0.422",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":350.877,
        "# output tokens":2.277,
        "# trials":3
    },
    {
        "Model":"J1-Grande v2 beta (17B)",
        "EM":0.851,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":350.877,
        "# output tokens":2.272,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "EM":0.856,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":350.877,
        "# output tokens":2.272,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "EM":0.856,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":350.877,
        "# output tokens":2.262,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Large (7.5B)",
        "EM":0.877,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":350.877,
        "# output tokens":2.297,
        "# trials":3
    },
    {
        "Model":"Luminous Base (13B)",
        "EM":0.862,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":369.6,
        "# output tokens":1.354,
        "# trials":3
    },
    {
        "Model":"Luminous Extended (30B)",
        "EM":0.836,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":369.6,
        "# output tokens":1.323,
        "# trials":3
    },
    {
        "Model":"Luminous Supreme (70B)",
        "EM":0.831,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":369.6,
        "# output tokens":1.323,
        "# trials":3
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B)",
        "EM":0.892,
        "Denoised inference time (s)":"0.625",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":368.764,
        "# output tokens":1.769,
        "# trials":3
    },
    {
        "Model":"BLOOM (176B)",
        "EM":0.831,
        "Denoised inference time (s)":"0.437",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":342.882,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"T0pp (11B)",
        "EM":0.0,
        "Denoised inference time (s)":"0.168",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":401.39,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B)",
        "EM":0.846,
        "Denoised inference time (s)":"0.487",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":374.959,
        "# output tokens":1.4,
        "# trials":3
    },
    {
        "Model":"Cohere large v20220720 (13.1B)",
        "EM":0.831,
        "Denoised inference time (s)":"0.302",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":374.959,
        "# output tokens":1.385,
        "# trials":3
    },
    {
        "Model":"Cohere medium v20220720 (6.1B)",
        "EM":0.862,
        "Denoised inference time (s)":"0.27",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":374.959,
        "# output tokens":1.41,
        "# trials":3
    },
    {
        "Model":"Cohere small v20220720 (410M)",
        "EM":0.846,
        "Denoised inference time (s)":"0.267",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":374.959,
        "# output tokens":1.405,
        "# trials":3
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B)",
        "EM":0.867,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":374.959,
        "# output tokens":1.462,
        "# trials":3
    },
    {
        "Model":"Cohere medium v20221108 (6.1B)",
        "EM":0.862,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":374.959,
        "# output tokens":1.415,
        "# trials":3
    },
    {
        "Model":"Cohere Command beta (6.1B)",
        "EM":0.831,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":374.959,
        "# output tokens":1.385,
        "# trials":3
    },
    {
        "Model":"Cohere Command beta (52.4B)",
        "EM":0.862,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":374.959,
        "# output tokens":1.431,
        "# trials":3
    },
    {
        "Model":"GPT-J (6B)",
        "EM":0.862,
        "Denoised inference time (s)":"0.121",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":368.764,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"GPT-NeoX (20B)",
        "EM":0.836,
        "Denoised inference time (s)":"0.265",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":382.364,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"Pythia (6.9B)",
        "EM":0.846,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":327.031,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"Pythia (12B)",
        "EM":0.831,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":327.031,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"T5 (11B)",
        "EM":0.841,
        "Denoised inference time (s)":"0.235",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":401.39,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"UL2 (20B)",
        "EM":0.831,
        "Denoised inference time (s)":"0.222",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":405.39,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"OPT (175B)",
        "EM":0.851,
        "Denoised inference time (s)":"0.447",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":368.764,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"OPT (66B)",
        "EM":0.862,
        "Denoised inference time (s)":"0.062",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":368.764,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"LLaMA (7B)",
        "EM":0.831,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":421.923,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"LLaMA (13B)",
        "EM":0.846,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":421.923,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"LLaMA (30B)",
        "EM":0.862,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":421.923,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"LLaMA (65B)",
        "EM":0.831,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":421.923,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (7B)",
        "EM":0.846,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":421.923,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (13B)",
        "EM":0.862,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":421.923,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (70B)",
        "EM":0.862,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":421.923,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Alpaca (7B)",
        "EM":0.877,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":421.923,
        "# output tokens":4.877,
        "# trials":1
    },
    {
        "Model":"Vicuna v1.3 (7B)",
        "EM":0.846,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":421.923,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"Vicuna v1.3 (13B)",
        "EM":0.846,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":421.923,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "EM":0.846,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":419.077,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"TNLG v2 (530B)",
        "EM":0.846,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":368.764,
        "# output tokens":1.385,
        "# trials":3
    },
    {
        "Model":"TNLG v2 (6.7B)",
        "EM":0.846,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":368.764,
        "# output tokens":1.385,
        "# trials":3
    },
    {
        "Model":"davinci (175B)",
        "EM":0.846,
        "Denoised inference time (s)":"0.227",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":368.764,
        "# output tokens":1.385,
        "# trials":3
    },
    {
        "Model":"curie (6.7B)",
        "EM":0.862,
        "Denoised inference time (s)":"0.093",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":368.764,
        "# output tokens":1.4,
        "# trials":3
    },
    {
        "Model":"babbage (1.3B)",
        "EM":0.862,
        "Denoised inference time (s)":"0.121",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":368.764,
        "# output tokens":1.4,
        "# trials":3
    },
    {
        "Model":"ada (350M)",
        "EM":0.846,
        "Denoised inference time (s)":"0.143",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":368.764,
        "# output tokens":1.4,
        "# trials":3
    },
    {
        "Model":"text-davinci-003",
        "EM":0.856,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":368.764,
        "# output tokens":1.436,
        "# trials":3
    },
    {
        "Model":"text-davinci-002",
        "EM":0.862,
        "Denoised inference time (s)":"0.219",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":368.764,
        "# output tokens":1.436,
        "# trials":3
    },
    {
        "Model":"text-curie-001",
        "EM":0.846,
        "Denoised inference time (s)":"0.132",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":368.764,
        "# output tokens":1.385,
        "# trials":3
    },
    {
        "Model":"text-babbage-001",
        "EM":0.862,
        "Denoised inference time (s)":"0.133",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":368.764,
        "# output tokens":1.4,
        "# trials":3
    },
    {
        "Model":"text-ada-001",
        "EM":0.841,
        "Denoised inference time (s)":"0.088",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":368.764,
        "# output tokens":1.4,
        "# trials":3
    },
    {
        "Model":"gpt-3.5-turbo-0301",
        "EM":0.846,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":313.708,
        "# output tokens":1.677,
        "# trials":1
    },
    {
        "Model":"gpt-3.5-turbo-0613",
        "EM":0.846,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":313.708,
        "# output tokens":1.677,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Base-v1 (3B)",
        "EM":0.831,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":327.031,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Instruct-v1 (3B)",
        "EM":0.877,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":327.031,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Base (7B)",
        "EM":0.831,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":327.031,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Instruct (7B)",
        "EM":0.846,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":327.031,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"MPT (30B)",
        "EM":0.862,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":327.031,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"MPT-Instruct (30B)",
        "EM":0.862,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":327.031,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon (7B)",
        "EM":0.846,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":351.631,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon-Instruct (7B)",
        "EM":0.846,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":351.631,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon (40B)",
        "EM":0.846,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":351.631,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon-Instruct (40B)",
        "EM":0.862,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":351.631,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"GLM (130B)",
        "EM":0.851,
        "Denoised inference time (s)":"0.906",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":428.138,
        "# output tokens":2.6,
        "# trials":3
    },
    {
        "Model":"InstructPalmyra (30B)",
        "EM":0.836,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":368.764,
        "# output tokens":1.395,
        "# trials":3
    },
    {
        "Model":"Palmyra X (43B)",
        "EM":0.815,
        "Denoised inference time (s)":"-",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":368.764,
        "# output tokens":1.785,
        "# trials":3
    },
    {
        "Model":"YaLM (100B)",
        "EM":0.733,
        "Denoised inference time (s)":"0.321",
        "# eval":65,
        "# train":5,
        "truncated":0,
        "# prompt tokens":338.938,
        "# output tokens":5.0,
        "# trials":3
    }
]