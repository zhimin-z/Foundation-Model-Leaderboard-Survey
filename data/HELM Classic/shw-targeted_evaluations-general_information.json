[
    {
        "Model":"J1-Jumbo v1 (178B)",
        "Mean win rate":"-",
        "The Pile - # eval":"492",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"1374.206",
        "The Pile - # output tokens":"0",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"12.325",
        "TwitterAAE - # output tokens":"0",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"1985.928",
        "ICE - # output tokens":"0",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"6.764",
        "BLiMP - # output tokens":"0",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"99.377",
        "NaturalQuestions (closed-book) - # output tokens":"5.602",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"62.466",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"4.348",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"355.015",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"396.74",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"62.054",
        "WikiFact - # output tokens":"18.947",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"266.968",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.794",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"317.547",
        "Synthetic reasoning (natural language) - # output tokens":"4.987",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":341.123,
        "bAbI - # output tokens":2.151,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"206.652",
        "Dyck - # output tokens":"3.423",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"789.727",
        "GSM8K - # output tokens":"158.822",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"359.279",
        "MATH - # output tokens":"4.1",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.134",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1091.485",
        "MATH (chain-of-thoughts) - # output tokens":"159.269",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"905.539",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"439.902",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"297.311",
        "Data imputation - # output tokens":"2.549",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"895.507",
        "Entity matching - # output tokens":"2",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0",
        "Copyright (text) - # prompt tokens":"99.733",
        "Copyright (text) - # output tokens":"1021.602",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"197.462",
        "Disinformation (reiteration) - # output tokens":"59.21",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"117.45",
        "Disinformation (wedging) - # output tokens":"150.108",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"314.18",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"9.034",
        "BOLD - # output tokens":"20",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"10.79",
        "RealToxicityPrompts - # output tokens":"99.507",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"18.143",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"J1-Large v1 (7.5B)",
        "Mean win rate":"-",
        "The Pile - # eval":"492",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"1374.206",
        "The Pile - # output tokens":"0",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"12.325",
        "TwitterAAE - # output tokens":"0",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"1985.928",
        "ICE - # output tokens":"0",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"6.764",
        "BLiMP - # output tokens":"0",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"99.377",
        "NaturalQuestions (closed-book) - # output tokens":"7.876",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"62.466",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"4.348",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"355.015",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"396.74",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"62.054",
        "WikiFact - # output tokens":"18.943",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"266.968",
        "Synthetic reasoning (abstract symbols) - # output tokens":"6.13",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"317.547",
        "Synthetic reasoning (natural language) - # output tokens":"5.528",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":341.123,
        "bAbI - # output tokens":2.113,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"206.652",
        "Dyck - # output tokens":"4.572",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"789.727",
        "GSM8K - # output tokens":"214.018",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"359.279",
        "MATH - # output tokens":"4.707",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.134",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1091.485",
        "MATH (chain-of-thoughts) - # output tokens":"154.34",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"905.539",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"439.902",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"297.311",
        "Data imputation - # output tokens":"2.494",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"895.507",
        "Entity matching - # output tokens":"2",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0",
        "Copyright (text) - # prompt tokens":"99.733",
        "Copyright (text) - # output tokens":"1020.722",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"197.462",
        "Disinformation (reiteration) - # output tokens":"57.418",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"117.45",
        "Disinformation (wedging) - # output tokens":"149.775",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"314.18",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"9.034",
        "BOLD - # output tokens":"20",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"10.79",
        "RealToxicityPrompts - # output tokens":"99.537",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"18.143",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"J1-Grande v1 (17B)",
        "Mean win rate":"-",
        "The Pile - # eval":"492",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"1374.206",
        "The Pile - # output tokens":"0",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"12.325",
        "TwitterAAE - # output tokens":"0",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"1985.928",
        "ICE - # output tokens":"0",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"6.764",
        "BLiMP - # output tokens":"0",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"99.377",
        "NaturalQuestions (closed-book) - # output tokens":"5.971",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"62.466",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"4.348",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"355.015",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"396.74",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"62.054",
        "WikiFact - # output tokens":"18.663",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"266.968",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.971",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"317.547",
        "Synthetic reasoning (natural language) - # output tokens":"5.47",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":341.123,
        "bAbI - # output tokens":2.104,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"206.652",
        "Dyck - # output tokens":"4.041",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"789.727",
        "GSM8K - # output tokens":"152.079",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"359.279",
        "MATH - # output tokens":"5.309",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.134",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1091.485",
        "MATH (chain-of-thoughts) - # output tokens":"145.058",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"905.539",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"439.902",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"297.311",
        "Data imputation - # output tokens":"2.503",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"895.507",
        "Entity matching - # output tokens":"2",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0",
        "Copyright (text) - # prompt tokens":"99.733",
        "Copyright (text) - # output tokens":"1021.154",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"197.462",
        "Disinformation (reiteration) - # output tokens":"59.333",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"117.45",
        "Disinformation (wedging) - # output tokens":"144.367",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"314.18",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"9.034",
        "BOLD - # output tokens":"19.991",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"10.79",
        "RealToxicityPrompts - # output tokens":"99.321",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"18.143",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"J1-Grande v2 beta (17B)",
        "Mean win rate":"-",
        "The Pile - # eval":"492",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"1374.206",
        "The Pile - # output tokens":"0",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"12.325",
        "TwitterAAE - # output tokens":"0",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"1985.928",
        "ICE - # output tokens":"0",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"6.764",
        "BLiMP - # output tokens":"0",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"99.377",
        "NaturalQuestions (closed-book) - # output tokens":"5.282",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"62.466",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"4.348",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"355.015",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"396.74",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"62.054",
        "WikiFact - # output tokens":"18.994",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"266.968",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.945",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"317.547",
        "Synthetic reasoning (natural language) - # output tokens":"5.442",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":341.123,
        "bAbI - # output tokens":2.108,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"206.652",
        "Dyck - # output tokens":"4.44",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"789.727",
        "GSM8K - # output tokens":"136.418",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"359.279",
        "MATH - # output tokens":"3.572",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.134",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1091.485",
        "MATH (chain-of-thoughts) - # output tokens":"139.029",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"905.539",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"439.902",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"297.311",
        "Data imputation - # output tokens":"2.483",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"895.507",
        "Entity matching - # output tokens":"2",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0",
        "Copyright (text) - # prompt tokens":"99.733",
        "Copyright (text) - # output tokens":"1019.138",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"197.462",
        "Disinformation (reiteration) - # output tokens":"59.038",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"117.45",
        "Disinformation (wedging) - # output tokens":"145.1",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"314.18",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"9.034",
        "BOLD - # output tokens":"19.984",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"10.79",
        "RealToxicityPrompts - # output tokens":"99.396",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"18.143",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "Mean win rate":"-",
        "The Pile - # eval":"492",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"3256.852",
        "The Pile - # output tokens":"0",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"12.325",
        "TwitterAAE - # output tokens":"0",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"3008.538",
        "ICE - # output tokens":"0",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"6.764",
        "BLiMP - # output tokens":"0",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"99.377",
        "NaturalQuestions (closed-book) - # output tokens":"5.365",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"62.466",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"4.348",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"355.015",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"396.74",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"62.054",
        "WikiFact - # output tokens":"16.293",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"266.968",
        "Synthetic reasoning (abstract symbols) - # output tokens":"6.011",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"317.547",
        "Synthetic reasoning (natural language) - # output tokens":"5.461",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":341.123,
        "bAbI - # output tokens":2.205,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"206.652",
        "Dyck - # output tokens":"3.672",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"789.727",
        "GSM8K - # output tokens":"111.793",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"359.279",
        "MATH - # output tokens":"3.38",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"8",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1327.994",
        "MATH (chain-of-thoughts) - # output tokens":"140.631",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"905.539",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"439.902",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"297.311",
        "Data imputation - # output tokens":"2.378",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"895.507",
        "Entity matching - # output tokens":"2",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0",
        "Copyright (text) - # prompt tokens":"99.733",
        "Copyright (text) - # output tokens":"1020.781",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"197.462",
        "Disinformation (reiteration) - # output tokens":"59.308",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"117.45",
        "Disinformation (wedging) - # output tokens":"151.992",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"314.18",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"9.034",
        "BOLD - # output tokens":"19.957",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"10.79",
        "RealToxicityPrompts - # output tokens":"99.706",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"18.143",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "Mean win rate":"-",
        "The Pile - # eval":"492",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"1374.206",
        "The Pile - # output tokens":"0",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"12.325",
        "TwitterAAE - # output tokens":"0",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"1985.928",
        "ICE - # output tokens":"0",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"6.764",
        "BLiMP - # output tokens":"0",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"99.377",
        "NaturalQuestions (closed-book) - # output tokens":"6.315",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"62.466",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"4.348",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"355.015",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"396.74",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"62.054",
        "WikiFact - # output tokens":"16.119",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"266.968",
        "Synthetic reasoning (abstract symbols) - # output tokens":"6.083",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"317.547",
        "Synthetic reasoning (natural language) - # output tokens":"5.534",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":341.123,
        "bAbI - # output tokens":2.117,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"206.652",
        "Dyck - # output tokens":"4.753",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"789.727",
        "GSM8K - # output tokens":"126.368",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"359.279",
        "MATH - # output tokens":"3.682",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.134",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1091.485",
        "MATH (chain-of-thoughts) - # output tokens":"147.728",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"905.539",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"439.902",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"297.311",
        "Data imputation - # output tokens":"2.387",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"895.507",
        "Entity matching - # output tokens":"2",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0",
        "Copyright (text) - # prompt tokens":"99.733",
        "Copyright (text) - # output tokens":"1021.102",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"197.462",
        "Disinformation (reiteration) - # output tokens":"59.118",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"117.45",
        "Disinformation (wedging) - # output tokens":"141.183",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"314.18",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"9.034",
        "BOLD - # output tokens":"19.929",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"10.79",
        "RealToxicityPrompts - # output tokens":"99.661",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"18.143",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"Jurassic-2 Large (7.5B)",
        "Mean win rate":"-",
        "The Pile - # eval":"492",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"1374.206",
        "The Pile - # output tokens":"0",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"12.325",
        "TwitterAAE - # output tokens":"0",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"1985.928",
        "ICE - # output tokens":"0",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"6.764",
        "BLiMP - # output tokens":"0",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"99.377",
        "NaturalQuestions (closed-book) - # output tokens":"6.729",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"62.466",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"4.348",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"355.015",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"396.74",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"734.667",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"61.068",
        "WikiFact - # output tokens":"16.577",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"266.968",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.432",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"317.547",
        "Synthetic reasoning (natural language) - # output tokens":"4.462",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":341.123,
        "bAbI - # output tokens":2.141,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"206.652",
        "Dyck - # output tokens":"4.587",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"789.727",
        "GSM8K - # output tokens":"208.932",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"359.279",
        "MATH - # output tokens":"4.437",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.134",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1091.485",
        "MATH (chain-of-thoughts) - # output tokens":"169.624",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"905.539",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"439.902",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"297.311",
        "Data imputation - # output tokens":"2.563",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"895.507",
        "Entity matching - # output tokens":"2",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0",
        "Copyright (text) - # prompt tokens":"99.733",
        "Copyright (text) - # output tokens":"1020.989",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"197.462",
        "Disinformation (reiteration) - # output tokens":"55.955",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"117.45",
        "Disinformation (wedging) - # output tokens":"135.708",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"314.18",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"9.034",
        "BOLD - # output tokens":"19.96",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"10.79",
        "RealToxicityPrompts - # output tokens":"99.717",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"18.143",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"Luminous Base (13B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"111.754",
        "NaturalQuestions (closed-book) - # output tokens":"5.287",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"514.073",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"471.075",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"80.518",
        "WikiFact - # output tokens":"16.071",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"328.281",
        "Synthetic reasoning (abstract symbols) - # output tokens":"2.567",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"413.838",
        "Synthetic reasoning (natural language) - # output tokens":"0",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":470.746,
        "bAbI - # output tokens":1.212,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"166.418",
        "Dyck - # output tokens":"2.511",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"877.454",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"417.252",
        "MATH - # output tokens":"3.103",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.048",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1239.049",
        "MATH (chain-of-thoughts) - # output tokens":"163.088",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1180.557",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"629.734",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"306.525",
        "Data imputation - # output tokens":"1.995",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"889.812",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0.001",
        "Copyright (text) - # prompt tokens":"145.421",
        "Copyright (text) - # output tokens":"1022.598",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"261.253",
        "Disinformation (reiteration) - # output tokens":"60.969",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"167.067",
        "Disinformation (wedging) - # output tokens":"199.775",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"424.298",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"1",
        "BOLD - # prompt tokens":"9.391",
        "BOLD - # output tokens":"19.999",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"15.432",
        "RealToxicityPrompts - # output tokens":"100",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"-",
        "Synthetic efficiency - # train":"-",
        "Synthetic efficiency - truncated":"-",
        "Synthetic efficiency - # prompt tokens":"-",
        "Synthetic efficiency - # output tokens":"-",
        "Synthetic efficiency - # trials":"-"
    },
    {
        "Model":"Luminous Extended (30B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"111.754",
        "NaturalQuestions (closed-book) - # output tokens":"6.119",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"514.073",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"471.075",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"80.518",
        "WikiFact - # output tokens":"15.326",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"328.281",
        "Synthetic reasoning (abstract symbols) - # output tokens":"2.604",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"413.838",
        "Synthetic reasoning (natural language) - # output tokens":"0",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":470.746,
        "bAbI - # output tokens":1.207,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"166.418",
        "Dyck - # output tokens":"2.721",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"877.454",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"417.252",
        "MATH - # output tokens":"2.621",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.048",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1239.049",
        "MATH (chain-of-thoughts) - # output tokens":"155.914",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1180.557",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"629.734",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"306.525",
        "Data imputation - # output tokens":"1.962",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"889.812",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0.001",
        "Copyright (text) - # prompt tokens":"145.421",
        "Copyright (text) - # output tokens":"1022.149",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"261.253",
        "Disinformation (reiteration) - # output tokens":"60.563",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"167.067",
        "Disinformation (wedging) - # output tokens":"184.533",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"424.298",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"1",
        "BOLD - # prompt tokens":"9.391",
        "BOLD - # output tokens":"20",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"15.432",
        "RealToxicityPrompts - # output tokens":"100",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"-",
        "Synthetic efficiency - # train":"-",
        "Synthetic efficiency - truncated":"-",
        "Synthetic efficiency - # prompt tokens":"-",
        "Synthetic efficiency - # output tokens":"-",
        "Synthetic efficiency - # trials":"-"
    },
    {
        "Model":"Luminous Supreme (70B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"111.754",
        "NaturalQuestions (closed-book) - # output tokens":"4.508",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"514.073",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"471.075",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"80.518",
        "WikiFact - # output tokens":"15.34",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"328.281",
        "Synthetic reasoning (abstract symbols) - # output tokens":"2.712",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"413.838",
        "Synthetic reasoning (natural language) - # output tokens":"0",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":470.746,
        "bAbI - # output tokens":1.218,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"166.418",
        "Dyck - # output tokens":"3.123",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"877.454",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"417.252",
        "MATH - # output tokens":"2.701",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.048",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1239.049",
        "MATH (chain-of-thoughts) - # output tokens":"135.03",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1180.557",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"629.734",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"306.525",
        "Data imputation - # output tokens":"1.923",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"889.812",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0.001",
        "Copyright (text) - # prompt tokens":"145.421",
        "Copyright (text) - # output tokens":"1021.848",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"261.253",
        "Disinformation (reiteration) - # output tokens":"59.193",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"167.067",
        "Disinformation (wedging) - # output tokens":"196.408",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"424.298",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"1",
        "BOLD - # prompt tokens":"9.391",
        "BOLD - # output tokens":"20",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"15.432",
        "RealToxicityPrompts - # output tokens":"99.984",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"-",
        "Synthetic efficiency - # train":"-",
        "Synthetic efficiency - truncated":"-",
        "Synthetic efficiency - # prompt tokens":"-",
        "Synthetic efficiency - # output tokens":"-",
        "Synthetic efficiency - # trials":"-"
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B)",
        "Mean win rate":"-",
        "The Pile - # eval":"492",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"4398.18",
        "The Pile - # output tokens":"24.407",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"16.422",
        "TwitterAAE - # output tokens":"5.959",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"3699.737",
        "ICE - # output tokens":"8.892",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"8.608",
        "BLiMP - # output tokens":"0.383",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"5.47",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"1.306",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0.132",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"3.75",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"404.621",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"2.5",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"83.576",
        "WikiFact - # output tokens":"16.654",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.589",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"4.989",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":469.022,
        "bAbI - # output tokens":1.113,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"2.479",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"98.636",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"2.229",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"8",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1414.164",
        "MATH (chain-of-thoughts) - # output tokens":"69.795",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"2.575",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0",
        "Copyright (text) - # prompt tokens":"142.932",
        "Copyright (text) - # output tokens":"1000.171",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"266.025",
        "Disinformation (reiteration) - # output tokens":"56.979",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"158.983",
        "Disinformation (wedging) - # output tokens":"448.15",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"424.437",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"11.805",
        "BOLD - # output tokens":"19.855",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"15.491",
        "RealToxicityPrompts - # output tokens":"99.821",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"17.017",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"BLOOM (176B)",
        "Mean win rate":"-",
        "The Pile - # eval":"492",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"1376.41",
        "The Pile - # output tokens":"0",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"15.618",
        "TwitterAAE - # output tokens":"0",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"2038.937",
        "ICE - # output tokens":"0",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"8.801",
        "BLiMP - # output tokens":"0",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"96.12",
        "NaturalQuestions (closed-book) - # output tokens":"48.109",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.875",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.444",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"3.75",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"370.611",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"2.5",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"436.99",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"78.129",
        "WikiFact - # output tokens":"39.917",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"303.916",
        "Synthetic reasoning (abstract symbols) - # output tokens":"11.17",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"378.218",
        "Synthetic reasoning (natural language) - # output tokens":"9.026",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":414.156,
        "bAbI - # output tokens":5.0,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"150.706",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"823.745",
        "GSM8K - # output tokens":"363.933",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"350.807",
        "MATH - # output tokens":"4.078",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.453",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1098.617",
        "MATH (chain-of-thoughts) - # output tokens":"145.273",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1156.451",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"620.172",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"274.468",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"850.495",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"375.375",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0",
        "Copyright (text) - # prompt tokens":"113.73",
        "Copyright (text) - # output tokens":"4480",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"245.423",
        "Disinformation (reiteration) - # output tokens":"131.675",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"151.975",
        "Disinformation (wedging) - # output tokens":"450",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"396.628",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"10.748",
        "BOLD - # output tokens":"20",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"15.351",
        "RealToxicityPrompts - # output tokens":"100",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"18.143",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"T0pp (11B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"113.556",
        "NaturalQuestions (closed-book) - # output tokens":"300",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"3.75",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"391.646",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"2.5",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"492.01",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"90.708",
        "WikiFact - # output tokens":"40",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"342.433",
        "Synthetic reasoning (abstract symbols) - # output tokens":"32.857",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"402.929",
        "Synthetic reasoning (natural language) - # output tokens":"20",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":4.986,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":428.442,
        "bAbI - # output tokens":5.0,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"265.076",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"4.927",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"927.104",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"7.956",
        "MATH - truncated":"0.005",
        "MATH - # prompt tokens":"416.359",
        "MATH - # output tokens":"20",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"5.085",
        "MATH (chain-of-thoughts) - truncated":"0.005",
        "MATH (chain-of-thoughts) - # prompt tokens":"836.999",
        "MATH (chain-of-thoughts) - # output tokens":"400",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"3.609",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"926.957",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"689.176",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"339.538",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"4.138",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"848.446",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"375.375",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0.084",
        "Copyright (text) - # prompt tokens":"126.309",
        "Copyright (text) - # output tokens":"4480",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"284.389",
        "Disinformation (reiteration) - # output tokens":"500",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"1",
        "Disinformation (wedging) - # prompt tokens":"170.433",
        "Disinformation (wedging) - # output tokens":"450",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"432.096",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0.995",
        "BOLD - # prompt tokens":"11.711",
        "BOLD - # output tokens":"20",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0.005",
        "RealToxicityPrompts - # prompt tokens":"17.569",
        "RealToxicityPrompts - # output tokens":"100",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0.42",
        "Synthetic efficiency - # prompt tokens":"563.42",
        "Synthetic efficiency - # output tokens":"18.143",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B)",
        "Mean win rate":"-",
        "The Pile - # eval":"376.667",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"1058.016",
        "The Pile - # output tokens":"0",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"16.471",
        "TwitterAAE - # output tokens":"0",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"2039.314",
        "ICE - # output tokens":"0",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"8.843",
        "BLiMP - # output tokens":"0",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"111.191",
        "NaturalQuestions (closed-book) - # output tokens":"5.844",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"514.648",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"481.26",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"84.338",
        "WikiFact - # output tokens":"16.567",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"325.454",
        "Synthetic reasoning (abstract symbols) - # output tokens":"2.909",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"407.57",
        "Synthetic reasoning (natural language) - # output tokens":"0",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":469.15,
        "bAbI - # output tokens":1.173,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"165.418",
        "Dyck - # output tokens":"3.857",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"883.757",
        "GSM8K - # output tokens":"154.41",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"407.975",
        "MATH - # output tokens":"4.041",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.016",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1241.182",
        "MATH (chain-of-thoughts) - # output tokens":"152.915",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1195.878",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"629.922",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"312.664",
        "Data imputation - # output tokens":"2.101",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"887.65",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0",
        "Copyright (text) - # prompt tokens":"145.628",
        "Copyright (text) - # output tokens":"1024",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"260.984",
        "Disinformation (reiteration) - # output tokens":"59.392",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"164.608",
        "Disinformation (wedging) - # output tokens":"184.458",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"428.412",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"11.895",
        "BOLD - # output tokens":"20",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"15.523",
        "RealToxicityPrompts - # output tokens":"100",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"18.143",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"Cohere large v20220720 (13.1B)",
        "Mean win rate":"-",
        "The Pile - # eval":"376.667",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"1058.016",
        "The Pile - # output tokens":"0",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"16.471",
        "TwitterAAE - # output tokens":"0",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"2039.314",
        "ICE - # output tokens":"0",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"8.843",
        "BLiMP - # output tokens":"0",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"111.191",
        "NaturalQuestions (closed-book) - # output tokens":"5.625",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"514.648",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"481.26",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"84.338",
        "WikiFact - # output tokens":"16.011",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"325.454",
        "Synthetic reasoning (abstract symbols) - # output tokens":"2.805",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"407.57",
        "Synthetic reasoning (natural language) - # output tokens":"0",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":469.15,
        "bAbI - # output tokens":1.385,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"165.418",
        "Dyck - # output tokens":"2.462",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"883.757",
        "GSM8K - # output tokens":"179.471",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"407.975",
        "MATH - # output tokens":"5.165",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.016",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1241.182",
        "MATH (chain-of-thoughts) - # output tokens":"168.911",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1195.878",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"629.922",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"312.664",
        "Data imputation - # output tokens":"2.188",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"887.65",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0",
        "Copyright (text) - # prompt tokens":"145.628",
        "Copyright (text) - # output tokens":"1024",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"260.984",
        "Disinformation (reiteration) - # output tokens":"61.691",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"164.608",
        "Disinformation (wedging) - # output tokens":"168.708",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"428.412",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"11.895",
        "BOLD - # output tokens":"20",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"15.523",
        "RealToxicityPrompts - # output tokens":"100",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"18.143",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"Cohere medium v20220720 (6.1B)",
        "Mean win rate":"-",
        "The Pile - # eval":"376.667",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"1058.016",
        "The Pile - # output tokens":"0",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"16.471",
        "TwitterAAE - # output tokens":"0",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"2039.314",
        "ICE - # output tokens":"0",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"8.843",
        "BLiMP - # output tokens":"0",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"111.191",
        "NaturalQuestions (closed-book) - # output tokens":"5.267",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"514.648",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"481.26",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"84.338",
        "WikiFact - # output tokens":"16.923",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"325.454",
        "Synthetic reasoning (abstract symbols) - # output tokens":"2.681",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"407.57",
        "Synthetic reasoning (natural language) - # output tokens":"0",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":469.15,
        "bAbI - # output tokens":1.254,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"165.418",
        "Dyck - # output tokens":"3.447",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"883.757",
        "GSM8K - # output tokens":"164.999",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"407.975",
        "MATH - # output tokens":"5.564",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.016",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1241.182",
        "MATH (chain-of-thoughts) - # output tokens":"185.04",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1195.878",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"629.922",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"312.664",
        "Data imputation - # output tokens":"2.207",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"887.65",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0",
        "Copyright (text) - # prompt tokens":"145.628",
        "Copyright (text) - # output tokens":"1024",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"260.984",
        "Disinformation (reiteration) - # output tokens":"55.744",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"164.608",
        "Disinformation (wedging) - # output tokens":"235.875",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"428.412",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"11.895",
        "BOLD - # output tokens":"20",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"15.523",
        "RealToxicityPrompts - # output tokens":"100",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"18.143",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"Cohere small v20220720 (410M)",
        "Mean win rate":"-",
        "The Pile - # eval":"376.667",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"1058.016",
        "The Pile - # output tokens":"0",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"16.471",
        "TwitterAAE - # output tokens":"0",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"2039.314",
        "ICE - # output tokens":"0",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"8.843",
        "BLiMP - # output tokens":"0",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"111.191",
        "NaturalQuestions (closed-book) - # output tokens":"5.149",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"514.648",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"481.26",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"84.338",
        "WikiFact - # output tokens":"16.088",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"325.454",
        "Synthetic reasoning (abstract symbols) - # output tokens":"2.542",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"407.57",
        "Synthetic reasoning (natural language) - # output tokens":"0",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":469.15,
        "bAbI - # output tokens":1.384,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"165.418",
        "Dyck - # output tokens":"3.203",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"883.757",
        "GSM8K - # output tokens":"241.523",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"407.975",
        "MATH - # output tokens":"6.897",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.016",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1241.182",
        "MATH (chain-of-thoughts) - # output tokens":"194.691",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1195.878",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"629.922",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"312.664",
        "Data imputation - # output tokens":"2.139",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"887.65",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0",
        "Copyright (text) - # prompt tokens":"145.628",
        "Copyright (text) - # output tokens":"1024",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"260.984",
        "Disinformation (reiteration) - # output tokens":"54.24",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"164.608",
        "Disinformation (wedging) - # output tokens":"220.375",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"428.412",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"11.895",
        "BOLD - # output tokens":"20",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"15.523",
        "RealToxicityPrompts - # output tokens":"100",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"18.143",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B)",
        "Mean win rate":"-",
        "The Pile - # eval":"376.667",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"1058.016",
        "The Pile - # output tokens":"0",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"16.471",
        "TwitterAAE - # output tokens":"0",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"2039.314",
        "ICE - # output tokens":"0",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"8.843",
        "BLiMP - # output tokens":"0",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"111.191",
        "NaturalQuestions (closed-book) - # output tokens":"4.808",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"514.648",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"481.26",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"84.338",
        "WikiFact - # output tokens":"17.275",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"325.454",
        "Synthetic reasoning (abstract symbols) - # output tokens":"2.769",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"407.57",
        "Synthetic reasoning (natural language) - # output tokens":"0",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":469.15,
        "bAbI - # output tokens":1.118,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"165.418",
        "Dyck - # output tokens":"3.887",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"883.757",
        "GSM8K - # output tokens":"130.87",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"407.975",
        "MATH - # output tokens":"5.357",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.016",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1241.182",
        "MATH (chain-of-thoughts) - # output tokens":"130.466",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1195.878",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"629.922",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"312.664",
        "Data imputation - # output tokens":"2.111",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"887.65",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0",
        "Copyright (text) - # prompt tokens":"145.628",
        "Copyright (text) - # output tokens":"1024",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"260.984",
        "Disinformation (reiteration) - # output tokens":"60.803",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"164.608",
        "Disinformation (wedging) - # output tokens":"191.917",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"428.412",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"11.895",
        "BOLD - # output tokens":"20",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"15.523",
        "RealToxicityPrompts - # output tokens":"100",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"18.143",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"Cohere medium v20221108 (6.1B)",
        "Mean win rate":"-",
        "The Pile - # eval":"376.667",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"1058.016",
        "The Pile - # output tokens":"0",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"16.471",
        "TwitterAAE - # output tokens":"0",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"2039.314",
        "ICE - # output tokens":"0",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"8.843",
        "BLiMP - # output tokens":"0",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"111.191",
        "NaturalQuestions (closed-book) - # output tokens":"6.745",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"514.648",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"481.26",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"84.338",
        "WikiFact - # output tokens":"16.617",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"325.454",
        "Synthetic reasoning (abstract symbols) - # output tokens":"2.654",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"407.57",
        "Synthetic reasoning (natural language) - # output tokens":"0",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":469.15,
        "bAbI - # output tokens":1.234,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"165.418",
        "Dyck - # output tokens":"1.959",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"883.757",
        "GSM8K - # output tokens":"114.619",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"407.975",
        "MATH - # output tokens":"5.529",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.016",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1241.182",
        "MATH (chain-of-thoughts) - # output tokens":"188.589",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1195.878",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"629.922",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"312.664",
        "Data imputation - # output tokens":"2.214",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"887.65",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0",
        "Copyright (text) - # prompt tokens":"145.628",
        "Copyright (text) - # output tokens":"1024",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"260.984",
        "Disinformation (reiteration) - # output tokens":"56.649",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"164.608",
        "Disinformation (wedging) - # output tokens":"172.625",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"428.412",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"11.895",
        "BOLD - # output tokens":"20",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"15.523",
        "RealToxicityPrompts - # output tokens":"100",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"18.143",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"Cohere Command beta (6.1B)",
        "Mean win rate":"-",
        "The Pile - # eval":"376.667",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"1049.308",
        "The Pile - # output tokens":"1.001",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"16.471",
        "TwitterAAE - # output tokens":"1.002",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"2011.814",
        "ICE - # output tokens":"0.984",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"8.843",
        "BLiMP - # output tokens":"1",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"111.191",
        "NaturalQuestions (closed-book) - # output tokens":"4.687",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"1",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"1",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"514.648",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"481.26",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"84.338",
        "WikiFact - # output tokens":"15.307",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"325.454",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.508",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"407.57",
        "Synthetic reasoning (natural language) - # output tokens":"5.094",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":469.15,
        "bAbI - # output tokens":1.336,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"165.418",
        "Dyck - # output tokens":"1.901",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"883.757",
        "GSM8K - # output tokens":"126.028",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"407.975",
        "MATH - # output tokens":"5.373",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.956",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1229.853",
        "MATH (chain-of-thoughts) - # output tokens":"158.213",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1195.878",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"629.922",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"312.664",
        "Data imputation - # output tokens":"2.192",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"887.65",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0",
        "Copyright (text) - # prompt tokens":"145.628",
        "Copyright (text) - # output tokens":"1010.5",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"260.984",
        "Disinformation (reiteration) - # output tokens":"64.893",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"164.608",
        "Disinformation (wedging) - # output tokens":"142.667",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"428.412",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"11.895",
        "BOLD - # output tokens":"19.838",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"15.523",
        "RealToxicityPrompts - # output tokens":"99.856",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"17.991",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"Cohere Command beta (52.4B)",
        "Mean win rate":"-",
        "The Pile - # eval":"376.667",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"1049.308",
        "The Pile - # output tokens":"1.001",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"16.471",
        "TwitterAAE - # output tokens":"1.002",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"2011.814",
        "ICE - # output tokens":"0.984",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"8.843",
        "BLiMP - # output tokens":"1",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"111.191",
        "NaturalQuestions (closed-book) - # output tokens":"4.325",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"1",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"1",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"514.648",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"481.26",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"84.338",
        "WikiFact - # output tokens":"16.846",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"325.454",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.527",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"407.57",
        "Synthetic reasoning (natural language) - # output tokens":"5.08",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":469.15,
        "bAbI - # output tokens":1.155,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"165.418",
        "Dyck - # output tokens":"1.591",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"883.757",
        "GSM8K - # output tokens":"111.456",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"407.975",
        "MATH - # output tokens":"4.715",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.956",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1229.853",
        "MATH (chain-of-thoughts) - # output tokens":"133.908",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1195.878",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"629.922",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"312.664",
        "Data imputation - # output tokens":"2.184",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"887.65",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0",
        "Copyright (text) - # prompt tokens":"145.628",
        "Copyright (text) - # output tokens":"977.373",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"260.984",
        "Disinformation (reiteration) - # output tokens":"66.057",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"164.608",
        "Disinformation (wedging) - # output tokens":"182.708",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"428.412",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"11.895",
        "BOLD - # output tokens":"19.853",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"15.523",
        "RealToxicityPrompts - # output tokens":"99.683",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"18.12",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"GPT-J (6B)",
        "Mean win rate":"-",
        "The Pile - # eval":"492",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"1442.192",
        "The Pile - # output tokens":"0",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"16.422",
        "TwitterAAE - # output tokens":"0",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"2039.669",
        "ICE - # output tokens":"0",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"8.608",
        "BLiMP - # output tokens":"0",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"282.837",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"3.75",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"404.621",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"2.5",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"83.576",
        "WikiFact - # output tokens":"40",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"15.544",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"9.611",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":469.022,
        "bAbI - # output tokens":5.0,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"367.283",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"2.93",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.131",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1227.862",
        "MATH (chain-of-thoughts) - # output tokens":"147.663",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"375.375",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0",
        "Copyright (text) - # prompt tokens":"117.797",
        "Copyright (text) - # output tokens":"4480",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"266.025",
        "Disinformation (reiteration) - # output tokens":"173.718",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"158.983",
        "Disinformation (wedging) - # output tokens":"450",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"424.437",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"11.805",
        "BOLD - # output tokens":"20",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"15.491",
        "RealToxicityPrompts - # output tokens":"100",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"18.143",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"GPT-NeoX (20B)",
        "Mean win rate":"-",
        "The Pile - # eval":"492",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"1407.518",
        "The Pile - # output tokens":"0",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"16.554",
        "TwitterAAE - # output tokens":"0",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"2039.888",
        "ICE - # output tokens":"0",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"8.859",
        "BLiMP - # output tokens":"0",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.966",
        "NaturalQuestions (closed-book) - # output tokens":"90.195",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.806",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.346",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"3.75",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"406.102",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"2.5",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"467.936",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"83.942",
        "WikiFact - # output tokens":"40",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"320.738",
        "Synthetic reasoning (abstract symbols) - # output tokens":"16.864",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"403.046",
        "Synthetic reasoning (natural language) - # output tokens":"9.2",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":469.338,
        "bAbI - # output tokens":5.0,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"889.249",
        "GSM8K - # output tokens":"386.245",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"387.171",
        "MATH - # output tokens":"3.274",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.312",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1175.984",
        "MATH (chain-of-thoughts) - # output tokens":"136.152",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1202.164",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"610.996",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"314.872",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"906.402",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"375.375",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0",
        "Copyright (text) - # prompt tokens":"113.993",
        "Copyright (text) - # output tokens":"4480",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"262.71",
        "Disinformation (reiteration) - # output tokens":"169.728",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"163.633",
        "Disinformation (wedging) - # output tokens":"450",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"426.123",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"11.029",
        "BOLD - # output tokens":"20",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"15.546",
        "RealToxicityPrompts - # output tokens":"100",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"18.143",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"Pythia (1B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"-",
        "NaturalQuestions (closed-book) - # train":"-",
        "NaturalQuestions (closed-book) - truncated":"-",
        "NaturalQuestions (closed-book) - # prompt tokens":"-",
        "NaturalQuestions (closed-book) - # output tokens":"-",
        "NaturalQuestions (closed-book) - # trials":"-",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"-",
        "TruthfulQA - # train":"-",
        "TruthfulQA - truncated":"-",
        "TruthfulQA - # prompt tokens":"-",
        "TruthfulQA - # output tokens":"-",
        "TruthfulQA - # trials":"-",
        "MMLU - # eval":"-",
        "MMLU - # train":"-",
        "MMLU - truncated":"-",
        "MMLU - # prompt tokens":"-",
        "MMLU - # output tokens":"-",
        "MMLU - # trials":"-",
        "WikiFact - # eval":"-",
        "WikiFact - # train":"-",
        "WikiFact - truncated":"-",
        "WikiFact - # prompt tokens":"-",
        "WikiFact - # output tokens":"-",
        "WikiFact - # trials":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"-",
        "Synthetic reasoning (abstract symbols) - # train":"-",
        "Synthetic reasoning (abstract symbols) - truncated":"-",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"-",
        "Synthetic reasoning (abstract symbols) - # output tokens":"-",
        "Synthetic reasoning (abstract symbols) - # trials":"-",
        "Synthetic reasoning (natural language) - # eval":"-",
        "Synthetic reasoning (natural language) - # train":"-",
        "Synthetic reasoning (natural language) - truncated":"-",
        "Synthetic reasoning (natural language) - # prompt tokens":"-",
        "Synthetic reasoning (natural language) - # output tokens":"-",
        "Synthetic reasoning (natural language) - # trials":"-",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":434.671,
        "bAbI - # output tokens":5.0,
        "bAbI - # trials":1,
        "Dyck - # eval":"-",
        "Dyck - # train":"-",
        "Dyck - truncated":"-",
        "Dyck - # prompt tokens":"-",
        "Dyck - # output tokens":"-",
        "Dyck - # trials":"-",
        "GSM8K - # eval":"-",
        "GSM8K - # train":"-",
        "GSM8K - truncated":"-",
        "GSM8K - # prompt tokens":"-",
        "GSM8K - # output tokens":"-",
        "GSM8K - # trials":"-",
        "MATH - # eval":"-",
        "MATH - # train":"-",
        "MATH - truncated":"-",
        "MATH - # prompt tokens":"-",
        "MATH - # output tokens":"-",
        "MATH - # trials":"-",
        "MATH (chain-of-thoughts) - # eval":"-",
        "MATH (chain-of-thoughts) - # train":"-",
        "MATH (chain-of-thoughts) - truncated":"-",
        "MATH (chain-of-thoughts) - # prompt tokens":"-",
        "MATH (chain-of-thoughts) - # output tokens":"-",
        "MATH (chain-of-thoughts) - # trials":"-",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"-",
        "LSAT - # train":"-",
        "LSAT - truncated":"-",
        "LSAT - # prompt tokens":"-",
        "LSAT - # output tokens":"-",
        "LSAT - # trials":"-",
        "LegalSupport - # eval":"-",
        "LegalSupport - # train":"-",
        "LegalSupport - truncated":"-",
        "LegalSupport - # prompt tokens":"-",
        "LegalSupport - # output tokens":"-",
        "LegalSupport - # trials":"-",
        "Data imputation - # eval":"-",
        "Data imputation - # train":"-",
        "Data imputation - truncated":"-",
        "Data imputation - # prompt tokens":"-",
        "Data imputation - # output tokens":"-",
        "Data imputation - # trials":"-",
        "Entity matching - # eval":"-",
        "Entity matching - # train":"-",
        "Entity matching - truncated":"-",
        "Entity matching - # prompt tokens":"-",
        "Entity matching - # output tokens":"-",
        "Entity matching - # trials":"-",
        "Copyright (text) - # eval":"-",
        "Copyright (text) - # train":"-",
        "Copyright (text) - truncated":"-",
        "Copyright (text) - # prompt tokens":"-",
        "Copyright (text) - # output tokens":"-",
        "Copyright (text) - # trials":"-",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"-",
        "Disinformation (reiteration) - # train":"-",
        "Disinformation (reiteration) - truncated":"-",
        "Disinformation (reiteration) - # prompt tokens":"-",
        "Disinformation (reiteration) - # output tokens":"-",
        "Disinformation (reiteration) - # trials":"-",
        "Disinformation (wedging) - # eval":"-",
        "Disinformation (wedging) - # train":"-",
        "Disinformation (wedging) - truncated":"-",
        "Disinformation (wedging) - # prompt tokens":"-",
        "Disinformation (wedging) - # output tokens":"-",
        "Disinformation (wedging) - # trials":"-",
        "BBQ - # eval":"-",
        "BBQ - # train":"-",
        "BBQ - truncated":"-",
        "BBQ - # prompt tokens":"-",
        "BBQ - # output tokens":"-",
        "BBQ - # trials":"-",
        "BOLD - # eval":"-",
        "BOLD - # train":"-",
        "BOLD - truncated":"-",
        "BOLD - # prompt tokens":"-",
        "BOLD - # output tokens":"-",
        "BOLD - # trials":"-",
        "RealToxicityPrompts - # eval":"-",
        "RealToxicityPrompts - # train":"-",
        "RealToxicityPrompts - truncated":"-",
        "RealToxicityPrompts - # prompt tokens":"-",
        "RealToxicityPrompts - # output tokens":"-",
        "RealToxicityPrompts - # trials":"-",
        "Synthetic efficiency - # eval":"-",
        "Synthetic efficiency - # train":"-",
        "Synthetic efficiency - truncated":"-",
        "Synthetic efficiency - # prompt tokens":"-",
        "Synthetic efficiency - # output tokens":"-",
        "Synthetic efficiency - # trials":"-"
    },
    {
        "Model":"Pythia (6.9B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"117.299",
        "NaturalQuestions (closed-book) - # output tokens":"300",
        "NaturalQuestions (closed-book) - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"505.352",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"467.936",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"83.942",
        "WikiFact - # output tokens":"4.71",
        "WikiFact - # trials":"1",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"312.071",
        "Synthetic reasoning (abstract symbols) - # output tokens":"32.857",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"388.046",
        "Synthetic reasoning (natural language) - # output tokens":"20",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":434.671,
        "bAbI - # output tokens":5.0,
        "bAbI - # trials":1,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"939.582",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"387.171",
        "MATH - # output tokens":"3.834",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.984",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1112.644",
        "MATH (chain-of-thoughts) - # output tokens":"133.796",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1176.83",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"597.329",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"291.539",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"942.402",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"1",
        "Copyright (text) - # eval":"-",
        "Copyright (text) - # train":"-",
        "Copyright (text) - truncated":"-",
        "Copyright (text) - # prompt tokens":"-",
        "Copyright (text) - # output tokens":"-",
        "Copyright (text) - # trials":"-",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"-",
        "Disinformation (reiteration) - # train":"-",
        "Disinformation (reiteration) - truncated":"-",
        "Disinformation (reiteration) - # prompt tokens":"-",
        "Disinformation (reiteration) - # output tokens":"-",
        "Disinformation (reiteration) - # trials":"-",
        "Disinformation (wedging) - # eval":"-",
        "Disinformation (wedging) - # train":"-",
        "Disinformation (wedging) - truncated":"-",
        "Disinformation (wedging) - # prompt tokens":"-",
        "Disinformation (wedging) - # output tokens":"-",
        "Disinformation (wedging) - # trials":"-",
        "BBQ - # eval":"-",
        "BBQ - # train":"-",
        "BBQ - truncated":"-",
        "BBQ - # prompt tokens":"-",
        "BBQ - # output tokens":"-",
        "BBQ - # trials":"-",
        "BOLD - # eval":"-",
        "BOLD - # train":"-",
        "BOLD - truncated":"-",
        "BOLD - # prompt tokens":"-",
        "BOLD - # output tokens":"-",
        "BOLD - # trials":"-",
        "RealToxicityPrompts - # eval":"-",
        "RealToxicityPrompts - # train":"-",
        "RealToxicityPrompts - truncated":"-",
        "RealToxicityPrompts - # prompt tokens":"-",
        "RealToxicityPrompts - # output tokens":"-",
        "RealToxicityPrompts - # trials":"-",
        "Synthetic efficiency - # eval":"-",
        "Synthetic efficiency - # train":"-",
        "Synthetic efficiency - truncated":"-",
        "Synthetic efficiency - # prompt tokens":"-",
        "Synthetic efficiency - # output tokens":"-",
        "Synthetic efficiency - # trials":"-"
    },
    {
        "Model":"Pythia (12B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"117.299",
        "NaturalQuestions (closed-book) - # output tokens":"300",
        "NaturalQuestions (closed-book) - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"505.352",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"467.936",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"83.942",
        "WikiFact - # output tokens":"4.212",
        "WikiFact - # trials":"1",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"312.071",
        "Synthetic reasoning (abstract symbols) - # output tokens":"32.857",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"388.046",
        "Synthetic reasoning (natural language) - # output tokens":"20",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":434.671,
        "bAbI - # output tokens":5.0,
        "bAbI - # trials":1,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"939.582",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"387.171",
        "MATH - # output tokens":"3.048",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.984",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1112.644",
        "MATH (chain-of-thoughts) - # output tokens":"121.888",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1176.83",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"597.329",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"291.539",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"942.402",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"1",
        "Copyright (text) - # eval":"-",
        "Copyright (text) - # train":"-",
        "Copyright (text) - truncated":"-",
        "Copyright (text) - # prompt tokens":"-",
        "Copyright (text) - # output tokens":"-",
        "Copyright (text) - # trials":"-",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"-",
        "Disinformation (reiteration) - # train":"-",
        "Disinformation (reiteration) - truncated":"-",
        "Disinformation (reiteration) - # prompt tokens":"-",
        "Disinformation (reiteration) - # output tokens":"-",
        "Disinformation (reiteration) - # trials":"-",
        "Disinformation (wedging) - # eval":"-",
        "Disinformation (wedging) - # train":"-",
        "Disinformation (wedging) - truncated":"-",
        "Disinformation (wedging) - # prompt tokens":"-",
        "Disinformation (wedging) - # output tokens":"-",
        "Disinformation (wedging) - # trials":"-",
        "BBQ - # eval":"-",
        "BBQ - # train":"-",
        "BBQ - truncated":"-",
        "BBQ - # prompt tokens":"-",
        "BBQ - # output tokens":"-",
        "BBQ - # trials":"-",
        "BOLD - # eval":"-",
        "BOLD - # train":"-",
        "BOLD - truncated":"-",
        "BOLD - # prompt tokens":"-",
        "BOLD - # output tokens":"-",
        "BOLD - # trials":"-",
        "RealToxicityPrompts - # eval":"-",
        "RealToxicityPrompts - # train":"-",
        "RealToxicityPrompts - truncated":"-",
        "RealToxicityPrompts - # prompt tokens":"-",
        "RealToxicityPrompts - # output tokens":"-",
        "RealToxicityPrompts - # trials":"-",
        "Synthetic efficiency - # eval":"-",
        "Synthetic efficiency - # train":"-",
        "Synthetic efficiency - truncated":"-",
        "Synthetic efficiency - # prompt tokens":"-",
        "Synthetic efficiency - # output tokens":"-",
        "Synthetic efficiency - # trials":"-"
    },
    {
        "Model":"T5 (11B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"113.556",
        "NaturalQuestions (closed-book) - # output tokens":"300",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"3.547",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"371.92",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"2.5",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"4.326",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"420.562",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"90.708",
        "WikiFact - # output tokens":"40",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"341.862",
        "Synthetic reasoning (abstract symbols) - # output tokens":"32.857",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"401.929",
        "Synthetic reasoning (natural language) - # output tokens":"20",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":4.878,
        "bAbI - truncated":0.007,
        "bAbI - # prompt tokens":419.224,
        "bAbI - # output tokens":5.0,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"265.076",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"1.905",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"460.351",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"7.75",
        "MATH - truncated":"0.005",
        "MATH - # prompt tokens":"403.624",
        "MATH - # output tokens":"20",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"2.51",
        "MATH (chain-of-thoughts) - truncated":"0.017",
        "MATH (chain-of-thoughts) - # prompt tokens":"404.903",
        "MATH (chain-of-thoughts) - # output tokens":"400",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"0.938",
        "LSAT - truncated":"0.064",
        "LSAT - # prompt tokens":"417.646",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"1.323",
        "LegalSupport - truncated":"0.002",
        "LegalSupport - # prompt tokens":"395.955",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"339.538",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"1.66",
        "Entity matching - truncated":"0.132",
        "Entity matching - # prompt tokens":"421.841",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"375.375",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0.084",
        "Copyright (text) - # prompt tokens":"126.284",
        "Copyright (text) - # output tokens":"4480",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"284.389",
        "Disinformation (reiteration) - # output tokens":"500",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"1",
        "Disinformation (wedging) - # prompt tokens":"170.433",
        "Disinformation (wedging) - # output tokens":"450",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"4.972",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"429.546",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0.995",
        "BOLD - # prompt tokens":"10.711",
        "BOLD - # output tokens":"20",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0.005",
        "RealToxicityPrompts - # prompt tokens":"17.569",
        "RealToxicityPrompts - # output tokens":"100",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0.52",
        "Synthetic efficiency - # prompt tokens":"357.62",
        "Synthetic efficiency - # output tokens":"18.143",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"UL2 (20B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"117.556",
        "NaturalQuestions (closed-book) - # output tokens":"300",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"3.513",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"372.668",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"2.5",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"4.316",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"423.395",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"94.708",
        "WikiFact - # output tokens":"40",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"345.862",
        "Synthetic reasoning (abstract symbols) - # output tokens":"32.857",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"405.929",
        "Synthetic reasoning (natural language) - # output tokens":"20",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":4.873,
        "bAbI - truncated":0.007,
        "bAbI - # prompt tokens":422.766,
        "bAbI - # output tokens":5.0,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"269.076",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"1.88",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"460.805",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"7.736",
        "MATH - truncated":"0.005",
        "MATH - # prompt tokens":"406.861",
        "MATH - # output tokens":"20",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"2.48",
        "MATH (chain-of-thoughts) - truncated":"0.017",
        "MATH (chain-of-thoughts) - # prompt tokens":"403.768",
        "MATH (chain-of-thoughts) - # output tokens":"400",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"0.93",
        "LSAT - truncated":"0.07",
        "LSAT - # prompt tokens":"420.317",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"1.286",
        "LegalSupport - truncated":"0.002",
        "LegalSupport - # prompt tokens":"392.164",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"343.538",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"1.631",
        "Entity matching - truncated":"0.143",
        "Entity matching - # prompt tokens":"419.916",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"375.375",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0.084",
        "Copyright (text) - # prompt tokens":"130.284",
        "Copyright (text) - # output tokens":"4480",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"288.389",
        "Disinformation (reiteration) - # output tokens":"500",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"1",
        "Disinformation (wedging) - # prompt tokens":"174.433",
        "Disinformation (wedging) - # output tokens":"450",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"4.966",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"433.066",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0.995",
        "BOLD - # prompt tokens":"14.711",
        "BOLD - # output tokens":"20",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0.005",
        "RealToxicityPrompts - # prompt tokens":"21.569",
        "RealToxicityPrompts - # output tokens":"100",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0.6",
        "Synthetic efficiency - # prompt tokens":"359.4",
        "Synthetic efficiency - # output tokens":"18.143",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"OPT (175B)",
        "Mean win rate":"-",
        "The Pile - # eval":"492",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"1442.191",
        "The Pile - # output tokens":"0",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"16.422",
        "TwitterAAE - # output tokens":"0",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"2039.669",
        "ICE - # output tokens":"0",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"8.608",
        "BLiMP - # output tokens":"0",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"278.02",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"3.75",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"404.621",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"2.5",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"83.576",
        "WikiFact - # output tokens":"39.844",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"11.955",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"9.228",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":469.022,
        "bAbI - # output tokens":5.0,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"391.689",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"2.608",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.131",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1227.862",
        "MATH (chain-of-thoughts) - # output tokens":"163.593",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"375.375",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0",
        "Copyright (text) - # prompt tokens":"117.797",
        "Copyright (text) - # output tokens":"4480",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"266.025",
        "Disinformation (reiteration) - # output tokens":"140.629",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"158.983",
        "Disinformation (wedging) - # output tokens":"450",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"424.437",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"11.805",
        "BOLD - # output tokens":"20",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"15.491",
        "RealToxicityPrompts - # output tokens":"100",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"18.143",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"OPT (66B)",
        "Mean win rate":"-",
        "The Pile - # eval":"492",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"1442.191",
        "The Pile - # output tokens":"0",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"16.422",
        "TwitterAAE - # output tokens":"0",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"2039.669",
        "ICE - # output tokens":"0",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"8.608",
        "BLiMP - # output tokens":"0",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"153.231",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0.2",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"1",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"3.75",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"404.621",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"2.5",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"83.576",
        "WikiFact - # output tokens":"39.838",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"12.484",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"9.026",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":469.022,
        "bAbI - # output tokens":5.0,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"2.591",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.131",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1227.862",
        "MATH (chain-of-thoughts) - # output tokens":"157.159",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"375.375",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0",
        "Copyright (text) - # prompt tokens":"117.797",
        "Copyright (text) - # output tokens":"4480",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"266.025",
        "Disinformation (reiteration) - # output tokens":"150.91",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"158.983",
        "Disinformation (wedging) - # output tokens":"450",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"424.437",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"11.805",
        "BOLD - # output tokens":"20",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"15.491",
        "RealToxicityPrompts - # output tokens":"100",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"18.143",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"LLaMA (7B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"137.383",
        "NaturalQuestions (closed-book) - # output tokens":"300",
        "NaturalQuestions (closed-book) - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"524.602",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"522.547",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"97.741",
        "WikiFact - # output tokens":"4.806",
        "WikiFact - # trials":"1",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"336.901",
        "Synthetic reasoning (abstract symbols) - # output tokens":"32.857",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"410.282",
        "Synthetic reasoning (natural language) - # output tokens":"20",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":475.71,
        "bAbI - # output tokens":5.0,
        "bAbI - # trials":1,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"1207.746",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"442.208",
        "MATH - # output tokens":"4.504",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.897",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1214.707",
        "MATH (chain-of-thoughts) - # output tokens":"400",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1261.191",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"707.947",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"370.63",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1109.316",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"1",
        "Copyright (text) - # eval":"-",
        "Copyright (text) - # train":"-",
        "Copyright (text) - truncated":"-",
        "Copyright (text) - # prompt tokens":"-",
        "Copyright (text) - # output tokens":"-",
        "Copyright (text) - # trials":"-",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"-",
        "Disinformation (reiteration) - # train":"-",
        "Disinformation (reiteration) - truncated":"-",
        "Disinformation (reiteration) - # prompt tokens":"-",
        "Disinformation (reiteration) - # output tokens":"-",
        "Disinformation (reiteration) - # trials":"-",
        "Disinformation (wedging) - # eval":"-",
        "Disinformation (wedging) - # train":"-",
        "Disinformation (wedging) - truncated":"-",
        "Disinformation (wedging) - # prompt tokens":"-",
        "Disinformation (wedging) - # output tokens":"-",
        "Disinformation (wedging) - # trials":"-",
        "BBQ - # eval":"-",
        "BBQ - # train":"-",
        "BBQ - truncated":"-",
        "BBQ - # prompt tokens":"-",
        "BBQ - # output tokens":"-",
        "BBQ - # trials":"-",
        "BOLD - # eval":"-",
        "BOLD - # train":"-",
        "BOLD - truncated":"-",
        "BOLD - # prompt tokens":"-",
        "BOLD - # output tokens":"-",
        "BOLD - # trials":"-",
        "RealToxicityPrompts - # eval":"-",
        "RealToxicityPrompts - # train":"-",
        "RealToxicityPrompts - truncated":"-",
        "RealToxicityPrompts - # prompt tokens":"-",
        "RealToxicityPrompts - # output tokens":"-",
        "RealToxicityPrompts - # trials":"-",
        "Synthetic efficiency - # eval":"-",
        "Synthetic efficiency - # train":"-",
        "Synthetic efficiency - truncated":"-",
        "Synthetic efficiency - # prompt tokens":"-",
        "Synthetic efficiency - # output tokens":"-",
        "Synthetic efficiency - # trials":"-"
    },
    {
        "Model":"LLaMA (13B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"137.383",
        "NaturalQuestions (closed-book) - # output tokens":"300",
        "NaturalQuestions (closed-book) - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"524.602",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"522.547",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"97.741",
        "WikiFact - # output tokens":"4.762",
        "WikiFact - # trials":"1",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"336.901",
        "Synthetic reasoning (abstract symbols) - # output tokens":"32.857",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"410.282",
        "Synthetic reasoning (natural language) - # output tokens":"20",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":475.71,
        "bAbI - # output tokens":5.0,
        "bAbI - # trials":1,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"1207.746",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"442.208",
        "MATH - # output tokens":"4.385",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.897",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1214.707",
        "MATH (chain-of-thoughts) - # output tokens":"400",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1261.191",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"707.947",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"370.63",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1109.316",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"1",
        "Copyright (text) - # eval":"-",
        "Copyright (text) - # train":"-",
        "Copyright (text) - truncated":"-",
        "Copyright (text) - # prompt tokens":"-",
        "Copyright (text) - # output tokens":"-",
        "Copyright (text) - # trials":"-",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"-",
        "Disinformation (reiteration) - # train":"-",
        "Disinformation (reiteration) - truncated":"-",
        "Disinformation (reiteration) - # prompt tokens":"-",
        "Disinformation (reiteration) - # output tokens":"-",
        "Disinformation (reiteration) - # trials":"-",
        "Disinformation (wedging) - # eval":"-",
        "Disinformation (wedging) - # train":"-",
        "Disinformation (wedging) - truncated":"-",
        "Disinformation (wedging) - # prompt tokens":"-",
        "Disinformation (wedging) - # output tokens":"-",
        "Disinformation (wedging) - # trials":"-",
        "BBQ - # eval":"-",
        "BBQ - # train":"-",
        "BBQ - truncated":"-",
        "BBQ - # prompt tokens":"-",
        "BBQ - # output tokens":"-",
        "BBQ - # trials":"-",
        "BOLD - # eval":"-",
        "BOLD - # train":"-",
        "BOLD - truncated":"-",
        "BOLD - # prompt tokens":"-",
        "BOLD - # output tokens":"-",
        "BOLD - # trials":"-",
        "RealToxicityPrompts - # eval":"-",
        "RealToxicityPrompts - # train":"-",
        "RealToxicityPrompts - truncated":"-",
        "RealToxicityPrompts - # prompt tokens":"-",
        "RealToxicityPrompts - # output tokens":"-",
        "RealToxicityPrompts - # trials":"-",
        "Synthetic efficiency - # eval":"-",
        "Synthetic efficiency - # train":"-",
        "Synthetic efficiency - truncated":"-",
        "Synthetic efficiency - # prompt tokens":"-",
        "Synthetic efficiency - # output tokens":"-",
        "Synthetic efficiency - # trials":"-"
    },
    {
        "Model":"LLaMA (30B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"137.383",
        "NaturalQuestions (closed-book) - # output tokens":"300",
        "NaturalQuestions (closed-book) - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"524.602",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"522.547",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"97.741",
        "WikiFact - # output tokens":"4.721",
        "WikiFact - # trials":"1",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"336.901",
        "Synthetic reasoning (abstract symbols) - # output tokens":"32.857",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"410.282",
        "Synthetic reasoning (natural language) - # output tokens":"20",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":475.71,
        "bAbI - # output tokens":5.0,
        "bAbI - # trials":1,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"1207.746",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"442.208",
        "MATH - # output tokens":"3.557",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.897",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1214.707",
        "MATH (chain-of-thoughts) - # output tokens":"400",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1261.191",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"707.947",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"370.63",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1109.316",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"1",
        "Copyright (text) - # eval":"-",
        "Copyright (text) - # train":"-",
        "Copyright (text) - truncated":"-",
        "Copyright (text) - # prompt tokens":"-",
        "Copyright (text) - # output tokens":"-",
        "Copyright (text) - # trials":"-",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"-",
        "Disinformation (reiteration) - # train":"-",
        "Disinformation (reiteration) - truncated":"-",
        "Disinformation (reiteration) - # prompt tokens":"-",
        "Disinformation (reiteration) - # output tokens":"-",
        "Disinformation (reiteration) - # trials":"-",
        "Disinformation (wedging) - # eval":"-",
        "Disinformation (wedging) - # train":"-",
        "Disinformation (wedging) - truncated":"-",
        "Disinformation (wedging) - # prompt tokens":"-",
        "Disinformation (wedging) - # output tokens":"-",
        "Disinformation (wedging) - # trials":"-",
        "BBQ - # eval":"-",
        "BBQ - # train":"-",
        "BBQ - truncated":"-",
        "BBQ - # prompt tokens":"-",
        "BBQ - # output tokens":"-",
        "BBQ - # trials":"-",
        "BOLD - # eval":"-",
        "BOLD - # train":"-",
        "BOLD - truncated":"-",
        "BOLD - # prompt tokens":"-",
        "BOLD - # output tokens":"-",
        "BOLD - # trials":"-",
        "RealToxicityPrompts - # eval":"-",
        "RealToxicityPrompts - # train":"-",
        "RealToxicityPrompts - truncated":"-",
        "RealToxicityPrompts - # prompt tokens":"-",
        "RealToxicityPrompts - # output tokens":"-",
        "RealToxicityPrompts - # trials":"-",
        "Synthetic efficiency - # eval":"-",
        "Synthetic efficiency - # train":"-",
        "Synthetic efficiency - truncated":"-",
        "Synthetic efficiency - # prompt tokens":"-",
        "Synthetic efficiency - # output tokens":"-",
        "Synthetic efficiency - # trials":"-"
    },
    {
        "Model":"LLaMA (65B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"137.383",
        "NaturalQuestions (closed-book) - # output tokens":"1",
        "NaturalQuestions (closed-book) - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"524.602",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"522.547",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"97.741",
        "WikiFact - # output tokens":"1",
        "WikiFact - # trials":"1",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"336.901",
        "Synthetic reasoning (abstract symbols) - # output tokens":"1",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"410.282",
        "Synthetic reasoning (natural language) - # output tokens":"1",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":475.71,
        "bAbI - # output tokens":1.0,
        "bAbI - # trials":1,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"0.91",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"1207.746",
        "GSM8K - # output tokens":"1",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"442.208",
        "MATH - # output tokens":"1",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.897",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1214.707",
        "MATH (chain-of-thoughts) - # output tokens":"1",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1261.191",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"707.947",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"370.63",
        "Data imputation - # output tokens":"1",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1109.316",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"1",
        "Copyright (text) - # eval":"-",
        "Copyright (text) - # train":"-",
        "Copyright (text) - truncated":"-",
        "Copyright (text) - # prompt tokens":"-",
        "Copyright (text) - # output tokens":"-",
        "Copyright (text) - # trials":"-",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"-",
        "Disinformation (reiteration) - # train":"-",
        "Disinformation (reiteration) - truncated":"-",
        "Disinformation (reiteration) - # prompt tokens":"-",
        "Disinformation (reiteration) - # output tokens":"-",
        "Disinformation (reiteration) - # trials":"-",
        "Disinformation (wedging) - # eval":"-",
        "Disinformation (wedging) - # train":"-",
        "Disinformation (wedging) - truncated":"-",
        "Disinformation (wedging) - # prompt tokens":"-",
        "Disinformation (wedging) - # output tokens":"-",
        "Disinformation (wedging) - # trials":"-",
        "BBQ - # eval":"-",
        "BBQ - # train":"-",
        "BBQ - truncated":"-",
        "BBQ - # prompt tokens":"-",
        "BBQ - # output tokens":"-",
        "BBQ - # trials":"-",
        "BOLD - # eval":"-",
        "BOLD - # train":"-",
        "BOLD - truncated":"-",
        "BOLD - # prompt tokens":"-",
        "BOLD - # output tokens":"-",
        "BOLD - # trials":"-",
        "RealToxicityPrompts - # eval":"-",
        "RealToxicityPrompts - # train":"-",
        "RealToxicityPrompts - truncated":"-",
        "RealToxicityPrompts - # prompt tokens":"-",
        "RealToxicityPrompts - # output tokens":"-",
        "RealToxicityPrompts - # trials":"-",
        "Synthetic efficiency - # eval":"-",
        "Synthetic efficiency - # train":"-",
        "Synthetic efficiency - truncated":"-",
        "Synthetic efficiency - # prompt tokens":"-",
        "Synthetic efficiency - # output tokens":"-",
        "Synthetic efficiency - # trials":"-"
    },
    {
        "Model":"Llama 2 (7B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"137.383",
        "NaturalQuestions (closed-book) - # output tokens":"0.998",
        "NaturalQuestions (closed-book) - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"524.602",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"522.547",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"97.741",
        "WikiFact - # output tokens":"1",
        "WikiFact - # trials":"1",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"336.901",
        "Synthetic reasoning (abstract symbols) - # output tokens":"1",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"410.282",
        "Synthetic reasoning (natural language) - # output tokens":"1",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":475.71,
        "bAbI - # output tokens":5.0,
        "bAbI - # trials":1,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"0.994",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"1207.746",
        "GSM8K - # output tokens":"1",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"442.208",
        "MATH - # output tokens":"1",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"8",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1438.636",
        "MATH (chain-of-thoughts) - # output tokens":"1",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1261.191",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"707.947",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"370.63",
        "Data imputation - # output tokens":"1",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1109.316",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"1",
        "Copyright (text) - # eval":"-",
        "Copyright (text) - # train":"-",
        "Copyright (text) - truncated":"-",
        "Copyright (text) - # prompt tokens":"-",
        "Copyright (text) - # output tokens":"-",
        "Copyright (text) - # trials":"-",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"-",
        "Disinformation (reiteration) - # train":"-",
        "Disinformation (reiteration) - truncated":"-",
        "Disinformation (reiteration) - # prompt tokens":"-",
        "Disinformation (reiteration) - # output tokens":"-",
        "Disinformation (reiteration) - # trials":"-",
        "Disinformation (wedging) - # eval":"-",
        "Disinformation (wedging) - # train":"-",
        "Disinformation (wedging) - truncated":"-",
        "Disinformation (wedging) - # prompt tokens":"-",
        "Disinformation (wedging) - # output tokens":"-",
        "Disinformation (wedging) - # trials":"-",
        "BBQ - # eval":"-",
        "BBQ - # train":"-",
        "BBQ - truncated":"-",
        "BBQ - # prompt tokens":"-",
        "BBQ - # output tokens":"-",
        "BBQ - # trials":"-",
        "BOLD - # eval":"-",
        "BOLD - # train":"-",
        "BOLD - truncated":"-",
        "BOLD - # prompt tokens":"-",
        "BOLD - # output tokens":"-",
        "BOLD - # trials":"-",
        "RealToxicityPrompts - # eval":"-",
        "RealToxicityPrompts - # train":"-",
        "RealToxicityPrompts - truncated":"-",
        "RealToxicityPrompts - # prompt tokens":"-",
        "RealToxicityPrompts - # output tokens":"-",
        "RealToxicityPrompts - # trials":"-",
        "Synthetic efficiency - # eval":"-",
        "Synthetic efficiency - # train":"-",
        "Synthetic efficiency - truncated":"-",
        "Synthetic efficiency - # prompt tokens":"-",
        "Synthetic efficiency - # output tokens":"-",
        "Synthetic efficiency - # trials":"-"
    },
    {
        "Model":"Llama 2 (13B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"137.383",
        "NaturalQuestions (closed-book) - # output tokens":"1",
        "NaturalQuestions (closed-book) - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"524.602",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"522.547",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"97.741",
        "WikiFact - # output tokens":"1",
        "WikiFact - # trials":"1",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"336.901",
        "Synthetic reasoning (abstract symbols) - # output tokens":"1",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"410.282",
        "Synthetic reasoning (natural language) - # output tokens":"1",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":475.71,
        "bAbI - # output tokens":5.0,
        "bAbI - # trials":1,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"1",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"1207.746",
        "GSM8K - # output tokens":"1",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"442.208",
        "MATH - # output tokens":"1",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"8",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1438.636",
        "MATH (chain-of-thoughts) - # output tokens":"1",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1261.191",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"707.947",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"370.63",
        "Data imputation - # output tokens":"1",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1109.316",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"1",
        "Copyright (text) - # eval":"-",
        "Copyright (text) - # train":"-",
        "Copyright (text) - truncated":"-",
        "Copyright (text) - # prompt tokens":"-",
        "Copyright (text) - # output tokens":"-",
        "Copyright (text) - # trials":"-",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"-",
        "Disinformation (reiteration) - # train":"-",
        "Disinformation (reiteration) - truncated":"-",
        "Disinformation (reiteration) - # prompt tokens":"-",
        "Disinformation (reiteration) - # output tokens":"-",
        "Disinformation (reiteration) - # trials":"-",
        "Disinformation (wedging) - # eval":"-",
        "Disinformation (wedging) - # train":"-",
        "Disinformation (wedging) - truncated":"-",
        "Disinformation (wedging) - # prompt tokens":"-",
        "Disinformation (wedging) - # output tokens":"-",
        "Disinformation (wedging) - # trials":"-",
        "BBQ - # eval":"-",
        "BBQ - # train":"-",
        "BBQ - truncated":"-",
        "BBQ - # prompt tokens":"-",
        "BBQ - # output tokens":"-",
        "BBQ - # trials":"-",
        "BOLD - # eval":"-",
        "BOLD - # train":"-",
        "BOLD - truncated":"-",
        "BOLD - # prompt tokens":"-",
        "BOLD - # output tokens":"-",
        "BOLD - # trials":"-",
        "RealToxicityPrompts - # eval":"-",
        "RealToxicityPrompts - # train":"-",
        "RealToxicityPrompts - truncated":"-",
        "RealToxicityPrompts - # prompt tokens":"-",
        "RealToxicityPrompts - # output tokens":"-",
        "RealToxicityPrompts - # trials":"-",
        "Synthetic efficiency - # eval":"-",
        "Synthetic efficiency - # train":"-",
        "Synthetic efficiency - truncated":"-",
        "Synthetic efficiency - # prompt tokens":"-",
        "Synthetic efficiency - # output tokens":"-",
        "Synthetic efficiency - # trials":"-"
    },
    {
        "Model":"Llama 2 (70B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"137.383",
        "NaturalQuestions (closed-book) - # output tokens":"1",
        "NaturalQuestions (closed-book) - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"524.602",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"522.547",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"97.741",
        "WikiFact - # output tokens":"1",
        "WikiFact - # trials":"1",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"336.901",
        "Synthetic reasoning (abstract symbols) - # output tokens":"1",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"410.282",
        "Synthetic reasoning (natural language) - # output tokens":"1",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":475.71,
        "bAbI - # output tokens":1.0,
        "bAbI - # trials":1,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"0.964",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"1207.746",
        "GSM8K - # output tokens":"1",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"442.208",
        "MATH - # output tokens":"1",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"8",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1438.636",
        "MATH (chain-of-thoughts) - # output tokens":"1",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1261.191",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"707.947",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"370.63",
        "Data imputation - # output tokens":"1",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1109.316",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"1",
        "Copyright (text) - # eval":"-",
        "Copyright (text) - # train":"-",
        "Copyright (text) - truncated":"-",
        "Copyright (text) - # prompt tokens":"-",
        "Copyright (text) - # output tokens":"-",
        "Copyright (text) - # trials":"-",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"-",
        "Disinformation (reiteration) - # train":"-",
        "Disinformation (reiteration) - truncated":"-",
        "Disinformation (reiteration) - # prompt tokens":"-",
        "Disinformation (reiteration) - # output tokens":"-",
        "Disinformation (reiteration) - # trials":"-",
        "Disinformation (wedging) - # eval":"-",
        "Disinformation (wedging) - # train":"-",
        "Disinformation (wedging) - truncated":"-",
        "Disinformation (wedging) - # prompt tokens":"-",
        "Disinformation (wedging) - # output tokens":"-",
        "Disinformation (wedging) - # trials":"-",
        "BBQ - # eval":"-",
        "BBQ - # train":"-",
        "BBQ - truncated":"-",
        "BBQ - # prompt tokens":"-",
        "BBQ - # output tokens":"-",
        "BBQ - # trials":"-",
        "BOLD - # eval":"-",
        "BOLD - # train":"-",
        "BOLD - truncated":"-",
        "BOLD - # prompt tokens":"-",
        "BOLD - # output tokens":"-",
        "BOLD - # trials":"-",
        "RealToxicityPrompts - # eval":"-",
        "RealToxicityPrompts - # train":"-",
        "RealToxicityPrompts - truncated":"-",
        "RealToxicityPrompts - # prompt tokens":"-",
        "RealToxicityPrompts - # output tokens":"-",
        "RealToxicityPrompts - # trials":"-",
        "Synthetic efficiency - # eval":"-",
        "Synthetic efficiency - # train":"-",
        "Synthetic efficiency - truncated":"-",
        "Synthetic efficiency - # prompt tokens":"-",
        "Synthetic efficiency - # output tokens":"-",
        "Synthetic efficiency - # trials":"-"
    },
    {
        "Model":"Alpaca (7B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"137.383",
        "NaturalQuestions (closed-book) - # output tokens":"84.53",
        "NaturalQuestions (closed-book) - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"524.602",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"522.547",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"97.741",
        "WikiFact - # output tokens":"4.937",
        "WikiFact - # trials":"1",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"336.901",
        "Synthetic reasoning (abstract symbols) - # output tokens":"13.598",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"410.282",
        "Synthetic reasoning (natural language) - # output tokens":"11.872",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":475.71,
        "bAbI - # output tokens":3.264,
        "bAbI - # trials":1,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"3.34",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"1207.746",
        "GSM8K - # output tokens":"97.861",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"442.208",
        "MATH - # output tokens":"4.532",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.897",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1214.707",
        "MATH (chain-of-thoughts) - # output tokens":"85.93",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1261.191",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"707.947",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"370.63",
        "Data imputation - # output tokens":"4.898",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1109.316",
        "Entity matching - # output tokens":"2.301",
        "Entity matching - # trials":"1",
        "Copyright (text) - # eval":"-",
        "Copyright (text) - # train":"-",
        "Copyright (text) - truncated":"-",
        "Copyright (text) - # prompt tokens":"-",
        "Copyright (text) - # output tokens":"-",
        "Copyright (text) - # trials":"-",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"-",
        "Disinformation (reiteration) - # train":"-",
        "Disinformation (reiteration) - truncated":"-",
        "Disinformation (reiteration) - # prompt tokens":"-",
        "Disinformation (reiteration) - # output tokens":"-",
        "Disinformation (reiteration) - # trials":"-",
        "Disinformation (wedging) - # eval":"-",
        "Disinformation (wedging) - # train":"-",
        "Disinformation (wedging) - truncated":"-",
        "Disinformation (wedging) - # prompt tokens":"-",
        "Disinformation (wedging) - # output tokens":"-",
        "Disinformation (wedging) - # trials":"-",
        "BBQ - # eval":"-",
        "BBQ - # train":"-",
        "BBQ - truncated":"-",
        "BBQ - # prompt tokens":"-",
        "BBQ - # output tokens":"-",
        "BBQ - # trials":"-",
        "BOLD - # eval":"-",
        "BOLD - # train":"-",
        "BOLD - truncated":"-",
        "BOLD - # prompt tokens":"-",
        "BOLD - # output tokens":"-",
        "BOLD - # trials":"-",
        "RealToxicityPrompts - # eval":"-",
        "RealToxicityPrompts - # train":"-",
        "RealToxicityPrompts - truncated":"-",
        "RealToxicityPrompts - # prompt tokens":"-",
        "RealToxicityPrompts - # output tokens":"-",
        "RealToxicityPrompts - # trials":"-",
        "Synthetic efficiency - # eval":"-",
        "Synthetic efficiency - # train":"-",
        "Synthetic efficiency - truncated":"-",
        "Synthetic efficiency - # prompt tokens":"-",
        "Synthetic efficiency - # output tokens":"-",
        "Synthetic efficiency - # trials":"-"
    },
    {
        "Model":"Vicuna v1.3 (7B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"137.383",
        "NaturalQuestions (closed-book) - # output tokens":"296.95",
        "NaturalQuestions (closed-book) - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"524.602",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"522.547",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"97.741",
        "WikiFact - # output tokens":"4.902",
        "WikiFact - # trials":"1",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"336.901",
        "Synthetic reasoning (abstract symbols) - # output tokens":"32.705",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"410.282",
        "Synthetic reasoning (natural language) - # output tokens":"19.977",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":475.71,
        "bAbI - # output tokens":4.997,
        "bAbI - # trials":1,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"4.194",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"1207.746",
        "GSM8K - # output tokens":"375.967",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"442.208",
        "MATH - # output tokens":"4.396",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.897",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1214.707",
        "MATH (chain-of-thoughts) - # output tokens":"359.987",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1261.191",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"707.947",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"370.63",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1109.316",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"1",
        "Copyright (text) - # eval":"-",
        "Copyright (text) - # train":"-",
        "Copyright (text) - truncated":"-",
        "Copyright (text) - # prompt tokens":"-",
        "Copyright (text) - # output tokens":"-",
        "Copyright (text) - # trials":"-",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"-",
        "Disinformation (reiteration) - # train":"-",
        "Disinformation (reiteration) - truncated":"-",
        "Disinformation (reiteration) - # prompt tokens":"-",
        "Disinformation (reiteration) - # output tokens":"-",
        "Disinformation (reiteration) - # trials":"-",
        "Disinformation (wedging) - # eval":"-",
        "Disinformation (wedging) - # train":"-",
        "Disinformation (wedging) - truncated":"-",
        "Disinformation (wedging) - # prompt tokens":"-",
        "Disinformation (wedging) - # output tokens":"-",
        "Disinformation (wedging) - # trials":"-",
        "BBQ - # eval":"-",
        "BBQ - # train":"-",
        "BBQ - truncated":"-",
        "BBQ - # prompt tokens":"-",
        "BBQ - # output tokens":"-",
        "BBQ - # trials":"-",
        "BOLD - # eval":"-",
        "BOLD - # train":"-",
        "BOLD - truncated":"-",
        "BOLD - # prompt tokens":"-",
        "BOLD - # output tokens":"-",
        "BOLD - # trials":"-",
        "RealToxicityPrompts - # eval":"-",
        "RealToxicityPrompts - # train":"-",
        "RealToxicityPrompts - truncated":"-",
        "RealToxicityPrompts - # prompt tokens":"-",
        "RealToxicityPrompts - # output tokens":"-",
        "RealToxicityPrompts - # trials":"-",
        "Synthetic efficiency - # eval":"-",
        "Synthetic efficiency - # train":"-",
        "Synthetic efficiency - truncated":"-",
        "Synthetic efficiency - # prompt tokens":"-",
        "Synthetic efficiency - # output tokens":"-",
        "Synthetic efficiency - # trials":"-"
    },
    {
        "Model":"Vicuna v1.3 (13B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"137.383",
        "NaturalQuestions (closed-book) - # output tokens":"299.508",
        "NaturalQuestions (closed-book) - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"524.602",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"522.547",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"97.741",
        "WikiFact - # output tokens":"4.765",
        "WikiFact - # trials":"1",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"336.901",
        "Synthetic reasoning (abstract symbols) - # output tokens":"32.649",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"410.282",
        "Synthetic reasoning (natural language) - # output tokens":"19.662",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":475.71,
        "bAbI - # output tokens":4.897,
        "bAbI - # trials":1,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"4.998",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"1207.746",
        "GSM8K - # output tokens":"395.86",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"442.208",
        "MATH - # output tokens":"5.024",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.897",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1214.707",
        "MATH (chain-of-thoughts) - # output tokens":"340.617",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1261.191",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"707.947",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"370.63",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1109.316",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"1",
        "Copyright (text) - # eval":"-",
        "Copyright (text) - # train":"-",
        "Copyright (text) - truncated":"-",
        "Copyright (text) - # prompt tokens":"-",
        "Copyright (text) - # output tokens":"-",
        "Copyright (text) - # trials":"-",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"-",
        "Disinformation (reiteration) - # train":"-",
        "Disinformation (reiteration) - truncated":"-",
        "Disinformation (reiteration) - # prompt tokens":"-",
        "Disinformation (reiteration) - # output tokens":"-",
        "Disinformation (reiteration) - # trials":"-",
        "Disinformation (wedging) - # eval":"-",
        "Disinformation (wedging) - # train":"-",
        "Disinformation (wedging) - truncated":"-",
        "Disinformation (wedging) - # prompt tokens":"-",
        "Disinformation (wedging) - # output tokens":"-",
        "Disinformation (wedging) - # trials":"-",
        "BBQ - # eval":"-",
        "BBQ - # train":"-",
        "BBQ - truncated":"-",
        "BBQ - # prompt tokens":"-",
        "BBQ - # output tokens":"-",
        "BBQ - # trials":"-",
        "BOLD - # eval":"-",
        "BOLD - # train":"-",
        "BOLD - truncated":"-",
        "BOLD - # prompt tokens":"-",
        "BOLD - # output tokens":"-",
        "BOLD - # trials":"-",
        "RealToxicityPrompts - # eval":"-",
        "RealToxicityPrompts - # train":"-",
        "RealToxicityPrompts - truncated":"-",
        "RealToxicityPrompts - # prompt tokens":"-",
        "RealToxicityPrompts - # output tokens":"-",
        "RealToxicityPrompts - # trials":"-",
        "Synthetic efficiency - # eval":"-",
        "Synthetic efficiency - # train":"-",
        "Synthetic efficiency - truncated":"-",
        "Synthetic efficiency - # prompt tokens":"-",
        "Synthetic efficiency - # output tokens":"-",
        "Synthetic efficiency - # trials":"-"
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"0",
        "NaturalQuestions (closed-book) - # output tokens":"1",
        "NaturalQuestions (closed-book) - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"0",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"0",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"0",
        "WikiFact - # output tokens":"1",
        "WikiFact - # trials":"1",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"0",
        "Synthetic reasoning (abstract symbols) - # output tokens":"1",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"0",
        "Synthetic reasoning (natural language) - # output tokens":"1",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":0.0,
        "bAbI - # output tokens":1.0,
        "bAbI - # trials":1,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"166.418",
        "Dyck - # output tokens":"0.964",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"0",
        "GSM8K - # output tokens":"1",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"0",
        "MATH - # output tokens":"1",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"8",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"0",
        "MATH (chain-of-thoughts) - # output tokens":"1",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"0",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"0",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"370.568",
        "Data imputation - # output tokens":"1",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1086.232",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"1",
        "Copyright (text) - # eval":"-",
        "Copyright (text) - # train":"-",
        "Copyright (text) - truncated":"-",
        "Copyright (text) - # prompt tokens":"-",
        "Copyright (text) - # output tokens":"-",
        "Copyright (text) - # trials":"-",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"-",
        "Disinformation (reiteration) - # train":"-",
        "Disinformation (reiteration) - truncated":"-",
        "Disinformation (reiteration) - # prompt tokens":"-",
        "Disinformation (reiteration) - # output tokens":"-",
        "Disinformation (reiteration) - # trials":"-",
        "Disinformation (wedging) - # eval":"-",
        "Disinformation (wedging) - # train":"-",
        "Disinformation (wedging) - truncated":"-",
        "Disinformation (wedging) - # prompt tokens":"-",
        "Disinformation (wedging) - # output tokens":"-",
        "Disinformation (wedging) - # trials":"-",
        "BBQ - # eval":"-",
        "BBQ - # train":"-",
        "BBQ - truncated":"-",
        "BBQ - # prompt tokens":"-",
        "BBQ - # output tokens":"-",
        "BBQ - # trials":"-",
        "BOLD - # eval":"-",
        "BOLD - # train":"-",
        "BOLD - truncated":"-",
        "BOLD - # prompt tokens":"-",
        "BOLD - # output tokens":"-",
        "BOLD - # trials":"-",
        "RealToxicityPrompts - # eval":"-",
        "RealToxicityPrompts - # train":"-",
        "RealToxicityPrompts - truncated":"-",
        "RealToxicityPrompts - # prompt tokens":"-",
        "RealToxicityPrompts - # output tokens":"-",
        "RealToxicityPrompts - # trials":"-",
        "Synthetic efficiency - # eval":"-",
        "Synthetic efficiency - # train":"-",
        "Synthetic efficiency - truncated":"-",
        "Synthetic efficiency - # prompt tokens":"-",
        "Synthetic efficiency - # output tokens":"-",
        "Synthetic efficiency - # trials":"-"
    },
    {
        "Model":"TNLG v2 (530B)",
        "Mean win rate":"-",
        "The Pile - # eval":"492",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"1441.58",
        "The Pile - # output tokens":"0.007",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"16.422",
        "TwitterAAE - # output tokens":"0",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"2038.685",
        "ICE - # output tokens":"0.016",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"8.608",
        "BLiMP - # output tokens":"0",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"4.569",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"511.121",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"83.576",
        "WikiFact - # output tokens":"17.836",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.194",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"4.536",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":469.022,
        "bAbI - # output tokens":1.108,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"2.252",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"125.933",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"2.769",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.13",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1227.7",
        "MATH (chain-of-thoughts) - # output tokens":"129.397",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"2.262",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0.001",
        "Copyright (text) - # prompt tokens":"142.617",
        "Copyright (text) - # output tokens":"1020.339",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"266.025",
        "Disinformation (reiteration) - # output tokens":"69.271",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"158.983",
        "Disinformation (wedging) - # output tokens":"333.475",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"424.437",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"11.805",
        "BOLD - # output tokens":"19.98",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"15.491",
        "RealToxicityPrompts - # output tokens":"98.812",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"18.143",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"TNLG v2 (6.7B)",
        "Mean win rate":"-",
        "The Pile - # eval":"492",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"1441.58",
        "The Pile - # output tokens":"0.007",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"16.422",
        "TwitterAAE - # output tokens":"0",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"2038.685",
        "ICE - # output tokens":"0.016",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"8.608",
        "BLiMP - # output tokens":"0",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"5.6",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"511.121",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"83.576",
        "WikiFact - # output tokens":"18.298",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.711",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"5.046",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":469.022,
        "bAbI - # output tokens":1.262,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"3.352",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"170.803",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"3.551",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.13",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1227.7",
        "MATH (chain-of-thoughts) - # output tokens":"164.789",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"2.208",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0.001",
        "Copyright (text) - # prompt tokens":"142.617",
        "Copyright (text) - # output tokens":"1022.431",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"266.025",
        "Disinformation (reiteration) - # output tokens":"70.856",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"158.983",
        "Disinformation (wedging) - # output tokens":"355.925",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"424.437",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"11.805",
        "BOLD - # output tokens":"19.96",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"15.491",
        "RealToxicityPrompts - # output tokens":"98.512",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"18.143",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"davinci (175B)",
        "Mean win rate":"-",
        "The Pile - # eval":"492",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"1442.192",
        "The Pile - # output tokens":"0",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"16.422",
        "TwitterAAE - # output tokens":"0",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"2039.669",
        "ICE - # output tokens":"0",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"8.608",
        "BLiMP - # output tokens":"0",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"5.361",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"511.121",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"83.576",
        "WikiFact - # output tokens":"18.065",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.387",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"5.018",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":469.022,
        "bAbI - # output tokens":1.134,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"1.946",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"128.738",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"2.633",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.131",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1227.862",
        "MATH (chain-of-thoughts) - # output tokens":"176.063",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"2.21",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0.001",
        "Copyright (text) - # prompt tokens":"142.618",
        "Copyright (text) - # output tokens":"1013.342",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"266.025",
        "Disinformation (reiteration) - # output tokens":"70.004",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"158.983",
        "Disinformation (wedging) - # output tokens":"200.65",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"424.437",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"11.805",
        "BOLD - # output tokens":"18.951",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"15.491",
        "RealToxicityPrompts - # output tokens":"99.197",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"18.143",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"curie (6.7B)",
        "Mean win rate":"-",
        "The Pile - # eval":"492",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"1442.192",
        "The Pile - # output tokens":"0",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"16.422",
        "TwitterAAE - # output tokens":"0",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"2039.669",
        "ICE - # output tokens":"0",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"8.608",
        "BLiMP - # output tokens":"0",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"6.313",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"511.121",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"83.576",
        "WikiFact - # output tokens":"18.112",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.46",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"4.839",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":469.022,
        "bAbI - # output tokens":1.233,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"1.479",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"176.395",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"3.099",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.131",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1227.862",
        "MATH (chain-of-thoughts) - # output tokens":"179.413",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"2.254",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0.001",
        "Copyright (text) - # prompt tokens":"142.618",
        "Copyright (text) - # output tokens":"1022.549",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"266.025",
        "Disinformation (reiteration) - # output tokens":"69.947",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"158.983",
        "Disinformation (wedging) - # output tokens":"228.083",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"424.437",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"11.805",
        "BOLD - # output tokens":"17.964",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"15.491",
        "RealToxicityPrompts - # output tokens":"99.267",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"18.143",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"babbage (1.3B)",
        "Mean win rate":"-",
        "The Pile - # eval":"492",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"1442.192",
        "The Pile - # output tokens":"0",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"16.422",
        "TwitterAAE - # output tokens":"0",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"2039.669",
        "ICE - # output tokens":"0",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"8.608",
        "BLiMP - # output tokens":"0",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"7.258",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"511.121",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"83.576",
        "WikiFact - # output tokens":"18.315",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"6.288",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"5.328",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":469.022,
        "bAbI - # output tokens":1.349,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"2.977",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"251.824",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"2.245",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.131",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1227.862",
        "MATH (chain-of-thoughts) - # output tokens":"200.846",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"2.473",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0.001",
        "Copyright (text) - # prompt tokens":"142.618",
        "Copyright (text) - # output tokens":"1021.98",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"266.025",
        "Disinformation (reiteration) - # output tokens":"70.754",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"158.983",
        "Disinformation (wedging) - # output tokens":"216.367",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"424.437",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"11.805",
        "BOLD - # output tokens":"18.484",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"15.491",
        "RealToxicityPrompts - # output tokens":"99.475",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"18.143",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"ada (350M)",
        "Mean win rate":"-",
        "The Pile - # eval":"492",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"1442.192",
        "The Pile - # output tokens":"0",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"16.422",
        "TwitterAAE - # output tokens":"0",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"2039.669",
        "ICE - # output tokens":"0",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"8.608",
        "BLiMP - # output tokens":"0",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"5.656",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"511.121",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"83.576",
        "WikiFact - # output tokens":"18.3",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"6.232",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"5.725",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":469.022,
        "bAbI - # output tokens":1.48,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"2.335",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"236.808",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"1.402",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.131",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1227.862",
        "MATH (chain-of-thoughts) - # output tokens":"209.617",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"2.2",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0.001",
        "Copyright (text) - # prompt tokens":"142.618",
        "Copyright (text) - # output tokens":"1023.493",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"266.025",
        "Disinformation (reiteration) - # output tokens":"77.629",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"158.983",
        "Disinformation (wedging) - # output tokens":"355.983",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"424.437",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"11.805",
        "BOLD - # output tokens":"19.132",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"15.491",
        "RealToxicityPrompts - # output tokens":"99.361",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"18.143",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"text-davinci-003",
        "Mean win rate":"-",
        "The Pile - # eval":"492",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"2476.022",
        "The Pile - # output tokens":"0",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"16.422",
        "TwitterAAE - # output tokens":"0",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"3374.319",
        "ICE - # output tokens":"0",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"8.608",
        "BLiMP - # output tokens":"0",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"7.964",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"511.121",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"83.576",
        "WikiFact - # output tokens":"19.49",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.607",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"5.01",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":469.022,
        "bAbI - # output tokens":1.521,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"2.297",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"93.018",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"2.741",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"8",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1414.164",
        "MATH (chain-of-thoughts) - # output tokens":"81.236",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"2.218",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0",
        "Copyright (text) - # prompt tokens":"142.932",
        "Copyright (text) - # output tokens":"175.788",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"266.025",
        "Disinformation (reiteration) - # output tokens":"68.156",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"158.983",
        "Disinformation (wedging) - # output tokens":"199.242",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"424.437",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"11.805",
        "BOLD - # output tokens":"19.802",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"15.491",
        "RealToxicityPrompts - # output tokens":"98.365",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"16.997",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"text-davinci-002",
        "Mean win rate":"-",
        "The Pile - # eval":"492",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"2476.022",
        "The Pile - # output tokens":"0",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"16.422",
        "TwitterAAE - # output tokens":"0",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"3374.319",
        "ICE - # output tokens":"0",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"8.608",
        "BLiMP - # output tokens":"0",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"3.954",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"511.121",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"83.576",
        "WikiFact - # output tokens":"16.338",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.662",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"5.191",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":469.022,
        "bAbI - # output tokens":1.311,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"1.637",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"85.812",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"2.555",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"8",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1414.164",
        "MATH (chain-of-thoughts) - # output tokens":"165.026",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"2.222",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0",
        "Copyright (text) - # prompt tokens":"142.932",
        "Copyright (text) - # output tokens":"833.249",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"266.025",
        "Disinformation (reiteration) - # output tokens":"66.609",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"158.983",
        "Disinformation (wedging) - # output tokens":"210.317",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"424.437",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"11.805",
        "BOLD - # output tokens":"18.764",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"15.491",
        "RealToxicityPrompts - # output tokens":"98.78",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"17.609",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"text-curie-001",
        "Mean win rate":"-",
        "The Pile - # eval":"492",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"1442.192",
        "The Pile - # output tokens":"0",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"16.422",
        "TwitterAAE - # output tokens":"0",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"2039.669",
        "ICE - # output tokens":"0",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"8.608",
        "BLiMP - # output tokens":"0",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"4.641",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"511.121",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"83.576",
        "WikiFact - # output tokens":"18.325",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.503",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"5.115",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":469.022,
        "bAbI - # output tokens":1.286,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"1.436",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"41.515",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"2.988",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.131",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1227.862",
        "MATH (chain-of-thoughts) - # output tokens":"60.835",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"2.237",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0.001",
        "Copyright (text) - # prompt tokens":"142.618",
        "Copyright (text) - # output tokens":"324.169",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"266.025",
        "Disinformation (reiteration) - # output tokens":"62.269",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"158.983",
        "Disinformation (wedging) - # output tokens":"204.992",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"424.437",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"11.805",
        "BOLD - # output tokens":"19.266",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"15.491",
        "RealToxicityPrompts - # output tokens":"96.88",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"17.114",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"text-babbage-001",
        "Mean win rate":"-",
        "The Pile - # eval":"492",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"1442.192",
        "The Pile - # output tokens":"0",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"16.422",
        "TwitterAAE - # output tokens":"0",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"2039.669",
        "ICE - # output tokens":"0",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"8.608",
        "BLiMP - # output tokens":"0",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"2.016",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"511.121",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"83.576",
        "WikiFact - # output tokens":"19.219",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.422",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"5.238",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":469.022,
        "bAbI - # output tokens":1.605,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"1.472",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"34.591",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"2.215",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.131",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1227.862",
        "MATH (chain-of-thoughts) - # output tokens":"52.3",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"2.033",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0.001",
        "Copyright (text) - # prompt tokens":"142.618",
        "Copyright (text) - # output tokens":"319.49",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"266.025",
        "Disinformation (reiteration) - # output tokens":"64.747",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"158.983",
        "Disinformation (wedging) - # output tokens":"181.283",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"424.437",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"11.805",
        "BOLD - # output tokens":"18.99",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"15.491",
        "RealToxicityPrompts - # output tokens":"97.411",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"16.929",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"text-ada-001",
        "Mean win rate":"-",
        "The Pile - # eval":"492",
        "The Pile - # train":"0",
        "The Pile - truncated":"0",
        "The Pile - # prompt tokens":"1442.192",
        "The Pile - # output tokens":"0",
        "The Pile - # trials":"1",
        "TwitterAAE - # eval":"1000",
        "TwitterAAE - # train":"0",
        "TwitterAAE - truncated":"0",
        "TwitterAAE - # prompt tokens":"16.422",
        "TwitterAAE - # output tokens":"0",
        "TwitterAAE - # trials":"1",
        "ICE - # eval":"489.833",
        "ICE - # train":"0",
        "ICE - truncated":"0",
        "ICE - # prompt tokens":"2039.669",
        "ICE - # output tokens":"0",
        "ICE - # trials":"1",
        "BLiMP - # eval":"1000",
        "BLiMP - # train":"0",
        "BLiMP - truncated":"0",
        "BLiMP - # prompt tokens":"8.608",
        "BLiMP - # output tokens":"0",
        "BLiMP - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"1.04",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"511.121",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"83.576",
        "WikiFact - # output tokens":"17.242",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.594",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"5.771",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":469.022,
        "bAbI - # output tokens":1.435,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"1.761",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"57.682",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"1.674",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.131",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1227.862",
        "MATH (chain-of-thoughts) - # output tokens":"88.337",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"1.96",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0.001",
        "Copyright (text) - # prompt tokens":"142.618",
        "Copyright (text) - # output tokens":"223.02",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"266.025",
        "Disinformation (reiteration) - # output tokens":"69.341",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"158.983",
        "Disinformation (wedging) - # output tokens":"219.667",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"424.437",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"11.805",
        "BOLD - # output tokens":"19.454",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"15.491",
        "RealToxicityPrompts - # output tokens":"97.979",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"17.331",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"code-davinci-002",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"-",
        "NaturalQuestions (closed-book) - # train":"-",
        "NaturalQuestions (closed-book) - truncated":"-",
        "NaturalQuestions (closed-book) - # prompt tokens":"-",
        "NaturalQuestions (closed-book) - # output tokens":"-",
        "NaturalQuestions (closed-book) - # trials":"-",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"-",
        "TruthfulQA - # train":"-",
        "TruthfulQA - truncated":"-",
        "TruthfulQA - # prompt tokens":"-",
        "TruthfulQA - # output tokens":"-",
        "TruthfulQA - # trials":"-",
        "MMLU - # eval":"-",
        "MMLU - # train":"-",
        "MMLU - truncated":"-",
        "MMLU - # prompt tokens":"-",
        "MMLU - # output tokens":"-",
        "MMLU - # trials":"-",
        "WikiFact - # eval":"-",
        "WikiFact - # train":"-",
        "WikiFact - truncated":"-",
        "WikiFact - # prompt tokens":"-",
        "WikiFact - # output tokens":"-",
        "WikiFact - # trials":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"3.221",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"0",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":469.022,
        "bAbI - # output tokens":2.074,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"3.267",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"113.059",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"3.58",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"8",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1414.164",
        "MATH (chain-of-thoughts) - # output tokens":"100.694",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"164",
        "HumanEval (Code) - # train":"0",
        "HumanEval (Code) - truncated":"0",
        "HumanEval (Code) - # prompt tokens":"170.348",
        "HumanEval (Code) - # output tokens":"74.811",
        "HumanEval (Code) - # trials":"1",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"-",
        "Data imputation - # train":"-",
        "Data imputation - truncated":"-",
        "Data imputation - # prompt tokens":"-",
        "Data imputation - # output tokens":"-",
        "Data imputation - # trials":"-",
        "Entity matching - # eval":"-",
        "Entity matching - # train":"-",
        "Entity matching - truncated":"-",
        "Entity matching - # prompt tokens":"-",
        "Entity matching - # output tokens":"-",
        "Entity matching - # trials":"-",
        "Copyright (text) - # eval":"-",
        "Copyright (text) - # train":"-",
        "Copyright (text) - truncated":"-",
        "Copyright (text) - # prompt tokens":"-",
        "Copyright (text) - # output tokens":"-",
        "Copyright (text) - # trials":"-",
        "Copyright (code) - # eval":"1000",
        "Copyright (code) - # train":"0",
        "Copyright (code) - truncated":"0",
        "Copyright (code) - # prompt tokens":"80.194",
        "Copyright (code) - # output tokens":"1009.753",
        "Copyright (code) - # trials":"1",
        "Disinformation (reiteration) - # eval":"-",
        "Disinformation (reiteration) - # train":"-",
        "Disinformation (reiteration) - truncated":"-",
        "Disinformation (reiteration) - # prompt tokens":"-",
        "Disinformation (reiteration) - # output tokens":"-",
        "Disinformation (reiteration) - # trials":"-",
        "Disinformation (wedging) - # eval":"-",
        "Disinformation (wedging) - # train":"-",
        "Disinformation (wedging) - truncated":"-",
        "Disinformation (wedging) - # prompt tokens":"-",
        "Disinformation (wedging) - # output tokens":"-",
        "Disinformation (wedging) - # trials":"-",
        "BBQ - # eval":"-",
        "BBQ - # train":"-",
        "BBQ - truncated":"-",
        "BBQ - # prompt tokens":"-",
        "BBQ - # output tokens":"-",
        "BBQ - # trials":"-",
        "BOLD - # eval":"-",
        "BOLD - # train":"-",
        "BOLD - truncated":"-",
        "BOLD - # prompt tokens":"-",
        "BOLD - # output tokens":"-",
        "BOLD - # trials":"-",
        "RealToxicityPrompts - # eval":"-",
        "RealToxicityPrompts - # train":"-",
        "RealToxicityPrompts - truncated":"-",
        "RealToxicityPrompts - # prompt tokens":"-",
        "RealToxicityPrompts - # output tokens":"-",
        "RealToxicityPrompts - # trials":"-",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"18.143",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"code-cushman-001 (12B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"-",
        "NaturalQuestions (closed-book) - # train":"-",
        "NaturalQuestions (closed-book) - truncated":"-",
        "NaturalQuestions (closed-book) - # prompt tokens":"-",
        "NaturalQuestions (closed-book) - # output tokens":"-",
        "NaturalQuestions (closed-book) - # trials":"-",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"-",
        "TruthfulQA - # train":"-",
        "TruthfulQA - truncated":"-",
        "TruthfulQA - # prompt tokens":"-",
        "TruthfulQA - # output tokens":"-",
        "TruthfulQA - # trials":"-",
        "MMLU - # eval":"-",
        "MMLU - # train":"-",
        "MMLU - truncated":"-",
        "MMLU - # prompt tokens":"-",
        "MMLU - # output tokens":"-",
        "MMLU - # trials":"-",
        "WikiFact - # eval":"-",
        "WikiFact - # train":"-",
        "WikiFact - truncated":"-",
        "WikiFact - # prompt tokens":"-",
        "WikiFact - # output tokens":"-",
        "WikiFact - # trials":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"3.135",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"0",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":469.022,
        "bAbI - # output tokens":2.131,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"2.593",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"136.778",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"4.521",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.131",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1227.862",
        "MATH (chain-of-thoughts) - # output tokens":"133.572",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"164",
        "HumanEval (Code) - # train":"0",
        "HumanEval (Code) - truncated":"0",
        "HumanEval (Code) - # prompt tokens":"170.348",
        "HumanEval (Code) - # output tokens":"88.774",
        "HumanEval (Code) - # trials":"1",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"-",
        "Data imputation - # train":"-",
        "Data imputation - truncated":"-",
        "Data imputation - # prompt tokens":"-",
        "Data imputation - # output tokens":"-",
        "Data imputation - # trials":"-",
        "Entity matching - # eval":"-",
        "Entity matching - # train":"-",
        "Entity matching - truncated":"-",
        "Entity matching - # prompt tokens":"-",
        "Entity matching - # output tokens":"-",
        "Entity matching - # trials":"-",
        "Copyright (text) - # eval":"-",
        "Copyright (text) - # train":"-",
        "Copyright (text) - truncated":"-",
        "Copyright (text) - # prompt tokens":"-",
        "Copyright (text) - # output tokens":"-",
        "Copyright (text) - # trials":"-",
        "Copyright (code) - # eval":"1000",
        "Copyright (code) - # train":"0",
        "Copyright (code) - truncated":"0",
        "Copyright (code) - # prompt tokens":"80.194",
        "Copyright (code) - # output tokens":"1014.998",
        "Copyright (code) - # trials":"1",
        "Disinformation (reiteration) - # eval":"-",
        "Disinformation (reiteration) - # train":"-",
        "Disinformation (reiteration) - truncated":"-",
        "Disinformation (reiteration) - # prompt tokens":"-",
        "Disinformation (reiteration) - # output tokens":"-",
        "Disinformation (reiteration) - # trials":"-",
        "Disinformation (wedging) - # eval":"-",
        "Disinformation (wedging) - # train":"-",
        "Disinformation (wedging) - truncated":"-",
        "Disinformation (wedging) - # prompt tokens":"-",
        "Disinformation (wedging) - # output tokens":"-",
        "Disinformation (wedging) - # trials":"-",
        "BBQ - # eval":"-",
        "BBQ - # train":"-",
        "BBQ - truncated":"-",
        "BBQ - # prompt tokens":"-",
        "BBQ - # output tokens":"-",
        "BBQ - # trials":"-",
        "BOLD - # eval":"-",
        "BOLD - # train":"-",
        "BOLD - truncated":"-",
        "BOLD - # prompt tokens":"-",
        "BOLD - # output tokens":"-",
        "BOLD - # trials":"-",
        "RealToxicityPrompts - # eval":"-",
        "RealToxicityPrompts - # train":"-",
        "RealToxicityPrompts - truncated":"-",
        "RealToxicityPrompts - # prompt tokens":"-",
        "RealToxicityPrompts - # output tokens":"-",
        "RealToxicityPrompts - # trials":"-",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0",
        "Synthetic efficiency - # prompt tokens":"665.8",
        "Synthetic efficiency - # output tokens":"17.963",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"gpt-3.5-turbo-0301",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.127",
        "NaturalQuestions (closed-book) - # output tokens":"16.241",
        "NaturalQuestions (closed-book) - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"464.434",
        "TruthfulQA - # output tokens":"1.047",
        "TruthfulQA - # trials":"1",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"460.72",
        "MMLU - # output tokens":"1.012",
        "MMLU - # trials":"1",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"79.057",
        "WikiFact - # output tokens":"27.193",
        "WikiFact - # trials":"1",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"279.734",
        "Synthetic reasoning (abstract symbols) - # output tokens":"6.206",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"343.552",
        "Synthetic reasoning (natural language) - # output tokens":"5.691",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":382.233,
        "bAbI - # output tokens":2.801,
        "bAbI - # trials":1,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"159.418",
        "Dyck - # output tokens":"5.666",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"959.035",
        "GSM8K - # output tokens":"113.005",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"372.768",
        "MATH - # output tokens":"3.048",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"8",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1262.911",
        "MATH (chain-of-thoughts) - # output tokens":"84.136",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1134.604",
        "LSAT - # output tokens":"1.004",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"579.986",
        "LegalSupport - # output tokens":"1.014",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"277.848",
        "Data imputation - # output tokens":"2.193",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"908.418",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"1",
        "Copyright (text) - # eval":"-",
        "Copyright (text) - # train":"-",
        "Copyright (text) - truncated":"-",
        "Copyright (text) - # prompt tokens":"-",
        "Copyright (text) - # output tokens":"-",
        "Copyright (text) - # trials":"-",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"-",
        "Disinformation (reiteration) - # train":"-",
        "Disinformation (reiteration) - truncated":"-",
        "Disinformation (reiteration) - # prompt tokens":"-",
        "Disinformation (reiteration) - # output tokens":"-",
        "Disinformation (reiteration) - # trials":"-",
        "Disinformation (wedging) - # eval":"-",
        "Disinformation (wedging) - # train":"-",
        "Disinformation (wedging) - truncated":"-",
        "Disinformation (wedging) - # prompt tokens":"-",
        "Disinformation (wedging) - # output tokens":"-",
        "Disinformation (wedging) - # trials":"-",
        "BBQ - # eval":"-",
        "BBQ - # train":"-",
        "BBQ - truncated":"-",
        "BBQ - # prompt tokens":"-",
        "BBQ - # output tokens":"-",
        "BBQ - # trials":"-",
        "BOLD - # eval":"-",
        "BOLD - # train":"-",
        "BOLD - truncated":"-",
        "BOLD - # prompt tokens":"-",
        "BOLD - # output tokens":"-",
        "BOLD - # trials":"-",
        "RealToxicityPrompts - # eval":"-",
        "RealToxicityPrompts - # train":"-",
        "RealToxicityPrompts - truncated":"-",
        "RealToxicityPrompts - # prompt tokens":"-",
        "RealToxicityPrompts - # output tokens":"-",
        "RealToxicityPrompts - # trials":"-",
        "Synthetic efficiency - # eval":"-",
        "Synthetic efficiency - # train":"-",
        "Synthetic efficiency - truncated":"-",
        "Synthetic efficiency - # prompt tokens":"-",
        "Synthetic efficiency - # output tokens":"-",
        "Synthetic efficiency - # trials":"-"
    },
    {
        "Model":"gpt-3.5-turbo-0613",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.127",
        "NaturalQuestions (closed-book) - # output tokens":"18.876",
        "NaturalQuestions (closed-book) - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"464.434",
        "TruthfulQA - # output tokens":"1.517",
        "TruthfulQA - # trials":"1",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"460.72",
        "MMLU - # output tokens":"1.371",
        "MMLU - # trials":"1",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"79.057",
        "WikiFact - # output tokens":"23.879",
        "WikiFact - # trials":"1",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"279.734",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.736",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"343.552",
        "Synthetic reasoning (natural language) - # output tokens":"5.056",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":382.233,
        "bAbI - # output tokens":2.757,
        "bAbI - # trials":1,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"159.418",
        "Dyck - # output tokens":"6",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"959.035",
        "GSM8K - # output tokens":"114.72",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"372.768",
        "MATH - # output tokens":"3.166",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"8",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1262.911",
        "MATH (chain-of-thoughts) - # output tokens":"85.76",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1134.604",
        "LSAT - # output tokens":"1.443",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"579.986",
        "LegalSupport - # output tokens":"1.254",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"277.848",
        "Data imputation - # output tokens":"2.042",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"908.418",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"1",
        "Copyright (text) - # eval":"-",
        "Copyright (text) - # train":"-",
        "Copyright (text) - truncated":"-",
        "Copyright (text) - # prompt tokens":"-",
        "Copyright (text) - # output tokens":"-",
        "Copyright (text) - # trials":"-",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"-",
        "Disinformation (reiteration) - # train":"-",
        "Disinformation (reiteration) - truncated":"-",
        "Disinformation (reiteration) - # prompt tokens":"-",
        "Disinformation (reiteration) - # output tokens":"-",
        "Disinformation (reiteration) - # trials":"-",
        "Disinformation (wedging) - # eval":"-",
        "Disinformation (wedging) - # train":"-",
        "Disinformation (wedging) - truncated":"-",
        "Disinformation (wedging) - # prompt tokens":"-",
        "Disinformation (wedging) - # output tokens":"-",
        "Disinformation (wedging) - # trials":"-",
        "BBQ - # eval":"-",
        "BBQ - # train":"-",
        "BBQ - truncated":"-",
        "BBQ - # prompt tokens":"-",
        "BBQ - # output tokens":"-",
        "BBQ - # trials":"-",
        "BOLD - # eval":"-",
        "BOLD - # train":"-",
        "BOLD - truncated":"-",
        "BOLD - # prompt tokens":"-",
        "BOLD - # output tokens":"-",
        "BOLD - # trials":"-",
        "RealToxicityPrompts - # eval":"-",
        "RealToxicityPrompts - # train":"-",
        "RealToxicityPrompts - truncated":"-",
        "RealToxicityPrompts - # prompt tokens":"-",
        "RealToxicityPrompts - # output tokens":"-",
        "RealToxicityPrompts - # trials":"-",
        "Synthetic efficiency - # eval":"-",
        "Synthetic efficiency - # train":"-",
        "Synthetic efficiency - truncated":"-",
        "Synthetic efficiency - # prompt tokens":"-",
        "Synthetic efficiency - # output tokens":"-",
        "Synthetic efficiency - # trials":"-"
    },
    {
        "Model":"RedPajama-INCITE-Base-v1 (3B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"117.299",
        "NaturalQuestions (closed-book) - # output tokens":"300",
        "NaturalQuestions (closed-book) - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"505.352",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"467.936",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"83.942",
        "WikiFact - # output tokens":"4.362",
        "WikiFact - # trials":"1",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"312.071",
        "Synthetic reasoning (abstract symbols) - # output tokens":"32.857",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"388.046",
        "Synthetic reasoning (natural language) - # output tokens":"20",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":434.671,
        "bAbI - # output tokens":5.0,
        "bAbI - # trials":1,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"939.582",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"387.171",
        "MATH - # output tokens":"2.771",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.984",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1112.644",
        "MATH (chain-of-thoughts) - # output tokens":"111.423",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1176.83",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"597.329",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"291.539",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"942.402",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"1",
        "Copyright (text) - # eval":"-",
        "Copyright (text) - # train":"-",
        "Copyright (text) - truncated":"-",
        "Copyright (text) - # prompt tokens":"-",
        "Copyright (text) - # output tokens":"-",
        "Copyright (text) - # trials":"-",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"-",
        "Disinformation (reiteration) - # train":"-",
        "Disinformation (reiteration) - truncated":"-",
        "Disinformation (reiteration) - # prompt tokens":"-",
        "Disinformation (reiteration) - # output tokens":"-",
        "Disinformation (reiteration) - # trials":"-",
        "Disinformation (wedging) - # eval":"-",
        "Disinformation (wedging) - # train":"-",
        "Disinformation (wedging) - truncated":"-",
        "Disinformation (wedging) - # prompt tokens":"-",
        "Disinformation (wedging) - # output tokens":"-",
        "Disinformation (wedging) - # trials":"-",
        "BBQ - # eval":"-",
        "BBQ - # train":"-",
        "BBQ - truncated":"-",
        "BBQ - # prompt tokens":"-",
        "BBQ - # output tokens":"-",
        "BBQ - # trials":"-",
        "BOLD - # eval":"-",
        "BOLD - # train":"-",
        "BOLD - truncated":"-",
        "BOLD - # prompt tokens":"-",
        "BOLD - # output tokens":"-",
        "BOLD - # trials":"-",
        "RealToxicityPrompts - # eval":"-",
        "RealToxicityPrompts - # train":"-",
        "RealToxicityPrompts - truncated":"-",
        "RealToxicityPrompts - # prompt tokens":"-",
        "RealToxicityPrompts - # output tokens":"-",
        "RealToxicityPrompts - # trials":"-",
        "Synthetic efficiency - # eval":"-",
        "Synthetic efficiency - # train":"-",
        "Synthetic efficiency - truncated":"-",
        "Synthetic efficiency - # prompt tokens":"-",
        "Synthetic efficiency - # output tokens":"-",
        "Synthetic efficiency - # trials":"-"
    },
    {
        "Model":"RedPajama-INCITE-Instruct-v1 (3B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"117.299",
        "NaturalQuestions (closed-book) - # output tokens":"300",
        "NaturalQuestions (closed-book) - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"505.352",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"467.936",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"83.942",
        "WikiFact - # output tokens":"4.264",
        "WikiFact - # trials":"1",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"312.071",
        "Synthetic reasoning (abstract symbols) - # output tokens":"32.857",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"388.046",
        "Synthetic reasoning (natural language) - # output tokens":"20",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":434.671,
        "bAbI - # output tokens":5.0,
        "bAbI - # trials":1,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"939.582",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"387.171",
        "MATH - # output tokens":"2.7",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.984",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1112.644",
        "MATH (chain-of-thoughts) - # output tokens":"101.479",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1176.83",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"597.329",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"291.539",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"942.402",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"1",
        "Copyright (text) - # eval":"-",
        "Copyright (text) - # train":"-",
        "Copyright (text) - truncated":"-",
        "Copyright (text) - # prompt tokens":"-",
        "Copyright (text) - # output tokens":"-",
        "Copyright (text) - # trials":"-",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"-",
        "Disinformation (reiteration) - # train":"-",
        "Disinformation (reiteration) - truncated":"-",
        "Disinformation (reiteration) - # prompt tokens":"-",
        "Disinformation (reiteration) - # output tokens":"-",
        "Disinformation (reiteration) - # trials":"-",
        "Disinformation (wedging) - # eval":"-",
        "Disinformation (wedging) - # train":"-",
        "Disinformation (wedging) - truncated":"-",
        "Disinformation (wedging) - # prompt tokens":"-",
        "Disinformation (wedging) - # output tokens":"-",
        "Disinformation (wedging) - # trials":"-",
        "BBQ - # eval":"-",
        "BBQ - # train":"-",
        "BBQ - truncated":"-",
        "BBQ - # prompt tokens":"-",
        "BBQ - # output tokens":"-",
        "BBQ - # trials":"-",
        "BOLD - # eval":"-",
        "BOLD - # train":"-",
        "BOLD - truncated":"-",
        "BOLD - # prompt tokens":"-",
        "BOLD - # output tokens":"-",
        "BOLD - # trials":"-",
        "RealToxicityPrompts - # eval":"-",
        "RealToxicityPrompts - # train":"-",
        "RealToxicityPrompts - truncated":"-",
        "RealToxicityPrompts - # prompt tokens":"-",
        "RealToxicityPrompts - # output tokens":"-",
        "RealToxicityPrompts - # trials":"-",
        "Synthetic efficiency - # eval":"-",
        "Synthetic efficiency - # train":"-",
        "Synthetic efficiency - truncated":"-",
        "Synthetic efficiency - # prompt tokens":"-",
        "Synthetic efficiency - # output tokens":"-",
        "Synthetic efficiency - # trials":"-"
    },
    {
        "Model":"RedPajama-INCITE-Base (7B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"117.299",
        "NaturalQuestions (closed-book) - # output tokens":"300",
        "NaturalQuestions (closed-book) - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"505.352",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"467.936",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"83.942",
        "WikiFact - # output tokens":"4.149",
        "WikiFact - # trials":"1",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"312.071",
        "Synthetic reasoning (abstract symbols) - # output tokens":"32.857",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"388.046",
        "Synthetic reasoning (natural language) - # output tokens":"20",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":434.671,
        "bAbI - # output tokens":5.0,
        "bAbI - # trials":1,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"939.582",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"387.171",
        "MATH - # output tokens":"3.257",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.984",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1112.644",
        "MATH (chain-of-thoughts) - # output tokens":"140.064",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1176.83",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"597.329",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"291.539",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"942.402",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"1",
        "Copyright (text) - # eval":"-",
        "Copyright (text) - # train":"-",
        "Copyright (text) - truncated":"-",
        "Copyright (text) - # prompt tokens":"-",
        "Copyright (text) - # output tokens":"-",
        "Copyright (text) - # trials":"-",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"-",
        "Disinformation (reiteration) - # train":"-",
        "Disinformation (reiteration) - truncated":"-",
        "Disinformation (reiteration) - # prompt tokens":"-",
        "Disinformation (reiteration) - # output tokens":"-",
        "Disinformation (reiteration) - # trials":"-",
        "Disinformation (wedging) - # eval":"-",
        "Disinformation (wedging) - # train":"-",
        "Disinformation (wedging) - truncated":"-",
        "Disinformation (wedging) - # prompt tokens":"-",
        "Disinformation (wedging) - # output tokens":"-",
        "Disinformation (wedging) - # trials":"-",
        "BBQ - # eval":"-",
        "BBQ - # train":"-",
        "BBQ - truncated":"-",
        "BBQ - # prompt tokens":"-",
        "BBQ - # output tokens":"-",
        "BBQ - # trials":"-",
        "BOLD - # eval":"-",
        "BOLD - # train":"-",
        "BOLD - truncated":"-",
        "BOLD - # prompt tokens":"-",
        "BOLD - # output tokens":"-",
        "BOLD - # trials":"-",
        "RealToxicityPrompts - # eval":"-",
        "RealToxicityPrompts - # train":"-",
        "RealToxicityPrompts - truncated":"-",
        "RealToxicityPrompts - # prompt tokens":"-",
        "RealToxicityPrompts - # output tokens":"-",
        "RealToxicityPrompts - # trials":"-",
        "Synthetic efficiency - # eval":"-",
        "Synthetic efficiency - # train":"-",
        "Synthetic efficiency - truncated":"-",
        "Synthetic efficiency - # prompt tokens":"-",
        "Synthetic efficiency - # output tokens":"-",
        "Synthetic efficiency - # trials":"-"
    },
    {
        "Model":"RedPajama-INCITE-Instruct (7B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"117.299",
        "NaturalQuestions (closed-book) - # output tokens":"300",
        "NaturalQuestions (closed-book) - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"505.352",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"467.936",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"83.942",
        "WikiFact - # output tokens":"4.018",
        "WikiFact - # trials":"1",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"312.071",
        "Synthetic reasoning (abstract symbols) - # output tokens":"32.857",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"388.046",
        "Synthetic reasoning (natural language) - # output tokens":"20",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":434.671,
        "bAbI - # output tokens":5.0,
        "bAbI - # trials":1,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"939.582",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"387.171",
        "MATH - # output tokens":"2.81",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.984",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1112.644",
        "MATH (chain-of-thoughts) - # output tokens":"132.17",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1176.83",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"597.329",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"291.539",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"942.402",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"1",
        "Copyright (text) - # eval":"-",
        "Copyright (text) - # train":"-",
        "Copyright (text) - truncated":"-",
        "Copyright (text) - # prompt tokens":"-",
        "Copyright (text) - # output tokens":"-",
        "Copyright (text) - # trials":"-",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"-",
        "Disinformation (reiteration) - # train":"-",
        "Disinformation (reiteration) - truncated":"-",
        "Disinformation (reiteration) - # prompt tokens":"-",
        "Disinformation (reiteration) - # output tokens":"-",
        "Disinformation (reiteration) - # trials":"-",
        "Disinformation (wedging) - # eval":"-",
        "Disinformation (wedging) - # train":"-",
        "Disinformation (wedging) - truncated":"-",
        "Disinformation (wedging) - # prompt tokens":"-",
        "Disinformation (wedging) - # output tokens":"-",
        "Disinformation (wedging) - # trials":"-",
        "BBQ - # eval":"-",
        "BBQ - # train":"-",
        "BBQ - truncated":"-",
        "BBQ - # prompt tokens":"-",
        "BBQ - # output tokens":"-",
        "BBQ - # trials":"-",
        "BOLD - # eval":"-",
        "BOLD - # train":"-",
        "BOLD - truncated":"-",
        "BOLD - # prompt tokens":"-",
        "BOLD - # output tokens":"-",
        "BOLD - # trials":"-",
        "RealToxicityPrompts - # eval":"-",
        "RealToxicityPrompts - # train":"-",
        "RealToxicityPrompts - truncated":"-",
        "RealToxicityPrompts - # prompt tokens":"-",
        "RealToxicityPrompts - # output tokens":"-",
        "RealToxicityPrompts - # trials":"-",
        "Synthetic efficiency - # eval":"-",
        "Synthetic efficiency - # train":"-",
        "Synthetic efficiency - truncated":"-",
        "Synthetic efficiency - # prompt tokens":"-",
        "Synthetic efficiency - # output tokens":"-",
        "Synthetic efficiency - # trials":"-"
    },
    {
        "Model":"MPT (30B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"117.299",
        "NaturalQuestions (closed-book) - # output tokens":"0.999",
        "NaturalQuestions (closed-book) - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"505.352",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"467.936",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"83.942",
        "WikiFact - # output tokens":"1",
        "WikiFact - # trials":"1",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"312.071",
        "Synthetic reasoning (abstract symbols) - # output tokens":"1",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"388.046",
        "Synthetic reasoning (natural language) - # output tokens":"1",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":434.671,
        "bAbI - # output tokens":1.0,
        "bAbI - # trials":1,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"0.932",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"939.582",
        "GSM8K - # output tokens":"1",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"387.171",
        "MATH - # output tokens":"1",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.984",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1112.644",
        "MATH (chain-of-thoughts) - # output tokens":"1",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1176.83",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"597.329",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"291.539",
        "Data imputation - # output tokens":"1",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"942.402",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"1",
        "Copyright (text) - # eval":"-",
        "Copyright (text) - # train":"-",
        "Copyright (text) - truncated":"-",
        "Copyright (text) - # prompt tokens":"-",
        "Copyright (text) - # output tokens":"-",
        "Copyright (text) - # trials":"-",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"-",
        "Disinformation (reiteration) - # train":"-",
        "Disinformation (reiteration) - truncated":"-",
        "Disinformation (reiteration) - # prompt tokens":"-",
        "Disinformation (reiteration) - # output tokens":"-",
        "Disinformation (reiteration) - # trials":"-",
        "Disinformation (wedging) - # eval":"-",
        "Disinformation (wedging) - # train":"-",
        "Disinformation (wedging) - truncated":"-",
        "Disinformation (wedging) - # prompt tokens":"-",
        "Disinformation (wedging) - # output tokens":"-",
        "Disinformation (wedging) - # trials":"-",
        "BBQ - # eval":"-",
        "BBQ - # train":"-",
        "BBQ - truncated":"-",
        "BBQ - # prompt tokens":"-",
        "BBQ - # output tokens":"-",
        "BBQ - # trials":"-",
        "BOLD - # eval":"-",
        "BOLD - # train":"-",
        "BOLD - truncated":"-",
        "BOLD - # prompt tokens":"-",
        "BOLD - # output tokens":"-",
        "BOLD - # trials":"-",
        "RealToxicityPrompts - # eval":"-",
        "RealToxicityPrompts - # train":"-",
        "RealToxicityPrompts - truncated":"-",
        "RealToxicityPrompts - # prompt tokens":"-",
        "RealToxicityPrompts - # output tokens":"-",
        "RealToxicityPrompts - # trials":"-",
        "Synthetic efficiency - # eval":"-",
        "Synthetic efficiency - # train":"-",
        "Synthetic efficiency - truncated":"-",
        "Synthetic efficiency - # prompt tokens":"-",
        "Synthetic efficiency - # output tokens":"-",
        "Synthetic efficiency - # trials":"-"
    },
    {
        "Model":"MPT-Instruct (30B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"117.299",
        "NaturalQuestions (closed-book) - # output tokens":"1",
        "NaturalQuestions (closed-book) - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"505.352",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"467.936",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"83.942",
        "WikiFact - # output tokens":"1",
        "WikiFact - # trials":"1",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"312.071",
        "Synthetic reasoning (abstract symbols) - # output tokens":"1",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"388.046",
        "Synthetic reasoning (natural language) - # output tokens":"1",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":434.671,
        "bAbI - # output tokens":1.0,
        "bAbI - # trials":1,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"1",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"939.582",
        "GSM8K - # output tokens":"1",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"387.171",
        "MATH - # output tokens":"1",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.984",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1112.644",
        "MATH (chain-of-thoughts) - # output tokens":"1",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1176.83",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"597.329",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"291.539",
        "Data imputation - # output tokens":"1",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"942.402",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"1",
        "Copyright (text) - # eval":"-",
        "Copyright (text) - # train":"-",
        "Copyright (text) - truncated":"-",
        "Copyright (text) - # prompt tokens":"-",
        "Copyright (text) - # output tokens":"-",
        "Copyright (text) - # trials":"-",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"-",
        "Disinformation (reiteration) - # train":"-",
        "Disinformation (reiteration) - truncated":"-",
        "Disinformation (reiteration) - # prompt tokens":"-",
        "Disinformation (reiteration) - # output tokens":"-",
        "Disinformation (reiteration) - # trials":"-",
        "Disinformation (wedging) - # eval":"-",
        "Disinformation (wedging) - # train":"-",
        "Disinformation (wedging) - truncated":"-",
        "Disinformation (wedging) - # prompt tokens":"-",
        "Disinformation (wedging) - # output tokens":"-",
        "Disinformation (wedging) - # trials":"-",
        "BBQ - # eval":"-",
        "BBQ - # train":"-",
        "BBQ - truncated":"-",
        "BBQ - # prompt tokens":"-",
        "BBQ - # output tokens":"-",
        "BBQ - # trials":"-",
        "BOLD - # eval":"-",
        "BOLD - # train":"-",
        "BOLD - truncated":"-",
        "BOLD - # prompt tokens":"-",
        "BOLD - # output tokens":"-",
        "BOLD - # trials":"-",
        "RealToxicityPrompts - # eval":"-",
        "RealToxicityPrompts - # train":"-",
        "RealToxicityPrompts - truncated":"-",
        "RealToxicityPrompts - # prompt tokens":"-",
        "RealToxicityPrompts - # output tokens":"-",
        "RealToxicityPrompts - # trials":"-",
        "Synthetic efficiency - # eval":"-",
        "Synthetic efficiency - # train":"-",
        "Synthetic efficiency - truncated":"-",
        "Synthetic efficiency - # prompt tokens":"-",
        "Synthetic efficiency - # output tokens":"-",
        "Synthetic efficiency - # trials":"-"
    },
    {
        "Model":"Falcon (7B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"124.246",
        "NaturalQuestions (closed-book) - # output tokens":"1",
        "NaturalQuestions (closed-book) - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"507.503",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"500.12",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"83.776",
        "WikiFact - # output tokens":"1",
        "WikiFact - # trials":"1",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"335.816",
        "Synthetic reasoning (abstract symbols) - # output tokens":"1",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"385.96",
        "Synthetic reasoning (natural language) - # output tokens":"1",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":430.859,
        "bAbI - # output tokens":1.0,
        "bAbI - # trials":1,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"296.836",
        "Dyck - # output tokens":"0.998",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"1056.967",
        "GSM8K - # output tokens":"1",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"429.658",
        "MATH - # output tokens":"1",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.818",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1150.049",
        "MATH (chain-of-thoughts) - # output tokens":"1",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1181.948",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"612.779",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"304.048",
        "Data imputation - # output tokens":"1",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1019.896",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"1",
        "Copyright (text) - # eval":"-",
        "Copyright (text) - # train":"-",
        "Copyright (text) - truncated":"-",
        "Copyright (text) - # prompt tokens":"-",
        "Copyright (text) - # output tokens":"-",
        "Copyright (text) - # trials":"-",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"-",
        "Disinformation (reiteration) - # train":"-",
        "Disinformation (reiteration) - truncated":"-",
        "Disinformation (reiteration) - # prompt tokens":"-",
        "Disinformation (reiteration) - # output tokens":"-",
        "Disinformation (reiteration) - # trials":"-",
        "Disinformation (wedging) - # eval":"-",
        "Disinformation (wedging) - # train":"-",
        "Disinformation (wedging) - truncated":"-",
        "Disinformation (wedging) - # prompt tokens":"-",
        "Disinformation (wedging) - # output tokens":"-",
        "Disinformation (wedging) - # trials":"-",
        "BBQ - # eval":"-",
        "BBQ - # train":"-",
        "BBQ - truncated":"-",
        "BBQ - # prompt tokens":"-",
        "BBQ - # output tokens":"-",
        "BBQ - # trials":"-",
        "BOLD - # eval":"-",
        "BOLD - # train":"-",
        "BOLD - truncated":"-",
        "BOLD - # prompt tokens":"-",
        "BOLD - # output tokens":"-",
        "BOLD - # trials":"-",
        "RealToxicityPrompts - # eval":"-",
        "RealToxicityPrompts - # train":"-",
        "RealToxicityPrompts - truncated":"-",
        "RealToxicityPrompts - # prompt tokens":"-",
        "RealToxicityPrompts - # output tokens":"-",
        "RealToxicityPrompts - # trials":"-",
        "Synthetic efficiency - # eval":"-",
        "Synthetic efficiency - # train":"-",
        "Synthetic efficiency - truncated":"-",
        "Synthetic efficiency - # prompt tokens":"-",
        "Synthetic efficiency - # output tokens":"-",
        "Synthetic efficiency - # trials":"-"
    },
    {
        "Model":"Falcon-Instruct (7B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"124.246",
        "NaturalQuestions (closed-book) - # output tokens":"0.999",
        "NaturalQuestions (closed-book) - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"507.503",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"500.12",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"83.776",
        "WikiFact - # output tokens":"1",
        "WikiFact - # trials":"1",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"335.816",
        "Synthetic reasoning (abstract symbols) - # output tokens":"1",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"385.96",
        "Synthetic reasoning (natural language) - # output tokens":"1",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":430.859,
        "bAbI - # output tokens":1.0,
        "bAbI - # trials":1,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"296.836",
        "Dyck - # output tokens":"1",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"1056.967",
        "GSM8K - # output tokens":"1",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"429.658",
        "MATH - # output tokens":"1",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.818",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1150.049",
        "MATH (chain-of-thoughts) - # output tokens":"1",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1181.948",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"612.779",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"304.048",
        "Data imputation - # output tokens":"1",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1019.896",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"1",
        "Copyright (text) - # eval":"-",
        "Copyright (text) - # train":"-",
        "Copyright (text) - truncated":"-",
        "Copyright (text) - # prompt tokens":"-",
        "Copyright (text) - # output tokens":"-",
        "Copyright (text) - # trials":"-",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"-",
        "Disinformation (reiteration) - # train":"-",
        "Disinformation (reiteration) - truncated":"-",
        "Disinformation (reiteration) - # prompt tokens":"-",
        "Disinformation (reiteration) - # output tokens":"-",
        "Disinformation (reiteration) - # trials":"-",
        "Disinformation (wedging) - # eval":"-",
        "Disinformation (wedging) - # train":"-",
        "Disinformation (wedging) - truncated":"-",
        "Disinformation (wedging) - # prompt tokens":"-",
        "Disinformation (wedging) - # output tokens":"-",
        "Disinformation (wedging) - # trials":"-",
        "BBQ - # eval":"-",
        "BBQ - # train":"-",
        "BBQ - truncated":"-",
        "BBQ - # prompt tokens":"-",
        "BBQ - # output tokens":"-",
        "BBQ - # trials":"-",
        "BOLD - # eval":"-",
        "BOLD - # train":"-",
        "BOLD - truncated":"-",
        "BOLD - # prompt tokens":"-",
        "BOLD - # output tokens":"-",
        "BOLD - # trials":"-",
        "RealToxicityPrompts - # eval":"-",
        "RealToxicityPrompts - # train":"-",
        "RealToxicityPrompts - truncated":"-",
        "RealToxicityPrompts - # prompt tokens":"-",
        "RealToxicityPrompts - # output tokens":"-",
        "RealToxicityPrompts - # trials":"-",
        "Synthetic efficiency - # eval":"-",
        "Synthetic efficiency - # train":"-",
        "Synthetic efficiency - truncated":"-",
        "Synthetic efficiency - # prompt tokens":"-",
        "Synthetic efficiency - # output tokens":"-",
        "Synthetic efficiency - # trials":"-"
    },
    {
        "Model":"Falcon (40B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"124.246",
        "NaturalQuestions (closed-book) - # output tokens":"1",
        "NaturalQuestions (closed-book) - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"507.503",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"500.12",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"83.776",
        "WikiFact - # output tokens":"1",
        "WikiFact - # trials":"1",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"335.816",
        "Synthetic reasoning (abstract symbols) - # output tokens":"1",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"385.96",
        "Synthetic reasoning (natural language) - # output tokens":"1",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":430.859,
        "bAbI - # output tokens":1.0,
        "bAbI - # trials":1,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"296.836",
        "Dyck - # output tokens":"0.996",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"1056.967",
        "GSM8K - # output tokens":"1",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"429.658",
        "MATH - # output tokens":"1",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.818",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1150.049",
        "MATH (chain-of-thoughts) - # output tokens":"1",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1181.948",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"612.779",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"304.048",
        "Data imputation - # output tokens":"1",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1019.896",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"1",
        "Copyright (text) - # eval":"-",
        "Copyright (text) - # train":"-",
        "Copyright (text) - truncated":"-",
        "Copyright (text) - # prompt tokens":"-",
        "Copyright (text) - # output tokens":"-",
        "Copyright (text) - # trials":"-",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"-",
        "Disinformation (reiteration) - # train":"-",
        "Disinformation (reiteration) - truncated":"-",
        "Disinformation (reiteration) - # prompt tokens":"-",
        "Disinformation (reiteration) - # output tokens":"-",
        "Disinformation (reiteration) - # trials":"-",
        "Disinformation (wedging) - # eval":"-",
        "Disinformation (wedging) - # train":"-",
        "Disinformation (wedging) - truncated":"-",
        "Disinformation (wedging) - # prompt tokens":"-",
        "Disinformation (wedging) - # output tokens":"-",
        "Disinformation (wedging) - # trials":"-",
        "BBQ - # eval":"-",
        "BBQ - # train":"-",
        "BBQ - truncated":"-",
        "BBQ - # prompt tokens":"-",
        "BBQ - # output tokens":"-",
        "BBQ - # trials":"-",
        "BOLD - # eval":"-",
        "BOLD - # train":"-",
        "BOLD - truncated":"-",
        "BOLD - # prompt tokens":"-",
        "BOLD - # output tokens":"-",
        "BOLD - # trials":"-",
        "RealToxicityPrompts - # eval":"-",
        "RealToxicityPrompts - # train":"-",
        "RealToxicityPrompts - truncated":"-",
        "RealToxicityPrompts - # prompt tokens":"-",
        "RealToxicityPrompts - # output tokens":"-",
        "RealToxicityPrompts - # trials":"-",
        "Synthetic efficiency - # eval":"-",
        "Synthetic efficiency - # train":"-",
        "Synthetic efficiency - truncated":"-",
        "Synthetic efficiency - # prompt tokens":"-",
        "Synthetic efficiency - # output tokens":"-",
        "Synthetic efficiency - # trials":"-"
    },
    {
        "Model":"Falcon-Instruct (40B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"124.246",
        "NaturalQuestions (closed-book) - # output tokens":"0.999",
        "NaturalQuestions (closed-book) - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"507.503",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"500.12",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"83.776",
        "WikiFact - # output tokens":"0.999",
        "WikiFact - # trials":"1",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"335.816",
        "Synthetic reasoning (abstract symbols) - # output tokens":"1",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"385.96",
        "Synthetic reasoning (natural language) - # output tokens":"1",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":430.859,
        "bAbI - # output tokens":1.0,
        "bAbI - # trials":1,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"296.836",
        "Dyck - # output tokens":"1",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"1056.967",
        "GSM8K - # output tokens":"1",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"429.658",
        "MATH - # output tokens":"1",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.818",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1150.049",
        "MATH (chain-of-thoughts) - # output tokens":"1",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1181.948",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"612.779",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"304.048",
        "Data imputation - # output tokens":"1",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1019.896",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"1",
        "Copyright (text) - # eval":"-",
        "Copyright (text) - # train":"-",
        "Copyright (text) - truncated":"-",
        "Copyright (text) - # prompt tokens":"-",
        "Copyright (text) - # output tokens":"-",
        "Copyright (text) - # trials":"-",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"-",
        "Disinformation (reiteration) - # train":"-",
        "Disinformation (reiteration) - truncated":"-",
        "Disinformation (reiteration) - # prompt tokens":"-",
        "Disinformation (reiteration) - # output tokens":"-",
        "Disinformation (reiteration) - # trials":"-",
        "Disinformation (wedging) - # eval":"-",
        "Disinformation (wedging) - # train":"-",
        "Disinformation (wedging) - truncated":"-",
        "Disinformation (wedging) - # prompt tokens":"-",
        "Disinformation (wedging) - # output tokens":"-",
        "Disinformation (wedging) - # trials":"-",
        "BBQ - # eval":"-",
        "BBQ - # train":"-",
        "BBQ - truncated":"-",
        "BBQ - # prompt tokens":"-",
        "BBQ - # output tokens":"-",
        "BBQ - # trials":"-",
        "BOLD - # eval":"-",
        "BOLD - # train":"-",
        "BOLD - truncated":"-",
        "BOLD - # prompt tokens":"-",
        "BOLD - # output tokens":"-",
        "BOLD - # trials":"-",
        "RealToxicityPrompts - # eval":"-",
        "RealToxicityPrompts - # train":"-",
        "RealToxicityPrompts - truncated":"-",
        "RealToxicityPrompts - # prompt tokens":"-",
        "RealToxicityPrompts - # output tokens":"-",
        "RealToxicityPrompts - # trials":"-",
        "Synthetic efficiency - # eval":"-",
        "Synthetic efficiency - # train":"-",
        "Synthetic efficiency - truncated":"-",
        "Synthetic efficiency - # prompt tokens":"-",
        "Synthetic efficiency - # output tokens":"-",
        "Synthetic efficiency - # trials":"-"
    },
    {
        "Model":"GLM (130B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"122.991",
        "NaturalQuestions (closed-book) - # output tokens":"6.707",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"3.75",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"389.036",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"2.5",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"460.637",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"85.716",
        "WikiFact - # output tokens":"23.531",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"296.268",
        "Synthetic reasoning (abstract symbols) - # output tokens":"6.402",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"366.73",
        "Synthetic reasoning (natural language) - # output tokens":"5.859",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":405.316,
        "bAbI - # output tokens":2.161,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"203.784",
        "Dyck - # output tokens":"4.583",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"1066.292",
        "GSM8K - # output tokens":"151.917",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"405.639",
        "MATH - # output tokens":"5.141",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.263",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1233.025",
        "MATH (chain-of-thoughts) - # output tokens":"129.137",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1156.543",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"609.591",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"368.375",
        "Data imputation - # output tokens":"3.457",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1041.057",
        "Entity matching - # output tokens":"2",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"375.375",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0.083",
        "Copyright (text) - # prompt tokens":"115.674",
        "Copyright (text) - # output tokens":"4404.41",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"259.377",
        "Disinformation (reiteration) - # output tokens":"388.479",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"1",
        "Disinformation (wedging) - # prompt tokens":"163.258",
        "Disinformation (wedging) - # output tokens":"450",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"388.369",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0.999",
        "BOLD - # prompt tokens":"9.506",
        "BOLD - # output tokens":"20",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0.005",
        "RealToxicityPrompts - # prompt tokens":"15.393",
        "RealToxicityPrompts - # output tokens":"99.953",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"1",
        "Synthetic efficiency - # prompt tokens":"665.62",
        "Synthetic efficiency - # output tokens":"18.143",
        "Synthetic efficiency - # trials":"1"
    },
    {
        "Model":"InstructPalmyra (30B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"4.247",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"511.121",
        "TruthfulQA - # output tokens":"0.999",
        "TruthfulQA - # trials":"3",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"83.576",
        "WikiFact - # output tokens":"16.251",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.313",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"5.04",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":469.022,
        "bAbI - # output tokens":1.191,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"1.045",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"76.416",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"2.845",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.13",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1227.7",
        "MATH (chain-of-thoughts) - # output tokens":"95.183",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"2.205",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0.001",
        "Copyright (text) - # prompt tokens":"142.617",
        "Copyright (text) - # output tokens":"888.667",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"266.025",
        "Disinformation (reiteration) - # output tokens":"64.289",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"158.983",
        "Disinformation (wedging) - # output tokens":"172.325",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"424.437",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"11.805",
        "BOLD - # output tokens":"19.789",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"15.491",
        "RealToxicityPrompts - # output tokens":"94.21",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"-",
        "Synthetic efficiency - # train":"-",
        "Synthetic efficiency - truncated":"-",
        "Synthetic efficiency - # prompt tokens":"-",
        "Synthetic efficiency - # output tokens":"-",
        "Synthetic efficiency - # trials":"-"
    },
    {
        "Model":"Palmyra X (43B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"3.19",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"511.121",
        "TruthfulQA - # output tokens":"0.949",
        "TruthfulQA - # trials":"3",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"83.576",
        "WikiFact - # output tokens":"16.098",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.711",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"5.288",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":469.022,
        "bAbI - # output tokens":1.479,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"2.711",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"90.399",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"2.352",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"8",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1414.164",
        "MATH (chain-of-thoughts) - # output tokens":"78.821",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"0.991",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"0.996",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"2.334",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"499.5",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0",
        "Copyright (text) - # prompt tokens":"142.932",
        "Copyright (text) - # output tokens":"178.26",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"266.025",
        "Disinformation (reiteration) - # output tokens":"70.298",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"158.983",
        "Disinformation (wedging) - # output tokens":"138.975",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"424.437",
        "BBQ - # output tokens":"0.959",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"11.805",
        "BOLD - # output tokens":"17.37",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0",
        "RealToxicityPrompts - # prompt tokens":"15.491",
        "RealToxicityPrompts - # output tokens":"54.651",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"-",
        "Synthetic efficiency - # train":"-",
        "Synthetic efficiency - truncated":"-",
        "Synthetic efficiency - # prompt tokens":"-",
        "Synthetic efficiency - # output tokens":"-",
        "Synthetic efficiency - # trials":"-"
    },
    {
        "Model":"YaLM (100B)",
        "Mean win rate":"-",
        "The Pile - # eval":"-",
        "The Pile - # train":"-",
        "The Pile - truncated":"-",
        "The Pile - # prompt tokens":"-",
        "The Pile - # output tokens":"-",
        "The Pile - # trials":"-",
        "TwitterAAE - # eval":"-",
        "TwitterAAE - # train":"-",
        "TwitterAAE - truncated":"-",
        "TwitterAAE - # prompt tokens":"-",
        "TwitterAAE - # output tokens":"-",
        "TwitterAAE - # trials":"-",
        "ICE - # eval":"-",
        "ICE - # train":"-",
        "ICE - truncated":"-",
        "ICE - # prompt tokens":"-",
        "ICE - # output tokens":"-",
        "ICE - # trials":"-",
        "BLiMP - # eval":"-",
        "BLiMP - # train":"-",
        "BLiMP - truncated":"-",
        "BLiMP - # prompt tokens":"-",
        "BLiMP - # output tokens":"-",
        "BLiMP - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"111.534",
        "NaturalQuestions (closed-book) - # output tokens":"299.515",
        "NaturalQuestions (closed-book) - # trials":"3",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"3.75",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"405.414",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"2.5",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"453.383",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "WikiFact - # eval":"746.2",
        "WikiFact - # train":"5",
        "WikiFact - truncated":"0",
        "WikiFact - # prompt tokens":"80.918",
        "WikiFact - # output tokens":"39.875",
        "WikiFact - # trials":"3",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"319.253",
        "Synthetic reasoning (abstract symbols) - # output tokens":"25.463",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"402.441",
        "Synthetic reasoning (natural language) - # output tokens":"15.611",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":906,
        "bAbI - # train":5.0,
        "bAbI - truncated":0.0,
        "bAbI - # prompt tokens":468.988,
        "bAbI - # output tokens":5.0,
        "bAbI - # trials":3,
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"829.652",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"360.702",
        "MATH - # output tokens":"19.186",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.513",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1127.115",
        "MATH (chain-of-thoughts) - # output tokens":"398.654",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1186.445",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"604.56",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"287.091",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"848.586",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"3",
        "Copyright (text) - # eval":"289.143",
        "Copyright (text) - # train":"0",
        "Copyright (text) - truncated":"0",
        "Copyright (text) - # prompt tokens":"116.88",
        "Copyright (text) - # output tokens":"3657.143",
        "Copyright (text) - # trials":"1",
        "Copyright (code) - # eval":"-",
        "Copyright (code) - # train":"-",
        "Copyright (code) - truncated":"-",
        "Copyright (code) - # prompt tokens":"-",
        "Copyright (code) - # output tokens":"-",
        "Copyright (code) - # trials":"-",
        "Disinformation (reiteration) - # eval":"34",
        "Disinformation (reiteration) - # train":"2",
        "Disinformation (reiteration) - truncated":"0",
        "Disinformation (reiteration) - # prompt tokens":"254.522",
        "Disinformation (reiteration) - # output tokens":"471.498",
        "Disinformation (reiteration) - # trials":"3",
        "Disinformation (wedging) - # eval":"2.75",
        "Disinformation (wedging) - # train":"0",
        "Disinformation (wedging) - truncated":"0",
        "Disinformation (wedging) - # prompt tokens":"157.492",
        "Disinformation (wedging) - # output tokens":"450",
        "Disinformation (wedging) - # trials":"1",
        "BBQ - # eval":"1000",
        "BBQ - # train":"5",
        "BBQ - truncated":"0",
        "BBQ - # prompt tokens":"429.662",
        "BBQ - # output tokens":"1",
        "BBQ - # trials":"3",
        "BOLD - # eval":"1000",
        "BOLD - # train":"0",
        "BOLD - truncated":"0",
        "BOLD - # prompt tokens":"11.445",
        "BOLD - # output tokens":"20",
        "BOLD - # trials":"1",
        "RealToxicityPrompts - # eval":"500",
        "RealToxicityPrompts - # train":"0",
        "RealToxicityPrompts - truncated":"0.001",
        "RealToxicityPrompts - # prompt tokens":"15.247",
        "RealToxicityPrompts - # output tokens":"100",
        "RealToxicityPrompts - # trials":"1",
        "Synthetic efficiency - # eval":"10",
        "Synthetic efficiency - # train":"0",
        "Synthetic efficiency - truncated":"0.26",
        "Synthetic efficiency - # prompt tokens":"665.54",
        "Synthetic efficiency - # output tokens":"18.143",
        "Synthetic efficiency - # trials":"1"
    }
]