[
    {
        "Model":"J1-Jumbo v1 (178B)",
        "EM":0.048,
        "Denoised inference time (s)":"0.584",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":154.108,
        "# output tokens":7.223,
        "# trials":3
    },
    {
        "Model":"J1-Large v1 (7.5B)",
        "EM":0.033,
        "Denoised inference time (s)":"0.377",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":154.108,
        "# output tokens":7.28,
        "# trials":3
    },
    {
        "Model":"J1-Grande v1 (17B)",
        "EM":0.048,
        "Denoised inference time (s)":"0.534",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":154.108,
        "# output tokens":6.971,
        "# trials":3
    },
    {
        "Model":"J1-Grande v2 beta (17B)",
        "EM":0.04,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":154.108,
        "# output tokens":7.021,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "EM":0.066,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":154.108,
        "# output tokens":7.413,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "EM":0.038,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":154.108,
        "# output tokens":7.328,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Large (7.5B)",
        "EM":0.044,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":154.108,
        "# output tokens":6.946,
        "# trials":3
    },
    {
        "Model":"Luminous Base (13B)",
        "EM":0.045,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":173.128,
        "# output tokens":5.862,
        "# trials":3
    },
    {
        "Model":"Luminous Extended (30B)",
        "EM":0.044,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":173.128,
        "# output tokens":6.203,
        "# trials":3
    },
    {
        "Model":"Luminous Supreme (70B)",
        "EM":0.052,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":173.128,
        "# output tokens":6.357,
        "# trials":3
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B)",
        "EM":0.052,
        "Denoised inference time (s)":"0.862",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":172.435,
        "# output tokens":6.201,
        "# trials":3
    },
    {
        "Model":"BLOOM (176B)",
        "EM":0.061,
        "Denoised inference time (s)":"0.372",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":163.472,
        "# output tokens":14.279,
        "# trials":3
    },
    {
        "Model":"T0pp (11B)",
        "EM":0.0,
        "Denoised inference time (s)":"0.288",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":205.478,
        "# output tokens":50.0,
        "# trials":3
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B)",
        "EM":0.042,
        "Denoised inference time (s)":"0.613",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":172.772,
        "# output tokens":6.228,
        "# trials":3
    },
    {
        "Model":"Cohere large v20220720 (13.1B)",
        "EM":0.031,
        "Denoised inference time (s)":"0.37",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":172.772,
        "# output tokens":6.311,
        "# trials":3
    },
    {
        "Model":"Cohere medium v20220720 (6.1B)",
        "EM":0.023,
        "Denoised inference time (s)":"0.278",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":172.772,
        "# output tokens":5.759,
        "# trials":3
    },
    {
        "Model":"Cohere small v20220720 (410M)",
        "EM":0.039,
        "Denoised inference time (s)":"0.26",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":172.772,
        "# output tokens":5.507,
        "# trials":3
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B)",
        "EM":0.047,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":172.772,
        "# output tokens":6.228,
        "# trials":3
    },
    {
        "Model":"Cohere medium v20221108 (6.1B)",
        "EM":0.041,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":172.772,
        "# output tokens":6.028,
        "# trials":3
    },
    {
        "Model":"Cohere Command beta (6.1B)",
        "EM":0.047,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":172.772,
        "# output tokens":5.768,
        "# trials":3
    },
    {
        "Model":"Cohere Command beta (52.4B)",
        "EM":0.05,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":172.772,
        "# output tokens":5.874,
        "# trials":3
    },
    {
        "Model":"GPT-J (6B)",
        "EM":0.049,
        "Denoised inference time (s)":"0.089",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":172.435,
        "# output tokens":11.026,
        "# trials":3
    },
    {
        "Model":"GPT-NeoX (20B)",
        "EM":0.043,
        "Denoised inference time (s)":"0.102",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":171.124,
        "# output tokens":14.867,
        "# trials":3
    },
    {
        "Model":"Pythia (6.9B)",
        "EM":0.021,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":170.79,
        "# output tokens":50.0,
        "# trials":1
    },
    {
        "Model":"Pythia (12B)",
        "EM":0.033,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":170.79,
        "# output tokens":50.0,
        "# trials":1
    },
    {
        "Model":"T5 (11B)",
        "EM":0.037,
        "Denoised inference time (s)":"0.573",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":205.478,
        "# output tokens":50.0,
        "# trials":3
    },
    {
        "Model":"UL2 (20B)",
        "EM":0.054,
        "Denoised inference time (s)":"0.388",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":209.478,
        "# output tokens":50.0,
        "# trials":3
    },
    {
        "Model":"OPT (175B)",
        "EM":0.018,
        "Denoised inference time (s)":"0.192",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":172.435,
        "# output tokens":9.696,
        "# trials":3
    },
    {
        "Model":"OPT (66B)",
        "EM":0.044,
        "Denoised inference time (s)":"0.058",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":172.435,
        "# output tokens":10.972,
        "# trials":3
    },
    {
        "Model":"LLaMA (7B)",
        "EM":0.037,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":204.08,
        "# output tokens":50.0,
        "# trials":1
    },
    {
        "Model":"LLaMA (13B)",
        "EM":0.049,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":204.08,
        "# output tokens":50.0,
        "# trials":1
    },
    {
        "Model":"LLaMA (30B)",
        "EM":0.068,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":204.08,
        "# output tokens":50.0,
        "# trials":1
    },
    {
        "Model":"LLaMA (65B)",
        "EM":0.06,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":204.08,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (7B)",
        "EM":0.066,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":204.08,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (13B)",
        "EM":0.05,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":204.08,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (70B)",
        "EM":0.08,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":204.08,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Alpaca (7B)",
        "EM":0.058,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":204.08,
        "# output tokens":18.689,
        "# trials":1
    },
    {
        "Model":"Vicuna v1.3 (7B)",
        "EM":0.06,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":204.08,
        "# output tokens":50.0,
        "# trials":1
    },
    {
        "Model":"Vicuna v1.3 (13B)",
        "EM":0.033,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":204.08,
        "# output tokens":50.0,
        "# trials":1
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "EM":0.064,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":0.0,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"TNLG v2 (530B)",
        "EM":0.069,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":172.435,
        "# output tokens":6.288,
        "# trials":3
    },
    {
        "Model":"TNLG v2 (6.7B)",
        "EM":0.043,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":172.435,
        "# output tokens":5.881,
        "# trials":3
    },
    {
        "Model":"davinci (175B)",
        "EM":0.047,
        "Denoised inference time (s)":"0.349",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":172.435,
        "# output tokens":5.865,
        "# trials":3
    },
    {
        "Model":"curie (6.7B)",
        "EM":0.039,
        "Denoised inference time (s)":"0.124",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":172.435,
        "# output tokens":6.016,
        "# trials":3
    },
    {
        "Model":"babbage (1.3B)",
        "EM":0.037,
        "Denoised inference time (s)":"0.145",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":172.435,
        "# output tokens":5.898,
        "# trials":3
    },
    {
        "Model":"ada (350M)",
        "EM":0.036,
        "Denoised inference time (s)":"0.166",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":172.435,
        "# output tokens":5.301,
        "# trials":3
    },
    {
        "Model":"text-davinci-003",
        "EM":0.111,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":172.435,
        "# output tokens":6.997,
        "# trials":3
    },
    {
        "Model":"text-davinci-002",
        "EM":0.103,
        "Denoised inference time (s)":"0.351",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":172.435,
        "# output tokens":6.741,
        "# trials":3
    },
    {
        "Model":"text-curie-001",
        "EM":0.037,
        "Denoised inference time (s)":"0.168",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":172.435,
        "# output tokens":6.272,
        "# trials":3
    },
    {
        "Model":"text-babbage-001",
        "EM":0.037,
        "Denoised inference time (s)":"0.163",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":172.435,
        "# output tokens":6.108,
        "# trials":3
    },
    {
        "Model":"text-ada-001",
        "EM":0.04,
        "Denoised inference time (s)":"0.113",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":172.435,
        "# output tokens":6.032,
        "# trials":3
    },
    {
        "Model":"code-davinci-002",
        "EM":0.111,
        "Denoised inference time (s)":"0.349",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":172.435,
        "# output tokens":8.203,
        "# trials":3
    },
    {
        "Model":"code-cushman-001 (12B)",
        "EM":0.052,
        "Denoised inference time (s)":"0.148",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":172.435,
        "# output tokens":7.648,
        "# trials":3
    },
    {
        "Model":"gpt-3.5-turbo-0301",
        "EM":0.047,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":157.546,
        "# output tokens":8.309,
        "# trials":1
    },
    {
        "Model":"gpt-3.5-turbo-0613",
        "EM":0.099,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":157.546,
        "# output tokens":7.113,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Base-v1 (3B)",
        "EM":0.035,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":170.79,
        "# output tokens":50.0,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Instruct-v1 (3B)",
        "EM":0.002,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":170.79,
        "# output tokens":50.0,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Base (7B)",
        "EM":0.041,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":170.79,
        "# output tokens":50.0,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Instruct (7B)",
        "EM":0.01,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":170.79,
        "# output tokens":50.0,
        "# trials":1
    },
    {
        "Model":"MPT (30B)",
        "EM":0.072,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":170.79,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"MPT-Instruct (30B)",
        "EM":0.068,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":170.79,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon (7B)",
        "EM":0.033,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":204.064,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon-Instruct (7B)",
        "EM":0.043,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":204.064,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon (40B)",
        "EM":0.054,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":204.064,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon-Instruct (40B)",
        "EM":0.052,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":204.064,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"GLM (130B)",
        "EM":0.041,
        "Denoised inference time (s)":"0.25",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":162.076,
        "# output tokens":7.425,
        "# trials":3
    },
    {
        "Model":"InstructPalmyra (30B)",
        "EM":0.042,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":172.435,
        "# output tokens":5.535,
        "# trials":3
    },
    {
        "Model":"Palmyra X (43B)",
        "EM":0.083,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":172.435,
        "# output tokens":6.738,
        "# trials":3
    },
    {
        "Model":"YaLM (100B)",
        "EM":0.008,
        "Denoised inference time (s)":"0.405",
        "# eval":515,
        "# train":5,
        "truncated":0,
        "# prompt tokens":168.407,
        "# output tokens":42.535,
        "# trials":3
    }
]