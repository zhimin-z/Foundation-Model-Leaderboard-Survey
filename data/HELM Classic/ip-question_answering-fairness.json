[
    {
        "Model":"J1-Jumbo v1 (178B)",
        "Mean win rate":0.483,
        "MMLU - EM (Fairness)":0.236,
        "BoolQ - EM (Fairness)":0.709,
        "NarrativeQA - F1 (Fairness)":"0.581",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.235,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.54",
        "QuAC - F1 (Fairness)":"0.268",
        "HellaSwag - EM (Fairness)":"0.614",
        "OpenbookQA - EM (Fairness)":"0.466",
        "TruthfulQA - EM (Fairness)":0.156
    },
    {
        "Model":"J1-Large v1 (7.5B)",
        "Mean win rate":0.25,
        "MMLU - EM (Fairness)":0.204,
        "BoolQ - EM (Fairness)":0.622,
        "NarrativeQA - F1 (Fairness)":"0.513",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.146,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.47",
        "QuAC - F1 (Fairness)":"0.241",
        "HellaSwag - EM (Fairness)":"0.528",
        "OpenbookQA - EM (Fairness)":"0.444",
        "TruthfulQA - EM (Fairness)":0.174
    },
    {
        "Model":"J1-Grande v1 (17B)",
        "Mean win rate":0.433,
        "MMLU - EM (Fairness)":0.232,
        "BoolQ - EM (Fairness)":0.678,
        "NarrativeQA - F1 (Fairness)":"0.547",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.187,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.521",
        "QuAC - F1 (Fairness)":"0.274",
        "HellaSwag - EM (Fairness)":"0.58",
        "OpenbookQA - EM (Fairness)":"0.472",
        "TruthfulQA - EM (Fairness)":0.163
    },
    {
        "Model":"J1-Grande v2 beta (17B)",
        "Mean win rate":0.711,
        "MMLU - EM (Fairness)":0.409,
        "BoolQ - EM (Fairness)":0.764,
        "NarrativeQA - F1 (Fairness)":"0.647",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.27,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.571",
        "QuAC - F1 (Fairness)":"0.308",
        "HellaSwag - EM (Fairness)":"0.623",
        "OpenbookQA - EM (Fairness)":"0.478",
        "TruthfulQA - EM (Fairness)":0.242
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "Mean win rate":0.839,
        "MMLU - EM (Fairness)":0.45,
        "BoolQ - EM (Fairness)":0.792,
        "NarrativeQA - F1 (Fairness)":"0.658",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.327,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.62",
        "QuAC - F1 (Fairness)":"0.34",
        "HellaSwag - EM (Fairness)":"0.655",
        "OpenbookQA - EM (Fairness)":"0.488",
        "TruthfulQA - EM (Fairness)":0.354
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "Mean win rate":0.742,
        "MMLU - EM (Fairness)":0.433,
        "BoolQ - EM (Fairness)":0.78,
        "NarrativeQA - F1 (Fairness)":"0.645",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.283,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.584",
        "QuAC - F1 (Fairness)":"0.34",
        "HellaSwag - EM (Fairness)":"0.632",
        "OpenbookQA - EM (Fairness)":"0.466",
        "TruthfulQA - EM (Fairness)":0.29
    },
    {
        "Model":"Jurassic-2 Large (7.5B)",
        "Mean win rate":0.464,
        "MMLU - EM (Fairness)":0.297,
        "BoolQ - EM (Fairness)":0.685,
        "NarrativeQA - F1 (Fairness)":"-",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.217,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.539",
        "QuAC - F1 (Fairness)":"-",
        "HellaSwag - EM (Fairness)":"0.567",
        "OpenbookQA - EM (Fairness)":"0.45",
        "TruthfulQA - EM (Fairness)":0.196
    },
    {
        "Model":"Luminous Base (13B)",
        "Mean win rate":0.254,
        "MMLU - EM (Fairness)":0.185,
        "BoolQ - EM (Fairness)":0.653,
        "NarrativeQA - F1 (Fairness)":"0.498",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.16,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.511",
        "QuAC - F1 (Fairness)":"0.266",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.125
    },
    {
        "Model":"Luminous Extended (30B)",
        "Mean win rate":0.457,
        "MMLU - EM (Fairness)":0.237,
        "BoolQ - EM (Fairness)":0.711,
        "NarrativeQA - F1 (Fairness)":"0.532",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.214,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.551",
        "QuAC - F1 (Fairness)":"0.277",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.16
    },
    {
        "Model":"Luminous Supreme (70B)",
        "Mean win rate":0.519,
        "MMLU - EM (Fairness)":0.264,
        "BoolQ - EM (Fairness)":0.694,
        "NarrativeQA - F1 (Fairness)":"0.603",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.241,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.597",
        "QuAC - F1 (Fairness)":"0.288",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.132
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B)",
        "Mean win rate":0.816,
        "MMLU - EM (Fairness)":0.447,
        "BoolQ - EM (Fairness)":0.782,
        "NarrativeQA - F1 (Fairness)":"0.646",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.239,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.642",
        "QuAC - F1 (Fairness)":"0.356",
        "HellaSwag - EM (Fairness)":"0.695",
        "OpenbookQA - EM (Fairness)":"0.482",
        "TruthfulQA - EM (Fairness)":0.3
    },
    {
        "Model":"BLOOM (176B)",
        "Mean win rate":0.522,
        "MMLU - EM (Fairness)":0.274,
        "BoolQ - EM (Fairness)":0.656,
        "NarrativeQA - F1 (Fairness)":"0.577",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.187,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.575",
        "QuAC - F1 (Fairness)":"0.273",
        "HellaSwag - EM (Fairness)":"0.585",
        "OpenbookQA - EM (Fairness)":"0.482",
        "TruthfulQA - EM (Fairness)":0.186
    },
    {
        "Model":"T0pp (11B)",
        "Mean win rate":0.268,
        "MMLU - EM (Fairness)":0.382,
        "BoolQ - EM (Fairness)":0.0,
        "NarrativeQA - F1 (Fairness)":"0.086",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.028,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.136",
        "QuAC - F1 (Fairness)":"0.067",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.35
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B)",
        "Mean win rate":0.521,
        "MMLU - EM (Fairness)":0.315,
        "BoolQ - EM (Fairness)":0.667,
        "NarrativeQA - F1 (Fairness)":"0.548",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.255,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.535",
        "QuAC - F1 (Fairness)":"0.281",
        "HellaSwag - EM (Fairness)":"0.66",
        "OpenbookQA - EM (Fairness)":"0.47",
        "TruthfulQA - EM (Fairness)":0.156
    },
    {
        "Model":"Cohere large v20220720 (13.1B)",
        "Mean win rate":0.37,
        "MMLU - EM (Fairness)":0.281,
        "BoolQ - EM (Fairness)":0.676,
        "NarrativeQA - F1 (Fairness)":"0.512",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.178,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.507",
        "QuAC - F1 (Fairness)":"0.256",
        "HellaSwag - EM (Fairness)":"0.575",
        "OpenbookQA - EM (Fairness)":"0.446",
        "TruthfulQA - EM (Fairness)":0.157
    },
    {
        "Model":"Cohere medium v20220720 (6.1B)",
        "Mean win rate":0.21,
        "MMLU - EM (Fairness)":0.237,
        "BoolQ - EM (Fairness)":0.597,
        "NarrativeQA - F1 (Fairness)":"0.438",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.126,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.432",
        "QuAC - F1 (Fairness)":"0.198",
        "HellaSwag - EM (Fairness)":"0.525",
        "OpenbookQA - EM (Fairness)":"0.42",
        "TruthfulQA - EM (Fairness)":0.174
    },
    {
        "Model":"Cohere small v20220720 (410M)",
        "Mean win rate":0.127,
        "MMLU - EM (Fairness)":0.222,
        "BoolQ - EM (Fairness)":0.374,
        "NarrativeQA - F1 (Fairness)":"0.179",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.055,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.219",
        "QuAC - F1 (Fairness)":"0.144",
        "HellaSwag - EM (Fairness)":"0.308",
        "OpenbookQA - EM (Fairness)":"0.28",
        "TruthfulQA - EM (Fairness)":0.203
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B)",
        "Mean win rate":0.599,
        "MMLU - EM (Fairness)":0.317,
        "BoolQ - EM (Fairness)":0.708,
        "NarrativeQA - F1 (Fairness)":"0.553",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.299,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.566",
        "QuAC - F1 (Fairness)":"0.275",
        "HellaSwag - EM (Fairness)":"0.687",
        "OpenbookQA - EM (Fairness)":"0.5",
        "TruthfulQA - EM (Fairness)":0.12
    },
    {
        "Model":"Cohere medium v20221108 (6.1B)",
        "Mean win rate":0.288,
        "MMLU - EM (Fairness)":0.22,
        "BoolQ - EM (Fairness)":0.642,
        "NarrativeQA - F1 (Fairness)":"0.497",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.149,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.45",
        "QuAC - F1 (Fairness)":"0.229",
        "HellaSwag - EM (Fairness)":"0.567",
        "OpenbookQA - EM (Fairness)":"0.44",
        "TruthfulQA - EM (Fairness)":0.182
    },
    {
        "Model":"Cohere Command beta (6.1B)",
        "Mean win rate":0.582,
        "MMLU - EM (Fairness)":0.366,
        "BoolQ - EM (Fairness)":0.748,
        "NarrativeQA - F1 (Fairness)":"0.595",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.167,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.654",
        "QuAC - F1 (Fairness)":"0.273",
        "HellaSwag - EM (Fairness)":"0.608",
        "OpenbookQA - EM (Fairness)":"0.468",
        "TruthfulQA - EM (Fairness)":0.163
    },
    {
        "Model":"Cohere Command beta (52.4B)",
        "Mean win rate":0.842,
        "MMLU - EM (Fairness)":0.407,
        "BoolQ - EM (Fairness)":0.822,
        "NarrativeQA - F1 (Fairness)":"0.657",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.296,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.706",
        "QuAC - F1 (Fairness)":"0.316",
        "HellaSwag - EM (Fairness)":"0.699",
        "OpenbookQA - EM (Fairness)":"0.508",
        "TruthfulQA - EM (Fairness)":0.222
    },
    {
        "Model":"GPT-J (6B)",
        "Mean win rate":0.23,
        "MMLU - EM (Fairness)":0.22,
        "BoolQ - EM (Fairness)":0.639,
        "NarrativeQA - F1 (Fairness)":"0.433",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.122,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.493",
        "QuAC - F1 (Fairness)":"0.249",
        "HellaSwag - EM (Fairness)":"0.486",
        "OpenbookQA - EM (Fairness)":"0.416",
        "TruthfulQA - EM (Fairness)":0.18
    },
    {
        "Model":"GPT-NeoX (20B)",
        "Mean win rate":0.272,
        "MMLU - EM (Fairness)":0.215,
        "BoolQ - EM (Fairness)":0.609,
        "NarrativeQA - F1 (Fairness)":"0.461",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.154,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.525",
        "QuAC - F1 (Fairness)":"0.232",
        "HellaSwag - EM (Fairness)":"0.552",
        "OpenbookQA - EM (Fairness)":"0.438",
        "TruthfulQA - EM (Fairness)":0.179
    },
    {
        "Model":"Pythia (6.9B)",
        "Mean win rate":0.166,
        "MMLU - EM (Fairness)":0.207,
        "BoolQ - EM (Fairness)":0.552,
        "NarrativeQA - F1 (Fairness)":"0.389",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.103,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.464",
        "QuAC - F1 (Fairness)":"0.198",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.18
    },
    {
        "Model":"Pythia (12B)",
        "Mean win rate":0.184,
        "MMLU - EM (Fairness)":0.212,
        "BoolQ - EM (Fairness)":0.547,
        "NarrativeQA - F1 (Fairness)":"0.449",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.131,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.523",
        "QuAC - F1 (Fairness)":"0.227",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.154
    },
    {
        "Model":"T5 (11B)",
        "Mean win rate":0.201,
        "MMLU - EM (Fairness)":0.235,
        "BoolQ - EM (Fairness)":0.723,
        "NarrativeQA - F1 (Fairness)":"0.05",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.159,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.424",
        "QuAC - F1 (Fairness)":"0.074",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.101
    },
    {
        "Model":"UL2 (20B)",
        "Mean win rate":0.233,
        "MMLU - EM (Fairness)":0.273,
        "BoolQ - EM (Fairness)":0.698,
        "NarrativeQA - F1 (Fairness)":"0.053",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.162,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.303",
        "QuAC - F1 (Fairness)":"0.107",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.162
    },
    {
        "Model":"OPT (175B)",
        "Mean win rate":0.628,
        "MMLU - EM (Fairness)":0.287,
        "BoolQ - EM (Fairness)":0.731,
        "NarrativeQA - F1 (Fairness)":"0.573",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.246,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.561",
        "QuAC - F1 (Fairness)":"0.266",
        "HellaSwag - EM (Fairness)":"0.66",
        "OpenbookQA - EM (Fairness)":"0.5",
        "TruthfulQA - EM (Fairness)":0.203
    },
    {
        "Model":"OPT (66B)",
        "Mean win rate":0.448,
        "MMLU - EM (Fairness)":0.229,
        "BoolQ - EM (Fairness)":0.71,
        "NarrativeQA - F1 (Fairness)":"0.526",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.218,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.536",
        "QuAC - F1 (Fairness)":"0.268",
        "HellaSwag - EM (Fairness)":"0.597",
        "OpenbookQA - EM (Fairness)":"0.454",
        "TruthfulQA - EM (Fairness)":0.173
    },
    {
        "Model":"LLaMA (7B)",
        "Mean win rate":0.533,
        "MMLU - EM (Fairness)":0.284,
        "BoolQ - EM (Fairness)":0.71,
        "NarrativeQA - F1 (Fairness)":"0.552",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.241,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.537",
        "QuAC - F1 (Fairness)":"0.257",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.219
    },
    {
        "Model":"LLaMA (13B)",
        "Mean win rate":0.615,
        "MMLU - EM (Fairness)":0.385,
        "BoolQ - EM (Fairness)":0.666,
        "NarrativeQA - F1 (Fairness)":"0.628",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.288,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.561",
        "QuAC - F1 (Fairness)":"0.267",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.234
    },
    {
        "Model":"LLaMA (30B)",
        "Mean win rate":0.858,
        "MMLU - EM (Fairness)":0.496,
        "BoolQ - EM (Fairness)":0.813,
        "NarrativeQA - F1 (Fairness)":"0.657",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.356,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.621",
        "QuAC - F1 (Fairness)":"0.325",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.266
    },
    {
        "Model":"LLaMA (65B)",
        "Mean win rate":0.93,
        "MMLU - EM (Fairness)":0.551,
        "BoolQ - EM (Fairness)":0.847,
        "NarrativeQA - F1 (Fairness)":"0.661",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.375,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.633",
        "QuAC - F1 (Fairness)":"0.333",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.42
    },
    {
        "Model":"Llama 2 (7B)",
        "Mean win rate":0.646,
        "MMLU - EM (Fairness)":0.392,
        "BoolQ - EM (Fairness)":0.706,
        "NarrativeQA - F1 (Fairness)":"0.596",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.264,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.55",
        "QuAC - F1 (Fairness)":"0.321",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.223
    },
    {
        "Model":"Llama 2 (13B)",
        "Mean win rate":0.808,
        "MMLU - EM (Fairness)":0.466,
        "BoolQ - EM (Fairness)":0.732,
        "NarrativeQA - F1 (Fairness)":"0.657",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.309,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.58",
        "QuAC - F1 (Fairness)":"0.351",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.274
    },
    {
        "Model":"Llama 2 (70B)",
        "Mean win rate":0.969,
        "MMLU - EM (Fairness)":0.557,
        "BoolQ - EM (Fairness)":0.859,
        "NarrativeQA - F1 (Fairness)":"0.709",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.4,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.637",
        "QuAC - F1 (Fairness)":"0.414",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.434
    },
    {
        "Model":"Alpaca (7B)",
        "Mean win rate":0.425,
        "MMLU - EM (Fairness)":0.346,
        "BoolQ - EM (Fairness)":0.729,
        "NarrativeQA - F1 (Fairness)":"0.299",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.21,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.53",
        "QuAC - F1 (Fairness)":"0.204",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.202
    },
    {
        "Model":"Vicuna v1.3 (7B)",
        "Mean win rate":0.596,
        "MMLU - EM (Fairness)":0.385,
        "BoolQ - EM (Fairness)":0.67,
        "NarrativeQA - F1 (Fairness)":"0.553",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.224,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.575",
        "QuAC - F1 (Fairness)":"0.304",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.235
    },
    {
        "Model":"Vicuna v1.3 (13B)",
        "Mean win rate":0.775,
        "MMLU - EM (Fairness)":0.424,
        "BoolQ - EM (Fairness)":0.748,
        "NarrativeQA - F1 (Fairness)":"0.607",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.266,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.63",
        "QuAC - F1 (Fairness)":"0.324",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.315
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "Mean win rate":0.871,
        "MMLU - EM (Fairness)":0.542,
        "BoolQ - EM (Fairness)":0.842,
        "NarrativeQA - F1 (Fairness)":"0.644",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.3,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.625",
        "QuAC - F1 (Fairness)":"0.353",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.332
    },
    {
        "Model":"TNLG v2 (530B)",
        "Mean win rate":0.766,
        "MMLU - EM (Fairness)":0.418,
        "BoolQ - EM (Fairness)":0.767,
        "NarrativeQA - F1 (Fairness)":"0.632",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.318,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.598",
        "QuAC - F1 (Fairness)":"0.313",
        "HellaSwag - EM (Fairness)":"0.678",
        "OpenbookQA - EM (Fairness)":"0.504",
        "TruthfulQA - EM (Fairness)":0.197
    },
    {
        "Model":"TNLG v2 (6.7B)",
        "Mean win rate":0.277,
        "MMLU - EM (Fairness)":0.212,
        "BoolQ - EM (Fairness)":0.665,
        "NarrativeQA - F1 (Fairness)":"0.517",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.162,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.501",
        "QuAC - F1 (Fairness)":"0.267",
        "HellaSwag - EM (Fairness)":"0.53",
        "OpenbookQA - EM (Fairness)":"0.412",
        "TruthfulQA - EM (Fairness)":0.144
    },
    {
        "Model":"davinci (175B)",
        "Mean win rate":0.599,
        "MMLU - EM (Fairness)":0.38,
        "BoolQ - EM (Fairness)":0.682,
        "NarrativeQA - F1 (Fairness)":"0.597",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.276,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.567",
        "QuAC - F1 (Fairness)":"0.279",
        "HellaSwag - EM (Fairness)":"0.641",
        "OpenbookQA - EM (Fairness)":"0.502",
        "TruthfulQA - EM (Fairness)":0.155
    },
    {
        "Model":"curie (6.7B)",
        "Mean win rate":0.254,
        "MMLU - EM (Fairness)":0.218,
        "BoolQ - EM (Fairness)":0.594,
        "NarrativeQA - F1 (Fairness)":"0.482",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.147,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.479",
        "QuAC - F1 (Fairness)":"0.243",
        "HellaSwag - EM (Fairness)":"0.522",
        "OpenbookQA - EM (Fairness)":"0.43",
        "TruthfulQA - EM (Fairness)":0.186
    },
    {
        "Model":"babbage (1.3B)",
        "Mean win rate":0.125,
        "MMLU - EM (Fairness)":0.206,
        "BoolQ - EM (Fairness)":0.436,
        "NarrativeQA - F1 (Fairness)":"0.367",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.084,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.381",
        "QuAC - F1 (Fairness)":"0.202",
        "HellaSwag - EM (Fairness)":"0.401",
        "OpenbookQA - EM (Fairness)":"0.326",
        "TruthfulQA - EM (Fairness)":0.178
    },
    {
        "Model":"ada (350M)",
        "Mean win rate":0.112,
        "MMLU - EM (Fairness)":0.21,
        "BoolQ - EM (Fairness)":0.507,
        "NarrativeQA - F1 (Fairness)":"0.205",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.057,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.273",
        "QuAC - F1 (Fairness)":"0.166",
        "HellaSwag - EM (Fairness)":"0.294",
        "OpenbookQA - EM (Fairness)":"0.318",
        "TruthfulQA - EM (Fairness)":0.185
    },
    {
        "Model":"text-davinci-003",
        "Mean win rate":0.978,
        "MMLU - EM (Fairness)":0.537,
        "BoolQ - EM (Fairness)":0.858,
        "NarrativeQA - F1 (Fairness)":"0.664",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.356,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.721",
        "QuAC - F1 (Fairness)":"0.45",
        "HellaSwag - EM (Fairness)":"0.729",
        "OpenbookQA - EM (Fairness)":"0.578",
        "TruthfulQA - EM (Fairness)":0.491
    },
    {
        "Model":"text-davinci-002",
        "Mean win rate":0.927,
        "MMLU - EM (Fairness)":0.531,
        "BoolQ - EM (Fairness)":0.837,
        "NarrativeQA - F1 (Fairness)":"0.646",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.32,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.659",
        "QuAC - F1 (Fairness)":"0.353",
        "HellaSwag - EM (Fairness)":"0.703",
        "OpenbookQA - EM (Fairness)":"0.54",
        "TruthfulQA - EM (Fairness)":0.515
    },
    {
        "Model":"text-curie-001",
        "Mean win rate":0.327,
        "MMLU - EM (Fairness)":0.231,
        "BoolQ - EM (Fairness)":0.576,
        "NarrativeQA - F1 (Fairness)":"0.463",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.132,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.5",
        "QuAC - F1 (Fairness)":"0.255",
        "HellaSwag - EM (Fairness)":"0.534",
        "OpenbookQA - EM (Fairness)":"0.452",
        "TruthfulQA - EM (Fairness)":0.239
    },
    {
        "Model":"text-babbage-001",
        "Mean win rate":0.144,
        "MMLU - EM (Fairness)":0.205,
        "BoolQ - EM (Fairness)":0.41,
        "NarrativeQA - F1 (Fairness)":"0.299",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.053,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.24",
        "QuAC - F1 (Fairness)":"0.196",
        "HellaSwag - EM (Fairness)":"0.405",
        "OpenbookQA - EM (Fairness)":"0.386",
        "TruthfulQA - EM (Fairness)":0.207
    },
    {
        "Model":"text-ada-001",
        "Mean win rate":0.066,
        "MMLU - EM (Fairness)":0.202,
        "BoolQ - EM (Fairness)":0.378,
        "NarrativeQA - F1 (Fairness)":"0.119",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.012,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.083",
        "QuAC - F1 (Fairness)":"0.091",
        "HellaSwag - EM (Fairness)":"0.27",
        "OpenbookQA - EM (Fairness)":"0.266",
        "TruthfulQA - EM (Fairness)":0.191
    },
    {
        "Model":"gpt-3.5-turbo-0301",
        "Mean win rate":0.761,
        "MMLU - EM (Fairness)":0.53,
        "BoolQ - EM (Fairness)":0.666,
        "NarrativeQA - F1 (Fairness)":"0.585",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.331,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.559",
        "QuAC - F1 (Fairness)":"0.417",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.514
    },
    {
        "Model":"gpt-3.5-turbo-0613",
        "Mean win rate":0.743,
        "MMLU - EM (Fairness)":0.313,
        "BoolQ - EM (Fairness)":0.817,
        "NarrativeQA - F1 (Fairness)":"0.547",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.287,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.627",
        "QuAC - F1 (Fairness)":"0.398",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.255
    },
    {
        "Model":"RedPajama-INCITE-Base-v1 (3B)",
        "Mean win rate":0.303,
        "MMLU - EM (Fairness)":0.232,
        "BoolQ - EM (Fairness)":0.624,
        "NarrativeQA - F1 (Fairness)":"0.42",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.145,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.452",
        "QuAC - F1 (Fairness)":"0.238",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.248
    },
    {
        "Model":"RedPajama-INCITE-Instruct-v1 (3B)",
        "Mean win rate":0.295,
        "MMLU - EM (Fairness)":0.222,
        "BoolQ - EM (Fairness)":0.648,
        "NarrativeQA - F1 (Fairness)":"0.506",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.143,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.571",
        "QuAC - F1 (Fairness)":"0.183",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.179
    },
    {
        "Model":"RedPajama-INCITE-Base (7B)",
        "Mean win rate":0.354,
        "MMLU - EM (Fairness)":0.276,
        "BoolQ - EM (Fairness)":0.65,
        "NarrativeQA - F1 (Fairness)":"0.524",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.193,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.514",
        "QuAC - F1 (Fairness)":"0.238",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.17
    },
    {
        "Model":"RedPajama-INCITE-Instruct (7B)",
        "Mean win rate":0.373,
        "MMLU - EM (Fairness)":0.305,
        "BoolQ - EM (Fairness)":0.616,
        "NarrativeQA - F1 (Fairness)":"0.506",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.164,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.592",
        "QuAC - F1 (Fairness)":"0.181",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.183
    },
    {
        "Model":"MPT (30B)",
        "Mean win rate":0.666,
        "MMLU - EM (Fairness)":0.41,
        "BoolQ - EM (Fairness)":0.631,
        "NarrativeQA - F1 (Fairness)":"0.653",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.287,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.624",
        "QuAC - F1 (Fairness)":"0.318",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.19
    },
    {
        "Model":"MPT-Instruct (30B)",
        "Mean win rate":0.649,
        "MMLU - EM (Fairness)":0.4,
        "BoolQ - EM (Fairness)":0.807,
        "NarrativeQA - F1 (Fairness)":"0.633",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.233,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.639",
        "QuAC - F1 (Fairness)":"0.252",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.18
    },
    {
        "Model":"Falcon (7B)",
        "Mean win rate":0.482,
        "MMLU - EM (Fairness)":0.261,
        "BoolQ - EM (Fairness)":0.702,
        "NarrativeQA - F1 (Fairness)":"0.52",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.233,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.537",
        "QuAC - F1 (Fairness)":"0.262",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.213
    },
    {
        "Model":"Falcon-Instruct (7B)",
        "Mean win rate":0.255,
        "MMLU - EM (Fairness)":0.261,
        "BoolQ - EM (Fairness)":0.637,
        "NarrativeQA - F1 (Fairness)":"0.354",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.148,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.383",
        "QuAC - F1 (Fairness)":"0.219",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.183
    },
    {
        "Model":"Falcon (40B)",
        "Mean win rate":0.752,
        "MMLU - EM (Fairness)":0.48,
        "BoolQ - EM (Fairness)":0.783,
        "NarrativeQA - F1 (Fairness)":"0.559",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.338,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.625",
        "QuAC - F1 (Fairness)":"0.256",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.292
    },
    {
        "Model":"Falcon-Instruct (40B)",
        "Mean win rate":0.768,
        "MMLU - EM (Fairness)":0.466,
        "BoolQ - EM (Fairness)":0.799,
        "NarrativeQA - F1 (Fairness)":"0.543",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.331,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.607",
        "QuAC - F1 (Fairness)":"0.308",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.312
    },
    {
        "Model":"GLM (130B)",
        "Mean win rate":0.477,
        "MMLU - EM (Fairness)":0.315,
        "BoolQ - EM (Fairness)":0.69,
        "NarrativeQA - F1 (Fairness)":"0.615",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.12,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.597",
        "QuAC - F1 (Fairness)":"0.205",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.192
    },
    {
        "Model":"InstructPalmyra (30B)",
        "Mean win rate":0.549,
        "MMLU - EM (Fairness)":0.371,
        "BoolQ - EM (Fairness)":0.7,
        "NarrativeQA - F1 (Fairness)":"0.405",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.276,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.63",
        "QuAC - F1 (Fairness)":"0.337",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.152
    },
    {
        "Model":"Palmyra X (43B)",
        "Mean win rate":0.967,
        "MMLU - EM (Fairness)":0.588,
        "BoolQ - EM (Fairness)":0.875,
        "NarrativeQA - F1 (Fairness)":"0.651",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.362,
        "NaturalQuestions (open-book) - F1 (Fairness)":"-",
        "QuAC - F1 (Fairness)":"0.399",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.542
    },
    {
        "Model":"YaLM (100B)",
        "Mean win rate":0.172,
        "MMLU - EM (Fairness)":0.243,
        "BoolQ - EM (Fairness)":0.583,
        "NarrativeQA - F1 (Fairness)":"0.146",
        "NaturalQuestions (closed-book) - F1 (Fairness)":0.052,
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.177",
        "QuAC - F1 (Fairness)":"0.1",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":0.202
    }
]