[
    {
        "Model":"J1-Jumbo v1 (178B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":396.74,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":694.652,
        "BoolQ - # output tokens":2.0,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"2.639",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1692.218",
        "NarrativeQA - # output tokens":"4.514",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":99.377,
        "NaturalQuestions (closed-book) - # output tokens":5.602,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.666",
        "NaturalQuestions (open-book) - truncated":"0.038",
        "NaturalQuestions (open-book) - # prompt tokens":"1418.457",
        "NaturalQuestions (open-book) - # output tokens":"5.682",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"1.829",
        "QuAC - truncated":"0.001",
        "QuAC - # prompt tokens":"1698.711",
        "QuAC - # output tokens":"26.784",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"62.466",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"4.348",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":355.015,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0
    },
    {
        "Model":"J1-Large v1 (7.5B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":396.74,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":694.652,
        "BoolQ - # output tokens":2.0,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"2.639",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1692.218",
        "NarrativeQA - # output tokens":"5.09",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":99.377,
        "NaturalQuestions (closed-book) - # output tokens":7.876,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.666",
        "NaturalQuestions (open-book) - truncated":"0.038",
        "NaturalQuestions (open-book) - # prompt tokens":"1418.457",
        "NaturalQuestions (open-book) - # output tokens":"5.946",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"1.829",
        "QuAC - truncated":"0.001",
        "QuAC - # prompt tokens":"1698.711",
        "QuAC - # output tokens":"27.642",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"62.466",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"4.348",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":355.015,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0
    },
    {
        "Model":"J1-Grande v1 (17B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":396.74,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":694.652,
        "BoolQ - # output tokens":2.0,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"2.639",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1692.218",
        "NarrativeQA - # output tokens":"4.528",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":99.377,
        "NaturalQuestions (closed-book) - # output tokens":5.971,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.666",
        "NaturalQuestions (open-book) - truncated":"0.038",
        "NaturalQuestions (open-book) - # prompt tokens":"1418.457",
        "NaturalQuestions (open-book) - # output tokens":"6.538",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"1.829",
        "QuAC - truncated":"0.001",
        "QuAC - # prompt tokens":"1698.711",
        "QuAC - # output tokens":"27.786",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"62.466",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"4.348",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":355.015,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0
    },
    {
        "Model":"J1-Grande v2 beta (17B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":396.74,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":694.652,
        "BoolQ - # output tokens":2.0,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"2.639",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1692.218",
        "NarrativeQA - # output tokens":"4.6",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":99.377,
        "NaturalQuestions (closed-book) - # output tokens":5.282,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.666",
        "NaturalQuestions (open-book) - truncated":"0.038",
        "NaturalQuestions (open-book) - # prompt tokens":"1418.457",
        "NaturalQuestions (open-book) - # output tokens":"5.27",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"1.829",
        "QuAC - truncated":"0.001",
        "QuAC - # prompt tokens":"1698.711",
        "QuAC - # output tokens":"23.053",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"62.466",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"4.348",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":355.015,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":396.74,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":694.652,
        "BoolQ - # output tokens":2.002,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"5",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"2818.1",
        "NarrativeQA - # output tokens":"6.406",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":99.377,
        "NaturalQuestions (closed-book) - # output tokens":5.365,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.93",
        "NaturalQuestions (open-book) - truncated":"0.012",
        "NaturalQuestions (open-book) - # prompt tokens":"1571.171",
        "NaturalQuestions (open-book) - # output tokens":"5.113",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"5",
        "QuAC - truncated":"0",
        "QuAC - # prompt tokens":"4018.779",
        "QuAC - # output tokens":"22.178",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"62.466",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"4.348",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":355.015,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":396.74,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":694.652,
        "BoolQ - # output tokens":2.002,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"2.639",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1692.218",
        "NarrativeQA - # output tokens":"5.261",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":99.377,
        "NaturalQuestions (closed-book) - # output tokens":6.315,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.666",
        "NaturalQuestions (open-book) - truncated":"0.038",
        "NaturalQuestions (open-book) - # prompt tokens":"1418.457",
        "NaturalQuestions (open-book) - # output tokens":"5.676",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"1.829",
        "QuAC - truncated":"0.001",
        "QuAC - # prompt tokens":"1698.711",
        "QuAC - # output tokens":"24.469",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"62.466",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"4.348",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":355.015,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0
    },
    {
        "Model":"Jurassic-2 Large (7.5B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":396.74,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":694.652,
        "BoolQ - # output tokens":2.0,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"-",
        "NarrativeQA - # train":"-",
        "NarrativeQA - truncated":"-",
        "NarrativeQA - # prompt tokens":"-",
        "NarrativeQA - # output tokens":"-",
        "NarrativeQA - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":99.377,
        "NaturalQuestions (closed-book) - # output tokens":6.729,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.666",
        "NaturalQuestions (open-book) - truncated":"0.038",
        "NaturalQuestions (open-book) - # prompt tokens":"1418.457",
        "NaturalQuestions (open-book) - # output tokens":"6.311",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"-",
        "QuAC - # train":"-",
        "QuAC - truncated":"-",
        "QuAC - # prompt tokens":"-",
        "QuAC - # output tokens":"-",
        "QuAC - # trials":"-",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"62.466",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"4.348",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":355.015,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0
    },
    {
        "Model":"Luminous Base (13B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":471.075,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":908.991,
        "BoolQ - # output tokens":1.002,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.621",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1647.783",
        "NarrativeQA - # output tokens":"6.798",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":111.754,
        "NaturalQuestions (closed-book) - # output tokens":5.287,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.711",
        "NaturalQuestions (open-book) - truncated":"0.039",
        "NaturalQuestions (open-book) - # prompt tokens":"1384.565",
        "NaturalQuestions (open-book) - # output tokens":"10.15",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.909",
        "QuAC - truncated":"0.033",
        "QuAC - # prompt tokens":"1641.256",
        "QuAC - # output tokens":"23.472",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":514.073,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0
    },
    {
        "Model":"Luminous Extended (30B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":471.075,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":908.991,
        "BoolQ - # output tokens":1.0,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.621",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1647.783",
        "NarrativeQA - # output tokens":"7.042",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":111.754,
        "NaturalQuestions (closed-book) - # output tokens":6.119,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.711",
        "NaturalQuestions (open-book) - truncated":"0.039",
        "NaturalQuestions (open-book) - # prompt tokens":"1384.565",
        "NaturalQuestions (open-book) - # output tokens":"10.3",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.909",
        "QuAC - truncated":"0.033",
        "QuAC - # prompt tokens":"1641.256",
        "QuAC - # output tokens":"21.144",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":514.073,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0
    },
    {
        "Model":"Luminous Supreme (70B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":471.075,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":908.991,
        "BoolQ - # output tokens":1.0,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.621",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1647.783",
        "NarrativeQA - # output tokens":"6.84",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":111.754,
        "NaturalQuestions (closed-book) - # output tokens":4.508,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.711",
        "NaturalQuestions (open-book) - truncated":"0.039",
        "NaturalQuestions (open-book) - # prompt tokens":"1384.565",
        "NaturalQuestions (open-book) - # output tokens":"6.362",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.909",
        "QuAC - truncated":"0.033",
        "QuAC - # prompt tokens":"1641.256",
        "QuAC - # output tokens":"26.241",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":514.073,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":908.406,
        "BoolQ - # output tokens":1.004,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"5",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"3803.911",
        "NarrativeQA - # output tokens":"6.952",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":5.47,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.964",
        "NaturalQuestions (open-book) - truncated":"0.007",
        "NaturalQuestions (open-book) - # prompt tokens":"1592.701",
        "NaturalQuestions (open-book) - # output tokens":"5.659",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"5",
        "QuAC - truncated":"0",
        "QuAC - # prompt tokens":"5199.788",
        "QuAC - # output tokens":"35.484",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"1.306",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0.132",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":3.75,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":404.621,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":2.5
    },
    {
        "Model":"BLOOM (176B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":436.99,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":897.107,
        "BoolQ - # output tokens":5.0,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.621",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1649.598",
        "NarrativeQA - # output tokens":"33.276",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":96.12,
        "NaturalQuestions (closed-book) - # output tokens":48.109,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.743",
        "NaturalQuestions (open-book) - truncated":"0.035",
        "NaturalQuestions (open-book) - # prompt tokens":"1313.422",
        "NaturalQuestions (open-book) - # output tokens":"38.803",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.944",
        "QuAC - truncated":"0.017",
        "QuAC - # prompt tokens":"1639.494",
        "QuAC - # output tokens":"90.164",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.875",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.444",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":3.75,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":370.611,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":2.5
    },
    {
        "Model":"T0pp (11B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":492.01,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":3.972,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":702.438,
        "BoolQ - # output tokens":5.0,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"0.187",
        "NarrativeQA - truncated":"0.372",
        "NarrativeQA - # prompt tokens":"877.742",
        "NarrativeQA - # output tokens":"100",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":113.556,
        "NaturalQuestions (closed-book) - # output tokens":300.0,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"3.396",
        "NaturalQuestions (open-book) - truncated":"0.057",
        "NaturalQuestions (open-book) - # prompt tokens":"903.877",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0",
        "QuAC - truncated":"0.985",
        "QuAC - # prompt tokens":"823.365",
        "QuAC - # output tokens":"100",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":3.75,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":391.646,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":2.5
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":481.26,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":925.307,
        "BoolQ - # output tokens":1.001,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.562",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1634.99",
        "NarrativeQA - # output tokens":"7.077",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":111.191,
        "NaturalQuestions (closed-book) - # output tokens":5.844,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.633",
        "NaturalQuestions (open-book) - truncated":"0.039",
        "NaturalQuestions (open-book) - # prompt tokens":"1481.344",
        "NaturalQuestions (open-book) - # output tokens":"8.834",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.881",
        "QuAC - truncated":"0.02",
        "QuAC - # prompt tokens":"1639.784",
        "QuAC - # output tokens":"32.717",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":514.648,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0
    },
    {
        "Model":"Cohere large v20220720 (13.1B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":481.26,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":925.307,
        "BoolQ - # output tokens":1.0,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.562",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1634.99",
        "NarrativeQA - # output tokens":"6.91",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":111.191,
        "NaturalQuestions (closed-book) - # output tokens":5.625,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.633",
        "NaturalQuestions (open-book) - truncated":"0.039",
        "NaturalQuestions (open-book) - # prompt tokens":"1481.344",
        "NaturalQuestions (open-book) - # output tokens":"10.443",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.881",
        "QuAC - truncated":"0.02",
        "QuAC - # prompt tokens":"1639.784",
        "QuAC - # output tokens":"30.036",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":514.648,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0
    },
    {
        "Model":"Cohere medium v20220720 (6.1B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":481.26,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":925.307,
        "BoolQ - # output tokens":1.0,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.562",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1634.99",
        "NarrativeQA - # output tokens":"6.771",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":111.191,
        "NaturalQuestions (closed-book) - # output tokens":5.267,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.633",
        "NaturalQuestions (open-book) - truncated":"0.039",
        "NaturalQuestions (open-book) - # prompt tokens":"1481.344",
        "NaturalQuestions (open-book) - # output tokens":"9.101",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.881",
        "QuAC - truncated":"0.02",
        "QuAC - # prompt tokens":"1639.784",
        "QuAC - # output tokens":"23.531",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":514.648,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0
    },
    {
        "Model":"Cohere small v20220720 (410M)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":481.26,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":925.307,
        "BoolQ - # output tokens":1.001,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.562",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1634.99",
        "NarrativeQA - # output tokens":"11.007",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":111.191,
        "NaturalQuestions (closed-book) - # output tokens":5.149,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.633",
        "NaturalQuestions (open-book) - truncated":"0.039",
        "NaturalQuestions (open-book) - # prompt tokens":"1481.344",
        "NaturalQuestions (open-book) - # output tokens":"22.835",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.881",
        "QuAC - truncated":"0.02",
        "QuAC - # prompt tokens":"1639.784",
        "QuAC - # output tokens":"20.639",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":514.648,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":481.26,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":925.307,
        "BoolQ - # output tokens":1.0,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.562",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1634.99",
        "NarrativeQA - # output tokens":"6.729",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":111.191,
        "NaturalQuestions (closed-book) - # output tokens":4.808,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.633",
        "NaturalQuestions (open-book) - truncated":"0.039",
        "NaturalQuestions (open-book) - # prompt tokens":"1481.344",
        "NaturalQuestions (open-book) - # output tokens":"6.093",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.881",
        "QuAC - truncated":"0.02",
        "QuAC - # prompt tokens":"1639.784",
        "QuAC - # output tokens":"27.944",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":514.648,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0
    },
    {
        "Model":"Cohere medium v20221108 (6.1B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":481.26,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":925.307,
        "BoolQ - # output tokens":1.0,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.562",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1634.99",
        "NarrativeQA - # output tokens":"7.144",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":111.191,
        "NaturalQuestions (closed-book) - # output tokens":6.745,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.633",
        "NaturalQuestions (open-book) - truncated":"0.039",
        "NaturalQuestions (open-book) - # prompt tokens":"1481.344",
        "NaturalQuestions (open-book) - # output tokens":"8.419",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.881",
        "QuAC - truncated":"0.02",
        "QuAC - # prompt tokens":"1639.784",
        "QuAC - # output tokens":"22.84",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":514.648,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0
    },
    {
        "Model":"Cohere Command beta (6.1B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":481.26,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":925.307,
        "BoolQ - # output tokens":1.0,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.508",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1600.684",
        "NarrativeQA - # output tokens":"5.807",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":111.191,
        "NaturalQuestions (closed-book) - # output tokens":4.687,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.602",
        "NaturalQuestions (open-book) - truncated":"0.039",
        "NaturalQuestions (open-book) - # prompt tokens":"1471.073",
        "NaturalQuestions (open-book) - # output tokens":"7.377",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.848",
        "QuAC - truncated":"0.022",
        "QuAC - # prompt tokens":"1610.503",
        "QuAC - # output tokens":"17.394",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"1",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"1",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":514.648,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0
    },
    {
        "Model":"Cohere Command beta (52.4B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":481.26,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":925.307,
        "BoolQ - # output tokens":1.0,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.508",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1600.684",
        "NarrativeQA - # output tokens":"5.992",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":111.191,
        "NaturalQuestions (closed-book) - # output tokens":4.325,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.602",
        "NaturalQuestions (open-book) - truncated":"0.039",
        "NaturalQuestions (open-book) - # prompt tokens":"1471.073",
        "NaturalQuestions (open-book) - # output tokens":"7.288",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.848",
        "QuAC - truncated":"0.022",
        "QuAC - # prompt tokens":"1610.503",
        "QuAC - # output tokens":"19.627",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"1",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"1",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":514.648,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0
    },
    {
        "Model":"GPT-J (6B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":908.406,
        "BoolQ - # output tokens":5.0,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.647",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1652.377",
        "NarrativeQA - # output tokens":"56.052",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":282.837,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.691",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1419.574",
        "NaturalQuestions (open-book) - # output tokens":"247.23",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.944",
        "QuAC - truncated":"0.016",
        "QuAC - # prompt tokens":"1644.831",
        "QuAC - # output tokens":"68.54",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":3.75,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":404.621,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":2.5
    },
    {
        "Model":"GPT-NeoX (20B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":467.936,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":913.897,
        "BoolQ - # output tokens":5.0,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.568",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1641.033",
        "NarrativeQA - # output tokens":"40.047",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.966,
        "NaturalQuestions (closed-book) - # output tokens":90.195,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.704",
        "NaturalQuestions (open-book) - truncated":"0.037",
        "NaturalQuestions (open-book) - # prompt tokens":"1394.229",
        "NaturalQuestions (open-book) - # output tokens":"87.693",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.889",
        "QuAC - truncated":"0.021",
        "QuAC - # prompt tokens":"1640.361",
        "QuAC - # output tokens":"77.489",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.806",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.346",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":3.75,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":406.102,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":2.5
    },
    {
        "Model":"Pythia (6.9B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":467.936,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":1251.897,
        "BoolQ - # output tokens":5.0,
        "BoolQ - # trials":1,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.969",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1691.082",
        "NarrativeQA - # output tokens":"100",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":117.299,
        "NaturalQuestions (closed-book) - # output tokens":300.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.704",
        "NaturalQuestions (open-book) - truncated":"0.037",
        "NaturalQuestions (open-book) - # prompt tokens":"1495.552",
        "NaturalQuestions (open-book) - # output tokens":"299.883",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.883",
        "QuAC - truncated":"0.021",
        "QuAC - # prompt tokens":"1655.708",
        "QuAC - # output tokens":"100",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":505.352,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0
    },
    {
        "Model":"Pythia (12B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":467.936,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":1251.897,
        "BoolQ - # output tokens":5.0,
        "BoolQ - # trials":1,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.969",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1691.082",
        "NarrativeQA - # output tokens":"100",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":117.299,
        "NaturalQuestions (closed-book) - # output tokens":300.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.704",
        "NaturalQuestions (open-book) - truncated":"0.037",
        "NaturalQuestions (open-book) - # prompt tokens":"1495.552",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.883",
        "QuAC - truncated":"0.021",
        "QuAC - # prompt tokens":"1655.708",
        "QuAC - # output tokens":"100",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":505.352,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0
    },
    {
        "Model":"T5 (11B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":4.326,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":420.562,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":1.588,
        "BoolQ - truncated":0.004,
        "BoolQ - # prompt tokens":401.944,
        "BoolQ - # output tokens":5.0,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"0",
        "NarrativeQA - truncated":"0.825",
        "NarrativeQA - # prompt tokens":"492.141",
        "NarrativeQA - # output tokens":"100",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":113.556,
        "NaturalQuestions (closed-book) - # output tokens":300.0,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0.924",
        "NaturalQuestions (open-book) - truncated":"0.349",
        "NaturalQuestions (open-book) - # prompt tokens":"301.907",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0",
        "QuAC - truncated":"0.999",
        "QuAC - # prompt tokens":"510.923",
        "QuAC - # output tokens":"100",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":3.547,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":371.92,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":2.5
    },
    {
        "Model":"UL2 (20B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":4.316,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":423.395,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":1.57,
        "BoolQ - truncated":0.004,
        "BoolQ - # prompt tokens":402.285,
        "BoolQ - # output tokens":5.0,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"0",
        "NarrativeQA - truncated":"0.834",
        "NarrativeQA - # prompt tokens":"492.876",
        "NarrativeQA - # output tokens":"100",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":117.556,
        "NaturalQuestions (closed-book) - # output tokens":300.0,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0.918",
        "NaturalQuestions (open-book) - truncated":"0.355",
        "NaturalQuestions (open-book) - # prompt tokens":"303.619",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0",
        "QuAC - truncated":"0.999",
        "QuAC - # prompt tokens":"510.938",
        "QuAC - # output tokens":"100",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":3.513,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":372.668,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":2.5
    },
    {
        "Model":"OPT (175B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":908.406,
        "BoolQ - # output tokens":5.0,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.647",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1652.377",
        "NarrativeQA - # output tokens":"40.781",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":278.02,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.691",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1419.574",
        "NaturalQuestions (open-book) - # output tokens":"194.671",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.944",
        "QuAC - truncated":"0.016",
        "QuAC - # prompt tokens":"1644.831",
        "QuAC - # output tokens":"77.836",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":3.75,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":404.621,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":2.5
    },
    {
        "Model":"OPT (66B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":908.406,
        "BoolQ - # output tokens":5.0,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.647",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1652.377",
        "NarrativeQA - # output tokens":"50.904",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":153.231,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.691",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1419.574",
        "NaturalQuestions (open-book) - # output tokens":"211.805",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.944",
        "QuAC - truncated":"0.016",
        "QuAC - # prompt tokens":"1644.831",
        "QuAC - # output tokens":"91.909",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0.2",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"1",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":3.75,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":404.621,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":2.5
    },
    {
        "Model":"LLaMA (7B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":522.547,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":1439.447,
        "BoolQ - # output tokens":5.0,
        "BoolQ - # trials":1,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.437",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1541.115",
        "NarrativeQA - # output tokens":"100",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":137.383,
        "NaturalQuestions (closed-book) - # output tokens":300.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"3.722",
        "NaturalQuestions (open-book) - truncated":"0.049",
        "NaturalQuestions (open-book) - # prompt tokens":"1407.178",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.507",
        "QuAC - truncated":"0.06",
        "QuAC - # prompt tokens":"1498.657",
        "QuAC - # output tokens":"99.794",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":524.602,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0
    },
    {
        "Model":"LLaMA (13B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":522.547,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":1439.447,
        "BoolQ - # output tokens":5.0,
        "BoolQ - # trials":1,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.437",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1541.115",
        "NarrativeQA - # output tokens":"100",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":137.383,
        "NaturalQuestions (closed-book) - # output tokens":300.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"3.722",
        "NaturalQuestions (open-book) - truncated":"0.049",
        "NaturalQuestions (open-book) - # prompt tokens":"1407.178",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.507",
        "QuAC - truncated":"0.06",
        "QuAC - # prompt tokens":"1498.657",
        "QuAC - # output tokens":"99.882",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":524.602,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0
    },
    {
        "Model":"LLaMA (30B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":522.547,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":1439.447,
        "BoolQ - # output tokens":5.0,
        "BoolQ - # trials":1,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.437",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1541.115",
        "NarrativeQA - # output tokens":"100",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":137.383,
        "NaturalQuestions (closed-book) - # output tokens":300.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"3.722",
        "NaturalQuestions (open-book) - truncated":"0.049",
        "NaturalQuestions (open-book) - # prompt tokens":"1407.178",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.507",
        "QuAC - truncated":"0.06",
        "QuAC - # prompt tokens":"1498.657",
        "QuAC - # output tokens":"99.987",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":524.602,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0
    },
    {
        "Model":"LLaMA (65B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":522.547,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":1439.447,
        "BoolQ - # output tokens":1.0,
        "BoolQ - # trials":1,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.437",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1541.115",
        "NarrativeQA - # output tokens":"1",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":137.383,
        "NaturalQuestions (closed-book) - # output tokens":1.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"3.722",
        "NaturalQuestions (open-book) - truncated":"0.049",
        "NaturalQuestions (open-book) - # prompt tokens":"1407.178",
        "NaturalQuestions (open-book) - # output tokens":"0.987",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.507",
        "QuAC - truncated":"0.06",
        "QuAC - # prompt tokens":"1498.657",
        "QuAC - # output tokens":"0.997",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":524.602,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0
    },
    {
        "Model":"Llama 2 (7B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":522.547,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":1439.447,
        "BoolQ - # output tokens":1.296,
        "BoolQ - # trials":1,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"4.414",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"3673.268",
        "NarrativeQA - # output tokens":"1",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":137.383,
        "NaturalQuestions (closed-book) - # output tokens":0.998,
        "NaturalQuestions (closed-book) - # trials":1,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.831",
        "NaturalQuestions (open-book) - truncated":"0.026",
        "NaturalQuestions (open-book) - # prompt tokens":"2289.409",
        "NaturalQuestions (open-book) - # output tokens":"0.955",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"3.204",
        "QuAC - truncated":"0",
        "QuAC - # prompt tokens":"3617.038",
        "QuAC - # output tokens":"1",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":524.602,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0
    },
    {
        "Model":"Llama 2 (13B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":522.547,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":1439.447,
        "BoolQ - # output tokens":5.0,
        "BoolQ - # trials":1,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"4.414",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"3673.268",
        "NarrativeQA - # output tokens":"1",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":137.383,
        "NaturalQuestions (closed-book) - # output tokens":1.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.831",
        "NaturalQuestions (open-book) - truncated":"0.026",
        "NaturalQuestions (open-book) - # prompt tokens":"2289.409",
        "NaturalQuestions (open-book) - # output tokens":"0.984",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"3.204",
        "QuAC - truncated":"0",
        "QuAC - # prompt tokens":"3617.038",
        "QuAC - # output tokens":"1",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":524.602,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0
    },
    {
        "Model":"Llama 2 (70B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":522.547,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":1439.447,
        "BoolQ - # output tokens":1.0,
        "BoolQ - # trials":1,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"4.414",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"3673.268",
        "NarrativeQA - # output tokens":"1",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":137.383,
        "NaturalQuestions (closed-book) - # output tokens":1.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.831",
        "NaturalQuestions (open-book) - truncated":"0.026",
        "NaturalQuestions (open-book) - # prompt tokens":"2289.409",
        "NaturalQuestions (open-book) - # output tokens":"0.998",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"3.204",
        "QuAC - truncated":"0",
        "QuAC - # prompt tokens":"3617.038",
        "QuAC - # output tokens":"1",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":524.602,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0
    },
    {
        "Model":"Alpaca (7B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":522.547,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":1439.447,
        "BoolQ - # output tokens":4.883,
        "BoolQ - # trials":1,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.437",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1541.115",
        "NarrativeQA - # output tokens":"26.006",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":137.383,
        "NaturalQuestions (closed-book) - # output tokens":84.53,
        "NaturalQuestions (closed-book) - # trials":1,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"3.722",
        "NaturalQuestions (open-book) - truncated":"0.049",
        "NaturalQuestions (open-book) - # prompt tokens":"1407.178",
        "NaturalQuestions (open-book) - # output tokens":"122.525",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.507",
        "QuAC - truncated":"0.06",
        "QuAC - # prompt tokens":"1498.657",
        "QuAC - # output tokens":"77.323",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":524.602,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0
    },
    {
        "Model":"Vicuna v1.3 (7B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":522.547,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":1439.447,
        "BoolQ - # output tokens":4.412,
        "BoolQ - # trials":1,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.437",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1541.115",
        "NarrativeQA - # output tokens":"19.287",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":137.383,
        "NaturalQuestions (closed-book) - # output tokens":296.95,
        "NaturalQuestions (closed-book) - # trials":1,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"3.722",
        "NaturalQuestions (open-book) - truncated":"0.049",
        "NaturalQuestions (open-book) - # prompt tokens":"1407.178",
        "NaturalQuestions (open-book) - # output tokens":"286.175",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.507",
        "QuAC - truncated":"0.06",
        "QuAC - # prompt tokens":"1498.657",
        "QuAC - # output tokens":"77.25",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":524.602,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0
    },
    {
        "Model":"Vicuna v1.3 (13B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":522.547,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":1439.447,
        "BoolQ - # output tokens":4.996,
        "BoolQ - # trials":1,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.437",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1541.115",
        "NarrativeQA - # output tokens":"67.575",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":137.383,
        "NaturalQuestions (closed-book) - # output tokens":299.508,
        "NaturalQuestions (closed-book) - # trials":1,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"3.722",
        "NaturalQuestions (open-book) - truncated":"0.049",
        "NaturalQuestions (open-book) - # prompt tokens":"1407.178",
        "NaturalQuestions (open-book) - # output tokens":"266.895",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.507",
        "QuAC - truncated":"0.06",
        "QuAC - # prompt tokens":"1498.657",
        "QuAC - # output tokens":"77.743",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":524.602,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":0.0,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":1418.259,
        "BoolQ - # output tokens":1.0,
        "BoolQ - # trials":1,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"4.575",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"3627.715",
        "NarrativeQA - # output tokens":"1",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":0.0,
        "NaturalQuestions (closed-book) - # output tokens":1.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.832",
        "NaturalQuestions (open-book) - truncated":"0.026",
        "NaturalQuestions (open-book) - # prompt tokens":"2268.728",
        "NaturalQuestions (open-book) - # output tokens":"0.987",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"3.44",
        "QuAC - truncated":"0",
        "QuAC - # prompt tokens":"3680.143",
        "QuAC - # output tokens":"0.999",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":0.0,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0
    },
    {
        "Model":"TNLG v2 (530B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":908.406,
        "BoolQ - # output tokens":1.0,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.646",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1651.848",
        "NarrativeQA - # output tokens":"5.982",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":4.569,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.691",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1419.328",
        "NaturalQuestions (open-book) - # output tokens":"6.015",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.944",
        "QuAC - truncated":"0.016",
        "QuAC - # prompt tokens":"1644.436",
        "QuAC - # output tokens":"29.956",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":511.121,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0
    },
    {
        "Model":"TNLG v2 (6.7B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":908.406,
        "BoolQ - # output tokens":1.0,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.646",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1651.848",
        "NarrativeQA - # output tokens":"6.499",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":5.6,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.691",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1419.328",
        "NaturalQuestions (open-book) - # output tokens":"8.369",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.944",
        "QuAC - truncated":"0.016",
        "QuAC - # prompt tokens":"1644.436",
        "QuAC - # output tokens":"19.574",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":511.121,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0
    },
    {
        "Model":"davinci (175B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":908.406,
        "BoolQ - # output tokens":1.0,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.647",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1652.377",
        "NarrativeQA - # output tokens":"5.709",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":5.361,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.691",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1419.574",
        "NaturalQuestions (open-book) - # output tokens":"8.992",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.944",
        "QuAC - truncated":"0.016",
        "QuAC - # prompt tokens":"1644.831",
        "QuAC - # output tokens":"29.572",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":511.121,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0
    },
    {
        "Model":"curie (6.7B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":908.406,
        "BoolQ - # output tokens":1.0,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.647",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1652.377",
        "NarrativeQA - # output tokens":"6.607",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":6.313,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.691",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1419.574",
        "NaturalQuestions (open-book) - # output tokens":"12.581",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.944",
        "QuAC - truncated":"0.016",
        "QuAC - # prompt tokens":"1644.831",
        "QuAC - # output tokens":"31.034",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":511.121,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0
    },
    {
        "Model":"babbage (1.3B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":908.406,
        "BoolQ - # output tokens":1.0,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.647",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1652.377",
        "NarrativeQA - # output tokens":"8.835",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":7.258,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.691",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1419.574",
        "NaturalQuestions (open-book) - # output tokens":"18.539",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.944",
        "QuAC - truncated":"0.016",
        "QuAC - # prompt tokens":"1644.831",
        "QuAC - # output tokens":"22.916",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":511.121,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0
    },
    {
        "Model":"ada (350M)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":908.406,
        "BoolQ - # output tokens":1.004,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.647",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1652.377",
        "NarrativeQA - # output tokens":"12.381",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":5.656,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.691",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1419.574",
        "NaturalQuestions (open-book) - # output tokens":"22.436",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.944",
        "QuAC - truncated":"0.016",
        "QuAC - # prompt tokens":"1644.831",
        "QuAC - # output tokens":"22.281",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":511.121,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0
    },
    {
        "Model":"text-davinci-003",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":908.406,
        "BoolQ - # output tokens":1.043,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"4.532",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"3579.093",
        "NarrativeQA - # output tokens":"9.164",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":7.964,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.883",
        "NaturalQuestions (open-book) - truncated":"0.02",
        "NaturalQuestions (open-book) - # prompt tokens":"1520.977",
        "NaturalQuestions (open-book) - # output tokens":"6.937",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"3.438",
        "QuAC - truncated":"0",
        "QuAC - # prompt tokens":"3249.907",
        "QuAC - # output tokens":"27.199",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":511.121,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0
    },
    {
        "Model":"text-davinci-002",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":908.406,
        "BoolQ - # output tokens":1.013,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"4.532",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"3579.093",
        "NarrativeQA - # output tokens":"7.378",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":3.954,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.883",
        "NaturalQuestions (open-book) - truncated":"0.02",
        "NaturalQuestions (open-book) - # prompt tokens":"1520.977",
        "NaturalQuestions (open-book) - # output tokens":"6.652",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"3.438",
        "QuAC - truncated":"0",
        "QuAC - # prompt tokens":"3249.907",
        "QuAC - # output tokens":"20.986",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":511.121,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0
    },
    {
        "Model":"text-curie-001",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":908.406,
        "BoolQ - # output tokens":1.007,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.647",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1652.377",
        "NarrativeQA - # output tokens":"8.971",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":4.641,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.691",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1419.574",
        "NaturalQuestions (open-book) - # output tokens":"6.634",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.944",
        "QuAC - truncated":"0.016",
        "QuAC - # prompt tokens":"1644.831",
        "QuAC - # output tokens":"22.198",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":511.121,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0
    },
    {
        "Model":"text-babbage-001",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":908.406,
        "BoolQ - # output tokens":1.004,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.647",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1652.377",
        "NarrativeQA - # output tokens":"12.829",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":2.016,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.691",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1419.574",
        "NaturalQuestions (open-book) - # output tokens":"7.772",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.944",
        "QuAC - truncated":"0.016",
        "QuAC - # prompt tokens":"1644.831",
        "QuAC - # output tokens":"22.966",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":511.121,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0
    },
    {
        "Model":"text-ada-001",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":908.406,
        "BoolQ - # output tokens":1.003,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.647",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1652.377",
        "NarrativeQA - # output tokens":"10.756",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":1.04,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.691",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1419.574",
        "NaturalQuestions (open-book) - # output tokens":"3.933",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.944",
        "QuAC - truncated":"0.016",
        "QuAC - # prompt tokens":"1644.831",
        "QuAC - # output tokens":"17.274",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":511.121,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0
    },
    {
        "Model":"gpt-3.5-turbo-0301",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":460.72,
        "MMLU - # output tokens":1.012,
        "MMLU - # trials":1,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":1220.329,
        "BoolQ - # output tokens":1.932,
        "BoolQ - # trials":1,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"4.966",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"3443.349",
        "NarrativeQA - # output tokens":"11.186",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.127,
        "NaturalQuestions (closed-book) - # output tokens":16.241,
        "NaturalQuestions (closed-book) - # trials":1,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.887",
        "NaturalQuestions (open-book) - truncated":"0.019",
        "NaturalQuestions (open-book) - # prompt tokens":"1590.821",
        "NaturalQuestions (open-book) - # output tokens":"12.998",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"3.871",
        "QuAC - truncated":"0",
        "QuAC - # prompt tokens":"3461.981",
        "QuAC - # output tokens":"23.136",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":464.434,
        "TruthfulQA - # output tokens":1.047,
        "TruthfulQA - # trials":1.0
    },
    {
        "Model":"gpt-3.5-turbo-0613",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":460.72,
        "MMLU - # output tokens":1.371,
        "MMLU - # trials":1,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":1220.329,
        "BoolQ - # output tokens":1.057,
        "BoolQ - # trials":1,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"4.966",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"3443.349",
        "NarrativeQA - # output tokens":"12.194",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.127,
        "NaturalQuestions (closed-book) - # output tokens":18.876,
        "NaturalQuestions (closed-book) - # trials":1,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.887",
        "NaturalQuestions (open-book) - truncated":"0.019",
        "NaturalQuestions (open-book) - # prompt tokens":"1590.821",
        "NaturalQuestions (open-book) - # output tokens":"11.901",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"3.871",
        "QuAC - truncated":"0",
        "QuAC - # prompt tokens":"3461.981",
        "QuAC - # output tokens":"25.691",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":464.434,
        "TruthfulQA - # output tokens":1.517,
        "TruthfulQA - # trials":1.0
    },
    {
        "Model":"RedPajama-INCITE-Base-v1 (3B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":467.936,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":1251.897,
        "BoolQ - # output tokens":5.0,
        "BoolQ - # trials":1,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.969",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1691.082",
        "NarrativeQA - # output tokens":"100",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":117.299,
        "NaturalQuestions (closed-book) - # output tokens":300.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.704",
        "NaturalQuestions (open-book) - truncated":"0.037",
        "NaturalQuestions (open-book) - # prompt tokens":"1495.552",
        "NaturalQuestions (open-book) - # output tokens":"299.738",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.883",
        "QuAC - truncated":"0.021",
        "QuAC - # prompt tokens":"1655.708",
        "QuAC - # output tokens":"100",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":505.352,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0
    },
    {
        "Model":"RedPajama-INCITE-Instruct-v1 (3B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":467.936,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":1251.897,
        "BoolQ - # output tokens":5.0,
        "BoolQ - # trials":1,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.969",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1691.082",
        "NarrativeQA - # output tokens":"100",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":117.299,
        "NaturalQuestions (closed-book) - # output tokens":300.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.704",
        "NaturalQuestions (open-book) - truncated":"0.037",
        "NaturalQuestions (open-book) - # prompt tokens":"1495.552",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.883",
        "QuAC - truncated":"0.021",
        "QuAC - # prompt tokens":"1655.708",
        "QuAC - # output tokens":"100",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":505.352,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0
    },
    {
        "Model":"RedPajama-INCITE-Base (7B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":467.936,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":1251.897,
        "BoolQ - # output tokens":5.0,
        "BoolQ - # trials":1,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.969",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1691.082",
        "NarrativeQA - # output tokens":"100",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":117.299,
        "NaturalQuestions (closed-book) - # output tokens":300.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.704",
        "NaturalQuestions (open-book) - truncated":"0.037",
        "NaturalQuestions (open-book) - # prompt tokens":"1495.552",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.883",
        "QuAC - truncated":"0.021",
        "QuAC - # prompt tokens":"1655.708",
        "QuAC - # output tokens":"100",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":505.352,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0
    },
    {
        "Model":"RedPajama-INCITE-Instruct (7B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":467.936,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":1251.897,
        "BoolQ - # output tokens":5.0,
        "BoolQ - # trials":1,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.969",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1691.082",
        "NarrativeQA - # output tokens":"100",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":117.299,
        "NaturalQuestions (closed-book) - # output tokens":300.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.704",
        "NaturalQuestions (open-book) - truncated":"0.037",
        "NaturalQuestions (open-book) - # prompt tokens":"1495.552",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.883",
        "QuAC - truncated":"0.021",
        "QuAC - # prompt tokens":"1655.708",
        "QuAC - # output tokens":"100",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":505.352,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0
    },
    {
        "Model":"MPT (30B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":467.936,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":1251.897,
        "BoolQ - # output tokens":1.0,
        "BoolQ - # trials":1,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.969",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1691.082",
        "NarrativeQA - # output tokens":"1",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":117.299,
        "NaturalQuestions (closed-book) - # output tokens":0.999,
        "NaturalQuestions (closed-book) - # trials":1,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.704",
        "NaturalQuestions (open-book) - truncated":"0.037",
        "NaturalQuestions (open-book) - # prompt tokens":"1495.552",
        "NaturalQuestions (open-book) - # output tokens":"0.993",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.883",
        "QuAC - truncated":"0.021",
        "QuAC - # prompt tokens":"1655.708",
        "QuAC - # output tokens":"0.997",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":505.352,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0
    },
    {
        "Model":"MPT-Instruct (30B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":467.936,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":1251.897,
        "BoolQ - # output tokens":1.0,
        "BoolQ - # trials":1,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.969",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1691.082",
        "NarrativeQA - # output tokens":"1",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":117.299,
        "NaturalQuestions (closed-book) - # output tokens":1.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.704",
        "NaturalQuestions (open-book) - truncated":"0.037",
        "NaturalQuestions (open-book) - # prompt tokens":"1495.552",
        "NaturalQuestions (open-book) - # output tokens":"0.994",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.883",
        "QuAC - truncated":"0.021",
        "QuAC - # prompt tokens":"1655.708",
        "QuAC - # output tokens":"0.998",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":505.352,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0
    },
    {
        "Model":"Falcon (7B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":500.12,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":1284.629,
        "BoolQ - # output tokens":1.0,
        "BoolQ - # trials":1,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"2.025",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1694.082",
        "NarrativeQA - # output tokens":"1",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":124.246,
        "NaturalQuestions (closed-book) - # output tokens":1.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.599",
        "NaturalQuestions (open-book) - truncated":"0.039",
        "NaturalQuestions (open-book) - # prompt tokens":"1587.334",
        "NaturalQuestions (open-book) - # output tokens":"0.994",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.862",
        "QuAC - truncated":"0.031",
        "QuAC - # prompt tokens":"1667.28",
        "QuAC - # output tokens":"0.995",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":507.503,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0
    },
    {
        "Model":"Falcon-Instruct (7B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":500.12,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":1284.629,
        "BoolQ - # output tokens":1.0,
        "BoolQ - # trials":1,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"2.025",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1694.082",
        "NarrativeQA - # output tokens":"1",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":124.246,
        "NaturalQuestions (closed-book) - # output tokens":0.999,
        "NaturalQuestions (closed-book) - # trials":1,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.599",
        "NaturalQuestions (open-book) - truncated":"0.039",
        "NaturalQuestions (open-book) - # prompt tokens":"1587.334",
        "NaturalQuestions (open-book) - # output tokens":"0.984",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.862",
        "QuAC - truncated":"0.031",
        "QuAC - # prompt tokens":"1667.28",
        "QuAC - # output tokens":"0.997",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":507.503,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0
    },
    {
        "Model":"Falcon (40B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":500.12,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":1284.629,
        "BoolQ - # output tokens":1.0,
        "BoolQ - # trials":1,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"2.025",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1694.082",
        "NarrativeQA - # output tokens":"1",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":124.246,
        "NaturalQuestions (closed-book) - # output tokens":1.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.599",
        "NaturalQuestions (open-book) - truncated":"0.039",
        "NaturalQuestions (open-book) - # prompt tokens":"1587.334",
        "NaturalQuestions (open-book) - # output tokens":"0.995",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.862",
        "QuAC - truncated":"0.031",
        "QuAC - # prompt tokens":"1667.28",
        "QuAC - # output tokens":"0.999",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":507.503,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0
    },
    {
        "Model":"Falcon-Instruct (40B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":500.12,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":1284.629,
        "BoolQ - # output tokens":1.0,
        "BoolQ - # trials":1,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"2.025",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1694.082",
        "NarrativeQA - # output tokens":"1",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":124.246,
        "NaturalQuestions (closed-book) - # output tokens":0.999,
        "NaturalQuestions (closed-book) - # trials":1,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.599",
        "NaturalQuestions (open-book) - truncated":"0.039",
        "NaturalQuestions (open-book) - # prompt tokens":"1587.334",
        "NaturalQuestions (open-book) - # output tokens":"0.995",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.862",
        "QuAC - truncated":"0.031",
        "QuAC - # prompt tokens":"1667.28",
        "QuAC - # output tokens":"0.999",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":507.503,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0
    },
    {
        "Model":"GLM (130B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":460.637,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":931.424,
        "BoolQ - # output tokens":2.0,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.675",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1658.811",
        "NarrativeQA - # output tokens":"9.939",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":122.991,
        "NaturalQuestions (closed-book) - # output tokens":6.707,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.631",
        "NaturalQuestions (open-book) - truncated":"0.047",
        "NaturalQuestions (open-book) - # prompt tokens":"1502.677",
        "NaturalQuestions (open-book) - # output tokens":"21.064",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.874",
        "QuAC - truncated":"0.134",
        "QuAC - # prompt tokens":"1651.972",
        "QuAC - # output tokens":"73.565",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":3.75,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":389.036,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":2.5
    },
    {
        "Model":"InstructPalmyra (30B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":908.406,
        "BoolQ - # output tokens":1.0,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.646",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1651.848",
        "NarrativeQA - # output tokens":"5.347",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":4.247,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.691",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1419.328",
        "NaturalQuestions (open-book) - # output tokens":"7.657",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.944",
        "QuAC - truncated":"0.016",
        "QuAC - # prompt tokens":"1644.436",
        "QuAC - # output tokens":"22.969",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":511.121,
        "TruthfulQA - # output tokens":0.999,
        "TruthfulQA - # trials":3.0
    },
    {
        "Model":"Palmyra X (43B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":908.406,
        "BoolQ - # output tokens":1.007,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"5",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"3803.911",
        "NarrativeQA - # output tokens":"6.272",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":3.19,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"-",
        "NaturalQuestions (open-book) - # train":"-",
        "NaturalQuestions (open-book) - truncated":"-",
        "NaturalQuestions (open-book) - # prompt tokens":"-",
        "NaturalQuestions (open-book) - # output tokens":"-",
        "NaturalQuestions (open-book) - # trials":"-",
        "QuAC - # eval":"1000",
        "QuAC - # train":"5",
        "QuAC - truncated":"0",
        "QuAC - # prompt tokens":"5199.788",
        "QuAC - # output tokens":"26.581",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":511.121,
        "TruthfulQA - # output tokens":0.949,
        "TruthfulQA - # trials":3.0
    },
    {
        "Model":"YaLM (100B)",
        "Mean win rate":"-",
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":453.383,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BoolQ - # eval":1000,
        "BoolQ - # train":5.0,
        "BoolQ - truncated":0.0,
        "BoolQ - # prompt tokens":899.006,
        "BoolQ - # output tokens":5.0,
        "BoolQ - # trials":3,
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.604",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1644.878",
        "NarrativeQA - # output tokens":"96.018",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":111.534,
        "NaturalQuestions (closed-book) - # output tokens":299.515,
        "NaturalQuestions (closed-book) - # trials":3,
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.702",
        "NaturalQuestions (open-book) - truncated":"0.038",
        "NaturalQuestions (open-book) - # prompt tokens":"1409.24",
        "NaturalQuestions (open-book) - # output tokens":"291.572",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.951",
        "QuAC - truncated":"0.016",
        "QuAC - # prompt tokens":"1646.729",
        "QuAC - # output tokens":"99.146",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":3.75,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":405.414,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":2.5
    }
]