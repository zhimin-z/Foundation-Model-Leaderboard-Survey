[
    {
        "Model":"J1-Jumbo v1 (178B) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"J1-Jumbo v1 (178B) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"J1-Jumbo v1 (178B) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"J1-Jumbo v1 (178B) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":32.0,
        "# trials":1
    },
    {
        "Model":"J1-Jumbo v1 (178B) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"J1-Jumbo v1 (178B) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":64.0,
        "# trials":1
    },
    {
        "Model":"J1-Jumbo v1 (178B) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"J1-Large v1 (7.5B) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"J1-Large v1 (7.5B) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"J1-Large v1 (7.5B) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"J1-Large v1 (7.5B) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":32.0,
        "# trials":1
    },
    {
        "Model":"J1-Large v1 (7.5B) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"J1-Large v1 (7.5B) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":64.0,
        "# trials":1
    },
    {
        "Model":"J1-Large v1 (7.5B) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"J1-Grande v1 (17B) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"J1-Grande v1 (17B) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"J1-Grande v1 (17B) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"J1-Grande v1 (17B) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":32.0,
        "# trials":1
    },
    {
        "Model":"J1-Grande v1 (17B) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"J1-Grande v1 (17B) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":64.0,
        "# trials":1
    },
    {
        "Model":"J1-Grande v1 (17B) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"J1-Grande v2 beta (17B) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"J1-Grande v2 beta (17B) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"J1-Grande v2 beta (17B) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"J1-Grande v2 beta (17B) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":32.0,
        "# trials":1
    },
    {
        "Model":"J1-Grande v2 beta (17B) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"J1-Grande v2 beta (17B) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":64.0,
        "# trials":1
    },
    {
        "Model":"J1-Grande v2 beta (17B) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"Jurassic-2 Jumbo (178B) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Jurassic-2 Jumbo (178B) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"Jurassic-2 Jumbo (178B) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"Jurassic-2 Jumbo (178B) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":32.0,
        "# trials":1
    },
    {
        "Model":"Jurassic-2 Jumbo (178B) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"Jurassic-2 Jumbo (178B) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":64.0,
        "# trials":1
    },
    {
        "Model":"Jurassic-2 Jumbo (178B) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"Jurassic-2 Grande (17B) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Jurassic-2 Grande (17B) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"Jurassic-2 Grande (17B) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"Jurassic-2 Grande (17B) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":32.0,
        "# trials":1
    },
    {
        "Model":"Jurassic-2 Grande (17B) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"Jurassic-2 Grande (17B) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":64.0,
        "# trials":1
    },
    {
        "Model":"Jurassic-2 Grande (17B) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"Jurassic-2 Large (7.5B) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Jurassic-2 Large (7.5B) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"Jurassic-2 Large (7.5B) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"Jurassic-2 Large (7.5B) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":32.0,
        "# trials":1
    },
    {
        "Model":"Jurassic-2 Large (7.5B) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"Jurassic-2 Large (7.5B) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":64.0,
        "# trials":1
    },
    {
        "Model":"Jurassic-2 Large (7.5B) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":15.52,
        "# trials":1
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":29.94,
        "# trials":1
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":58.74,
        "# trials":1
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":7.92,
        "# trials":1
    },
    {
        "Model":"BLOOM (176B) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"BLOOM (176B) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"BLOOM (176B) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"BLOOM (176B) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":32.0,
        "# trials":1
    },
    {
        "Model":"BLOOM (176B) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"BLOOM (176B) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":64.0,
        "# trials":1
    },
    {
        "Model":"BLOOM (176B) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"T0pp (11B) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.42,
        "# prompt tokens":563.42,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"T0pp (11B) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.42,
        "# prompt tokens":563.42,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"T0pp (11B) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.42,
        "# prompt tokens":563.42,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"T0pp (11B) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.42,
        "# prompt tokens":563.42,
        "# output tokens":32.0,
        "# trials":1
    },
    {
        "Model":"T0pp (11B) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.42,
        "# prompt tokens":563.42,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"T0pp (11B) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.42,
        "# prompt tokens":563.42,
        "# output tokens":64.0,
        "# trials":1
    },
    {
        "Model":"T0pp (11B) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.42,
        "# prompt tokens":563.42,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":32.0,
        "# trials":1
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":64.0,
        "# trials":1
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"Cohere large v20220720 (13.1B) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Cohere large v20220720 (13.1B) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"Cohere large v20220720 (13.1B) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"Cohere large v20220720 (13.1B) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":32.0,
        "# trials":1
    },
    {
        "Model":"Cohere large v20220720 (13.1B) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"Cohere large v20220720 (13.1B) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":64.0,
        "# trials":1
    },
    {
        "Model":"Cohere large v20220720 (13.1B) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"Cohere medium v20220720 (6.1B) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Cohere medium v20220720 (6.1B) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"Cohere medium v20220720 (6.1B) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"Cohere medium v20220720 (6.1B) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":32.0,
        "# trials":1
    },
    {
        "Model":"Cohere medium v20220720 (6.1B) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"Cohere medium v20220720 (6.1B) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":64.0,
        "# trials":1
    },
    {
        "Model":"Cohere medium v20220720 (6.1B) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"Cohere small v20220720 (410M) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Cohere small v20220720 (410M) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"Cohere small v20220720 (410M) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"Cohere small v20220720 (410M) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":32.0,
        "# trials":1
    },
    {
        "Model":"Cohere small v20220720 (410M) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"Cohere small v20220720 (410M) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":64.0,
        "# trials":1
    },
    {
        "Model":"Cohere small v20220720 (410M) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":32.0,
        "# trials":1
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":64.0,
        "# trials":1
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"Cohere medium v20221108 (6.1B) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Cohere medium v20221108 (6.1B) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"Cohere medium v20221108 (6.1B) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"Cohere medium v20221108 (6.1B) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":32.0,
        "# trials":1
    },
    {
        "Model":"Cohere medium v20221108 (6.1B) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"Cohere medium v20221108 (6.1B) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":64.0,
        "# trials":1
    },
    {
        "Model":"Cohere medium v20221108 (6.1B) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"Cohere Command beta (6.1B) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Cohere Command beta (6.1B) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":15.86,
        "# trials":1
    },
    {
        "Model":"Cohere Command beta (6.1B) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.98,
        "# trials":1
    },
    {
        "Model":"Cohere Command beta (6.1B) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":31.72,
        "# trials":1
    },
    {
        "Model":"Cohere Command beta (6.1B) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":3.96,
        "# trials":1
    },
    {
        "Model":"Cohere Command beta (6.1B) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":63.48,
        "# trials":1
    },
    {
        "Model":"Cohere Command beta (6.1B) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":7.94,
        "# trials":1
    },
    {
        "Model":"Cohere Command beta (52.4B) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Cohere Command beta (52.4B) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"Cohere Command beta (52.4B) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"Cohere Command beta (52.4B) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":31.9,
        "# trials":1
    },
    {
        "Model":"Cohere Command beta (52.4B) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"Cohere Command beta (52.4B) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":63.98,
        "# trials":1
    },
    {
        "Model":"Cohere Command beta (52.4B) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":7.96,
        "# trials":1
    },
    {
        "Model":"GPT-J (6B) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"GPT-J (6B) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"GPT-J (6B) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"GPT-J (6B) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":32.0,
        "# trials":1
    },
    {
        "Model":"GPT-J (6B) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"GPT-J (6B) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":64.0,
        "# trials":1
    },
    {
        "Model":"GPT-J (6B) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"GPT-NeoX (20B) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"GPT-NeoX (20B) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"GPT-NeoX (20B) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"GPT-NeoX (20B) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":32.0,
        "# trials":1
    },
    {
        "Model":"GPT-NeoX (20B) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"GPT-NeoX (20B) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":64.0,
        "# trials":1
    },
    {
        "Model":"GPT-NeoX (20B) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"T5 (11B) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.52,
        "# prompt tokens":357.62,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"T5 (11B) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.52,
        "# prompt tokens":357.62,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"T5 (11B) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.52,
        "# prompt tokens":357.62,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"T5 (11B) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.52,
        "# prompt tokens":357.62,
        "# output tokens":32.0,
        "# trials":1
    },
    {
        "Model":"T5 (11B) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.52,
        "# prompt tokens":357.62,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"T5 (11B) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.52,
        "# prompt tokens":357.62,
        "# output tokens":64.0,
        "# trials":1
    },
    {
        "Model":"T5 (11B) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.52,
        "# prompt tokens":357.62,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"UL2 (20B) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.6,
        "# prompt tokens":359.4,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"UL2 (20B) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.6,
        "# prompt tokens":359.4,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"UL2 (20B) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.6,
        "# prompt tokens":359.4,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"UL2 (20B) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.6,
        "# prompt tokens":359.4,
        "# output tokens":32.0,
        "# trials":1
    },
    {
        "Model":"UL2 (20B) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.6,
        "# prompt tokens":359.4,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"UL2 (20B) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.6,
        "# prompt tokens":359.4,
        "# output tokens":64.0,
        "# trials":1
    },
    {
        "Model":"UL2 (20B) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.6,
        "# prompt tokens":359.4,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"OPT (175B) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"OPT (175B) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"OPT (175B) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"OPT (175B) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":32.0,
        "# trials":1
    },
    {
        "Model":"OPT (175B) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"OPT (175B) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":64.0,
        "# trials":1
    },
    {
        "Model":"OPT (175B) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"OPT (66B) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"OPT (66B) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"OPT (66B) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"OPT (66B) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":32.0,
        "# trials":1
    },
    {
        "Model":"OPT (66B) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"OPT (66B) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":64.0,
        "# trials":1
    },
    {
        "Model":"OPT (66B) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"TNLG v2 (530B) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"TNLG v2 (530B) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"TNLG v2 (530B) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"TNLG v2 (530B) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":32.0,
        "# trials":1
    },
    {
        "Model":"TNLG v2 (530B) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"TNLG v2 (530B) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":64.0,
        "# trials":1
    },
    {
        "Model":"TNLG v2 (530B) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"TNLG v2 (6.7B) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"TNLG v2 (6.7B) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"TNLG v2 (6.7B) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"TNLG v2 (6.7B) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":32.0,
        "# trials":1
    },
    {
        "Model":"TNLG v2 (6.7B) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"TNLG v2 (6.7B) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":64.0,
        "# trials":1
    },
    {
        "Model":"TNLG v2 (6.7B) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"davinci (175B) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"davinci (175B) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"davinci (175B) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"davinci (175B) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":32.0,
        "# trials":1
    },
    {
        "Model":"davinci (175B) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"davinci (175B) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":64.0,
        "# trials":1
    },
    {
        "Model":"davinci (175B) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"curie (6.7B) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"curie (6.7B) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"curie (6.7B) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"curie (6.7B) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":32.0,
        "# trials":1
    },
    {
        "Model":"curie (6.7B) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"curie (6.7B) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":64.0,
        "# trials":1
    },
    {
        "Model":"curie (6.7B) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"babbage (1.3B) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"babbage (1.3B) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"babbage (1.3B) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"babbage (1.3B) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":32.0,
        "# trials":1
    },
    {
        "Model":"babbage (1.3B) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"babbage (1.3B) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":64.0,
        "# trials":1
    },
    {
        "Model":"babbage (1.3B) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"ada (350M) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"ada (350M) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"ada (350M) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"ada (350M) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":32.0,
        "# trials":1
    },
    {
        "Model":"ada (350M) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"ada (350M) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":64.0,
        "# trials":1
    },
    {
        "Model":"ada (350M) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"text-davinci-003 [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"text-davinci-003 [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"text-davinci-003 [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"text-davinci-003 [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":30.8,
        "# trials":1
    },
    {
        "Model":"text-davinci-003 [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"text-davinci-003 [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":57.18,
        "# trials":1
    },
    {
        "Model":"text-davinci-003 [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"text-davinci-002 [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":0.98,
        "# trials":1
    },
    {
        "Model":"text-davinci-002 [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":15.68,
        "# trials":1
    },
    {
        "Model":"text-davinci-002 [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.96,
        "# trials":1
    },
    {
        "Model":"text-davinci-002 [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":31.08,
        "# trials":1
    },
    {
        "Model":"text-davinci-002 [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":3.92,
        "# trials":1
    },
    {
        "Model":"text-davinci-002 [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":61.8,
        "# trials":1
    },
    {
        "Model":"text-davinci-002 [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":7.84,
        "# trials":1
    },
    {
        "Model":"text-curie-001 [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"text-curie-001 [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":15.86,
        "# trials":1
    },
    {
        "Model":"text-curie-001 [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"text-curie-001 [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":30.76,
        "# trials":1
    },
    {
        "Model":"text-curie-001 [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"text-curie-001 [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":58.18,
        "# trials":1
    },
    {
        "Model":"text-curie-001 [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"text-babbage-001 [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"text-babbage-001 [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":15.66,
        "# trials":1
    },
    {
        "Model":"text-babbage-001 [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"text-babbage-001 [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":30.26,
        "# trials":1
    },
    {
        "Model":"text-babbage-001 [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"text-babbage-001 [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":57.58,
        "# trials":1
    },
    {
        "Model":"text-babbage-001 [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"text-ada-001 [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"text-ada-001 [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":15.98,
        "# trials":1
    },
    {
        "Model":"text-ada-001 [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"text-ada-001 [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":31.34,
        "# trials":1
    },
    {
        "Model":"text-ada-001 [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"text-ada-001 [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":59.0,
        "# trials":1
    },
    {
        "Model":"text-ada-001 [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"code-davinci-002 [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"code-davinci-002 [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"code-davinci-002 [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"code-davinci-002 [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":32.0,
        "# trials":1
    },
    {
        "Model":"code-davinci-002 [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"code-davinci-002 [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":64.0,
        "# trials":1
    },
    {
        "Model":"code-davinci-002 [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"code-cushman-001 (12B) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"code-cushman-001 (12B) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"code-cushman-001 (12B) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"code-cushman-001 (12B) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":32.0,
        "# trials":1
    },
    {
        "Model":"code-cushman-001 (12B) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"code-cushman-001 (12B) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":62.74,
        "# trials":1
    },
    {
        "Model":"code-cushman-001 (12B) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.0,
        "# prompt tokens":665.8,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"GLM (130B) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":1.0,
        "# prompt tokens":665.62,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"GLM (130B) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":1.0,
        "# prompt tokens":665.62,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"GLM (130B) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":1.0,
        "# prompt tokens":665.62,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"GLM (130B) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":1.0,
        "# prompt tokens":665.62,
        "# output tokens":32.0,
        "# trials":1
    },
    {
        "Model":"GLM (130B) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":1.0,
        "# prompt tokens":665.62,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"GLM (130B) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":1.0,
        "# prompt tokens":665.62,
        "# output tokens":64.0,
        "# trials":1
    },
    {
        "Model":"GLM (130B) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":1.0,
        "# prompt tokens":665.62,
        "# output tokens":8.0,
        "# trials":1
    },
    {
        "Model":"YaLM (100B) [max_tokens: 1]",
        "# eval":10,
        "# train":0,
        "truncated":0.26,
        "# prompt tokens":665.54,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"YaLM (100B) [max_tokens: 16]",
        "# eval":10,
        "# train":0,
        "truncated":0.26,
        "# prompt tokens":665.54,
        "# output tokens":16.0,
        "# trials":1
    },
    {
        "Model":"YaLM (100B) [max_tokens: 2]",
        "# eval":10,
        "# train":0,
        "truncated":0.26,
        "# prompt tokens":665.54,
        "# output tokens":2.0,
        "# trials":1
    },
    {
        "Model":"YaLM (100B) [max_tokens: 32]",
        "# eval":10,
        "# train":0,
        "truncated":0.26,
        "# prompt tokens":665.54,
        "# output tokens":32.0,
        "# trials":1
    },
    {
        "Model":"YaLM (100B) [max_tokens: 4]",
        "# eval":10,
        "# train":0,
        "truncated":0.26,
        "# prompt tokens":665.54,
        "# output tokens":4.0,
        "# trials":1
    },
    {
        "Model":"YaLM (100B) [max_tokens: 64]",
        "# eval":10,
        "# train":0,
        "truncated":0.26,
        "# prompt tokens":665.54,
        "# output tokens":64.0,
        "# trials":1
    },
    {
        "Model":"YaLM (100B) [max_tokens: 8]",
        "# eval":10,
        "# train":0,
        "truncated":0.26,
        "# prompt tokens":665.54,
        "# output tokens":8.0,
        "# trials":1
    }
]