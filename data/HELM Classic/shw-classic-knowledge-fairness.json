[
    {
        "Model":"J1-Jumbo v1 (178B)",
        "Mean win rate":"0.423",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.235",
        "HellaSwag - EM (Fairness)":"0.614",
        "OpenbookQA - EM (Fairness)":"0.466",
        "TruthfulQA - EM (Fairness)":"0.156",
        "MMLU - EM (Fairness)":"0.236"
    },
    {
        "Model":"J1-Large v1 (7.5B)",
        "Mean win rate":"0.226",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.146",
        "HellaSwag - EM (Fairness)":"0.528",
        "OpenbookQA - EM (Fairness)":"0.444",
        "TruthfulQA - EM (Fairness)":"0.174",
        "MMLU - EM (Fairness)":"0.204"
    },
    {
        "Model":"J1-Grande v1 (17B)",
        "Mean win rate":"0.405",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.187",
        "HellaSwag - EM (Fairness)":"0.58",
        "OpenbookQA - EM (Fairness)":"0.472",
        "TruthfulQA - EM (Fairness)":"0.163",
        "MMLU - EM (Fairness)":"0.232"
    },
    {
        "Model":"J1-Grande v2 beta (17B)",
        "Mean win rate":"0.695",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.27",
        "HellaSwag - EM (Fairness)":"0.623",
        "OpenbookQA - EM (Fairness)":"0.478",
        "TruthfulQA - EM (Fairness)":"0.242",
        "MMLU - EM (Fairness)":"0.409"
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "Mean win rate":"0.827",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.327",
        "HellaSwag - EM (Fairness)":"0.655",
        "OpenbookQA - EM (Fairness)":"0.488",
        "TruthfulQA - EM (Fairness)":"0.354",
        "MMLU - EM (Fairness)":"0.45"
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "Mean win rate":"0.712",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.283",
        "HellaSwag - EM (Fairness)":"0.632",
        "OpenbookQA - EM (Fairness)":"0.466",
        "TruthfulQA - EM (Fairness)":"0.29",
        "MMLU - EM (Fairness)":"0.433"
    },
    {
        "Model":"Jurassic-2 Large (7.5B)",
        "Mean win rate":"0.461",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.217",
        "HellaSwag - EM (Fairness)":"0.567",
        "OpenbookQA - EM (Fairness)":"0.45",
        "TruthfulQA - EM (Fairness)":"0.196",
        "MMLU - EM (Fairness)":"0.297"
    },
    {
        "Model":"Luminous Base (13B)",
        "Mean win rate":"0.116",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.16",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.125",
        "MMLU - EM (Fairness)":"0.185"
    },
    {
        "Model":"Luminous Extended (30B)",
        "Mean win rate":"0.323",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.214",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.16",
        "MMLU - EM (Fairness)":"0.237"
    },
    {
        "Model":"Luminous Supreme (70B)",
        "Mean win rate":"0.348",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.241",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.132",
        "MMLU - EM (Fairness)":"0.264"
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B)",
        "Mean win rate":"0.768",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.239",
        "HellaSwag - EM (Fairness)":"0.695",
        "OpenbookQA - EM (Fairness)":"0.482",
        "TruthfulQA - EM (Fairness)":"0.3",
        "MMLU - EM (Fairness)":"0.447"
    },
    {
        "Model":"BLOOM (176B)",
        "Mean win rate":"0.509",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.187",
        "HellaSwag - EM (Fairness)":"0.585",
        "OpenbookQA - EM (Fairness)":"0.482",
        "TruthfulQA - EM (Fairness)":"0.186",
        "MMLU - EM (Fairness)":"0.274"
    },
    {
        "Model":"T0pp (11B)",
        "Mean win rate":"0.52",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.028",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.35",
        "MMLU - EM (Fairness)":"0.382"
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B)",
        "Mean win rate":"0.544",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.255",
        "HellaSwag - EM (Fairness)":"0.66",
        "OpenbookQA - EM (Fairness)":"0.47",
        "TruthfulQA - EM (Fairness)":"0.156",
        "MMLU - EM (Fairness)":"0.315"
    },
    {
        "Model":"Cohere large v20220720 (13.1B)",
        "Mean win rate":"0.368",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.178",
        "HellaSwag - EM (Fairness)":"0.575",
        "OpenbookQA - EM (Fairness)":"0.446",
        "TruthfulQA - EM (Fairness)":"0.157",
        "MMLU - EM (Fairness)":"0.281"
    },
    {
        "Model":"Cohere medium v20220720 (6.1B)",
        "Mean win rate":"0.239",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.126",
        "HellaSwag - EM (Fairness)":"0.525",
        "OpenbookQA - EM (Fairness)":"0.42",
        "TruthfulQA - EM (Fairness)":"0.174",
        "MMLU - EM (Fairness)":"0.237"
    },
    {
        "Model":"Cohere small v20220720 (410M)",
        "Mean win rate":"0.189",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.055",
        "HellaSwag - EM (Fairness)":"0.308",
        "OpenbookQA - EM (Fairness)":"0.28",
        "TruthfulQA - EM (Fairness)":"0.203",
        "MMLU - EM (Fairness)":"0.222"
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B)",
        "Mean win rate":"0.614",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.299",
        "HellaSwag - EM (Fairness)":"0.687",
        "OpenbookQA - EM (Fairness)":"0.5",
        "TruthfulQA - EM (Fairness)":"0.12",
        "MMLU - EM (Fairness)":"0.317"
    },
    {
        "Model":"Cohere medium v20221108 (6.1B)",
        "Mean win rate":"0.315",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.149",
        "HellaSwag - EM (Fairness)":"0.567",
        "OpenbookQA - EM (Fairness)":"0.44",
        "TruthfulQA - EM (Fairness)":"0.182",
        "MMLU - EM (Fairness)":"0.22"
    },
    {
        "Model":"Cohere Command beta (6.1B)",
        "Mean win rate":"0.469",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.167",
        "HellaSwag - EM (Fairness)":"0.608",
        "OpenbookQA - EM (Fairness)":"0.468",
        "TruthfulQA - EM (Fairness)":"0.163",
        "MMLU - EM (Fairness)":"0.366"
    },
    {
        "Model":"Cohere Command beta (52.4B)",
        "Mean win rate":"0.808",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.296",
        "HellaSwag - EM (Fairness)":"0.699",
        "OpenbookQA - EM (Fairness)":"0.508",
        "TruthfulQA - EM (Fairness)":"0.222",
        "MMLU - EM (Fairness)":"0.407"
    },
    {
        "Model":"GPT-J (6B)",
        "Mean win rate":"0.198",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.122",
        "HellaSwag - EM (Fairness)":"0.486",
        "OpenbookQA - EM (Fairness)":"0.416",
        "TruthfulQA - EM (Fairness)":"0.18",
        "MMLU - EM (Fairness)":"0.22"
    },
    {
        "Model":"GPT-NeoX (20B)",
        "Mean win rate":"0.274",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.154",
        "HellaSwag - EM (Fairness)":"0.552",
        "OpenbookQA - EM (Fairness)":"0.438",
        "TruthfulQA - EM (Fairness)":"0.179",
        "MMLU - EM (Fairness)":"0.215"
    },
    {
        "Model":"Pythia (6.9B)",
        "Mean win rate":"0.177",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.103",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.18",
        "MMLU - EM (Fairness)":"0.207"
    },
    {
        "Model":"Pythia (12B)",
        "Mean win rate":"0.121",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.131",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.154",
        "MMLU - EM (Fairness)":"0.212"
    },
    {
        "Model":"T5 (11B)",
        "Mean win rate":"0.197",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.159",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.101",
        "MMLU - EM (Fairness)":"0.235"
    },
    {
        "Model":"UL2 (20B)",
        "Mean win rate":"0.313",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.162",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.162",
        "MMLU - EM (Fairness)":"0.273"
    },
    {
        "Model":"OPT (175B)",
        "Mean win rate":"0.668",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.246",
        "HellaSwag - EM (Fairness)":"0.66",
        "OpenbookQA - EM (Fairness)":"0.5",
        "TruthfulQA - EM (Fairness)":"0.203",
        "MMLU - EM (Fairness)":"0.287"
    },
    {
        "Model":"OPT (66B)",
        "Mean win rate":"0.4",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.218",
        "HellaSwag - EM (Fairness)":"0.597",
        "OpenbookQA - EM (Fairness)":"0.454",
        "TruthfulQA - EM (Fairness)":"0.173",
        "MMLU - EM (Fairness)":"0.229"
    },
    {
        "Model":"LLaMA (7B)",
        "Mean win rate":"0.566",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.241",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.219",
        "MMLU - EM (Fairness)":"0.284"
    },
    {
        "Model":"LLaMA (13B)",
        "Mean win rate":"0.712",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.288",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.234",
        "MMLU - EM (Fairness)":"0.385"
    },
    {
        "Model":"LLaMA (30B)",
        "Mean win rate":"0.869",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.356",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.266",
        "MMLU - EM (Fairness)":"0.496"
    },
    {
        "Model":"LLaMA (65B)",
        "Mean win rate":"0.96",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.375",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.42",
        "MMLU - EM (Fairness)":"0.551"
    },
    {
        "Model":"Llama 2 (7B)",
        "Mean win rate":"0.672",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.264",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.223",
        "MMLU - EM (Fairness)":"0.392"
    },
    {
        "Model":"Llama 2 (13B)",
        "Mean win rate":"0.828",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.309",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.274",
        "MMLU - EM (Fairness)":"0.466"
    },
    {
        "Model":"Llama 2 (70B)",
        "Mean win rate":"0.975",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.4",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.434",
        "MMLU - EM (Fairness)":"0.557"
    },
    {
        "Model":"Alpaca (7B)",
        "Mean win rate":"0.53",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.21",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.202",
        "MMLU - EM (Fairness)":"0.346"
    },
    {
        "Model":"Vicuna v1.3 (7B)",
        "Mean win rate":"0.626",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.224",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.235",
        "MMLU - EM (Fairness)":"0.385"
    },
    {
        "Model":"Vicuna v1.3 (13B)",
        "Mean win rate":"0.773",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.266",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.315",
        "MMLU - EM (Fairness)":"0.424"
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "Mean win rate":"0.884",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.3",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.332",
        "MMLU - EM (Fairness)":"0.542"
    },
    {
        "Model":"TNLG v2 (530B)",
        "Mean win rate":"0.779",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.318",
        "HellaSwag - EM (Fairness)":"0.678",
        "OpenbookQA - EM (Fairness)":"0.504",
        "TruthfulQA - EM (Fairness)":"0.197",
        "MMLU - EM (Fairness)":"0.418"
    },
    {
        "Model":"TNLG v2 (6.7B)",
        "Mean win rate":"0.193",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.162",
        "HellaSwag - EM (Fairness)":"0.53",
        "OpenbookQA - EM (Fairness)":"0.412",
        "TruthfulQA - EM (Fairness)":"0.144",
        "MMLU - EM (Fairness)":"0.212"
    },
    {
        "Model":"davinci (175B)",
        "Mean win rate":"0.604",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.276",
        "HellaSwag - EM (Fairness)":"0.641",
        "OpenbookQA - EM (Fairness)":"0.502",
        "TruthfulQA - EM (Fairness)":"0.155",
        "MMLU - EM (Fairness)":"0.38"
    },
    {
        "Model":"curie (6.7B)",
        "Mean win rate":"0.257",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.147",
        "HellaSwag - EM (Fairness)":"0.522",
        "OpenbookQA - EM (Fairness)":"0.43",
        "TruthfulQA - EM (Fairness)":"0.186",
        "MMLU - EM (Fairness)":"0.218"
    },
    {
        "Model":"babbage (1.3B)",
        "Mean win rate":"0.127",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.084",
        "HellaSwag - EM (Fairness)":"0.401",
        "OpenbookQA - EM (Fairness)":"0.326",
        "TruthfulQA - EM (Fairness)":"0.178",
        "MMLU - EM (Fairness)":"0.206"
    },
    {
        "Model":"ada (350M)",
        "Mean win rate":"0.138",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.057",
        "HellaSwag - EM (Fairness)":"0.294",
        "OpenbookQA - EM (Fairness)":"0.318",
        "TruthfulQA - EM (Fairness)":"0.185",
        "MMLU - EM (Fairness)":"0.21"
    },
    {
        "Model":"text-davinci-003",
        "Mean win rate":"0.97",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.356",
        "HellaSwag - EM (Fairness)":"0.729",
        "OpenbookQA - EM (Fairness)":"0.578",
        "TruthfulQA - EM (Fairness)":"0.491",
        "MMLU - EM (Fairness)":"0.537"
    },
    {
        "Model":"text-davinci-002",
        "Mean win rate":"0.942",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.32",
        "HellaSwag - EM (Fairness)":"0.703",
        "OpenbookQA - EM (Fairness)":"0.54",
        "TruthfulQA - EM (Fairness)":"0.515",
        "MMLU - EM (Fairness)":"0.531"
    },
    {
        "Model":"text-curie-001",
        "Mean win rate":"0.382",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.132",
        "HellaSwag - EM (Fairness)":"0.534",
        "OpenbookQA - EM (Fairness)":"0.452",
        "TruthfulQA - EM (Fairness)":"0.239",
        "MMLU - EM (Fairness)":"0.231"
    },
    {
        "Model":"text-babbage-001",
        "Mean win rate":"0.191",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.053",
        "HellaSwag - EM (Fairness)":"0.405",
        "OpenbookQA - EM (Fairness)":"0.386",
        "TruthfulQA - EM (Fairness)":"0.207",
        "MMLU - EM (Fairness)":"0.205"
    },
    {
        "Model":"text-ada-001",
        "Mean win rate":"0.1",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.012",
        "HellaSwag - EM (Fairness)":"0.27",
        "OpenbookQA - EM (Fairness)":"0.266",
        "TruthfulQA - EM (Fairness)":"0.191",
        "MMLU - EM (Fairness)":"0.202"
    },
    {
        "Model":"gpt-3.5-turbo-0301",
        "Mean win rate":"0.929",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.331",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.514",
        "MMLU - EM (Fairness)":"0.53"
    },
    {
        "Model":"gpt-3.5-turbo-0613",
        "Mean win rate":"0.677",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.287",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.255",
        "MMLU - EM (Fairness)":"0.313"
    },
    {
        "Model":"RedPajama-INCITE-Base-v1 (3B)",
        "Mean win rate":"0.409",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.145",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.248",
        "MMLU - EM (Fairness)":"0.232"
    },
    {
        "Model":"RedPajama-INCITE-Instruct-v1 (3B)",
        "Mean win rate":"0.237",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.143",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.179",
        "MMLU - EM (Fairness)":"0.222"
    },
    {
        "Model":"RedPajama-INCITE-Base (7B)",
        "Mean win rate":"0.369",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.193",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.17",
        "MMLU - EM (Fairness)":"0.276"
    },
    {
        "Model":"RedPajama-INCITE-Instruct (7B)",
        "Mean win rate":"0.424",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.164",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.183",
        "MMLU - EM (Fairness)":"0.305"
    },
    {
        "Model":"MPT (30B)",
        "Mean win rate":"0.662",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.287",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.19",
        "MMLU - EM (Fairness)":"0.41"
    },
    {
        "Model":"MPT-Instruct (30B)",
        "Mean win rate":"0.535",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.233",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.18",
        "MMLU - EM (Fairness)":"0.4"
    },
    {
        "Model":"Falcon (7B)",
        "Mean win rate":"0.51",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.233",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.213",
        "MMLU - EM (Fairness)":"0.261"
    },
    {
        "Model":"Falcon-Instruct (7B)",
        "Mean win rate":"0.348",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.148",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.183",
        "MMLU - EM (Fairness)":"0.261"
    },
    {
        "Model":"Falcon (40B)",
        "Mean win rate":"0.874",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.338",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.292",
        "MMLU - EM (Fairness)":"0.48"
    },
    {
        "Model":"Falcon-Instruct (40B)",
        "Mean win rate":"0.864",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.331",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.312",
        "MMLU - EM (Fairness)":"0.466"
    },
    {
        "Model":"GLM (130B)",
        "Mean win rate":"0.389",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.12",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.192",
        "MMLU - EM (Fairness)":"0.315"
    },
    {
        "Model":"InstructPalmyra (30B)",
        "Mean win rate":"0.47",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.276",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.152",
        "MMLU - EM (Fairness)":"0.371"
    },
    {
        "Model":"Palmyra X (43B)",
        "Mean win rate":"0.99",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.362",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.542",
        "MMLU - EM (Fairness)":"0.588"
    },
    {
        "Model":"YaLM (100B)",
        "Mean win rate":"0.313",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.052",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.202",
        "MMLU - EM (Fairness)":"0.243"
    }
]