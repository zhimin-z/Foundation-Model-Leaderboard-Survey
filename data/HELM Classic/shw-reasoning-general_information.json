[
    {
        "Model":"J1-Jumbo v1 (178B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"266.968",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.794",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"317.547",
        "Synthetic reasoning (natural language) - # output tokens":"4.987",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"341.123",
        "bAbI - # output tokens":"2.151",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"206.652",
        "Dyck - # output tokens":"3.423",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"789.727",
        "GSM8K - # output tokens":"158.822",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"359.279",
        "MATH - # output tokens":"4.1",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.134",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1091.485",
        "MATH (chain-of-thoughts) - # output tokens":"159.269",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"905.539",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"439.902",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"297.311",
        "Data imputation - # output tokens":"2.549",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"895.507",
        "Entity matching - # output tokens":"2",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"J1-Large v1 (7.5B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"266.968",
        "Synthetic reasoning (abstract symbols) - # output tokens":"6.13",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"317.547",
        "Synthetic reasoning (natural language) - # output tokens":"5.528",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"341.123",
        "bAbI - # output tokens":"2.113",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"206.652",
        "Dyck - # output tokens":"4.572",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"789.727",
        "GSM8K - # output tokens":"214.018",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"359.279",
        "MATH - # output tokens":"4.707",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.134",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1091.485",
        "MATH (chain-of-thoughts) - # output tokens":"154.34",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"905.539",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"439.902",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"297.311",
        "Data imputation - # output tokens":"2.494",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"895.507",
        "Entity matching - # output tokens":"2",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"J1-Grande v1 (17B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"266.968",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.971",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"317.547",
        "Synthetic reasoning (natural language) - # output tokens":"5.47",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"341.123",
        "bAbI - # output tokens":"2.104",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"206.652",
        "Dyck - # output tokens":"4.041",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"789.727",
        "GSM8K - # output tokens":"152.079",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"359.279",
        "MATH - # output tokens":"5.309",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.134",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1091.485",
        "MATH (chain-of-thoughts) - # output tokens":"145.058",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"905.539",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"439.902",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"297.311",
        "Data imputation - # output tokens":"2.503",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"895.507",
        "Entity matching - # output tokens":"2",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"J1-Grande v2 beta (17B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"266.968",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.945",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"317.547",
        "Synthetic reasoning (natural language) - # output tokens":"5.442",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"341.123",
        "bAbI - # output tokens":"2.108",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"206.652",
        "Dyck - # output tokens":"4.44",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"789.727",
        "GSM8K - # output tokens":"136.418",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"359.279",
        "MATH - # output tokens":"3.572",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.134",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1091.485",
        "MATH (chain-of-thoughts) - # output tokens":"139.029",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"905.539",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"439.902",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"297.311",
        "Data imputation - # output tokens":"2.483",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"895.507",
        "Entity matching - # output tokens":"2",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"266.968",
        "Synthetic reasoning (abstract symbols) - # output tokens":"6.011",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"317.547",
        "Synthetic reasoning (natural language) - # output tokens":"5.461",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"341.123",
        "bAbI - # output tokens":"2.205",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"206.652",
        "Dyck - # output tokens":"3.672",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"789.727",
        "GSM8K - # output tokens":"111.793",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"359.279",
        "MATH - # output tokens":"3.38",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"8",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1327.994",
        "MATH (chain-of-thoughts) - # output tokens":"140.631",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"905.539",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"439.902",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"297.311",
        "Data imputation - # output tokens":"2.378",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"895.507",
        "Entity matching - # output tokens":"2",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"266.968",
        "Synthetic reasoning (abstract symbols) - # output tokens":"6.083",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"317.547",
        "Synthetic reasoning (natural language) - # output tokens":"5.534",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"341.123",
        "bAbI - # output tokens":"2.117",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"206.652",
        "Dyck - # output tokens":"4.753",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"789.727",
        "GSM8K - # output tokens":"126.368",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"359.279",
        "MATH - # output tokens":"3.682",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.134",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1091.485",
        "MATH (chain-of-thoughts) - # output tokens":"147.728",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"905.539",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"439.902",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"297.311",
        "Data imputation - # output tokens":"2.387",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"895.507",
        "Entity matching - # output tokens":"2",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"Jurassic-2 Large (7.5B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"266.968",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.432",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"317.547",
        "Synthetic reasoning (natural language) - # output tokens":"4.462",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"341.123",
        "bAbI - # output tokens":"2.141",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"206.652",
        "Dyck - # output tokens":"4.587",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"789.727",
        "GSM8K - # output tokens":"208.932",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"359.279",
        "MATH - # output tokens":"4.437",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.134",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1091.485",
        "MATH (chain-of-thoughts) - # output tokens":"169.624",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"905.539",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"439.902",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"297.311",
        "Data imputation - # output tokens":"2.563",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"895.507",
        "Entity matching - # output tokens":"2",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"Luminous Base (13B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"328.281",
        "Synthetic reasoning (abstract symbols) - # output tokens":"2.567",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"413.838",
        "Synthetic reasoning (natural language) - # output tokens":"0",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"470.746",
        "bAbI - # output tokens":"1.212",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"166.418",
        "Dyck - # output tokens":"2.511",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"877.454",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"417.252",
        "MATH - # output tokens":"3.103",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.048",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1239.049",
        "MATH (chain-of-thoughts) - # output tokens":"163.088",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1180.557",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"629.734",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"306.525",
        "Data imputation - # output tokens":"1.995",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"889.812",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"Luminous Extended (30B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"328.281",
        "Synthetic reasoning (abstract symbols) - # output tokens":"2.604",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"413.838",
        "Synthetic reasoning (natural language) - # output tokens":"0",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"470.746",
        "bAbI - # output tokens":"1.207",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"166.418",
        "Dyck - # output tokens":"2.721",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"877.454",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"417.252",
        "MATH - # output tokens":"2.621",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.048",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1239.049",
        "MATH (chain-of-thoughts) - # output tokens":"155.914",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1180.557",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"629.734",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"306.525",
        "Data imputation - # output tokens":"1.962",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"889.812",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"Luminous Supreme (70B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"328.281",
        "Synthetic reasoning (abstract symbols) - # output tokens":"2.712",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"413.838",
        "Synthetic reasoning (natural language) - # output tokens":"0",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"470.746",
        "bAbI - # output tokens":"1.218",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"166.418",
        "Dyck - # output tokens":"3.123",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"877.454",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"417.252",
        "MATH - # output tokens":"2.701",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.048",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1239.049",
        "MATH (chain-of-thoughts) - # output tokens":"135.03",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1180.557",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"629.734",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"306.525",
        "Data imputation - # output tokens":"1.923",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"889.812",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.589",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"4.989",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"469.022",
        "bAbI - # output tokens":"1.113",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"2.479",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"98.636",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"2.229",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"8",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1414.164",
        "MATH (chain-of-thoughts) - # output tokens":"69.795",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"2.575",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"BLOOM (176B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"303.916",
        "Synthetic reasoning (abstract symbols) - # output tokens":"11.17",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"378.218",
        "Synthetic reasoning (natural language) - # output tokens":"9.026",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"414.156",
        "bAbI - # output tokens":"5",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"150.706",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"823.745",
        "GSM8K - # output tokens":"363.933",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"350.807",
        "MATH - # output tokens":"4.078",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.453",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1098.617",
        "MATH (chain-of-thoughts) - # output tokens":"145.273",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1156.451",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"620.172",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"274.468",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"850.495",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"T0pp (11B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"342.433",
        "Synthetic reasoning (abstract symbols) - # output tokens":"32.857",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"402.929",
        "Synthetic reasoning (natural language) - # output tokens":"20",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"4.986",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"428.442",
        "bAbI - # output tokens":"5",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"265.076",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"4.927",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"927.104",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"7.956",
        "MATH - truncated":"0.005",
        "MATH - # prompt tokens":"416.359",
        "MATH - # output tokens":"20",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"5.085",
        "MATH (chain-of-thoughts) - truncated":"0.005",
        "MATH (chain-of-thoughts) - # prompt tokens":"836.999",
        "MATH (chain-of-thoughts) - # output tokens":"400",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"3.609",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"926.957",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"689.176",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"339.538",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"4.138",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"848.446",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"325.454",
        "Synthetic reasoning (abstract symbols) - # output tokens":"2.909",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"407.57",
        "Synthetic reasoning (natural language) - # output tokens":"0",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"469.15",
        "bAbI - # output tokens":"1.173",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"165.418",
        "Dyck - # output tokens":"3.857",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"883.757",
        "GSM8K - # output tokens":"154.41",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"407.975",
        "MATH - # output tokens":"4.041",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.016",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1241.182",
        "MATH (chain-of-thoughts) - # output tokens":"152.915",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1195.878",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"629.922",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"312.664",
        "Data imputation - # output tokens":"2.101",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"887.65",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"Cohere large v20220720 (13.1B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"325.454",
        "Synthetic reasoning (abstract symbols) - # output tokens":"2.805",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"407.57",
        "Synthetic reasoning (natural language) - # output tokens":"0",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"469.15",
        "bAbI - # output tokens":"1.385",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"165.418",
        "Dyck - # output tokens":"2.462",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"883.757",
        "GSM8K - # output tokens":"179.471",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"407.975",
        "MATH - # output tokens":"5.165",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.016",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1241.182",
        "MATH (chain-of-thoughts) - # output tokens":"168.911",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1195.878",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"629.922",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"312.664",
        "Data imputation - # output tokens":"2.188",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"887.65",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"Cohere medium v20220720 (6.1B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"325.454",
        "Synthetic reasoning (abstract symbols) - # output tokens":"2.681",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"407.57",
        "Synthetic reasoning (natural language) - # output tokens":"0",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"469.15",
        "bAbI - # output tokens":"1.254",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"165.418",
        "Dyck - # output tokens":"3.447",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"883.757",
        "GSM8K - # output tokens":"164.999",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"407.975",
        "MATH - # output tokens":"5.564",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.016",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1241.182",
        "MATH (chain-of-thoughts) - # output tokens":"185.04",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1195.878",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"629.922",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"312.664",
        "Data imputation - # output tokens":"2.207",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"887.65",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"Cohere small v20220720 (410M)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"325.454",
        "Synthetic reasoning (abstract symbols) - # output tokens":"2.542",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"407.57",
        "Synthetic reasoning (natural language) - # output tokens":"0",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"469.15",
        "bAbI - # output tokens":"1.384",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"165.418",
        "Dyck - # output tokens":"3.203",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"883.757",
        "GSM8K - # output tokens":"241.523",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"407.975",
        "MATH - # output tokens":"6.897",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.016",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1241.182",
        "MATH (chain-of-thoughts) - # output tokens":"194.691",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1195.878",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"629.922",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"312.664",
        "Data imputation - # output tokens":"2.139",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"887.65",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"325.454",
        "Synthetic reasoning (abstract symbols) - # output tokens":"2.769",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"407.57",
        "Synthetic reasoning (natural language) - # output tokens":"0",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"469.15",
        "bAbI - # output tokens":"1.118",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"165.418",
        "Dyck - # output tokens":"3.887",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"883.757",
        "GSM8K - # output tokens":"130.87",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"407.975",
        "MATH - # output tokens":"5.357",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.016",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1241.182",
        "MATH (chain-of-thoughts) - # output tokens":"130.466",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1195.878",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"629.922",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"312.664",
        "Data imputation - # output tokens":"2.111",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"887.65",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"Cohere medium v20221108 (6.1B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"325.454",
        "Synthetic reasoning (abstract symbols) - # output tokens":"2.654",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"407.57",
        "Synthetic reasoning (natural language) - # output tokens":"0",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"469.15",
        "bAbI - # output tokens":"1.234",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"165.418",
        "Dyck - # output tokens":"1.959",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"883.757",
        "GSM8K - # output tokens":"114.619",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"407.975",
        "MATH - # output tokens":"5.529",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.016",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1241.182",
        "MATH (chain-of-thoughts) - # output tokens":"188.589",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1195.878",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"629.922",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"312.664",
        "Data imputation - # output tokens":"2.214",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"887.65",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"Cohere Command beta (6.1B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"325.454",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.508",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"407.57",
        "Synthetic reasoning (natural language) - # output tokens":"5.094",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"469.15",
        "bAbI - # output tokens":"1.336",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"165.418",
        "Dyck - # output tokens":"1.901",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"883.757",
        "GSM8K - # output tokens":"126.028",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"407.975",
        "MATH - # output tokens":"5.373",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.956",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1229.853",
        "MATH (chain-of-thoughts) - # output tokens":"158.213",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1195.878",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"629.922",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"312.664",
        "Data imputation - # output tokens":"2.192",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"887.65",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"Cohere Command beta (52.4B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"325.454",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.527",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"407.57",
        "Synthetic reasoning (natural language) - # output tokens":"5.08",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"469.15",
        "bAbI - # output tokens":"1.155",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"165.418",
        "Dyck - # output tokens":"1.591",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"883.757",
        "GSM8K - # output tokens":"111.456",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"407.975",
        "MATH - # output tokens":"4.715",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.956",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1229.853",
        "MATH (chain-of-thoughts) - # output tokens":"133.908",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1195.878",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"629.922",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"312.664",
        "Data imputation - # output tokens":"2.184",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"887.65",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"GPT-J (6B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"15.544",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"9.611",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"469.022",
        "bAbI - # output tokens":"5",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"367.283",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"2.93",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.131",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1227.862",
        "MATH (chain-of-thoughts) - # output tokens":"147.663",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"GPT-NeoX (20B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"320.738",
        "Synthetic reasoning (abstract symbols) - # output tokens":"16.864",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"403.046",
        "Synthetic reasoning (natural language) - # output tokens":"9.2",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"469.338",
        "bAbI - # output tokens":"5",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"889.249",
        "GSM8K - # output tokens":"386.245",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"387.171",
        "MATH - # output tokens":"3.274",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.312",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1175.984",
        "MATH (chain-of-thoughts) - # output tokens":"136.152",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1202.164",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"610.996",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"314.872",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"906.402",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"Pythia (1B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"-",
        "Synthetic reasoning (abstract symbols) - # train":"-",
        "Synthetic reasoning (abstract symbols) - truncated":"-",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"-",
        "Synthetic reasoning (abstract symbols) - # output tokens":"-",
        "Synthetic reasoning (abstract symbols) - # trials":"-",
        "Synthetic reasoning (natural language) - # eval":"-",
        "Synthetic reasoning (natural language) - # train":"-",
        "Synthetic reasoning (natural language) - truncated":"-",
        "Synthetic reasoning (natural language) - # prompt tokens":"-",
        "Synthetic reasoning (natural language) - # output tokens":"-",
        "Synthetic reasoning (natural language) - # trials":"-",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"434.671",
        "bAbI - # output tokens":"5",
        "bAbI - # trials":"1",
        "Dyck - # eval":"-",
        "Dyck - # train":"-",
        "Dyck - truncated":"-",
        "Dyck - # prompt tokens":"-",
        "Dyck - # output tokens":"-",
        "Dyck - # trials":"-",
        "GSM8K - # eval":"-",
        "GSM8K - # train":"-",
        "GSM8K - truncated":"-",
        "GSM8K - # prompt tokens":"-",
        "GSM8K - # output tokens":"-",
        "GSM8K - # trials":"-",
        "MATH - # eval":"-",
        "MATH - # train":"-",
        "MATH - truncated":"-",
        "MATH - # prompt tokens":"-",
        "MATH - # output tokens":"-",
        "MATH - # trials":"-",
        "MATH (chain-of-thoughts) - # eval":"-",
        "MATH (chain-of-thoughts) - # train":"-",
        "MATH (chain-of-thoughts) - truncated":"-",
        "MATH (chain-of-thoughts) - # prompt tokens":"-",
        "MATH (chain-of-thoughts) - # output tokens":"-",
        "MATH (chain-of-thoughts) - # trials":"-",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"-",
        "LSAT - # train":"-",
        "LSAT - truncated":"-",
        "LSAT - # prompt tokens":"-",
        "LSAT - # output tokens":"-",
        "LSAT - # trials":"-",
        "LegalSupport - # eval":"-",
        "LegalSupport - # train":"-",
        "LegalSupport - truncated":"-",
        "LegalSupport - # prompt tokens":"-",
        "LegalSupport - # output tokens":"-",
        "LegalSupport - # trials":"-",
        "Data imputation - # eval":"-",
        "Data imputation - # train":"-",
        "Data imputation - truncated":"-",
        "Data imputation - # prompt tokens":"-",
        "Data imputation - # output tokens":"-",
        "Data imputation - # trials":"-",
        "Entity matching - # eval":"-",
        "Entity matching - # train":"-",
        "Entity matching - truncated":"-",
        "Entity matching - # prompt tokens":"-",
        "Entity matching - # output tokens":"-",
        "Entity matching - # trials":"-"
    },
    {
        "Model":"Pythia (6.9B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"312.071",
        "Synthetic reasoning (abstract symbols) - # output tokens":"32.857",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"388.046",
        "Synthetic reasoning (natural language) - # output tokens":"20",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"434.671",
        "bAbI - # output tokens":"5",
        "bAbI - # trials":"1",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"939.582",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"387.171",
        "MATH - # output tokens":"3.834",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.984",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1112.644",
        "MATH (chain-of-thoughts) - # output tokens":"133.796",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1176.83",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"597.329",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"291.539",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"942.402",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"1"
    },
    {
        "Model":"Pythia (12B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"312.071",
        "Synthetic reasoning (abstract symbols) - # output tokens":"32.857",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"388.046",
        "Synthetic reasoning (natural language) - # output tokens":"20",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"434.671",
        "bAbI - # output tokens":"5",
        "bAbI - # trials":"1",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"939.582",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"387.171",
        "MATH - # output tokens":"3.048",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.984",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1112.644",
        "MATH (chain-of-thoughts) - # output tokens":"121.888",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1176.83",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"597.329",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"291.539",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"942.402",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"1"
    },
    {
        "Model":"T5 (11B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"341.862",
        "Synthetic reasoning (abstract symbols) - # output tokens":"32.857",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"401.929",
        "Synthetic reasoning (natural language) - # output tokens":"20",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"4.878",
        "bAbI - truncated":"0.007",
        "bAbI - # prompt tokens":"419.224",
        "bAbI - # output tokens":"5",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"265.076",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"1.905",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"460.351",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"7.75",
        "MATH - truncated":"0.005",
        "MATH - # prompt tokens":"403.624",
        "MATH - # output tokens":"20",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"2.51",
        "MATH (chain-of-thoughts) - truncated":"0.017",
        "MATH (chain-of-thoughts) - # prompt tokens":"404.903",
        "MATH (chain-of-thoughts) - # output tokens":"400",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"0.938",
        "LSAT - truncated":"0.064",
        "LSAT - # prompt tokens":"417.646",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"1.323",
        "LegalSupport - truncated":"0.002",
        "LegalSupport - # prompt tokens":"395.955",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"339.538",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"1.66",
        "Entity matching - truncated":"0.132",
        "Entity matching - # prompt tokens":"421.841",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"UL2 (20B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"345.862",
        "Synthetic reasoning (abstract symbols) - # output tokens":"32.857",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"405.929",
        "Synthetic reasoning (natural language) - # output tokens":"20",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"4.873",
        "bAbI - truncated":"0.007",
        "bAbI - # prompt tokens":"422.766",
        "bAbI - # output tokens":"5",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"269.076",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"1.88",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"460.805",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"7.736",
        "MATH - truncated":"0.005",
        "MATH - # prompt tokens":"406.861",
        "MATH - # output tokens":"20",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"2.48",
        "MATH (chain-of-thoughts) - truncated":"0.017",
        "MATH (chain-of-thoughts) - # prompt tokens":"403.768",
        "MATH (chain-of-thoughts) - # output tokens":"400",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"0.93",
        "LSAT - truncated":"0.07",
        "LSAT - # prompt tokens":"420.317",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"1.286",
        "LegalSupport - truncated":"0.002",
        "LegalSupport - # prompt tokens":"392.164",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"343.538",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"1.631",
        "Entity matching - truncated":"0.143",
        "Entity matching - # prompt tokens":"419.916",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"OPT (175B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"11.955",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"9.228",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"469.022",
        "bAbI - # output tokens":"5",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"391.689",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"2.608",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.131",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1227.862",
        "MATH (chain-of-thoughts) - # output tokens":"163.593",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"OPT (66B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"12.484",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"9.026",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"469.022",
        "bAbI - # output tokens":"5",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"2.591",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.131",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1227.862",
        "MATH (chain-of-thoughts) - # output tokens":"157.159",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"LLaMA (7B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"336.901",
        "Synthetic reasoning (abstract symbols) - # output tokens":"32.857",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"410.282",
        "Synthetic reasoning (natural language) - # output tokens":"20",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"475.71",
        "bAbI - # output tokens":"5",
        "bAbI - # trials":"1",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"1207.746",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"442.208",
        "MATH - # output tokens":"4.504",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.897",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1214.707",
        "MATH (chain-of-thoughts) - # output tokens":"400",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1261.191",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"707.947",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"370.63",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1109.316",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"1"
    },
    {
        "Model":"LLaMA (13B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"336.901",
        "Synthetic reasoning (abstract symbols) - # output tokens":"32.857",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"410.282",
        "Synthetic reasoning (natural language) - # output tokens":"20",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"475.71",
        "bAbI - # output tokens":"5",
        "bAbI - # trials":"1",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"1207.746",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"442.208",
        "MATH - # output tokens":"4.385",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.897",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1214.707",
        "MATH (chain-of-thoughts) - # output tokens":"400",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1261.191",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"707.947",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"370.63",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1109.316",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"1"
    },
    {
        "Model":"LLaMA (30B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"336.901",
        "Synthetic reasoning (abstract symbols) - # output tokens":"32.857",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"410.282",
        "Synthetic reasoning (natural language) - # output tokens":"20",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"475.71",
        "bAbI - # output tokens":"5",
        "bAbI - # trials":"1",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"1207.746",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"442.208",
        "MATH - # output tokens":"3.557",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.897",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1214.707",
        "MATH (chain-of-thoughts) - # output tokens":"400",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1261.191",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"707.947",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"370.63",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1109.316",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"1"
    },
    {
        "Model":"LLaMA (65B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"336.901",
        "Synthetic reasoning (abstract symbols) - # output tokens":"1",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"410.282",
        "Synthetic reasoning (natural language) - # output tokens":"1",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"475.71",
        "bAbI - # output tokens":"1",
        "bAbI - # trials":"1",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"0.91",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"1207.746",
        "GSM8K - # output tokens":"1",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"442.208",
        "MATH - # output tokens":"1",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.897",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1214.707",
        "MATH (chain-of-thoughts) - # output tokens":"1",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1261.191",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"707.947",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"370.63",
        "Data imputation - # output tokens":"1",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1109.316",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"1"
    },
    {
        "Model":"Llama 2 (7B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"336.901",
        "Synthetic reasoning (abstract symbols) - # output tokens":"1",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"410.282",
        "Synthetic reasoning (natural language) - # output tokens":"1",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"475.71",
        "bAbI - # output tokens":"5",
        "bAbI - # trials":"1",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"0.994",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"1207.746",
        "GSM8K - # output tokens":"1",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"442.208",
        "MATH - # output tokens":"1",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"8",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1438.636",
        "MATH (chain-of-thoughts) - # output tokens":"1",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1261.191",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"707.947",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"370.63",
        "Data imputation - # output tokens":"1",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1109.316",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"1"
    },
    {
        "Model":"Llama 2 (13B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"336.901",
        "Synthetic reasoning (abstract symbols) - # output tokens":"1",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"410.282",
        "Synthetic reasoning (natural language) - # output tokens":"1",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"475.71",
        "bAbI - # output tokens":"5",
        "bAbI - # trials":"1",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"1",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"1207.746",
        "GSM8K - # output tokens":"1",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"442.208",
        "MATH - # output tokens":"1",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"8",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1438.636",
        "MATH (chain-of-thoughts) - # output tokens":"1",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1261.191",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"707.947",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"370.63",
        "Data imputation - # output tokens":"1",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1109.316",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"1"
    },
    {
        "Model":"Llama 2 (70B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"336.901",
        "Synthetic reasoning (abstract symbols) - # output tokens":"1",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"410.282",
        "Synthetic reasoning (natural language) - # output tokens":"1",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"475.71",
        "bAbI - # output tokens":"1",
        "bAbI - # trials":"1",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"0.964",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"1207.746",
        "GSM8K - # output tokens":"1",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"442.208",
        "MATH - # output tokens":"1",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"8",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1438.636",
        "MATH (chain-of-thoughts) - # output tokens":"1",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1261.191",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"707.947",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"370.63",
        "Data imputation - # output tokens":"1",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1109.316",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"1"
    },
    {
        "Model":"Alpaca (7B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"336.901",
        "Synthetic reasoning (abstract symbols) - # output tokens":"13.598",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"410.282",
        "Synthetic reasoning (natural language) - # output tokens":"11.872",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"475.71",
        "bAbI - # output tokens":"3.264",
        "bAbI - # trials":"1",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"3.34",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"1207.746",
        "GSM8K - # output tokens":"97.861",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"442.208",
        "MATH - # output tokens":"4.532",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.897",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1214.707",
        "MATH (chain-of-thoughts) - # output tokens":"85.93",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1261.191",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"707.947",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"370.63",
        "Data imputation - # output tokens":"4.898",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1109.316",
        "Entity matching - # output tokens":"2.301",
        "Entity matching - # trials":"1"
    },
    {
        "Model":"Vicuna v1.3 (7B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"336.901",
        "Synthetic reasoning (abstract symbols) - # output tokens":"32.705",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"410.282",
        "Synthetic reasoning (natural language) - # output tokens":"19.977",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"475.71",
        "bAbI - # output tokens":"4.997",
        "bAbI - # trials":"1",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"4.194",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"1207.746",
        "GSM8K - # output tokens":"375.967",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"442.208",
        "MATH - # output tokens":"4.396",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.897",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1214.707",
        "MATH (chain-of-thoughts) - # output tokens":"359.987",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1261.191",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"707.947",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"370.63",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1109.316",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"1"
    },
    {
        "Model":"Vicuna v1.3 (13B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"336.901",
        "Synthetic reasoning (abstract symbols) - # output tokens":"32.649",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"410.282",
        "Synthetic reasoning (natural language) - # output tokens":"19.662",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"475.71",
        "bAbI - # output tokens":"4.897",
        "bAbI - # trials":"1",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"4.998",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"1207.746",
        "GSM8K - # output tokens":"395.86",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"442.208",
        "MATH - # output tokens":"5.024",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.897",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1214.707",
        "MATH (chain-of-thoughts) - # output tokens":"340.617",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1261.191",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"707.947",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"370.63",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1109.316",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"1"
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"0",
        "Synthetic reasoning (abstract symbols) - # output tokens":"1",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"0",
        "Synthetic reasoning (natural language) - # output tokens":"1",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"0",
        "bAbI - # output tokens":"1",
        "bAbI - # trials":"1",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"166.418",
        "Dyck - # output tokens":"0.964",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"0",
        "GSM8K - # output tokens":"1",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"0",
        "MATH - # output tokens":"1",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"8",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"0",
        "MATH (chain-of-thoughts) - # output tokens":"1",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"0",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"0",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"370.568",
        "Data imputation - # output tokens":"1",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1086.232",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"1"
    },
    {
        "Model":"TNLG v2 (530B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.194",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"4.536",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"469.022",
        "bAbI - # output tokens":"1.108",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"2.252",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"125.933",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"2.769",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.13",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1227.7",
        "MATH (chain-of-thoughts) - # output tokens":"129.397",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"2.262",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"TNLG v2 (6.7B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.711",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"5.046",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"469.022",
        "bAbI - # output tokens":"1.262",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"3.352",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"170.803",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"3.551",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.13",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1227.7",
        "MATH (chain-of-thoughts) - # output tokens":"164.789",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"2.208",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"davinci (175B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.387",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"5.018",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"469.022",
        "bAbI - # output tokens":"1.134",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"1.946",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"128.738",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"2.633",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.131",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1227.862",
        "MATH (chain-of-thoughts) - # output tokens":"176.063",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"2.21",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"curie (6.7B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.46",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"4.839",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"469.022",
        "bAbI - # output tokens":"1.233",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"1.479",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"176.395",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"3.099",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.131",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1227.862",
        "MATH (chain-of-thoughts) - # output tokens":"179.413",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"2.254",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"babbage (1.3B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"6.288",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"5.328",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"469.022",
        "bAbI - # output tokens":"1.349",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"2.977",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"251.824",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"2.245",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.131",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1227.862",
        "MATH (chain-of-thoughts) - # output tokens":"200.846",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"2.473",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"ada (350M)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"6.232",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"5.725",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"469.022",
        "bAbI - # output tokens":"1.48",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"2.335",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"236.808",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"1.402",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.131",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1227.862",
        "MATH (chain-of-thoughts) - # output tokens":"209.617",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"2.2",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"text-davinci-003",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.607",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"5.01",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"469.022",
        "bAbI - # output tokens":"1.521",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"2.297",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"93.018",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"2.741",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"8",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1414.164",
        "MATH (chain-of-thoughts) - # output tokens":"81.236",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"2.218",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"text-davinci-002",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.662",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"5.191",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"469.022",
        "bAbI - # output tokens":"1.311",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"1.637",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"85.812",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"2.555",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"8",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1414.164",
        "MATH (chain-of-thoughts) - # output tokens":"165.026",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"2.222",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"text-curie-001",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.503",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"5.115",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"469.022",
        "bAbI - # output tokens":"1.286",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"1.436",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"41.515",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"2.988",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.131",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1227.862",
        "MATH (chain-of-thoughts) - # output tokens":"60.835",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"2.237",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"text-babbage-001",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.422",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"5.238",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"469.022",
        "bAbI - # output tokens":"1.605",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"1.472",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"34.591",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"2.215",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.131",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1227.862",
        "MATH (chain-of-thoughts) - # output tokens":"52.3",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"2.033",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"text-ada-001",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.594",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"5.771",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"469.022",
        "bAbI - # output tokens":"1.435",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"1.761",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"57.682",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"1.674",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.131",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1227.862",
        "MATH (chain-of-thoughts) - # output tokens":"88.337",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"1.96",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"code-davinci-002",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"3.221",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"0",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"469.022",
        "bAbI - # output tokens":"2.074",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"3.267",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"113.059",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"3.58",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"8",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1414.164",
        "MATH (chain-of-thoughts) - # output tokens":"100.694",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"164",
        "HumanEval (Code) - # train":"0",
        "HumanEval (Code) - truncated":"0",
        "HumanEval (Code) - # prompt tokens":"170.348",
        "HumanEval (Code) - # output tokens":"74.811",
        "HumanEval (Code) - # trials":"1",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"-",
        "Data imputation - # train":"-",
        "Data imputation - truncated":"-",
        "Data imputation - # prompt tokens":"-",
        "Data imputation - # output tokens":"-",
        "Data imputation - # trials":"-",
        "Entity matching - # eval":"-",
        "Entity matching - # train":"-",
        "Entity matching - truncated":"-",
        "Entity matching - # prompt tokens":"-",
        "Entity matching - # output tokens":"-",
        "Entity matching - # trials":"-"
    },
    {
        "Model":"code-cushman-001 (12B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"3.135",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"0",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"469.022",
        "bAbI - # output tokens":"2.131",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"2.593",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"136.778",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"4.521",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.131",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1227.862",
        "MATH (chain-of-thoughts) - # output tokens":"133.572",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"164",
        "HumanEval (Code) - # train":"0",
        "HumanEval (Code) - truncated":"0",
        "HumanEval (Code) - # prompt tokens":"170.348",
        "HumanEval (Code) - # output tokens":"88.774",
        "HumanEval (Code) - # trials":"1",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"-",
        "Data imputation - # train":"-",
        "Data imputation - truncated":"-",
        "Data imputation - # prompt tokens":"-",
        "Data imputation - # output tokens":"-",
        "Data imputation - # trials":"-",
        "Entity matching - # eval":"-",
        "Entity matching - # train":"-",
        "Entity matching - truncated":"-",
        "Entity matching - # prompt tokens":"-",
        "Entity matching - # output tokens":"-",
        "Entity matching - # trials":"-"
    },
    {
        "Model":"gpt-3.5-turbo-0301",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"279.734",
        "Synthetic reasoning (abstract symbols) - # output tokens":"6.206",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"343.552",
        "Synthetic reasoning (natural language) - # output tokens":"5.691",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"382.233",
        "bAbI - # output tokens":"2.801",
        "bAbI - # trials":"1",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"159.418",
        "Dyck - # output tokens":"5.666",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"959.035",
        "GSM8K - # output tokens":"113.005",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"372.768",
        "MATH - # output tokens":"3.048",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"8",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1262.911",
        "MATH (chain-of-thoughts) - # output tokens":"84.136",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1134.604",
        "LSAT - # output tokens":"1.004",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"579.986",
        "LegalSupport - # output tokens":"1.014",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"277.848",
        "Data imputation - # output tokens":"2.193",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"908.418",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"1"
    },
    {
        "Model":"gpt-3.5-turbo-0613",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"279.734",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.736",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"343.552",
        "Synthetic reasoning (natural language) - # output tokens":"5.056",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"382.233",
        "bAbI - # output tokens":"2.757",
        "bAbI - # trials":"1",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"159.418",
        "Dyck - # output tokens":"6",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"959.035",
        "GSM8K - # output tokens":"114.72",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"372.768",
        "MATH - # output tokens":"3.166",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"8",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1262.911",
        "MATH (chain-of-thoughts) - # output tokens":"85.76",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1134.604",
        "LSAT - # output tokens":"1.443",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"579.986",
        "LegalSupport - # output tokens":"1.254",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"277.848",
        "Data imputation - # output tokens":"2.042",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"908.418",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"1"
    },
    {
        "Model":"RedPajama-INCITE-Base-v1 (3B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"312.071",
        "Synthetic reasoning (abstract symbols) - # output tokens":"32.857",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"388.046",
        "Synthetic reasoning (natural language) - # output tokens":"20",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"434.671",
        "bAbI - # output tokens":"5",
        "bAbI - # trials":"1",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"939.582",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"387.171",
        "MATH - # output tokens":"2.771",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.984",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1112.644",
        "MATH (chain-of-thoughts) - # output tokens":"111.423",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1176.83",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"597.329",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"291.539",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"942.402",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"1"
    },
    {
        "Model":"RedPajama-INCITE-Instruct-v1 (3B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"312.071",
        "Synthetic reasoning (abstract symbols) - # output tokens":"32.857",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"388.046",
        "Synthetic reasoning (natural language) - # output tokens":"20",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"434.671",
        "bAbI - # output tokens":"5",
        "bAbI - # trials":"1",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"939.582",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"387.171",
        "MATH - # output tokens":"2.7",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.984",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1112.644",
        "MATH (chain-of-thoughts) - # output tokens":"101.479",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1176.83",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"597.329",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"291.539",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"942.402",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"1"
    },
    {
        "Model":"RedPajama-INCITE-Base (7B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"312.071",
        "Synthetic reasoning (abstract symbols) - # output tokens":"32.857",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"388.046",
        "Synthetic reasoning (natural language) - # output tokens":"20",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"434.671",
        "bAbI - # output tokens":"5",
        "bAbI - # trials":"1",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"939.582",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"387.171",
        "MATH - # output tokens":"3.257",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.984",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1112.644",
        "MATH (chain-of-thoughts) - # output tokens":"140.064",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1176.83",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"597.329",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"291.539",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"942.402",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"1"
    },
    {
        "Model":"RedPajama-INCITE-Instruct (7B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"312.071",
        "Synthetic reasoning (abstract symbols) - # output tokens":"32.857",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"388.046",
        "Synthetic reasoning (natural language) - # output tokens":"20",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"434.671",
        "bAbI - # output tokens":"5",
        "bAbI - # trials":"1",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"939.582",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"387.171",
        "MATH - # output tokens":"2.81",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.984",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1112.644",
        "MATH (chain-of-thoughts) - # output tokens":"132.17",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1176.83",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"597.329",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"291.539",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"942.402",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"1"
    },
    {
        "Model":"MPT (30B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"312.071",
        "Synthetic reasoning (abstract symbols) - # output tokens":"1",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"388.046",
        "Synthetic reasoning (natural language) - # output tokens":"1",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"434.671",
        "bAbI - # output tokens":"1",
        "bAbI - # trials":"1",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"0.932",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"939.582",
        "GSM8K - # output tokens":"1",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"387.171",
        "MATH - # output tokens":"1",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.984",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1112.644",
        "MATH (chain-of-thoughts) - # output tokens":"1",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1176.83",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"597.329",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"291.539",
        "Data imputation - # output tokens":"1",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"942.402",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"1"
    },
    {
        "Model":"MPT-Instruct (30B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"312.071",
        "Synthetic reasoning (abstract symbols) - # output tokens":"1",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"388.046",
        "Synthetic reasoning (natural language) - # output tokens":"1",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"434.671",
        "bAbI - # output tokens":"1",
        "bAbI - # trials":"1",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"1",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"939.582",
        "GSM8K - # output tokens":"1",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"387.171",
        "MATH - # output tokens":"1",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.984",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1112.644",
        "MATH (chain-of-thoughts) - # output tokens":"1",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1176.83",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"597.329",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"291.539",
        "Data imputation - # output tokens":"1",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"942.402",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"1"
    },
    {
        "Model":"Falcon (7B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"335.816",
        "Synthetic reasoning (abstract symbols) - # output tokens":"1",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"385.96",
        "Synthetic reasoning (natural language) - # output tokens":"1",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"430.859",
        "bAbI - # output tokens":"1",
        "bAbI - # trials":"1",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"296.836",
        "Dyck - # output tokens":"0.998",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"1056.967",
        "GSM8K - # output tokens":"1",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"429.658",
        "MATH - # output tokens":"1",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.818",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1150.049",
        "MATH (chain-of-thoughts) - # output tokens":"1",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1181.948",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"612.779",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"304.048",
        "Data imputation - # output tokens":"1",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1019.896",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"1"
    },
    {
        "Model":"Falcon-Instruct (7B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"335.816",
        "Synthetic reasoning (abstract symbols) - # output tokens":"1",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"385.96",
        "Synthetic reasoning (natural language) - # output tokens":"1",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"430.859",
        "bAbI - # output tokens":"1",
        "bAbI - # trials":"1",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"296.836",
        "Dyck - # output tokens":"1",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"1056.967",
        "GSM8K - # output tokens":"1",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"429.658",
        "MATH - # output tokens":"1",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.818",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1150.049",
        "MATH (chain-of-thoughts) - # output tokens":"1",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1181.948",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"612.779",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"304.048",
        "Data imputation - # output tokens":"1",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1019.896",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"1"
    },
    {
        "Model":"Falcon (40B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"335.816",
        "Synthetic reasoning (abstract symbols) - # output tokens":"1",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"385.96",
        "Synthetic reasoning (natural language) - # output tokens":"1",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"430.859",
        "bAbI - # output tokens":"1",
        "bAbI - # trials":"1",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"296.836",
        "Dyck - # output tokens":"0.996",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"1056.967",
        "GSM8K - # output tokens":"1",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"429.658",
        "MATH - # output tokens":"1",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.818",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1150.049",
        "MATH (chain-of-thoughts) - # output tokens":"1",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1181.948",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"612.779",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"304.048",
        "Data imputation - # output tokens":"1",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1019.896",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"1"
    },
    {
        "Model":"Falcon-Instruct (40B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"335.816",
        "Synthetic reasoning (abstract symbols) - # output tokens":"1",
        "Synthetic reasoning (abstract symbols) - # trials":"1",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"385.96",
        "Synthetic reasoning (natural language) - # output tokens":"1",
        "Synthetic reasoning (natural language) - # trials":"1",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"430.859",
        "bAbI - # output tokens":"1",
        "bAbI - # trials":"1",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"296.836",
        "Dyck - # output tokens":"1",
        "Dyck - # trials":"1",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"1056.967",
        "GSM8K - # output tokens":"1",
        "GSM8K - # trials":"1",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"429.658",
        "MATH - # output tokens":"1",
        "MATH - # trials":"1",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"6.818",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1150.049",
        "MATH (chain-of-thoughts) - # output tokens":"1",
        "MATH (chain-of-thoughts) - # trials":"1",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1181.948",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"1",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"612.779",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"1",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"304.048",
        "Data imputation - # output tokens":"1",
        "Data imputation - # trials":"1",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1019.896",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"1"
    },
    {
        "Model":"GLM (130B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"296.268",
        "Synthetic reasoning (abstract symbols) - # output tokens":"6.402",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"366.73",
        "Synthetic reasoning (natural language) - # output tokens":"5.859",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"405.316",
        "bAbI - # output tokens":"2.161",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"203.784",
        "Dyck - # output tokens":"4.583",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"1066.292",
        "GSM8K - # output tokens":"151.917",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"405.639",
        "MATH - # output tokens":"5.141",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.263",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1233.025",
        "MATH (chain-of-thoughts) - # output tokens":"129.137",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1156.543",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"609.591",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"368.375",
        "Data imputation - # output tokens":"3.457",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"1041.057",
        "Entity matching - # output tokens":"2",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"InstructPalmyra (30B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.313",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"5.04",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"469.022",
        "bAbI - # output tokens":"1.191",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"1.045",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"76.416",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"2.845",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.13",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1227.7",
        "MATH (chain-of-thoughts) - # output tokens":"95.183",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"2.205",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"Palmyra X (43B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"318.652",
        "Synthetic reasoning (abstract symbols) - # output tokens":"5.711",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"400.15",
        "Synthetic reasoning (natural language) - # output tokens":"5.288",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"469.022",
        "bAbI - # output tokens":"1.479",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"2.711",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"880.202",
        "GSM8K - # output tokens":"90.399",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"402.45",
        "MATH - # output tokens":"2.352",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"8",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1414.164",
        "MATH (chain-of-thoughts) - # output tokens":"78.821",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1190.129",
        "LSAT - # output tokens":"0.991",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"605.753",
        "LegalSupport - # output tokens":"0.996",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"307.878",
        "Data imputation - # output tokens":"2.334",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"890.493",
        "Entity matching - # output tokens":"1",
        "Entity matching - # trials":"3"
    },
    {
        "Model":"YaLM (100B)",
        "Mean win rate":"-",
        "Synthetic reasoning (abstract symbols) - # eval":"515",
        "Synthetic reasoning (abstract symbols) - # train":"3.857",
        "Synthetic reasoning (abstract symbols) - truncated":"0",
        "Synthetic reasoning (abstract symbols) - # prompt tokens":"319.253",
        "Synthetic reasoning (abstract symbols) - # output tokens":"25.463",
        "Synthetic reasoning (abstract symbols) - # trials":"3",
        "Synthetic reasoning (natural language) - # eval":"515",
        "Synthetic reasoning (natural language) - # train":"3",
        "Synthetic reasoning (natural language) - truncated":"0",
        "Synthetic reasoning (natural language) - # prompt tokens":"402.441",
        "Synthetic reasoning (natural language) - # output tokens":"15.611",
        "Synthetic reasoning (natural language) - # trials":"3",
        "bAbI - # eval":"906",
        "bAbI - # train":"5",
        "bAbI - truncated":"0",
        "bAbI - # prompt tokens":"468.988",
        "bAbI - # output tokens":"5",
        "bAbI - # trials":"3",
        "Dyck - # eval":"500",
        "Dyck - # train":"3",
        "Dyck - truncated":"0",
        "Dyck - # prompt tokens":"164.418",
        "Dyck - # output tokens":"5",
        "Dyck - # trials":"3",
        "GSM8K - # eval":"1000",
        "GSM8K - # train":"5",
        "GSM8K - truncated":"0",
        "GSM8K - # prompt tokens":"829.652",
        "GSM8K - # output tokens":"400",
        "GSM8K - # trials":"3",
        "MATH - # eval":"62.429",
        "MATH - # train":"8",
        "MATH - truncated":"0",
        "MATH - # prompt tokens":"360.702",
        "MATH - # output tokens":"19.186",
        "MATH - # trials":"3",
        "MATH (chain-of-thoughts) - # eval":"62.429",
        "MATH (chain-of-thoughts) - # train":"7.513",
        "MATH (chain-of-thoughts) - truncated":"0",
        "MATH (chain-of-thoughts) - # prompt tokens":"1127.115",
        "MATH (chain-of-thoughts) - # output tokens":"398.654",
        "MATH (chain-of-thoughts) - # trials":"3",
        "APPS (Code) - # eval":"-",
        "APPS (Code) - # train":"-",
        "APPS (Code) - truncated":"-",
        "APPS (Code) - # prompt tokens":"-",
        "APPS (Code) - # output tokens":"-",
        "APPS (Code) - # trials":"-",
        "HumanEval (Code) - # eval":"-",
        "HumanEval (Code) - # train":"-",
        "HumanEval (Code) - truncated":"-",
        "HumanEval (Code) - # prompt tokens":"-",
        "HumanEval (Code) - # output tokens":"-",
        "HumanEval (Code) - # trials":"-",
        "LSAT - # eval":"230",
        "LSAT - # train":"5",
        "LSAT - truncated":"0",
        "LSAT - # prompt tokens":"1186.445",
        "LSAT - # output tokens":"1",
        "LSAT - # trials":"3",
        "LegalSupport - # eval":"489",
        "LegalSupport - # train":"3",
        "LegalSupport - truncated":"0",
        "LegalSupport - # prompt tokens":"604.56",
        "LegalSupport - # output tokens":"1",
        "LegalSupport - # trials":"3",
        "Data imputation - # eval":"75.5",
        "Data imputation - # train":"5",
        "Data imputation - truncated":"0",
        "Data imputation - # prompt tokens":"287.091",
        "Data imputation - # output tokens":"5",
        "Data imputation - # trials":"3",
        "Entity matching - # eval":"231",
        "Entity matching - # train":"5",
        "Entity matching - truncated":"0",
        "Entity matching - # prompt tokens":"848.586",
        "Entity matching - # output tokens":"5",
        "Entity matching - # trials":"3"
    }
]