[
    {
        "Model":"J1-Jumbo v1 (178B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":99.377,
        "NaturalQuestions (closed-book) - # output tokens":5.602,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"62.466",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"4.348",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":355.015,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":396.74,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":62.054,
        "WikiFact - # output tokens":18.947,
        "WikiFact - # trials":3
    },
    {
        "Model":"J1-Large v1 (7.5B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":99.377,
        "NaturalQuestions (closed-book) - # output tokens":7.876,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"62.466",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"4.348",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":355.015,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":396.74,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":62.054,
        "WikiFact - # output tokens":18.943,
        "WikiFact - # trials":3
    },
    {
        "Model":"J1-Grande v1 (17B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":99.377,
        "NaturalQuestions (closed-book) - # output tokens":5.971,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"62.466",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"4.348",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":355.015,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":396.74,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":62.054,
        "WikiFact - # output tokens":18.663,
        "WikiFact - # trials":3
    },
    {
        "Model":"J1-Grande v2 beta (17B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":99.377,
        "NaturalQuestions (closed-book) - # output tokens":5.282,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"62.466",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"4.348",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":355.015,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":396.74,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":62.054,
        "WikiFact - # output tokens":18.994,
        "WikiFact - # trials":3
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":99.377,
        "NaturalQuestions (closed-book) - # output tokens":5.365,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"62.466",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"4.348",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":355.015,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":396.74,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":62.054,
        "WikiFact - # output tokens":16.293,
        "WikiFact - # trials":3
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":99.377,
        "NaturalQuestions (closed-book) - # output tokens":6.315,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"62.466",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"4.348",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":355.015,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":396.74,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":62.054,
        "WikiFact - # output tokens":16.119,
        "WikiFact - # trials":3
    },
    {
        "Model":"Jurassic-2 Large (7.5B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":99.377,
        "NaturalQuestions (closed-book) - # output tokens":6.729,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"62.466",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"4.348",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":355.015,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":396.74,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":734.667,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":61.068,
        "WikiFact - # output tokens":16.577,
        "WikiFact - # trials":3
    },
    {
        "Model":"Luminous Base (13B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":111.754,
        "NaturalQuestions (closed-book) - # output tokens":5.287,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":514.073,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":471.075,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":80.518,
        "WikiFact - # output tokens":16.071,
        "WikiFact - # trials":3
    },
    {
        "Model":"Luminous Extended (30B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":111.754,
        "NaturalQuestions (closed-book) - # output tokens":6.119,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":514.073,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":471.075,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":80.518,
        "WikiFact - # output tokens":15.326,
        "WikiFact - # trials":3
    },
    {
        "Model":"Luminous Supreme (70B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":111.754,
        "NaturalQuestions (closed-book) - # output tokens":4.508,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":514.073,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":471.075,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":80.518,
        "WikiFact - # output tokens":15.34,
        "WikiFact - # trials":3
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":5.47,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"1.306",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0.132",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":3.75,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":404.621,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":2.5,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":83.576,
        "WikiFact - # output tokens":16.654,
        "WikiFact - # trials":3
    },
    {
        "Model":"BLOOM (176B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":96.12,
        "NaturalQuestions (closed-book) - # output tokens":48.109,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.875",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.444",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":3.75,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":370.611,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":2.5,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":436.99,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":78.129,
        "WikiFact - # output tokens":39.917,
        "WikiFact - # trials":3
    },
    {
        "Model":"T0pp (11B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":113.556,
        "NaturalQuestions (closed-book) - # output tokens":300.0,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":3.75,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":391.646,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":2.5,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":492.01,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":90.708,
        "WikiFact - # output tokens":40.0,
        "WikiFact - # trials":3
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":111.191,
        "NaturalQuestions (closed-book) - # output tokens":5.844,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":514.648,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":481.26,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":84.338,
        "WikiFact - # output tokens":16.567,
        "WikiFact - # trials":3
    },
    {
        "Model":"Cohere large v20220720 (13.1B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":111.191,
        "NaturalQuestions (closed-book) - # output tokens":5.625,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":514.648,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":481.26,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":84.338,
        "WikiFact - # output tokens":16.011,
        "WikiFact - # trials":3
    },
    {
        "Model":"Cohere medium v20220720 (6.1B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":111.191,
        "NaturalQuestions (closed-book) - # output tokens":5.267,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":514.648,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":481.26,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":84.338,
        "WikiFact - # output tokens":16.923,
        "WikiFact - # trials":3
    },
    {
        "Model":"Cohere small v20220720 (410M)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":111.191,
        "NaturalQuestions (closed-book) - # output tokens":5.149,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":514.648,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":481.26,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":84.338,
        "WikiFact - # output tokens":16.088,
        "WikiFact - # trials":3
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":111.191,
        "NaturalQuestions (closed-book) - # output tokens":4.808,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":514.648,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":481.26,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":84.338,
        "WikiFact - # output tokens":17.275,
        "WikiFact - # trials":3
    },
    {
        "Model":"Cohere medium v20221108 (6.1B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":111.191,
        "NaturalQuestions (closed-book) - # output tokens":6.745,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":514.648,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":481.26,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":84.338,
        "WikiFact - # output tokens":16.617,
        "WikiFact - # trials":3
    },
    {
        "Model":"Cohere Command beta (6.1B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":111.191,
        "NaturalQuestions (closed-book) - # output tokens":4.687,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"1",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"1",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":514.648,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":481.26,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":84.338,
        "WikiFact - # output tokens":15.307,
        "WikiFact - # trials":3
    },
    {
        "Model":"Cohere Command beta (52.4B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":111.191,
        "NaturalQuestions (closed-book) - # output tokens":4.325,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"1",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"1",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":514.648,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":481.26,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":84.338,
        "WikiFact - # output tokens":16.846,
        "WikiFact - # trials":3
    },
    {
        "Model":"GPT-J (6B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":282.837,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":3.75,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":404.621,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":2.5,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":83.576,
        "WikiFact - # output tokens":40.0,
        "WikiFact - # trials":3
    },
    {
        "Model":"GPT-NeoX (20B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.966,
        "NaturalQuestions (closed-book) - # output tokens":90.195,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.806",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.346",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":3.75,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":406.102,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":2.5,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":467.936,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":83.942,
        "WikiFact - # output tokens":40.0,
        "WikiFact - # trials":3
    },
    {
        "Model":"Pythia (6.9B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":117.299,
        "NaturalQuestions (closed-book) - # output tokens":300.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":505.352,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":467.936,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":83.942,
        "WikiFact - # output tokens":4.71,
        "WikiFact - # trials":1
    },
    {
        "Model":"Pythia (12B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":117.299,
        "NaturalQuestions (closed-book) - # output tokens":300.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":505.352,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":467.936,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":83.942,
        "WikiFact - # output tokens":4.212,
        "WikiFact - # trials":1
    },
    {
        "Model":"T5 (11B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":113.556,
        "NaturalQuestions (closed-book) - # output tokens":300.0,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":3.547,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":371.92,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":2.5,
        "MMLU - # eval":102.8,
        "MMLU - # train":4.326,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":420.562,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":90.708,
        "WikiFact - # output tokens":40.0,
        "WikiFact - # trials":3
    },
    {
        "Model":"UL2 (20B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":117.556,
        "NaturalQuestions (closed-book) - # output tokens":300.0,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":3.513,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":372.668,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":2.5,
        "MMLU - # eval":102.8,
        "MMLU - # train":4.316,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":423.395,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":94.708,
        "WikiFact - # output tokens":40.0,
        "WikiFact - # trials":3
    },
    {
        "Model":"OPT (175B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":278.02,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":3.75,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":404.621,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":2.5,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":83.576,
        "WikiFact - # output tokens":39.844,
        "WikiFact - # trials":3
    },
    {
        "Model":"OPT (66B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":153.231,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0.2",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"1",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":3.75,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":404.621,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":2.5,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":83.576,
        "WikiFact - # output tokens":39.838,
        "WikiFact - # trials":3
    },
    {
        "Model":"LLaMA (7B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":137.383,
        "NaturalQuestions (closed-book) - # output tokens":300.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":524.602,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":522.547,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":97.741,
        "WikiFact - # output tokens":4.806,
        "WikiFact - # trials":1
    },
    {
        "Model":"LLaMA (13B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":137.383,
        "NaturalQuestions (closed-book) - # output tokens":300.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":524.602,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":522.547,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":97.741,
        "WikiFact - # output tokens":4.762,
        "WikiFact - # trials":1
    },
    {
        "Model":"LLaMA (30B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":137.383,
        "NaturalQuestions (closed-book) - # output tokens":300.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":524.602,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":522.547,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":97.741,
        "WikiFact - # output tokens":4.721,
        "WikiFact - # trials":1
    },
    {
        "Model":"LLaMA (65B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":137.383,
        "NaturalQuestions (closed-book) - # output tokens":1.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":524.602,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":522.547,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":97.741,
        "WikiFact - # output tokens":1.0,
        "WikiFact - # trials":1
    },
    {
        "Model":"Llama 2 (7B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":137.383,
        "NaturalQuestions (closed-book) - # output tokens":0.998,
        "NaturalQuestions (closed-book) - # trials":1,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":524.602,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":522.547,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":97.741,
        "WikiFact - # output tokens":1.0,
        "WikiFact - # trials":1
    },
    {
        "Model":"Llama 2 (13B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":137.383,
        "NaturalQuestions (closed-book) - # output tokens":1.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":524.602,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":522.547,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":97.741,
        "WikiFact - # output tokens":1.0,
        "WikiFact - # trials":1
    },
    {
        "Model":"Llama 2 (70B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":137.383,
        "NaturalQuestions (closed-book) - # output tokens":1.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":524.602,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":522.547,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":97.741,
        "WikiFact - # output tokens":1.0,
        "WikiFact - # trials":1
    },
    {
        "Model":"Alpaca (7B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":137.383,
        "NaturalQuestions (closed-book) - # output tokens":84.53,
        "NaturalQuestions (closed-book) - # trials":1,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":524.602,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":522.547,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":97.741,
        "WikiFact - # output tokens":4.937,
        "WikiFact - # trials":1
    },
    {
        "Model":"Vicuna v1.3 (7B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":137.383,
        "NaturalQuestions (closed-book) - # output tokens":296.95,
        "NaturalQuestions (closed-book) - # trials":1,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":524.602,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":522.547,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":97.741,
        "WikiFact - # output tokens":4.902,
        "WikiFact - # trials":1
    },
    {
        "Model":"Vicuna v1.3 (13B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":137.383,
        "NaturalQuestions (closed-book) - # output tokens":299.508,
        "NaturalQuestions (closed-book) - # trials":1,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":524.602,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":522.547,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":97.741,
        "WikiFact - # output tokens":4.765,
        "WikiFact - # trials":1
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":0.0,
        "NaturalQuestions (closed-book) - # output tokens":1.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":0.0,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":0.0,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":0.0,
        "WikiFact - # output tokens":1.0,
        "WikiFact - # trials":1
    },
    {
        "Model":"TNLG v2 (530B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":4.569,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":511.121,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":83.576,
        "WikiFact - # output tokens":17.836,
        "WikiFact - # trials":3
    },
    {
        "Model":"TNLG v2 (6.7B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":5.6,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":511.121,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":83.576,
        "WikiFact - # output tokens":18.298,
        "WikiFact - # trials":3
    },
    {
        "Model":"davinci (175B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":5.361,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":511.121,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":83.576,
        "WikiFact - # output tokens":18.065,
        "WikiFact - # trials":3
    },
    {
        "Model":"curie (6.7B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":6.313,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":511.121,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":83.576,
        "WikiFact - # output tokens":18.112,
        "WikiFact - # trials":3
    },
    {
        "Model":"babbage (1.3B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":7.258,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":511.121,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":83.576,
        "WikiFact - # output tokens":18.315,
        "WikiFact - # trials":3
    },
    {
        "Model":"ada (350M)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":5.656,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":511.121,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":83.576,
        "WikiFact - # output tokens":18.3,
        "WikiFact - # trials":3
    },
    {
        "Model":"text-davinci-003",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":7.964,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":511.121,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":83.576,
        "WikiFact - # output tokens":19.49,
        "WikiFact - # trials":3
    },
    {
        "Model":"text-davinci-002",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":3.954,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":511.121,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":83.576,
        "WikiFact - # output tokens":16.338,
        "WikiFact - # trials":3
    },
    {
        "Model":"text-curie-001",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":4.641,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":511.121,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":83.576,
        "WikiFact - # output tokens":18.325,
        "WikiFact - # trials":3
    },
    {
        "Model":"text-babbage-001",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":2.016,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":511.121,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":83.576,
        "WikiFact - # output tokens":19.219,
        "WikiFact - # trials":3
    },
    {
        "Model":"text-ada-001",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":1.04,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":511.121,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":83.576,
        "WikiFact - # output tokens":17.242,
        "WikiFact - # trials":3
    },
    {
        "Model":"gpt-3.5-turbo-0301",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.127,
        "NaturalQuestions (closed-book) - # output tokens":16.241,
        "NaturalQuestions (closed-book) - # trials":1,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":464.434,
        "TruthfulQA - # output tokens":1.047,
        "TruthfulQA - # trials":1.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":460.72,
        "MMLU - # output tokens":1.012,
        "MMLU - # trials":1,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":79.057,
        "WikiFact - # output tokens":27.193,
        "WikiFact - # trials":1
    },
    {
        "Model":"gpt-3.5-turbo-0613",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.127,
        "NaturalQuestions (closed-book) - # output tokens":18.876,
        "NaturalQuestions (closed-book) - # trials":1,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":464.434,
        "TruthfulQA - # output tokens":1.517,
        "TruthfulQA - # trials":1.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":460.72,
        "MMLU - # output tokens":1.371,
        "MMLU - # trials":1,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":79.057,
        "WikiFact - # output tokens":23.879,
        "WikiFact - # trials":1
    },
    {
        "Model":"RedPajama-INCITE-Base-v1 (3B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":117.299,
        "NaturalQuestions (closed-book) - # output tokens":300.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":505.352,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":467.936,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":83.942,
        "WikiFact - # output tokens":4.362,
        "WikiFact - # trials":1
    },
    {
        "Model":"RedPajama-INCITE-Instruct-v1 (3B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":117.299,
        "NaturalQuestions (closed-book) - # output tokens":300.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":505.352,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":467.936,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":83.942,
        "WikiFact - # output tokens":4.264,
        "WikiFact - # trials":1
    },
    {
        "Model":"RedPajama-INCITE-Base (7B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":117.299,
        "NaturalQuestions (closed-book) - # output tokens":300.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":505.352,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":467.936,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":83.942,
        "WikiFact - # output tokens":4.149,
        "WikiFact - # trials":1
    },
    {
        "Model":"RedPajama-INCITE-Instruct (7B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":117.299,
        "NaturalQuestions (closed-book) - # output tokens":300.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":505.352,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":467.936,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":83.942,
        "WikiFact - # output tokens":4.018,
        "WikiFact - # trials":1
    },
    {
        "Model":"MPT (30B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":117.299,
        "NaturalQuestions (closed-book) - # output tokens":0.999,
        "NaturalQuestions (closed-book) - # trials":1,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":505.352,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":467.936,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":83.942,
        "WikiFact - # output tokens":1.0,
        "WikiFact - # trials":1
    },
    {
        "Model":"MPT-Instruct (30B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":117.299,
        "NaturalQuestions (closed-book) - # output tokens":1.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":505.352,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":467.936,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":83.942,
        "WikiFact - # output tokens":1.0,
        "WikiFact - # trials":1
    },
    {
        "Model":"Falcon (7B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":124.246,
        "NaturalQuestions (closed-book) - # output tokens":1.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":507.503,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":500.12,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":83.776,
        "WikiFact - # output tokens":1.0,
        "WikiFact - # trials":1
    },
    {
        "Model":"Falcon-Instruct (7B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":124.246,
        "NaturalQuestions (closed-book) - # output tokens":0.999,
        "NaturalQuestions (closed-book) - # trials":1,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":507.503,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":500.12,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":83.776,
        "WikiFact - # output tokens":1.0,
        "WikiFact - # trials":1
    },
    {
        "Model":"Falcon (40B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":124.246,
        "NaturalQuestions (closed-book) - # output tokens":1.0,
        "NaturalQuestions (closed-book) - # trials":1,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":507.503,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":500.12,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":83.776,
        "WikiFact - # output tokens":1.0,
        "WikiFact - # trials":1
    },
    {
        "Model":"Falcon-Instruct (40B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":124.246,
        "NaturalQuestions (closed-book) - # output tokens":0.999,
        "NaturalQuestions (closed-book) - # trials":1,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":507.503,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":1.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":500.12,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":1,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":83.776,
        "WikiFact - # output tokens":0.999,
        "WikiFact - # trials":1
    },
    {
        "Model":"GLM (130B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":122.991,
        "NaturalQuestions (closed-book) - # output tokens":6.707,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":3.75,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":389.036,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":2.5,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":460.637,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":85.716,
        "WikiFact - # output tokens":23.531,
        "WikiFact - # trials":3
    },
    {
        "Model":"InstructPalmyra (30B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":4.247,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":511.121,
        "TruthfulQA - # output tokens":0.999,
        "TruthfulQA - # trials":3.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":83.576,
        "WikiFact - # output tokens":16.251,
        "WikiFact - # trials":3
    },
    {
        "Model":"Palmyra X (43B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":112.254,
        "NaturalQuestions (closed-book) - # output tokens":3.19,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5.0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":511.121,
        "TruthfulQA - # output tokens":0.949,
        "TruthfulQA - # trials":3.0,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":83.576,
        "WikiFact - # output tokens":16.098,
        "WikiFact - # trials":3
    },
    {
        "Model":"YaLM (100B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - # eval":1000,
        "NaturalQuestions (closed-book) - # train":5,
        "NaturalQuestions (closed-book) - truncated":0,
        "NaturalQuestions (closed-book) - # prompt tokens":111.534,
        "NaturalQuestions (closed-book) - # output tokens":299.515,
        "NaturalQuestions (closed-book) - # trials":3,
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":3.75,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":405.414,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":2.5,
        "MMLU - # eval":102.8,
        "MMLU - # train":5.0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":453.383,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "WikiFact - # eval":746.2,
        "WikiFact - # train":5,
        "WikiFact - truncated":0,
        "WikiFact - # prompt tokens":80.918,
        "WikiFact - # output tokens":39.875,
        "WikiFact - # trials":3
    }
]