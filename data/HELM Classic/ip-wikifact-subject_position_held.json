[
    {
        "Model":"J1-Jumbo v1 (178B)",
        "EM":0.078,
        "Denoised inference time (s)":"1.092",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":64.58,
        "# output tokens":20.229,
        "# trials":3
    },
    {
        "Model":"J1-Large v1 (7.5B)",
        "EM":0.089,
        "Denoised inference time (s)":"0.551",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":64.58,
        "# output tokens":19.601,
        "# trials":3
    },
    {
        "Model":"J1-Grande v1 (17B)",
        "EM":0.121,
        "Denoised inference time (s)":"0.72",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":64.58,
        "# output tokens":19.443,
        "# trials":3
    },
    {
        "Model":"J1-Grande v2 beta (17B)",
        "EM":0.138,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":64.58,
        "# output tokens":19.853,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "EM":0.149,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":64.58,
        "# output tokens":17.251,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "EM":0.132,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":64.58,
        "# output tokens":17.416,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Large (7.5B)",
        "EM":0.065,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":64.58,
        "# output tokens":19.333,
        "# trials":3
    },
    {
        "Model":"Luminous Base (13B)",
        "EM":0.098,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":82.017,
        "# output tokens":32.841,
        "# trials":3
    },
    {
        "Model":"Luminous Extended (30B)",
        "EM":0.149,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":82.017,
        "# output tokens":30.291,
        "# trials":3
    },
    {
        "Model":"Luminous Supreme (70B)",
        "EM":0.146,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":82.017,
        "# output tokens":31.696,
        "# trials":3
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B)",
        "EM":0.176,
        "Denoised inference time (s)":"2.025",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":86.631,
        "# output tokens":27.682,
        "# trials":3
    },
    {
        "Model":"BLOOM (176B)",
        "EM":0.088,
        "Denoised inference time (s)":"0.814",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":78.115,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"T0pp (11B)",
        "EM":0.004,
        "Denoised inference time (s)":"0.155",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":95.043,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B)",
        "EM":0.142,
        "Denoised inference time (s)":"1.589",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":86.247,
        "# output tokens":31.783,
        "# trials":3
    },
    {
        "Model":"Cohere large v20220720 (13.1B)",
        "EM":0.117,
        "Denoised inference time (s)":"0.925",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":86.247,
        "# output tokens":31.48,
        "# trials":3
    },
    {
        "Model":"Cohere medium v20220720 (6.1B)",
        "EM":0.09,
        "Denoised inference time (s)":"0.581",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":86.247,
        "# output tokens":32.466,
        "# trials":3
    },
    {
        "Model":"Cohere small v20220720 (410M)",
        "EM":0.012,
        "Denoised inference time (s)":"0.398",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":86.247,
        "# output tokens":30.157,
        "# trials":3
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B)",
        "EM":0.128,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":86.247,
        "# output tokens":33.491,
        "# trials":3
    },
    {
        "Model":"Cohere medium v20221108 (6.1B)",
        "EM":0.098,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":86.247,
        "# output tokens":32.856,
        "# trials":3
    },
    {
        "Model":"Cohere Command beta (6.1B)",
        "EM":0.176,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":86.247,
        "# output tokens":27.443,
        "# trials":3
    },
    {
        "Model":"Cohere Command beta (52.4B)",
        "EM":0.17,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":86.247,
        "# output tokens":31.053,
        "# trials":3
    },
    {
        "Model":"GPT-J (6B)",
        "EM":0.075,
        "Denoised inference time (s)":"0.091",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":86.631,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"GPT-NeoX (20B)",
        "EM":0.073,
        "Denoised inference time (s)":"0.048",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":88.713,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"Pythia (6.9B)",
        "EM":0.031,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":89.046,
        "# output tokens":5.761,
        "# trials":1
    },
    {
        "Model":"Pythia (12B)",
        "EM":0.033,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":89.046,
        "# output tokens":3.033,
        "# trials":1
    },
    {
        "Model":"T5 (11B)",
        "EM":0.008,
        "Denoised inference time (s)":"0.252",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":95.043,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"UL2 (20B)",
        "EM":0.07,
        "Denoised inference time (s)":"0.222",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":99.043,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"OPT (175B)",
        "EM":0.09,
        "Denoised inference time (s)":"0.596",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":86.631,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"OPT (66B)",
        "EM":0.094,
        "Denoised inference time (s)":"0.146",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":86.631,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"LLaMA (7B)",
        "EM":0.065,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":85.232,
        "# output tokens":5.139,
        "# trials":1
    },
    {
        "Model":"LLaMA (13B)",
        "EM":0.073,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":85.232,
        "# output tokens":4.181,
        "# trials":1
    },
    {
        "Model":"LLaMA (30B)",
        "EM":0.105,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":85.232,
        "# output tokens":4.478,
        "# trials":1
    },
    {
        "Model":"LLaMA (65B)",
        "EM":0.233,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":85.232,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (7B)",
        "EM":0.169,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":85.232,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (13B)",
        "EM":0.179,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":85.232,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (70B)",
        "EM":0.313,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":85.232,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Alpaca (7B)",
        "EM":0.095,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":85.232,
        "# output tokens":5.342,
        "# trials":1
    },
    {
        "Model":"Vicuna v1.3 (7B)",
        "EM":0.091,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":85.232,
        "# output tokens":5.602,
        "# trials":1
    },
    {
        "Model":"Vicuna v1.3 (13B)",
        "EM":0.087,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":85.232,
        "# output tokens":4.56,
        "# trials":1
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "EM":0.196,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":0.0,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"TNLG v2 (530B)",
        "EM":0.136,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":86.631,
        "# output tokens":23.563,
        "# trials":3
    },
    {
        "Model":"TNLG v2 (6.7B)",
        "EM":0.075,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":86.631,
        "# output tokens":24.083,
        "# trials":3
    },
    {
        "Model":"davinci (175B)",
        "EM":0.151,
        "Denoised inference time (s)":"0.85",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":86.631,
        "# output tokens":22.885,
        "# trials":3
    },
    {
        "Model":"curie (6.7B)",
        "EM":0.085,
        "Denoised inference time (s)":"0.256",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":86.631,
        "# output tokens":25.643,
        "# trials":3
    },
    {
        "Model":"babbage (1.3B)",
        "EM":0.075,
        "Denoised inference time (s)":"0.269",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":86.631,
        "# output tokens":27.007,
        "# trials":3
    },
    {
        "Model":"ada (350M)",
        "EM":0.052,
        "Denoised inference time (s)":"0.274",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":86.631,
        "# output tokens":23.579,
        "# trials":3
    },
    {
        "Model":"text-davinci-003",
        "EM":0.163,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":86.631,
        "# output tokens":25.12,
        "# trials":3
    },
    {
        "Model":"text-davinci-002",
        "EM":0.192,
        "Denoised inference time (s)":"0.665",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":86.631,
        "# output tokens":18.029,
        "# trials":3
    },
    {
        "Model":"text-curie-001",
        "EM":0.087,
        "Denoised inference time (s)":"0.289",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":86.631,
        "# output tokens":24.437,
        "# trials":3
    },
    {
        "Model":"text-babbage-001",
        "EM":0.056,
        "Denoised inference time (s)":"0.32",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":86.631,
        "# output tokens":28.89,
        "# trials":3
    },
    {
        "Model":"text-ada-001",
        "EM":0.04,
        "Denoised inference time (s)":"0.207",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":86.631,
        "# output tokens":22.304,
        "# trials":3
    },
    {
        "Model":"gpt-3.5-turbo-0301",
        "EM":0.086,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":82.754,
        "# output tokens":29.821,
        "# trials":1
    },
    {
        "Model":"gpt-3.5-turbo-0613",
        "EM":0.099,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":82.754,
        "# output tokens":19.272,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Base-v1 (3B)",
        "EM":0.048,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":89.046,
        "# output tokens":4.905,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Instruct-v1 (3B)",
        "EM":0.035,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":89.046,
        "# output tokens":3.171,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Base (7B)",
        "EM":0.044,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":89.046,
        "# output tokens":3.768,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Instruct (7B)",
        "EM":0.039,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":89.046,
        "# output tokens":3.091,
        "# trials":1
    },
    {
        "Model":"MPT (30B)",
        "EM":0.189,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":89.046,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"MPT-Instruct (30B)",
        "EM":0.112,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":89.046,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon (7B)",
        "EM":0.096,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":85.76,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon-Instruct (7B)",
        "EM":0.078,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":85.76,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon (40B)",
        "EM":0.173,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":85.76,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon-Instruct (40B)",
        "EM":0.181,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":85.76,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"GLM (130B)",
        "EM":0.101,
        "Denoised inference time (s)":"0.525",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":84.9,
        "# output tokens":27.965,
        "# trials":3
    },
    {
        "Model":"InstructPalmyra (30B)",
        "EM":0.072,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":86.631,
        "# output tokens":19.993,
        "# trials":3
    },
    {
        "Model":"Palmyra X (43B)",
        "EM":0.192,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":86.631,
        "# output tokens":22.72,
        "# trials":3
    },
    {
        "Model":"YaLM (100B)",
        "EM":0.022,
        "Denoised inference time (s)":"0.34",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":84.488,
        "# output tokens":40.0,
        "# trials":3
    }
]