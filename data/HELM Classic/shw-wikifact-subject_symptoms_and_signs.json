[
    {
        "Model":"J1-Jumbo v1 (178B)",
        "EM":0.324,
        "Denoised inference time (s)":"0.892",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":50.259,
        "# output tokens":15.619,
        "# trials":3
    },
    {
        "Model":"J1-Large v1 (7.5B)",
        "EM":0.314,
        "Denoised inference time (s)":"0.48",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":50.259,
        "# output tokens":15.401,
        "# trials":3
    },
    {
        "Model":"J1-Grande v1 (17B)",
        "EM":0.324,
        "Denoised inference time (s)":"0.615",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":50.259,
        "# output tokens":15.031,
        "# trials":3
    },
    {
        "Model":"J1-Grande v2 beta (17B)",
        "EM":0.345,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":50.259,
        "# output tokens":14.275,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "EM":0.291,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":50.259,
        "# output tokens":12.208,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "EM":0.31,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":50.259,
        "# output tokens":12.207,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Large (7.5B)",
        "EM":0.27,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":50.259,
        "# output tokens":15.717,
        "# trials":3
    },
    {
        "Model":"Luminous Base (13B)",
        "EM":0.361,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":67.984,
        "# output tokens":8.072,
        "# trials":3
    },
    {
        "Model":"Luminous Extended (30B)",
        "EM":0.369,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":67.984,
        "# output tokens":8.673,
        "# trials":3
    },
    {
        "Model":"Luminous Supreme (70B)",
        "EM":0.398,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":67.984,
        "# output tokens":7.876,
        "# trials":3
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B)",
        "EM":0.373,
        "Denoised inference time (s)":"0.946",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":70.09,
        "# output tokens":8.958,
        "# trials":3
    },
    {
        "Model":"BLOOM (176B)",
        "EM":0.286,
        "Denoised inference time (s)":"0.82",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":68.226,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"T0pp (11B)",
        "EM":0.013,
        "Denoised inference time (s)":"0.127",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":78.111,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B)",
        "EM":0.372,
        "Denoised inference time (s)":"0.651",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":71.845,
        "# output tokens":8.533,
        "# trials":3
    },
    {
        "Model":"Cohere large v20220720 (13.1B)",
        "EM":0.342,
        "Denoised inference time (s)":"0.376",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":71.845,
        "# output tokens":7.779,
        "# trials":3
    },
    {
        "Model":"Cohere medium v20220720 (6.1B)",
        "EM":0.328,
        "Denoised inference time (s)":"0.278",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":71.845,
        "# output tokens":7.503,
        "# trials":3
    },
    {
        "Model":"Cohere small v20220720 (410M)",
        "EM":0.246,
        "Denoised inference time (s)":"0.254",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":71.845,
        "# output tokens":6.346,
        "# trials":3
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B)",
        "EM":0.376,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":71.845,
        "# output tokens":8.702,
        "# trials":3
    },
    {
        "Model":"Cohere medium v20221108 (6.1B)",
        "EM":0.317,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":71.845,
        "# output tokens":9.042,
        "# trials":3
    },
    {
        "Model":"Cohere Command beta (6.1B)",
        "EM":0.325,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":71.845,
        "# output tokens":8.292,
        "# trials":3
    },
    {
        "Model":"Cohere Command beta (52.4B)",
        "EM":0.365,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":71.845,
        "# output tokens":8.545,
        "# trials":3
    },
    {
        "Model":"GPT-J (6B)",
        "EM":0.216,
        "Denoised inference time (s)":"0.097",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":70.09,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"GPT-NeoX (20B)",
        "EM":0.224,
        "Denoised inference time (s)":"0.064",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":69.315,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"Pythia (6.9B)",
        "EM":0.219,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":70.648,
        "# output tokens":3.443,
        "# trials":1
    },
    {
        "Model":"Pythia (12B)",
        "EM":0.189,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":70.648,
        "# output tokens":3.049,
        "# trials":1
    },
    {
        "Model":"T5 (11B)",
        "EM":0.077,
        "Denoised inference time (s)":"0.33",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":78.111,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"UL2 (20B)",
        "EM":0.197,
        "Denoised inference time (s)":"0.224",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":82.111,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"OPT (175B)",
        "EM":0.199,
        "Denoised inference time (s)":"0.605",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":70.09,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"OPT (66B)",
        "EM":0.221,
        "Denoised inference time (s)":"0.147",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":70.09,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"LLaMA (7B)",
        "EM":0.239,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":91.327,
        "# output tokens":4.348,
        "# trials":1
    },
    {
        "Model":"LLaMA (13B)",
        "EM":0.235,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":91.327,
        "# output tokens":4.528,
        "# trials":1
    },
    {
        "Model":"LLaMA (30B)",
        "EM":0.249,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":91.327,
        "# output tokens":4.329,
        "# trials":1
    },
    {
        "Model":"LLaMA (65B)",
        "EM":0.431,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":91.327,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (7B)",
        "EM":0.408,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":91.327,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (13B)",
        "EM":0.422,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":91.327,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (70B)",
        "EM":0.451,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":91.327,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Alpaca (7B)",
        "EM":0.244,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":91.327,
        "# output tokens":4.636,
        "# trials":1
    },
    {
        "Model":"Vicuna v1.3 (7B)",
        "EM":0.246,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":91.327,
        "# output tokens":4.468,
        "# trials":1
    },
    {
        "Model":"Vicuna v1.3 (13B)",
        "EM":0.253,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":91.327,
        "# output tokens":4.155,
        "# trials":1
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "EM":0.428,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":0.0,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"TNLG v2 (530B)",
        "EM":0.326,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":70.09,
        "# output tokens":12.815,
        "# trials":3
    },
    {
        "Model":"TNLG v2 (6.7B)",
        "EM":0.303,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":70.09,
        "# output tokens":15.085,
        "# trials":3
    },
    {
        "Model":"davinci (175B)",
        "EM":0.314,
        "Denoised inference time (s)":"0.562",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":70.09,
        "# output tokens":13.363,
        "# trials":3
    },
    {
        "Model":"curie (6.7B)",
        "EM":0.303,
        "Denoised inference time (s)":"0.171",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":70.09,
        "# output tokens":13.559,
        "# trials":3
    },
    {
        "Model":"babbage (1.3B)",
        "EM":0.255,
        "Denoised inference time (s)":"0.203",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":70.09,
        "# output tokens":16.008,
        "# trials":3
    },
    {
        "Model":"ada (350M)",
        "EM":0.205,
        "Denoised inference time (s)":"0.218",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":70.09,
        "# output tokens":14.406,
        "# trials":3
    },
    {
        "Model":"text-davinci-003",
        "EM":0.284,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":70.09,
        "# output tokens":17.446,
        "# trials":3
    },
    {
        "Model":"text-davinci-002",
        "EM":0.376,
        "Denoised inference time (s)":"0.449",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":70.09,
        "# output tokens":10.681,
        "# trials":3
    },
    {
        "Model":"text-curie-001",
        "EM":0.17,
        "Denoised inference time (s)":"0.284",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":70.09,
        "# output tokens":23.926,
        "# trials":3
    },
    {
        "Model":"text-babbage-001",
        "EM":0.132,
        "Denoised inference time (s)":"0.279",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":70.09,
        "# output tokens":23.155,
        "# trials":3
    },
    {
        "Model":"text-ada-001",
        "EM":0.178,
        "Denoised inference time (s)":"0.162",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":70.09,
        "# output tokens":14.866,
        "# trials":3
    },
    {
        "Model":"gpt-3.5-turbo-0301",
        "EM":0.009,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":66.924,
        "# output tokens":39.436,
        "# trials":1
    },
    {
        "Model":"gpt-3.5-turbo-0613",
        "EM":0.032,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":66.924,
        "# output tokens":33.913,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Base-v1 (3B)",
        "EM":0.231,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":70.648,
        "# output tokens":2.996,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Instruct-v1 (3B)",
        "EM":0.226,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":70.648,
        "# output tokens":3.034,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Base (7B)",
        "EM":0.228,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":70.648,
        "# output tokens":2.996,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Instruct (7B)",
        "EM":0.235,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":70.648,
        "# output tokens":3.035,
        "# trials":1
    },
    {
        "Model":"MPT (30B)",
        "EM":0.412,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":70.648,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"MPT-Instruct (30B)",
        "EM":0.364,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":70.648,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon (7B)",
        "EM":0.378,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":69.79,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon-Instruct (7B)",
        "EM":0.208,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":69.79,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon (40B)",
        "EM":0.428,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":69.79,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon-Instruct (40B)",
        "EM":0.387,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":69.79,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"GLM (130B)",
        "EM":0.283,
        "Denoised inference time (s)":"0.516",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":62.769,
        "# output tokens":20.327,
        "# trials":3
    },
    {
        "Model":"InstructPalmyra (30B)",
        "EM":0.18,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":70.09,
        "# output tokens":12.134,
        "# trials":3
    },
    {
        "Model":"Palmyra X (43B)",
        "EM":0.356,
        "Denoised inference time (s)":"-",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":70.09,
        "# output tokens":9.147,
        "# trials":3
    },
    {
        "Model":"YaLM (100B)",
        "EM":0.065,
        "Denoised inference time (s)":"0.338",
        "# eval":566,
        "# train":5,
        "truncated":0,
        "# prompt tokens":65.057,
        "# output tokens":40.0,
        "# trials":3
    }
]