[
    {
        "Model":"J1-Jumbo v1 (178B)",
        "Mean win rate":0.398,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.179,
        "HellaSwag - EM (Robustness)":"0.726",
        "OpenbookQA - EM (Robustness)":"0.43",
        "TruthfulQA - EM (Robustness)":0.154,
        "MMLU - EM (Robustness)":0.221
    },
    {
        "Model":"J1-Large v1 (7.5B)",
        "Mean win rate":0.204,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.098,
        "HellaSwag - EM (Robustness)":"0.646",
        "OpenbookQA - EM (Robustness)":"0.412",
        "TruthfulQA - EM (Robustness)":0.155,
        "MMLU - EM (Robustness)":0.2
    },
    {
        "Model":"J1-Grande v1 (17B)",
        "Mean win rate":0.335,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.17,
        "HellaSwag - EM (Robustness)":"0.695",
        "OpenbookQA - EM (Robustness)":"0.424",
        "TruthfulQA - EM (Robustness)":0.142,
        "MMLU - EM (Robustness)":0.225
    },
    {
        "Model":"J1-Grande v2 beta (17B)",
        "Mean win rate":0.696,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.235,
        "HellaSwag - EM (Robustness)":"0.732",
        "OpenbookQA - EM (Robustness)":"0.474",
        "TruthfulQA - EM (Robustness)":0.252,
        "MMLU - EM (Robustness)":0.392
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "Mean win rate":0.802,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.315,
        "HellaSwag - EM (Robustness)":"0.754",
        "OpenbookQA - EM (Robustness)":"0.47",
        "TruthfulQA - EM (Robustness)":0.39,
        "MMLU - EM (Robustness)":0.417
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "Mean win rate":0.785,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.285,
        "HellaSwag - EM (Robustness)":"0.755",
        "OpenbookQA - EM (Robustness)":"0.474",
        "TruthfulQA - EM (Robustness)":0.293,
        "MMLU - EM (Robustness)":0.411
    },
    {
        "Model":"Jurassic-2 Large (7.5B)",
        "Mean win rate":0.505,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.187,
        "HellaSwag - EM (Robustness)":"0.687",
        "OpenbookQA - EM (Robustness)":"0.448",
        "TruthfulQA - EM (Robustness)":0.21,
        "MMLU - EM (Robustness)":0.263
    },
    {
        "Model":"Luminous Base (13B)",
        "Mean win rate":0.141,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.163,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.112,
        "MMLU - EM (Robustness)":0.183
    },
    {
        "Model":"Luminous Extended (30B)",
        "Mean win rate":0.348,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.212,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.151,
        "MMLU - EM (Robustness)":0.23
    },
    {
        "Model":"Luminous Supreme (70B)",
        "Mean win rate":0.359,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.252,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.106,
        "MMLU - EM (Robustness)":0.255
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B)",
        "Mean win rate":0.783,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.245,
        "HellaSwag - EM (Robustness)":"0.766",
        "OpenbookQA - EM (Robustness)":"0.472",
        "TruthfulQA - EM (Robustness)":0.326,
        "MMLU - EM (Robustness)":0.434
    },
    {
        "Model":"BLOOM (176B)",
        "Mean win rate":0.477,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.185,
        "HellaSwag - EM (Robustness)":"0.699",
        "OpenbookQA - EM (Robustness)":"0.438",
        "TruthfulQA - EM (Robustness)":0.183,
        "MMLU - EM (Robustness)":0.25
    },
    {
        "Model":"T0pp (11B)",
        "Mean win rate":0.545,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.031,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.365,
        "MMLU - EM (Robustness)":0.378
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B)",
        "Mean win rate":0.551,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.238,
        "HellaSwag - EM (Robustness)":"0.759",
        "OpenbookQA - EM (Robustness)":"0.448",
        "TruthfulQA - EM (Robustness)":0.151,
        "MMLU - EM (Robustness)":0.29
    },
    {
        "Model":"Cohere large v20220720 (13.1B)",
        "Mean win rate":0.387,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.172,
        "HellaSwag - EM (Robustness)":"0.687",
        "OpenbookQA - EM (Robustness)":"0.43",
        "TruthfulQA - EM (Robustness)":0.154,
        "MMLU - EM (Robustness)":0.253
    },
    {
        "Model":"Cohere medium v20220720 (6.1B)",
        "Mean win rate":0.154,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.102,
        "HellaSwag - EM (Robustness)":"0.651",
        "OpenbookQA - EM (Robustness)":"0.382",
        "TruthfulQA - EM (Robustness)":0.149,
        "MMLU - EM (Robustness)":0.184
    },
    {
        "Model":"Cohere small v20220720 (410M)",
        "Mean win rate":0.195,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.025,
        "HellaSwag - EM (Robustness)":"0.405",
        "OpenbookQA - EM (Robustness)":"0.238",
        "TruthfulQA - EM (Robustness)":0.204,
        "MMLU - EM (Robustness)":0.226
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B)",
        "Mean win rate":0.618,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.283,
        "HellaSwag - EM (Robustness)":"0.764",
        "OpenbookQA - EM (Robustness)":"0.482",
        "TruthfulQA - EM (Robustness)":0.116,
        "MMLU - EM (Robustness)":0.299
    },
    {
        "Model":"Cohere medium v20221108 (6.1B)",
        "Mean win rate":0.279,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.105,
        "HellaSwag - EM (Robustness)":"0.687",
        "OpenbookQA - EM (Robustness)":"0.414",
        "TruthfulQA - EM (Robustness)":0.17,
        "MMLU - EM (Robustness)":0.207
    },
    {
        "Model":"Cohere Command beta (6.1B)",
        "Mean win rate":0.483,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.163,
        "HellaSwag - EM (Robustness)":"0.696",
        "OpenbookQA - EM (Robustness)":"0.448",
        "TruthfulQA - EM (Robustness)":0.171,
        "MMLU - EM (Robustness)":0.334
    },
    {
        "Model":"Cohere Command beta (52.4B)",
        "Mean win rate":0.817,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.289,
        "HellaSwag - EM (Robustness)":"0.774",
        "OpenbookQA - EM (Robustness)":"0.492",
        "TruthfulQA - EM (Robustness)":0.229,
        "MMLU - EM (Robustness)":0.387
    },
    {
        "Model":"GPT-J (6B)",
        "Mean win rate":0.235,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.099,
        "HellaSwag - EM (Robustness)":"0.619",
        "OpenbookQA - EM (Robustness)":"0.398",
        "TruthfulQA - EM (Robustness)":0.181,
        "MMLU - EM (Robustness)":0.217
    },
    {
        "Model":"GPT-NeoX (20B)",
        "Mean win rate":0.287,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.133,
        "HellaSwag - EM (Robustness)":"0.661",
        "OpenbookQA - EM (Robustness)":"0.414",
        "TruthfulQA - EM (Robustness)":0.175,
        "MMLU - EM (Robustness)":0.189
    },
    {
        "Model":"Pythia (6.9B)",
        "Mean win rate":0.111,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.094,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.139,
        "MMLU - EM (Robustness)":0.201
    },
    {
        "Model":"Pythia (12B)",
        "Mean win rate":0.172,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.108,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.138,
        "MMLU - EM (Robustness)":0.22
    },
    {
        "Model":"T5 (11B)",
        "Mean win rate":0.278,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.153,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.122,
        "MMLU - EM (Robustness)":0.258
    },
    {
        "Model":"UL2 (20B)",
        "Mean win rate":0.419,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.141,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.178,
        "MMLU - EM (Robustness)":0.272
    },
    {
        "Model":"OPT (175B)",
        "Mean win rate":0.653,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.208,
        "HellaSwag - EM (Robustness)":"0.744",
        "OpenbookQA - EM (Robustness)":"0.488",
        "TruthfulQA - EM (Robustness)":0.205,
        "MMLU - EM (Robustness)":0.27
    },
    {
        "Model":"OPT (66B)",
        "Mean win rate":0.457,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.206,
        "HellaSwag - EM (Robustness)":"0.699",
        "OpenbookQA - EM (Robustness)":"0.45",
        "TruthfulQA - EM (Robustness)":0.174,
        "MMLU - EM (Robustness)":0.216
    },
    {
        "Model":"LLaMA (7B)",
        "Mean win rate":0.586,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.222,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.229,
        "MMLU - EM (Robustness)":0.268
    },
    {
        "Model":"LLaMA (13B)",
        "Mean win rate":0.712,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.272,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.274,
        "MMLU - EM (Robustness)":0.37
    },
    {
        "Model":"LLaMA (30B)",
        "Mean win rate":0.869,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.36,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.281,
        "MMLU - EM (Robustness)":0.461
    },
    {
        "Model":"LLaMA (65B)",
        "Mean win rate":0.939,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.388,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.448,
        "MMLU - EM (Robustness)":0.504
    },
    {
        "Model":"Llama 2 (7B)",
        "Mean win rate":0.687,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.261,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.234,
        "MMLU - EM (Robustness)":0.373
    },
    {
        "Model":"Llama 2 (13B)",
        "Mean win rate":0.838,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.324,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.287,
        "MMLU - EM (Robustness)":0.444
    },
    {
        "Model":"Llama 2 (70B)",
        "Mean win rate":0.975,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.42,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.468,
        "MMLU - EM (Robustness)":0.545
    },
    {
        "Model":"Alpaca (7B)",
        "Mean win rate":0.551,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.203,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.199,
        "MMLU - EM (Robustness)":0.324
    },
    {
        "Model":"Vicuna v1.3 (7B)",
        "Mean win rate":0.662,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.214,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.258,
        "MMLU - EM (Robustness)":0.371
    },
    {
        "Model":"Vicuna v1.3 (13B)",
        "Mean win rate":0.808,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.273,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.341,
        "MMLU - EM (Robustness)":0.413
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "Mean win rate":0.889,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.305,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.339,
        "MMLU - EM (Robustness)":0.533
    },
    {
        "Model":"TNLG v2 (530B)",
        "Mean win rate":0.768,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.307,
        "HellaSwag - EM (Robustness)":"0.757",
        "OpenbookQA - EM (Robustness)":"0.476",
        "TruthfulQA - EM (Robustness)":0.202,
        "MMLU - EM (Robustness)":0.403
    },
    {
        "Model":"TNLG v2 (6.7B)",
        "Mean win rate":0.198,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.149,
        "HellaSwag - EM (Robustness)":"0.656",
        "OpenbookQA - EM (Robustness)":"0.408",
        "TruthfulQA - EM (Robustness)":0.136,
        "MMLU - EM (Robustness)":0.169
    },
    {
        "Model":"davinci (175B)",
        "Mean win rate":0.579,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.256,
        "HellaSwag - EM (Robustness)":"0.738",
        "OpenbookQA - EM (Robustness)":"0.474",
        "TruthfulQA - EM (Robustness)":0.145,
        "MMLU - EM (Robustness)":0.34
    },
    {
        "Model":"curie (6.7B)",
        "Mean win rate":0.244,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.126,
        "HellaSwag - EM (Robustness)":"0.632",
        "OpenbookQA - EM (Robustness)":"0.396",
        "TruthfulQA - EM (Robustness)":0.186,
        "MMLU - EM (Robustness)":0.19
    },
    {
        "Model":"babbage (1.3B)",
        "Mean win rate":0.112,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.068,
        "HellaSwag - EM (Robustness)":"0.489",
        "OpenbookQA - EM (Robustness)":"0.314",
        "TruthfulQA - EM (Robustness)":0.162,
        "MMLU - EM (Robustness)":0.166
    },
    {
        "Model":"ada (350M)",
        "Mean win rate":0.107,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.031,
        "HellaSwag - EM (Robustness)":"0.37",
        "OpenbookQA - EM (Robustness)":"0.27",
        "TruthfulQA - EM (Robustness)":0.167,
        "MMLU - EM (Robustness)":0.204
    },
    {
        "Model":"text-davinci-003",
        "Mean win rate":0.97,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.369,
        "HellaSwag - EM (Robustness)":"0.798",
        "OpenbookQA - EM (Robustness)":"0.572",
        "TruthfulQA - EM (Robustness)":0.516,
        "MMLU - EM (Robustness)":0.517
    },
    {
        "Model":"text-davinci-002",
        "Mean win rate":0.933,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.299,
        "HellaSwag - EM (Robustness)":"0.776",
        "OpenbookQA - EM (Robustness)":"0.52",
        "TruthfulQA - EM (Robustness)":0.547,
        "MMLU - EM (Robustness)":0.525
    },
    {
        "Model":"text-curie-001",
        "Mean win rate":0.356,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.121,
        "HellaSwag - EM (Robustness)":"0.625",
        "OpenbookQA - EM (Robustness)":"0.424",
        "TruthfulQA - EM (Robustness)":0.235,
        "MMLU - EM (Robustness)":0.22
    },
    {
        "Model":"text-babbage-001",
        "Mean win rate":0.179,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.04,
        "HellaSwag - EM (Robustness)":"0.468",
        "OpenbookQA - EM (Robustness)":"0.39",
        "TruthfulQA - EM (Robustness)":0.195,
        "MMLU - EM (Robustness)":0.186
    },
    {
        "Model":"text-ada-001",
        "Mean win rate":0.088,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.008,
        "HellaSwag - EM (Robustness)":"0.32",
        "OpenbookQA - EM (Robustness)":"0.248",
        "TruthfulQA - EM (Robustness)":0.175,
        "MMLU - EM (Robustness)":0.178
    },
    {
        "Model":"gpt-3.5-turbo-0301",
        "Mean win rate":0.944,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.327,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.566,
        "MMLU - EM (Robustness)":0.525
    },
    {
        "Model":"gpt-3.5-turbo-0613",
        "Mean win rate":0.571,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.284,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.187,
        "MMLU - EM (Robustness)":0.262
    },
    {
        "Model":"RedPajama-INCITE-Base-v1 (3B)",
        "Mean win rate":0.379,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.134,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.226,
        "MMLU - EM (Robustness)":0.217
    },
    {
        "Model":"RedPajama-INCITE-Instruct-v1 (3B)",
        "Mean win rate":0.263,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.132,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.173,
        "MMLU - EM (Robustness)":0.218
    },
    {
        "Model":"RedPajama-INCITE-Base (7B)",
        "Mean win rate":0.369,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.167,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.173,
        "MMLU - EM (Robustness)":0.25
    },
    {
        "Model":"RedPajama-INCITE-Instruct (7B)",
        "Mean win rate":0.46,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.137,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.197,
        "MMLU - EM (Robustness)":0.291
    },
    {
        "Model":"MPT (30B)",
        "Mean win rate":0.606,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.272,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.177,
        "MMLU - EM (Robustness)":0.381
    },
    {
        "Model":"MPT-Instruct (30B)",
        "Mean win rate":0.545,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.202,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.177,
        "MMLU - EM (Robustness)":0.383
    },
    {
        "Model":"Falcon (7B)",
        "Mean win rate":0.47,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.185,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.205,
        "MMLU - EM (Robustness)":0.236
    },
    {
        "Model":"Falcon-Instruct (7B)",
        "Mean win rate":0.298,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.132,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.17,
        "MMLU - EM (Robustness)":0.25
    },
    {
        "Model":"Falcon (40B)",
        "Mean win rate":0.869,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.329,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.303,
        "MMLU - EM (Robustness)":0.457
    },
    {
        "Model":"Falcon-Instruct (40B)",
        "Mean win rate":0.879,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.335,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.338,
        "MMLU - EM (Robustness)":0.446
    },
    {
        "Model":"GLM (130B)",
        "Mean win rate":0.429,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.117,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.196,
        "MMLU - EM (Robustness)":0.32
    },
    {
        "Model":"InstructPalmyra (30B)",
        "Mean win rate":0.495,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.267,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.151,
        "MMLU - EM (Robustness)":0.348
    },
    {
        "Model":"Palmyra X (43B)",
        "Mean win rate":0.985,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.363,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.568,
        "MMLU - EM (Robustness)":0.566
    },
    {
        "Model":"YaLM (100B)",
        "Mean win rate":0.328,
        "NaturalQuestions (closed-book) - F1 (Robustness)":0.047,
        "HellaSwag - EM (Robustness)":"-",
        "OpenbookQA - EM (Robustness)":"-",
        "TruthfulQA - EM (Robustness)":0.202,
        "MMLU - EM (Robustness)":0.243
    }
]