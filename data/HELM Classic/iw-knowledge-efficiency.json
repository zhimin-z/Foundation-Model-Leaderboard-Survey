[
    {
        "Model":"J1-Jumbo v1 (178B)",
        "Mean win rate":"0.173",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.493",
        "HellaSwag - Denoised inference time (s)":"0.284",
        "OpenbookQA - Denoised inference time (s)":"0.259",
        "TruthfulQA - Denoised inference time (s)":"0.443",
        "MMLU - Denoised inference time (s)":"0.457",
        "WikiFact - Denoised inference time (s)":"1.037"
    },
    {
        "Model":"J1-Large v1 (7.5B)",
        "Mean win rate":"0.29",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.372",
        "HellaSwag - Denoised inference time (s)":"0.253",
        "OpenbookQA - Denoised inference time (s)":"0.238",
        "TruthfulQA - Denoised inference time (s)":"0.365",
        "MMLU - Denoised inference time (s)":"0.377",
        "WikiFact - Denoised inference time (s)":"0.54"
    },
    {
        "Model":"J1-Grande v1 (17B)",
        "Mean win rate":"0.21",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.466",
        "HellaSwag - Denoised inference time (s)":"0.33",
        "OpenbookQA - Denoised inference time (s)":"0.281",
        "TruthfulQA - Denoised inference time (s)":"0.396",
        "MMLU - Denoised inference time (s)":"0.411",
        "WikiFact - Denoised inference time (s)":"0.701"
    },
    {
        "Model":"J1-Grande v2 beta (17B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"Jurassic-2 Large (7.5B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"Luminous Base (13B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"Luminous Extended (30B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"Luminous Supreme (70B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B)",
        "Mean win rate":"0.07",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.777",
        "HellaSwag - Denoised inference time (s)":"0.549",
        "OpenbookQA - Denoised inference time (s)":"0.447",
        "TruthfulQA - Denoised inference time (s)":"0.568",
        "MMLU - Denoised inference time (s)":"0.578",
        "WikiFact - Denoised inference time (s)":"1.394"
    },
    {
        "Model":"BLOOM (176B)",
        "Mean win rate":"0.507",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"1.115",
        "HellaSwag - Denoised inference time (s)":"0.075",
        "OpenbookQA - Denoised inference time (s)":"0.032",
        "TruthfulQA - Denoised inference time (s)":"0.143",
        "MMLU - Denoised inference time (s)":"0.233",
        "WikiFact - Denoised inference time (s)":"0.832"
    },
    {
        "Model":"T0pp (11B)",
        "Mean win rate":"0.56",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"1.457",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"0.142",
        "MMLU - Denoised inference time (s)":"0.145",
        "WikiFact - Denoised inference time (s)":"0.137"
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B)",
        "Mean win rate":"0.127",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.565",
        "HellaSwag - Denoised inference time (s)":"0.359",
        "OpenbookQA - Denoised inference time (s)":"0.314",
        "TruthfulQA - Denoised inference time (s)":"0.501",
        "MMLU - Denoised inference time (s)":"0.489",
        "WikiFact - Denoised inference time (s)":"0.979"
    },
    {
        "Model":"Cohere large v20220720 (13.1B)",
        "Mean win rate":"0.335",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.337",
        "HellaSwag - Denoised inference time (s)":"0.225",
        "OpenbookQA - Denoised inference time (s)":"0.201",
        "TruthfulQA - Denoised inference time (s)":"0.325",
        "MMLU - Denoised inference time (s)":"0.317",
        "WikiFact - Denoised inference time (s)":"0.569"
    },
    {
        "Model":"Cohere medium v20220720 (6.1B)",
        "Mean win rate":"0.435",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.259",
        "HellaSwag - Denoised inference time (s)":"0.204",
        "OpenbookQA - Denoised inference time (s)":"0.187",
        "TruthfulQA - Denoised inference time (s)":"0.287",
        "MMLU - Denoised inference time (s)":"0.281",
        "WikiFact - Denoised inference time (s)":"0.394"
    },
    {
        "Model":"Cohere small v20220720 (410M)",
        "Mean win rate":"0.408",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.251",
        "HellaSwag - Denoised inference time (s)":"0.223",
        "OpenbookQA - Denoised inference time (s)":"0.214",
        "TruthfulQA - Denoised inference time (s)":"0.289",
        "MMLU - Denoised inference time (s)":"0.284",
        "WikiFact - Denoised inference time (s)":"0.313"
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"Cohere medium v20221108 (6.1B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"Cohere Command beta (6.1B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"Cohere Command beta (52.4B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"GPT-J (6B)",
        "Mean win rate":"0.832",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"1.777",
        "HellaSwag - Denoised inference time (s)":"0.03",
        "OpenbookQA - Denoised inference time (s)":"0.019",
        "TruthfulQA - Denoised inference time (s)":"0.044",
        "MMLU - Denoised inference time (s)":"0.07",
        "WikiFact - Denoised inference time (s)":"0.094"
    },
    {
        "Model":"GPT-NeoX (20B)",
        "Mean win rate":"0.845",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.482",
        "HellaSwag - Denoised inference time (s)":"0.025",
        "OpenbookQA - Denoised inference time (s)":"0.024",
        "TruthfulQA - Denoised inference time (s)":"0.084",
        "MMLU - Denoised inference time (s)":"0.133",
        "WikiFact - Denoised inference time (s)":"0.059"
    },
    {
        "Model":"Pythia (6.9B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"Pythia (12B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"T5 (11B)",
        "Mean win rate":"0.34",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"2.856",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"0.21",
        "MMLU - Denoised inference time (s)":"0.218",
        "WikiFact - Denoised inference time (s)":"0.264"
    },
    {
        "Model":"UL2 (20B)",
        "Mean win rate":"0.45",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"1.994",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"0.168",
        "MMLU - Denoised inference time (s)":"0.182",
        "WikiFact - Denoised inference time (s)":"0.238"
    },
    {
        "Model":"OPT (175B)",
        "Mean win rate":"0.43",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"4.548",
        "HellaSwag - Denoised inference time (s)":"0.71",
        "OpenbookQA - Denoised inference time (s)":"0.038",
        "TruthfulQA - Denoised inference time (s)":"0.141",
        "MMLU - Denoised inference time (s)":"0.12",
        "WikiFact - Denoised inference time (s)":"0.597"
    },
    {
        "Model":"OPT (66B)",
        "Mean win rate":"0.598",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.611",
        "HellaSwag - Denoised inference time (s)":"0.971",
        "OpenbookQA - Denoised inference time (s)":"0.188",
        "TruthfulQA - Denoised inference time (s)":"0.041",
        "MMLU - Denoised inference time (s)":"0.055",
        "WikiFact - Denoised inference time (s)":"0.145"
    },
    {
        "Model":"LLaMA (7B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"LLaMA (13B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"LLaMA (30B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"LLaMA (65B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"Llama 2 (7B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"Llama 2 (13B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"Llama 2 (70B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"Alpaca (7B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"Vicuna v1.3 (7B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"Vicuna v1.3 (13B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"TNLG v2 (530B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"TNLG v2 (6.7B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"davinci (175B)",
        "Mean win rate":"0.418",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.327",
        "HellaSwag - Denoised inference time (s)":"0.193",
        "OpenbookQA - Denoised inference time (s)":"0.184",
        "TruthfulQA - Denoised inference time (s)":"0.215",
        "MMLU - Denoised inference time (s)":"0.212",
        "WikiFact - Denoised inference time (s)":"0.705"
    },
    {
        "Model":"curie (6.7B)",
        "Mean win rate":"0.832",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.122",
        "HellaSwag - Denoised inference time (s)":"0.084",
        "OpenbookQA - Denoised inference time (s)":"0.079",
        "TruthfulQA - Denoised inference time (s)":"0.094",
        "MMLU - Denoised inference time (s)":"0.092",
        "WikiFact - Denoised inference time (s)":"0.203"
    },
    {
        "Model":"babbage (1.3B)",
        "Mean win rate":"0.782",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.152",
        "HellaSwag - Denoised inference time (s)":"0.113",
        "OpenbookQA - Denoised inference time (s)":"0.111",
        "TruthfulQA - Denoised inference time (s)":"0.12",
        "MMLU - Denoised inference time (s)":"0.119",
        "WikiFact - Denoised inference time (s)":"0.217"
    },
    {
        "Model":"ada (350M)",
        "Mean win rate":"0.652",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.167",
        "HellaSwag - Denoised inference time (s)":"0.138",
        "OpenbookQA - Denoised inference time (s)":"0.136",
        "TruthfulQA - Denoised inference time (s)":"0.141",
        "MMLU - Denoised inference time (s)":"0.14",
        "WikiFact - Denoised inference time (s)":"0.242"
    },
    {
        "Model":"text-davinci-003",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"text-davinci-002",
        "Mean win rate":"0.475",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.264",
        "HellaSwag - Denoised inference time (s)":"0.171",
        "OpenbookQA - Denoised inference time (s)":"0.158",
        "TruthfulQA - Denoised inference time (s)":"0.2",
        "MMLU - Denoised inference time (s)":"0.196",
        "WikiFact - Denoised inference time (s)":"0.615"
    },
    {
        "Model":"text-curie-001",
        "Mean win rate":"0.69",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.153",
        "HellaSwag - Denoised inference time (s)":"0.125",
        "OpenbookQA - Denoised inference time (s)":"0.119",
        "TruthfulQA - Denoised inference time (s)":"0.134",
        "MMLU - Denoised inference time (s)":"0.133",
        "WikiFact - Denoised inference time (s)":"0.246"
    },
    {
        "Model":"text-babbage-001",
        "Mean win rate":"0.717",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.136",
        "HellaSwag - Denoised inference time (s)":"0.125",
        "OpenbookQA - Denoised inference time (s)":"0.122",
        "TruthfulQA - Denoised inference time (s)":"0.134",
        "MMLU - Denoised inference time (s)":"0.133",
        "WikiFact - Denoised inference time (s)":"0.252"
    },
    {
        "Model":"text-ada-001",
        "Mean win rate":"0.882",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.085",
        "HellaSwag - Denoised inference time (s)":"0.079",
        "OpenbookQA - Denoised inference time (s)":"0.076",
        "TruthfulQA - Denoised inference time (s)":"0.089",
        "MMLU - Denoised inference time (s)":"0.088",
        "WikiFact - Denoised inference time (s)":"0.177"
    },
    {
        "Model":"gpt-3.5-turbo-0301",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"gpt-3.5-turbo-0613",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"RedPajama-INCITE-Base-v1 (3B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"RedPajama-INCITE-Instruct-v1 (3B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"RedPajama-INCITE-Base (7B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"RedPajama-INCITE-Instruct (7B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"MPT (30B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"MPT-Instruct (30B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"Falcon (7B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"Falcon-Instruct (7B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"Falcon (40B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"Falcon-Instruct (40B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"GLM (130B)",
        "Mean win rate":"0.34",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.953",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"0.158",
        "MMLU - Denoised inference time (s)":"0.335",
        "WikiFact - Denoised inference time (s)":"0.515"
    },
    {
        "Model":"InstructPalmyra (30B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"Palmyra X (43B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MMLU - Denoised inference time (s)":"-",
        "WikiFact - Denoised inference time (s)":"-"
    },
    {
        "Model":"YaLM (100B)",
        "Mean win rate":"0.5",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"2.722",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"0.092",
        "MMLU - Denoised inference time (s)":"0.143",
        "WikiFact - Denoised inference time (s)":"0.339"
    }
]