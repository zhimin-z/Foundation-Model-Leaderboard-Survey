[
    {
        "Model":"J1-Jumbo v1 (178B)",
        "F1":0.137,
        "Denoised inference time (s)":"0.583",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":316.877,
        "# output tokens":4.808,
        "# trials":3
    },
    {
        "Model":"J1-Large v1 (7.5B)",
        "F1":0.139,
        "Denoised inference time (s)":"0.423",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":316.877,
        "# output tokens":5.537,
        "# trials":3
    },
    {
        "Model":"J1-Grande v1 (17B)",
        "F1":0.12,
        "Denoised inference time (s)":"0.478",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":316.877,
        "# output tokens":5.483,
        "# trials":3
    },
    {
        "Model":"J1-Grande v2 beta (17B)",
        "F1":0.104,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":316.877,
        "# output tokens":5.487,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "F1":0.079,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":316.877,
        "# output tokens":5.553,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "F1":0.133,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":316.877,
        "# output tokens":5.545,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Large (7.5B)",
        "F1":0.167,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":316.877,
        "# output tokens":5.016,
        "# trials":3
    },
    {
        "Model":"Luminous Base (13B)",
        "F1":0.0,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":414.778,
        "# output tokens":0.0,
        "# trials":3
    },
    {
        "Model":"Luminous Extended (30B)",
        "F1":0.0,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":414.778,
        "# output tokens":0.0,
        "# trials":3
    },
    {
        "Model":"Luminous Supreme (70B)",
        "F1":0.0,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":414.778,
        "# output tokens":0.0,
        "# trials":3
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B)",
        "F1":0.197,
        "Denoised inference time (s)":"0.824",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":401.15,
        "# output tokens":5.05,
        "# trials":3
    },
    {
        "Model":"BLOOM (176B)",
        "F1":0.128,
        "Denoised inference time (s)":"0.284",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":381.014,
        "# output tokens":9.052,
        "# trials":3
    },
    {
        "Model":"T0pp (11B)",
        "F1":0.002,
        "Denoised inference time (s)":"0.262",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":395.103,
        "# output tokens":20.0,
        "# trials":3
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B)",
        "F1":0.0,
        "Denoised inference time (s)":"0.487",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":411.033,
        "# output tokens":0.0,
        "# trials":3
    },
    {
        "Model":"Cohere large v20220720 (13.1B)",
        "F1":0.0,
        "Denoised inference time (s)":"0.302",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":411.033,
        "# output tokens":0.0,
        "# trials":3
    },
    {
        "Model":"Cohere medium v20220720 (6.1B)",
        "F1":0.0,
        "Denoised inference time (s)":"0.273",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":411.033,
        "# output tokens":0.0,
        "# trials":3
    },
    {
        "Model":"Cohere small v20220720 (410M)",
        "F1":0.0,
        "Denoised inference time (s)":"0.27",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":411.033,
        "# output tokens":0.0,
        "# trials":3
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B)",
        "F1":0.0,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":411.033,
        "# output tokens":0.0,
        "# trials":3
    },
    {
        "Model":"Cohere medium v20221108 (6.1B)",
        "F1":0.0,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":411.033,
        "# output tokens":0.0,
        "# trials":3
    },
    {
        "Model":"Cohere Command beta (6.1B)",
        "F1":0.208,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":411.033,
        "# output tokens":5.125,
        "# trials":3
    },
    {
        "Model":"Cohere Command beta (52.4B)",
        "F1":0.201,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":411.033,
        "# output tokens":5.104,
        "# trials":3
    },
    {
        "Model":"GPT-J (6B)",
        "F1":0.171,
        "Denoised inference time (s)":"0.102",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":401.15,
        "# output tokens":9.696,
        "# trials":3
    },
    {
        "Model":"GPT-NeoX (20B)",
        "F1":0.148,
        "Denoised inference time (s)":"0.101",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":405.918,
        "# output tokens":9.248,
        "# trials":3
    },
    {
        "Model":"Pythia (6.9B)",
        "F1":0.0,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":383.918,
        "# output tokens":20.0,
        "# trials":1
    },
    {
        "Model":"Pythia (12B)",
        "F1":0.0,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":383.918,
        "# output tokens":20.0,
        "# trials":1
    },
    {
        "Model":"T5 (11B)",
        "F1":0.119,
        "Denoised inference time (s)":"0.44",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":394.103,
        "# output tokens":20.0,
        "# trials":3
    },
    {
        "Model":"UL2 (20B)",
        "F1":0.156,
        "Denoised inference time (s)":"0.341",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":398.103,
        "# output tokens":20.0,
        "# trials":3
    },
    {
        "Model":"OPT (175B)",
        "F1":0.135,
        "Denoised inference time (s)":"0.215",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":401.15,
        "# output tokens":9.052,
        "# trials":3
    },
    {
        "Model":"OPT (66B)",
        "F1":0.177,
        "Denoised inference time (s)":"0.068",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":401.15,
        "# output tokens":9.0,
        "# trials":3
    },
    {
        "Model":"LLaMA (7B)",
        "F1":0.0,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":411.783,
        "# output tokens":20.0,
        "# trials":1
    },
    {
        "Model":"LLaMA (13B)",
        "F1":0.0,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":411.783,
        "# output tokens":20.0,
        "# trials":1
    },
    {
        "Model":"LLaMA (30B)",
        "F1":0.0,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":411.783,
        "# output tokens":20.0,
        "# trials":1
    },
    {
        "Model":"LLaMA (65B)",
        "F1":0.278,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":411.783,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (7B)",
        "F1":0.199,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":411.783,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (13B)",
        "F1":0.197,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":411.783,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (70B)",
        "F1":0.444,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":411.783,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Alpaca (7B)",
        "F1":0.189,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":411.783,
        "# output tokens":14.784,
        "# trials":1
    },
    {
        "Model":"Vicuna v1.3 (7B)",
        "F1":0.192,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":411.783,
        "# output tokens":19.977,
        "# trials":1
    },
    {
        "Model":"Vicuna v1.3 (13B)",
        "F1":0.226,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":411.783,
        "# output tokens":19.779,
        "# trials":1
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "F1":0.405,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":0.0,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"TNLG v2 (530B)",
        "F1":0.188,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":401.15,
        "# output tokens":4.619,
        "# trials":3
    },
    {
        "Model":"TNLG v2 (6.7B)",
        "F1":0.207,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":401.15,
        "# output tokens":5.073,
        "# trials":3
    },
    {
        "Model":"davinci (175B)",
        "F1":0.128,
        "Denoised inference time (s)":"0.34",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":401.15,
        "# output tokens":5.06,
        "# trials":3
    },
    {
        "Model":"curie (6.7B)",
        "F1":0.143,
        "Denoised inference time (s)":"0.118",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":401.15,
        "# output tokens":4.814,
        "# trials":3
    },
    {
        "Model":"babbage (1.3B)",
        "F1":0.116,
        "Denoised inference time (s)":"0.146",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":401.15,
        "# output tokens":5.383,
        "# trials":3
    },
    {
        "Model":"ada (350M)",
        "F1":0.07,
        "Denoised inference time (s)":"0.168",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":401.15,
        "# output tokens":5.579,
        "# trials":3
    },
    {
        "Model":"text-davinci-003",
        "F1":0.641,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":401.15,
        "# output tokens":4.983,
        "# trials":3
    },
    {
        "Model":"text-davinci-002",
        "F1":0.526,
        "Denoised inference time (s)":"0.334",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":401.15,
        "# output tokens":5.277,
        "# trials":3
    },
    {
        "Model":"text-curie-001",
        "F1":0.174,
        "Denoised inference time (s)":"0.159",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":401.15,
        "# output tokens":5.133,
        "# trials":3
    },
    {
        "Model":"text-babbage-001",
        "F1":0.181,
        "Denoised inference time (s)":"0.162",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":401.15,
        "# output tokens":5.303,
        "# trials":3
    },
    {
        "Model":"text-ada-001",
        "F1":0.09,
        "Denoised inference time (s)":"0.115",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":401.15,
        "# output tokens":5.672,
        "# trials":3
    },
    {
        "Model":"code-davinci-002",
        "F1":0.598,
        "Denoised inference time (s)":"0.158",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":401.15,
        "# output tokens":0.0,
        "# trials":3
    },
    {
        "Model":"code-cushman-001 (12B)",
        "F1":0.101,
        "Denoised inference time (s)":"0.099",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":401.15,
        "# output tokens":0.0,
        "# trials":3
    },
    {
        "Model":"gpt-3.5-turbo-0301",
        "F1":0.532,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":339.394,
        "# output tokens":5.843,
        "# trials":1
    },
    {
        "Model":"gpt-3.5-turbo-0613",
        "F1":0.53,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":339.394,
        "# output tokens":5.072,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Base-v1 (3B)",
        "F1":0.0,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":383.918,
        "# output tokens":20.0,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Instruct-v1 (3B)",
        "F1":0.0,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":383.918,
        "# output tokens":20.0,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Base (7B)",
        "F1":0.0,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":383.918,
        "# output tokens":20.0,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Instruct (7B)",
        "F1":0.0,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":383.918,
        "# output tokens":20.0,
        "# trials":1
    },
    {
        "Model":"MPT (30B)",
        "F1":0.263,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":383.918,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"MPT-Instruct (30B)",
        "F1":0.231,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":383.918,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon (7B)",
        "F1":0.13,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":381.522,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon-Instruct (7B)",
        "F1":0.08,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":381.522,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon (40B)",
        "F1":0.13,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":381.522,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon-Instruct (40B)",
        "F1":0.178,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":381.522,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"GLM (130B)",
        "F1":0.23,
        "Denoised inference time (s)":"0.621",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":362.993,
        "# output tokens":5.849,
        "# trials":3
    },
    {
        "Model":"InstructPalmyra (30B)",
        "F1":0.175,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":401.15,
        "# output tokens":5.063,
        "# trials":3
    },
    {
        "Model":"Palmyra X (43B)",
        "F1":0.449,
        "Denoised inference time (s)":"-",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":401.15,
        "# output tokens":5.423,
        "# trials":3
    },
    {
        "Model":"YaLM (100B)",
        "F1":0.073,
        "Denoised inference time (s)":"0.207",
        "# eval":515,
        "# train":3,
        "truncated":0,
        "# prompt tokens":404.939,
        "# output tokens":15.414,
        "# trials":3
    }
]