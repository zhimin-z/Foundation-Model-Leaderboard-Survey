[
    {
        "Model":"J1-Jumbo v1 (178B)",
        "EM":0.247,
        "Denoised inference time (s)":"1.093",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":59.969,
        "# output tokens":20.307,
        "# trials":3
    },
    {
        "Model":"J1-Large v1 (7.5B)",
        "EM":0.071,
        "Denoised inference time (s)":"0.559",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":59.969,
        "# output tokens":20.16,
        "# trials":3
    },
    {
        "Model":"J1-Grande v1 (17B)",
        "EM":0.132,
        "Denoised inference time (s)":"0.723",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":59.969,
        "# output tokens":19.813,
        "# trials":3
    },
    {
        "Model":"J1-Grande v2 beta (17B)",
        "EM":0.266,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":59.969,
        "# output tokens":20.328,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "EM":0.37,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":59.969,
        "# output tokens":18.575,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "EM":0.275,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":59.969,
        "# output tokens":19.256,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Large (7.5B)",
        "EM":0.072,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":59.969,
        "# output tokens":18.124,
        "# trials":3
    },
    {
        "Model":"Luminous Base (13B)",
        "EM":0.113,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":80.46,
        "# output tokens":17.152,
        "# trials":3
    },
    {
        "Model":"Luminous Extended (30B)",
        "EM":0.193,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":80.46,
        "# output tokens":17.193,
        "# trials":3
    },
    {
        "Model":"Luminous Supreme (70B)",
        "EM":0.267,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":80.46,
        "# output tokens":17.176,
        "# trials":3
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B)",
        "EM":0.337,
        "Denoised inference time (s)":"1.58",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":82.064,
        "# output tokens":19.927,
        "# trials":3
    },
    {
        "Model":"BLOOM (176B)",
        "EM":0.091,
        "Denoised inference time (s)":"0.871",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":78.002,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"T0pp (11B)",
        "EM":0.004,
        "Denoised inference time (s)":"0.13",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":89.72,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B)",
        "EM":0.297,
        "Denoised inference time (s)":"1.093",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":85.284,
        "# output tokens":19.393,
        "# trials":3
    },
    {
        "Model":"Cohere large v20220720 (13.1B)",
        "EM":0.125,
        "Denoised inference time (s)":"0.626",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":85.284,
        "# output tokens":18.495,
        "# trials":3
    },
    {
        "Model":"Cohere medium v20220720 (6.1B)",
        "EM":0.076,
        "Denoised inference time (s)":"0.427",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":85.284,
        "# output tokens":19.689,
        "# trials":3
    },
    {
        "Model":"Cohere small v20220720 (410M)",
        "EM":0.008,
        "Denoised inference time (s)":"0.328",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":85.284,
        "# output tokens":18.545,
        "# trials":3
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B)",
        "EM":0.362,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":85.284,
        "# output tokens":19.238,
        "# trials":3
    },
    {
        "Model":"Cohere medium v20221108 (6.1B)",
        "EM":0.097,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":85.284,
        "# output tokens":18.209,
        "# trials":3
    },
    {
        "Model":"Cohere Command beta (6.1B)",
        "EM":0.105,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":85.284,
        "# output tokens":17.496,
        "# trials":3
    },
    {
        "Model":"Cohere Command beta (52.4B)",
        "EM":0.302,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":85.284,
        "# output tokens":18.934,
        "# trials":3
    },
    {
        "Model":"GPT-J (6B)",
        "EM":0.064,
        "Denoised inference time (s)":"0.094",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":82.064,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"GPT-NeoX (20B)",
        "EM":0.12,
        "Denoised inference time (s)":"0.065",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":84.122,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"Pythia (6.9B)",
        "EM":0.022,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":77.122,
        "# output tokens":5.736,
        "# trials":1
    },
    {
        "Model":"Pythia (12B)",
        "EM":0.036,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":77.122,
        "# output tokens":5.844,
        "# trials":1
    },
    {
        "Model":"T5 (11B)",
        "EM":0.024,
        "Denoised inference time (s)":"0.235",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":89.72,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"UL2 (20B)",
        "EM":0.063,
        "Denoised inference time (s)":"0.219",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":93.72,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"OPT (175B)",
        "EM":0.171,
        "Denoised inference time (s)":"0.595",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":82.064,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"OPT (66B)",
        "EM":0.115,
        "Denoised inference time (s)":"0.146",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":82.064,
        "# output tokens":40.0,
        "# trials":3
    },
    {
        "Model":"LLaMA (7B)",
        "EM":0.231,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":81.792,
        "# output tokens":5.349,
        "# trials":1
    },
    {
        "Model":"LLaMA (13B)",
        "EM":0.299,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":81.792,
        "# output tokens":5.354,
        "# trials":1
    },
    {
        "Model":"LLaMA (30B)",
        "EM":0.448,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":81.792,
        "# output tokens":5.318,
        "# trials":1
    },
    {
        "Model":"LLaMA (65B)",
        "EM":0.593,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":81.792,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (7B)",
        "EM":0.344,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":81.792,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (13B)",
        "EM":0.421,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":81.792,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (70B)",
        "EM":0.615,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":81.792,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Alpaca (7B)",
        "EM":0.222,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":81.792,
        "# output tokens":5.362,
        "# trials":1
    },
    {
        "Model":"Vicuna v1.3 (7B)",
        "EM":0.219,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":81.792,
        "# output tokens":5.309,
        "# trials":1
    },
    {
        "Model":"Vicuna v1.3 (13B)",
        "EM":0.295,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":81.792,
        "# output tokens":5.382,
        "# trials":1
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "EM":0.311,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":0.0,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"TNLG v2 (530B)",
        "EM":0.413,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":82.064,
        "# output tokens":19.162,
        "# trials":3
    },
    {
        "Model":"TNLG v2 (6.7B)",
        "EM":0.082,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":82.064,
        "# output tokens":18.904,
        "# trials":3
    },
    {
        "Model":"davinci (175B)",
        "EM":0.295,
        "Denoised inference time (s)":"0.737",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":82.064,
        "# output tokens":19.131,
        "# trials":3
    },
    {
        "Model":"curie (6.7B)",
        "EM":0.094,
        "Denoised inference time (s)":"0.21",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":82.064,
        "# output tokens":19.026,
        "# trials":3
    },
    {
        "Model":"babbage (1.3B)",
        "EM":0.03,
        "Denoised inference time (s)":"0.222",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":82.064,
        "# output tokens":19.068,
        "# trials":3
    },
    {
        "Model":"ada (350M)",
        "EM":0.007,
        "Denoised inference time (s)":"0.247",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":82.064,
        "# output tokens":19.175,
        "# trials":3
    },
    {
        "Model":"text-davinci-003",
        "EM":0.538,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":82.064,
        "# output tokens":20.564,
        "# trials":3
    },
    {
        "Model":"text-davinci-002",
        "EM":0.508,
        "Denoised inference time (s)":"0.675",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":82.064,
        "# output tokens":18.402,
        "# trials":3
    },
    {
        "Model":"text-curie-001",
        "EM":0.083,
        "Denoised inference time (s)":"0.251",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":82.064,
        "# output tokens":19.087,
        "# trials":3
    },
    {
        "Model":"text-babbage-001",
        "EM":0.018,
        "Denoised inference time (s)":"0.259",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":82.064,
        "# output tokens":20.135,
        "# trials":3
    },
    {
        "Model":"text-ada-001",
        "EM":0.005,
        "Denoised inference time (s)":"0.197",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":82.064,
        "# output tokens":20.699,
        "# trials":3
    },
    {
        "Model":"gpt-3.5-turbo-0301",
        "EM":0.568,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":73.724,
        "# output tokens":22.9,
        "# trials":1
    },
    {
        "Model":"gpt-3.5-turbo-0613",
        "EM":0.562,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":73.724,
        "# output tokens":21.665,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Base-v1 (3B)",
        "EM":0.06,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":77.122,
        "# output tokens":5.513,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Instruct-v1 (3B)",
        "EM":0.073,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":77.122,
        "# output tokens":5.622,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Base (7B)",
        "EM":0.111,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":77.122,
        "# output tokens":5.245,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Instruct (7B)",
        "EM":0.114,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":77.122,
        "# output tokens":5.348,
        "# trials":1
    },
    {
        "Model":"MPT (30B)",
        "EM":0.284,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":77.122,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"MPT-Instruct (30B)",
        "EM":0.231,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":77.122,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon (7B)",
        "EM":0.254,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":77.828,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon-Instruct (7B)",
        "EM":0.099,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":77.828,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon (40B)",
        "EM":0.464,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":77.828,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon-Instruct (40B)",
        "EM":0.434,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":77.828,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"GLM (130B)",
        "EM":0.036,
        "Denoised inference time (s)":"0.512",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":80.593,
        "# output tokens":24.537,
        "# trials":3
    },
    {
        "Model":"InstructPalmyra (30B)",
        "EM":0.209,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":82.064,
        "# output tokens":18.361,
        "# trials":3
    },
    {
        "Model":"Palmyra X (43B)",
        "EM":0.407,
        "Denoised inference time (s)":"-",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":82.064,
        "# output tokens":18.882,
        "# trials":3
    },
    {
        "Model":"YaLM (100B)",
        "EM":0.012,
        "Denoised inference time (s)":"0.34",
        "# eval":850,
        "# train":5,
        "truncated":0,
        "# prompt tokens":83.895,
        "# output tokens":40.0,
        "# trials":3
    }
]