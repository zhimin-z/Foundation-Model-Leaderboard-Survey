[
    {
        "Model":"Anthropic-LM v4-s3 (52B) [method: multiple_choice_joint]",
        "Mean win rate":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "MMLU - ECE (10-bin)":"-"
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B) [method: multiple_choice_separate_calibrated]",
        "Mean win rate":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "MMLU - ECE (10-bin)":"-"
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B) [method: multiple_choice_separate_original]",
        "Mean win rate":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "MMLU - ECE (10-bin)":"-"
    },
    {
        "Model":"BLOOM (176B) [method: multiple_choice_joint]",
        "Mean win rate":"0.75",
        "HellaSwag - ECE (10-bin)":"0.039",
        "OpenbookQA - ECE (10-bin)":"0.063",
        "TruthfulQA - ECE (10-bin)":"0.111",
        "MMLU - ECE (10-bin)":"0.137"
    },
    {
        "Model":"BLOOM (176B) [method: multiple_choice_separate_calibrated]",
        "Mean win rate":"0.143",
        "HellaSwag - ECE (10-bin)":"0.385",
        "OpenbookQA - ECE (10-bin)":"0.248",
        "TruthfulQA - ECE (10-bin)":"0.41",
        "MMLU - ECE (10-bin)":"0.336"
    },
    {
        "Model":"BLOOM (176B) [method: multiple_choice_separate_original]",
        "Mean win rate":"0.554",
        "HellaSwag - ECE (10-bin)":"0.293",
        "OpenbookQA - ECE (10-bin)":"0.179",
        "TruthfulQA - ECE (10-bin)":"0.136",
        "MMLU - ECE (10-bin)":"0.146"
    },
    {
        "Model":"GPT-J (6B) [method: multiple_choice_joint]",
        "Mean win rate":"0.857",
        "HellaSwag - ECE (10-bin)":"0.041",
        "OpenbookQA - ECE (10-bin)":"0.058",
        "TruthfulQA - ECE (10-bin)":"0.085",
        "MMLU - ECE (10-bin)":"0.115"
    },
    {
        "Model":"GPT-J (6B) [method: multiple_choice_separate_calibrated]",
        "Mean win rate":"0.089",
        "HellaSwag - ECE (10-bin)":"0.44",
        "OpenbookQA - ECE (10-bin)":"0.235",
        "TruthfulQA - ECE (10-bin)":"0.376",
        "MMLU - ECE (10-bin)":"0.435"
    },
    {
        "Model":"GPT-J (6B) [method: multiple_choice_separate_original]",
        "Mean win rate":"0.464",
        "HellaSwag - ECE (10-bin)":"0.233",
        "OpenbookQA - ECE (10-bin)":"0.22",
        "TruthfulQA - ECE (10-bin)":"0.111",
        "MMLU - ECE (10-bin)":"0.198"
    },
    {
        "Model":"GPT-NeoX (20B) [method: multiple_choice_joint]",
        "Mean win rate":"0.964",
        "HellaSwag - ECE (10-bin)":"0.031",
        "OpenbookQA - ECE (10-bin)":"0.044",
        "TruthfulQA - ECE (10-bin)":"0.058",
        "MMLU - ECE (10-bin)":"0.122"
    },
    {
        "Model":"GPT-NeoX (20B) [method: multiple_choice_separate_calibrated]",
        "Mean win rate":"0.125",
        "HellaSwag - ECE (10-bin)":"0.419",
        "OpenbookQA - ECE (10-bin)":"0.232",
        "TruthfulQA - ECE (10-bin)":"0.399",
        "MMLU - ECE (10-bin)":"0.382"
    },
    {
        "Model":"GPT-NeoX (20B) [method: multiple_choice_separate_original]",
        "Mean win rate":"0.464",
        "HellaSwag - ECE (10-bin)":"0.277",
        "OpenbookQA - ECE (10-bin)":"0.21",
        "TruthfulQA - ECE (10-bin)":"0.123",
        "MMLU - ECE (10-bin)":"0.195"
    },
    {
        "Model":"OPT (175B) [method: multiple_choice_joint]",
        "Mean win rate":"0.821",
        "HellaSwag - ECE (10-bin)":"0.041",
        "OpenbookQA - ECE (10-bin)":"0.062",
        "TruthfulQA - ECE (10-bin)":"0.058",
        "MMLU - ECE (10-bin)":"0.147"
    },
    {
        "Model":"OPT (175B) [method: multiple_choice_separate_calibrated]",
        "Mean win rate":"0.268",
        "HellaSwag - ECE (10-bin)":"0.39",
        "OpenbookQA - ECE (10-bin)":"0.209",
        "TruthfulQA - ECE (10-bin)":"0.363",
        "MMLU - ECE (10-bin)":"0.386"
    },
    {
        "Model":"OPT (175B) [method: multiple_choice_separate_original]",
        "Mean win rate":"0.554",
        "HellaSwag - ECE (10-bin)":"0.325",
        "OpenbookQA - ECE (10-bin)":"0.199",
        "TruthfulQA - ECE (10-bin)":"0.096",
        "MMLU - ECE (10-bin)":"0.193"
    },
    {
        "Model":"OPT (66B) [method: multiple_choice_joint]",
        "Mean win rate":"0.839",
        "HellaSwag - ECE (10-bin)":"0.045",
        "OpenbookQA - ECE (10-bin)":"0.051",
        "TruthfulQA - ECE (10-bin)":"0.082",
        "MMLU - ECE (10-bin)":"0.135"
    },
    {
        "Model":"OPT (66B) [method: multiple_choice_separate_calibrated]",
        "Mean win rate":"0.143",
        "HellaSwag - ECE (10-bin)":"0.402",
        "OpenbookQA - ECE (10-bin)":"0.237",
        "TruthfulQA - ECE (10-bin)":"0.381",
        "MMLU - ECE (10-bin)":"0.377"
    },
    {
        "Model":"OPT (66B) [method: multiple_choice_separate_original]",
        "Mean win rate":"0.464",
        "HellaSwag - ECE (10-bin)":"0.293",
        "OpenbookQA - ECE (10-bin)":"0.226",
        "TruthfulQA - ECE (10-bin)":"0.109",
        "MMLU - ECE (10-bin)":"0.194"
    }
]