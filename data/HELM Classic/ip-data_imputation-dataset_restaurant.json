[
    {
        "Model":"J1-Jumbo v1 (178B)",
        "EM":0.593,
        "Denoised inference time (s)":"0.436",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":243.744,
        "# output tokens":2.791,
        "# trials":3
    },
    {
        "Model":"J1-Large v1 (7.5B)",
        "EM":0.616,
        "Denoised inference time (s)":"0.327",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":243.744,
        "# output tokens":2.733,
        "# trials":3
    },
    {
        "Model":"J1-Grande v1 (17B)",
        "EM":0.628,
        "Denoised inference time (s)":"0.519",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":243.744,
        "# output tokens":2.729,
        "# trials":3
    },
    {
        "Model":"J1-Grande v2 beta (17B)",
        "EM":0.748,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":243.744,
        "# output tokens":2.694,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "EM":0.826,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":243.744,
        "# output tokens":2.484,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "EM":0.802,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":243.744,
        "# output tokens":2.512,
        "# trials":3
    },
    {
        "Model":"Jurassic-2 Large (7.5B)",
        "EM":0.585,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":243.744,
        "# output tokens":2.829,
        "# trials":3
    },
    {
        "Model":"Luminous Base (13B)",
        "EM":0.589,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":243.45,
        "# output tokens":2.636,
        "# trials":3
    },
    {
        "Model":"Luminous Extended (30B)",
        "EM":0.609,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":243.45,
        "# output tokens":2.601,
        "# trials":3
    },
    {
        "Model":"Luminous Supreme (70B)",
        "EM":0.686,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":243.45,
        "# output tokens":2.523,
        "# trials":3
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B)",
        "EM":0.574,
        "Denoised inference time (s)":"0.747",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":246.992,
        "# output tokens":3.38,
        "# trials":3
    },
    {
        "Model":"BLOOM (176B)",
        "EM":0.523,
        "Denoised inference time (s)":"0.485",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":206.054,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"T0pp (11B)",
        "EM":0.008,
        "Denoised inference time (s)":"0.167",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":277.686,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B)",
        "EM":0.725,
        "Denoised inference time (s)":"0.514",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":250.368,
        "# output tokens":2.802,
        "# trials":3
    },
    {
        "Model":"Cohere large v20220720 (13.1B)",
        "EM":0.589,
        "Denoised inference time (s)":"0.314",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":250.368,
        "# output tokens":2.992,
        "# trials":3
    },
    {
        "Model":"Cohere medium v20220720 (6.1B)",
        "EM":0.581,
        "Denoised inference time (s)":"0.26",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":250.368,
        "# output tokens":3.004,
        "# trials":3
    },
    {
        "Model":"Cohere small v20220720 (410M)",
        "EM":0.093,
        "Denoised inference time (s)":"0.254",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":250.368,
        "# output tokens":2.872,
        "# trials":3
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B)",
        "EM":0.74,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":250.368,
        "# output tokens":2.76,
        "# trials":3
    },
    {
        "Model":"Cohere medium v20221108 (6.1B)",
        "EM":0.578,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":250.368,
        "# output tokens":3.012,
        "# trials":3
    },
    {
        "Model":"Cohere Command beta (6.1B)",
        "EM":0.562,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":250.368,
        "# output tokens":3.0,
        "# trials":3
    },
    {
        "Model":"Cohere Command beta (52.4B)",
        "EM":0.643,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":250.368,
        "# output tokens":2.938,
        "# trials":3
    },
    {
        "Model":"GPT-J (6B)",
        "EM":0.484,
        "Denoised inference time (s)":"0.182",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":246.992,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"GPT-NeoX (20B)",
        "EM":0.574,
        "Denoised inference time (s)":"0.344",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":247.38,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"Pythia (6.9B)",
        "EM":0.442,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":256.047,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"Pythia (12B)",
        "EM":0.535,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":256.047,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"T5 (11B)",
        "EM":0.407,
        "Denoised inference time (s)":"0.227",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":277.686,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"UL2 (20B)",
        "EM":0.391,
        "Denoised inference time (s)":"0.221",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":281.686,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"OPT (175B)",
        "EM":0.593,
        "Denoised inference time (s)":"0.455",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":246.992,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"OPT (66B)",
        "EM":0.574,
        "Denoised inference time (s)":"0.052",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":246.992,
        "# output tokens":5.0,
        "# trials":3
    },
    {
        "Model":"LLaMA (7B)",
        "EM":0.837,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":319.337,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"LLaMA (13B)",
        "EM":0.802,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":319.337,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"LLaMA (30B)",
        "EM":0.826,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":319.337,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"LLaMA (65B)",
        "EM":0.814,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":319.337,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (7B)",
        "EM":0.756,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":319.337,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (13B)",
        "EM":0.826,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":319.337,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Llama 2 (70B)",
        "EM":0.814,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":319.337,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Alpaca (7B)",
        "EM":0.593,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":319.337,
        "# output tokens":4.919,
        "# trials":1
    },
    {
        "Model":"Vicuna v1.3 (7B)",
        "EM":0.57,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":319.337,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"Vicuna v1.3 (13B)",
        "EM":0.581,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":319.337,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "EM":0.744,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":322.058,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"TNLG v2 (530B)",
        "EM":0.775,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":246.992,
        "# output tokens":3.14,
        "# trials":3
    },
    {
        "Model":"TNLG v2 (6.7B)",
        "EM":0.806,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":246.992,
        "# output tokens":3.031,
        "# trials":3
    },
    {
        "Model":"davinci (175B)",
        "EM":0.826,
        "Denoised inference time (s)":"0.272",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":246.992,
        "# output tokens":3.035,
        "# trials":3
    },
    {
        "Model":"curie (6.7B)",
        "EM":0.76,
        "Denoised inference time (s)":"0.107",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":246.992,
        "# output tokens":3.109,
        "# trials":3
    },
    {
        "Model":"babbage (1.3B)",
        "EM":0.337,
        "Denoised inference time (s)":"0.133",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":246.992,
        "# output tokens":3.547,
        "# trials":3
    },
    {
        "Model":"ada (350M)",
        "EM":0.318,
        "Denoised inference time (s)":"0.154",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":246.992,
        "# output tokens":3.0,
        "# trials":3
    },
    {
        "Model":"text-davinci-003",
        "EM":0.822,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":246.992,
        "# output tokens":3.0,
        "# trials":3
    },
    {
        "Model":"text-davinci-002",
        "EM":0.822,
        "Denoised inference time (s)":"0.254",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":246.992,
        "# output tokens":3.008,
        "# trials":3
    },
    {
        "Model":"text-curie-001",
        "EM":0.736,
        "Denoised inference time (s)":"0.15",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":246.992,
        "# output tokens":3.089,
        "# trials":3
    },
    {
        "Model":"text-babbage-001",
        "EM":0.547,
        "Denoised inference time (s)":"0.141",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":246.992,
        "# output tokens":2.667,
        "# trials":3
    },
    {
        "Model":"text-ada-001",
        "EM":0.353,
        "Denoised inference time (s)":"0.095",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":246.992,
        "# output tokens":2.519,
        "# trials":3
    },
    {
        "Model":"gpt-3.5-turbo-0301",
        "EM":0.709,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":241.988,
        "# output tokens":2.709,
        "# trials":1
    },
    {
        "Model":"gpt-3.5-turbo-0613",
        "EM":0.826,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":241.988,
        "# output tokens":2.407,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Base-v1 (3B)",
        "EM":0.826,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":256.047,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Instruct-v1 (3B)",
        "EM":0.628,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":256.047,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Base (7B)",
        "EM":0.581,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":256.047,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"RedPajama-INCITE-Instruct (7B)",
        "EM":0.698,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":256.047,
        "# output tokens":5.0,
        "# trials":1
    },
    {
        "Model":"MPT (30B)",
        "EM":0.849,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":256.047,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"MPT-Instruct (30B)",
        "EM":0.791,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":256.047,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon (7B)",
        "EM":0.605,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":256.465,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon-Instruct (7B)",
        "EM":0.826,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":256.465,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon (40B)",
        "EM":0.814,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":256.465,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"Falcon-Instruct (40B)",
        "EM":0.593,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":256.465,
        "# output tokens":1.0,
        "# trials":1
    },
    {
        "Model":"GLM (130B)",
        "EM":0.469,
        "Denoised inference time (s)":"0.619",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":308.612,
        "# output tokens":4.314,
        "# trials":3
    },
    {
        "Model":"InstructPalmyra (30B)",
        "EM":0.829,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":246.992,
        "# output tokens":3.016,
        "# trials":3
    },
    {
        "Model":"Palmyra X (43B)",
        "EM":0.833,
        "Denoised inference time (s)":"-",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":246.992,
        "# output tokens":2.884,
        "# trials":3
    },
    {
        "Model":"YaLM (100B)",
        "EM":0.105,
        "Denoised inference time (s)":"0.436",
        "# eval":86,
        "# train":5,
        "truncated":0,
        "# prompt tokens":235.244,
        "# output tokens":5.0,
        "# trials":3
    }
]