[
    {
        "Model":"J1-Jumbo v1 (178B)",
        "Mean win rate":"0.607",
        "MMLU - ECE (10-bin)":"0.131",
        "BoolQ - ECE (10-bin)":"0.215",
        "NarrativeQA - ECE (10-bin)":"0.034",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.035",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.065",
        "QuAC - ECE (10-bin)":"0.043",
        "HellaSwag - ECE (10-bin)":"0.217",
        "OpenbookQA - ECE (10-bin)":"0.25",
        "TruthfulQA - ECE (10-bin)":"0.113"
    },
    {
        "Model":"J1-Large v1 (7.5B)",
        "Mean win rate":"0.701",
        "MMLU - ECE (10-bin)":"0.123",
        "BoolQ - ECE (10-bin)":"0.106",
        "NarrativeQA - ECE (10-bin)":"0.046",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.015",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.086",
        "QuAC - ECE (10-bin)":"0.024",
        "HellaSwag - ECE (10-bin)":"0.192",
        "OpenbookQA - ECE (10-bin)":"0.25",
        "TruthfulQA - ECE (10-bin)":"0.112"
    },
    {
        "Model":"J1-Grande v1 (17B)",
        "Mean win rate":"0.651",
        "MMLU - ECE (10-bin)":"0.114",
        "BoolQ - ECE (10-bin)":"0.154",
        "NarrativeQA - ECE (10-bin)":"0.047",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.029",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.081",
        "QuAC - ECE (10-bin)":"0.036",
        "HellaSwag - ECE (10-bin)":"0.213",
        "OpenbookQA - ECE (10-bin)":"0.258",
        "TruthfulQA - ECE (10-bin)":"0.091"
    },
    {
        "Model":"J1-Grande v2 beta (17B)",
        "Mean win rate":"0.641",
        "MMLU - ECE (10-bin)":"0.139",
        "BoolQ - ECE (10-bin)":"0.167",
        "NarrativeQA - ECE (10-bin)":"0.041",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.036",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.065",
        "QuAC - ECE (10-bin)":"0.04",
        "HellaSwag - ECE (10-bin)":"0.226",
        "OpenbookQA - ECE (10-bin)":"0.215",
        "TruthfulQA - ECE (10-bin)":"0.123"
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "Mean win rate":"0.658",
        "MMLU - ECE (10-bin)":"0.137",
        "BoolQ - ECE (10-bin)":"0.175",
        "NarrativeQA - ECE (10-bin)":"0.073",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.018",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.073",
        "QuAC - ECE (10-bin)":"0.035",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.068"
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "Mean win rate":"0.621",
        "MMLU - ECE (10-bin)":"0.134",
        "BoolQ - ECE (10-bin)":"0.209",
        "NarrativeQA - ECE (10-bin)":"0.126",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.018",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.063",
        "QuAC - ECE (10-bin)":"0.035",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.097"
    },
    {
        "Model":"Jurassic-2 Large (7.5B)",
        "Mean win rate":"0.598",
        "MMLU - ECE (10-bin)":"0.141",
        "BoolQ - ECE (10-bin)":"0.147",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.014",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.084",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.102"
    },
    {
        "Model":"Luminous Base (13B)",
        "Mean win rate":"0.695",
        "MMLU - ECE (10-bin)":"0.111",
        "BoolQ - ECE (10-bin)":"0.066",
        "NarrativeQA - ECE (10-bin)":"0.048",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.045",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.07",
        "QuAC - ECE (10-bin)":"0.098",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.081"
    },
    {
        "Model":"Luminous Extended (30B)",
        "Mean win rate":"0.634",
        "MMLU - ECE (10-bin)":"0.135",
        "BoolQ - ECE (10-bin)":"0.129",
        "NarrativeQA - ECE (10-bin)":"0.046",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.022",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.09",
        "QuAC - ECE (10-bin)":"0.096",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.064"
    },
    {
        "Model":"Luminous Supreme (70B)",
        "Mean win rate":"0.589",
        "MMLU - ECE (10-bin)":"0.154",
        "BoolQ - ECE (10-bin)":"0.083",
        "NarrativeQA - ECE (10-bin)":"0.049",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.041",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.074",
        "QuAC - ECE (10-bin)":"0.058",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.092"
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-"
    },
    {
        "Model":"BLOOM (176B)",
        "Mean win rate":"0.341",
        "MMLU - ECE (10-bin)":"0.137",
        "BoolQ - ECE (10-bin)":"0.209",
        "NarrativeQA - ECE (10-bin)":"0.237",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.116",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.347",
        "QuAC - ECE (10-bin)":"0.122",
        "HellaSwag - ECE (10-bin)":"0.293",
        "OpenbookQA - ECE (10-bin)":"0.248",
        "TruthfulQA - ECE (10-bin)":"0.096"
    },
    {
        "Model":"T0pp (11B)",
        "Mean win rate":"0.741",
        "MMLU - ECE (10-bin)":"0.168",
        "BoolQ - ECE (10-bin)":"0.322",
        "NarrativeQA - ECE (10-bin)":"0",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0",
        "QuAC - ECE (10-bin)":"0.001",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.154"
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B)",
        "Mean win rate":"0.506",
        "MMLU - ECE (10-bin)":"0.149",
        "BoolQ - ECE (10-bin)":"0.04",
        "NarrativeQA - ECE (10-bin)":"0.062",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.068",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.085",
        "QuAC - ECE (10-bin)":"0.067",
        "HellaSwag - ECE (10-bin)":"0.341",
        "OpenbookQA - ECE (10-bin)":"0.235",
        "TruthfulQA - ECE (10-bin)":"0.099"
    },
    {
        "Model":"Cohere large v20220720 (13.1B)",
        "Mean win rate":"0.691",
        "MMLU - ECE (10-bin)":"0.112",
        "BoolQ - ECE (10-bin)":"0.088",
        "NarrativeQA - ECE (10-bin)":"0.037",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.025",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.143",
        "QuAC - ECE (10-bin)":"0.033",
        "HellaSwag - ECE (10-bin)":"0.288",
        "OpenbookQA - ECE (10-bin)":"0.225",
        "TruthfulQA - ECE (10-bin)":"0.105"
    },
    {
        "Model":"Cohere medium v20220720 (6.1B)",
        "Mean win rate":"0.614",
        "MMLU - ECE (10-bin)":"0.114",
        "BoolQ - ECE (10-bin)":"0.082",
        "NarrativeQA - ECE (10-bin)":"0.047",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.026",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.142",
        "QuAC - ECE (10-bin)":"0.048",
        "HellaSwag - ECE (10-bin)":"0.271",
        "OpenbookQA - ECE (10-bin)":"0.275",
        "TruthfulQA - ECE (10-bin)":"0.094"
    },
    {
        "Model":"Cohere small v20220720 (410M)",
        "Mean win rate":"0.642",
        "MMLU - ECE (10-bin)":"0.136",
        "BoolQ - ECE (10-bin)":"0.095",
        "NarrativeQA - ECE (10-bin)":"0.031",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.023",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.198",
        "QuAC - ECE (10-bin)":"0.036",
        "HellaSwag - ECE (10-bin)":"0.083",
        "OpenbookQA - ECE (10-bin)":"0.379",
        "TruthfulQA - ECE (10-bin)":"0.076"
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B)",
        "Mean win rate":"0.544",
        "MMLU - ECE (10-bin)":"0.143",
        "BoolQ - ECE (10-bin)":"0.051",
        "NarrativeQA - ECE (10-bin)":"0.059",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.054",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.073",
        "QuAC - ECE (10-bin)":"0.063",
        "HellaSwag - ECE (10-bin)":"0.333",
        "OpenbookQA - ECE (10-bin)":"0.207",
        "TruthfulQA - ECE (10-bin)":"0.211"
    },
    {
        "Model":"Cohere medium v20221108 (6.1B)",
        "Mean win rate":"0.716",
        "MMLU - ECE (10-bin)":"0.113",
        "BoolQ - ECE (10-bin)":"0.095",
        "NarrativeQA - ECE (10-bin)":"0.028",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.015",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.233",
        "QuAC - ECE (10-bin)":"0.041",
        "HellaSwag - ECE (10-bin)":"0.281",
        "OpenbookQA - ECE (10-bin)":"0.23",
        "TruthfulQA - ECE (10-bin)":"0.08"
    },
    {
        "Model":"Cohere Command beta (6.1B)",
        "Mean win rate":"0.483",
        "MMLU - ECE (10-bin)":"0.155",
        "BoolQ - ECE (10-bin)":"0.059",
        "NarrativeQA - ECE (10-bin)":"0.076",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.042",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.057",
        "QuAC - ECE (10-bin)":"0.062",
        "HellaSwag - ECE (10-bin)":"0.293",
        "OpenbookQA - ECE (10-bin)":"0.25",
        "TruthfulQA - ECE (10-bin)":"0.3"
    },
    {
        "Model":"Cohere Command beta (52.4B)",
        "Mean win rate":"0.504",
        "MMLU - ECE (10-bin)":"0.183",
        "BoolQ - ECE (10-bin)":"0.023",
        "NarrativeQA - ECE (10-bin)":"0.058",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.084",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.056",
        "QuAC - ECE (10-bin)":"0.06",
        "HellaSwag - ECE (10-bin)":"0.325",
        "OpenbookQA - ECE (10-bin)":"0.231",
        "TruthfulQA - ECE (10-bin)":"0.311"
    },
    {
        "Model":"GPT-J (6B)",
        "Mean win rate":"0.547",
        "MMLU - ECE (10-bin)":"0.115",
        "BoolQ - ECE (10-bin)":"0.062",
        "NarrativeQA - ECE (10-bin)":"0.199",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.075",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.354",
        "QuAC - ECE (10-bin)":"0.13",
        "HellaSwag - ECE (10-bin)":"0.233",
        "OpenbookQA - ECE (10-bin)":"0.235",
        "TruthfulQA - ECE (10-bin)":"0.078"
    },
    {
        "Model":"GPT-NeoX (20B)",
        "Mean win rate":"0.467",
        "MMLU - ECE (10-bin)":"0.122",
        "BoolQ - ECE (10-bin)":"0.195",
        "NarrativeQA - ECE (10-bin)":"0.224",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.103",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.373",
        "QuAC - ECE (10-bin)":"0.115",
        "HellaSwag - ECE (10-bin)":"0.277",
        "OpenbookQA - ECE (10-bin)":"0.232",
        "TruthfulQA - ECE (10-bin)":"0.058"
    },
    {
        "Model":"Pythia (6.9B)",
        "Mean win rate":"0.457",
        "MMLU - ECE (10-bin)":"0.136",
        "BoolQ - ECE (10-bin)":"0.106",
        "NarrativeQA - ECE (10-bin)":"0.217",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.07",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.369",
        "QuAC - ECE (10-bin)":"0.1",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.076"
    },
    {
        "Model":"Pythia (12B)",
        "Mean win rate":"0.41",
        "MMLU - ECE (10-bin)":"0.111",
        "BoolQ - ECE (10-bin)":"0.14",
        "NarrativeQA - ECE (10-bin)":"0.239",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.094",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.39",
        "QuAC - ECE (10-bin)":"0.138",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.094"
    },
    {
        "Model":"T5 (11B)",
        "Mean win rate":"0.486",
        "MMLU - ECE (10-bin)":"0.151",
        "BoolQ - ECE (10-bin)":"0.433",
        "NarrativeQA - ECE (10-bin)":"0",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.076",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.239",
        "QuAC - ECE (10-bin)":"0",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.143"
    },
    {
        "Model":"UL2 (20B)",
        "Mean win rate":"0.544",
        "MMLU - ECE (10-bin)":"0.134",
        "BoolQ - ECE (10-bin)":"0.46",
        "NarrativeQA - ECE (10-bin)":"0",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.092",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.179",
        "QuAC - ECE (10-bin)":"0",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.125"
    },
    {
        "Model":"OPT (175B)",
        "Mean win rate":"0.35",
        "MMLU - ECE (10-bin)":"0.147",
        "BoolQ - ECE (10-bin)":"0.194",
        "NarrativeQA - ECE (10-bin)":"0.254",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.173",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.372",
        "QuAC - ECE (10-bin)":"0.148",
        "HellaSwag - ECE (10-bin)":"0.325",
        "OpenbookQA - ECE (10-bin)":"0.209",
        "TruthfulQA - ECE (10-bin)":"0.054"
    },
    {
        "Model":"OPT (66B)",
        "Mean win rate":"0.338",
        "MMLU - ECE (10-bin)":"0.135",
        "BoolQ - ECE (10-bin)":"0.2",
        "NarrativeQA - ECE (10-bin)":"0.245",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.141",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.384",
        "QuAC - ECE (10-bin)":"0.154",
        "HellaSwag - ECE (10-bin)":"0.293",
        "OpenbookQA - ECE (10-bin)":"0.237",
        "TruthfulQA - ECE (10-bin)":"0.073"
    },
    {
        "Model":"LLaMA (7B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-"
    },
    {
        "Model":"LLaMA (13B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-"
    },
    {
        "Model":"LLaMA (30B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-"
    },
    {
        "Model":"LLaMA (65B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-"
    },
    {
        "Model":"Llama 2 (7B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-"
    },
    {
        "Model":"Llama 2 (13B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-"
    },
    {
        "Model":"Llama 2 (70B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-"
    },
    {
        "Model":"Alpaca (7B)",
        "Mean win rate":"0.33",
        "MMLU - ECE (10-bin)":"0.234",
        "BoolQ - ECE (10-bin)":"0.343",
        "NarrativeQA - ECE (10-bin)":"0.046",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.134",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.238",
        "QuAC - ECE (10-bin)":"0.04",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.375"
    },
    {
        "Model":"Vicuna v1.3 (7B)",
        "Mean win rate":"0.2",
        "MMLU - ECE (10-bin)":"0.176",
        "BoolQ - ECE (10-bin)":"0.322",
        "NarrativeQA - ECE (10-bin)":"0.084",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.162",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.413",
        "QuAC - ECE (10-bin)":"0.109",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.227"
    },
    {
        "Model":"Vicuna v1.3 (13B)",
        "Mean win rate":"0.16",
        "MMLU - ECE (10-bin)":"0.194",
        "BoolQ - ECE (10-bin)":"0.159",
        "NarrativeQA - ECE (10-bin)":"0.257",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.202",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.43",
        "QuAC - ECE (10-bin)":"0.103",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.316"
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-"
    },
    {
        "Model":"TNLG v2 (530B)",
        "Mean win rate":"0.547",
        "MMLU - ECE (10-bin)":"0.127",
        "BoolQ - ECE (10-bin)":"0.048",
        "NarrativeQA - ECE (10-bin)":"0.05",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.04",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.075",
        "QuAC - ECE (10-bin)":"0.08",
        "HellaSwag - ECE (10-bin)":"0.322",
        "OpenbookQA - ECE (10-bin)":"0.243",
        "TruthfulQA - ECE (10-bin)":"0.226"
    },
    {
        "Model":"TNLG v2 (6.7B)",
        "Mean win rate":"0.578",
        "MMLU - ECE (10-bin)":"0.132",
        "BoolQ - ECE (10-bin)":"0.065",
        "NarrativeQA - ECE (10-bin)":"0.046",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.031",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.089",
        "QuAC - ECE (10-bin)":"0.056",
        "HellaSwag - ECE (10-bin)":"0.268",
        "OpenbookQA - ECE (10-bin)":"0.282",
        "TruthfulQA - ECE (10-bin)":"0.117"
    },
    {
        "Model":"davinci (175B)",
        "Mean win rate":"0.556",
        "MMLU - ECE (10-bin)":"0.132",
        "BoolQ - ECE (10-bin)":"0.072",
        "NarrativeQA - ECE (10-bin)":"0.067",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.061",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.079",
        "QuAC - ECE (10-bin)":"0.068",
        "HellaSwag - ECE (10-bin)":"0.31",
        "OpenbookQA - ECE (10-bin)":"0.204",
        "TruthfulQA - ECE (10-bin)":"0.211"
    },
    {
        "Model":"curie (6.7B)",
        "Mean win rate":"0.657",
        "MMLU - ECE (10-bin)":"0.138",
        "BoolQ - ECE (10-bin)":"0.079",
        "NarrativeQA - ECE (10-bin)":"0.045",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.017",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.134",
        "QuAC - ECE (10-bin)":"0.043",
        "HellaSwag - ECE (10-bin)":"0.25",
        "OpenbookQA - ECE (10-bin)":"0.26",
        "TruthfulQA - ECE (10-bin)":"0.062"
    },
    {
        "Model":"babbage (1.3B)",
        "Mean win rate":"0.617",
        "MMLU - ECE (10-bin)":"0.14",
        "BoolQ - ECE (10-bin)":"0.068",
        "NarrativeQA - ECE (10-bin)":"0.027",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.016",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.147",
        "QuAC - ECE (10-bin)":"0.045",
        "HellaSwag - ECE (10-bin)":"0.144",
        "OpenbookQA - ECE (10-bin)":"0.3",
        "TruthfulQA - ECE (10-bin)":"0.142"
    },
    {
        "Model":"ada (350M)",
        "Mean win rate":"0.677",
        "MMLU - ECE (10-bin)":"0.128",
        "BoolQ - ECE (10-bin)":"0.067",
        "NarrativeQA - ECE (10-bin)":"0.046",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.028",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.18",
        "QuAC - ECE (10-bin)":"0.039",
        "HellaSwag - ECE (10-bin)":"0.057",
        "OpenbookQA - ECE (10-bin)":"0.346",
        "TruthfulQA - ECE (10-bin)":"0.071"
    },
    {
        "Model":"text-davinci-003",
        "Mean win rate":"0.271",
        "MMLU - ECE (10-bin)":"0.317",
        "BoolQ - ECE (10-bin)":"0.098",
        "NarrativeQA - ECE (10-bin)":"0.37",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.286",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.323",
        "QuAC - ECE (10-bin)":"0.27",
        "HellaSwag - ECE (10-bin)":"0.278",
        "OpenbookQA - ECE (10-bin)":"0.216",
        "TruthfulQA - ECE (10-bin)":"0.348"
    },
    {
        "Model":"text-davinci-002",
        "Mean win rate":"0.313",
        "MMLU - ECE (10-bin)":"0.176",
        "BoolQ - ECE (10-bin)":"0.064",
        "NarrativeQA - ECE (10-bin)":"0.239",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.341",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.242",
        "QuAC - ECE (10-bin)":"0.274",
        "HellaSwag - ECE (10-bin)":"0.286",
        "OpenbookQA - ECE (10-bin)":"0.238",
        "TruthfulQA - ECE (10-bin)":"0.199"
    },
    {
        "Model":"text-curie-001",
        "Mean win rate":"0.229",
        "MMLU - ECE (10-bin)":"0.462",
        "BoolQ - ECE (10-bin)":"0.253",
        "NarrativeQA - ECE (10-bin)":"0.221",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.253",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.216",
        "QuAC - ECE (10-bin)":"0.254",
        "HellaSwag - ECE (10-bin)":"0.153",
        "OpenbookQA - ECE (10-bin)":"0.321",
        "TruthfulQA - ECE (10-bin)":"0.355"
    },
    {
        "Model":"text-babbage-001",
        "Mean win rate":"0.218",
        "MMLU - ECE (10-bin)":"0.311",
        "BoolQ - ECE (10-bin)":"0.344",
        "NarrativeQA - ECE (10-bin)":"0.186",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.522",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.385",
        "QuAC - ECE (10-bin)":"0.24",
        "HellaSwag - ECE (10-bin)":"0.083",
        "OpenbookQA - ECE (10-bin)":"0.362",
        "TruthfulQA - ECE (10-bin)":"0.251"
    },
    {
        "Model":"text-ada-001",
        "Mean win rate":"0.116",
        "MMLU - ECE (10-bin)":"0.506",
        "BoolQ - ECE (10-bin)":"0.346",
        "NarrativeQA - ECE (10-bin)":"0.319",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.764",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.691",
        "QuAC - ECE (10-bin)":"0.268",
        "HellaSwag - ECE (10-bin)":"0.103",
        "OpenbookQA - ECE (10-bin)":"0.487",
        "TruthfulQA - ECE (10-bin)":"0.465"
    },
    {
        "Model":"gpt-3.5-turbo-0301",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-"
    },
    {
        "Model":"gpt-3.5-turbo-0613",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-"
    },
    {
        "Model":"RedPajama-INCITE-Base-v1 (3B)",
        "Mean win rate":"0.48",
        "MMLU - ECE (10-bin)":"0.115",
        "BoolQ - ECE (10-bin)":"0.187",
        "NarrativeQA - ECE (10-bin)":"0.234",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.116",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.345",
        "QuAC - ECE (10-bin)":"0.078",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.048"
    },
    {
        "Model":"RedPajama-INCITE-Instruct-v1 (3B)",
        "Mean win rate":"0.355",
        "MMLU - ECE (10-bin)":"0.124",
        "BoolQ - ECE (10-bin)":"0.141",
        "NarrativeQA - ECE (10-bin)":"0.254",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.12",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.454",
        "QuAC - ECE (10-bin)":"0.1",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.097"
    },
    {
        "Model":"RedPajama-INCITE-Base (7B)",
        "Mean win rate":"0.428",
        "MMLU - ECE (10-bin)":"0.098",
        "BoolQ - ECE (10-bin)":"0.127",
        "NarrativeQA - ECE (10-bin)":"0.276",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.127",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.396",
        "QuAC - ECE (10-bin)":"0.131",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.063"
    },
    {
        "Model":"RedPajama-INCITE-Instruct (7B)",
        "Mean win rate":"0.327",
        "MMLU - ECE (10-bin)":"0.143",
        "BoolQ - ECE (10-bin)":"0.035",
        "NarrativeQA - ECE (10-bin)":"0.247",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.142",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.466",
        "QuAC - ECE (10-bin)":"0.074",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.232"
    },
    {
        "Model":"MPT (30B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-"
    },
    {
        "Model":"MPT-Instruct (30B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-"
    },
    {
        "Model":"Falcon (7B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-"
    },
    {
        "Model":"Falcon-Instruct (7B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-"
    },
    {
        "Model":"Falcon (40B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-"
    },
    {
        "Model":"Falcon-Instruct (40B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-"
    },
    {
        "Model":"GLM (130B)",
        "Mean win rate":"0.721",
        "MMLU - ECE (10-bin)":"0.128",
        "BoolQ - ECE (10-bin)":"0.171",
        "NarrativeQA - ECE (10-bin)":"0.037",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.022",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.076",
        "QuAC - ECE (10-bin)":"0.027",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.088"
    },
    {
        "Model":"InstructPalmyra (30B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-"
    },
    {
        "Model":"Palmyra X (43B)",
        "Mean win rate":"-",
        "MMLU - ECE (10-bin)":"-",
        "BoolQ - ECE (10-bin)":"-",
        "NarrativeQA - ECE (10-bin)":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "NaturalQuestions (open-book) - ECE (10-bin)":"-",
        "QuAC - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-"
    },
    {
        "Model":"YaLM (100B)",
        "Mean win rate":"0.471",
        "MMLU - ECE (10-bin)":"0.708",
        "BoolQ - ECE (10-bin)":"0.147",
        "NarrativeQA - ECE (10-bin)":"0.06",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.02",
        "NaturalQuestions (open-book) - ECE (10-bin)":"0.086",
        "QuAC - ECE (10-bin)":"0.029",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.679"
    }
]