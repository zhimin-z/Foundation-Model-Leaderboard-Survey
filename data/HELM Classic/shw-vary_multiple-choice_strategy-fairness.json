[
    {
        "Model":"Anthropic-LM v4-s3 (52B) [method: multiple_choice_joint]",
        "Mean win rate":0.809,
        "HellaSwag - EM (Fairness)":0.543,
        "OpenbookQA - EM (Fairness)":0.684,
        "TruthfulQA - EM (Fairness)":0.361,
        "MMLU - EM (Fairness)":0.481
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B) [method: multiple_choice_separate_calibrated]",
        "Mean win rate":0.765,
        "HellaSwag - EM (Fairness)":0.593,
        "OpenbookQA - EM (Fairness)":0.558,
        "TruthfulQA - EM (Fairness)":0.332,
        "MMLU - EM (Fairness)":0.408
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B) [method: multiple_choice_separate_original]",
        "Mean win rate":0.662,
        "HellaSwag - EM (Fairness)":0.807,
        "OpenbookQA - EM (Fairness)":0.446,
        "TruthfulQA - EM (Fairness)":0.257,
        "MMLU - EM (Fairness)":0.37
    },
    {
        "Model":"BLOOM (176B) [method: multiple_choice_joint]",
        "Mean win rate":0.176,
        "HellaSwag - EM (Fairness)":0.289,
        "OpenbookQA - EM (Fairness)":0.351,
        "TruthfulQA - EM (Fairness)":0.204,
        "MMLU - EM (Fairness)":0.299
    },
    {
        "Model":"BLOOM (176B) [method: multiple_choice_separate_calibrated]",
        "Mean win rate":0.735,
        "HellaSwag - EM (Fairness)":0.548,
        "OpenbookQA - EM (Fairness)":0.534,
        "TruthfulQA - EM (Fairness)":0.358,
        "MMLU - EM (Fairness)":0.411
    },
    {
        "Model":"BLOOM (176B) [method: multiple_choice_separate_original]",
        "Mean win rate":0.529,
        "HellaSwag - EM (Fairness)":0.744,
        "OpenbookQA - EM (Fairness)":0.448,
        "TruthfulQA - EM (Fairness)":0.242,
        "MMLU - EM (Fairness)":0.345
    },
    {
        "Model":"GPT-J (6B) [method: multiple_choice_joint]",
        "Mean win rate":0.044,
        "HellaSwag - EM (Fairness)":0.265,
        "OpenbookQA - EM (Fairness)":0.282,
        "TruthfulQA - EM (Fairness)":0.196,
        "MMLU - EM (Fairness)":0.249
    },
    {
        "Model":"GPT-J (6B) [method: multiple_choice_separate_calibrated]",
        "Mean win rate":0.603,
        "HellaSwag - EM (Fairness)":0.49,
        "OpenbookQA - EM (Fairness)":0.514,
        "TruthfulQA - EM (Fairness)":0.396,
        "MMLU - EM (Fairness)":0.35
    },
    {
        "Model":"GPT-J (6B) [method: multiple_choice_separate_original]",
        "Mean win rate":0.456,
        "HellaSwag - EM (Fairness)":0.663,
        "OpenbookQA - EM (Fairness)":0.42,
        "TruthfulQA - EM (Fairness)":0.257,
        "MMLU - EM (Fairness)":0.328
    },
    {
        "Model":"GPT-NeoX (20B) [method: multiple_choice_joint]",
        "Mean win rate":0.118,
        "HellaSwag - EM (Fairness)":0.271,
        "OpenbookQA - EM (Fairness)":0.28,
        "TruthfulQA - EM (Fairness)":0.213,
        "MMLU - EM (Fairness)":0.276
    },
    {
        "Model":"GPT-NeoX (20B) [method: multiple_choice_separate_calibrated]",
        "Mean win rate":0.676,
        "HellaSwag - EM (Fairness)":0.514,
        "OpenbookQA - EM (Fairness)":0.524,
        "TruthfulQA - EM (Fairness)":0.37,
        "MMLU - EM (Fairness)":0.371
    },
    {
        "Model":"GPT-NeoX (20B) [method: multiple_choice_separate_original]",
        "Mean win rate":0.529,
        "HellaSwag - EM (Fairness)":0.718,
        "OpenbookQA - EM (Fairness)":0.42,
        "TruthfulQA - EM (Fairness)":0.24,
        "MMLU - EM (Fairness)":0.361
    },
    {
        "Model":"OPT (175B) [method: multiple_choice_joint]",
        "Mean win rate":0.25,
        "HellaSwag - EM (Fairness)":0.305,
        "OpenbookQA - EM (Fairness)":0.341,
        "TruthfulQA - EM (Fairness)":0.251,
        "MMLU - EM (Fairness)":0.318
    },
    {
        "Model":"OPT (175B) [method: multiple_choice_separate_calibrated]",
        "Mean win rate":0.75,
        "HellaSwag - EM (Fairness)":0.548,
        "OpenbookQA - EM (Fairness)":0.586,
        "TruthfulQA - EM (Fairness)":0.405,
        "MMLU - EM (Fairness)":0.35
    },
    {
        "Model":"OPT (175B) [method: multiple_choice_separate_original]",
        "Mean win rate":0.662,
        "HellaSwag - EM (Fairness)":0.791,
        "OpenbookQA - EM (Fairness)":0.446,
        "TruthfulQA - EM (Fairness)":0.292,
        "MMLU - EM (Fairness)":0.353
    },
    {
        "Model":"OPT (66B) [method: multiple_choice_joint]",
        "Mean win rate":0.029,
        "HellaSwag - EM (Fairness)":0.253,
        "OpenbookQA - EM (Fairness)":0.257,
        "TruthfulQA - EM (Fairness)":0.199,
        "MMLU - EM (Fairness)":0.276
    },
    {
        "Model":"OPT (66B) [method: multiple_choice_separate_calibrated]",
        "Mean win rate":0.691,
        "HellaSwag - EM (Fairness)":0.525,
        "OpenbookQA - EM (Fairness)":0.534,
        "TruthfulQA - EM (Fairness)":0.385,
        "MMLU - EM (Fairness)":0.36
    },
    {
        "Model":"OPT (66B) [method: multiple_choice_separate_original]",
        "Mean win rate":0.515,
        "HellaSwag - EM (Fairness)":0.745,
        "OpenbookQA - EM (Fairness)":0.41,
        "TruthfulQA - EM (Fairness)":0.275,
        "MMLU - EM (Fairness)":0.337
    }
]