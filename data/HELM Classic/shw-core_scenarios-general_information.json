[
    {
        "Model":"J1-Jumbo v1 (178B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"396.74",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"694.652",
        "BoolQ - # output tokens":"2",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"2.639",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1692.218",
        "NarrativeQA - # output tokens":"4.514",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"99.377",
        "NaturalQuestions (closed-book) - # output tokens":"5.602",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.666",
        "NaturalQuestions (open-book) - truncated":"0.038",
        "NaturalQuestions (open-book) - # prompt tokens":"1418.457",
        "NaturalQuestions (open-book) - # output tokens":"5.682",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"1.829",
        "QuAC - truncated":"0.001",
        "QuAC - # prompt tokens":"1698.711",
        "QuAC - # output tokens":"26.784",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"62.466",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"4.348",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"355.015",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MS MARCO (regular) - # eval":"1000",
        "MS MARCO (regular) - # train":"2",
        "MS MARCO (regular) - truncated":"0",
        "MS MARCO (regular) - # prompt tokens":"385.636",
        "MS MARCO (regular) - # output tokens":"2.001",
        "MS MARCO (regular) - # trials":"3",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"373.38",
        "MS MARCO (TREC) - # output tokens":"2.047",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1213.032",
        "CNN\/DailyMail - # output tokens":"72.469",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"5",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1133.388",
        "XSUM - # output tokens":"22.013",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.972",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1281.577",
        "IMDB - # output tokens":"2",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"532.602",
        "CivilComments - # output tokens":"2",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.658",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"712.248",
        "RAFT - # output tokens":"3.634",
        "RAFT - # trials":"3"
    },
    {
        "Model":"J1-Large v1 (7.5B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"396.74",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"694.652",
        "BoolQ - # output tokens":"2",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"2.639",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1692.218",
        "NarrativeQA - # output tokens":"5.09",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"99.377",
        "NaturalQuestions (closed-book) - # output tokens":"7.876",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.666",
        "NaturalQuestions (open-book) - truncated":"0.038",
        "NaturalQuestions (open-book) - # prompt tokens":"1418.457",
        "NaturalQuestions (open-book) - # output tokens":"5.946",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"1.829",
        "QuAC - truncated":"0.001",
        "QuAC - # prompt tokens":"1698.711",
        "QuAC - # output tokens":"27.642",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"62.466",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"4.348",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"355.015",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MS MARCO (regular) - # eval":"1000",
        "MS MARCO (regular) - # train":"2",
        "MS MARCO (regular) - truncated":"0",
        "MS MARCO (regular) - # prompt tokens":"385.636",
        "MS MARCO (regular) - # output tokens":"2.072",
        "MS MARCO (regular) - # trials":"3",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"373.38",
        "MS MARCO (TREC) - # output tokens":"2.116",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1213.032",
        "CNN\/DailyMail - # output tokens":"89.614",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"5",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1133.388",
        "XSUM - # output tokens":"21.299",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.972",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1281.577",
        "IMDB - # output tokens":"2",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"532.602",
        "CivilComments - # output tokens":"2",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.658",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"712.248",
        "RAFT - # output tokens":"3.499",
        "RAFT - # trials":"3"
    },
    {
        "Model":"J1-Grande v1 (17B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"396.74",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"694.652",
        "BoolQ - # output tokens":"2",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"2.639",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1692.218",
        "NarrativeQA - # output tokens":"4.528",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"99.377",
        "NaturalQuestions (closed-book) - # output tokens":"5.971",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.666",
        "NaturalQuestions (open-book) - truncated":"0.038",
        "NaturalQuestions (open-book) - # prompt tokens":"1418.457",
        "NaturalQuestions (open-book) - # output tokens":"6.538",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"1.829",
        "QuAC - truncated":"0.001",
        "QuAC - # prompt tokens":"1698.711",
        "QuAC - # output tokens":"27.786",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"62.466",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"4.348",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"355.015",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MS MARCO (regular) - # eval":"1000",
        "MS MARCO (regular) - # train":"2",
        "MS MARCO (regular) - truncated":"0",
        "MS MARCO (regular) - # prompt tokens":"385.636",
        "MS MARCO (regular) - # output tokens":"2.011",
        "MS MARCO (regular) - # trials":"3",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"373.38",
        "MS MARCO (TREC) - # output tokens":"2.023",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1213.032",
        "CNN\/DailyMail - # output tokens":"67.049",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"5",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1133.388",
        "XSUM - # output tokens":"20.468",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.972",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1281.577",
        "IMDB - # output tokens":"2",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"532.602",
        "CivilComments - # output tokens":"2",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.658",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"712.248",
        "RAFT - # output tokens":"3.59",
        "RAFT - # trials":"3"
    },
    {
        "Model":"J1-Grande v2 beta (17B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"396.74",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"694.652",
        "BoolQ - # output tokens":"2",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"2.639",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1692.218",
        "NarrativeQA - # output tokens":"4.6",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"99.377",
        "NaturalQuestions (closed-book) - # output tokens":"5.282",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.666",
        "NaturalQuestions (open-book) - truncated":"0.038",
        "NaturalQuestions (open-book) - # prompt tokens":"1418.457",
        "NaturalQuestions (open-book) - # output tokens":"5.27",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"1.829",
        "QuAC - truncated":"0.001",
        "QuAC - # prompt tokens":"1698.711",
        "QuAC - # output tokens":"23.053",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"62.466",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"4.348",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"355.015",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MS MARCO (regular) - # eval":"1000",
        "MS MARCO (regular) - # train":"2",
        "MS MARCO (regular) - truncated":"0",
        "MS MARCO (regular) - # prompt tokens":"385.636",
        "MS MARCO (regular) - # output tokens":"2.009",
        "MS MARCO (regular) - # trials":"3",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"373.38",
        "MS MARCO (TREC) - # output tokens":"2.023",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1213.032",
        "CNN\/DailyMail - # output tokens":"53.215",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"5",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1133.388",
        "XSUM - # output tokens":"22.092",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.972",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1281.577",
        "IMDB - # output tokens":"2",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"532.602",
        "CivilComments - # output tokens":"2",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.658",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"712.248",
        "RAFT - # output tokens":"3.574",
        "RAFT - # trials":"3"
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"396.74",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"694.652",
        "BoolQ - # output tokens":"2.002",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"5",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"2818.1",
        "NarrativeQA - # output tokens":"6.406",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"99.377",
        "NaturalQuestions (closed-book) - # output tokens":"5.365",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.93",
        "NaturalQuestions (open-book) - truncated":"0.012",
        "NaturalQuestions (open-book) - # prompt tokens":"1571.171",
        "NaturalQuestions (open-book) - # output tokens":"5.113",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"5",
        "QuAC - truncated":"0",
        "QuAC - # prompt tokens":"4018.779",
        "QuAC - # output tokens":"22.178",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"62.466",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"4.348",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"355.015",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MS MARCO (regular) - # eval":"1000",
        "MS MARCO (regular) - # train":"2",
        "MS MARCO (regular) - truncated":"0",
        "MS MARCO (regular) - # prompt tokens":"385.636",
        "MS MARCO (regular) - # output tokens":"2.001",
        "MS MARCO (regular) - # trials":"3",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"373.38",
        "MS MARCO (TREC) - # output tokens":"2",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1213.032",
        "CNN\/DailyMail - # output tokens":"49.239",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"5",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1133.388",
        "XSUM - # output tokens":"22.142",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"5",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1288.518",
        "IMDB - # output tokens":"2",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"532.602",
        "CivilComments - # output tokens":"2",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"5",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"944.157",
        "RAFT - # output tokens":"3.597",
        "RAFT - # trials":"3"
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"396.74",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"694.652",
        "BoolQ - # output tokens":"2.002",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"2.639",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1692.218",
        "NarrativeQA - # output tokens":"5.261",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"99.377",
        "NaturalQuestions (closed-book) - # output tokens":"6.315",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.666",
        "NaturalQuestions (open-book) - truncated":"0.038",
        "NaturalQuestions (open-book) - # prompt tokens":"1418.457",
        "NaturalQuestions (open-book) - # output tokens":"5.676",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"1.829",
        "QuAC - truncated":"0.001",
        "QuAC - # prompt tokens":"1698.711",
        "QuAC - # output tokens":"24.469",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"62.466",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"4.348",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"355.015",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MS MARCO (regular) - # eval":"1000",
        "MS MARCO (regular) - # train":"2",
        "MS MARCO (regular) - truncated":"0",
        "MS MARCO (regular) - # prompt tokens":"385.636",
        "MS MARCO (regular) - # output tokens":"2.006",
        "MS MARCO (regular) - # trials":"3",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"373.38",
        "MS MARCO (TREC) - # output tokens":"2.023",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1213.032",
        "CNN\/DailyMail - # output tokens":"55.762",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"5",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1133.388",
        "XSUM - # output tokens":"21.75",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.972",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1281.577",
        "IMDB - # output tokens":"2",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"532.602",
        "CivilComments - # output tokens":"2",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.658",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"712.248",
        "RAFT - # output tokens":"3.644",
        "RAFT - # trials":"3"
    },
    {
        "Model":"Jurassic-2 Large (7.5B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"396.74",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"694.652",
        "BoolQ - # output tokens":"2",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"-",
        "NarrativeQA - # train":"-",
        "NarrativeQA - truncated":"-",
        "NarrativeQA - # prompt tokens":"-",
        "NarrativeQA - # output tokens":"-",
        "NarrativeQA - # trials":"-",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"99.377",
        "NaturalQuestions (closed-book) - # output tokens":"6.729",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.666",
        "NaturalQuestions (open-book) - truncated":"0.038",
        "NaturalQuestions (open-book) - # prompt tokens":"1418.457",
        "NaturalQuestions (open-book) - # output tokens":"6.311",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"-",
        "QuAC - # train":"-",
        "QuAC - truncated":"-",
        "QuAC - # prompt tokens":"-",
        "QuAC - # output tokens":"-",
        "QuAC - # trials":"-",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"62.466",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"4.348",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"355.015",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MS MARCO (regular) - # eval":"1000",
        "MS MARCO (regular) - # train":"2",
        "MS MARCO (regular) - truncated":"0",
        "MS MARCO (regular) - # prompt tokens":"385.636",
        "MS MARCO (regular) - # output tokens":"2.012",
        "MS MARCO (regular) - # trials":"3",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"373.38",
        "MS MARCO (TREC) - # output tokens":"2.023",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1213.032",
        "CNN\/DailyMail - # output tokens":"58.246",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"5",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1133.388",
        "XSUM - # output tokens":"21.228",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.972",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1281.577",
        "IMDB - # output tokens":"2",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"532.602",
        "CivilComments - # output tokens":"2",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.658",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"712.248",
        "RAFT - # output tokens":"3.562",
        "RAFT - # trials":"3"
    },
    {
        "Model":"Luminous Base (13B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"471.075",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"908.991",
        "BoolQ - # output tokens":"1.002",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.621",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1647.783",
        "NarrativeQA - # output tokens":"6.798",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"111.754",
        "NaturalQuestions (closed-book) - # output tokens":"5.287",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.711",
        "NaturalQuestions (open-book) - truncated":"0.039",
        "NaturalQuestions (open-book) - # prompt tokens":"1384.565",
        "NaturalQuestions (open-book) - # output tokens":"10.15",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.909",
        "QuAC - truncated":"0.033",
        "QuAC - # prompt tokens":"1641.256",
        "QuAC - # output tokens":"23.472",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"514.073",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1578.648",
        "CNN\/DailyMail - # output tokens":"80.866",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"4.999",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1532.912",
        "XSUM - # output tokens":"26.021",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.236",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1560.056",
        "IMDB - # output tokens":"1",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"724.782",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.56",
        "RAFT - truncated":"0.002",
        "RAFT - # prompt tokens":"810.769",
        "RAFT - # output tokens":"2.916",
        "RAFT - # trials":"3"
    },
    {
        "Model":"Luminous Extended (30B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"471.075",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"908.991",
        "BoolQ - # output tokens":"1",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.621",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1647.783",
        "NarrativeQA - # output tokens":"7.042",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"111.754",
        "NaturalQuestions (closed-book) - # output tokens":"6.119",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.711",
        "NaturalQuestions (open-book) - truncated":"0.039",
        "NaturalQuestions (open-book) - # prompt tokens":"1384.565",
        "NaturalQuestions (open-book) - # output tokens":"10.3",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.909",
        "QuAC - truncated":"0.033",
        "QuAC - # prompt tokens":"1641.256",
        "QuAC - # output tokens":"21.144",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"514.073",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1578.648",
        "CNN\/DailyMail - # output tokens":"83.112",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"4.999",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1532.912",
        "XSUM - # output tokens":"25.987",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.236",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1560.056",
        "IMDB - # output tokens":"1",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"724.782",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.56",
        "RAFT - truncated":"0.002",
        "RAFT - # prompt tokens":"810.769",
        "RAFT - # output tokens":"2.796",
        "RAFT - # trials":"3"
    },
    {
        "Model":"Luminous Supreme (70B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"471.075",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"908.991",
        "BoolQ - # output tokens":"1",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.621",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1647.783",
        "NarrativeQA - # output tokens":"6.84",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"111.754",
        "NaturalQuestions (closed-book) - # output tokens":"4.508",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.711",
        "NaturalQuestions (open-book) - truncated":"0.039",
        "NaturalQuestions (open-book) - # prompt tokens":"1384.565",
        "NaturalQuestions (open-book) - # output tokens":"6.362",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.909",
        "QuAC - truncated":"0.033",
        "QuAC - # prompt tokens":"1641.256",
        "QuAC - # output tokens":"26.241",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"514.073",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1578.648",
        "CNN\/DailyMail - # output tokens":"75.51",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"4.999",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1532.912",
        "XSUM - # output tokens":"26.423",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.236",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1560.056",
        "IMDB - # output tokens":"1",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"724.782",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.56",
        "RAFT - truncated":"0.002",
        "RAFT - # prompt tokens":"810.769",
        "RAFT - # output tokens":"3.097",
        "RAFT - # trials":"3"
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"908.406",
        "BoolQ - # output tokens":"1.004",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"5",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"3803.911",
        "NarrativeQA - # output tokens":"6.952",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"5.47",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.964",
        "NaturalQuestions (open-book) - truncated":"0.007",
        "NaturalQuestions (open-book) - # prompt tokens":"1592.701",
        "NaturalQuestions (open-book) - # output tokens":"5.659",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"5",
        "QuAC - truncated":"0",
        "QuAC - # prompt tokens":"5199.788",
        "QuAC - # output tokens":"35.484",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"1.306",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0.132",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"3.75",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"404.621",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"2.5",
        "MS MARCO (regular) - # eval":"1000",
        "MS MARCO (regular) - # train":"2",
        "MS MARCO (regular) - truncated":"0",
        "MS MARCO (regular) - # prompt tokens":"532.565",
        "MS MARCO (regular) - # output tokens":"1.005",
        "MS MARCO (regular) - # trials":"3",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"515.822",
        "MS MARCO (TREC) - # output tokens":"1",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1549.919",
        "CNN\/DailyMail - # output tokens":"58.035",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"5",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1510.735",
        "XSUM - # output tokens":"28.94",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"5",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1422.545",
        "IMDB - # output tokens":"1.014",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"722.635",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"5",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"1279.572",
        "RAFT - # output tokens":"2.986",
        "RAFT - # trials":"3"
    },
    {
        "Model":"BLOOM (176B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"436.99",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"897.107",
        "BoolQ - # output tokens":"5",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.621",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1649.598",
        "NarrativeQA - # output tokens":"33.276",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"96.12",
        "NaturalQuestions (closed-book) - # output tokens":"48.109",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.743",
        "NaturalQuestions (open-book) - truncated":"0.035",
        "NaturalQuestions (open-book) - # prompt tokens":"1313.422",
        "NaturalQuestions (open-book) - # output tokens":"38.803",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.944",
        "QuAC - truncated":"0.017",
        "QuAC - # prompt tokens":"1639.494",
        "QuAC - # output tokens":"90.164",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.875",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.444",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"3.75",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"370.611",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"2.5",
        "MS MARCO (regular) - # eval":"1000",
        "MS MARCO (regular) - # train":"2",
        "MS MARCO (regular) - truncated":"0",
        "MS MARCO (regular) - # prompt tokens":"524.472",
        "MS MARCO (regular) - # output tokens":"5",
        "MS MARCO (regular) - # trials":"3",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"506.814",
        "MS MARCO (TREC) - # output tokens":"5",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1541.33",
        "CNN\/DailyMail - # output tokens":"117.435",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"5",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1501.338",
        "XSUM - # output tokens":"54.066",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.943",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1375.21",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"683.498",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.567",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"779.203",
        "RAFT - # output tokens":"7.127",
        "RAFT - # trials":"3"
    },
    {
        "Model":"T0pp (11B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"492.01",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"3.972",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"702.438",
        "BoolQ - # output tokens":"5",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"0.187",
        "NarrativeQA - truncated":"0.372",
        "NarrativeQA - # prompt tokens":"877.742",
        "NarrativeQA - # output tokens":"100",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"113.556",
        "NaturalQuestions (closed-book) - # output tokens":"300",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"3.396",
        "NaturalQuestions (open-book) - truncated":"0.057",
        "NaturalQuestions (open-book) - # prompt tokens":"903.877",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0",
        "QuAC - truncated":"0.985",
        "QuAC - # prompt tokens":"823.365",
        "QuAC - # output tokens":"100",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"3.75",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"391.646",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"2.5",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"1.335",
        "CNN\/DailyMail - truncated":"0.004",
        "CNN\/DailyMail - # prompt tokens":"886.838",
        "CNN\/DailyMail - # output tokens":"128",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"2.068",
        "XSUM - truncated":"0.01",
        "XSUM - # prompt tokens":"907.769",
        "XSUM - # output tokens":"64",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"2.44",
        "IMDB - truncated":"0.03",
        "IMDB - # prompt tokens":"910.174",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"4.861",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"744.109",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"3.913",
        "RAFT - truncated":"0.09",
        "RAFT - # prompt tokens":"650.012",
        "RAFT - # output tokens":"30",
        "RAFT - # trials":"3"
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"481.26",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"925.307",
        "BoolQ - # output tokens":"1.001",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.562",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1634.99",
        "NarrativeQA - # output tokens":"7.077",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"111.191",
        "NaturalQuestions (closed-book) - # output tokens":"5.844",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.633",
        "NaturalQuestions (open-book) - truncated":"0.039",
        "NaturalQuestions (open-book) - # prompt tokens":"1481.344",
        "NaturalQuestions (open-book) - # output tokens":"8.834",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.881",
        "QuAC - truncated":"0.02",
        "QuAC - # prompt tokens":"1639.784",
        "QuAC - # output tokens":"32.717",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"514.648",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MS MARCO (regular) - # eval":"1000",
        "MS MARCO (regular) - # train":"2",
        "MS MARCO (regular) - truncated":"0",
        "MS MARCO (regular) - # prompt tokens":"536.614",
        "MS MARCO (regular) - # output tokens":"1",
        "MS MARCO (regular) - # trials":"3",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"519.496",
        "MS MARCO (TREC) - # output tokens":"1",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1575.036",
        "CNN\/DailyMail - # output tokens":"89.431",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"4.998",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1537.452",
        "XSUM - # output tokens":"24.802",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.229",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1562.808",
        "IMDB - # output tokens":"1",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"732.514",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.557",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"814.446",
        "RAFT - # output tokens":"3.051",
        "RAFT - # trials":"3"
    },
    {
        "Model":"Cohere large v20220720 (13.1B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"481.26",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"925.307",
        "BoolQ - # output tokens":"1",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.562",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1634.99",
        "NarrativeQA - # output tokens":"6.91",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"111.191",
        "NaturalQuestions (closed-book) - # output tokens":"5.625",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.633",
        "NaturalQuestions (open-book) - truncated":"0.039",
        "NaturalQuestions (open-book) - # prompt tokens":"1481.344",
        "NaturalQuestions (open-book) - # output tokens":"10.443",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.881",
        "QuAC - truncated":"0.02",
        "QuAC - # prompt tokens":"1639.784",
        "QuAC - # output tokens":"30.036",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"514.648",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MS MARCO (regular) - # eval":"1000",
        "MS MARCO (regular) - # train":"2",
        "MS MARCO (regular) - truncated":"0",
        "MS MARCO (regular) - # prompt tokens":"536.614",
        "MS MARCO (regular) - # output tokens":"1.025",
        "MS MARCO (regular) - # trials":"3",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"519.496",
        "MS MARCO (TREC) - # output tokens":"1.031",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1575.036",
        "CNN\/DailyMail - # output tokens":"74.505",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"4.998",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1537.452",
        "XSUM - # output tokens":"22.992",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.93",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1398.654",
        "IMDB - # output tokens":"1",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"732.514",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.557",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"814.446",
        "RAFT - # output tokens":"3.02",
        "RAFT - # trials":"3"
    },
    {
        "Model":"Cohere medium v20220720 (6.1B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"481.26",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"925.307",
        "BoolQ - # output tokens":"1",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.562",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1634.99",
        "NarrativeQA - # output tokens":"6.771",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"111.191",
        "NaturalQuestions (closed-book) - # output tokens":"5.267",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.633",
        "NaturalQuestions (open-book) - truncated":"0.039",
        "NaturalQuestions (open-book) - # prompt tokens":"1481.344",
        "NaturalQuestions (open-book) - # output tokens":"9.101",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.881",
        "QuAC - truncated":"0.02",
        "QuAC - # prompt tokens":"1639.784",
        "QuAC - # output tokens":"23.531",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"514.648",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MS MARCO (regular) - # eval":"1000",
        "MS MARCO (regular) - # train":"2",
        "MS MARCO (regular) - truncated":"0",
        "MS MARCO (regular) - # prompt tokens":"536.614",
        "MS MARCO (regular) - # output tokens":"1.005",
        "MS MARCO (regular) - # trials":"3",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"519.496",
        "MS MARCO (TREC) - # output tokens":"1.016",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1575.036",
        "CNN\/DailyMail - # output tokens":"63.193",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"4.998",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1537.452",
        "XSUM - # output tokens":"24.055",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.229",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1562.808",
        "IMDB - # output tokens":"1.003",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"732.514",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.557",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"814.446",
        "RAFT - # output tokens":"2.965",
        "RAFT - # trials":"3"
    },
    {
        "Model":"Cohere small v20220720 (410M)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"481.26",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"925.307",
        "BoolQ - # output tokens":"1.001",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.562",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1634.99",
        "NarrativeQA - # output tokens":"11.007",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"111.191",
        "NaturalQuestions (closed-book) - # output tokens":"5.149",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.633",
        "NaturalQuestions (open-book) - truncated":"0.039",
        "NaturalQuestions (open-book) - # prompt tokens":"1481.344",
        "NaturalQuestions (open-book) - # output tokens":"22.835",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.881",
        "QuAC - truncated":"0.02",
        "QuAC - # prompt tokens":"1639.784",
        "QuAC - # output tokens":"20.639",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"514.648",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"519.496",
        "MS MARCO (TREC) - # output tokens":"1.031",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1575.036",
        "CNN\/DailyMail - # output tokens":"78.352",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"4.998",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1537.452",
        "XSUM - # output tokens":"27.394",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.93",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1398.654",
        "IMDB - # output tokens":"1",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"732.514",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.557",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"814.446",
        "RAFT - # output tokens":"3.239",
        "RAFT - # trials":"3"
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"481.26",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"925.307",
        "BoolQ - # output tokens":"1",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.562",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1634.99",
        "NarrativeQA - # output tokens":"6.729",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"111.191",
        "NaturalQuestions (closed-book) - # output tokens":"4.808",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.633",
        "NaturalQuestions (open-book) - truncated":"0.039",
        "NaturalQuestions (open-book) - # prompt tokens":"1481.344",
        "NaturalQuestions (open-book) - # output tokens":"6.093",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.881",
        "QuAC - truncated":"0.02",
        "QuAC - # prompt tokens":"1639.784",
        "QuAC - # output tokens":"27.944",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"514.648",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MS MARCO (regular) - # eval":"1000",
        "MS MARCO (regular) - # train":"2",
        "MS MARCO (regular) - truncated":"0",
        "MS MARCO (regular) - # prompt tokens":"536.614",
        "MS MARCO (regular) - # output tokens":"1.002",
        "MS MARCO (regular) - # trials":"3",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"519.496",
        "MS MARCO (TREC) - # output tokens":"1",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1575.036",
        "CNN\/DailyMail - # output tokens":"91.338",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"4.998",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1537.452",
        "XSUM - # output tokens":"26.153",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.229",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1562.808",
        "IMDB - # output tokens":"1",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"732.514",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.557",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"814.446",
        "RAFT - # output tokens":"2.99",
        "RAFT - # trials":"3"
    },
    {
        "Model":"Cohere medium v20221108 (6.1B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"481.26",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"925.307",
        "BoolQ - # output tokens":"1",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.562",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1634.99",
        "NarrativeQA - # output tokens":"7.144",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"111.191",
        "NaturalQuestions (closed-book) - # output tokens":"6.745",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.633",
        "NaturalQuestions (open-book) - truncated":"0.039",
        "NaturalQuestions (open-book) - # prompt tokens":"1481.344",
        "NaturalQuestions (open-book) - # output tokens":"8.419",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.881",
        "QuAC - truncated":"0.02",
        "QuAC - # prompt tokens":"1639.784",
        "QuAC - # output tokens":"22.84",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"514.648",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MS MARCO (regular) - # eval":"1000",
        "MS MARCO (regular) - # train":"2",
        "MS MARCO (regular) - truncated":"0",
        "MS MARCO (regular) - # prompt tokens":"536.614",
        "MS MARCO (regular) - # output tokens":"1.005",
        "MS MARCO (regular) - # trials":"3",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"519.496",
        "MS MARCO (TREC) - # output tokens":"1",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1575.036",
        "CNN\/DailyMail - # output tokens":"68.601",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"4.998",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1537.452",
        "XSUM - # output tokens":"23.626",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.229",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1562.808",
        "IMDB - # output tokens":"1.003",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"732.514",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.557",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"814.446",
        "RAFT - # output tokens":"3.038",
        "RAFT - # trials":"3"
    },
    {
        "Model":"Cohere Command beta (6.1B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"481.26",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"925.307",
        "BoolQ - # output tokens":"1",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.508",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1600.684",
        "NarrativeQA - # output tokens":"5.807",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"111.191",
        "NaturalQuestions (closed-book) - # output tokens":"4.687",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.602",
        "NaturalQuestions (open-book) - truncated":"0.039",
        "NaturalQuestions (open-book) - # prompt tokens":"1471.073",
        "NaturalQuestions (open-book) - # output tokens":"7.377",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.848",
        "QuAC - truncated":"0.022",
        "QuAC - # prompt tokens":"1610.503",
        "QuAC - # output tokens":"17.394",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"1",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"1",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"514.648",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MS MARCO (regular) - # eval":"1000",
        "MS MARCO (regular) - # train":"2",
        "MS MARCO (regular) - truncated":"0",
        "MS MARCO (regular) - # prompt tokens":"536.614",
        "MS MARCO (regular) - # output tokens":"1",
        "MS MARCO (regular) - # trials":"3",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"519.496",
        "MS MARCO (TREC) - # output tokens":"1",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1575.036",
        "CNN\/DailyMail - # output tokens":"73.723",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"4.997",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1537.293",
        "XSUM - # output tokens":"23.421",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.217",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1557.741",
        "IMDB - # output tokens":"1",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"732.514",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.554",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"813.265",
        "RAFT - # output tokens":"3.148",
        "RAFT - # trials":"3"
    },
    {
        "Model":"Cohere Command beta (52.4B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"481.26",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"925.307",
        "BoolQ - # output tokens":"1",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.508",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1600.684",
        "NarrativeQA - # output tokens":"5.992",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"111.191",
        "NaturalQuestions (closed-book) - # output tokens":"4.325",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.602",
        "NaturalQuestions (open-book) - truncated":"0.039",
        "NaturalQuestions (open-book) - # prompt tokens":"1471.073",
        "NaturalQuestions (open-book) - # output tokens":"7.288",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.848",
        "QuAC - truncated":"0.022",
        "QuAC - # prompt tokens":"1610.503",
        "QuAC - # output tokens":"19.627",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.855",
        "HellaSwag - # output tokens":"1",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.358",
        "OpenbookQA - # output tokens":"1",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"514.648",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MS MARCO (regular) - # eval":"1000",
        "MS MARCO (regular) - # train":"2",
        "MS MARCO (regular) - truncated":"0",
        "MS MARCO (regular) - # prompt tokens":"536.614",
        "MS MARCO (regular) - # output tokens":"1",
        "MS MARCO (regular) - # trials":"3",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"519.496",
        "MS MARCO (TREC) - # output tokens":"1",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1575.036",
        "CNN\/DailyMail - # output tokens":"74.406",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"4.997",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1537.293",
        "XSUM - # output tokens":"24.351",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.217",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1557.741",
        "IMDB - # output tokens":"1",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"732.514",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.554",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"813.265",
        "RAFT - # output tokens":"3.15",
        "RAFT - # trials":"3"
    },
    {
        "Model":"GPT-J (6B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"908.406",
        "BoolQ - # output tokens":"5",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.647",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1652.377",
        "NarrativeQA - # output tokens":"56.052",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"282.837",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.691",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1419.574",
        "NaturalQuestions (open-book) - # output tokens":"247.23",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.944",
        "QuAC - truncated":"0.016",
        "QuAC - # prompt tokens":"1644.831",
        "QuAC - # output tokens":"68.54",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"3.75",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"404.621",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"2.5",
        "MS MARCO (regular) - # eval":"1000",
        "MS MARCO (regular) - # train":"2",
        "MS MARCO (regular) - truncated":"0",
        "MS MARCO (regular) - # prompt tokens":"532.565",
        "MS MARCO (regular) - # output tokens":"5",
        "MS MARCO (regular) - # trials":"3",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"515.822",
        "MS MARCO (TREC) - # output tokens":"5",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1549.919",
        "CNN\/DailyMail - # output tokens":"83.931",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"4.999",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1510.418",
        "XSUM - # output tokens":"25.529",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.933",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1389.454",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"722.635",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.556",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"812.938",
        "RAFT - # output tokens":"14.276",
        "RAFT - # trials":"3"
    },
    {
        "Model":"GPT-NeoX (20B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"467.936",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"913.897",
        "BoolQ - # output tokens":"5",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.568",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1641.033",
        "NarrativeQA - # output tokens":"40.047",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.966",
        "NaturalQuestions (closed-book) - # output tokens":"90.195",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.704",
        "NaturalQuestions (open-book) - truncated":"0.037",
        "NaturalQuestions (open-book) - # prompt tokens":"1394.229",
        "NaturalQuestions (open-book) - # output tokens":"87.693",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.889",
        "QuAC - truncated":"0.021",
        "QuAC - # prompt tokens":"1640.361",
        "QuAC - # output tokens":"77.489",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"88.806",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.346",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"3.75",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"406.102",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"2.5",
        "MS MARCO (regular) - # eval":"1000",
        "MS MARCO (regular) - # train":"2",
        "MS MARCO (regular) - truncated":"0",
        "MS MARCO (regular) - # prompt tokens":"537.908",
        "MS MARCO (regular) - # output tokens":"5",
        "MS MARCO (regular) - # trials":"3",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"519.473",
        "MS MARCO (TREC) - # output tokens":"5",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1582.608",
        "CNN\/DailyMail - # output tokens":"80.409",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"4.997",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1545.148",
        "XSUM - # output tokens":"25.402",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.93",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1398.09",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"726.728",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.56",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"807.97",
        "RAFT - # output tokens":"13.945",
        "RAFT - # trials":"3"
    },
    {
        "Model":"Pythia (6.9B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"467.936",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"1251.897",
        "BoolQ - # output tokens":"5",
        "BoolQ - # trials":"1",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.969",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1691.082",
        "NarrativeQA - # output tokens":"100",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"117.299",
        "NaturalQuestions (closed-book) - # output tokens":"300",
        "NaturalQuestions (closed-book) - # trials":"1",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.704",
        "NaturalQuestions (open-book) - truncated":"0.037",
        "NaturalQuestions (open-book) - # prompt tokens":"1495.552",
        "NaturalQuestions (open-book) - # output tokens":"299.883",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.883",
        "QuAC - truncated":"0.021",
        "QuAC - # prompt tokens":"1655.708",
        "QuAC - # output tokens":"100",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"505.352",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"-",
        "CNN\/DailyMail - # train":"-",
        "CNN\/DailyMail - truncated":"-",
        "CNN\/DailyMail - # prompt tokens":"-",
        "CNN\/DailyMail - # output tokens":"-",
        "CNN\/DailyMail - # trials":"-",
        "XSUM - # eval":"-",
        "XSUM - # train":"-",
        "XSUM - truncated":"-",
        "XSUM - # prompt tokens":"-",
        "XSUM - # output tokens":"-",
        "XSUM - # trials":"-",
        "IMDB - # eval":"1000",
        "IMDB - # train":"2.911",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1619.568",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"771.654",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"1",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.605",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"869.691",
        "RAFT - # output tokens":"30",
        "RAFT - # trials":"1"
    },
    {
        "Model":"Pythia (12B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"467.936",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"1251.897",
        "BoolQ - # output tokens":"5",
        "BoolQ - # trials":"1",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.969",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1691.082",
        "NarrativeQA - # output tokens":"100",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"117.299",
        "NaturalQuestions (closed-book) - # output tokens":"300",
        "NaturalQuestions (closed-book) - # trials":"1",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.704",
        "NaturalQuestions (open-book) - truncated":"0.037",
        "NaturalQuestions (open-book) - # prompt tokens":"1495.552",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.883",
        "QuAC - truncated":"0.021",
        "QuAC - # prompt tokens":"1655.708",
        "QuAC - # output tokens":"100",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"505.352",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"-",
        "CNN\/DailyMail - # train":"-",
        "CNN\/DailyMail - truncated":"-",
        "CNN\/DailyMail - # prompt tokens":"-",
        "CNN\/DailyMail - # output tokens":"-",
        "CNN\/DailyMail - # trials":"-",
        "XSUM - # eval":"-",
        "XSUM - # train":"-",
        "XSUM - truncated":"-",
        "XSUM - # prompt tokens":"-",
        "XSUM - # output tokens":"-",
        "XSUM - # trials":"-",
        "IMDB - # eval":"1000",
        "IMDB - # train":"2.911",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1619.568",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"771.654",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"1",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.605",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"869.691",
        "RAFT - # output tokens":"30",
        "RAFT - # trials":"1"
    },
    {
        "Model":"T5 (11B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"4.326",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"420.562",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"1.588",
        "BoolQ - truncated":"0.004",
        "BoolQ - # prompt tokens":"401.944",
        "BoolQ - # output tokens":"5",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"0",
        "NarrativeQA - truncated":"0.825",
        "NarrativeQA - # prompt tokens":"492.141",
        "NarrativeQA - # output tokens":"100",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"113.556",
        "NaturalQuestions (closed-book) - # output tokens":"300",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0.924",
        "NaturalQuestions (open-book) - truncated":"0.349",
        "NaturalQuestions (open-book) - # prompt tokens":"301.907",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0",
        "QuAC - truncated":"0.999",
        "QuAC - # prompt tokens":"510.923",
        "QuAC - # output tokens":"100",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"3.547",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"371.92",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"2.5",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"0.064",
        "CNN\/DailyMail - truncated":"0.932",
        "CNN\/DailyMail - # prompt tokens":"500.553",
        "CNN\/DailyMail - # output tokens":"128",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"0.3",
        "XSUM - truncated":"0.671",
        "XSUM - # prompt tokens":"436.826",
        "XSUM - # output tokens":"64",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"0.466",
        "IMDB - truncated":"0.173",
        "IMDB - # prompt tokens":"408.425",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"2.636",
        "CivilComments - truncated":"0.002",
        "CivilComments - # prompt tokens":"416.791",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"2.433",
        "RAFT - truncated":"0.394",
        "RAFT - # prompt tokens":"420.742",
        "RAFT - # output tokens":"30",
        "RAFT - # trials":"3"
    },
    {
        "Model":"UL2 (20B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"4.316",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"423.395",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"1.57",
        "BoolQ - truncated":"0.004",
        "BoolQ - # prompt tokens":"402.285",
        "BoolQ - # output tokens":"5",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"0",
        "NarrativeQA - truncated":"0.834",
        "NarrativeQA - # prompt tokens":"492.876",
        "NarrativeQA - # output tokens":"100",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"117.556",
        "NaturalQuestions (closed-book) - # output tokens":"300",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0.918",
        "NaturalQuestions (open-book) - truncated":"0.355",
        "NaturalQuestions (open-book) - # prompt tokens":"303.619",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0",
        "QuAC - truncated":"0.999",
        "QuAC - # prompt tokens":"510.938",
        "QuAC - # output tokens":"100",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"3.513",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"372.668",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"2.5",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"0.061",
        "CNN\/DailyMail - truncated":"0.935",
        "CNN\/DailyMail - # prompt tokens":"500.829",
        "CNN\/DailyMail - # output tokens":"128",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"0.293",
        "XSUM - truncated":"0.677",
        "XSUM - # prompt tokens":"437.97",
        "XSUM - # output tokens":"64",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"0.449",
        "IMDB - truncated":"0.176",
        "IMDB - # prompt tokens":"407.098",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"2.608",
        "CivilComments - truncated":"0.003",
        "CivilComments - # prompt tokens":"416.896",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"2.433",
        "RAFT - truncated":"0.394",
        "RAFT - # prompt tokens":"423.537",
        "RAFT - # output tokens":"30",
        "RAFT - # trials":"3"
    },
    {
        "Model":"OPT (175B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"908.406",
        "BoolQ - # output tokens":"5",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.647",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1652.377",
        "NarrativeQA - # output tokens":"40.781",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"278.02",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.691",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1419.574",
        "NaturalQuestions (open-book) - # output tokens":"194.671",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.944",
        "QuAC - truncated":"0.016",
        "QuAC - # prompt tokens":"1644.831",
        "QuAC - # output tokens":"77.836",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"3.75",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"404.621",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"2.5",
        "MS MARCO (regular) - # eval":"1000",
        "MS MARCO (regular) - # train":"2",
        "MS MARCO (regular) - truncated":"0",
        "MS MARCO (regular) - # prompt tokens":"532.565",
        "MS MARCO (regular) - # output tokens":"5",
        "MS MARCO (regular) - # trials":"3",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"515.822",
        "MS MARCO (TREC) - # output tokens":"5",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1549.919",
        "CNN\/DailyMail - # output tokens":"73.533",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"4.999",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1510.418",
        "XSUM - # output tokens":"26.229",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.933",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1389.454",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"722.635",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.556",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"812.938",
        "RAFT - # output tokens":"9.057",
        "RAFT - # trials":"3"
    },
    {
        "Model":"OPT (66B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"908.406",
        "BoolQ - # output tokens":"5",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.647",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1652.377",
        "NarrativeQA - # output tokens":"50.904",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"153.231",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.691",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1419.574",
        "NaturalQuestions (open-book) - # output tokens":"211.805",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.944",
        "QuAC - truncated":"0.016",
        "QuAC - # prompt tokens":"1644.831",
        "QuAC - # output tokens":"91.909",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0.2",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"1",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"3.75",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"404.621",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"2.5",
        "MS MARCO (regular) - # eval":"1000",
        "MS MARCO (regular) - # train":"2",
        "MS MARCO (regular) - truncated":"0",
        "MS MARCO (regular) - # prompt tokens":"532.565",
        "MS MARCO (regular) - # output tokens":"5",
        "MS MARCO (regular) - # trials":"3",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"515.822",
        "MS MARCO (TREC) - # output tokens":"5",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1549.919",
        "CNN\/DailyMail - # output tokens":"77.928",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"4.999",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1510.418",
        "XSUM - # output tokens":"24.362",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.933",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1389.454",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"722.635",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.556",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"812.938",
        "RAFT - # output tokens":"18.712",
        "RAFT - # trials":"3"
    },
    {
        "Model":"LLaMA (7B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"522.547",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"1439.447",
        "BoolQ - # output tokens":"5",
        "BoolQ - # trials":"1",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.437",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1541.115",
        "NarrativeQA - # output tokens":"100",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"137.383",
        "NaturalQuestions (closed-book) - # output tokens":"300",
        "NaturalQuestions (closed-book) - # trials":"1",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"3.722",
        "NaturalQuestions (open-book) - truncated":"0.049",
        "NaturalQuestions (open-book) - # prompt tokens":"1407.178",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.507",
        "QuAC - truncated":"0.06",
        "QuAC - # prompt tokens":"1498.657",
        "QuAC - # output tokens":"99.794",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"524.602",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"-",
        "CNN\/DailyMail - # train":"-",
        "CNN\/DailyMail - truncated":"-",
        "CNN\/DailyMail - # prompt tokens":"-",
        "CNN\/DailyMail - # output tokens":"-",
        "CNN\/DailyMail - # trials":"-",
        "XSUM - # eval":"-",
        "XSUM - # train":"-",
        "XSUM - truncated":"-",
        "XSUM - # prompt tokens":"-",
        "XSUM - # output tokens":"-",
        "XSUM - # trials":"-",
        "IMDB - # eval":"1000",
        "IMDB - # train":"2.781",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1751.213",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"855.241",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"1",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.552",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"954.111",
        "RAFT - # output tokens":"29.961",
        "RAFT - # trials":"1"
    },
    {
        "Model":"LLaMA (13B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"522.547",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"1439.447",
        "BoolQ - # output tokens":"5",
        "BoolQ - # trials":"1",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.437",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1541.115",
        "NarrativeQA - # output tokens":"100",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"137.383",
        "NaturalQuestions (closed-book) - # output tokens":"300",
        "NaturalQuestions (closed-book) - # trials":"1",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"3.722",
        "NaturalQuestions (open-book) - truncated":"0.049",
        "NaturalQuestions (open-book) - # prompt tokens":"1407.178",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.507",
        "QuAC - truncated":"0.06",
        "QuAC - # prompt tokens":"1498.657",
        "QuAC - # output tokens":"99.882",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"524.602",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"-",
        "CNN\/DailyMail - # train":"-",
        "CNN\/DailyMail - truncated":"-",
        "CNN\/DailyMail - # prompt tokens":"-",
        "CNN\/DailyMail - # output tokens":"-",
        "CNN\/DailyMail - # trials":"-",
        "XSUM - # eval":"-",
        "XSUM - # train":"-",
        "XSUM - truncated":"-",
        "XSUM - # prompt tokens":"-",
        "XSUM - # output tokens":"-",
        "XSUM - # trials":"-",
        "IMDB - # eval":"1000",
        "IMDB - # train":"2.781",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1751.213",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"855.241",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"1",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.552",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"954.111",
        "RAFT - # output tokens":"29.361",
        "RAFT - # trials":"1"
    },
    {
        "Model":"LLaMA (30B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"522.547",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"1439.447",
        "BoolQ - # output tokens":"5",
        "BoolQ - # trials":"1",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.437",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1541.115",
        "NarrativeQA - # output tokens":"100",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"137.383",
        "NaturalQuestions (closed-book) - # output tokens":"300",
        "NaturalQuestions (closed-book) - # trials":"1",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"3.722",
        "NaturalQuestions (open-book) - truncated":"0.049",
        "NaturalQuestions (open-book) - # prompt tokens":"1407.178",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.507",
        "QuAC - truncated":"0.06",
        "QuAC - # prompt tokens":"1498.657",
        "QuAC - # output tokens":"99.987",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"524.602",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"-",
        "CNN\/DailyMail - # train":"-",
        "CNN\/DailyMail - truncated":"-",
        "CNN\/DailyMail - # prompt tokens":"-",
        "CNN\/DailyMail - # output tokens":"-",
        "CNN\/DailyMail - # trials":"-",
        "XSUM - # eval":"-",
        "XSUM - # train":"-",
        "XSUM - truncated":"-",
        "XSUM - # prompt tokens":"-",
        "XSUM - # output tokens":"-",
        "XSUM - # trials":"-",
        "IMDB - # eval":"1000",
        "IMDB - # train":"2.781",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1751.213",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"855.241",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"1",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.552",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"954.111",
        "RAFT - # output tokens":"30",
        "RAFT - # trials":"1"
    },
    {
        "Model":"LLaMA (65B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"522.547",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"1439.447",
        "BoolQ - # output tokens":"1",
        "BoolQ - # trials":"1",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.437",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1541.115",
        "NarrativeQA - # output tokens":"1",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"137.383",
        "NaturalQuestions (closed-book) - # output tokens":"1",
        "NaturalQuestions (closed-book) - # trials":"1",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"3.722",
        "NaturalQuestions (open-book) - truncated":"0.049",
        "NaturalQuestions (open-book) - # prompt tokens":"1407.178",
        "NaturalQuestions (open-book) - # output tokens":"0.987",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.507",
        "QuAC - truncated":"0.06",
        "QuAC - # prompt tokens":"1498.657",
        "QuAC - # output tokens":"0.997",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"524.602",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"-",
        "CNN\/DailyMail - # train":"-",
        "CNN\/DailyMail - truncated":"-",
        "CNN\/DailyMail - # prompt tokens":"-",
        "CNN\/DailyMail - # output tokens":"-",
        "CNN\/DailyMail - # trials":"-",
        "XSUM - # eval":"-",
        "XSUM - # train":"-",
        "XSUM - truncated":"-",
        "XSUM - # prompt tokens":"-",
        "XSUM - # output tokens":"-",
        "XSUM - # trials":"-",
        "IMDB - # eval":"1000",
        "IMDB - # train":"2.781",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1751.213",
        "IMDB - # output tokens":"1",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"855.241",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"1",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.552",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"954.111",
        "RAFT - # output tokens":"0.982",
        "RAFT - # trials":"1"
    },
    {
        "Model":"Llama 2 (7B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"522.547",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"1439.447",
        "BoolQ - # output tokens":"1.296",
        "BoolQ - # trials":"1",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"4.414",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"3673.268",
        "NarrativeQA - # output tokens":"1",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"137.383",
        "NaturalQuestions (closed-book) - # output tokens":"0.998",
        "NaturalQuestions (closed-book) - # trials":"1",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.831",
        "NaturalQuestions (open-book) - truncated":"0.026",
        "NaturalQuestions (open-book) - # prompt tokens":"2289.409",
        "NaturalQuestions (open-book) - # output tokens":"0.955",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"3.204",
        "QuAC - truncated":"0",
        "QuAC - # prompt tokens":"3617.038",
        "QuAC - # output tokens":"1",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"524.602",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"-",
        "CNN\/DailyMail - # train":"-",
        "CNN\/DailyMail - truncated":"-",
        "CNN\/DailyMail - # prompt tokens":"-",
        "CNN\/DailyMail - # output tokens":"-",
        "CNN\/DailyMail - # trials":"-",
        "XSUM - # eval":"-",
        "XSUM - # train":"-",
        "XSUM - truncated":"-",
        "XSUM - # prompt tokens":"-",
        "XSUM - # output tokens":"-",
        "XSUM - # trials":"-",
        "IMDB - # eval":"1000",
        "IMDB - # train":"5",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"2897.409",
        "IMDB - # output tokens":"1",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"855.241",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"1",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.78",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"1153.852",
        "RAFT - # output tokens":"1",
        "RAFT - # trials":"1"
    },
    {
        "Model":"Llama 2 (13B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"522.547",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"1439.447",
        "BoolQ - # output tokens":"5",
        "BoolQ - # trials":"1",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"4.414",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"3673.268",
        "NarrativeQA - # output tokens":"1",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"137.383",
        "NaturalQuestions (closed-book) - # output tokens":"1",
        "NaturalQuestions (closed-book) - # trials":"1",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.831",
        "NaturalQuestions (open-book) - truncated":"0.026",
        "NaturalQuestions (open-book) - # prompt tokens":"2289.409",
        "NaturalQuestions (open-book) - # output tokens":"0.984",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"3.204",
        "QuAC - truncated":"0",
        "QuAC - # prompt tokens":"3617.038",
        "QuAC - # output tokens":"1",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"524.602",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"-",
        "CNN\/DailyMail - # train":"-",
        "CNN\/DailyMail - truncated":"-",
        "CNN\/DailyMail - # prompt tokens":"-",
        "CNN\/DailyMail - # output tokens":"-",
        "CNN\/DailyMail - # trials":"-",
        "XSUM - # eval":"-",
        "XSUM - # train":"-",
        "XSUM - truncated":"-",
        "XSUM - # prompt tokens":"-",
        "XSUM - # output tokens":"-",
        "XSUM - # trials":"-",
        "IMDB - # eval":"1000",
        "IMDB - # train":"5",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"2897.409",
        "IMDB - # output tokens":"1",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"855.241",
        "CivilComments - # output tokens":"2.692",
        "CivilComments - # trials":"1",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.78",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"1153.852",
        "RAFT - # output tokens":"1",
        "RAFT - # trials":"1"
    },
    {
        "Model":"Llama 2 (70B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"522.547",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"1439.447",
        "BoolQ - # output tokens":"1",
        "BoolQ - # trials":"1",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"4.414",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"3673.268",
        "NarrativeQA - # output tokens":"1",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"137.383",
        "NaturalQuestions (closed-book) - # output tokens":"1",
        "NaturalQuestions (closed-book) - # trials":"1",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.831",
        "NaturalQuestions (open-book) - truncated":"0.026",
        "NaturalQuestions (open-book) - # prompt tokens":"2289.409",
        "NaturalQuestions (open-book) - # output tokens":"0.998",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"3.204",
        "QuAC - truncated":"0",
        "QuAC - # prompt tokens":"3617.038",
        "QuAC - # output tokens":"1",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"524.602",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"-",
        "CNN\/DailyMail - # train":"-",
        "CNN\/DailyMail - truncated":"-",
        "CNN\/DailyMail - # prompt tokens":"-",
        "CNN\/DailyMail - # output tokens":"-",
        "CNN\/DailyMail - # trials":"-",
        "XSUM - # eval":"-",
        "XSUM - # train":"-",
        "XSUM - truncated":"-",
        "XSUM - # prompt tokens":"-",
        "XSUM - # output tokens":"-",
        "XSUM - # trials":"-",
        "IMDB - # eval":"1000",
        "IMDB - # train":"5",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"2897.409",
        "IMDB - # output tokens":"1",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"855.241",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"1",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.78",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"1153.852",
        "RAFT - # output tokens":"1",
        "RAFT - # trials":"1"
    },
    {
        "Model":"Alpaca (7B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"522.547",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"1439.447",
        "BoolQ - # output tokens":"4.883",
        "BoolQ - # trials":"1",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.437",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1541.115",
        "NarrativeQA - # output tokens":"26.006",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"137.383",
        "NaturalQuestions (closed-book) - # output tokens":"84.53",
        "NaturalQuestions (closed-book) - # trials":"1",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"3.722",
        "NaturalQuestions (open-book) - truncated":"0.049",
        "NaturalQuestions (open-book) - # prompt tokens":"1407.178",
        "NaturalQuestions (open-book) - # output tokens":"122.525",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.507",
        "QuAC - truncated":"0.06",
        "QuAC - # prompt tokens":"1498.657",
        "QuAC - # output tokens":"77.323",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"524.602",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"-",
        "CNN\/DailyMail - # train":"-",
        "CNN\/DailyMail - truncated":"-",
        "CNN\/DailyMail - # prompt tokens":"-",
        "CNN\/DailyMail - # output tokens":"-",
        "CNN\/DailyMail - # trials":"-",
        "XSUM - # eval":"-",
        "XSUM - # train":"-",
        "XSUM - truncated":"-",
        "XSUM - # prompt tokens":"-",
        "XSUM - # output tokens":"-",
        "XSUM - # trials":"-",
        "IMDB - # eval":"1000",
        "IMDB - # train":"2.781",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1751.213",
        "IMDB - # output tokens":"4.966",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"855.241",
        "CivilComments - # output tokens":"4.216",
        "CivilComments - # trials":"1",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.552",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"954.111",
        "RAFT - # output tokens":"19.468",
        "RAFT - # trials":"1"
    },
    {
        "Model":"Vicuna v1.3 (7B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"522.547",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"1439.447",
        "BoolQ - # output tokens":"4.412",
        "BoolQ - # trials":"1",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.437",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1541.115",
        "NarrativeQA - # output tokens":"19.287",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"137.383",
        "NaturalQuestions (closed-book) - # output tokens":"296.95",
        "NaturalQuestions (closed-book) - # trials":"1",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"3.722",
        "NaturalQuestions (open-book) - truncated":"0.049",
        "NaturalQuestions (open-book) - # prompt tokens":"1407.178",
        "NaturalQuestions (open-book) - # output tokens":"286.175",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.507",
        "QuAC - truncated":"0.06",
        "QuAC - # prompt tokens":"1498.657",
        "QuAC - # output tokens":"77.25",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"524.602",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"-",
        "CNN\/DailyMail - # train":"-",
        "CNN\/DailyMail - truncated":"-",
        "CNN\/DailyMail - # prompt tokens":"-",
        "CNN\/DailyMail - # output tokens":"-",
        "CNN\/DailyMail - # trials":"-",
        "XSUM - # eval":"-",
        "XSUM - # train":"-",
        "XSUM - truncated":"-",
        "XSUM - # prompt tokens":"-",
        "XSUM - # output tokens":"-",
        "XSUM - # trials":"-",
        "IMDB - # eval":"1000",
        "IMDB - # train":"2.781",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1751.213",
        "IMDB - # output tokens":"3.258",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"855.241",
        "CivilComments - # output tokens":"4.98",
        "CivilComments - # trials":"1",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.552",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"954.111",
        "RAFT - # output tokens":"24.4",
        "RAFT - # trials":"1"
    },
    {
        "Model":"Vicuna v1.3 (13B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"522.547",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"1439.447",
        "BoolQ - # output tokens":"4.996",
        "BoolQ - # trials":"1",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.437",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1541.115",
        "NarrativeQA - # output tokens":"67.575",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"137.383",
        "NaturalQuestions (closed-book) - # output tokens":"299.508",
        "NaturalQuestions (closed-book) - # trials":"1",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"3.722",
        "NaturalQuestions (open-book) - truncated":"0.049",
        "NaturalQuestions (open-book) - # prompt tokens":"1407.178",
        "NaturalQuestions (open-book) - # output tokens":"266.895",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.507",
        "QuAC - truncated":"0.06",
        "QuAC - # prompt tokens":"1498.657",
        "QuAC - # output tokens":"77.743",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"524.602",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"-",
        "CNN\/DailyMail - # train":"-",
        "CNN\/DailyMail - truncated":"-",
        "CNN\/DailyMail - # prompt tokens":"-",
        "CNN\/DailyMail - # output tokens":"-",
        "CNN\/DailyMail - # trials":"-",
        "XSUM - # eval":"-",
        "XSUM - # train":"-",
        "XSUM - truncated":"-",
        "XSUM - # prompt tokens":"-",
        "XSUM - # output tokens":"-",
        "XSUM - # trials":"-",
        "IMDB - # eval":"1000",
        "IMDB - # train":"2.781",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1751.213",
        "IMDB - # output tokens":"3.32",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"855.241",
        "CivilComments - # output tokens":"2.59",
        "CivilComments - # trials":"1",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.552",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"954.111",
        "RAFT - # output tokens":"15.4",
        "RAFT - # trials":"1"
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"0",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"1418.259",
        "BoolQ - # output tokens":"1",
        "BoolQ - # trials":"1",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"4.575",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"3627.715",
        "NarrativeQA - # output tokens":"1",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"0",
        "NaturalQuestions (closed-book) - # output tokens":"1",
        "NaturalQuestions (closed-book) - # trials":"1",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.832",
        "NaturalQuestions (open-book) - truncated":"0.026",
        "NaturalQuestions (open-book) - # prompt tokens":"2268.728",
        "NaturalQuestions (open-book) - # output tokens":"0.987",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"3.44",
        "QuAC - truncated":"0",
        "QuAC - # prompt tokens":"3680.143",
        "QuAC - # output tokens":"0.999",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"0",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"-",
        "CNN\/DailyMail - # train":"-",
        "CNN\/DailyMail - truncated":"-",
        "CNN\/DailyMail - # prompt tokens":"-",
        "CNN\/DailyMail - # output tokens":"-",
        "CNN\/DailyMail - # trials":"-",
        "XSUM - # eval":"-",
        "XSUM - # train":"-",
        "XSUM - truncated":"-",
        "XSUM - # prompt tokens":"-",
        "XSUM - # output tokens":"-",
        "XSUM - # trials":"-",
        "IMDB - # eval":"1000",
        "IMDB - # train":"5",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"2811.31",
        "IMDB - # output tokens":"1",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"831.904",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"1",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.789",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"328.595",
        "RAFT - # output tokens":"1",
        "RAFT - # trials":"1"
    },
    {
        "Model":"TNLG v2 (530B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"908.406",
        "BoolQ - # output tokens":"1",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.646",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1651.848",
        "NarrativeQA - # output tokens":"5.982",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"4.569",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.691",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1419.328",
        "NaturalQuestions (open-book) - # output tokens":"6.015",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.944",
        "QuAC - truncated":"0.016",
        "QuAC - # prompt tokens":"1644.436",
        "QuAC - # output tokens":"29.956",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"511.121",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MS MARCO (regular) - # eval":"1000",
        "MS MARCO (regular) - # train":"2",
        "MS MARCO (regular) - truncated":"0",
        "MS MARCO (regular) - # prompt tokens":"532.565",
        "MS MARCO (regular) - # output tokens":"1.011",
        "MS MARCO (regular) - # trials":"3",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"515.822",
        "MS MARCO (TREC) - # output tokens":"1.016",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1549.919",
        "CNN\/DailyMail - # output tokens":"66.904",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"4.999",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1510.418",
        "XSUM - # output tokens":"27.501",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.932",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1389.183",
        "IMDB - # output tokens":"1",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"722.635",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.556",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"812.938",
        "RAFT - # output tokens":"3.023",
        "RAFT - # trials":"3"
    },
    {
        "Model":"TNLG v2 (6.7B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"908.406",
        "BoolQ - # output tokens":"1",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.646",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1651.848",
        "NarrativeQA - # output tokens":"6.499",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"5.6",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.691",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1419.328",
        "NaturalQuestions (open-book) - # output tokens":"8.369",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.944",
        "QuAC - truncated":"0.016",
        "QuAC - # prompt tokens":"1644.436",
        "QuAC - # output tokens":"19.574",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"511.121",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MS MARCO (regular) - # eval":"1000",
        "MS MARCO (regular) - # train":"2",
        "MS MARCO (regular) - truncated":"0",
        "MS MARCO (regular) - # prompt tokens":"532.565",
        "MS MARCO (regular) - # output tokens":"1.067",
        "MS MARCO (regular) - # trials":"3",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"515.822",
        "MS MARCO (TREC) - # output tokens":"1.047",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1549.919",
        "CNN\/DailyMail - # output tokens":"83.556",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"4.999",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1510.418",
        "XSUM - # output tokens":"23.579",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.932",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1389.183",
        "IMDB - # output tokens":"1",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"722.635",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.556",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"812.938",
        "RAFT - # output tokens":"2.76",
        "RAFT - # trials":"3"
    },
    {
        "Model":"davinci (175B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"908.406",
        "BoolQ - # output tokens":"1",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.647",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1652.377",
        "NarrativeQA - # output tokens":"5.709",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"5.361",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.691",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1419.574",
        "NaturalQuestions (open-book) - # output tokens":"8.992",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.944",
        "QuAC - truncated":"0.016",
        "QuAC - # prompt tokens":"1644.831",
        "QuAC - # output tokens":"29.572",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"511.121",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MS MARCO (regular) - # eval":"1000",
        "MS MARCO (regular) - # train":"2",
        "MS MARCO (regular) - truncated":"0",
        "MS MARCO (regular) - # prompt tokens":"532.565",
        "MS MARCO (regular) - # output tokens":"1",
        "MS MARCO (regular) - # trials":"3",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"515.822",
        "MS MARCO (TREC) - # output tokens":"1",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"4.286",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1411.872",
        "CNN\/DailyMail - # output tokens":"68.76",
        "CNN\/DailyMail - # trials":"2.714",
        "XSUM - # eval":"518",
        "XSUM - # train":"4.285",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1350.13",
        "XSUM - # output tokens":"31.877",
        "XSUM - # trials":"2.714",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.242",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1553.363",
        "IMDB - # output tokens":"1",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"722.635",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.556",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"812.938",
        "RAFT - # output tokens":"3.056",
        "RAFT - # trials":"3"
    },
    {
        "Model":"curie (6.7B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"908.406",
        "BoolQ - # output tokens":"1",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.647",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1652.377",
        "NarrativeQA - # output tokens":"6.607",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"6.313",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.691",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1419.574",
        "NaturalQuestions (open-book) - # output tokens":"12.581",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.944",
        "QuAC - truncated":"0.016",
        "QuAC - # prompt tokens":"1644.831",
        "QuAC - # output tokens":"31.034",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"511.121",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MS MARCO (regular) - # eval":"1000",
        "MS MARCO (regular) - # train":"2",
        "MS MARCO (regular) - truncated":"0",
        "MS MARCO (regular) - # prompt tokens":"532.565",
        "MS MARCO (regular) - # output tokens":"1.112",
        "MS MARCO (regular) - # trials":"3",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"515.822",
        "MS MARCO (TREC) - # output tokens":"1.248",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"4.286",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1411.872",
        "CNN\/DailyMail - # output tokens":"74.606",
        "CNN\/DailyMail - # trials":"2.714",
        "XSUM - # eval":"518",
        "XSUM - # train":"4.285",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1350.13",
        "XSUM - # output tokens":"27.757",
        "XSUM - # trials":"2.714",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.242",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1553.363",
        "IMDB - # output tokens":"1",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"722.635",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.556",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"812.938",
        "RAFT - # output tokens":"2.867",
        "RAFT - # trials":"3"
    },
    {
        "Model":"babbage (1.3B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"908.406",
        "BoolQ - # output tokens":"1",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.647",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1652.377",
        "NarrativeQA - # output tokens":"8.835",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"7.258",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.691",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1419.574",
        "NaturalQuestions (open-book) - # output tokens":"18.539",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.944",
        "QuAC - truncated":"0.016",
        "QuAC - # prompt tokens":"1644.831",
        "QuAC - # output tokens":"22.916",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"511.121",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MS MARCO (regular) - # eval":"1000",
        "MS MARCO (regular) - # train":"2",
        "MS MARCO (regular) - truncated":"0",
        "MS MARCO (regular) - # prompt tokens":"532.565",
        "MS MARCO (regular) - # output tokens":"1.537",
        "MS MARCO (regular) - # trials":"3",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"515.822",
        "MS MARCO (TREC) - # output tokens":"1.496",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1549.919",
        "CNN\/DailyMail - # output tokens":"68.44",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"4.999",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1510.418",
        "XSUM - # output tokens":"25.051",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.242",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1553.363",
        "IMDB - # output tokens":"1",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"722.635",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.556",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"812.938",
        "RAFT - # output tokens":"3.511",
        "RAFT - # trials":"3"
    },
    {
        "Model":"ada (350M)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"908.406",
        "BoolQ - # output tokens":"1.004",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.647",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1652.377",
        "NarrativeQA - # output tokens":"12.381",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"5.656",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.691",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1419.574",
        "NaturalQuestions (open-book) - # output tokens":"22.436",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.944",
        "QuAC - truncated":"0.016",
        "QuAC - # prompt tokens":"1644.831",
        "QuAC - # output tokens":"22.281",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"511.121",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MS MARCO (regular) - # eval":"1000",
        "MS MARCO (regular) - # train":"2",
        "MS MARCO (regular) - truncated":"0",
        "MS MARCO (regular) - # prompt tokens":"532.565",
        "MS MARCO (regular) - # output tokens":"1.219",
        "MS MARCO (regular) - # trials":"3",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"515.822",
        "MS MARCO (TREC) - # output tokens":"1.171",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1549.919",
        "CNN\/DailyMail - # output tokens":"76.958",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"4.999",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1510.418",
        "XSUM - # output tokens":"16.878",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.242",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1553.363",
        "IMDB - # output tokens":"1",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"722.635",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.556",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"812.938",
        "RAFT - # output tokens":"3.125",
        "RAFT - # trials":"3"
    },
    {
        "Model":"text-davinci-003",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"908.406",
        "BoolQ - # output tokens":"1.043",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"4.532",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"3579.093",
        "NarrativeQA - # output tokens":"9.164",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"7.964",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.883",
        "NaturalQuestions (open-book) - truncated":"0.02",
        "NaturalQuestions (open-book) - # prompt tokens":"1520.977",
        "NaturalQuestions (open-book) - # output tokens":"6.937",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"3.438",
        "QuAC - truncated":"0",
        "QuAC - # prompt tokens":"3249.907",
        "QuAC - # output tokens":"27.199",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"511.121",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MS MARCO (regular) - # eval":"1000",
        "MS MARCO (regular) - # train":"2",
        "MS MARCO (regular) - truncated":"0",
        "MS MARCO (regular) - # prompt tokens":"532.565",
        "MS MARCO (regular) - # output tokens":"1",
        "MS MARCO (regular) - # trials":"3",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"515.822",
        "MS MARCO (TREC) - # output tokens":"1",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1549.919",
        "CNN\/DailyMail - # output tokens":"64.315",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"5",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1510.735",
        "XSUM - # output tokens":"35.293",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"5",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1897.464",
        "IMDB - # output tokens":"1",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"722.635",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.752",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"1033.465",
        "RAFT - # output tokens":"3.137",
        "RAFT - # trials":"3"
    },
    {
        "Model":"text-davinci-002",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"908.406",
        "BoolQ - # output tokens":"1.013",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"4.532",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"3579.093",
        "NarrativeQA - # output tokens":"7.378",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"3.954",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.883",
        "NaturalQuestions (open-book) - truncated":"0.02",
        "NaturalQuestions (open-book) - # prompt tokens":"1520.977",
        "NaturalQuestions (open-book) - # output tokens":"6.652",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"3.438",
        "QuAC - truncated":"0",
        "QuAC - # prompt tokens":"3249.907",
        "QuAC - # output tokens":"20.986",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"511.121",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MS MARCO (regular) - # eval":"1000",
        "MS MARCO (regular) - # train":"2",
        "MS MARCO (regular) - truncated":"0",
        "MS MARCO (regular) - # prompt tokens":"532.565",
        "MS MARCO (regular) - # output tokens":"1.014",
        "MS MARCO (regular) - # trials":"3",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"515.822",
        "MS MARCO (TREC) - # output tokens":"0.992",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"4.286",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1411.872",
        "CNN\/DailyMail - # output tokens":"70.37",
        "CNN\/DailyMail - # trials":"2.714",
        "XSUM - # eval":"518",
        "XSUM - # train":"4.286",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1350.402",
        "XSUM - # output tokens":"28.674",
        "XSUM - # trials":"2.714",
        "IMDB - # eval":"1000",
        "IMDB - # train":"5",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1897.464",
        "IMDB - # output tokens":"1",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"722.635",
        "CivilComments - # output tokens":"0.997",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.752",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"1033.465",
        "RAFT - # output tokens":"3.057",
        "RAFT - # trials":"3"
    },
    {
        "Model":"text-curie-001",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"908.406",
        "BoolQ - # output tokens":"1.007",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.647",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1652.377",
        "NarrativeQA - # output tokens":"8.971",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"4.641",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.691",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1419.574",
        "NaturalQuestions (open-book) - # output tokens":"6.634",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.944",
        "QuAC - truncated":"0.016",
        "QuAC - # prompt tokens":"1644.831",
        "QuAC - # output tokens":"22.198",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"511.121",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MS MARCO (regular) - # eval":"1000",
        "MS MARCO (regular) - # train":"2",
        "MS MARCO (regular) - truncated":"0",
        "MS MARCO (regular) - # prompt tokens":"532.565",
        "MS MARCO (regular) - # output tokens":"1.031",
        "MS MARCO (regular) - # trials":"3",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"515.822",
        "MS MARCO (TREC) - # output tokens":"1.078",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"4.286",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1411.872",
        "CNN\/DailyMail - # output tokens":"94.314",
        "CNN\/DailyMail - # trials":"2.714",
        "XSUM - # eval":"518",
        "XSUM - # train":"4.285",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1350.13",
        "XSUM - # output tokens":"32.345",
        "XSUM - # trials":"2.714",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.242",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1553.363",
        "IMDB - # output tokens":"0.999",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"722.635",
        "CivilComments - # output tokens":"0.979",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.556",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"812.938",
        "RAFT - # output tokens":"2.751",
        "RAFT - # trials":"3"
    },
    {
        "Model":"text-babbage-001",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"908.406",
        "BoolQ - # output tokens":"1.004",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.647",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1652.377",
        "NarrativeQA - # output tokens":"12.829",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"2.016",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.691",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1419.574",
        "NaturalQuestions (open-book) - # output tokens":"7.772",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.944",
        "QuAC - truncated":"0.016",
        "QuAC - # prompt tokens":"1644.831",
        "QuAC - # output tokens":"22.966",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"511.121",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MS MARCO (regular) - # eval":"1000",
        "MS MARCO (regular) - # train":"2",
        "MS MARCO (regular) - truncated":"0",
        "MS MARCO (regular) - # prompt tokens":"532.565",
        "MS MARCO (regular) - # output tokens":"1.212",
        "MS MARCO (regular) - # trials":"3",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"515.822",
        "MS MARCO (TREC) - # output tokens":"1.132",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1549.919",
        "CNN\/DailyMail - # output tokens":"116.858",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"4.999",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1510.418",
        "XSUM - # output tokens":"40.165",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.242",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1553.363",
        "IMDB - # output tokens":"1.001",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"722.635",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.556",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"812.938",
        "RAFT - # output tokens":"2.774",
        "RAFT - # trials":"3"
    },
    {
        "Model":"text-ada-001",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"908.406",
        "BoolQ - # output tokens":"1.003",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.647",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1652.377",
        "NarrativeQA - # output tokens":"10.756",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"1.04",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.691",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1419.574",
        "NaturalQuestions (open-book) - # output tokens":"3.933",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.944",
        "QuAC - truncated":"0.016",
        "QuAC - # prompt tokens":"1644.831",
        "QuAC - # output tokens":"17.274",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"1000",
        "HellaSwag - # train":"0",
        "HellaSwag - truncated":"0",
        "HellaSwag - # prompt tokens":"87.888",
        "HellaSwag - # output tokens":"0",
        "HellaSwag - # trials":"1",
        "OpenbookQA - # eval":"500",
        "OpenbookQA - # train":"0",
        "OpenbookQA - truncated":"0",
        "OpenbookQA - # prompt tokens":"5.27",
        "OpenbookQA - # output tokens":"0",
        "OpenbookQA - # trials":"1",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"511.121",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"3",
        "MS MARCO (regular) - # eval":"1000",
        "MS MARCO (regular) - # train":"2",
        "MS MARCO (regular) - truncated":"0",
        "MS MARCO (regular) - # prompt tokens":"532.565",
        "MS MARCO (regular) - # output tokens":"1.123",
        "MS MARCO (regular) - # trials":"3",
        "MS MARCO (TREC) - # eval":"43",
        "MS MARCO (TREC) - # train":"2",
        "MS MARCO (TREC) - truncated":"0",
        "MS MARCO (TREC) - # prompt tokens":"515.822",
        "MS MARCO (TREC) - # output tokens":"1.101",
        "MS MARCO (TREC) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1549.919",
        "CNN\/DailyMail - # output tokens":"114.938",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"4.999",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1510.418",
        "XSUM - # output tokens":"34.806",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.242",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1553.363",
        "IMDB - # output tokens":"1.013",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"722.635",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.556",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"812.938",
        "RAFT - # output tokens":"2.997",
        "RAFT - # trials":"3"
    },
    {
        "Model":"gpt-3.5-turbo-0301",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"460.72",
        "MMLU - # output tokens":"1.012",
        "MMLU - # trials":"1",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"1220.329",
        "BoolQ - # output tokens":"1.932",
        "BoolQ - # trials":"1",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"4.966",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"3443.349",
        "NarrativeQA - # output tokens":"11.186",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.127",
        "NaturalQuestions (closed-book) - # output tokens":"16.241",
        "NaturalQuestions (closed-book) - # trials":"1",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.887",
        "NaturalQuestions (open-book) - truncated":"0.019",
        "NaturalQuestions (open-book) - # prompt tokens":"1590.821",
        "NaturalQuestions (open-book) - # output tokens":"12.998",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"3.871",
        "QuAC - truncated":"0",
        "QuAC - # prompt tokens":"3461.981",
        "QuAC - # output tokens":"23.136",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"464.434",
        "TruthfulQA - # output tokens":"1.047",
        "TruthfulQA - # trials":"1",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"-",
        "CNN\/DailyMail - # train":"-",
        "CNN\/DailyMail - truncated":"-",
        "CNN\/DailyMail - # prompt tokens":"-",
        "CNN\/DailyMail - # output tokens":"-",
        "CNN\/DailyMail - # trials":"-",
        "XSUM - # eval":"-",
        "XSUM - # train":"-",
        "XSUM - truncated":"-",
        "XSUM - # prompt tokens":"-",
        "XSUM - # output tokens":"-",
        "XSUM - # trials":"-",
        "IMDB - # eval":"1000",
        "IMDB - # train":"5",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"2543.665",
        "IMDB - # output tokens":"1.006",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"733.362",
        "CivilComments - # output tokens":"1.023",
        "CivilComments - # trials":"1",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.818",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"1002.239",
        "RAFT - # output tokens":"2.982",
        "RAFT - # trials":"1"
    },
    {
        "Model":"gpt-3.5-turbo-0613",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"460.72",
        "MMLU - # output tokens":"1.371",
        "MMLU - # trials":"1",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"1220.329",
        "BoolQ - # output tokens":"1.057",
        "BoolQ - # trials":"1",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"4.966",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"3443.349",
        "NarrativeQA - # output tokens":"12.194",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.127",
        "NaturalQuestions (closed-book) - # output tokens":"18.876",
        "NaturalQuestions (closed-book) - # trials":"1",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.887",
        "NaturalQuestions (open-book) - truncated":"0.019",
        "NaturalQuestions (open-book) - # prompt tokens":"1590.821",
        "NaturalQuestions (open-book) - # output tokens":"11.901",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"3.871",
        "QuAC - truncated":"0",
        "QuAC - # prompt tokens":"3461.981",
        "QuAC - # output tokens":"25.691",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"464.434",
        "TruthfulQA - # output tokens":"1.517",
        "TruthfulQA - # trials":"1",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"-",
        "CNN\/DailyMail - # train":"-",
        "CNN\/DailyMail - truncated":"-",
        "CNN\/DailyMail - # prompt tokens":"-",
        "CNN\/DailyMail - # output tokens":"-",
        "CNN\/DailyMail - # trials":"-",
        "XSUM - # eval":"-",
        "XSUM - # train":"-",
        "XSUM - truncated":"-",
        "XSUM - # prompt tokens":"-",
        "XSUM - # output tokens":"-",
        "XSUM - # trials":"-",
        "IMDB - # eval":"1000",
        "IMDB - # train":"5",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"2543.665",
        "IMDB - # output tokens":"1",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"733.362",
        "CivilComments - # output tokens":"1.001",
        "CivilComments - # trials":"1",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.818",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"1002.239",
        "RAFT - # output tokens":"2.955",
        "RAFT - # trials":"1"
    },
    {
        "Model":"RedPajama-INCITE-Base-v1 (3B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"467.936",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"1251.897",
        "BoolQ - # output tokens":"5",
        "BoolQ - # trials":"1",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.969",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1691.082",
        "NarrativeQA - # output tokens":"100",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"117.299",
        "NaturalQuestions (closed-book) - # output tokens":"300",
        "NaturalQuestions (closed-book) - # trials":"1",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.704",
        "NaturalQuestions (open-book) - truncated":"0.037",
        "NaturalQuestions (open-book) - # prompt tokens":"1495.552",
        "NaturalQuestions (open-book) - # output tokens":"299.738",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.883",
        "QuAC - truncated":"0.021",
        "QuAC - # prompt tokens":"1655.708",
        "QuAC - # output tokens":"100",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"505.352",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"-",
        "CNN\/DailyMail - # train":"-",
        "CNN\/DailyMail - truncated":"-",
        "CNN\/DailyMail - # prompt tokens":"-",
        "CNN\/DailyMail - # output tokens":"-",
        "CNN\/DailyMail - # trials":"-",
        "XSUM - # eval":"-",
        "XSUM - # train":"-",
        "XSUM - truncated":"-",
        "XSUM - # prompt tokens":"-",
        "XSUM - # output tokens":"-",
        "XSUM - # trials":"-",
        "IMDB - # eval":"1000",
        "IMDB - # train":"2.911",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1619.568",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"771.654",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"1",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.605",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"869.691",
        "RAFT - # output tokens":"30",
        "RAFT - # trials":"1"
    },
    {
        "Model":"RedPajama-INCITE-Instruct-v1 (3B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"467.936",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"1251.897",
        "BoolQ - # output tokens":"5",
        "BoolQ - # trials":"1",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.969",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1691.082",
        "NarrativeQA - # output tokens":"100",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"117.299",
        "NaturalQuestions (closed-book) - # output tokens":"300",
        "NaturalQuestions (closed-book) - # trials":"1",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.704",
        "NaturalQuestions (open-book) - truncated":"0.037",
        "NaturalQuestions (open-book) - # prompt tokens":"1495.552",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.883",
        "QuAC - truncated":"0.021",
        "QuAC - # prompt tokens":"1655.708",
        "QuAC - # output tokens":"100",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"505.352",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"-",
        "CNN\/DailyMail - # train":"-",
        "CNN\/DailyMail - truncated":"-",
        "CNN\/DailyMail - # prompt tokens":"-",
        "CNN\/DailyMail - # output tokens":"-",
        "CNN\/DailyMail - # trials":"-",
        "XSUM - # eval":"-",
        "XSUM - # train":"-",
        "XSUM - truncated":"-",
        "XSUM - # prompt tokens":"-",
        "XSUM - # output tokens":"-",
        "XSUM - # trials":"-",
        "IMDB - # eval":"1000",
        "IMDB - # train":"2.911",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1619.568",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"771.654",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"1",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.605",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"869.691",
        "RAFT - # output tokens":"30",
        "RAFT - # trials":"1"
    },
    {
        "Model":"RedPajama-INCITE-Base (7B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"467.936",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"1251.897",
        "BoolQ - # output tokens":"5",
        "BoolQ - # trials":"1",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.969",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1691.082",
        "NarrativeQA - # output tokens":"100",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"117.299",
        "NaturalQuestions (closed-book) - # output tokens":"300",
        "NaturalQuestions (closed-book) - # trials":"1",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.704",
        "NaturalQuestions (open-book) - truncated":"0.037",
        "NaturalQuestions (open-book) - # prompt tokens":"1495.552",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.883",
        "QuAC - truncated":"0.021",
        "QuAC - # prompt tokens":"1655.708",
        "QuAC - # output tokens":"100",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"505.352",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"-",
        "CNN\/DailyMail - # train":"-",
        "CNN\/DailyMail - truncated":"-",
        "CNN\/DailyMail - # prompt tokens":"-",
        "CNN\/DailyMail - # output tokens":"-",
        "CNN\/DailyMail - # trials":"-",
        "XSUM - # eval":"-",
        "XSUM - # train":"-",
        "XSUM - truncated":"-",
        "XSUM - # prompt tokens":"-",
        "XSUM - # output tokens":"-",
        "XSUM - # trials":"-",
        "IMDB - # eval":"1000",
        "IMDB - # train":"2.911",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1619.568",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"771.654",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"1",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.605",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"869.691",
        "RAFT - # output tokens":"30",
        "RAFT - # trials":"1"
    },
    {
        "Model":"RedPajama-INCITE-Instruct (7B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"467.936",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"1251.897",
        "BoolQ - # output tokens":"5",
        "BoolQ - # trials":"1",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.969",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1691.082",
        "NarrativeQA - # output tokens":"100",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"117.299",
        "NaturalQuestions (closed-book) - # output tokens":"300",
        "NaturalQuestions (closed-book) - # trials":"1",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.704",
        "NaturalQuestions (open-book) - truncated":"0.037",
        "NaturalQuestions (open-book) - # prompt tokens":"1495.552",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.883",
        "QuAC - truncated":"0.021",
        "QuAC - # prompt tokens":"1655.708",
        "QuAC - # output tokens":"100",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"505.352",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"-",
        "CNN\/DailyMail - # train":"-",
        "CNN\/DailyMail - truncated":"-",
        "CNN\/DailyMail - # prompt tokens":"-",
        "CNN\/DailyMail - # output tokens":"-",
        "CNN\/DailyMail - # trials":"-",
        "XSUM - # eval":"-",
        "XSUM - # train":"-",
        "XSUM - truncated":"-",
        "XSUM - # prompt tokens":"-",
        "XSUM - # output tokens":"-",
        "XSUM - # trials":"-",
        "IMDB - # eval":"1000",
        "IMDB - # train":"2.911",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1619.568",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"771.654",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"1",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.605",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"869.691",
        "RAFT - # output tokens":"30",
        "RAFT - # trials":"1"
    },
    {
        "Model":"MPT (30B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"467.936",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"1251.897",
        "BoolQ - # output tokens":"1",
        "BoolQ - # trials":"1",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.969",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1691.082",
        "NarrativeQA - # output tokens":"1",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"117.299",
        "NaturalQuestions (closed-book) - # output tokens":"0.999",
        "NaturalQuestions (closed-book) - # trials":"1",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.704",
        "NaturalQuestions (open-book) - truncated":"0.037",
        "NaturalQuestions (open-book) - # prompt tokens":"1495.552",
        "NaturalQuestions (open-book) - # output tokens":"0.993",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.883",
        "QuAC - truncated":"0.021",
        "QuAC - # prompt tokens":"1655.708",
        "QuAC - # output tokens":"0.997",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"505.352",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"-",
        "CNN\/DailyMail - # train":"-",
        "CNN\/DailyMail - truncated":"-",
        "CNN\/DailyMail - # prompt tokens":"-",
        "CNN\/DailyMail - # output tokens":"-",
        "CNN\/DailyMail - # trials":"-",
        "XSUM - # eval":"-",
        "XSUM - # train":"-",
        "XSUM - truncated":"-",
        "XSUM - # prompt tokens":"-",
        "XSUM - # output tokens":"-",
        "XSUM - # trials":"-",
        "IMDB - # eval":"1000",
        "IMDB - # train":"2.911",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1619.568",
        "IMDB - # output tokens":"1",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"771.654",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"1",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.605",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"869.691",
        "RAFT - # output tokens":"0.975",
        "RAFT - # trials":"1"
    },
    {
        "Model":"MPT-Instruct (30B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"467.936",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"1251.897",
        "BoolQ - # output tokens":"1",
        "BoolQ - # trials":"1",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.969",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1691.082",
        "NarrativeQA - # output tokens":"1",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"117.299",
        "NaturalQuestions (closed-book) - # output tokens":"1",
        "NaturalQuestions (closed-book) - # trials":"1",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.704",
        "NaturalQuestions (open-book) - truncated":"0.037",
        "NaturalQuestions (open-book) - # prompt tokens":"1495.552",
        "NaturalQuestions (open-book) - # output tokens":"0.994",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.883",
        "QuAC - truncated":"0.021",
        "QuAC - # prompt tokens":"1655.708",
        "QuAC - # output tokens":"0.998",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"505.352",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"-",
        "CNN\/DailyMail - # train":"-",
        "CNN\/DailyMail - truncated":"-",
        "CNN\/DailyMail - # prompt tokens":"-",
        "CNN\/DailyMail - # output tokens":"-",
        "CNN\/DailyMail - # trials":"-",
        "XSUM - # eval":"-",
        "XSUM - # train":"-",
        "XSUM - truncated":"-",
        "XSUM - # prompt tokens":"-",
        "XSUM - # output tokens":"-",
        "XSUM - # trials":"-",
        "IMDB - # eval":"1000",
        "IMDB - # train":"2.911",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1619.568",
        "IMDB - # output tokens":"1",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"771.654",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"1",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.605",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"869.691",
        "RAFT - # output tokens":"1",
        "RAFT - # trials":"1"
    },
    {
        "Model":"Falcon (7B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"500.12",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"1284.629",
        "BoolQ - # output tokens":"1",
        "BoolQ - # trials":"1",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"2.025",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1694.082",
        "NarrativeQA - # output tokens":"1",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"124.246",
        "NaturalQuestions (closed-book) - # output tokens":"1",
        "NaturalQuestions (closed-book) - # trials":"1",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.599",
        "NaturalQuestions (open-book) - truncated":"0.039",
        "NaturalQuestions (open-book) - # prompt tokens":"1587.334",
        "NaturalQuestions (open-book) - # output tokens":"0.994",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.862",
        "QuAC - truncated":"0.031",
        "QuAC - # prompt tokens":"1667.28",
        "QuAC - # output tokens":"0.995",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"507.503",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"-",
        "CNN\/DailyMail - # train":"-",
        "CNN\/DailyMail - truncated":"-",
        "CNN\/DailyMail - # prompt tokens":"-",
        "CNN\/DailyMail - # output tokens":"-",
        "CNN\/DailyMail - # trials":"-",
        "XSUM - # eval":"-",
        "XSUM - # train":"-",
        "XSUM - truncated":"-",
        "XSUM - # prompt tokens":"-",
        "XSUM - # output tokens":"-",
        "XSUM - # trials":"-",
        "IMDB - # eval":"1000",
        "IMDB - # train":"2.871",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1666.079",
        "IMDB - # output tokens":"1",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"782.759",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"1",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.6",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"877.464",
        "RAFT - # output tokens":"0.975",
        "RAFT - # trials":"1"
    },
    {
        "Model":"Falcon-Instruct (7B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"500.12",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"1284.629",
        "BoolQ - # output tokens":"1",
        "BoolQ - # trials":"1",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"2.025",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1694.082",
        "NarrativeQA - # output tokens":"1",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"124.246",
        "NaturalQuestions (closed-book) - # output tokens":"0.999",
        "NaturalQuestions (closed-book) - # trials":"1",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.599",
        "NaturalQuestions (open-book) - truncated":"0.039",
        "NaturalQuestions (open-book) - # prompt tokens":"1587.334",
        "NaturalQuestions (open-book) - # output tokens":"0.984",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.862",
        "QuAC - truncated":"0.031",
        "QuAC - # prompt tokens":"1667.28",
        "QuAC - # output tokens":"0.997",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"507.503",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"-",
        "CNN\/DailyMail - # train":"-",
        "CNN\/DailyMail - truncated":"-",
        "CNN\/DailyMail - # prompt tokens":"-",
        "CNN\/DailyMail - # output tokens":"-",
        "CNN\/DailyMail - # trials":"-",
        "XSUM - # eval":"-",
        "XSUM - # train":"-",
        "XSUM - truncated":"-",
        "XSUM - # prompt tokens":"-",
        "XSUM - # output tokens":"-",
        "XSUM - # trials":"-",
        "IMDB - # eval":"1000",
        "IMDB - # train":"2.871",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1666.079",
        "IMDB - # output tokens":"1",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"782.759",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"1",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.6",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"877.464",
        "RAFT - # output tokens":"0.995",
        "RAFT - # trials":"1"
    },
    {
        "Model":"Falcon (40B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"500.12",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"1284.629",
        "BoolQ - # output tokens":"1",
        "BoolQ - # trials":"1",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"2.025",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1694.082",
        "NarrativeQA - # output tokens":"1",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"124.246",
        "NaturalQuestions (closed-book) - # output tokens":"1",
        "NaturalQuestions (closed-book) - # trials":"1",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.599",
        "NaturalQuestions (open-book) - truncated":"0.039",
        "NaturalQuestions (open-book) - # prompt tokens":"1587.334",
        "NaturalQuestions (open-book) - # output tokens":"0.995",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.862",
        "QuAC - truncated":"0.031",
        "QuAC - # prompt tokens":"1667.28",
        "QuAC - # output tokens":"0.999",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"507.503",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"-",
        "CNN\/DailyMail - # train":"-",
        "CNN\/DailyMail - truncated":"-",
        "CNN\/DailyMail - # prompt tokens":"-",
        "CNN\/DailyMail - # output tokens":"-",
        "CNN\/DailyMail - # trials":"-",
        "XSUM - # eval":"-",
        "XSUM - # train":"-",
        "XSUM - truncated":"-",
        "XSUM - # prompt tokens":"-",
        "XSUM - # output tokens":"-",
        "XSUM - # trials":"-",
        "IMDB - # eval":"1000",
        "IMDB - # train":"2.871",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1666.079",
        "IMDB - # output tokens":"1",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"782.759",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"1",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.6",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"877.464",
        "RAFT - # output tokens":"0.973",
        "RAFT - # trials":"1"
    },
    {
        "Model":"Falcon-Instruct (40B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"500.12",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"1",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"1284.629",
        "BoolQ - # output tokens":"1",
        "BoolQ - # trials":"1",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"2.025",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1694.082",
        "NarrativeQA - # output tokens":"1",
        "NarrativeQA - # trials":"1",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"124.246",
        "NaturalQuestions (closed-book) - # output tokens":"0.999",
        "NaturalQuestions (closed-book) - # trials":"1",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.599",
        "NaturalQuestions (open-book) - truncated":"0.039",
        "NaturalQuestions (open-book) - # prompt tokens":"1587.334",
        "NaturalQuestions (open-book) - # output tokens":"0.995",
        "NaturalQuestions (open-book) - # trials":"1",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.862",
        "QuAC - truncated":"0.031",
        "QuAC - # prompt tokens":"1667.28",
        "QuAC - # output tokens":"0.999",
        "QuAC - # trials":"1",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"507.503",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"1",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"-",
        "CNN\/DailyMail - # train":"-",
        "CNN\/DailyMail - truncated":"-",
        "CNN\/DailyMail - # prompt tokens":"-",
        "CNN\/DailyMail - # output tokens":"-",
        "CNN\/DailyMail - # trials":"-",
        "XSUM - # eval":"-",
        "XSUM - # train":"-",
        "XSUM - truncated":"-",
        "XSUM - # prompt tokens":"-",
        "XSUM - # output tokens":"-",
        "XSUM - # trials":"-",
        "IMDB - # eval":"1000",
        "IMDB - # train":"2.871",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1666.079",
        "IMDB - # output tokens":"1",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"782.759",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"1",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.6",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"877.464",
        "RAFT - # output tokens":"0.984",
        "RAFT - # trials":"1"
    },
    {
        "Model":"GLM (130B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"460.637",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"931.424",
        "BoolQ - # output tokens":"2",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.675",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1658.811",
        "NarrativeQA - # output tokens":"9.939",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"122.991",
        "NaturalQuestions (closed-book) - # output tokens":"6.707",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.631",
        "NaturalQuestions (open-book) - truncated":"0.047",
        "NaturalQuestions (open-book) - # prompt tokens":"1502.677",
        "NaturalQuestions (open-book) - # output tokens":"21.064",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.874",
        "QuAC - truncated":"0.134",
        "QuAC - # prompt tokens":"1651.972",
        "QuAC - # output tokens":"73.565",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"3.75",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"389.036",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"2.5",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1657.124",
        "CNN\/DailyMail - # output tokens":"82.997",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"4.996",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1567.312",
        "XSUM - # output tokens":"25.737",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.923",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1412.285",
        "IMDB - # output tokens":"2",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"694.39",
        "CivilComments - # output tokens":"2",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.563",
        "RAFT - truncated":"0.07",
        "RAFT - # prompt tokens":"803.318",
        "RAFT - # output tokens":"4.886",
        "RAFT - # trials":"3"
    },
    {
        "Model":"InstructPalmyra (30B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"908.406",
        "BoolQ - # output tokens":"1",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.646",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1651.848",
        "NarrativeQA - # output tokens":"5.347",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"4.247",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.691",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1419.328",
        "NaturalQuestions (open-book) - # output tokens":"7.657",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.944",
        "QuAC - truncated":"0.016",
        "QuAC - # prompt tokens":"1644.436",
        "QuAC - # output tokens":"22.969",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"511.121",
        "TruthfulQA - # output tokens":"0.999",
        "TruthfulQA - # trials":"3",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1549.919",
        "CNN\/DailyMail - # output tokens":"83.965",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"4.999",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1510.418",
        "XSUM - # output tokens":"26.632",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.242",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1553.363",
        "IMDB - # output tokens":"0.997",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"722.635",
        "CivilComments - # output tokens":"0.905",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.556",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"812.938",
        "RAFT - # output tokens":"2.967",
        "RAFT - # trials":"3"
    },
    {
        "Model":"Palmyra X (43B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"472.274",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"908.406",
        "BoolQ - # output tokens":"1.007",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"5",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"3803.911",
        "NarrativeQA - # output tokens":"6.272",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"112.254",
        "NaturalQuestions (closed-book) - # output tokens":"3.19",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"-",
        "NaturalQuestions (open-book) - # train":"-",
        "NaturalQuestions (open-book) - truncated":"-",
        "NaturalQuestions (open-book) - # prompt tokens":"-",
        "NaturalQuestions (open-book) - # output tokens":"-",
        "NaturalQuestions (open-book) - # trials":"-",
        "QuAC - # eval":"1000",
        "QuAC - # train":"5",
        "QuAC - truncated":"0",
        "QuAC - # prompt tokens":"5199.788",
        "QuAC - # output tokens":"26.581",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"5",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"511.121",
        "TruthfulQA - # output tokens":"0.949",
        "TruthfulQA - # trials":"3",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1549.919",
        "CNN\/DailyMail - # output tokens":"17.63",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"5",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1510.735",
        "XSUM - # output tokens":"25.248",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"5",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1897.464",
        "IMDB - # output tokens":"1.939",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"722.635",
        "CivilComments - # output tokens":"0.011",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"5",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"1279.572",
        "RAFT - # output tokens":"3.07",
        "RAFT - # trials":"3"
    },
    {
        "Model":"YaLM (100B)",
        "Mean win rate":"-",
        "MMLU - # eval":"102.8",
        "MMLU - # train":"5",
        "MMLU - truncated":"0",
        "MMLU - # prompt tokens":"453.383",
        "MMLU - # output tokens":"1",
        "MMLU - # trials":"3",
        "BoolQ - # eval":"1000",
        "BoolQ - # train":"5",
        "BoolQ - truncated":"0",
        "BoolQ - # prompt tokens":"899.006",
        "BoolQ - # output tokens":"5",
        "BoolQ - # trials":"3",
        "NarrativeQA - # eval":"355",
        "NarrativeQA - # train":"1.604",
        "NarrativeQA - truncated":"0",
        "NarrativeQA - # prompt tokens":"1644.878",
        "NarrativeQA - # output tokens":"96.018",
        "NarrativeQA - # trials":"3",
        "NaturalQuestions (closed-book) - # eval":"1000",
        "NaturalQuestions (closed-book) - # train":"5",
        "NaturalQuestions (closed-book) - truncated":"0",
        "NaturalQuestions (closed-book) - # prompt tokens":"111.534",
        "NaturalQuestions (closed-book) - # output tokens":"299.515",
        "NaturalQuestions (closed-book) - # trials":"3",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.702",
        "NaturalQuestions (open-book) - truncated":"0.038",
        "NaturalQuestions (open-book) - # prompt tokens":"1409.24",
        "NaturalQuestions (open-book) - # output tokens":"291.572",
        "NaturalQuestions (open-book) - # trials":"3",
        "QuAC - # eval":"1000",
        "QuAC - # train":"0.951",
        "QuAC - truncated":"0.016",
        "QuAC - # prompt tokens":"1646.729",
        "QuAC - # output tokens":"99.146",
        "QuAC - # trials":"3",
        "HellaSwag - # eval":"-",
        "HellaSwag - # train":"-",
        "HellaSwag - truncated":"-",
        "HellaSwag - # prompt tokens":"-",
        "HellaSwag - # output tokens":"-",
        "HellaSwag - # trials":"-",
        "OpenbookQA - # eval":"-",
        "OpenbookQA - # train":"-",
        "OpenbookQA - truncated":"-",
        "OpenbookQA - # prompt tokens":"-",
        "OpenbookQA - # output tokens":"-",
        "OpenbookQA - # trials":"-",
        "TruthfulQA - # eval":"654",
        "TruthfulQA - # train":"3.75",
        "TruthfulQA - truncated":"0",
        "TruthfulQA - # prompt tokens":"405.414",
        "TruthfulQA - # output tokens":"1",
        "TruthfulQA - # trials":"2.5",
        "MS MARCO (regular) - # eval":"-",
        "MS MARCO (regular) - # train":"-",
        "MS MARCO (regular) - truncated":"-",
        "MS MARCO (regular) - # prompt tokens":"-",
        "MS MARCO (regular) - # output tokens":"-",
        "MS MARCO (regular) - # trials":"-",
        "MS MARCO (TREC) - # eval":"-",
        "MS MARCO (TREC) - # train":"-",
        "MS MARCO (TREC) - truncated":"-",
        "MS MARCO (TREC) - # prompt tokens":"-",
        "MS MARCO (TREC) - # output tokens":"-",
        "MS MARCO (TREC) - # trials":"-",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1544.765",
        "CNN\/DailyMail - # output tokens":"102.407",
        "CNN\/DailyMail - # trials":"3",
        "XSUM - # eval":"518",
        "XSUM - # train":"5",
        "XSUM - truncated":"0",
        "XSUM - # prompt tokens":"1507.497",
        "XSUM - # output tokens":"49.401",
        "XSUM - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4.929",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1402.276",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"371.556",
        "CivilComments - # train":"5",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"729.671",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3",
        "RAFT - # eval":"40",
        "RAFT - # train":"4.562",
        "RAFT - truncated":"0",
        "RAFT - # prompt tokens":"784.961",
        "RAFT - # output tokens":"13.615",
        "RAFT - # trials":"3"
    }
]