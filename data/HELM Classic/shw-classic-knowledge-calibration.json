[
    {
        "Model":"J1-Jumbo v1 (178B)",
        "Mean win rate":"0.575",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.035",
        "HellaSwag - ECE (10-bin)":"0.217",
        "OpenbookQA - ECE (10-bin)":"0.25",
        "TruthfulQA - ECE (10-bin)":"0.113",
        "MMLU - ECE (10-bin)":"0.131"
    },
    {
        "Model":"J1-Large v1 (7.5B)",
        "Mean win rate":"0.683",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.015",
        "HellaSwag - ECE (10-bin)":"0.192",
        "OpenbookQA - ECE (10-bin)":"0.25",
        "TruthfulQA - ECE (10-bin)":"0.112",
        "MMLU - ECE (10-bin)":"0.123"
    },
    {
        "Model":"J1-Grande v1 (17B)",
        "Mean win rate":"0.662",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.029",
        "HellaSwag - ECE (10-bin)":"0.213",
        "OpenbookQA - ECE (10-bin)":"0.258",
        "TruthfulQA - ECE (10-bin)":"0.091",
        "MMLU - ECE (10-bin)":"0.114"
    },
    {
        "Model":"J1-Grande v2 beta (17B)",
        "Mean win rate":"0.6",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.036",
        "HellaSwag - ECE (10-bin)":"0.226",
        "OpenbookQA - ECE (10-bin)":"0.215",
        "TruthfulQA - ECE (10-bin)":"0.123",
        "MMLU - ECE (10-bin)":"0.139"
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "Mean win rate":"0.73",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.018",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.068",
        "MMLU - ECE (10-bin)":"0.137"
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "Mean win rate":"0.681",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.018",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.097",
        "MMLU - ECE (10-bin)":"0.134"
    },
    {
        "Model":"Jurassic-2 Large (7.5B)",
        "Mean win rate":"0.624",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.014",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.102",
        "MMLU - ECE (10-bin)":"0.141"
    },
    {
        "Model":"Luminous Base (13B)",
        "Mean win rate":"0.745",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.045",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.081",
        "MMLU - ECE (10-bin)":"0.111"
    },
    {
        "Model":"Luminous Extended (30B)",
        "Mean win rate":"0.759",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.022",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.064",
        "MMLU - ECE (10-bin)":"0.135"
    },
    {
        "Model":"Luminous Supreme (70B)",
        "Mean win rate":"0.496",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.041",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.092",
        "MMLU - ECE (10-bin)":"0.154"
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "MMLU - ECE (10-bin)":"-"
    },
    {
        "Model":"BLOOM (176B)",
        "Mean win rate":"0.432",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.116",
        "HellaSwag - ECE (10-bin)":"0.293",
        "OpenbookQA - ECE (10-bin)":"0.248",
        "TruthfulQA - ECE (10-bin)":"0.096",
        "MMLU - ECE (10-bin)":"0.137"
    },
    {
        "Model":"T0pp (11B)",
        "Mean win rate":"0.511",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.154",
        "MMLU - ECE (10-bin)":"0.168"
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B)",
        "Mean win rate":"0.393",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.068",
        "HellaSwag - ECE (10-bin)":"0.341",
        "OpenbookQA - ECE (10-bin)":"0.235",
        "TruthfulQA - ECE (10-bin)":"0.099",
        "MMLU - ECE (10-bin)":"0.149"
    },
    {
        "Model":"Cohere large v20220720 (13.1B)",
        "Mean win rate":"0.664",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.025",
        "HellaSwag - ECE (10-bin)":"0.288",
        "OpenbookQA - ECE (10-bin)":"0.225",
        "TruthfulQA - ECE (10-bin)":"0.105",
        "MMLU - ECE (10-bin)":"0.112"
    },
    {
        "Model":"Cohere medium v20220720 (6.1B)",
        "Mean win rate":"0.602",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.026",
        "HellaSwag - ECE (10-bin)":"0.271",
        "OpenbookQA - ECE (10-bin)":"0.275",
        "TruthfulQA - ECE (10-bin)":"0.094",
        "MMLU - ECE (10-bin)":"0.114"
    },
    {
        "Model":"Cohere small v20220720 (410M)",
        "Mean win rate":"0.61",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.023",
        "HellaSwag - ECE (10-bin)":"0.083",
        "OpenbookQA - ECE (10-bin)":"0.379",
        "TruthfulQA - ECE (10-bin)":"0.076",
        "MMLU - ECE (10-bin)":"0.136"
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B)",
        "Mean win rate":"0.421",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.054",
        "HellaSwag - ECE (10-bin)":"0.333",
        "OpenbookQA - ECE (10-bin)":"0.207",
        "TruthfulQA - ECE (10-bin)":"0.211",
        "MMLU - ECE (10-bin)":"0.143"
    },
    {
        "Model":"Cohere medium v20221108 (6.1B)",
        "Mean win rate":"0.76",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.015",
        "HellaSwag - ECE (10-bin)":"0.281",
        "OpenbookQA - ECE (10-bin)":"0.23",
        "TruthfulQA - ECE (10-bin)":"0.08",
        "MMLU - ECE (10-bin)":"0.113"
    },
    {
        "Model":"Cohere Command beta (6.1B)",
        "Mean win rate":"0.313",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.042",
        "HellaSwag - ECE (10-bin)":"0.293",
        "OpenbookQA - ECE (10-bin)":"0.25",
        "TruthfulQA - ECE (10-bin)":"0.3",
        "MMLU - ECE (10-bin)":"0.155"
    },
    {
        "Model":"Cohere Command beta (52.4B)",
        "Mean win rate":"0.295",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.084",
        "HellaSwag - ECE (10-bin)":"0.325",
        "OpenbookQA - ECE (10-bin)":"0.231",
        "TruthfulQA - ECE (10-bin)":"0.311",
        "MMLU - ECE (10-bin)":"0.183"
    },
    {
        "Model":"GPT-J (6B)",
        "Mean win rate":"0.656",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.075",
        "HellaSwag - ECE (10-bin)":"0.233",
        "OpenbookQA - ECE (10-bin)":"0.235",
        "TruthfulQA - ECE (10-bin)":"0.078",
        "MMLU - ECE (10-bin)":"0.115"
    },
    {
        "Model":"GPT-NeoX (20B)",
        "Mean win rate":"0.654",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.103",
        "HellaSwag - ECE (10-bin)":"0.277",
        "OpenbookQA - ECE (10-bin)":"0.232",
        "TruthfulQA - ECE (10-bin)":"0.058",
        "MMLU - ECE (10-bin)":"0.122"
    },
    {
        "Model":"Pythia (6.9B)",
        "Mean win rate":"0.589",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.07",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.076",
        "MMLU - ECE (10-bin)":"0.136"
    },
    {
        "Model":"Pythia (12B)",
        "Mean win rate":"0.645",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.094",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.094",
        "MMLU - ECE (10-bin)":"0.111"
    },
    {
        "Model":"T5 (11B)",
        "Mean win rate":"0.34",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.076",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.143",
        "MMLU - ECE (10-bin)":"0.151"
    },
    {
        "Model":"UL2 (20B)",
        "Mean win rate":"0.454",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.092",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.125",
        "MMLU - ECE (10-bin)":"0.134"
    },
    {
        "Model":"OPT (175B)",
        "Mean win rate":"0.493",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.173",
        "HellaSwag - ECE (10-bin)":"0.325",
        "OpenbookQA - ECE (10-bin)":"0.209",
        "TruthfulQA - ECE (10-bin)":"0.054",
        "MMLU - ECE (10-bin)":"0.147"
    },
    {
        "Model":"OPT (66B)",
        "Mean win rate":"0.478",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.141",
        "HellaSwag - ECE (10-bin)":"0.293",
        "OpenbookQA - ECE (10-bin)":"0.237",
        "TruthfulQA - ECE (10-bin)":"0.073",
        "MMLU - ECE (10-bin)":"0.135"
    },
    {
        "Model":"LLaMA (7B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "MMLU - ECE (10-bin)":"-"
    },
    {
        "Model":"LLaMA (13B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "MMLU - ECE (10-bin)":"-"
    },
    {
        "Model":"LLaMA (30B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "MMLU - ECE (10-bin)":"-"
    },
    {
        "Model":"LLaMA (65B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "MMLU - ECE (10-bin)":"-"
    },
    {
        "Model":"Llama 2 (7B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "MMLU - ECE (10-bin)":"-"
    },
    {
        "Model":"Llama 2 (13B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "MMLU - ECE (10-bin)":"-"
    },
    {
        "Model":"Llama 2 (70B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "MMLU - ECE (10-bin)":"-"
    },
    {
        "Model":"Alpaca (7B)",
        "Mean win rate":"0.121",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.134",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.375",
        "MMLU - ECE (10-bin)":"0.234"
    },
    {
        "Model":"Vicuna v1.3 (7B)",
        "Mean win rate":"0.184",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.162",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.227",
        "MMLU - ECE (10-bin)":"0.176"
    },
    {
        "Model":"Vicuna v1.3 (13B)",
        "Mean win rate":"0.113",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.202",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.316",
        "MMLU - ECE (10-bin)":"0.194"
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "MMLU - ECE (10-bin)":"-"
    },
    {
        "Model":"TNLG v2 (530B)",
        "Mean win rate":"0.448",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.04",
        "HellaSwag - ECE (10-bin)":"0.322",
        "OpenbookQA - ECE (10-bin)":"0.243",
        "TruthfulQA - ECE (10-bin)":"0.226",
        "MMLU - ECE (10-bin)":"0.127"
    },
    {
        "Model":"TNLG v2 (6.7B)",
        "Mean win rate":"0.5",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.031",
        "HellaSwag - ECE (10-bin)":"0.268",
        "OpenbookQA - ECE (10-bin)":"0.282",
        "TruthfulQA - ECE (10-bin)":"0.117",
        "MMLU - ECE (10-bin)":"0.132"
    },
    {
        "Model":"davinci (175B)",
        "Mean win rate":"0.522",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.061",
        "HellaSwag - ECE (10-bin)":"0.31",
        "OpenbookQA - ECE (10-bin)":"0.204",
        "TruthfulQA - ECE (10-bin)":"0.211",
        "MMLU - ECE (10-bin)":"0.132"
    },
    {
        "Model":"curie (6.7B)",
        "Mean win rate":"0.633",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.017",
        "HellaSwag - ECE (10-bin)":"0.25",
        "OpenbookQA - ECE (10-bin)":"0.26",
        "TruthfulQA - ECE (10-bin)":"0.062",
        "MMLU - ECE (10-bin)":"0.138"
    },
    {
        "Model":"babbage (1.3B)",
        "Mean win rate":"0.544",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.016",
        "HellaSwag - ECE (10-bin)":"0.144",
        "OpenbookQA - ECE (10-bin)":"0.3",
        "TruthfulQA - ECE (10-bin)":"0.142",
        "MMLU - ECE (10-bin)":"0.14"
    },
    {
        "Model":"ada (350M)",
        "Mean win rate":"0.673",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.028",
        "HellaSwag - ECE (10-bin)":"0.057",
        "OpenbookQA - ECE (10-bin)":"0.346",
        "TruthfulQA - ECE (10-bin)":"0.071",
        "MMLU - ECE (10-bin)":"0.128"
    },
    {
        "Model":"text-davinci-003",
        "Mean win rate":"0.302",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.286",
        "HellaSwag - ECE (10-bin)":"0.278",
        "OpenbookQA - ECE (10-bin)":"0.216",
        "TruthfulQA - ECE (10-bin)":"0.348",
        "MMLU - ECE (10-bin)":"0.317"
    },
    {
        "Model":"text-davinci-002",
        "Mean win rate":"0.287",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.341",
        "HellaSwag - ECE (10-bin)":"0.286",
        "OpenbookQA - ECE (10-bin)":"0.238",
        "TruthfulQA - ECE (10-bin)":"0.199",
        "MMLU - ECE (10-bin)":"0.176"
    },
    {
        "Model":"text-curie-001",
        "Mean win rate":"0.231",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.253",
        "HellaSwag - ECE (10-bin)":"0.153",
        "OpenbookQA - ECE (10-bin)":"0.321",
        "TruthfulQA - ECE (10-bin)":"0.355",
        "MMLU - ECE (10-bin)":"0.462"
    },
    {
        "Model":"text-babbage-001",
        "Mean win rate":"0.263",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.522",
        "HellaSwag - ECE (10-bin)":"0.083",
        "OpenbookQA - ECE (10-bin)":"0.362",
        "TruthfulQA - ECE (10-bin)":"0.251",
        "MMLU - ECE (10-bin)":"0.311"
    },
    {
        "Model":"text-ada-001",
        "Mean win rate":"0.186",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.764",
        "HellaSwag - ECE (10-bin)":"0.103",
        "OpenbookQA - ECE (10-bin)":"0.487",
        "TruthfulQA - ECE (10-bin)":"0.465",
        "MMLU - ECE (10-bin)":"0.506"
    },
    {
        "Model":"gpt-3.5-turbo-0301",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "MMLU - ECE (10-bin)":"-"
    },
    {
        "Model":"gpt-3.5-turbo-0613",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "MMLU - ECE (10-bin)":"-"
    },
    {
        "Model":"RedPajama-INCITE-Base-v1 (3B)",
        "Mean win rate":"0.709",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.116",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.048",
        "MMLU - ECE (10-bin)":"0.115"
    },
    {
        "Model":"RedPajama-INCITE-Instruct-v1 (3B)",
        "Mean win rate":"0.525",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.12",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.097",
        "MMLU - ECE (10-bin)":"0.124"
    },
    {
        "Model":"RedPajama-INCITE-Base (7B)",
        "Mean win rate":"0.716",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.127",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.063",
        "MMLU - ECE (10-bin)":"0.098"
    },
    {
        "Model":"RedPajama-INCITE-Instruct (7B)",
        "Mean win rate":"0.241",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.142",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.232",
        "MMLU - ECE (10-bin)":"0.143"
    },
    {
        "Model":"MPT (30B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "MMLU - ECE (10-bin)":"-"
    },
    {
        "Model":"MPT-Instruct (30B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "MMLU - ECE (10-bin)":"-"
    },
    {
        "Model":"Falcon (7B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "MMLU - ECE (10-bin)":"-"
    },
    {
        "Model":"Falcon-Instruct (7B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "MMLU - ECE (10-bin)":"-"
    },
    {
        "Model":"Falcon (40B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "MMLU - ECE (10-bin)":"-"
    },
    {
        "Model":"Falcon-Instruct (40B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "MMLU - ECE (10-bin)":"-"
    },
    {
        "Model":"GLM (130B)",
        "Mean win rate":"0.738",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.022",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.088",
        "MMLU - ECE (10-bin)":"0.128"
    },
    {
        "Model":"InstructPalmyra (30B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "MMLU - ECE (10-bin)":"-"
    },
    {
        "Model":"Palmyra X (43B)",
        "Mean win rate":"-",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"-",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"-",
        "MMLU - ECE (10-bin)":"-"
    },
    {
        "Model":"YaLM (100B)",
        "Mean win rate":"0.277",
        "NaturalQuestions (closed-book) - ECE (10-bin)":"0.02",
        "HellaSwag - ECE (10-bin)":"-",
        "OpenbookQA - ECE (10-bin)":"-",
        "TruthfulQA - ECE (10-bin)":"0.679",
        "MMLU - ECE (10-bin)":"0.708"
    }
]