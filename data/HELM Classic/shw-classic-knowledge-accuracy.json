[
    {
        "Model":"J1-Jumbo v1 (178B)",
        "Mean win rate":"0.418",
        "NaturalQuestions (closed-book) - F1":"0.293",
        "HellaSwag - EM":"0.765",
        "OpenbookQA - EM":"0.534",
        "TruthfulQA - EM":"0.175",
        "MMLU - EM":"0.259",
        "WikiFact - EM":"0.28"
    },
    {
        "Model":"J1-Large v1 (7.5B)",
        "Mean win rate":"0.23",
        "NaturalQuestions (closed-book) - F1":"0.19",
        "HellaSwag - EM":"0.7",
        "OpenbookQA - EM":"0.514",
        "TruthfulQA - EM":"0.197",
        "MMLU - EM":"0.241",
        "WikiFact - EM":"0.226"
    },
    {
        "Model":"J1-Grande v1 (17B)",
        "Mean win rate":"0.37",
        "NaturalQuestions (closed-book) - F1":"0.233",
        "HellaSwag - EM":"0.739",
        "OpenbookQA - EM":"0.52",
        "TruthfulQA - EM":"0.193",
        "MMLU - EM":"0.27",
        "WikiFact - EM":"0.269"
    },
    {
        "Model":"J1-Grande v2 beta (17B)",
        "Mean win rate":"0.708",
        "NaturalQuestions (closed-book) - F1":"0.337",
        "HellaSwag - EM":"0.764",
        "OpenbookQA - EM":"0.56",
        "TruthfulQA - EM":"0.306",
        "MMLU - EM":"0.445",
        "WikiFact - EM":"0.313"
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "Mean win rate":"0.82",
        "NaturalQuestions (closed-book) - F1":"0.385",
        "HellaSwag - EM":"0.788",
        "OpenbookQA - EM":"0.558",
        "TruthfulQA - EM":"0.437",
        "MMLU - EM":"0.48",
        "WikiFact - EM":"0.343"
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "Mean win rate":"0.73",
        "NaturalQuestions (closed-book) - F1":"0.356",
        "HellaSwag - EM":"0.781",
        "OpenbookQA - EM":"0.542",
        "TruthfulQA - EM":"0.348",
        "MMLU - EM":"0.475",
        "WikiFact - EM":"0.32"
    },
    {
        "Model":"Jurassic-2 Large (7.5B)",
        "Mean win rate":"0.463",
        "NaturalQuestions (closed-book) - F1":"0.274",
        "HellaSwag - EM":"0.729",
        "OpenbookQA - EM":"0.53",
        "TruthfulQA - EM":"0.245",
        "MMLU - EM":"0.339",
        "WikiFact - EM":"0.222"
    },
    {
        "Model":"Luminous Base (13B)",
        "Mean win rate":"0.292",
        "NaturalQuestions (closed-book) - F1":"0.202",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.182",
        "MMLU - EM":"0.27",
        "WikiFact - EM":"0.275"
    },
    {
        "Model":"Luminous Extended (30B)",
        "Mean win rate":"0.504",
        "NaturalQuestions (closed-book) - F1":"0.254",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.221",
        "MMLU - EM":"0.321",
        "WikiFact - EM":"0.308"
    },
    {
        "Model":"Luminous Supreme (70B)",
        "Mean win rate":"0.576",
        "NaturalQuestions (closed-book) - F1":"0.293",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.222",
        "MMLU - EM":"0.38",
        "WikiFact - EM":"0.335"
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B)",
        "Mean win rate":"0.763",
        "NaturalQuestions (closed-book) - F1":"0.288",
        "HellaSwag - EM":"0.807",
        "OpenbookQA - EM":"0.558",
        "TruthfulQA - EM":"0.368",
        "MMLU - EM":"0.481",
        "WikiFact - EM":"0.336"
    },
    {
        "Model":"BLOOM (176B)",
        "Mean win rate":"0.399",
        "NaturalQuestions (closed-book) - F1":"0.216",
        "HellaSwag - EM":"0.744",
        "OpenbookQA - EM":"0.534",
        "TruthfulQA - EM":"0.205",
        "MMLU - EM":"0.299",
        "WikiFact - EM":"0.221"
    },
    {
        "Model":"T0pp (11B)",
        "Mean win rate":"0.375",
        "NaturalQuestions (closed-book) - F1":"0.039",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.377",
        "MMLU - EM":"0.407",
        "WikiFact - EM":"0.013"
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B)",
        "Mean win rate":"0.614",
        "NaturalQuestions (closed-book) - F1":"0.312",
        "HellaSwag - EM":"0.811",
        "OpenbookQA - EM":"0.55",
        "TruthfulQA - EM":"0.198",
        "MMLU - EM":"0.353",
        "WikiFact - EM":"0.336"
    },
    {
        "Model":"Cohere large v20220720 (13.1B)",
        "Mean win rate":"0.435",
        "NaturalQuestions (closed-book) - F1":"0.232",
        "HellaSwag - EM":"0.736",
        "OpenbookQA - EM":"0.542",
        "TruthfulQA - EM":"0.181",
        "MMLU - EM":"0.324",
        "WikiFact - EM":"0.286"
    },
    {
        "Model":"Cohere medium v20220720 (6.1B)",
        "Mean win rate":"0.275",
        "NaturalQuestions (closed-book) - F1":"0.177",
        "HellaSwag - EM":"0.706",
        "OpenbookQA - EM":"0.496",
        "TruthfulQA - EM":"0.19",
        "MMLU - EM":"0.279",
        "WikiFact - EM":"0.254"
    },
    {
        "Model":"Cohere small v20220720 (410M)",
        "Mean win rate":"0.145",
        "NaturalQuestions (closed-book) - F1":"0.078",
        "HellaSwag - EM":"0.483",
        "OpenbookQA - EM":"0.348",
        "TruthfulQA - EM":"0.217",
        "MMLU - EM":"0.264",
        "WikiFact - EM":"0.141"
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B)",
        "Mean win rate":"0.67",
        "NaturalQuestions (closed-book) - F1":"0.361",
        "HellaSwag - EM":"0.81",
        "OpenbookQA - EM":"0.588",
        "TruthfulQA - EM":"0.169",
        "MMLU - EM":"0.382",
        "WikiFact - EM":"0.342"
    },
    {
        "Model":"Cohere medium v20221108 (6.1B)",
        "Mean win rate":"0.376",
        "NaturalQuestions (closed-book) - F1":"0.199",
        "HellaSwag - EM":"0.726",
        "OpenbookQA - EM":"0.538",
        "TruthfulQA - EM":"0.215",
        "MMLU - EM":"0.254",
        "WikiFact - EM":"0.254"
    },
    {
        "Model":"Cohere Command beta (6.1B)",
        "Mean win rate":"0.525",
        "NaturalQuestions (closed-book) - F1":"0.229",
        "HellaSwag - EM":"0.752",
        "OpenbookQA - EM":"0.55",
        "TruthfulQA - EM":"0.203",
        "MMLU - EM":"0.406",
        "WikiFact - EM":"0.288"
    },
    {
        "Model":"Cohere Command beta (52.4B)",
        "Mean win rate":"0.811",
        "NaturalQuestions (closed-book) - F1":"0.372",
        "HellaSwag - EM":"0.811",
        "OpenbookQA - EM":"0.582",
        "TruthfulQA - EM":"0.269",
        "MMLU - EM":"0.452",
        "WikiFact - EM":"0.348"
    },
    {
        "Model":"GPT-J (6B)",
        "Mean win rate":"0.181",
        "NaturalQuestions (closed-book) - F1":"0.156",
        "HellaSwag - EM":"0.663",
        "OpenbookQA - EM":"0.514",
        "TruthfulQA - EM":"0.199",
        "MMLU - EM":"0.249",
        "WikiFact - EM":"0.168"
    },
    {
        "Model":"GPT-NeoX (20B)",
        "Mean win rate":"0.318",
        "NaturalQuestions (closed-book) - F1":"0.193",
        "HellaSwag - EM":"0.718",
        "OpenbookQA - EM":"0.524",
        "TruthfulQA - EM":"0.216",
        "MMLU - EM":"0.276",
        "WikiFact - EM":"0.207"
    },
    {
        "Model":"Pythia (6.9B)",
        "Mean win rate":"0.144",
        "NaturalQuestions (closed-book) - F1":"0.142",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.213",
        "MMLU - EM":"0.236",
        "WikiFact - EM":"0.159"
    },
    {
        "Model":"Pythia (12B)",
        "Mean win rate":"0.163",
        "NaturalQuestions (closed-book) - F1":"0.175",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.177",
        "MMLU - EM":"0.274",
        "WikiFact - EM":"0.175"
    },
    {
        "Model":"T5 (11B)",
        "Mean win rate":"0.155",
        "NaturalQuestions (closed-book) - F1":"0.194",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.133",
        "MMLU - EM":"0.29",
        "WikiFact - EM":"0.118"
    },
    {
        "Model":"UL2 (20B)",
        "Mean win rate":"0.246",
        "NaturalQuestions (closed-book) - F1":"0.204",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.193",
        "MMLU - EM":"0.291",
        "WikiFact - EM":"0.168"
    },
    {
        "Model":"OPT (175B)",
        "Mean win rate":"0.597",
        "NaturalQuestions (closed-book) - F1":"0.297",
        "HellaSwag - EM":"0.791",
        "OpenbookQA - EM":"0.586",
        "TruthfulQA - EM":"0.25",
        "MMLU - EM":"0.318",
        "WikiFact - EM":"0.22"
    },
    {
        "Model":"OPT (66B)",
        "Mean win rate":"0.384",
        "NaturalQuestions (closed-book) - F1":"0.258",
        "HellaSwag - EM":"0.745",
        "OpenbookQA - EM":"0.534",
        "TruthfulQA - EM":"0.201",
        "MMLU - EM":"0.276",
        "WikiFact - EM":"0.202"
    },
    {
        "Model":"LLaMA (7B)",
        "Mean win rate":"0.489",
        "NaturalQuestions (closed-book) - F1":"0.297",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.28",
        "MMLU - EM":"0.321",
        "WikiFact - EM":"0.184"
    },
    {
        "Model":"LLaMA (13B)",
        "Mean win rate":"0.636",
        "NaturalQuestions (closed-book) - F1":"0.346",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.324",
        "MMLU - EM":"0.422",
        "WikiFact - EM":"0.233"
    },
    {
        "Model":"LLaMA (30B)",
        "Mean win rate":"0.799",
        "NaturalQuestions (closed-book) - F1":"0.408",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.344",
        "MMLU - EM":"0.531",
        "WikiFact - EM":"0.275"
    },
    {
        "Model":"LLaMA (65B)",
        "Mean win rate":"0.966",
        "NaturalQuestions (closed-book) - F1":"0.431",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.508",
        "MMLU - EM":"0.584",
        "WikiFact - EM":"0.421"
    },
    {
        "Model":"Llama 2 (7B)",
        "Mean win rate":"0.697",
        "NaturalQuestions (closed-book) - F1":"0.337",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.272",
        "MMLU - EM":"0.431",
        "WikiFact - EM":"0.335"
    },
    {
        "Model":"Llama 2 (13B)",
        "Mean win rate":"0.841",
        "NaturalQuestions (closed-book) - F1":"0.376",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.33",
        "MMLU - EM":"0.507",
        "WikiFact - EM":"0.365"
    },
    {
        "Model":"Llama 2 (70B)",
        "Mean win rate":"0.973",
        "NaturalQuestions (closed-book) - F1":"0.458",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.554",
        "MMLU - EM":"0.582",
        "WikiFact - EM":"0.449"
    },
    {
        "Model":"Alpaca (7B)",
        "Mean win rate":"0.5",
        "NaturalQuestions (closed-book) - F1":"0.266",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.243",
        "MMLU - EM":"0.385",
        "WikiFact - EM":"0.224"
    },
    {
        "Model":"Vicuna v1.3 (7B)",
        "Mean win rate":"0.568",
        "NaturalQuestions (closed-book) - F1":"0.287",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.292",
        "MMLU - EM":"0.434",
        "WikiFact - EM":"0.22"
    },
    {
        "Model":"Vicuna v1.3 (13B)",
        "Mean win rate":"0.72",
        "NaturalQuestions (closed-book) - F1":"0.346",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.385",
        "MMLU - EM":"0.462",
        "WikiFact - EM":"0.268"
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "Mean win rate":"0.879",
        "NaturalQuestions (closed-book) - F1":"0.365",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.422",
        "MMLU - EM":"0.572",
        "WikiFact - EM":"0.349"
    },
    {
        "Model":"TNLG v2 (530B)",
        "Mean win rate":"0.784",
        "NaturalQuestions (closed-book) - F1":"0.384",
        "HellaSwag - EM":"0.799",
        "OpenbookQA - EM":"0.562",
        "TruthfulQA - EM":"0.251",
        "MMLU - EM":"0.469",
        "WikiFact - EM":"0.337"
    },
    {
        "Model":"TNLG v2 (6.7B)",
        "Mean win rate":"0.227",
        "NaturalQuestions (closed-book) - F1":"0.21",
        "HellaSwag - EM":"0.704",
        "OpenbookQA - EM":"0.478",
        "TruthfulQA - EM":"0.167",
        "MMLU - EM":"0.242",
        "WikiFact - EM":"0.236"
    },
    {
        "Model":"davinci (175B)",
        "Mean win rate":"0.622",
        "NaturalQuestions (closed-book) - F1":"0.329",
        "HellaSwag - EM":"0.775",
        "OpenbookQA - EM":"0.586",
        "TruthfulQA - EM":"0.194",
        "MMLU - EM":"0.422",
        "WikiFact - EM":"0.306"
    },
    {
        "Model":"curie (6.7B)",
        "Mean win rate":"0.292",
        "NaturalQuestions (closed-book) - F1":"0.199",
        "HellaSwag - EM":"0.682",
        "OpenbookQA - EM":"0.502",
        "TruthfulQA - EM":"0.232",
        "MMLU - EM":"0.243",
        "WikiFact - EM":"0.236"
    },
    {
        "Model":"babbage (1.3B)",
        "Mean win rate":"0.103",
        "NaturalQuestions (closed-book) - F1":"0.119",
        "HellaSwag - EM":"0.555",
        "OpenbookQA - EM":"0.438",
        "TruthfulQA - EM":"0.188",
        "MMLU - EM":"0.235",
        "WikiFact - EM":"0.184"
    },
    {
        "Model":"ada (350M)",
        "Mean win rate":"0.117",
        "NaturalQuestions (closed-book) - F1":"0.082",
        "HellaSwag - EM":"0.435",
        "OpenbookQA - EM":"0.38",
        "TruthfulQA - EM":"0.215",
        "MMLU - EM":"0.243",
        "WikiFact - EM":"0.124"
    },
    {
        "Model":"text-davinci-003",
        "Mean win rate":"0.96",
        "NaturalQuestions (closed-book) - F1":"0.406",
        "HellaSwag - EM":"0.822",
        "OpenbookQA - EM":"0.646",
        "TruthfulQA - EM":"0.593",
        "MMLU - EM":"0.569",
        "WikiFact - EM":"0.373"
    },
    {
        "Model":"text-davinci-002",
        "Mean win rate":"0.944",
        "NaturalQuestions (closed-book) - F1":"0.383",
        "HellaSwag - EM":"0.815",
        "OpenbookQA - EM":"0.594",
        "TruthfulQA - EM":"0.61",
        "MMLU - EM":"0.568",
        "WikiFact - EM":"0.392"
    },
    {
        "Model":"text-curie-001",
        "Mean win rate":"0.278",
        "NaturalQuestions (closed-book) - F1":"0.175",
        "HellaSwag - EM":"0.676",
        "OpenbookQA - EM":"0.514",
        "TruthfulQA - EM":"0.257",
        "MMLU - EM":"0.237",
        "WikiFact - EM":"0.214"
    },
    {
        "Model":"text-babbage-001",
        "Mean win rate":"0.152",
        "NaturalQuestions (closed-book) - F1":"0.07",
        "HellaSwag - EM":"0.561",
        "OpenbookQA - EM":"0.452",
        "TruthfulQA - EM":"0.233",
        "MMLU - EM":"0.229",
        "WikiFact - EM":"0.15"
    },
    {
        "Model":"text-ada-001",
        "Mean win rate":"0.101",
        "NaturalQuestions (closed-book) - F1":"0.025",
        "HellaSwag - EM":"0.429",
        "OpenbookQA - EM":"0.346",
        "TruthfulQA - EM":"0.232",
        "MMLU - EM":"0.238",
        "WikiFact - EM":"0.119"
    },
    {
        "Model":"gpt-3.5-turbo-0301",
        "Mean win rate":"0.86",
        "NaturalQuestions (closed-book) - F1":"0.39",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.609",
        "MMLU - EM":"0.59",
        "WikiFact - EM":"0.279"
    },
    {
        "Model":"gpt-3.5-turbo-0613",
        "Mean win rate":"0.689",
        "NaturalQuestions (closed-book) - F1":"0.348",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.339",
        "MMLU - EM":"0.391",
        "WikiFact - EM":"0.289"
    },
    {
        "Model":"RedPajama-INCITE-Base-v1 (3B)",
        "Mean win rate":"0.352",
        "NaturalQuestions (closed-book) - F1":"0.207",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.277",
        "MMLU - EM":"0.263",
        "WikiFact - EM":"0.177"
    },
    {
        "Model":"RedPajama-INCITE-Instruct-v1 (3B)",
        "Mean win rate":"0.239",
        "NaturalQuestions (closed-book) - F1":"0.203",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.208",
        "MMLU - EM":"0.257",
        "WikiFact - EM":"0.174"
    },
    {
        "Model":"RedPajama-INCITE-Base (7B)",
        "Mean win rate":"0.352",
        "NaturalQuestions (closed-book) - F1":"0.25",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.205",
        "MMLU - EM":"0.302",
        "WikiFact - EM":"0.207"
    },
    {
        "Model":"RedPajama-INCITE-Instruct (7B)",
        "Mean win rate":"0.451",
        "NaturalQuestions (closed-book) - F1":"0.232",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.243",
        "MMLU - EM":"0.363",
        "WikiFact - EM":"0.211"
    },
    {
        "Model":"MPT (30B)",
        "Mean win rate":"0.712",
        "NaturalQuestions (closed-book) - F1":"0.347",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.231",
        "MMLU - EM":"0.437",
        "WikiFact - EM":"0.369"
    },
    {
        "Model":"MPT-Instruct (30B)",
        "Mean win rate":"0.652",
        "NaturalQuestions (closed-book) - F1":"0.304",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.234",
        "MMLU - EM":"0.444",
        "WikiFact - EM":"0.328"
    },
    {
        "Model":"Falcon (7B)",
        "Mean win rate":"0.523",
        "NaturalQuestions (closed-book) - F1":"0.285",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.234",
        "MMLU - EM":"0.286",
        "WikiFact - EM":"0.309"
    },
    {
        "Model":"Falcon-Instruct (7B)",
        "Mean win rate":"0.322",
        "NaturalQuestions (closed-book) - F1":"0.194",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.213",
        "MMLU - EM":"0.275",
        "WikiFact - EM":"0.232"
    },
    {
        "Model":"Falcon (40B)",
        "Mean win rate":"0.894",
        "NaturalQuestions (closed-book) - F1":"0.392",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.353",
        "MMLU - EM":"0.509",
        "WikiFact - EM":"0.38"
    },
    {
        "Model":"Falcon-Instruct (40B)",
        "Mean win rate":"0.864",
        "NaturalQuestions (closed-book) - F1":"0.377",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.384",
        "MMLU - EM":"0.497",
        "WikiFact - EM":"0.359"
    },
    {
        "Model":"GLM (130B)",
        "Mean win rate":"0.379",
        "NaturalQuestions (closed-book) - F1":"0.148",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.218",
        "MMLU - EM":"0.344",
        "WikiFact - EM":"0.237"
    },
    {
        "Model":"InstructPalmyra (30B)",
        "Mean win rate":"0.413",
        "NaturalQuestions (closed-book) - F1":"0.33",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.185",
        "MMLU - EM":"0.403",
        "WikiFact - EM":"0.209"
    },
    {
        "Model":"Palmyra X (43B)",
        "Mean win rate":"0.947",
        "NaturalQuestions (closed-book) - F1":"0.413",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.616",
        "MMLU - EM":"0.609",
        "WikiFact - EM":"0.338"
    },
    {
        "Model":"YaLM (100B)",
        "Mean win rate":"0.11",
        "NaturalQuestions (closed-book) - F1":"0.068",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.202",
        "MMLU - EM":"0.243",
        "WikiFact - EM":"0.049"
    }
]