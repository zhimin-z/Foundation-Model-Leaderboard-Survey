[
    {
        "Model":"J1-Jumbo v1 (178B)",
        "Mean win rate":"0.52",
        "The Pile - BPB":"0.545",
        "TwitterAAE - BPB":"2.18",
        "ICE - BPB":"0.849",
        "BLiMP - EM":"0.811",
        "NaturalQuestions (closed-book) - F1":"0.293",
        "HellaSwag - EM":"0.765",
        "OpenbookQA - EM":"0.534",
        "TruthfulQA - EM":"0.175",
        "MMLU - EM":"0.259",
        "WikiFact - EM":"0.28",
        "Synthetic reasoning (abstract symbols) - EM":"0.263",
        "Synthetic reasoning (natural language) - F1":"0.174",
        "bAbI - EM":"0.543",
        "Dyck - EM":"0.445",
        "GSM8K - EM":"0.054",
        "MATH - Equivalent":"0.089",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.033",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.232",
        "LegalSupport - EM":"0.484",
        "Data imputation - EM":"0.735",
        "Entity matching - EM":"0.841",
        "BBQ - EM":"0.378"
    },
    {
        "Model":"J1-Large v1 (7.5B)",
        "Mean win rate":"0.348",
        "The Pile - BPB":"0.645",
        "TwitterAAE - BPB":"2.245",
        "ICE - BPB":"0.895",
        "BLiMP - EM":"0.806",
        "NaturalQuestions (closed-book) - F1":"0.19",
        "HellaSwag - EM":"0.7",
        "OpenbookQA - EM":"0.514",
        "TruthfulQA - EM":"0.197",
        "MMLU - EM":"0.241",
        "WikiFact - EM":"0.226",
        "Synthetic reasoning (abstract symbols) - EM":"0.201",
        "Synthetic reasoning (natural language) - F1":"0.154",
        "bAbI - EM":"0.469",
        "Dyck - EM":"0.414",
        "GSM8K - EM":"0.014",
        "MATH - Equivalent":"0.049",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.031",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.196",
        "LegalSupport - EM":"0.514",
        "Data imputation - EM":"0.729",
        "Entity matching - EM":"0.827",
        "BBQ - EM":"0.352"
    },
    {
        "Model":"J1-Grande v1 (17B)",
        "Mean win rate":"0.458",
        "The Pile - BPB":"0.6",
        "TwitterAAE - BPB":"2.205",
        "ICE - BPB":"0.868",
        "BLiMP - EM":"0.803",
        "NaturalQuestions (closed-book) - F1":"0.233",
        "HellaSwag - EM":"0.739",
        "OpenbookQA - EM":"0.52",
        "TruthfulQA - EM":"0.193",
        "MMLU - EM":"0.27",
        "WikiFact - EM":"0.269",
        "Synthetic reasoning (abstract symbols) - EM":"0.247",
        "Synthetic reasoning (natural language) - F1":"0.154",
        "bAbI - EM":"0.458",
        "Dyck - EM":"0.696",
        "GSM8K - EM":"0.054",
        "MATH - Equivalent":"0.08",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.045",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.188",
        "LegalSupport - EM":"0.504",
        "Data imputation - EM":"0.729",
        "Entity matching - EM":"0.831",
        "BBQ - EM":"0.361"
    },
    {
        "Model":"J1-Grande v2 beta (17B)",
        "Mean win rate":"0.624",
        "The Pile - BPB":"0.602",
        "TwitterAAE - BPB":"2.156",
        "ICE - BPB":"0.836",
        "BLiMP - EM":"0.805",
        "NaturalQuestions (closed-book) - F1":"0.337",
        "HellaSwag - EM":"0.764",
        "OpenbookQA - EM":"0.56",
        "TruthfulQA - EM":"0.306",
        "MMLU - EM":"0.445",
        "WikiFact - EM":"0.313",
        "Synthetic reasoning (abstract symbols) - EM":"0.286",
        "Synthetic reasoning (natural language) - F1":"0.139",
        "bAbI - EM":"0.47",
        "Dyck - EM":"0.617",
        "GSM8K - EM":"0.096",
        "MATH - Equivalent":"0.127",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.068",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.191",
        "LegalSupport - EM":"0.562",
        "Data imputation - EM":"0.8",
        "Entity matching - EM":"0.844",
        "BBQ - EM":"0.476"
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "Mean win rate":"0.777",
        "The Pile - BPB":"0.533",
        "TwitterAAE - BPB":"2.121",
        "ICE - BPB":"0.799",
        "BLiMP - EM":"0.81",
        "NaturalQuestions (closed-book) - F1":"0.385",
        "HellaSwag - EM":"0.788",
        "OpenbookQA - EM":"0.558",
        "TruthfulQA - EM":"0.437",
        "MMLU - EM":"0.48",
        "WikiFact - EM":"0.343",
        "Synthetic reasoning (abstract symbols) - EM":"0.393",
        "Synthetic reasoning (natural language) - F1":"0.144",
        "bAbI - EM":"0.521",
        "Dyck - EM":"0.709",
        "GSM8K - EM":"0.225",
        "MATH - Equivalent":"0.196",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.086",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.219",
        "LegalSupport - EM":"0.639",
        "Data imputation - EM":"0.841",
        "Entity matching - EM":"0.79",
        "BBQ - EM":"0.55"
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "Mean win rate":"0.671",
        "The Pile - BPB":"0.593",
        "TwitterAAE - BPB":"2.137",
        "ICE - BPB":"0.83",
        "BLiMP - EM":"0.809",
        "NaturalQuestions (closed-book) - F1":"0.356",
        "HellaSwag - EM":"0.781",
        "OpenbookQA - EM":"0.542",
        "TruthfulQA - EM":"0.348",
        "MMLU - EM":"0.475",
        "WikiFact - EM":"0.32",
        "Synthetic reasoning (abstract symbols) - EM":"0.301",
        "Synthetic reasoning (natural language) - F1":"0.164",
        "bAbI - EM":"0.455",
        "Dyck - EM":"0.529",
        "GSM8K - EM":"0.133",
        "MATH - Equivalent":"0.146",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.054",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.22",
        "LegalSupport - EM":"0.575",
        "Data imputation - EM":"0.829",
        "Entity matching - EM":"0.849",
        "BBQ - EM":"0.494"
    },
    {
        "Model":"Jurassic-2 Large (7.5B)",
        "Mean win rate":"0.495",
        "The Pile - BPB":"0.639",
        "TwitterAAE - BPB":"2.179",
        "ICE - BPB":"0.865",
        "BLiMP - EM":"0.816",
        "NaturalQuestions (closed-book) - F1":"0.274",
        "HellaSwag - EM":"0.729",
        "OpenbookQA - EM":"0.53",
        "TruthfulQA - EM":"0.245",
        "MMLU - EM":"0.339",
        "WikiFact - EM":"0.222",
        "Synthetic reasoning (abstract symbols) - EM":"0.192",
        "Synthetic reasoning (natural language) - F1":"0.176",
        "bAbI - EM":"0.504",
        "Dyck - EM":"0.559",
        "GSM8K - EM":"0.03",
        "MATH - Equivalent":"0.07",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.023",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.217",
        "LegalSupport - EM":"0.558",
        "Data imputation - EM":"0.731",
        "Entity matching - EM":"0.836",
        "BBQ - EM":"0.347"
    },
    {
        "Model":"Luminous Base (13B)",
        "Mean win rate":"0.336",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.202",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.182",
        "MMLU - EM":"0.27",
        "WikiFact - EM":"0.275",
        "Synthetic reasoning (abstract symbols) - EM":"0.209",
        "Synthetic reasoning (natural language) - F1":"0",
        "bAbI - EM":"0.452",
        "Dyck - EM":"0.517",
        "GSM8K - EM":"0.026",
        "MATH - Equivalent":"0.089",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.026",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.235",
        "LegalSupport - EM":"0.513",
        "Data imputation - EM":"0.725",
        "Entity matching - EM":"0.543",
        "BBQ - EM":"0.333"
    },
    {
        "Model":"Luminous Extended (30B)",
        "Mean win rate":"0.448",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.254",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.221",
        "MMLU - EM":"0.321",
        "WikiFact - EM":"0.308",
        "Synthetic reasoning (abstract symbols) - EM":"0.225",
        "Synthetic reasoning (natural language) - F1":"0",
        "bAbI - EM":"0.475",
        "Dyck - EM":"0.666",
        "GSM8K - EM":"0.067",
        "MATH - Equivalent":"0.111",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.035",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.188",
        "LegalSupport - EM":"0.517",
        "Data imputation - EM":"0.722",
        "Entity matching - EM":"0.635",
        "BBQ - EM":"0.355"
    },
    {
        "Model":"Luminous Supreme (70B)",
        "Mean win rate":"0.586",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.293",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.222",
        "MMLU - EM":"0.38",
        "WikiFact - EM":"0.335",
        "Synthetic reasoning (abstract symbols) - EM":"0.312",
        "Synthetic reasoning (natural language) - F1":"0",
        "bAbI - EM":"0.504",
        "Dyck - EM":"0.729",
        "GSM8K - EM":"0.112",
        "MATH - Equivalent":"0.149",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.057",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.212",
        "LegalSupport - EM":"0.53",
        "Data imputation - EM":"0.758",
        "Entity matching - EM":"0.707",
        "BBQ - EM":"0.411"
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B)",
        "Mean win rate":"0.749",
        "The Pile - BPB":"0.597",
        "TwitterAAE - BPB":"2.109",
        "ICE - BPB":"0.822",
        "BLiMP - EM":"0.829",
        "NaturalQuestions (closed-book) - F1":"0.288",
        "HellaSwag - EM":"0.807",
        "OpenbookQA - EM":"0.558",
        "TruthfulQA - EM":"0.368",
        "MMLU - EM":"0.481",
        "WikiFact - EM":"0.336",
        "Synthetic reasoning (abstract symbols) - EM":"0.432",
        "Synthetic reasoning (natural language) - F1":"0.259",
        "bAbI - EM":"0.461",
        "Dyck - EM":"0.849",
        "GSM8K - EM":"0.171",
        "MATH - Equivalent":"0.198",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.162",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.213",
        "LegalSupport - EM":"0.624",
        "Data imputation - EM":"0.733",
        "Entity matching - EM":"0.71",
        "BBQ - EM":"0.551"
    },
    {
        "Model":"BLOOM (176B)",
        "Mean win rate":"0.519",
        "The Pile - BPB":"0.571",
        "TwitterAAE - BPB":"1.986",
        "ICE - BPB":"0.715",
        "BLiMP - EM":"0.819",
        "NaturalQuestions (closed-book) - F1":"0.216",
        "HellaSwag - EM":"0.744",
        "OpenbookQA - EM":"0.534",
        "TruthfulQA - EM":"0.205",
        "MMLU - EM":"0.299",
        "WikiFact - EM":"0.221",
        "Synthetic reasoning (abstract symbols) - EM":"0.304",
        "Synthetic reasoning (natural language) - F1":"0.197",
        "bAbI - EM":"0.447",
        "Dyck - EM":"0.545",
        "GSM8K - EM":"0.095",
        "MATH - Equivalent":"0.043",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.055",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.209",
        "LegalSupport - EM":"0.543",
        "Data imputation - EM":"0.677",
        "Entity matching - EM":"0.852",
        "BBQ - EM":"0.375"
    },
    {
        "Model":"T0pp (11B)",
        "Mean win rate":"0.234",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.039",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.377",
        "MMLU - EM":"0.407",
        "WikiFact - EM":"0.013",
        "Synthetic reasoning (abstract symbols) - EM":"0",
        "Synthetic reasoning (natural language) - F1":"0.002",
        "bAbI - EM":"0",
        "Dyck - EM":"0.011",
        "GSM8K - EM":"0",
        "MATH - Equivalent":"0",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.186",
        "LegalSupport - EM":"0.611",
        "Data imputation - EM":"0.004",
        "Entity matching - EM":"0",
        "BBQ - EM":"0.491"
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B)",
        "Mean win rate":"0.532",
        "The Pile - BPB":"0.757",
        "TwitterAAE - BPB":"2.305",
        "ICE - BPB":"0.875",
        "BLiMP - EM":"0.83",
        "NaturalQuestions (closed-book) - F1":"0.312",
        "HellaSwag - EM":"0.811",
        "OpenbookQA - EM":"0.55",
        "TruthfulQA - EM":"0.198",
        "MMLU - EM":"0.353",
        "WikiFact - EM":"0.336",
        "Synthetic reasoning (abstract symbols) - EM":"0.194",
        "Synthetic reasoning (natural language) - F1":"0",
        "bAbI - EM":"0.486",
        "Dyck - EM":"0.594",
        "GSM8K - EM":"0.07",
        "MATH - Equivalent":"0.135",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.054",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.2",
        "LegalSupport - EM":"0.558",
        "Data imputation - EM":"0.785",
        "Entity matching - EM":"0.823",
        "BBQ - EM":"0.345"
    },
    {
        "Model":"Cohere large v20220720 (13.1B)",
        "Mean win rate":"0.359",
        "The Pile - BPB":"0.811",
        "TwitterAAE - BPB":"2.324",
        "ICE - BPB":"0.916",
        "BLiMP - EM":"0.833",
        "NaturalQuestions (closed-book) - F1":"0.232",
        "HellaSwag - EM":"0.736",
        "OpenbookQA - EM":"0.542",
        "TruthfulQA - EM":"0.181",
        "MMLU - EM":"0.324",
        "WikiFact - EM":"0.286",
        "Synthetic reasoning (abstract symbols) - EM":"0.128",
        "Synthetic reasoning (natural language) - F1":"0",
        "bAbI - EM":"0.36",
        "Dyck - EM":"0.531",
        "GSM8K - EM":"0.018",
        "MATH - Equivalent":"0.073",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.035",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.193",
        "LegalSupport - EM":"0.491",
        "Data imputation - EM":"0.71",
        "Entity matching - EM":"0.713",
        "BBQ - EM":"0.328"
    },
    {
        "Model":"Cohere medium v20220720 (6.1B)",
        "Mean win rate":"0.284",
        "The Pile - BPB":"0.845",
        "TwitterAAE - BPB":"2.341",
        "ICE - BPB":"0.954",
        "BLiMP - EM":"0.823",
        "NaturalQuestions (closed-book) - F1":"0.177",
        "HellaSwag - EM":"0.706",
        "OpenbookQA - EM":"0.496",
        "TruthfulQA - EM":"0.19",
        "MMLU - EM":"0.279",
        "WikiFact - EM":"0.254",
        "Synthetic reasoning (abstract symbols) - EM":"0.129",
        "Synthetic reasoning (natural language) - F1":"0",
        "bAbI - EM":"0.391",
        "Dyck - EM":"0.511",
        "GSM8K - EM":"0.015",
        "MATH - Equivalent":"0.049",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.027",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.212",
        "LegalSupport - EM":"0.507",
        "Data imputation - EM":"0.721",
        "Entity matching - EM":"0.482",
        "BBQ - EM":"0.344"
    },
    {
        "Model":"Cohere small v20220720 (410M)",
        "Mean win rate":"0.152",
        "The Pile - BPB":"0.996",
        "TwitterAAE - BPB":"2.525",
        "ICE - BPB":"1.112",
        "BLiMP - EM":"0.793",
        "NaturalQuestions (closed-book) - F1":"0.078",
        "HellaSwag - EM":"0.483",
        "OpenbookQA - EM":"0.348",
        "TruthfulQA - EM":"0.217",
        "MMLU - EM":"0.264",
        "WikiFact - EM":"0.141",
        "Synthetic reasoning (abstract symbols) - EM":"0.121",
        "Synthetic reasoning (natural language) - F1":"0",
        "bAbI - EM":"0.306",
        "Dyck - EM":"0.358",
        "GSM8K - EM":"0.004",
        "MATH - Equivalent":"0.016",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.003",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.187",
        "LegalSupport - EM":"0.524",
        "Data imputation - EM":"0.47",
        "Entity matching - EM":"0.176",
        "BBQ - EM":"0.363"
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B)",
        "Mean win rate":"0.561",
        "The Pile - BPB":"0.741",
        "TwitterAAE - BPB":"2.325",
        "ICE - BPB":"0.923",
        "BLiMP - EM":"0.832",
        "NaturalQuestions (closed-book) - F1":"0.361",
        "HellaSwag - EM":"0.81",
        "OpenbookQA - EM":"0.588",
        "TruthfulQA - EM":"0.169",
        "MMLU - EM":"0.382",
        "WikiFact - EM":"0.342",
        "Synthetic reasoning (abstract symbols) - EM":"0.229",
        "Synthetic reasoning (natural language) - F1":"0",
        "bAbI - EM":"0.439",
        "Dyck - EM":"0.587",
        "GSM8K - EM":"0.1",
        "MATH - Equivalent":"0.132",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.063",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.204",
        "LegalSupport - EM":"0.526",
        "Data imputation - EM":"0.803",
        "Entity matching - EM":"0.812",
        "BBQ - EM":"0.397"
    },
    {
        "Model":"Cohere medium v20221108 (6.1B)",
        "Mean win rate":"0.314",
        "The Pile - BPB":"0.858",
        "TwitterAAE - BPB":"2.428",
        "ICE - BPB":"1.009",
        "BLiMP - EM":"0.832",
        "NaturalQuestions (closed-book) - F1":"0.199",
        "HellaSwag - EM":"0.726",
        "OpenbookQA - EM":"0.538",
        "TruthfulQA - EM":"0.215",
        "MMLU - EM":"0.254",
        "WikiFact - EM":"0.254",
        "Synthetic reasoning (abstract symbols) - EM":"0.096",
        "Synthetic reasoning (natural language) - F1":"0",
        "bAbI - EM":"0.471",
        "Dyck - EM":"0.411",
        "GSM8K - EM":"0.017",
        "MATH - Equivalent":"0.052",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.021",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.206",
        "LegalSupport - EM":"0.489",
        "Data imputation - EM":"0.72",
        "Entity matching - EM":"0.535",
        "BBQ - EM":"0.359"
    },
    {
        "Model":"Cohere Command beta (6.1B)",
        "Mean win rate":"0.35",
        "The Pile - BPB":"0.875",
        "TwitterAAE - BPB":"2.61",
        "ICE - BPB":"1.024",
        "BLiMP - EM":"0.79",
        "NaturalQuestions (closed-book) - F1":"0.229",
        "HellaSwag - EM":"0.752",
        "OpenbookQA - EM":"0.55",
        "TruthfulQA - EM":"0.203",
        "MMLU - EM":"0.406",
        "WikiFact - EM":"0.288",
        "Synthetic reasoning (abstract symbols) - EM":"0.123",
        "Synthetic reasoning (natural language) - F1":"0.254",
        "bAbI - EM":"0.473",
        "Dyck - EM":"0.372",
        "GSM8K - EM":"0.036",
        "MATH - Equivalent":"0.076",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.038",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.178",
        "LegalSupport - EM":"0.566",
        "Data imputation - EM":"0.696",
        "Entity matching - EM":"0.532",
        "BBQ - EM":"0.302"
    },
    {
        "Model":"Cohere Command beta (52.4B)",
        "Mean win rate":"0.614",
        "The Pile - BPB":"0.781",
        "TwitterAAE - BPB":"2.73",
        "ICE - BPB":"0.943",
        "BLiMP - EM":"0.805",
        "NaturalQuestions (closed-book) - F1":"0.372",
        "HellaSwag - EM":"0.811",
        "OpenbookQA - EM":"0.582",
        "TruthfulQA - EM":"0.269",
        "MMLU - EM":"0.452",
        "WikiFact - EM":"0.348",
        "Synthetic reasoning (abstract symbols) - EM":"0.243",
        "Synthetic reasoning (natural language) - F1":"0.245",
        "bAbI - EM":"0.497",
        "Dyck - EM":"0.421",
        "GSM8K - EM":"0.138",
        "MATH - Equivalent":"0.133",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.075",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.229",
        "LegalSupport - EM":"0.606",
        "Data imputation - EM":"0.752",
        "Entity matching - EM":"0.569",
        "BBQ - EM":"0.463"
    },
    {
        "Model":"GPT-J (6B)",
        "Mean win rate":"0.33",
        "The Pile - BPB":"0.49",
        "TwitterAAE - BPB":"1.989",
        "ICE - BPB":"0.752",
        "BLiMP - EM":"0.834",
        "NaturalQuestions (closed-book) - F1":"0.156",
        "HellaSwag - EM":"0.663",
        "OpenbookQA - EM":"0.514",
        "TruthfulQA - EM":"0.199",
        "MMLU - EM":"0.249",
        "WikiFact - EM":"0.168",
        "Synthetic reasoning (abstract symbols) - EM":"0.174",
        "Synthetic reasoning (natural language) - F1":"0.199",
        "bAbI - EM":"0.459",
        "Dyck - EM":"0.337",
        "GSM8K - EM":"0.036",
        "MATH - Equivalent":"0.111",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.042",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.175",
        "LegalSupport - EM":"0.479",
        "Data imputation - EM":"0.673",
        "Entity matching - EM":"0.194",
        "BBQ - EM":"0.307"
    },
    {
        "Model":"GPT-NeoX (20B)",
        "Mean win rate":"0.499",
        "The Pile - BPB":"0.466",
        "TwitterAAE - BPB":"1.943",
        "ICE - BPB":"0.742",
        "BLiMP - EM":"0.839",
        "NaturalQuestions (closed-book) - F1":"0.193",
        "HellaSwag - EM":"0.718",
        "OpenbookQA - EM":"0.524",
        "TruthfulQA - EM":"0.216",
        "MMLU - EM":"0.276",
        "WikiFact - EM":"0.207",
        "Synthetic reasoning (abstract symbols) - EM":"0.204",
        "Synthetic reasoning (natural language) - F1":"0.167",
        "bAbI - EM":"0.468",
        "Dyck - EM":"0.747",
        "GSM8K - EM":"0.053",
        "MATH - Equivalent":"0.141",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.071",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.191",
        "LegalSupport - EM":"0.515",
        "Data imputation - EM":"0.705",
        "Entity matching - EM":"0.82",
        "BBQ - EM":"0.316"
    },
    {
        "Model":"Pythia (1B)",
        "Mean win rate":"0.116",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"-",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"-",
        "MMLU - EM":"-",
        "WikiFact - EM":"-",
        "Synthetic reasoning (abstract symbols) - EM":"-",
        "Synthetic reasoning (natural language) - F1":"-",
        "bAbI - EM":"0.379",
        "Dyck - EM":"-",
        "GSM8K - EM":"-",
        "MATH - Equivalent":"-",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"-",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"-",
        "LegalSupport - EM":"-",
        "Data imputation - EM":"-",
        "Entity matching - EM":"-",
        "BBQ - EM":"-"
    },
    {
        "Model":"Pythia (6.9B)",
        "Mean win rate":"0.323",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.142",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.213",
        "MMLU - EM":"0.236",
        "WikiFact - EM":"0.159",
        "Synthetic reasoning (abstract symbols) - EM":"0.139",
        "Synthetic reasoning (natural language) - F1":"0",
        "bAbI - EM":"0.479",
        "Dyck - EM":"0.652",
        "GSM8K - EM":"0.014",
        "MATH - Equivalent":"0.091",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.048",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.196",
        "LegalSupport - EM":"0.521",
        "Data imputation - EM":"0.644",
        "Entity matching - EM":"0.824",
        "BBQ - EM":"-"
    },
    {
        "Model":"Pythia (12B)",
        "Mean win rate":"0.345",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.175",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.177",
        "MMLU - EM":"0.274",
        "WikiFact - EM":"0.175",
        "Synthetic reasoning (abstract symbols) - EM":"0.176",
        "Synthetic reasoning (natural language) - F1":"0",
        "bAbI - EM":"0.491",
        "Dyck - EM":"0.798",
        "GSM8K - EM":"0.032",
        "MATH - Equivalent":"0.1",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.068",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.183",
        "LegalSupport - EM":"0.491",
        "Data imputation - EM":"0.683",
        "Entity matching - EM":"0.78",
        "BBQ - EM":"-"
    },
    {
        "Model":"T5 (11B)",
        "Mean win rate":"0.197",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.194",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.133",
        "MMLU - EM":"0.29",
        "WikiFact - EM":"0.118",
        "Synthetic reasoning (abstract symbols) - EM":"0.196",
        "Synthetic reasoning (natural language) - F1":"0.101",
        "bAbI - EM":"0.412",
        "Dyck - EM":"0.347",
        "GSM8K - EM":"0.023",
        "MATH - Equivalent":"0",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.159",
        "LegalSupport - EM":"0.558",
        "Data imputation - EM":"0.624",
        "Entity matching - EM":"0.652",
        "BBQ - EM":"0.312"
    },
    {
        "Model":"UL2 (20B)",
        "Mean win rate":"0.277",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.204",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.193",
        "MMLU - EM":"0.291",
        "WikiFact - EM":"0.168",
        "Synthetic reasoning (abstract symbols) - EM":"0.205",
        "Synthetic reasoning (natural language) - F1":"0.187",
        "bAbI - EM":"0.501",
        "Dyck - EM":"0.14",
        "GSM8K - EM":"0.024",
        "MATH - Equivalent":"0",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.207",
        "LegalSupport - EM":"0.506",
        "Data imputation - EM":"0.611",
        "Entity matching - EM":"0.672",
        "BBQ - EM":"0.321"
    },
    {
        "Model":"OPT (175B)",
        "Mean win rate":"0.556",
        "The Pile - BPB":"0.592",
        "TwitterAAE - BPB":"1.81",
        "ICE - BPB":"0.741",
        "BLiMP - EM":"0.831",
        "NaturalQuestions (closed-book) - F1":"0.297",
        "HellaSwag - EM":"0.791",
        "OpenbookQA - EM":"0.586",
        "TruthfulQA - EM":"0.25",
        "MMLU - EM":"0.318",
        "WikiFact - EM":"0.22",
        "Synthetic reasoning (abstract symbols) - EM":"0.225",
        "Synthetic reasoning (natural language) - F1":"0.248",
        "bAbI - EM":"0.507",
        "Dyck - EM":"0.494",
        "GSM8K - EM":"0.04",
        "MATH - Equivalent":"0.065",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.026",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.22",
        "LegalSupport - EM":"0.532",
        "Data imputation - EM":"0.722",
        "Entity matching - EM":"0.374",
        "BBQ - EM":"0.347"
    },
    {
        "Model":"OPT (66B)",
        "Mean win rate":"0.403",
        "The Pile - BPB":"0.618",
        "TwitterAAE - BPB":"1.85",
        "ICE - BPB":"0.764",
        "BLiMP - EM":"0.827",
        "NaturalQuestions (closed-book) - F1":"0.258",
        "HellaSwag - EM":"0.745",
        "OpenbookQA - EM":"0.534",
        "TruthfulQA - EM":"0.201",
        "MMLU - EM":"0.276",
        "WikiFact - EM":"0.202",
        "Synthetic reasoning (abstract symbols) - EM":"0.193",
        "Synthetic reasoning (natural language) - F1":"0.213",
        "bAbI - EM":"0.408",
        "Dyck - EM":"0.471",
        "GSM8K - EM":"0.018",
        "MATH - Equivalent":"0.048",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.029",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.175",
        "LegalSupport - EM":"0.527",
        "Data imputation - EM":"0.718",
        "Entity matching - EM":"0.466",
        "BBQ - EM":"0.355"
    },
    {
        "Model":"LLaMA (7B)",
        "Mean win rate":"0.507",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.297",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.28",
        "MMLU - EM":"0.321",
        "WikiFact - EM":"0.184",
        "Synthetic reasoning (abstract symbols) - EM":"0.143",
        "Synthetic reasoning (natural language) - F1":"0",
        "bAbI - EM":"0.531",
        "Dyck - EM":"0.56",
        "GSM8K - EM":"0.08",
        "MATH - Equivalent":"0.112",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.061",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.209",
        "LegalSupport - EM":"0.485",
        "Data imputation - EM":"0.834",
        "Entity matching - EM":"0.836",
        "BBQ - EM":"-"
    },
    {
        "Model":"LLaMA (13B)",
        "Mean win rate":"0.636",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.346",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.324",
        "MMLU - EM":"0.422",
        "WikiFact - EM":"0.233",
        "Synthetic reasoning (abstract symbols) - EM":"0.214",
        "Synthetic reasoning (natural language) - F1":"0",
        "bAbI - EM":"0.561",
        "Dyck - EM":"0.686",
        "GSM8K - EM":"0.154",
        "MATH - Equivalent":"0.134",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.11",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.187",
        "LegalSupport - EM":"0.587",
        "Data imputation - EM":"0.824",
        "Entity matching - EM":"0.859",
        "BBQ - EM":"-"
    },
    {
        "Model":"LLaMA (30B)",
        "Mean win rate":"0.804",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.408",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.344",
        "MMLU - EM":"0.531",
        "WikiFact - EM":"0.275",
        "Synthetic reasoning (abstract symbols) - EM":"0.412",
        "Synthetic reasoning (natural language) - F1":"0",
        "bAbI - EM":"0.634",
        "Dyck - EM":"0.706",
        "GSM8K - EM":"0.32",
        "MATH - Equivalent":"0.197",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.223",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.213",
        "LegalSupport - EM":"0.638",
        "Data imputation - EM":"0.844",
        "Entity matching - EM":"0.908",
        "BBQ - EM":"-"
    },
    {
        "Model":"LLaMA (65B)",
        "Mean win rate":"0.867",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.431",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.508",
        "MMLU - EM":"0.584",
        "WikiFact - EM":"0.421",
        "Synthetic reasoning (abstract symbols) - EM":"0.438",
        "Synthetic reasoning (natural language) - F1":"0.432",
        "bAbI - EM":"0.677",
        "Dyck - EM":"0.65",
        "GSM8K - EM":"0.466",
        "MATH - Equivalent":"0.224",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.253",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.213",
        "LegalSupport - EM":"0.591",
        "Data imputation - EM":"0.822",
        "Entity matching - EM":"0.861",
        "BBQ - EM":"-"
    },
    {
        "Model":"Llama 2 (7B)",
        "Mean win rate":"0.641",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.337",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.272",
        "MMLU - EM":"0.431",
        "WikiFact - EM":"0.335",
        "Synthetic reasoning (abstract symbols) - EM":"0.222",
        "Synthetic reasoning (natural language) - F1":"0.26",
        "bAbI - EM":"0.55",
        "Dyck - EM":"0.658",
        "GSM8K - EM":"0.133",
        "MATH - Equivalent":"0.107",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.099",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.148",
        "LegalSupport - EM":"0.532",
        "Data imputation - EM":"0.801",
        "Entity matching - EM":"0.851",
        "BBQ - EM":"-"
    },
    {
        "Model":"Llama 2 (13B)",
        "Mean win rate":"0.78",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.376",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.33",
        "MMLU - EM":"0.507",
        "WikiFact - EM":"0.365",
        "Synthetic reasoning (abstract symbols) - EM":"0.324",
        "Synthetic reasoning (natural language) - F1":"0.271",
        "bAbI - EM":"0.584",
        "Dyck - EM":"0.624",
        "GSM8K - EM":"0.245",
        "MATH - Equivalent":"0.145",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.122",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.235",
        "LegalSupport - EM":"0.591",
        "Data imputation - EM":"0.844",
        "Entity matching - EM":"0.528",
        "BBQ - EM":"-"
    },
    {
        "Model":"Llama 2 (70B)",
        "Mean win rate":"0.896",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.458",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.554",
        "MMLU - EM":"0.582",
        "WikiFact - EM":"0.449",
        "Synthetic reasoning (abstract symbols) - EM":"0.489",
        "Synthetic reasoning (natural language) - F1":"0.472",
        "bAbI - EM":"0.705",
        "Dyck - EM":"0.742",
        "GSM8K - EM":"0.484",
        "MATH - Equivalent":"0.261",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.33",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.235",
        "LegalSupport - EM":"0.585",
        "Data imputation - EM":"0.838",
        "Entity matching - EM":"0.732",
        "BBQ - EM":"-"
    },
    {
        "Model":"Alpaca (7B)",
        "Mean win rate":"0.471",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.266",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.243",
        "MMLU - EM":"0.385",
        "WikiFact - EM":"0.224",
        "Synthetic reasoning (abstract symbols) - EM":"0.155",
        "Synthetic reasoning (natural language) - F1":"0.215",
        "bAbI - EM":"0.503",
        "Dyck - EM":"0.524",
        "GSM8K - EM":"0.012",
        "MATH - Equivalent":"0.104",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.028",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.222",
        "LegalSupport - EM":"0.483",
        "Data imputation - EM":"0.735",
        "Entity matching - EM":"0.853",
        "BBQ - EM":"-"
    },
    {
        "Model":"Vicuna v1.3 (7B)",
        "Mean win rate":"0.558",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.287",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.292",
        "MMLU - EM":"0.434",
        "WikiFact - EM":"0.22",
        "Synthetic reasoning (abstract symbols) - EM":"0.179",
        "Synthetic reasoning (natural language) - F1":"0.236",
        "bAbI - EM":"0.51",
        "Dyck - EM":"0.622",
        "GSM8K - EM":"0.134",
        "MATH - Equivalent":"0.088",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.076",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.183",
        "LegalSupport - EM":"0.605",
        "Data imputation - EM":"0.708",
        "Entity matching - EM":"0.848",
        "BBQ - EM":"-"
    },
    {
        "Model":"Vicuna v1.3 (13B)",
        "Mean win rate":"0.706",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.346",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.385",
        "MMLU - EM":"0.462",
        "WikiFact - EM":"0.268",
        "Synthetic reasoning (abstract symbols) - EM":"0.234",
        "Synthetic reasoning (natural language) - F1":"0.296",
        "bAbI - EM":"0.533",
        "Dyck - EM":"0.686",
        "GSM8K - EM":"0.226",
        "MATH - Equivalent":"0.12",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.122",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.209",
        "LegalSupport - EM":"0.589",
        "Data imputation - EM":"0.714",
        "Entity matching - EM":"0.879",
        "BBQ - EM":"-"
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "Mean win rate":"0.845",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.365",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.422",
        "MMLU - EM":"0.572",
        "WikiFact - EM":"0.349",
        "Synthetic reasoning (abstract symbols) - EM":"0.451",
        "Synthetic reasoning (natural language) - F1":"0.392",
        "bAbI - EM":"0.593",
        "Dyck - EM":"0.696",
        "GSM8K - EM":"0.381",
        "MATH - Equivalent":"0.209",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.293",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.213",
        "LegalSupport - EM":"0.585",
        "Data imputation - EM":"0.795",
        "Entity matching - EM":"0.938",
        "BBQ - EM":"-"
    },
    {
        "Model":"TNLG v2 (530B)",
        "Mean win rate":"0.743",
        "The Pile - BPB":"0.61",
        "TwitterAAE - BPB":"2.245",
        "ICE - BPB":"0.85",
        "BLiMP - EM":"0.826",
        "NaturalQuestions (closed-book) - F1":"0.384",
        "HellaSwag - EM":"0.799",
        "OpenbookQA - EM":"0.562",
        "TruthfulQA - EM":"0.251",
        "MMLU - EM":"0.469",
        "WikiFact - EM":"0.337",
        "Synthetic reasoning (abstract symbols) - EM":"0.362",
        "Synthetic reasoning (natural language) - F1":"0.243",
        "bAbI - EM":"0.481",
        "Dyck - EM":"0.753",
        "GSM8K - EM":"0.146",
        "MATH - Equivalent":"0.155",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.114",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.228",
        "LegalSupport - EM":"0.58",
        "Data imputation - EM":"0.811",
        "Entity matching - EM":"0.881",
        "BBQ - EM":"0.479"
    },
    {
        "Model":"TNLG v2 (6.7B)",
        "Mean win rate":"0.376",
        "The Pile - BPB":"0.704",
        "TwitterAAE - BPB":"2.323",
        "ICE - BPB":"0.918",
        "BLiMP - EM":"0.835",
        "NaturalQuestions (closed-book) - F1":"0.21",
        "HellaSwag - EM":"0.704",
        "OpenbookQA - EM":"0.478",
        "TruthfulQA - EM":"0.167",
        "MMLU - EM":"0.242",
        "WikiFact - EM":"0.236",
        "Synthetic reasoning (abstract symbols) - EM":"0.232",
        "Synthetic reasoning (natural language) - F1":"0.247",
        "bAbI - EM":"0.411",
        "Dyck - EM":"0.554",
        "GSM8K - EM":"0.018",
        "MATH - Equivalent":"0.068",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.023",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.19",
        "LegalSupport - EM":"0.504",
        "Data imputation - EM":"0.826",
        "Entity matching - EM":"0.694",
        "BBQ - EM":"0.337"
    },
    {
        "Model":"davinci (175B)",
        "Mean win rate":"0.574",
        "The Pile - BPB":"0.713",
        "TwitterAAE - BPB":"2.141",
        "ICE - BPB":"0.977",
        "BLiMP - EM":"0.84",
        "NaturalQuestions (closed-book) - F1":"0.329",
        "HellaSwag - EM":"0.775",
        "OpenbookQA - EM":"0.586",
        "TruthfulQA - EM":"0.194",
        "MMLU - EM":"0.422",
        "WikiFact - EM":"0.306",
        "Synthetic reasoning (abstract symbols) - EM":"0.236",
        "Synthetic reasoning (natural language) - F1":"0.165",
        "bAbI - EM":"0.462",
        "Dyck - EM":"0.668",
        "GSM8K - EM":"0.09",
        "MATH - Equivalent":"0.099",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.043",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.191",
        "LegalSupport - EM":"0.496",
        "Data imputation - EM":"0.836",
        "Entity matching - EM":"0.603",
        "BBQ - EM":"0.39"
    },
    {
        "Model":"curie (6.7B)",
        "Mean win rate":"0.343",
        "The Pile - BPB":"0.789",
        "TwitterAAE - BPB":"2.21",
        "ICE - BPB":"0.958",
        "BLiMP - EM":"0.832",
        "NaturalQuestions (closed-book) - F1":"0.199",
        "HellaSwag - EM":"0.682",
        "OpenbookQA - EM":"0.502",
        "TruthfulQA - EM":"0.232",
        "MMLU - EM":"0.243",
        "WikiFact - EM":"0.236",
        "Synthetic reasoning (abstract symbols) - EM":"0.223",
        "Synthetic reasoning (natural language) - F1":"0.149",
        "bAbI - EM":"0.415",
        "Dyck - EM":"0.47",
        "GSM8K - EM":"0.016",
        "MATH - Equivalent":"0.05",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.023",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.18",
        "LegalSupport - EM":"0.49",
        "Data imputation - EM":"0.811",
        "Entity matching - EM":"0.453",
        "BBQ - EM":"0.362"
    },
    {
        "Model":"babbage (1.3B)",
        "Mean win rate":"0.249",
        "The Pile - BPB":"0.866",
        "TwitterAAE - BPB":"2.291",
        "ICE - BPB":"1.048",
        "BLiMP - EM":"0.833",
        "NaturalQuestions (closed-book) - F1":"0.119",
        "HellaSwag - EM":"0.555",
        "OpenbookQA - EM":"0.438",
        "TruthfulQA - EM":"0.188",
        "MMLU - EM":"0.235",
        "WikiFact - EM":"0.184",
        "Synthetic reasoning (abstract symbols) - EM":"0.16",
        "Synthetic reasoning (natural language) - F1":"0.12",
        "bAbI - EM":"0.348",
        "Dyck - EM":"0.473",
        "GSM8K - EM":"0.007",
        "MATH - Equivalent":"0.048",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.009",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.261",
        "LegalSupport - EM":"0.492",
        "Data imputation - EM":"0.599",
        "Entity matching - EM":"0.632",
        "BBQ - EM":"0.319"
    },
    {
        "Model":"ada (350M)",
        "Mean win rate":"0.144",
        "The Pile - BPB":"0.96",
        "TwitterAAE - BPB":"2.395",
        "ICE - BPB":"1.136",
        "BLiMP - EM":"0.818",
        "NaturalQuestions (closed-book) - F1":"0.082",
        "HellaSwag - EM":"0.435",
        "OpenbookQA - EM":"0.38",
        "TruthfulQA - EM":"0.215",
        "MMLU - EM":"0.243",
        "WikiFact - EM":"0.124",
        "Synthetic reasoning (abstract symbols) - EM":"0.1",
        "Synthetic reasoning (natural language) - F1":"0.088",
        "bAbI - EM":"0.306",
        "Dyck - EM":"0.406",
        "GSM8K - EM":"0.006",
        "MATH - Equivalent":"0.046",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.006",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.188",
        "LegalSupport - EM":"0.372",
        "Data imputation - EM":"0.582",
        "Entity matching - EM":"0.349",
        "BBQ - EM":"0.322"
    },
    {
        "Model":"text-davinci-003",
        "Mean win rate":"0.901",
        "The Pile - BPB":"0.576",
        "TwitterAAE - BPB":"2.192",
        "ICE - BPB":"0.838",
        "BLiMP - EM":"0.823",
        "NaturalQuestions (closed-book) - F1":"0.406",
        "HellaSwag - EM":"0.822",
        "OpenbookQA - EM":"0.646",
        "TruthfulQA - EM":"0.593",
        "MMLU - EM":"0.569",
        "WikiFact - EM":"0.373",
        "Synthetic reasoning (abstract symbols) - EM":"0.502",
        "Synthetic reasoning (natural language) - F1":"0.734",
        "bAbI - EM":"0.653",
        "Dyck - EM":"0.751",
        "GSM8K - EM":"0.506",
        "MATH - Equivalent":"0.39",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.449",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.233",
        "LegalSupport - EM":"0.622",
        "Data imputation - EM":"0.839",
        "Entity matching - EM":"0.93",
        "BBQ - EM":"0.862"
    },
    {
        "Model":"text-davinci-002",
        "Mean win rate":"0.876",
        "The Pile - BPB":"0.578",
        "TwitterAAE - BPB":"2.147",
        "ICE - BPB":"0.823",
        "BLiMP - EM":"0.812",
        "NaturalQuestions (closed-book) - F1":"0.383",
        "HellaSwag - EM":"0.815",
        "OpenbookQA - EM":"0.594",
        "TruthfulQA - EM":"0.61",
        "MMLU - EM":"0.568",
        "WikiFact - EM":"0.392",
        "Synthetic reasoning (abstract symbols) - EM":"0.488",
        "Synthetic reasoning (natural language) - F1":"0.623",
        "bAbI - EM":"0.618",
        "Dyck - EM":"0.603",
        "GSM8K - EM":"0.415",
        "MATH - Equivalent":"0.328",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.381",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.229",
        "LegalSupport - EM":"0.615",
        "Data imputation - EM":"0.842",
        "Entity matching - EM":"0.931",
        "BBQ - EM":"0.881"
    },
    {
        "Model":"text-curie-001",
        "Mean win rate":"0.248",
        "The Pile - BPB":"0.969",
        "TwitterAAE - BPB":"2.628",
        "ICE - BPB":"1.192",
        "BLiMP - EM":"0.761",
        "NaturalQuestions (closed-book) - F1":"0.175",
        "HellaSwag - EM":"0.676",
        "OpenbookQA - EM":"0.514",
        "TruthfulQA - EM":"0.257",
        "MMLU - EM":"0.237",
        "WikiFact - EM":"0.214",
        "Synthetic reasoning (abstract symbols) - EM":"0.19",
        "Synthetic reasoning (natural language) - F1":"0.221",
        "bAbI - EM":"0.384",
        "Dyck - EM":"0.41",
        "GSM8K - EM":"0.006",
        "MATH - Equivalent":"0.045",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.015",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.172",
        "LegalSupport - EM":"0.442",
        "Data imputation - EM":"0.791",
        "Entity matching - EM":"0.852",
        "BBQ - EM":"0.342"
    },
    {
        "Model":"text-babbage-001",
        "Mean win rate":"0.187",
        "The Pile - BPB":"1.103",
        "TwitterAAE - BPB":"2.946",
        "ICE - BPB":"1.326",
        "BLiMP - EM":"0.77",
        "NaturalQuestions (closed-book) - F1":"0.07",
        "HellaSwag - EM":"0.561",
        "OpenbookQA - EM":"0.452",
        "TruthfulQA - EM":"0.233",
        "MMLU - EM":"0.229",
        "WikiFact - EM":"0.15",
        "Synthetic reasoning (abstract symbols) - EM":"0.123",
        "Synthetic reasoning (natural language) - F1":"0.212",
        "bAbI - EM":"0.289",
        "Dyck - EM":"0.231",
        "GSM8K - EM":"0",
        "MATH - Equivalent":"0.016",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.007",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.19",
        "LegalSupport - EM":"0.517",
        "Data imputation - EM":"0.704",
        "Entity matching - EM":"0.734",
        "BBQ - EM":"0.359"
    },
    {
        "Model":"text-ada-001",
        "Mean win rate":"0.16",
        "The Pile - BPB":"1.61",
        "TwitterAAE - BPB":"3.505",
        "ICE - BPB":"1.975",
        "BLiMP - EM":"0.728",
        "NaturalQuestions (closed-book) - F1":"0.025",
        "HellaSwag - EM":"0.429",
        "OpenbookQA - EM":"0.346",
        "TruthfulQA - EM":"0.232",
        "MMLU - EM":"0.238",
        "WikiFact - EM":"0.119",
        "Synthetic reasoning (abstract symbols) - EM":"0.063",
        "Synthetic reasoning (natural language) - F1":"0.145",
        "bAbI - EM":"0.226",
        "Dyck - EM":"0.161",
        "GSM8K - EM":"0.004",
        "MATH - Equivalent":"0.02",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.001",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.213",
        "LegalSupport - EM":"0.515",
        "Data imputation - EM":"0.597",
        "Entity matching - EM":"0.808",
        "BBQ - EM":"0.359"
    },
    {
        "Model":"code-davinci-002",
        "Mean win rate":"0.785",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"-",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"-",
        "MMLU - EM":"-",
        "WikiFact - EM":"-",
        "Synthetic reasoning (abstract symbols) - EM":"0.54",
        "Synthetic reasoning (natural language) - F1":"0.684",
        "bAbI - EM":"0.686",
        "Dyck - EM":"0.805",
        "GSM8K - EM":"0.568",
        "MATH - Equivalent":"0.41",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.433",
        "HumanEval (Code) - pass@1":"0.463",
        "LSAT - EM":"0",
        "LegalSupport - EM":"0",
        "Data imputation - EM":"-",
        "Entity matching - EM":"-",
        "BBQ - EM":"-"
    },
    {
        "Model":"code-cushman-001 (12B)",
        "Mean win rate":"0.37",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"-",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"-",
        "MMLU - EM":"-",
        "WikiFact - EM":"-",
        "Synthetic reasoning (abstract symbols) - EM":"0.341",
        "Synthetic reasoning (natural language) - F1":"0.164",
        "bAbI - EM":"0.481",
        "Dyck - EM":"0.451",
        "GSM8K - EM":"0.049",
        "MATH - Equivalent":"0.099",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.072",
        "HumanEval (Code) - pass@1":"0.317",
        "LSAT - EM":"0",
        "LegalSupport - EM":"0",
        "Data imputation - EM":"-",
        "Entity matching - EM":"-",
        "BBQ - EM":"-"
    },
    {
        "Model":"gpt-3.5-turbo-0301",
        "Mean win rate":"0.835",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.39",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.609",
        "MMLU - EM":"0.59",
        "WikiFact - EM":"0.279",
        "Synthetic reasoning (abstract symbols) - EM":"0.45",
        "Synthetic reasoning (natural language) - F1":"0.631",
        "bAbI - EM":"0.515",
        "Dyck - EM":"0.172",
        "GSM8K - EM":"0.531",
        "MATH - Equivalent":"0.488",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.689",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.252",
        "LegalSupport - EM":"0.628",
        "Data imputation - EM":"0.778",
        "Entity matching - EM":"0.943",
        "BBQ - EM":"-"
    },
    {
        "Model":"gpt-3.5-turbo-0613",
        "Mean win rate":"0.686",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.348",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.339",
        "MMLU - EM":"0.391",
        "WikiFact - EM":"0.289",
        "Synthetic reasoning (abstract symbols) - EM":"0.509",
        "Synthetic reasoning (natural language) - F1":"0.586",
        "bAbI - EM":"0.528",
        "Dyck - EM":"0.01",
        "GSM8K - EM":"0.469",
        "MATH - Equivalent":"0.453",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.719",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.161",
        "LegalSupport - EM":"0.468",
        "Data imputation - EM":"0.836",
        "Entity matching - EM":"0.918",
        "BBQ - EM":"-"
    },
    {
        "Model":"RedPajama-INCITE-Base-v1 (3B)",
        "Mean win rate":"0.363",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.207",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.277",
        "MMLU - EM":"0.263",
        "WikiFact - EM":"0.177",
        "Synthetic reasoning (abstract symbols) - EM":"0.154",
        "Synthetic reasoning (natural language) - F1":"0",
        "bAbI - EM":"0.466",
        "Dyck - EM":"0.53",
        "GSM8K - EM":"0.01",
        "MATH - Equivalent":"0.059",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.032",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.213",
        "LegalSupport - EM":"0.513",
        "Data imputation - EM":"0.828",
        "Entity matching - EM":"0.533",
        "BBQ - EM":"-"
    },
    {
        "Model":"RedPajama-INCITE-Instruct-v1 (3B)",
        "Mean win rate":"0.322",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.203",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.208",
        "MMLU - EM":"0.257",
        "WikiFact - EM":"0.174",
        "Synthetic reasoning (abstract symbols) - EM":"0.142",
        "Synthetic reasoning (natural language) - F1":"0",
        "bAbI - EM":"0.522",
        "Dyck - EM":"0.708",
        "GSM8K - EM":"0.011",
        "MATH - Equivalent":"0.06",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.004",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.17",
        "LegalSupport - EM":"0.485",
        "Data imputation - EM":"0.752",
        "Entity matching - EM":"0.828",
        "BBQ - EM":"-"
    },
    {
        "Model":"RedPajama-INCITE-Base (7B)",
        "Mean win rate":"0.331",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.25",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.205",
        "MMLU - EM":"0.302",
        "WikiFact - EM":"0.207",
        "Synthetic reasoning (abstract symbols) - EM":"0.137",
        "Synthetic reasoning (natural language) - F1":"0",
        "bAbI - EM":"0.46",
        "Dyck - EM":"0.528",
        "GSM8K - EM":"0.021",
        "MATH - Equivalent":"0.1",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.052",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.178",
        "LegalSupport - EM":"0.517",
        "Data imputation - EM":"0.706",
        "Entity matching - EM":"0.554",
        "BBQ - EM":"-"
    },
    {
        "Model":"RedPajama-INCITE-Instruct (7B)",
        "Mean win rate":"0.487",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.232",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.243",
        "MMLU - EM":"0.363",
        "WikiFact - EM":"0.211",
        "Synthetic reasoning (abstract symbols) - EM":"0.184",
        "Synthetic reasoning (natural language) - F1":"0",
        "bAbI - EM":"0.545",
        "Dyck - EM":"0.546",
        "GSM8K - EM":"0.016",
        "MATH - Equivalent":"0.058",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.028",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.243",
        "LegalSupport - EM":"0.569",
        "Data imputation - EM":"0.772",
        "Entity matching - EM":"0.843",
        "BBQ - EM":"-"
    },
    {
        "Model":"MPT (30B)",
        "Mean win rate":"0.781",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.347",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.231",
        "MMLU - EM":"0.437",
        "WikiFact - EM":"0.369",
        "Synthetic reasoning (abstract symbols) - EM":"0.335",
        "Synthetic reasoning (natural language) - F1":"0.228",
        "bAbI - EM":"0.55",
        "Dyck - EM":"0.634",
        "GSM8K - EM":"0.164",
        "MATH - Equivalent":"0.178",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.124",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.248",
        "LegalSupport - EM":"0.564",
        "Data imputation - EM":"0.855",
        "Entity matching - EM":"0.897",
        "BBQ - EM":"-"
    },
    {
        "Model":"MPT-Instruct (30B)",
        "Mean win rate":"0.726",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.304",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.234",
        "MMLU - EM":"0.444",
        "WikiFact - EM":"0.328",
        "Synthetic reasoning (abstract symbols) - EM":"0.35",
        "Synthetic reasoning (natural language) - F1":"0.33",
        "bAbI - EM":"0.517",
        "Dyck - EM":"0.624",
        "GSM8K - EM":"0.344",
        "MATH - Equivalent":"0.127",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.125",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.213",
        "LegalSupport - EM":"0.538",
        "Data imputation - EM":"0.826",
        "Entity matching - EM":"0.895",
        "BBQ - EM":"-"
    },
    {
        "Model":"Falcon (7B)",
        "Mean win rate":"0.397",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.285",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.234",
        "MMLU - EM":"0.286",
        "WikiFact - EM":"0.309",
        "Synthetic reasoning (abstract symbols) - EM":"0.1",
        "Synthetic reasoning (natural language) - F1":"0.144",
        "bAbI - EM":"0.415",
        "Dyck - EM":"0.458",
        "GSM8K - EM":"0.04",
        "MATH - Equivalent":"0.108",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.042",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.204",
        "LegalSupport - EM":"0.511",
        "Data imputation - EM":"0.725",
        "Entity matching - EM":"0.602",
        "BBQ - EM":"-"
    },
    {
        "Model":"Falcon-Instruct (7B)",
        "Mean win rate":"0.336",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.194",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.213",
        "MMLU - EM":"0.275",
        "WikiFact - EM":"0.232",
        "Synthetic reasoning (abstract symbols) - EM":"0.037",
        "Synthetic reasoning (natural language) - F1":"0.1",
        "bAbI - EM":"0.424",
        "Dyck - EM":"0.192",
        "GSM8K - EM":"0.052",
        "MATH - Equivalent":"0.069",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.03",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.204",
        "LegalSupport - EM":"0.452",
        "Data imputation - EM":"0.836",
        "Entity matching - EM":"0.824",
        "BBQ - EM":"-"
    },
    {
        "Model":"Falcon (40B)",
        "Mean win rate":"0.768",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.392",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.353",
        "MMLU - EM":"0.509",
        "WikiFact - EM":"0.38",
        "Synthetic reasoning (abstract symbols) - EM":"0.238",
        "Synthetic reasoning (natural language) - F1":"0.139",
        "bAbI - EM":"0.576",
        "Dyck - EM":"0.534",
        "GSM8K - EM":"0.25",
        "MATH - Equivalent":"0.21",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.13",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.213",
        "LegalSupport - EM":"0.605",
        "Data imputation - EM":"0.83",
        "Entity matching - EM":"0.847",
        "BBQ - EM":"-"
    },
    {
        "Model":"Falcon-Instruct (40B)",
        "Mean win rate":"0.691",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.377",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.384",
        "MMLU - EM":"0.497",
        "WikiFact - EM":"0.359",
        "Synthetic reasoning (abstract symbols) - EM":"0.216",
        "Synthetic reasoning (natural language) - F1":"0.183",
        "bAbI - EM":"0.606",
        "Dyck - EM":"0.364",
        "GSM8K - EM":"0.338",
        "MATH - Equivalent":"0.181",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.137",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.183",
        "LegalSupport - EM":"0.605",
        "Data imputation - EM":"0.727",
        "Entity matching - EM":"0.872",
        "BBQ - EM":"-"
    },
    {
        "Model":"GLM (130B)",
        "Mean win rate":"0.355",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.148",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.218",
        "MMLU - EM":"0.344",
        "WikiFact - EM":"0.237",
        "Synthetic reasoning (abstract symbols) - EM":"0.252",
        "Synthetic reasoning (natural language) - F1":"0.254",
        "bAbI - EM":"0.443",
        "Dyck - EM":"0.549",
        "GSM8K - EM":"0.061",
        "MATH - Equivalent":"0",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.059",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.193",
        "LegalSupport - EM":"0.451",
        "Data imputation - EM":"0.66",
        "Entity matching - EM":"0.472",
        "BBQ - EM":"0.316"
    },
    {
        "Model":"InstructPalmyra (30B)",
        "Mean win rate":"0.466",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.33",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.185",
        "MMLU - EM":"0.403",
        "WikiFact - EM":"0.209",
        "Synthetic reasoning (abstract symbols) - EM":"0.231",
        "Synthetic reasoning (natural language) - F1":"0.247",
        "bAbI - EM":"0.468",
        "Dyck - EM":"0.447",
        "GSM8K - EM":"0.063",
        "MATH - Equivalent":"0.099",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.039",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.187",
        "LegalSupport - EM":"0.492",
        "Data imputation - EM":"0.833",
        "Entity matching - EM":"0.77",
        "BBQ - EM":"0.37"
    },
    {
        "Model":"Palmyra X (43B)",
        "Mean win rate":"0.909",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.413",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.616",
        "MMLU - EM":"0.609",
        "WikiFact - EM":"0.338",
        "Synthetic reasoning (abstract symbols) - EM":"0.504",
        "Synthetic reasoning (natural language) - F1":"0.576",
        "bAbI - EM":"0.702",
        "Dyck - EM":"0.579",
        "GSM8K - EM":"0.633",
        "MATH - Equivalent":"0.301",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0.38",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.225",
        "LegalSupport - EM":"0.623",
        "Data imputation - EM":"0.824",
        "Entity matching - EM":"0.966",
        "BBQ - EM":"0.781"
    },
    {
        "Model":"YaLM (100B)",
        "Mean win rate":"0.182",
        "The Pile - BPB":"-",
        "TwitterAAE - BPB":"-",
        "ICE - BPB":"-",
        "BLiMP - EM":"-",
        "NaturalQuestions (closed-book) - F1":"0.068",
        "HellaSwag - EM":"-",
        "OpenbookQA - EM":"-",
        "TruthfulQA - EM":"0.202",
        "MMLU - EM":"0.243",
        "WikiFact - EM":"0.049",
        "Synthetic reasoning (abstract symbols) - EM":"0.056",
        "Synthetic reasoning (natural language) - F1":"0.061",
        "bAbI - EM":"0.346",
        "Dyck - EM":"0.633",
        "GSM8K - EM":"0",
        "MATH - Equivalent":"0",
        "MATH (chain-of-thoughts) - Equivalent (chain of thought)":"0",
        "HumanEval (Code) - pass@1":"-",
        "LSAT - EM":"0.23",
        "LegalSupport - EM":"0.484",
        "Data imputation - EM":"0.419",
        "Entity matching - EM":"0.176",
        "BBQ - EM":"0.328"
    }
]