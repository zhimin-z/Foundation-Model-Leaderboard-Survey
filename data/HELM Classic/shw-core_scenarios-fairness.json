[
    {
        "Model":"J1-Jumbo v1 (178B)",
        "Mean win rate":"0.488",
        "MMLU - EM (Fairness)":"0.236",
        "BoolQ - EM (Fairness)":"0.709",
        "NarrativeQA - F1 (Fairness)":"0.581",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.235",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.54",
        "QuAC - F1 (Fairness)":"0.268",
        "HellaSwag - EM (Fairness)":"0.614",
        "OpenbookQA - EM (Fairness)":"0.466",
        "TruthfulQA - EM (Fairness)":"0.156",
        "MS MARCO (regular) - RR@10 (Fairness)":"0.18",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"0.348",
        "IMDB - EM (Fairness)":"0.932",
        "CivilComments - EM (Fairness)":"0.478",
        "RAFT - EM (Fairness)":"0.623"
    },
    {
        "Model":"J1-Large v1 (7.5B)",
        "Mean win rate":"0.275",
        "MMLU - EM (Fairness)":"0.204",
        "BoolQ - EM (Fairness)":"0.622",
        "NarrativeQA - F1 (Fairness)":"0.513",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.146",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.47",
        "QuAC - F1 (Fairness)":"0.241",
        "HellaSwag - EM (Fairness)":"0.528",
        "OpenbookQA - EM (Fairness)":"0.444",
        "TruthfulQA - EM (Fairness)":"0.174",
        "MS MARCO (regular) - RR@10 (Fairness)":"0.117",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"0.28",
        "IMDB - EM (Fairness)":"0.946",
        "CivilComments - EM (Fairness)":"0.447",
        "RAFT - EM (Fairness)":"0.511"
    },
    {
        "Model":"J1-Grande v1 (17B)",
        "Mean win rate":"0.454",
        "MMLU - EM (Fairness)":"0.232",
        "BoolQ - EM (Fairness)":"0.678",
        "NarrativeQA - F1 (Fairness)":"0.547",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.187",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.521",
        "QuAC - F1 (Fairness)":"0.274",
        "HellaSwag - EM (Fairness)":"0.58",
        "OpenbookQA - EM (Fairness)":"0.472",
        "TruthfulQA - EM (Fairness)":"0.163",
        "MS MARCO (regular) - RR@10 (Fairness)":"0.138",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"0.328",
        "IMDB - EM (Fairness)":"0.946",
        "CivilComments - EM (Fairness)":"0.482",
        "RAFT - EM (Fairness)":"0.636"
    },
    {
        "Model":"J1-Grande v2 beta (17B)",
        "Mean win rate":"0.677",
        "MMLU - EM (Fairness)":"0.409",
        "BoolQ - EM (Fairness)":"0.764",
        "NarrativeQA - F1 (Fairness)":"0.647",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.27",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.571",
        "QuAC - F1 (Fairness)":"0.308",
        "HellaSwag - EM (Fairness)":"0.623",
        "OpenbookQA - EM (Fairness)":"0.478",
        "TruthfulQA - EM (Fairness)":"0.242",
        "MS MARCO (regular) - RR@10 (Fairness)":"0.253",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"0.435",
        "IMDB - EM (Fairness)":"0.95",
        "CivilComments - EM (Fairness)":"0.404",
        "RAFT - EM (Fairness)":"0.637"
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "Mean win rate":"0.836",
        "MMLU - EM (Fairness)":"0.45",
        "BoolQ - EM (Fairness)":"0.792",
        "NarrativeQA - F1 (Fairness)":"0.658",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.327",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.62",
        "QuAC - F1 (Fairness)":"0.34",
        "HellaSwag - EM (Fairness)":"0.655",
        "OpenbookQA - EM (Fairness)":"0.488",
        "TruthfulQA - EM (Fairness)":"0.354",
        "MS MARCO (regular) - RR@10 (Fairness)":"0.342",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"0.62",
        "IMDB - EM (Fairness)":"0.933",
        "CivilComments - EM (Fairness)":"0.507",
        "RAFT - EM (Fairness)":"0.711"
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "Mean win rate":"0.704",
        "MMLU - EM (Fairness)":"0.433",
        "BoolQ - EM (Fairness)":"0.78",
        "NarrativeQA - F1 (Fairness)":"0.645",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.283",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.584",
        "QuAC - F1 (Fairness)":"0.34",
        "HellaSwag - EM (Fairness)":"0.632",
        "OpenbookQA - EM (Fairness)":"0.466",
        "TruthfulQA - EM (Fairness)":"0.29",
        "MS MARCO (regular) - RR@10 (Fairness)":"0.243",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"0.471",
        "IMDB - EM (Fairness)":"0.931",
        "CivilComments - EM (Fairness)":"0.445",
        "RAFT - EM (Fairness)":"0.689"
    },
    {
        "Model":"Jurassic-2 Large (7.5B)",
        "Mean win rate":"0.483",
        "MMLU - EM (Fairness)":"0.297",
        "BoolQ - EM (Fairness)":"0.685",
        "NarrativeQA - F1 (Fairness)":"-",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.217",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.539",
        "QuAC - F1 (Fairness)":"-",
        "HellaSwag - EM (Fairness)":"0.567",
        "OpenbookQA - EM (Fairness)":"0.45",
        "TruthfulQA - EM (Fairness)":"0.196",
        "MS MARCO (regular) - RR@10 (Fairness)":"0.215",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"0.44",
        "IMDB - EM (Fairness)":"0.945",
        "CivilComments - EM (Fairness)":"0.403",
        "RAFT - EM (Fairness)":"0.567"
    },
    {
        "Model":"Luminous Base (13B)",
        "Mean win rate":"0.238",
        "MMLU - EM (Fairness)":"0.185",
        "BoolQ - EM (Fairness)":"0.653",
        "NarrativeQA - F1 (Fairness)":"0.498",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.16",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.511",
        "QuAC - F1 (Fairness)":"0.266",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.125",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.912",
        "CivilComments - EM (Fairness)":"0.397",
        "RAFT - EM (Fairness)":"0.445"
    },
    {
        "Model":"Luminous Extended (30B)",
        "Mean win rate":"0.451",
        "MMLU - EM (Fairness)":"0.237",
        "BoolQ - EM (Fairness)":"0.711",
        "NarrativeQA - F1 (Fairness)":"0.532",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.214",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.551",
        "QuAC - F1 (Fairness)":"0.277",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.16",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.937",
        "CivilComments - EM (Fairness)":"0.462",
        "RAFT - EM (Fairness)":"0.489"
    },
    {
        "Model":"Luminous Supreme (70B)",
        "Mean win rate":"0.522",
        "MMLU - EM (Fairness)":"0.264",
        "BoolQ - EM (Fairness)":"0.694",
        "NarrativeQA - F1 (Fairness)":"0.603",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.241",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.597",
        "QuAC - F1 (Fairness)":"0.288",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.132",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.949",
        "CivilComments - EM (Fairness)":"0.432",
        "RAFT - EM (Fairness)":"0.601"
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B)",
        "Mean win rate":"0.794",
        "MMLU - EM (Fairness)":"0.447",
        "BoolQ - EM (Fairness)":"0.782",
        "NarrativeQA - F1 (Fairness)":"0.646",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.239",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.642",
        "QuAC - F1 (Fairness)":"0.356",
        "HellaSwag - EM (Fairness)":"0.695",
        "OpenbookQA - EM (Fairness)":"0.482",
        "TruthfulQA - EM (Fairness)":"0.3",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.925",
        "CivilComments - EM (Fairness)":"0.512",
        "RAFT - EM (Fairness)":"0.67"
    },
    {
        "Model":"BLOOM (176B)",
        "Mean win rate":"0.551",
        "MMLU - EM (Fairness)":"0.274",
        "BoolQ - EM (Fairness)":"0.656",
        "NarrativeQA - F1 (Fairness)":"0.577",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.187",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.575",
        "QuAC - F1 (Fairness)":"0.273",
        "HellaSwag - EM (Fairness)":"0.585",
        "OpenbookQA - EM (Fairness)":"0.482",
        "TruthfulQA - EM (Fairness)":"0.186",
        "MS MARCO (regular) - RR@10 (Fairness)":"0.211",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"0.371",
        "IMDB - EM (Fairness)":"0.938",
        "CivilComments - EM (Fairness)":"0.546",
        "RAFT - EM (Fairness)":"0.563"
    },
    {
        "Model":"T0pp (11B)",
        "Mean win rate":"0.203",
        "MMLU - EM (Fairness)":"0.382",
        "BoolQ - EM (Fairness)":"0",
        "NarrativeQA - F1 (Fairness)":"0.086",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.028",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.136",
        "QuAC - F1 (Fairness)":"0.067",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.35",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.168",
        "CivilComments - EM (Fairness)":"0.165",
        "RAFT - EM (Fairness)":"0.106"
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B)",
        "Mean win rate":"0.55",
        "MMLU - EM (Fairness)":"0.315",
        "BoolQ - EM (Fairness)":"0.667",
        "NarrativeQA - F1 (Fairness)":"0.548",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.255",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.535",
        "QuAC - F1 (Fairness)":"0.281",
        "HellaSwag - EM (Fairness)":"0.66",
        "OpenbookQA - EM (Fairness)":"0.47",
        "TruthfulQA - EM (Fairness)":"0.156",
        "MS MARCO (regular) - RR@10 (Fairness)":"0.233",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"0.431",
        "IMDB - EM (Fairness)":"0.949",
        "CivilComments - EM (Fairness)":"0.479",
        "RAFT - EM (Fairness)":"0.598"
    },
    {
        "Model":"Cohere large v20220720 (13.1B)",
        "Mean win rate":"0.362",
        "MMLU - EM (Fairness)":"0.281",
        "BoolQ - EM (Fairness)":"0.676",
        "NarrativeQA - F1 (Fairness)":"0.512",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.178",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.507",
        "QuAC - F1 (Fairness)":"0.256",
        "HellaSwag - EM (Fairness)":"0.575",
        "OpenbookQA - EM (Fairness)":"0.446",
        "TruthfulQA - EM (Fairness)":"0.157",
        "MS MARCO (regular) - RR@10 (Fairness)":"0.164",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"0.312",
        "IMDB - EM (Fairness)":"0.92",
        "CivilComments - EM (Fairness)":"0.443",
        "RAFT - EM (Fairness)":"0.564"
    },
    {
        "Model":"Cohere medium v20220720 (6.1B)",
        "Mean win rate":"0.269",
        "MMLU - EM (Fairness)":"0.237",
        "BoolQ - EM (Fairness)":"0.597",
        "NarrativeQA - F1 (Fairness)":"0.438",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.126",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.432",
        "QuAC - F1 (Fairness)":"0.198",
        "HellaSwag - EM (Fairness)":"0.525",
        "OpenbookQA - EM (Fairness)":"0.42",
        "TruthfulQA - EM (Fairness)":"0.174",
        "MS MARCO (regular) - RR@10 (Fairness)":"0.132",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"0.357",
        "IMDB - EM (Fairness)":"0.918",
        "CivilComments - EM (Fairness)":"0.489",
        "RAFT - EM (Fairness)":"0.5"
    },
    {
        "Model":"Cohere small v20220720 (410M)",
        "Mean win rate":"0.154",
        "MMLU - EM (Fairness)":"0.222",
        "BoolQ - EM (Fairness)":"0.374",
        "NarrativeQA - F1 (Fairness)":"0.179",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.055",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.219",
        "QuAC - F1 (Fairness)":"0.144",
        "HellaSwag - EM (Fairness)":"0.308",
        "OpenbookQA - EM (Fairness)":"0.28",
        "TruthfulQA - EM (Fairness)":"0.203",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"0.28",
        "IMDB - EM (Fairness)":"0.518",
        "CivilComments - EM (Fairness)":"0.495",
        "RAFT - EM (Fairness)":"0.452"
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B)",
        "Mean win rate":"0.608",
        "MMLU - EM (Fairness)":"0.317",
        "BoolQ - EM (Fairness)":"0.708",
        "NarrativeQA - F1 (Fairness)":"0.553",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.299",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.566",
        "QuAC - F1 (Fairness)":"0.275",
        "HellaSwag - EM (Fairness)":"0.687",
        "OpenbookQA - EM (Fairness)":"0.5",
        "TruthfulQA - EM (Fairness)":"0.12",
        "MS MARCO (regular) - RR@10 (Fairness)":"0.267",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"0.522",
        "IMDB - EM (Fairness)":"0.949",
        "CivilComments - EM (Fairness)":"0.415",
        "RAFT - EM (Fairness)":"0.604"
    },
    {
        "Model":"Cohere medium v20221108 (6.1B)",
        "Mean win rate":"0.34",
        "MMLU - EM (Fairness)":"0.22",
        "BoolQ - EM (Fairness)":"0.642",
        "NarrativeQA - F1 (Fairness)":"0.497",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.149",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.45",
        "QuAC - F1 (Fairness)":"0.229",
        "HellaSwag - EM (Fairness)":"0.567",
        "OpenbookQA - EM (Fairness)":"0.44",
        "TruthfulQA - EM (Fairness)":"0.182",
        "MS MARCO (regular) - RR@10 (Fairness)":"0.145",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"0.353",
        "IMDB - EM (Fairness)":"0.917",
        "CivilComments - EM (Fairness)":"0.493",
        "RAFT - EM (Fairness)":"0.571"
    },
    {
        "Model":"Cohere Command beta (6.1B)",
        "Mean win rate":"0.662",
        "MMLU - EM (Fairness)":"0.366",
        "BoolQ - EM (Fairness)":"0.748",
        "NarrativeQA - F1 (Fairness)":"0.595",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.167",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.654",
        "QuAC - F1 (Fairness)":"0.273",
        "HellaSwag - EM (Fairness)":"0.608",
        "OpenbookQA - EM (Fairness)":"0.468",
        "TruthfulQA - EM (Fairness)":"0.163",
        "MS MARCO (regular) - RR@10 (Fairness)":"0.411",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"0.69",
        "IMDB - EM (Fairness)":"0.95",
        "CivilComments - EM (Fairness)":"0.496",
        "RAFT - EM (Fairness)":"0.609"
    },
    {
        "Model":"Cohere Command beta (52.4B)",
        "Mean win rate":"0.866",
        "MMLU - EM (Fairness)":"0.407",
        "BoolQ - EM (Fairness)":"0.822",
        "NarrativeQA - F1 (Fairness)":"0.657",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.296",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.706",
        "QuAC - F1 (Fairness)":"0.316",
        "HellaSwag - EM (Fairness)":"0.699",
        "OpenbookQA - EM (Fairness)":"0.508",
        "TruthfulQA - EM (Fairness)":"0.222",
        "MS MARCO (regular) - RR@10 (Fairness)":"0.45",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"0.748",
        "IMDB - EM (Fairness)":"0.957",
        "CivilComments - EM (Fairness)":"0.544",
        "RAFT - EM (Fairness)":"0.627"
    },
    {
        "Model":"GPT-J (6B)",
        "Mean win rate":"0.29",
        "MMLU - EM (Fairness)":"0.22",
        "BoolQ - EM (Fairness)":"0.639",
        "NarrativeQA - F1 (Fairness)":"0.433",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.122",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.493",
        "QuAC - F1 (Fairness)":"0.249",
        "HellaSwag - EM (Fairness)":"0.486",
        "OpenbookQA - EM (Fairness)":"0.416",
        "TruthfulQA - EM (Fairness)":"0.18",
        "MS MARCO (regular) - RR@10 (Fairness)":"0.129",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"0.332",
        "IMDB - EM (Fairness)":"0.927",
        "CivilComments - EM (Fairness)":"0.488",
        "RAFT - EM (Fairness)":"0.594"
    },
    {
        "Model":"GPT-NeoX (20B)",
        "Mean win rate":"0.331",
        "MMLU - EM (Fairness)":"0.215",
        "BoolQ - EM (Fairness)":"0.609",
        "NarrativeQA - F1 (Fairness)":"0.461",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.154",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.525",
        "QuAC - F1 (Fairness)":"0.232",
        "HellaSwag - EM (Fairness)":"0.552",
        "OpenbookQA - EM (Fairness)":"0.438",
        "TruthfulQA - EM (Fairness)":"0.179",
        "MS MARCO (regular) - RR@10 (Fairness)":"0.148",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"0.381",
        "IMDB - EM (Fairness)":"0.928",
        "CivilComments - EM (Fairness)":"0.491",
        "RAFT - EM (Fairness)":"0.475"
    },
    {
        "Model":"Pythia (6.9B)",
        "Mean win rate":"0.171",
        "MMLU - EM (Fairness)":"0.207",
        "BoolQ - EM (Fairness)":"0.552",
        "NarrativeQA - F1 (Fairness)":"0.389",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.103",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.464",
        "QuAC - F1 (Fairness)":"0.198",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.18",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.911",
        "CivilComments - EM (Fairness)":"0.333",
        "RAFT - EM (Fairness)":"0.45"
    },
    {
        "Model":"Pythia (12B)",
        "Mean win rate":"0.226",
        "MMLU - EM (Fairness)":"0.212",
        "BoolQ - EM (Fairness)":"0.547",
        "NarrativeQA - F1 (Fairness)":"0.449",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.131",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.523",
        "QuAC - F1 (Fairness)":"0.227",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.154",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.916",
        "CivilComments - EM (Fairness)":"0.448",
        "RAFT - EM (Fairness)":"0.489"
    },
    {
        "Model":"T5 (11B)",
        "Mean win rate":"0.15",
        "MMLU - EM (Fairness)":"0.235",
        "BoolQ - EM (Fairness)":"0.723",
        "NarrativeQA - F1 (Fairness)":"0.05",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.159",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.424",
        "QuAC - F1 (Fairness)":"0.074",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.101",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.303",
        "CivilComments - EM (Fairness)":"0.329",
        "RAFT - EM (Fairness)":"0.351"
    },
    {
        "Model":"UL2 (20B)",
        "Mean win rate":"0.186",
        "MMLU - EM (Fairness)":"0.273",
        "BoolQ - EM (Fairness)":"0.698",
        "NarrativeQA - F1 (Fairness)":"0.053",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.162",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.303",
        "QuAC - F1 (Fairness)":"0.107",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.162",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.271",
        "CivilComments - EM (Fairness)":"0.423",
        "RAFT - EM (Fairness)":"0.375"
    },
    {
        "Model":"OPT (175B)",
        "Mean win rate":"0.622",
        "MMLU - EM (Fairness)":"0.287",
        "BoolQ - EM (Fairness)":"0.731",
        "NarrativeQA - F1 (Fairness)":"0.573",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.246",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.561",
        "QuAC - F1 (Fairness)":"0.266",
        "HellaSwag - EM (Fairness)":"0.66",
        "OpenbookQA - EM (Fairness)":"0.5",
        "TruthfulQA - EM (Fairness)":"0.203",
        "MS MARCO (regular) - RR@10 (Fairness)":"0.26",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"0.419",
        "IMDB - EM (Fairness)":"0.944",
        "CivilComments - EM (Fairness)":"0.491",
        "RAFT - EM (Fairness)":"0.58"
    },
    {
        "Model":"OPT (66B)",
        "Mean win rate":"0.476",
        "MMLU - EM (Fairness)":"0.229",
        "BoolQ - EM (Fairness)":"0.71",
        "NarrativeQA - F1 (Fairness)":"0.526",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.218",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.536",
        "QuAC - F1 (Fairness)":"0.268",
        "HellaSwag - EM (Fairness)":"0.597",
        "OpenbookQA - EM (Fairness)":"0.454",
        "TruthfulQA - EM (Fairness)":"0.173",
        "MS MARCO (regular) - RR@10 (Fairness)":"0.214",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"0.471",
        "IMDB - EM (Fairness)":"0.908",
        "CivilComments - EM (Fairness)":"0.5",
        "RAFT - EM (Fairness)":"0.536"
    },
    {
        "Model":"LLaMA (7B)",
        "Mean win rate":"0.553",
        "MMLU - EM (Fairness)":"0.284",
        "BoolQ - EM (Fairness)":"0.71",
        "NarrativeQA - F1 (Fairness)":"0.552",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.241",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.537",
        "QuAC - F1 (Fairness)":"0.257",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.219",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.936",
        "CivilComments - EM (Fairness)":"0.505",
        "RAFT - EM (Fairness)":"0.545"
    },
    {
        "Model":"LLaMA (13B)",
        "Mean win rate":"0.602",
        "MMLU - EM (Fairness)":"0.385",
        "BoolQ - EM (Fairness)":"0.666",
        "NarrativeQA - F1 (Fairness)":"0.628",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.288",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.561",
        "QuAC - F1 (Fairness)":"0.267",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.234",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.903",
        "CivilComments - EM (Fairness)":"0.533",
        "RAFT - EM (Fairness)":"0.605"
    },
    {
        "Model":"LLaMA (30B)",
        "Mean win rate":"0.822",
        "MMLU - EM (Fairness)":"0.496",
        "BoolQ - EM (Fairness)":"0.813",
        "NarrativeQA - F1 (Fairness)":"0.657",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.356",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.621",
        "QuAC - F1 (Fairness)":"0.325",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.266",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.913",
        "CivilComments - EM (Fairness)":"0.508",
        "RAFT - EM (Fairness)":"0.718"
    },
    {
        "Model":"LLaMA (65B)",
        "Mean win rate":"0.924",
        "MMLU - EM (Fairness)":"0.551",
        "BoolQ - EM (Fairness)":"0.847",
        "NarrativeQA - F1 (Fairness)":"0.661",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.375",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.633",
        "QuAC - F1 (Fairness)":"0.333",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.42",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.953",
        "CivilComments - EM (Fairness)":"0.574",
        "RAFT - EM (Fairness)":"0.668"
    },
    {
        "Model":"Llama 2 (7B)",
        "Mean win rate":"0.61",
        "MMLU - EM (Fairness)":"0.392",
        "BoolQ - EM (Fairness)":"0.706",
        "NarrativeQA - F1 (Fairness)":"0.596",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.264",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.55",
        "QuAC - F1 (Fairness)":"0.321",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.223",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.871",
        "CivilComments - EM (Fairness)":"0.503",
        "RAFT - EM (Fairness)":"0.609"
    },
    {
        "Model":"Llama 2 (13B)",
        "Mean win rate":"0.808",
        "MMLU - EM (Fairness)":"0.466",
        "BoolQ - EM (Fairness)":"0.732",
        "NarrativeQA - F1 (Fairness)":"0.657",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.309",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.58",
        "QuAC - F1 (Fairness)":"0.351",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.274",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.957",
        "CivilComments - EM (Fairness)":"0.489",
        "RAFT - EM (Fairness)":"0.673"
    },
    {
        "Model":"Llama 2 (70B)",
        "Mean win rate":"0.959",
        "MMLU - EM (Fairness)":"0.557",
        "BoolQ - EM (Fairness)":"0.859",
        "NarrativeQA - F1 (Fairness)":"0.709",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.4",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.637",
        "QuAC - F1 (Fairness)":"0.414",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.434",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.954",
        "CivilComments - EM (Fairness)":"0.551",
        "RAFT - EM (Fairness)":"0.7"
    },
    {
        "Model":"Alpaca (7B)",
        "Mean win rate":"0.372",
        "MMLU - EM (Fairness)":"0.346",
        "BoolQ - EM (Fairness)":"0.729",
        "NarrativeQA - F1 (Fairness)":"0.299",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.21",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.53",
        "QuAC - F1 (Fairness)":"0.204",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.202",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.699",
        "CivilComments - EM (Fairness)":"0.483",
        "RAFT - EM (Fairness)":"0.459"
    },
    {
        "Model":"Vicuna v1.3 (7B)",
        "Mean win rate":"0.622",
        "MMLU - EM (Fairness)":"0.385",
        "BoolQ - EM (Fairness)":"0.67",
        "NarrativeQA - F1 (Fairness)":"0.553",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.224",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.575",
        "QuAC - F1 (Fairness)":"0.304",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.235",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.906",
        "CivilComments - EM (Fairness)":"0.564",
        "RAFT - EM (Fairness)":"0.643"
    },
    {
        "Model":"Vicuna v1.3 (13B)",
        "Mean win rate":"0.715",
        "MMLU - EM (Fairness)":"0.424",
        "BoolQ - EM (Fairness)":"0.748",
        "NarrativeQA - F1 (Fairness)":"0.607",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.266",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.63",
        "QuAC - F1 (Fairness)":"0.324",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.315",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.707",
        "CivilComments - EM (Fairness)":"0.569",
        "RAFT - EM (Fairness)":"0.62"
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "Mean win rate":"0.861",
        "MMLU - EM (Fairness)":"0.542",
        "BoolQ - EM (Fairness)":"0.842",
        "NarrativeQA - F1 (Fairness)":"0.644",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.3",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.625",
        "QuAC - F1 (Fairness)":"0.353",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.332",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.952",
        "CivilComments - EM (Fairness)":"0.52",
        "RAFT - EM (Fairness)":"0.664"
    },
    {
        "Model":"TNLG v2 (530B)",
        "Mean win rate":"0.752",
        "MMLU - EM (Fairness)":"0.418",
        "BoolQ - EM (Fairness)":"0.767",
        "NarrativeQA - F1 (Fairness)":"0.632",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.318",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.598",
        "QuAC - F1 (Fairness)":"0.313",
        "HellaSwag - EM (Fairness)":"0.678",
        "OpenbookQA - EM (Fairness)":"0.504",
        "TruthfulQA - EM (Fairness)":"0.197",
        "MS MARCO (regular) - RR@10 (Fairness)":"0.341",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"0.612",
        "IMDB - EM (Fairness)":"0.936",
        "CivilComments - EM (Fairness)":"0.48",
        "RAFT - EM (Fairness)":"0.644"
    },
    {
        "Model":"TNLG v2 (6.7B)",
        "Mean win rate":"0.291",
        "MMLU - EM (Fairness)":"0.212",
        "BoolQ - EM (Fairness)":"0.665",
        "NarrativeQA - F1 (Fairness)":"0.517",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.162",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.501",
        "QuAC - F1 (Fairness)":"0.267",
        "HellaSwag - EM (Fairness)":"0.53",
        "OpenbookQA - EM (Fairness)":"0.412",
        "TruthfulQA - EM (Fairness)":"0.144",
        "MS MARCO (regular) - RR@10 (Fairness)":"0.14",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"0.317",
        "IMDB - EM (Fairness)":"0.912",
        "CivilComments - EM (Fairness)":"0.473",
        "RAFT - EM (Fairness)":"0.502"
    },
    {
        "Model":"davinci (175B)",
        "Mean win rate":"0.558",
        "MMLU - EM (Fairness)":"0.38",
        "BoolQ - EM (Fairness)":"0.682",
        "NarrativeQA - F1 (Fairness)":"0.597",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.276",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.567",
        "QuAC - F1 (Fairness)":"0.279",
        "HellaSwag - EM (Fairness)":"0.641",
        "OpenbookQA - EM (Fairness)":"0.502",
        "TruthfulQA - EM (Fairness)":"0.155",
        "MS MARCO (regular) - RR@10 (Fairness)":"0.185",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"0.357",
        "IMDB - EM (Fairness)":"0.921",
        "CivilComments - EM (Fairness)":"0.478",
        "RAFT - EM (Fairness)":"0.605"
    },
    {
        "Model":"curie (6.7B)",
        "Mean win rate":"0.231",
        "MMLU - EM (Fairness)":"0.218",
        "BoolQ - EM (Fairness)":"0.594",
        "NarrativeQA - F1 (Fairness)":"0.482",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.147",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.479",
        "QuAC - F1 (Fairness)":"0.243",
        "HellaSwag - EM (Fairness)":"0.522",
        "OpenbookQA - EM (Fairness)":"0.43",
        "TruthfulQA - EM (Fairness)":"0.186",
        "MS MARCO (regular) - RR@10 (Fairness)":"0.14",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"0.284",
        "IMDB - EM (Fairness)":"0.86",
        "CivilComments - EM (Fairness)":"0.412",
        "RAFT - EM (Fairness)":"0.473"
    },
    {
        "Model":"babbage (1.3B)",
        "Mean win rate":"0.134",
        "MMLU - EM (Fairness)":"0.206",
        "BoolQ - EM (Fairness)":"0.436",
        "NarrativeQA - F1 (Fairness)":"0.367",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.084",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.381",
        "QuAC - F1 (Fairness)":"0.202",
        "HellaSwag - EM (Fairness)":"0.401",
        "OpenbookQA - EM (Fairness)":"0.326",
        "TruthfulQA - EM (Fairness)":"0.178",
        "MS MARCO (regular) - RR@10 (Fairness)":"0.105",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"0.301",
        "IMDB - EM (Fairness)":"0.534",
        "CivilComments - EM (Fairness)":"0.474",
        "RAFT - EM (Fairness)":"0.438"
    },
    {
        "Model":"ada (350M)",
        "Mean win rate":"0.105",
        "MMLU - EM (Fairness)":"0.21",
        "BoolQ - EM (Fairness)":"0.507",
        "NarrativeQA - F1 (Fairness)":"0.205",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.057",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.273",
        "QuAC - F1 (Fairness)":"0.166",
        "HellaSwag - EM (Fairness)":"0.294",
        "OpenbookQA - EM (Fairness)":"0.318",
        "TruthfulQA - EM (Fairness)":"0.185",
        "MS MARCO (regular) - RR@10 (Fairness)":"0.086",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"0.268",
        "IMDB - EM (Fairness)":"0.806",
        "CivilComments - EM (Fairness)":"0.436",
        "RAFT - EM (Fairness)":"0.395"
    },
    {
        "Model":"text-davinci-003",
        "Mean win rate":"0.903",
        "MMLU - EM (Fairness)":"0.537",
        "BoolQ - EM (Fairness)":"0.858",
        "NarrativeQA - F1 (Fairness)":"0.664",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.356",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.721",
        "QuAC - F1 (Fairness)":"0.45",
        "HellaSwag - EM (Fairness)":"0.729",
        "OpenbookQA - EM (Fairness)":"0.578",
        "TruthfulQA - EM (Fairness)":"0.491",
        "MS MARCO (regular) - RR@10 (Fairness)":"0.335",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"0.633",
        "IMDB - EM (Fairness)":"0.833",
        "CivilComments - EM (Fairness)":"0.559",
        "RAFT - EM (Fairness)":"0.705"
    },
    {
        "Model":"text-davinci-002",
        "Mean win rate":"0.864",
        "MMLU - EM (Fairness)":"0.531",
        "BoolQ - EM (Fairness)":"0.837",
        "NarrativeQA - F1 (Fairness)":"0.646",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.32",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.659",
        "QuAC - F1 (Fairness)":"0.353",
        "HellaSwag - EM (Fairness)":"0.703",
        "OpenbookQA - EM (Fairness)":"0.54",
        "TruthfulQA - EM (Fairness)":"0.515",
        "MS MARCO (regular) - RR@10 (Fairness)":"0.373",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"0.639",
        "IMDB - EM (Fairness)":"0.934",
        "CivilComments - EM (Fairness)":"0.463",
        "RAFT - EM (Fairness)":"0.671"
    },
    {
        "Model":"text-curie-001",
        "Mean win rate":"0.377",
        "MMLU - EM (Fairness)":"0.231",
        "BoolQ - EM (Fairness)":"0.576",
        "NarrativeQA - F1 (Fairness)":"0.463",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.132",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.5",
        "QuAC - F1 (Fairness)":"0.255",
        "HellaSwag - EM (Fairness)":"0.534",
        "OpenbookQA - EM (Fairness)":"0.452",
        "TruthfulQA - EM (Fairness)":"0.239",
        "MS MARCO (regular) - RR@10 (Fairness)":"0.244",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"0.482",
        "IMDB - EM (Fairness)":"0.91",
        "CivilComments - EM (Fairness)":"0.471",
        "RAFT - EM (Fairness)":"0.458"
    },
    {
        "Model":"text-babbage-001",
        "Mean win rate":"0.244",
        "MMLU - EM (Fairness)":"0.205",
        "BoolQ - EM (Fairness)":"0.41",
        "NarrativeQA - F1 (Fairness)":"0.299",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.053",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.24",
        "QuAC - F1 (Fairness)":"0.196",
        "HellaSwag - EM (Fairness)":"0.405",
        "OpenbookQA - EM (Fairness)":"0.386",
        "TruthfulQA - EM (Fairness)":"0.207",
        "MS MARCO (regular) - RR@10 (Fairness)":"0.174",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"0.424",
        "IMDB - EM (Fairness)":"0.887",
        "CivilComments - EM (Fairness)":"0.499",
        "RAFT - EM (Fairness)":"0.475"
    },
    {
        "Model":"text-ada-001",
        "Mean win rate":"0.108",
        "MMLU - EM (Fairness)":"0.202",
        "BoolQ - EM (Fairness)":"0.378",
        "NarrativeQA - F1 (Fairness)":"0.119",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.012",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.083",
        "QuAC - F1 (Fairness)":"0.091",
        "HellaSwag - EM (Fairness)":"0.27",
        "OpenbookQA - EM (Fairness)":"0.266",
        "TruthfulQA - EM (Fairness)":"0.191",
        "MS MARCO (regular) - RR@10 (Fairness)":"0.107",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"0.276",
        "IMDB - EM (Fairness)":"0.769",
        "CivilComments - EM (Fairness)":"0.497",
        "RAFT - EM (Fairness)":"0.376"
    },
    {
        "Model":"gpt-3.5-turbo-0301",
        "Mean win rate":"0.662",
        "MMLU - EM (Fairness)":"0.53",
        "BoolQ - EM (Fairness)":"0.666",
        "NarrativeQA - F1 (Fairness)":"0.585",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.331",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.559",
        "QuAC - F1 (Fairness)":"0.417",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.514",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.844",
        "CivilComments - EM (Fairness)":"0.422",
        "RAFT - EM (Fairness)":"0.689"
    },
    {
        "Model":"gpt-3.5-turbo-0613",
        "Mean win rate":"0.718",
        "MMLU - EM (Fairness)":"0.313",
        "BoolQ - EM (Fairness)":"0.817",
        "NarrativeQA - F1 (Fairness)":"0.547",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.287",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.627",
        "QuAC - F1 (Fairness)":"0.398",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.255",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.912",
        "CivilComments - EM (Fairness)":"0.525",
        "RAFT - EM (Fairness)":"0.641"
    },
    {
        "Model":"RedPajama-INCITE-Base-v1 (3B)",
        "Mean win rate":"0.27",
        "MMLU - EM (Fairness)":"0.232",
        "BoolQ - EM (Fairness)":"0.624",
        "NarrativeQA - F1 (Fairness)":"0.42",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.145",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.452",
        "QuAC - F1 (Fairness)":"0.238",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.248",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.89",
        "CivilComments - EM (Fairness)":"0.393",
        "RAFT - EM (Fairness)":"0.475"
    },
    {
        "Model":"RedPajama-INCITE-Instruct-v1 (3B)",
        "Mean win rate":"0.369",
        "MMLU - EM (Fairness)":"0.222",
        "BoolQ - EM (Fairness)":"0.648",
        "NarrativeQA - F1 (Fairness)":"0.506",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.143",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.571",
        "QuAC - F1 (Fairness)":"0.183",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.179",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.876",
        "CivilComments - EM (Fairness)":"0.499",
        "RAFT - EM (Fairness)":"0.632"
    },
    {
        "Model":"RedPajama-INCITE-Base (7B)",
        "Mean win rate":"0.323",
        "MMLU - EM (Fairness)":"0.276",
        "BoolQ - EM (Fairness)":"0.65",
        "NarrativeQA - F1 (Fairness)":"0.524",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.193",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.514",
        "QuAC - F1 (Fairness)":"0.238",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.17",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.694",
        "CivilComments - EM (Fairness)":"0.431",
        "RAFT - EM (Fairness)":"0.595"
    },
    {
        "Model":"RedPajama-INCITE-Instruct (7B)",
        "Mean win rate":"0.466",
        "MMLU - EM (Fairness)":"0.305",
        "BoolQ - EM (Fairness)":"0.616",
        "NarrativeQA - F1 (Fairness)":"0.506",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.164",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.592",
        "QuAC - F1 (Fairness)":"0.181",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.183",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.907",
        "CivilComments - EM (Fairness)":"0.54",
        "RAFT - EM (Fairness)":"0.67"
    },
    {
        "Model":"MPT (30B)",
        "Mean win rate":"0.746",
        "MMLU - EM (Fairness)":"0.41",
        "BoolQ - EM (Fairness)":"0.631",
        "NarrativeQA - F1 (Fairness)":"0.653",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.287",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.624",
        "QuAC - F1 (Fairness)":"0.318",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.19",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.955",
        "CivilComments - EM (Fairness)":"0.553",
        "RAFT - EM (Fairness)":"0.68"
    },
    {
        "Model":"MPT-Instruct (30B)",
        "Mean win rate":"0.687",
        "MMLU - EM (Fairness)":"0.4",
        "BoolQ - EM (Fairness)":"0.807",
        "NarrativeQA - F1 (Fairness)":"0.633",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.233",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.639",
        "QuAC - F1 (Fairness)":"0.252",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.18",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.944",
        "CivilComments - EM (Fairness)":"0.527",
        "RAFT - EM (Fairness)":"0.636"
    },
    {
        "Model":"Falcon (7B)",
        "Mean win rate":"0.447",
        "MMLU - EM (Fairness)":"0.261",
        "BoolQ - EM (Fairness)":"0.702",
        "NarrativeQA - F1 (Fairness)":"0.52",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.233",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.537",
        "QuAC - F1 (Fairness)":"0.262",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.213",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.794",
        "CivilComments - EM (Fairness)":"0.494",
        "RAFT - EM (Fairness)":"0.555"
    },
    {
        "Model":"Falcon-Instruct (7B)",
        "Mean win rate":"0.297",
        "MMLU - EM (Fairness)":"0.261",
        "BoolQ - EM (Fairness)":"0.637",
        "NarrativeQA - F1 (Fairness)":"0.354",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.148",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.383",
        "QuAC - F1 (Fairness)":"0.219",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.183",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.811",
        "CivilComments - EM (Fairness)":"0.502",
        "RAFT - EM (Fairness)":"0.5"
    },
    {
        "Model":"Falcon (40B)",
        "Mean win rate":"0.686",
        "MMLU - EM (Fairness)":"0.48",
        "BoolQ - EM (Fairness)":"0.783",
        "NarrativeQA - F1 (Fairness)":"0.559",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.338",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.625",
        "QuAC - F1 (Fairness)":"0.256",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.292",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.954",
        "CivilComments - EM (Fairness)":"0.292",
        "RAFT - EM (Fairness)":"0.611"
    },
    {
        "Model":"Falcon-Instruct (40B)",
        "Mean win rate":"0.709",
        "MMLU - EM (Fairness)":"0.466",
        "BoolQ - EM (Fairness)":"0.799",
        "NarrativeQA - F1 (Fairness)":"0.543",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.331",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.607",
        "QuAC - F1 (Fairness)":"0.308",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.312",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.957",
        "CivilComments - EM (Fairness)":"0.462",
        "RAFT - EM (Fairness)":"0.561"
    },
    {
        "Model":"GLM (130B)",
        "Mean win rate":"0.513",
        "MMLU - EM (Fairness)":"0.315",
        "BoolQ - EM (Fairness)":"0.69",
        "NarrativeQA - F1 (Fairness)":"0.615",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.12",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.597",
        "QuAC - F1 (Fairness)":"0.205",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.192",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.933",
        "CivilComments - EM (Fairness)":"0.5",
        "RAFT - EM (Fairness)":"0.575"
    },
    {
        "Model":"InstructPalmyra (30B)",
        "Mean win rate":"0.538",
        "MMLU - EM (Fairness)":"0.371",
        "BoolQ - EM (Fairness)":"0.7",
        "NarrativeQA - F1 (Fairness)":"0.405",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.276",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.63",
        "QuAC - F1 (Fairness)":"0.337",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.152",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.931",
        "CivilComments - EM (Fairness)":"0.449",
        "RAFT - EM (Fairness)":"0.618"
    },
    {
        "Model":"Palmyra X (43B)",
        "Mean win rate":"0.797",
        "MMLU - EM (Fairness)":"0.588",
        "BoolQ - EM (Fairness)":"0.875",
        "NarrativeQA - F1 (Fairness)":"0.651",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.362",
        "NaturalQuestions (open-book) - F1 (Fairness)":"-",
        "QuAC - F1 (Fairness)":"0.399",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.542",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.918",
        "CivilComments - EM (Fairness)":"0.006",
        "RAFT - EM (Fairness)":"0.672"
    },
    {
        "Model":"YaLM (100B)",
        "Mean win rate":"0.167",
        "MMLU - EM (Fairness)":"0.243",
        "BoolQ - EM (Fairness)":"0.583",
        "NarrativeQA - F1 (Fairness)":"0.146",
        "NaturalQuestions (closed-book) - F1 (Fairness)":"0.052",
        "NaturalQuestions (open-book) - F1 (Fairness)":"0.177",
        "QuAC - F1 (Fairness)":"0.1",
        "HellaSwag - EM (Fairness)":"-",
        "OpenbookQA - EM (Fairness)":"-",
        "TruthfulQA - EM (Fairness)":"0.202",
        "MS MARCO (regular) - RR@10 (Fairness)":"-",
        "MS MARCO (TREC) - NDCG@10 (Fairness)":"-",
        "IMDB - EM (Fairness)":"0.8",
        "CivilComments - EM (Fairness)":"0.456",
        "RAFT - EM (Fairness)":"0.342"
    }
]