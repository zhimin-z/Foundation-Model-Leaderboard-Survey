[
    {
        "Model":"J1-Jumbo v1 (178B)",
        "Mean win rate":"0.222",
        "MMLU - Denoised inference time (s)":"0.457",
        "BoolQ - Denoised inference time (s)":"0.62",
        "NarrativeQA - Denoised inference time (s)":"1.126",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.493",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"1.06",
        "QuAC - Denoised inference time (s)":"2.064",
        "HellaSwag - Denoised inference time (s)":"0.284",
        "OpenbookQA - Denoised inference time (s)":"0.259",
        "TruthfulQA - Denoised inference time (s)":"0.443",
        "MS MARCO (regular) - Denoised inference time (s)":"0.501",
        "MS MARCO (TREC) - Denoised inference time (s)":"0.496",
        "CNN\/DailyMail - Denoised inference time (s)":"3.777",
        "XSUM - Denoised inference time (s)":"1.629",
        "IMDB - Denoised inference time (s)":"0.852",
        "CivilComments - Denoised inference time (s)":"0.552",
        "RAFT - Denoised inference time (s)":"0.687"
    },
    {
        "Model":"J1-Large v1 (7.5B)",
        "Mean win rate":"0.389",
        "MMLU - Denoised inference time (s)":"0.377",
        "BoolQ - Denoised inference time (s)":"0.485",
        "NarrativeQA - Denoised inference time (s)":"0.797",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.372",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"0.733",
        "QuAC - Denoised inference time (s)":"1.16",
        "HellaSwag - Denoised inference time (s)":"0.253",
        "OpenbookQA - Denoised inference time (s)":"0.238",
        "TruthfulQA - Denoised inference time (s)":"0.365",
        "MS MARCO (regular) - Denoised inference time (s)":"0.393",
        "MS MARCO (TREC) - Denoised inference time (s)":"0.389",
        "CNN\/DailyMail - Denoised inference time (s)":"2.011",
        "XSUM - Denoised inference time (s)":"0.903",
        "IMDB - Denoised inference time (s)":"0.637",
        "CivilComments - Denoised inference time (s)":"0.434",
        "RAFT - Denoised inference time (s)":"0.499"
    },
    {
        "Model":"J1-Grande v1 (17B)",
        "Mean win rate":"0.317",
        "MMLU - Denoised inference time (s)":"0.411",
        "BoolQ - Denoised inference time (s)":"0.535",
        "NarrativeQA - Denoised inference time (s)":"0.923",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.466",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"0.873",
        "QuAC - Denoised inference time (s)":"1.413",
        "HellaSwag - Denoised inference time (s)":"0.33",
        "OpenbookQA - Denoised inference time (s)":"0.281",
        "TruthfulQA - Denoised inference time (s)":"0.396",
        "MS MARCO (regular) - Denoised inference time (s)":"0.428",
        "MS MARCO (TREC) - Denoised inference time (s)":"0.424",
        "CNN\/DailyMail - Denoised inference time (s)":"2.074",
        "XSUM - Denoised inference time (s)":"1.07",
        "IMDB - Denoised inference time (s)":"0.732",
        "CivilComments - Denoised inference time (s)":"0.482",
        "RAFT - Denoised inference time (s)":"0.59"
    },
    {
        "Model":"J1-Grande v2 beta (17B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"Jurassic-2 Large (7.5B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"Luminous Base (13B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"Luminous Extended (30B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"Luminous Supreme (70B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B)",
        "Mean win rate":"0.138",
        "MMLU - Denoised inference time (s)":"0.578",
        "BoolQ - Denoised inference time (s)":"0.637",
        "NarrativeQA - Denoised inference time (s)":"1.722",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.777",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"1.102",
        "QuAC - Denoised inference time (s)":"3.694",
        "HellaSwag - Denoised inference time (s)":"0.549",
        "OpenbookQA - Denoised inference time (s)":"0.447",
        "TruthfulQA - Denoised inference time (s)":"0.568",
        "MS MARCO (regular) - Denoised inference time (s)":"0.578",
        "MS MARCO (TREC) - Denoised inference time (s)":"0.587",
        "CNN\/DailyMail - Denoised inference time (s)":"4.076",
        "XSUM - Denoised inference time (s)":"2.408",
        "IMDB - Denoised inference time (s)":"0.79",
        "CivilComments - Denoised inference time (s)":"0.594",
        "RAFT - Denoised inference time (s)":"0.883"
    },
    {
        "Model":"BLOOM (176B)",
        "Mean win rate":"0.268",
        "MMLU - Denoised inference time (s)":"0.233",
        "BoolQ - Denoised inference time (s)":"0.853",
        "NarrativeQA - Denoised inference time (s)":"2.598",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"1.115",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"2.547",
        "QuAC - Denoised inference time (s)":"5.306",
        "HellaSwag - Denoised inference time (s)":"0.075",
        "OpenbookQA - Denoised inference time (s)":"0.032",
        "TruthfulQA - Denoised inference time (s)":"0.143",
        "MS MARCO (regular) - Denoised inference time (s)":"0.257",
        "MS MARCO (TREC) - Denoised inference time (s)":"0.246",
        "CNN\/DailyMail - Denoised inference time (s)":"5.584",
        "XSUM - Denoised inference time (s)":"3.9",
        "IMDB - Denoised inference time (s)":"3.536",
        "CivilComments - Denoised inference time (s)":"0.533",
        "RAFT - Denoised inference time (s)":"1.866"
    },
    {
        "Model":"T0pp (11B)",
        "Mean win rate":"0.42",
        "MMLU - Denoised inference time (s)":"0.145",
        "BoolQ - Denoised inference time (s)":"0.374",
        "NarrativeQA - Denoised inference time (s)":"0.945",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"1.457",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"2.895",
        "QuAC - Denoised inference time (s)":"1.239",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"0.142",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"1.066",
        "XSUM - Denoised inference time (s)":"0.554",
        "IMDB - Denoised inference time (s)":"0.393",
        "CivilComments - Denoised inference time (s)":"0.391",
        "RAFT - Denoised inference time (s)":"0.586"
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B)",
        "Mean win rate":"0.199",
        "MMLU - Denoised inference time (s)":"0.489",
        "BoolQ - Denoised inference time (s)":"0.598",
        "NarrativeQA - Denoised inference time (s)":"1.062",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.565",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"1.085",
        "QuAC - Denoised inference time (s)":"2.089",
        "HellaSwag - Denoised inference time (s)":"0.359",
        "OpenbookQA - Denoised inference time (s)":"0.314",
        "TruthfulQA - Denoised inference time (s)":"0.501",
        "MS MARCO (regular) - Denoised inference time (s)":"0.499",
        "MS MARCO (TREC) - Denoised inference time (s)":"0.501",
        "CNN\/DailyMail - Denoised inference time (s)":"4.337",
        "XSUM - Denoised inference time (s)":"1.741",
        "IMDB - Denoised inference time (s)":"0.796",
        "CivilComments - Denoised inference time (s)":"0.546",
        "RAFT - Denoised inference time (s)":"0.667"
    },
    {
        "Model":"Cohere large v20220720 (13.1B)",
        "Mean win rate":"0.407",
        "MMLU - Denoised inference time (s)":"0.317",
        "BoolQ - Denoised inference time (s)":"0.421",
        "NarrativeQA - Denoised inference time (s)":"0.729",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.337",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"0.774",
        "QuAC - Denoised inference time (s)":"1.262",
        "HellaSwag - Denoised inference time (s)":"0.225",
        "OpenbookQA - Denoised inference time (s)":"0.201",
        "TruthfulQA - Denoised inference time (s)":"0.325",
        "MS MARCO (regular) - Denoised inference time (s)":"0.33",
        "MS MARCO (TREC) - Denoised inference time (s)":"0.327",
        "CNN\/DailyMail - Denoised inference time (s)":"2.269",
        "XSUM - Denoised inference time (s)":"1.075",
        "IMDB - Denoised inference time (s)":"0.536",
        "CivilComments - Denoised inference time (s)":"0.375",
        "RAFT - Denoised inference time (s)":"0.444"
    },
    {
        "Model":"Cohere medium v20220720 (6.1B)",
        "Mean win rate":"0.541",
        "MMLU - Denoised inference time (s)":"0.281",
        "BoolQ - Denoised inference time (s)":"0.35",
        "NarrativeQA - Denoised inference time (s)":"0.533",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.259",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"0.535",
        "QuAC - Denoised inference time (s)":"0.735",
        "HellaSwag - Denoised inference time (s)":"0.204",
        "OpenbookQA - Denoised inference time (s)":"0.187",
        "TruthfulQA - Denoised inference time (s)":"0.287",
        "MS MARCO (regular) - Denoised inference time (s)":"0.289",
        "MS MARCO (TREC) - Denoised inference time (s)":"0.288",
        "CNN\/DailyMail - Denoised inference time (s)":"1.2",
        "XSUM - Denoised inference time (s)":"0.724",
        "IMDB - Denoised inference time (s)":"0.452",
        "CivilComments - Denoised inference time (s)":"0.321",
        "RAFT - Denoised inference time (s)":"0.358"
    },
    {
        "Model":"Cohere small v20220720 (410M)",
        "Mean win rate":"0.534",
        "MMLU - Denoised inference time (s)":"0.284",
        "BoolQ - Denoised inference time (s)":"0.367",
        "NarrativeQA - Denoised inference time (s)":"0.56",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.251",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"0.605",
        "QuAC - Denoised inference time (s)":"0.619",
        "HellaSwag - Denoised inference time (s)":"0.223",
        "OpenbookQA - Denoised inference time (s)":"0.214",
        "TruthfulQA - Denoised inference time (s)":"0.289",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"0.291",
        "CNN\/DailyMail - Denoised inference time (s)":"0.954",
        "XSUM - Denoised inference time (s)":"0.642",
        "IMDB - Denoised inference time (s)":"0.458",
        "CivilComments - Denoised inference time (s)":"0.329",
        "RAFT - Denoised inference time (s)":"0.36"
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"Cohere medium v20221108 (6.1B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"Cohere Command beta (6.1B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"Cohere Command beta (52.4B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"GPT-J (6B)",
        "Mean win rate":"0.601",
        "MMLU - Denoised inference time (s)":"0.07",
        "BoolQ - Denoised inference time (s)":"0.499",
        "NarrativeQA - Denoised inference time (s)":"1.311",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"1.777",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"3.866",
        "QuAC - Denoised inference time (s)":"1.389",
        "HellaSwag - Denoised inference time (s)":"0.03",
        "OpenbookQA - Denoised inference time (s)":"0.019",
        "TruthfulQA - Denoised inference time (s)":"0.044",
        "MS MARCO (regular) - Denoised inference time (s)":"0.084",
        "MS MARCO (TREC) - Denoised inference time (s)":"0.081",
        "CNN\/DailyMail - Denoised inference time (s)":"2.076",
        "XSUM - Denoised inference time (s)":"0.742",
        "IMDB - Denoised inference time (s)":"0.701",
        "CivilComments - Denoised inference time (s)":"0.307",
        "RAFT - Denoised inference time (s)":"0.628"
    },
    {
        "Model":"GPT-NeoX (20B)",
        "Mean win rate":"0.514",
        "MMLU - Denoised inference time (s)":"0.133",
        "BoolQ - Denoised inference time (s)":"0.773",
        "NarrativeQA - Denoised inference time (s)":"1.468",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.482",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"2.137",
        "QuAC - Denoised inference time (s)":"2.025",
        "HellaSwag - Denoised inference time (s)":"0.025",
        "OpenbookQA - Denoised inference time (s)":"0.024",
        "TruthfulQA - Denoised inference time (s)":"0.084",
        "MS MARCO (regular) - Denoised inference time (s)":"0.118",
        "MS MARCO (TREC) - Denoised inference time (s)":"0.116",
        "CNN\/DailyMail - Denoised inference time (s)":"2.133",
        "XSUM - Denoised inference time (s)":"1.116",
        "IMDB - Denoised inference time (s)":"0.862",
        "CivilComments - Denoised inference time (s)":"0.408",
        "RAFT - Denoised inference time (s)":"1.156"
    },
    {
        "Model":"Pythia (6.9B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"Pythia (12B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"T5 (11B)",
        "Mean win rate":"0.434",
        "MMLU - Denoised inference time (s)":"0.218",
        "BoolQ - Denoised inference time (s)":"0.271",
        "NarrativeQA - Denoised inference time (s)":"1.054",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"2.856",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"12.846",
        "QuAC - Denoised inference time (s)":"1.032",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"0.21",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"1.654",
        "XSUM - Denoised inference time (s)":"1.159",
        "IMDB - Denoised inference time (s)":"0.278",
        "CivilComments - Denoised inference time (s)":"0.27",
        "RAFT - Denoised inference time (s)":"0.448"
    },
    {
        "Model":"UL2 (20B)",
        "Mean win rate":"0.506",
        "MMLU - Denoised inference time (s)":"0.182",
        "BoolQ - Denoised inference time (s)":"0.313",
        "NarrativeQA - Denoised inference time (s)":"1.182",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"1.994",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"3.093",
        "QuAC - Denoised inference time (s)":"1.226",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"0.168",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"1.108",
        "XSUM - Denoised inference time (s)":"0.774",
        "IMDB - Denoised inference time (s)":"0.215",
        "CivilComments - Denoised inference time (s)":"0.264",
        "RAFT - Denoised inference time (s)":"0.434"
    },
    {
        "Model":"OPT (175B)",
        "Mean win rate":"0.241",
        "MMLU - Denoised inference time (s)":"0.12",
        "BoolQ - Denoised inference time (s)":"0.869",
        "NarrativeQA - Denoised inference time (s)":"2.783",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"4.548",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"7.78",
        "QuAC - Denoised inference time (s)":"4.049",
        "HellaSwag - Denoised inference time (s)":"0.71",
        "OpenbookQA - Denoised inference time (s)":"0.038",
        "TruthfulQA - Denoised inference time (s)":"0.141",
        "MS MARCO (regular) - Denoised inference time (s)":"0.241",
        "MS MARCO (TREC) - Denoised inference time (s)":"0.226",
        "CNN\/DailyMail - Denoised inference time (s)":"4.729",
        "XSUM - Denoised inference time (s)":"2.523",
        "IMDB - Denoised inference time (s)":"1.575",
        "CivilComments - Denoised inference time (s)":"0.498",
        "RAFT - Denoised inference time (s)":"0.962"
    },
    {
        "Model":"OPT (66B)",
        "Mean win rate":"0.467",
        "MMLU - Denoised inference time (s)":"0.055",
        "BoolQ - Denoised inference time (s)":"0.834",
        "NarrativeQA - Denoised inference time (s)":"1.98",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.611",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"3.632",
        "QuAC - Denoised inference time (s)":"2.658",
        "HellaSwag - Denoised inference time (s)":"0.971",
        "OpenbookQA - Denoised inference time (s)":"0.188",
        "TruthfulQA - Denoised inference time (s)":"0.041",
        "MS MARCO (regular) - Denoised inference time (s)":"0.076",
        "MS MARCO (TREC) - Denoised inference time (s)":"0.102",
        "CNN\/DailyMail - Denoised inference time (s)":"1.972",
        "XSUM - Denoised inference time (s)":"0.885",
        "IMDB - Denoised inference time (s)":"0.54",
        "CivilComments - Denoised inference time (s)":"0.212",
        "RAFT - Denoised inference time (s)":"1.871"
    },
    {
        "Model":"LLaMA (7B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"LLaMA (13B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"LLaMA (30B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"LLaMA (65B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"Llama 2 (7B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"Llama 2 (13B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"Llama 2 (70B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"Alpaca (7B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"Vicuna v1.3 (7B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"Vicuna v1.3 (13B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"Mistral v0.1 (7B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"TNLG v2 (530B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"TNLG v2 (6.7B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"davinci (175B)",
        "Mean win rate":"0.558",
        "MMLU - Denoised inference time (s)":"0.212",
        "BoolQ - Denoised inference time (s)":"0.21",
        "NarrativeQA - Denoised inference time (s)":"0.369",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.327",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"0.462",
        "QuAC - Denoised inference time (s)":"1.085",
        "HellaSwag - Denoised inference time (s)":"0.193",
        "OpenbookQA - Denoised inference time (s)":"0.184",
        "TruthfulQA - Denoised inference time (s)":"0.215",
        "MS MARCO (regular) - Denoised inference time (s)":"0.211",
        "MS MARCO (TREC) - Denoised inference time (s)":"0.214",
        "CNN\/DailyMail - Denoised inference time (s)":"2.256",
        "XSUM - Denoised inference time (s)":"1.148",
        "IMDB - Denoised inference time (s)":"0.225",
        "CivilComments - Denoised inference time (s)":"0.21",
        "RAFT - Denoised inference time (s)":"0.279"
    },
    {
        "Model":"curie (6.7B)",
        "Mean win rate":"0.895",
        "MMLU - Denoised inference time (s)":"0.092",
        "BoolQ - Denoised inference time (s)":"0.1",
        "NarrativeQA - Denoised inference time (s)":"0.152",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.122",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"0.189",
        "QuAC - Denoised inference time (s)":"0.323",
        "HellaSwag - Denoised inference time (s)":"0.084",
        "OpenbookQA - Denoised inference time (s)":"0.079",
        "TruthfulQA - Denoised inference time (s)":"0.094",
        "MS MARCO (regular) - Denoised inference time (s)":"0.094",
        "MS MARCO (TREC) - Denoised inference time (s)":"0.095",
        "CNN\/DailyMail - Denoised inference time (s)":"0.623",
        "XSUM - Denoised inference time (s)":"0.294",
        "IMDB - Denoised inference time (s)":"0.11",
        "CivilComments - Denoised inference time (s)":"0.097",
        "RAFT - Denoised inference time (s)":"0.112"
    },
    {
        "Model":"babbage (1.3B)",
        "Mean win rate":"0.861",
        "MMLU - Denoised inference time (s)":"0.119",
        "BoolQ - Denoised inference time (s)":"0.121",
        "NarrativeQA - Denoised inference time (s)":"0.176",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.152",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"0.232",
        "QuAC - Denoised inference time (s)":"0.261",
        "HellaSwag - Denoised inference time (s)":"0.113",
        "OpenbookQA - Denoised inference time (s)":"0.111",
        "TruthfulQA - Denoised inference time (s)":"0.12",
        "MS MARCO (regular) - Denoised inference time (s)":"0.122",
        "MS MARCO (TREC) - Denoised inference time (s)":"0.122",
        "CNN\/DailyMail - Denoised inference time (s)":"0.533",
        "XSUM - Denoised inference time (s)":"0.272",
        "IMDB - Denoised inference time (s)":"0.128",
        "CivilComments - Denoised inference time (s)":"0.12",
        "RAFT - Denoised inference time (s)":"0.137"
    },
    {
        "Model":"ada (350M)",
        "Mean win rate":"0.77",
        "MMLU - Denoised inference time (s)":"0.14",
        "BoolQ - Denoised inference time (s)":"0.141",
        "NarrativeQA - Denoised inference time (s)":"0.211",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.167",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"0.271",
        "QuAC - Denoised inference time (s)":"0.27",
        "HellaSwag - Denoised inference time (s)":"0.138",
        "OpenbookQA - Denoised inference time (s)":"0.136",
        "TruthfulQA - Denoised inference time (s)":"0.141",
        "MS MARCO (regular) - Denoised inference time (s)":"0.142",
        "MS MARCO (TREC) - Denoised inference time (s)":"0.142",
        "CNN\/DailyMail - Denoised inference time (s)":"0.598",
        "XSUM - Denoised inference time (s)":"0.237",
        "IMDB - Denoised inference time (s)":"0.142",
        "CivilComments - Denoised inference time (s)":"0.141",
        "RAFT - Denoised inference time (s)":"0.154"
    },
    {
        "Model":"text-davinci-003",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"text-davinci-002",
        "Mean win rate":"0.604",
        "MMLU - Denoised inference time (s)":"0.196",
        "BoolQ - Denoised inference time (s)":"0.191",
        "NarrativeQA - Denoised inference time (s)":"0.512",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.264",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"0.394",
        "QuAC - Denoised inference time (s)":"0.891",
        "HellaSwag - Denoised inference time (s)":"0.171",
        "OpenbookQA - Denoised inference time (s)":"0.158",
        "TruthfulQA - Denoised inference time (s)":"0.2",
        "MS MARCO (regular) - Denoised inference time (s)":"0.192",
        "MS MARCO (TREC) - Denoised inference time (s)":"0.198",
        "CNN\/DailyMail - Denoised inference time (s)":"2.236",
        "XSUM - Denoised inference time (s)":"1.026",
        "IMDB - Denoised inference time (s)":"0.247",
        "CivilComments - Denoised inference time (s)":"0.186",
        "RAFT - Denoised inference time (s)":"0.276"
    },
    {
        "Model":"text-curie-001",
        "Mean win rate":"0.783",
        "MMLU - Denoised inference time (s)":"0.133",
        "BoolQ - Denoised inference time (s)":"0.143",
        "NarrativeQA - Denoised inference time (s)":"0.205",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.153",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"0.185",
        "QuAC - Denoised inference time (s)":"0.298",
        "HellaSwag - Denoised inference time (s)":"0.125",
        "OpenbookQA - Denoised inference time (s)":"0.119",
        "TruthfulQA - Denoised inference time (s)":"0.134",
        "MS MARCO (regular) - Denoised inference time (s)":"0.136",
        "MS MARCO (TREC) - Denoised inference time (s)":"0.135",
        "CNN\/DailyMail - Denoised inference time (s)":"0.799",
        "XSUM - Denoised inference time (s)":"0.364",
        "IMDB - Denoised inference time (s)":"0.147",
        "CivilComments - Denoised inference time (s)":"0.142",
        "RAFT - Denoised inference time (s)":"0.152"
    },
    {
        "Model":"text-babbage-001",
        "Mean win rate":"0.778",
        "MMLU - Denoised inference time (s)":"0.133",
        "BoolQ - Denoised inference time (s)":"0.142",
        "NarrativeQA - Denoised inference time (s)":"0.243",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.136",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"0.204",
        "QuAC - Denoised inference time (s)":"0.314",
        "HellaSwag - Denoised inference time (s)":"0.125",
        "OpenbookQA - Denoised inference time (s)":"0.122",
        "TruthfulQA - Denoised inference time (s)":"0.134",
        "MS MARCO (regular) - Denoised inference time (s)":"0.136",
        "MS MARCO (TREC) - Denoised inference time (s)":"0.135",
        "CNN\/DailyMail - Denoised inference time (s)":"0.968",
        "XSUM - Denoised inference time (s)":"0.431",
        "IMDB - Denoised inference time (s)":"0.157",
        "CivilComments - Denoised inference time (s)":"0.138",
        "RAFT - Denoised inference time (s)":"0.153"
    },
    {
        "Model":"text-ada-001",
        "Mean win rate":"0.938",
        "MMLU - Denoised inference time (s)":"0.088",
        "BoolQ - Denoised inference time (s)":"0.096",
        "NarrativeQA - Denoised inference time (s)":"0.171",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.085",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"0.128",
        "QuAC - Denoised inference time (s)":"0.21",
        "HellaSwag - Denoised inference time (s)":"0.079",
        "OpenbookQA - Denoised inference time (s)":"0.076",
        "TruthfulQA - Denoised inference time (s)":"0.089",
        "MS MARCO (regular) - Denoised inference time (s)":"0.09",
        "MS MARCO (TREC) - Denoised inference time (s)":"0.09",
        "CNN\/DailyMail - Denoised inference time (s)":"0.793",
        "XSUM - Denoised inference time (s)":"0.311",
        "IMDB - Denoised inference time (s)":"0.109",
        "CivilComments - Denoised inference time (s)":"0.092",
        "RAFT - Denoised inference time (s)":"0.107"
    },
    {
        "Model":"gpt-3.5-turbo-0301",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"gpt-3.5-turbo-0613",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"RedPajama-INCITE-Base-v1 (3B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"RedPajama-INCITE-Instruct-v1 (3B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"RedPajama-INCITE-Base (7B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"RedPajama-INCITE-Instruct (7B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"MPT (30B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"MPT-Instruct (30B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"Falcon (7B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"Falcon-Instruct (7B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"Falcon (40B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"Falcon-Instruct (40B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"GLM (130B)",
        "Mean win rate":"0.151",
        "MMLU - Denoised inference time (s)":"0.335",
        "BoolQ - Denoised inference time (s)":"1.191",
        "NarrativeQA - Denoised inference time (s)":"2.315",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"0.953",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"2.369",
        "QuAC - Denoised inference time (s)":"4.219",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"0.158",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"3.514",
        "XSUM - Denoised inference time (s)":"2.537",
        "IMDB - Denoised inference time (s)":"1.497",
        "CivilComments - Denoised inference time (s)":"0.695",
        "RAFT - Denoised inference time (s)":"1.471"
    },
    {
        "Model":"InstructPalmyra (30B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"Palmyra X (43B)",
        "Mean win rate":"-",
        "MMLU - Denoised inference time (s)":"-",
        "BoolQ - Denoised inference time (s)":"-",
        "NarrativeQA - Denoised inference time (s)":"-",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"-",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"-",
        "QuAC - Denoised inference time (s)":"-",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"-",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"-",
        "XSUM - Denoised inference time (s)":"-",
        "IMDB - Denoised inference time (s)":"-",
        "CivilComments - Denoised inference time (s)":"-",
        "RAFT - Denoised inference time (s)":"-"
    },
    {
        "Model":"YaLM (100B)",
        "Mean win rate":"0.266",
        "MMLU - Denoised inference time (s)":"0.143",
        "BoolQ - Denoised inference time (s)":"0.828",
        "NarrativeQA - Denoised inference time (s)":"2.314",
        "NaturalQuestions (closed-book) - Denoised inference time (s)":"2.722",
        "NaturalQuestions (open-book) - Denoised inference time (s)":"4.463",
        "QuAC - Denoised inference time (s)":"2.278",
        "HellaSwag - Denoised inference time (s)":"-",
        "OpenbookQA - Denoised inference time (s)":"-",
        "TruthfulQA - Denoised inference time (s)":"0.092",
        "MS MARCO (regular) - Denoised inference time (s)":"-",
        "MS MARCO (TREC) - Denoised inference time (s)":"-",
        "CNN\/DailyMail - Denoised inference time (s)":"2.346",
        "XSUM - Denoised inference time (s)":"1.671",
        "IMDB - Denoised inference time (s)":"1.137",
        "CivilComments - Denoised inference time (s)":"0.41",
        "RAFT - Denoised inference time (s)":"0.89"
    }
]