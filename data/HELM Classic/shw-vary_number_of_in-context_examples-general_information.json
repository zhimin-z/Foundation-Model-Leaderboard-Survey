[
    {
        "Model":"Anthropic-LM v4-s3 (52B) [max_train_instances: 0]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0",
        "NaturalQuestions (open-book) - truncated":"0.007",
        "NaturalQuestions (open-book) - # prompt tokens":"355.297",
        "NaturalQuestions (open-book) - # output tokens":"32.787",
        "NaturalQuestions (open-book) - # trials":"1",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"0",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"583.586",
        "CNN\/DailyMail - # output tokens":"87.906",
        "CNN\/DailyMail - # trials":"1",
        "IMDB - # eval":"1000",
        "IMDB - # train":"0",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"293.212",
        "IMDB - # output tokens":"2.28",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"0",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"87.818",
        "CivilComments - # output tokens":"4.996",
        "CivilComments - # trials":"1"
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B) [max_train_instances: 1]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0.993",
        "NaturalQuestions (open-book) - truncated":"0.007",
        "NaturalQuestions (open-book) - # prompt tokens":"655.183",
        "NaturalQuestions (open-book) - # output tokens":"13.132",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"1",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"716.586",
        "CNN\/DailyMail - # output tokens":"49.639",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"1",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"533.545",
        "IMDB - # output tokens":"1.01",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"1",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"225.818",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B) [max_train_instances: 16]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"13.451",
        "NaturalQuestions (open-book) - truncated":"0.007",
        "NaturalQuestions (open-book) - # prompt tokens":"7554.375",
        "NaturalQuestions (open-book) - # output tokens":"5.146",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"16",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"4028.253",
        "CNN\/DailyMail - # output tokens":"56.049",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"16",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"4633.545",
        "IMDB - # output tokens":"1.001",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"16",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"1351.151",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B) [max_train_instances: 2]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"1.986",
        "NaturalQuestions (open-book) - truncated":"0.007",
        "NaturalQuestions (open-book) - # prompt tokens":"805.788",
        "NaturalQuestions (open-book) - # output tokens":"8.498",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"2",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"954.253",
        "CNN\/DailyMail - # output tokens":"68.366",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"2",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"738.879",
        "IMDB - # output tokens":"1.013",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"2",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"296.485",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B) [max_train_instances: 4]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"3.972",
        "NaturalQuestions (open-book) - truncated":"0.007",
        "NaturalQuestions (open-book) - # prompt tokens":"1349.29",
        "NaturalQuestions (open-book) - # output tokens":"6.054",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"4",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1359.253",
        "CNN\/DailyMail - # output tokens":"66.863",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"4",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1098.879",
        "IMDB - # output tokens":"1.012",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"4",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"417.818",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B) [max_train_instances: 8]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"7.932",
        "NaturalQuestions (open-book) - truncated":"0.007",
        "NaturalQuestions (open-book) - # prompt tokens":"2425.501",
        "NaturalQuestions (open-book) - # output tokens":"5.064",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"8",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"2254.586",
        "CNN\/DailyMail - # output tokens":"58.062",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"8",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"2255.879",
        "IMDB - # output tokens":"1.004",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"8",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"741.485",
        "CivilComments - # output tokens":"1",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"BLOOM (176B) [max_train_instances: 0]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0",
        "NaturalQuestions (open-book) - truncated":"0.035",
        "NaturalQuestions (open-book) - # prompt tokens":"241.206",
        "NaturalQuestions (open-book) - # output tokens":"283.841",
        "NaturalQuestions (open-book) - # trials":"1",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"0",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"587.33",
        "CNN\/DailyMail - # output tokens":"128",
        "CNN\/DailyMail - # trials":"1",
        "IMDB - # eval":"1000",
        "IMDB - # train":"0",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"292.598",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"0",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"83.76",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"1"
    },
    {
        "Model":"BLOOM (176B) [max_train_instances: 1]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0.961",
        "NaturalQuestions (open-book) - truncated":"0.035",
        "NaturalQuestions (open-book) - # prompt tokens":"494.511",
        "NaturalQuestions (open-book) - # output tokens":"77.601",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"1",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"713.33",
        "CNN\/DailyMail - # output tokens":"78.786",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"1",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"527.265",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"1",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"209.093",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"BLOOM (176B) [max_train_instances: 16]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"5.694",
        "NaturalQuestions (open-book) - truncated":"0.035",
        "NaturalQuestions (open-book) - # prompt tokens":"1527.264",
        "NaturalQuestions (open-book) - # output tokens":"28.895",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"6.215",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1811.542",
        "CNN\/DailyMail - # output tokens":"118.494",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"6.898",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1814.765",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"16",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"1264.093",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"BLOOM (176B) [max_train_instances: 2]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"1.919",
        "NaturalQuestions (open-book) - truncated":"0.035",
        "NaturalQuestions (open-book) - # prompt tokens":"637.888",
        "NaturalQuestions (open-book) - # output tokens":"44.452",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"2",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"949.664",
        "CNN\/DailyMail - # output tokens":"115.041",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"2",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"728.512",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"2",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"277.427",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"BLOOM (176B) [max_train_instances: 4]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"3.811",
        "NaturalQuestions (open-book) - truncated":"0.035",
        "NaturalQuestions (open-book) - # prompt tokens":"1109.541",
        "NaturalQuestions (open-book) - # output tokens":"34.282",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"4",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1353.33",
        "CNN\/DailyMail - # output tokens":"121.986",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"3.989",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1080.602",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"4",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"390.093",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"BLOOM (176B) [max_train_instances: 8]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"5.694",
        "NaturalQuestions (open-book) - truncated":"0.035",
        "NaturalQuestions (open-book) - # prompt tokens":"1527.264",
        "NaturalQuestions (open-book) - # output tokens":"28.895",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"6.214",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1811.42",
        "CNN\/DailyMail - # output tokens":"118.494",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"6.611",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1750.508",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"8",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"700.093",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"T0pp (11B) [max_train_instances: 0]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0",
        "NaturalQuestions (open-book) - truncated":"0.932",
        "NaturalQuestions (open-book) - # prompt tokens":"221.802",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"1",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"0",
        "CNN\/DailyMail - truncated":"0.976",
        "CNN\/DailyMail - # prompt tokens":"658.182",
        "CNN\/DailyMail - # output tokens":"128",
        "CNN\/DailyMail - # trials":"1",
        "IMDB - # eval":"1000",
        "IMDB - # train":"0",
        "IMDB - truncated":"0.03",
        "IMDB - # prompt tokens":"333.557",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"0",
        "CivilComments - truncated":"0.538",
        "CivilComments - # prompt tokens":"94.075",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"1"
    },
    {
        "Model":"T0pp (11B) [max_train_instances: 1]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0.942",
        "NaturalQuestions (open-book) - truncated":"0.057",
        "NaturalQuestions (open-book) - # prompt tokens":"442.787",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"0.996",
        "CNN\/DailyMail - truncated":"0.004",
        "CNN\/DailyMail - # prompt tokens":"801.564",
        "CNN\/DailyMail - # output tokens":"128",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"0.926",
        "IMDB - truncated":"0.03",
        "IMDB - # prompt tokens":"584.345",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"1",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"236.742",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"T0pp (11B) [max_train_instances: 16]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"3.396",
        "NaturalQuestions (open-book) - truncated":"0.057",
        "NaturalQuestions (open-book) - # prompt tokens":"903.877",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"1.335",
        "CNN\/DailyMail - truncated":"0.004",
        "CNN\/DailyMail - # prompt tokens":"886.838",
        "CNN\/DailyMail - # output tokens":"128",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"2.44",
        "IMDB - truncated":"0.03",
        "IMDB - # prompt tokens":"910.174",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"9.945",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"976.032",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"T0pp (11B) [max_train_instances: 2]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"1.876",
        "NaturalQuestions (open-book) - truncated":"0.057",
        "NaturalQuestions (open-book) - # prompt tokens":"588.397",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"1.261",
        "CNN\/DailyMail - truncated":"0.004",
        "CNN\/DailyMail - # prompt tokens":"871.951",
        "CNN\/DailyMail - # output tokens":"128",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"1.746",
        "IMDB - truncated":"0.03",
        "IMDB - # prompt tokens":"770.124",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"2",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"312.075",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"T0pp (11B) [max_train_instances: 4]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"3.381",
        "NaturalQuestions (open-book) - truncated":"0.057",
        "NaturalQuestions (open-book) - # prompt tokens":"902.003",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"1.335",
        "CNN\/DailyMail - truncated":"0.004",
        "CNN\/DailyMail - # prompt tokens":"886.838",
        "CNN\/DailyMail - # output tokens":"128",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"2.44",
        "IMDB - truncated":"0.03",
        "IMDB - # prompt tokens":"910.041",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"4",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"444.742",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"T0pp (11B) [max_train_instances: 8]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"3.396",
        "NaturalQuestions (open-book) - truncated":"0.057",
        "NaturalQuestions (open-book) - # prompt tokens":"903.877",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"1.335",
        "CNN\/DailyMail - truncated":"0.004",
        "CNN\/DailyMail - # prompt tokens":"886.838",
        "CNN\/DailyMail - # output tokens":"128",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"2.44",
        "IMDB - truncated":"0.03",
        "IMDB - # prompt tokens":"910.174",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"7.944",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"789.972",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"GPT-J (6B) [max_train_instances: 0]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"253.473",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"1",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"0",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"583.586",
        "CNN\/DailyMail - # output tokens":"119.341",
        "CNN\/DailyMail - # trials":"1",
        "IMDB - # eval":"1000",
        "IMDB - # train":"0",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"293.212",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"0",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"87.818",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"1"
    },
    {
        "Model":"GPT-J (6B) [max_train_instances: 1]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0.957",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"541.35",
        "NaturalQuestions (open-book) - # output tokens":"115.307",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"1",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"716.586",
        "CNN\/DailyMail - # output tokens":"69.491",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"1",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"533.545",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"1",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"225.818",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"GPT-J (6B) [max_train_instances: 16]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"5.221",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1499.338",
        "NaturalQuestions (open-book) - # output tokens":"134.477",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"6.154",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1814.624",
        "CNN\/DailyMail - # output tokens":"83.506",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"6.722",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1735.666",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"16",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"1351.151",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"GPT-J (6B) [max_train_instances: 2]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"1.91",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"685.895",
        "NaturalQuestions (open-book) - # output tokens":"69.479",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"2",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"954.253",
        "CNN\/DailyMail - # output tokens":"92.393",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"1.999",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"738.637",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"2",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"296.485",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"GPT-J (6B) [max_train_instances: 4]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"3.786",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1198.879",
        "NaturalQuestions (open-book) - # output tokens":"68.329",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"4",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1359.253",
        "CNN\/DailyMail - # output tokens":"89.29",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"3.987",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1096.399",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"4",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"417.818",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"GPT-J (6B) [max_train_instances: 8]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"5.221",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1499.338",
        "NaturalQuestions (open-book) - # output tokens":"134.477",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"6.154",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1814.624",
        "CNN\/DailyMail - # output tokens":"83.506",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"6.453",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1676.282",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"8",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"741.485",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"GPT-NeoX (20B) [max_train_instances: 0]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0",
        "NaturalQuestions (open-book) - truncated":"0.038",
        "NaturalQuestions (open-book) - # prompt tokens":"254.57",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"1",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"0",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"594.275",
        "CNN\/DailyMail - # output tokens":"123.938",
        "CNN\/DailyMail - # trials":"1",
        "IMDB - # eval":"1000",
        "IMDB - # train":"0",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"296.586",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"0",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"89.008",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"1"
    },
    {
        "Model":"GPT-NeoX (20B) [max_train_instances: 1]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0.957",
        "NaturalQuestions (open-book) - truncated":"0.037",
        "NaturalQuestions (open-book) - # prompt tokens":"527.873",
        "NaturalQuestions (open-book) - # output tokens":"89.733",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"1",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"727.608",
        "CNN\/DailyMail - # output tokens":"51.195",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"1",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"539.253",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"1",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"225.675",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"GPT-NeoX (20B) [max_train_instances: 16]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"5.263",
        "NaturalQuestions (open-book) - truncated":"0.037",
        "NaturalQuestions (open-book) - # prompt tokens":"1479.204",
        "NaturalQuestions (open-book) - # output tokens":"55.143",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5.948",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1799.69",
        "CNN\/DailyMail - # output tokens":"88.318",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"6.706",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1752.35",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"16",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"1361.008",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"GPT-NeoX (20B) [max_train_instances: 2]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"1.91",
        "NaturalQuestions (open-book) - truncated":"0.037",
        "NaturalQuestions (open-book) - # prompt tokens":"674.96",
        "NaturalQuestions (open-book) - # output tokens":"67.183",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"2",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"974.275",
        "CNN\/DailyMail - # output tokens":"104.195",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"1.999",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"747.097",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"2",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"297.008",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"GPT-NeoX (20B) [max_train_instances: 4]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"3.788",
        "NaturalQuestions (open-book) - truncated":"0.037",
        "NaturalQuestions (open-book) - # prompt tokens":"1181.277",
        "NaturalQuestions (open-book) - # output tokens":"53.323",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"4",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1387.275",
        "CNN\/DailyMail - # output tokens":"84.589",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"3.986",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1105.593",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"4",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"420.008",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"GPT-NeoX (20B) [max_train_instances: 8]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"5.263",
        "NaturalQuestions (open-book) - truncated":"0.037",
        "NaturalQuestions (open-book) - # prompt tokens":"1479.204",
        "NaturalQuestions (open-book) - # output tokens":"55.143",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5.948",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1799.69",
        "CNN\/DailyMail - # output tokens":"88.318",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"6.448",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1695.428",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"8",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"750.342",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"T5 (11B) [max_train_instances: 0]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0",
        "NaturalQuestions (open-book) - truncated":"0.933",
        "NaturalQuestions (open-book) - # prompt tokens":"191.402",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"1",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"0",
        "CNN\/DailyMail - truncated":"0.996",
        "CNN\/DailyMail - # prompt tokens":"491.388",
        "CNN\/DailyMail - # output tokens":"128",
        "CNN\/DailyMail - # trials":"1",
        "IMDB - # eval":"1000",
        "IMDB - # train":"0",
        "IMDB - truncated":"0.173",
        "IMDB - # prompt tokens":"290.462",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"0",
        "CivilComments - truncated":"0.538",
        "CivilComments - # prompt tokens":"94.074",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"1"
    },
    {
        "Model":"T5 (11B) [max_train_instances: 1]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0.628",
        "NaturalQuestions (open-book) - truncated":"0.349",
        "NaturalQuestions (open-book) - # prompt tokens":"285.35",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"0.064",
        "CNN\/DailyMail - truncated":"0.932",
        "CNN\/DailyMail - # prompt tokens":"500.553",
        "CNN\/DailyMail - # output tokens":"128",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"0.434",
        "IMDB - truncated":"0.173",
        "IMDB - # prompt tokens":"402.978",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"0.995",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"235.708",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"T5 (11B) [max_train_instances: 16]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0.924",
        "NaturalQuestions (open-book) - truncated":"0.349",
        "NaturalQuestions (open-book) - # prompt tokens":"301.907",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"0.064",
        "CNN\/DailyMail - truncated":"0.932",
        "CNN\/DailyMail - # prompt tokens":"500.553",
        "CNN\/DailyMail - # output tokens":"128",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"0.466",
        "IMDB - truncated":"0.173",
        "IMDB - # prompt tokens":"408.425",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"4.512",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"455.923",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"T5 (11B) [max_train_instances: 2]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0.924",
        "NaturalQuestions (open-book) - truncated":"0.349",
        "NaturalQuestions (open-book) - # prompt tokens":"301.907",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"0.064",
        "CNN\/DailyMail - truncated":"0.932",
        "CNN\/DailyMail - # prompt tokens":"500.553",
        "CNN\/DailyMail - # output tokens":"128",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"0.466",
        "IMDB - truncated":"0.173",
        "IMDB - # prompt tokens":"408.425",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"1.951",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"306.594",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"T5 (11B) [max_train_instances: 4]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0.924",
        "NaturalQuestions (open-book) - truncated":"0.349",
        "NaturalQuestions (open-book) - # prompt tokens":"301.907",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"0.064",
        "CNN\/DailyMail - truncated":"0.932",
        "CNN\/DailyMail - # prompt tokens":"500.553",
        "CNN\/DailyMail - # output tokens":"128",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"0.466",
        "IMDB - truncated":"0.173",
        "IMDB - # prompt tokens":"408.425",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"3.615",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"414.632",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"T5 (11B) [max_train_instances: 8]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0.924",
        "NaturalQuestions (open-book) - truncated":"0.349",
        "NaturalQuestions (open-book) - # prompt tokens":"301.907",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"0.064",
        "CNN\/DailyMail - truncated":"0.932",
        "CNN\/DailyMail - # prompt tokens":"500.553",
        "CNN\/DailyMail - # output tokens":"128",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"0.466",
        "IMDB - truncated":"0.173",
        "IMDB - # prompt tokens":"408.425",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"4.512",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"455.923",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"UL2 (20B) [max_train_instances: 0]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0",
        "NaturalQuestions (open-book) - truncated":"0.933",
        "NaturalQuestions (open-book) - # prompt tokens":"195.114",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"1",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"0",
        "CNN\/DailyMail - truncated":"0.996",
        "CNN\/DailyMail - # prompt tokens":"492.075",
        "CNN\/DailyMail - # output tokens":"128",
        "CNN\/DailyMail - # trials":"1",
        "IMDB - # eval":"1000",
        "IMDB - # train":"0",
        "IMDB - truncated":"0.176",
        "IMDB - # prompt tokens":"293.7",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"0",
        "CivilComments - truncated":"0.538",
        "CivilComments - # prompt tokens":"98.074",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"1"
    },
    {
        "Model":"UL2 (20B) [max_train_instances: 1]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0.623",
        "NaturalQuestions (open-book) - truncated":"0.355",
        "NaturalQuestions (open-book) - # prompt tokens":"287.062",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"0.061",
        "CNN\/DailyMail - truncated":"0.935",
        "CNN\/DailyMail - # prompt tokens":"500.829",
        "CNN\/DailyMail - # output tokens":"128",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"0.42",
        "IMDB - truncated":"0.176",
        "IMDB - # prompt tokens":"402.11",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"0.995",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"239.708",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"UL2 (20B) [max_train_instances: 16]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0.918",
        "NaturalQuestions (open-book) - truncated":"0.355",
        "NaturalQuestions (open-book) - # prompt tokens":"303.619",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"0.061",
        "CNN\/DailyMail - truncated":"0.935",
        "CNN\/DailyMail - # prompt tokens":"500.829",
        "CNN\/DailyMail - # output tokens":"128",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"0.449",
        "IMDB - truncated":"0.176",
        "IMDB - # prompt tokens":"407.098",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"4.456",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"456.06",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"UL2 (20B) [max_train_instances: 2]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0.918",
        "NaturalQuestions (open-book) - truncated":"0.355",
        "NaturalQuestions (open-book) - # prompt tokens":"303.619",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"0.061",
        "CNN\/DailyMail - truncated":"0.935",
        "CNN\/DailyMail - # prompt tokens":"500.829",
        "CNN\/DailyMail - # output tokens":"128",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"0.449",
        "IMDB - truncated":"0.176",
        "IMDB - # prompt tokens":"407.098",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"1.949",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"310.313",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"UL2 (20B) [max_train_instances: 4]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0.918",
        "NaturalQuestions (open-book) - truncated":"0.355",
        "NaturalQuestions (open-book) - # prompt tokens":"303.619",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"0.061",
        "CNN\/DailyMail - truncated":"0.935",
        "CNN\/DailyMail - # prompt tokens":"500.829",
        "CNN\/DailyMail - # output tokens":"128",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"0.449",
        "IMDB - truncated":"0.176",
        "IMDB - # prompt tokens":"407.098",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"3.592",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"416.999",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"UL2 (20B) [max_train_instances: 8]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0.918",
        "NaturalQuestions (open-book) - truncated":"0.355",
        "NaturalQuestions (open-book) - # prompt tokens":"303.619",
        "NaturalQuestions (open-book) - # output tokens":"300",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"0.061",
        "CNN\/DailyMail - truncated":"0.935",
        "CNN\/DailyMail - # prompt tokens":"500.829",
        "CNN\/DailyMail - # output tokens":"128",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"0.449",
        "IMDB - truncated":"0.176",
        "IMDB - # prompt tokens":"407.098",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"4.456",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"456.06",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"OPT (175B) [max_train_instances: 0]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"253.473",
        "NaturalQuestions (open-book) - # output tokens":"228.883",
        "NaturalQuestions (open-book) - # trials":"1",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"0",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"583.586",
        "CNN\/DailyMail - # output tokens":"127.483",
        "CNN\/DailyMail - # trials":"1",
        "IMDB - # eval":"1000",
        "IMDB - # train":"0",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"293.212",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"0",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"87.818",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"1"
    },
    {
        "Model":"OPT (175B) [max_train_instances: 1]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0.957",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"541.35",
        "NaturalQuestions (open-book) - # output tokens":"79.382",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"1",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"716.586",
        "CNN\/DailyMail - # output tokens":"50.056",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"1",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"533.545",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"1",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"225.818",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"OPT (175B) [max_train_instances: 16]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"5.221",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1499.338",
        "NaturalQuestions (open-book) - # output tokens":"97.343",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"6.154",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1814.624",
        "CNN\/DailyMail - # output tokens":"76.25",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"6.722",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1735.666",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"16",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"1351.151",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"OPT (175B) [max_train_instances: 2]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"1.91",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"685.895",
        "NaturalQuestions (open-book) - # output tokens":"46.888",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"2",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"954.253",
        "CNN\/DailyMail - # output tokens":"88.043",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"1.999",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"738.637",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"2",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"296.485",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"OPT (175B) [max_train_instances: 4]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"3.786",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1198.879",
        "NaturalQuestions (open-book) - # output tokens":"50.507",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"4",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1359.253",
        "CNN\/DailyMail - # output tokens":"83.678",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"3.987",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1096.399",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"4",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"417.818",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"OPT (175B) [max_train_instances: 8]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"5.221",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1499.338",
        "NaturalQuestions (open-book) - # output tokens":"97.343",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"6.154",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1814.624",
        "CNN\/DailyMail - # output tokens":"76.25",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"6.453",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1676.282",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"8",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"741.485",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"OPT (66B) [max_train_instances: 0]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"253.473",
        "NaturalQuestions (open-book) - # output tokens":"292.529",
        "NaturalQuestions (open-book) - # trials":"1",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"0",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"583.586",
        "CNN\/DailyMail - # output tokens":"128",
        "CNN\/DailyMail - # trials":"1",
        "IMDB - # eval":"1000",
        "IMDB - # train":"0",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"293.212",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"0",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"87.818",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"1"
    },
    {
        "Model":"OPT (66B) [max_train_instances: 1]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0.957",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"541.35",
        "NaturalQuestions (open-book) - # output tokens":"128.598",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"1",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"716.586",
        "CNN\/DailyMail - # output tokens":"50.81",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"1",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"533.545",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"1",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"225.818",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"OPT (66B) [max_train_instances: 16]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"5.221",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1499.338",
        "NaturalQuestions (open-book) - # output tokens":"124.122",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"6.154",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1814.624",
        "CNN\/DailyMail - # output tokens":"77.426",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"6.722",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1735.666",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"16",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"1351.151",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"OPT (66B) [max_train_instances: 2]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"1.91",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"685.895",
        "NaturalQuestions (open-book) - # output tokens":"104.238",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"2",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"954.253",
        "CNN\/DailyMail - # output tokens":"86.29",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"1.999",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"738.637",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"2",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"296.485",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"OPT (66B) [max_train_instances: 4]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"3.786",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1198.879",
        "NaturalQuestions (open-book) - # output tokens":"104.394",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"4",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1359.253",
        "CNN\/DailyMail - # output tokens":"82.17",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"3.987",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1096.399",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"4",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"417.818",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"OPT (66B) [max_train_instances: 8]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"5.221",
        "NaturalQuestions (open-book) - truncated":"0.036",
        "NaturalQuestions (open-book) - # prompt tokens":"1499.338",
        "NaturalQuestions (open-book) - # output tokens":"124.122",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"6.154",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1814.624",
        "CNN\/DailyMail - # output tokens":"77.426",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"6.453",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1676.282",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"8",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"741.485",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"GLM (130B) [max_train_instances: 0]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0",
        "NaturalQuestions (open-book) - truncated":"0.975",
        "NaturalQuestions (open-book) - # prompt tokens":"256.204",
        "NaturalQuestions (open-book) - # output tokens":"298.333",
        "NaturalQuestions (open-book) - # trials":"1",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"0",
        "CNN\/DailyMail - truncated":"0.989",
        "CNN\/DailyMail - # prompt tokens":"606.124",
        "CNN\/DailyMail - # output tokens":"25.856",
        "CNN\/DailyMail - # trials":"1",
        "IMDB - # eval":"1000",
        "IMDB - # train":"0",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"298.707",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"0",
        "CivilComments - truncated":"0.557",
        "CivilComments - # prompt tokens":"84.41",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"1"
    },
    {
        "Model":"GLM (130B) [max_train_instances: 1]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0.953",
        "NaturalQuestions (open-book) - truncated":"0.047",
        "NaturalQuestions (open-book) - # prompt tokens":"599.983",
        "NaturalQuestions (open-book) - # output tokens":"23.94",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"1",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"752.791",
        "CNN\/DailyMail - # output tokens":"39.17",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"1",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"547.041",
        "IMDB - # output tokens":"2.008",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"1",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"220.744",
        "CivilComments - # output tokens":"2",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"GLM (130B) [max_train_instances: 16]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.959",
        "NaturalQuestions (open-book) - truncated":"0.047",
        "NaturalQuestions (open-book) - # prompt tokens":"1554.457",
        "NaturalQuestions (open-book) - # output tokens":"20.94",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5.544",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1783.559",
        "CNN\/DailyMail - # output tokens":"83.259",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"6.678",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1757.791",
        "IMDB - # output tokens":"2",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"16",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"1302.411",
        "CivilComments - # output tokens":"2",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"GLM (130B) [max_train_instances: 2]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"1.903",
        "NaturalQuestions (open-book) - truncated":"0.047",
        "NaturalQuestions (open-book) - # prompt tokens":"760.041",
        "NaturalQuestions (open-book) - # output tokens":"22.657",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"2",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1011.124",
        "CNN\/DailyMail - # output tokens":"78.232",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"1.999",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"757.369",
        "IMDB - # output tokens":"2.001",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"2",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"288.744",
        "CivilComments - # output tokens":"2",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"GLM (130B) [max_train_instances: 4]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"3.76",
        "NaturalQuestions (open-book) - truncated":"0.047",
        "NaturalQuestions (open-book) - # prompt tokens":"1269.731",
        "NaturalQuestions (open-book) - # output tokens":"21.146",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"4",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1438.124",
        "CNN\/DailyMail - # output tokens":"86.906",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"3.984",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1119.52",
        "IMDB - # output tokens":"2",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"4",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"405.411",
        "CivilComments - # output tokens":"2",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"GLM (130B) [max_train_instances: 8]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"4.959",
        "NaturalQuestions (open-book) - truncated":"0.047",
        "NaturalQuestions (open-book) - # prompt tokens":"1554.457",
        "NaturalQuestions (open-book) - # output tokens":"20.94",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"5.544",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1783.559",
        "CNN\/DailyMail - # output tokens":"83.259",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"6.424",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1702.337",
        "IMDB - # output tokens":"2",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"8",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"722.411",
        "CivilComments - # output tokens":"2",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"YaLM (100B) [max_train_instances: 0]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0",
        "NaturalQuestions (open-book) - truncated":"0.038",
        "NaturalQuestions (open-book) - # prompt tokens":"249.995",
        "NaturalQuestions (open-book) - # output tokens":"250.44",
        "NaturalQuestions (open-book) - # trials":"1",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"0",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"583.099",
        "CNN\/DailyMail - # output tokens":"127.73",
        "CNN\/DailyMail - # trials":"1",
        "IMDB - # eval":"1000",
        "IMDB - # train":"0",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"296.124",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"1",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"0",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"89.019",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"1"
    },
    {
        "Model":"YaLM (100B) [max_train_instances: 1]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"0.96",
        "NaturalQuestions (open-book) - truncated":"0.038",
        "NaturalQuestions (open-book) - # prompt tokens":"524.392",
        "NaturalQuestions (open-book) - # output tokens":"295.798",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"1",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"718.099",
        "CNN\/DailyMail - # output tokens":"127.876",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"1",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"542.791",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"1",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"224.352",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"YaLM (100B) [max_train_instances: 16]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"5.221",
        "NaturalQuestions (open-book) - truncated":"0.038",
        "NaturalQuestions (open-book) - # prompt tokens":"1487.936",
        "NaturalQuestions (open-book) - # output tokens":"293.034",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"6.189",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1812.857",
        "CNN\/DailyMail - # output tokens":"107.569",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"6.698",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1753.626",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"16",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"1365.019",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"YaLM (100B) [max_train_instances: 2]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"1.915",
        "NaturalQuestions (open-book) - truncated":"0.038",
        "NaturalQuestions (open-book) - # prompt tokens":"662.316",
        "NaturalQuestions (open-book) - # output tokens":"295.798",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"2",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"956.432",
        "CNN\/DailyMail - # output tokens":"125.571",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"1.999",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"751.967",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"2",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"296.019",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"YaLM (100B) [max_train_instances: 4]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"3.796",
        "NaturalQuestions (open-book) - truncated":"0.038",
        "NaturalQuestions (open-book) - # prompt tokens":"1190.225",
        "NaturalQuestions (open-book) - # output tokens":"295.436",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"4",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1359.432",
        "CNN\/DailyMail - # output tokens":"118.239",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"3.985",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1110.301",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"4",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"418.352",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    },
    {
        "Model":"YaLM (100B) [max_train_instances: 8]",
        "Mean win rate":"-",
        "NaturalQuestions (open-book) - # eval":"1000",
        "NaturalQuestions (open-book) - # train":"5.221",
        "NaturalQuestions (open-book) - truncated":"0.038",
        "NaturalQuestions (open-book) - # prompt tokens":"1487.936",
        "NaturalQuestions (open-book) - # output tokens":"293.034",
        "NaturalQuestions (open-book) - # trials":"3",
        "CNN\/DailyMail - # eval":"466",
        "CNN\/DailyMail - # train":"6.189",
        "CNN\/DailyMail - truncated":"0",
        "CNN\/DailyMail - # prompt tokens":"1812.857",
        "CNN\/DailyMail - # output tokens":"107.569",
        "CNN\/DailyMail - # trials":"3",
        "IMDB - # eval":"1000",
        "IMDB - # train":"6.443",
        "IMDB - truncated":"0",
        "IMDB - # prompt tokens":"1697.155",
        "IMDB - # output tokens":"5",
        "IMDB - # trials":"3",
        "CivilComments - # eval":"382.5",
        "CivilComments - # train":"8",
        "CivilComments - truncated":"0",
        "CivilComments - # prompt tokens":"748.352",
        "CivilComments - # output tokens":"5",
        "CivilComments - # trials":"3"
    }
]