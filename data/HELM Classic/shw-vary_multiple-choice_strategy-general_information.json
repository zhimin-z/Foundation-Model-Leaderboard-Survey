[
    {
        "Model":"Anthropic-LM v4-s3 (52B) [method: multiple_choice_joint]",
        "Mean win rate":"-",
        "HellaSwag - # eval":1000,
        "HellaSwag - # train":5,
        "HellaSwag - truncated":0,
        "HellaSwag - # prompt tokens":614.348,
        "HellaSwag - # output tokens":1.0,
        "HellaSwag - # trials":3,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":248.21,
        "OpenbookQA - # output tokens":1.0,
        "OpenbookQA - # trials":3,
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":511.121,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BLiMP - # eval":1000,
        "BLiMP - # train":0,
        "BLiMP - truncated":0,
        "BLiMP - # prompt tokens":33.936,
        "BLiMP - # output tokens":1.0,
        "BLiMP - # trials":3,
        "LegalSupport - # eval":489,
        "LegalSupport - # train":3,
        "LegalSupport - truncated":0,
        "LegalSupport - # prompt tokens":605.753,
        "LegalSupport - # output tokens":1.0,
        "LegalSupport - # trials":3,
        "LSAT - # eval":230,
        "LSAT - # train":5,
        "LSAT - truncated":0,
        "LSAT - # prompt tokens":1190.129,
        "LSAT - # output tokens":1.0,
        "LSAT - # trials":3,
        "BBQ - # eval":1000,
        "BBQ - # train":5,
        "BBQ - truncated":0,
        "BBQ - # prompt tokens":424.437,
        "BBQ - # output tokens":1.0,
        "BBQ - # trials":3
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B) [method: multiple_choice_separate_calibrated]",
        "Mean win rate":"-",
        "HellaSwag - # eval":1000,
        "HellaSwag - # train":0,
        "HellaSwag - truncated":0,
        "HellaSwag - # prompt tokens":33.273,
        "HellaSwag - # output tokens":0.499,
        "HellaSwag - # trials":1,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":0,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":5.27,
        "OpenbookQA - # output tokens":0.132,
        "OpenbookQA - # trials":1,
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":12.908,
        "TruthfulQA - # output tokens":0.327,
        "TruthfulQA - # trials":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":9.217,
        "MMLU - # output tokens":0.094,
        "MMLU - # trials":1,
        "BLiMP - # eval":1000,
        "BLiMP - # train":0,
        "BLiMP - truncated":0,
        "BLiMP - # prompt tokens":10.608,
        "BLiMP - # output tokens":0.383,
        "BLiMP - # trials":1,
        "LegalSupport - # eval":489,
        "LegalSupport - # train":0,
        "LegalSupport - truncated":0,
        "LegalSupport - # prompt tokens":32.62,
        "LegalSupport - # output tokens":0.509,
        "LegalSupport - # trials":1,
        "LSAT - # eval":230,
        "LSAT - # train":0,
        "LSAT - truncated":0,
        "LSAT - # prompt tokens":11.717,
        "LSAT - # output tokens":0.37,
        "LSAT - # trials":1,
        "BBQ - # eval":1000,
        "BBQ - # train":0,
        "BBQ - truncated":0,
        "BBQ - # prompt tokens":5.405,
        "BBQ - # output tokens":0.161,
        "BBQ - # trials":1
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B) [method: multiple_choice_separate_original]",
        "Mean win rate":"-",
        "HellaSwag - # eval":1000,
        "HellaSwag - # train":0,
        "HellaSwag - truncated":0,
        "HellaSwag - # prompt tokens":87.888,
        "HellaSwag - # output tokens":1.306,
        "HellaSwag - # trials":1,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":0,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":15.024,
        "OpenbookQA - # output tokens":0.322,
        "OpenbookQA - # trials":1,
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":24.203,
        "TruthfulQA - # output tokens":0.589,
        "TruthfulQA - # trials":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":38.647,
        "MMLU - # output tokens":0.192,
        "MMLU - # trials":1,
        "BLiMP - # eval":1000,
        "BLiMP - # train":0,
        "BLiMP - truncated":0,
        "BLiMP - # prompt tokens":8.608,
        "BLiMP - # output tokens":0.383,
        "BLiMP - # trials":1,
        "LegalSupport - # eval":489,
        "LegalSupport - # train":0,
        "LegalSupport - truncated":0,
        "LegalSupport - # prompt tokens":92.671,
        "LegalSupport - # output tokens":1.374,
        "LegalSupport - # trials":1,
        "LSAT - # eval":230,
        "LSAT - # train":0,
        "LSAT - truncated":0,
        "LSAT - # prompt tokens":165.309,
        "LSAT - # output tokens":2.67,
        "LSAT - # trials":1,
        "BBQ - # eval":1000,
        "BBQ - # train":0,
        "BBQ - truncated":0,
        "BBQ - # prompt tokens":57.788,
        "BBQ - # output tokens":1.323,
        "BBQ - # trials":1
    },
    {
        "Model":"BLOOM (176B) [method: multiple_choice_joint]",
        "Mean win rate":"-",
        "HellaSwag - # eval":1000,
        "HellaSwag - # train":5,
        "HellaSwag - truncated":0,
        "HellaSwag - # prompt tokens":588.5,
        "HellaSwag - # output tokens":1.0,
        "HellaSwag - # trials":3,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":231.359,
        "OpenbookQA - # output tokens":1.0,
        "OpenbookQA - # trials":3,
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":467.694,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":436.99,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BLiMP - # eval":1000,
        "BLiMP - # train":0,
        "BLiMP - truncated":0,
        "BLiMP - # prompt tokens":30.359,
        "BLiMP - # output tokens":1.0,
        "BLiMP - # trials":3,
        "LegalSupport - # eval":489,
        "LegalSupport - # train":3,
        "LegalSupport - truncated":0,
        "LegalSupport - # prompt tokens":620.172,
        "LegalSupport - # output tokens":1.0,
        "LegalSupport - # trials":3,
        "LSAT - # eval":230,
        "LSAT - # train":5,
        "LSAT - truncated":0,
        "LSAT - # prompt tokens":1156.451,
        "LSAT - # output tokens":1.0,
        "LSAT - # trials":3,
        "BBQ - # eval":1000,
        "BBQ - # train":5,
        "BBQ - truncated":0,
        "BBQ - # prompt tokens":396.628,
        "BBQ - # output tokens":1.0,
        "BBQ - # trials":3
    },
    {
        "Model":"BLOOM (176B) [method: multiple_choice_separate_calibrated]",
        "Mean win rate":"-",
        "HellaSwag - # eval":1000,
        "HellaSwag - # train":0,
        "HellaSwag - truncated":0,
        "HellaSwag - # prompt tokens":33.86,
        "HellaSwag - # output tokens":0.0,
        "HellaSwag - # trials":1,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":0,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":5.444,
        "OpenbookQA - # output tokens":0.0,
        "OpenbookQA - # trials":1,
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":13.136,
        "TruthfulQA - # output tokens":0.0,
        "TruthfulQA - # trials":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":8.973,
        "MMLU - # output tokens":0.0,
        "MMLU - # trials":1,
        "BLiMP - # eval":1000,
        "BLiMP - # train":0,
        "BLiMP - truncated":0,
        "BLiMP - # prompt tokens":10.758,
        "BLiMP - # output tokens":0.0,
        "BLiMP - # trials":1,
        "LegalSupport - # eval":489,
        "LegalSupport - # train":0,
        "LegalSupport - truncated":0,
        "LegalSupport - # prompt tokens":34.082,
        "LegalSupport - # output tokens":0.0,
        "LegalSupport - # trials":1,
        "LSAT - # eval":230,
        "LSAT - # train":0,
        "LSAT - truncated":0,
        "LSAT - # prompt tokens":11.67,
        "LSAT - # output tokens":0.0,
        "LSAT - # trials":1,
        "BBQ - # eval":1000,
        "BBQ - # train":0,
        "BBQ - truncated":0,
        "BBQ - # prompt tokens":5.288,
        "BBQ - # output tokens":0.0,
        "BBQ - # trials":1
    },
    {
        "Model":"BLOOM (176B) [method: multiple_choice_separate_original]",
        "Mean win rate":"-",
        "HellaSwag - # eval":1000,
        "HellaSwag - # train":0,
        "HellaSwag - truncated":0,
        "HellaSwag - # prompt tokens":88.875,
        "HellaSwag - # output tokens":0.0,
        "HellaSwag - # trials":1,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":0,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":15.424,
        "OpenbookQA - # output tokens":0.0,
        "OpenbookQA - # trials":1,
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":24.52,
        "TruthfulQA - # output tokens":0.0,
        "TruthfulQA - # trials":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":36.44,
        "MMLU - # output tokens":0.0,
        "MMLU - # trials":1,
        "BLiMP - # eval":1000,
        "BLiMP - # train":0,
        "BLiMP - truncated":0,
        "BLiMP - # prompt tokens":8.801,
        "BLiMP - # output tokens":0.0,
        "BLiMP - # trials":1,
        "LegalSupport - # eval":489,
        "LegalSupport - # train":0,
        "LegalSupport - truncated":0,
        "LegalSupport - # prompt tokens":96.53,
        "LegalSupport - # output tokens":0.0,
        "LegalSupport - # trials":1,
        "LSAT - # eval":230,
        "LSAT - # train":0,
        "LSAT - truncated":0,
        "LSAT - # prompt tokens":164.465,
        "LSAT - # output tokens":0.0,
        "LSAT - # trials":1,
        "BBQ - # eval":1000,
        "BBQ - # train":0,
        "BBQ - truncated":0,
        "BBQ - # prompt tokens":56.095,
        "BBQ - # output tokens":0.0,
        "BBQ - # trials":1
    },
    {
        "Model":"GPT-J (6B) [method: multiple_choice_joint]",
        "Mean win rate":"-",
        "HellaSwag - # eval":1000,
        "HellaSwag - # train":5,
        "HellaSwag - truncated":0,
        "HellaSwag - # prompt tokens":614.348,
        "HellaSwag - # output tokens":1.0,
        "HellaSwag - # trials":3,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":248.21,
        "OpenbookQA - # output tokens":1.0,
        "OpenbookQA - # trials":3,
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":511.121,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BLiMP - # eval":1000,
        "BLiMP - # train":0,
        "BLiMP - truncated":0,
        "BLiMP - # prompt tokens":33.936,
        "BLiMP - # output tokens":1.0,
        "BLiMP - # trials":3,
        "LegalSupport - # eval":489,
        "LegalSupport - # train":3,
        "LegalSupport - truncated":0,
        "LegalSupport - # prompt tokens":605.753,
        "LegalSupport - # output tokens":1.0,
        "LegalSupport - # trials":3,
        "LSAT - # eval":230,
        "LSAT - # train":5,
        "LSAT - truncated":0,
        "LSAT - # prompt tokens":1190.129,
        "LSAT - # output tokens":1.0,
        "LSAT - # trials":3,
        "BBQ - # eval":1000,
        "BBQ - # train":5,
        "BBQ - truncated":0,
        "BBQ - # prompt tokens":424.437,
        "BBQ - # output tokens":1.0,
        "BBQ - # trials":3
    },
    {
        "Model":"GPT-J (6B) [method: multiple_choice_separate_calibrated]",
        "Mean win rate":"-",
        "HellaSwag - # eval":1000,
        "HellaSwag - # train":0,
        "HellaSwag - truncated":0,
        "HellaSwag - # prompt tokens":33.273,
        "HellaSwag - # output tokens":0.0,
        "HellaSwag - # trials":1,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":0,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":5.27,
        "OpenbookQA - # output tokens":0.0,
        "OpenbookQA - # trials":1,
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":12.908,
        "TruthfulQA - # output tokens":0.0,
        "TruthfulQA - # trials":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":9.217,
        "MMLU - # output tokens":0.0,
        "MMLU - # trials":1,
        "BLiMP - # eval":1000,
        "BLiMP - # train":0,
        "BLiMP - truncated":0,
        "BLiMP - # prompt tokens":10.608,
        "BLiMP - # output tokens":0.0,
        "BLiMP - # trials":1,
        "LegalSupport - # eval":489,
        "LegalSupport - # train":0,
        "LegalSupport - truncated":0,
        "LegalSupport - # prompt tokens":32.62,
        "LegalSupport - # output tokens":0.0,
        "LegalSupport - # trials":1,
        "LSAT - # eval":230,
        "LSAT - # train":0,
        "LSAT - truncated":0,
        "LSAT - # prompt tokens":11.717,
        "LSAT - # output tokens":0.0,
        "LSAT - # trials":1,
        "BBQ - # eval":1000,
        "BBQ - # train":0,
        "BBQ - truncated":0,
        "BBQ - # prompt tokens":5.405,
        "BBQ - # output tokens":0.0,
        "BBQ - # trials":1
    },
    {
        "Model":"GPT-J (6B) [method: multiple_choice_separate_original]",
        "Mean win rate":"-",
        "HellaSwag - # eval":1000,
        "HellaSwag - # train":0,
        "HellaSwag - truncated":0,
        "HellaSwag - # prompt tokens":87.888,
        "HellaSwag - # output tokens":0.0,
        "HellaSwag - # trials":1,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":0,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":15.024,
        "OpenbookQA - # output tokens":0.0,
        "OpenbookQA - # trials":1,
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":24.203,
        "TruthfulQA - # output tokens":0.0,
        "TruthfulQA - # trials":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":38.647,
        "MMLU - # output tokens":0.0,
        "MMLU - # trials":1,
        "BLiMP - # eval":1000,
        "BLiMP - # train":0,
        "BLiMP - truncated":0,
        "BLiMP - # prompt tokens":8.608,
        "BLiMP - # output tokens":0.0,
        "BLiMP - # trials":1,
        "LegalSupport - # eval":489,
        "LegalSupport - # train":0,
        "LegalSupport - truncated":0,
        "LegalSupport - # prompt tokens":92.671,
        "LegalSupport - # output tokens":0.0,
        "LegalSupport - # trials":1,
        "LSAT - # eval":230,
        "LSAT - # train":0,
        "LSAT - truncated":0,
        "LSAT - # prompt tokens":165.309,
        "LSAT - # output tokens":0.0,
        "LSAT - # trials":1,
        "BBQ - # eval":1000,
        "BBQ - # train":0,
        "BBQ - truncated":0,
        "BBQ - # prompt tokens":57.788,
        "BBQ - # output tokens":0.0,
        "BBQ - # trials":1
    },
    {
        "Model":"GPT-NeoX (20B) [method: multiple_choice_joint]",
        "Mean win rate":"-",
        "HellaSwag - # eval":1000,
        "HellaSwag - # train":5,
        "HellaSwag - truncated":0,
        "HellaSwag - # prompt tokens":620.786,
        "HellaSwag - # output tokens":1.0,
        "HellaSwag - # trials":3,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":248.556,
        "OpenbookQA - # output tokens":1.0,
        "OpenbookQA - # trials":3,
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":512.685,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":467.936,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BLiMP - # eval":1000,
        "BLiMP - # train":0,
        "BLiMP - truncated":0,
        "BLiMP - # prompt tokens":34.43,
        "BLiMP - # output tokens":1.0,
        "BLiMP - # trials":3,
        "LegalSupport - # eval":489,
        "LegalSupport - # train":3,
        "LegalSupport - truncated":0,
        "LegalSupport - # prompt tokens":610.996,
        "LegalSupport - # output tokens":1.0,
        "LegalSupport - # trials":3,
        "LSAT - # eval":230,
        "LSAT - # train":5,
        "LSAT - truncated":0,
        "LSAT - # prompt tokens":1202.164,
        "LSAT - # output tokens":1.0,
        "LSAT - # trials":3,
        "BBQ - # eval":1000,
        "BBQ - # train":5,
        "BBQ - truncated":0,
        "BBQ - # prompt tokens":426.123,
        "BBQ - # output tokens":1.0,
        "BBQ - # trials":3
    },
    {
        "Model":"GPT-NeoX (20B) [method: multiple_choice_separate_calibrated]",
        "Mean win rate":"-",
        "HellaSwag - # eval":1000,
        "HellaSwag - # train":0,
        "HellaSwag - truncated":0,
        "HellaSwag - # prompt tokens":33.589,
        "HellaSwag - # output tokens":0.0,
        "HellaSwag - # trials":1,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":0,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":5.346,
        "OpenbookQA - # output tokens":0.0,
        "OpenbookQA - # trials":1,
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":13.107,
        "TruthfulQA - # output tokens":0.0,
        "TruthfulQA - # trials":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":9.164,
        "MMLU - # output tokens":0.0,
        "MMLU - # trials":1,
        "BLiMP - # eval":1000,
        "BLiMP - # train":0,
        "BLiMP - truncated":0,
        "BLiMP - # prompt tokens":10.859,
        "BLiMP - # output tokens":0.0,
        "BLiMP - # trials":1,
        "LegalSupport - # eval":489,
        "LegalSupport - # train":0,
        "LegalSupport - truncated":0,
        "LegalSupport - # prompt tokens":32.45,
        "LegalSupport - # output tokens":0.0,
        "LegalSupport - # trials":1,
        "LSAT - # eval":230,
        "LSAT - # train":0,
        "LSAT - truncated":0,
        "LSAT - # prompt tokens":11.961,
        "LSAT - # output tokens":0.0,
        "LSAT - # trials":1,
        "BBQ - # eval":1000,
        "BBQ - # train":0,
        "BBQ - truncated":0,
        "BBQ - # prompt tokens":5.484,
        "BBQ - # output tokens":0.0,
        "BBQ - # trials":1
    },
    {
        "Model":"GPT-NeoX (20B) [method: multiple_choice_separate_original]",
        "Mean win rate":"-",
        "HellaSwag - # eval":1000,
        "HellaSwag - # train":0,
        "HellaSwag - truncated":0,
        "HellaSwag - # prompt tokens":88.806,
        "HellaSwag - # output tokens":0.0,
        "HellaSwag - # trials":1,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":0,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":15.19,
        "OpenbookQA - # output tokens":0.0,
        "OpenbookQA - # trials":1,
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":24.563,
        "TruthfulQA - # output tokens":0.0,
        "TruthfulQA - # trials":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":37.609,
        "MMLU - # output tokens":0.0,
        "MMLU - # trials":1,
        "BLiMP - # eval":1000,
        "BLiMP - # train":0,
        "BLiMP - truncated":0,
        "BLiMP - # prompt tokens":8.859,
        "BLiMP - # output tokens":0.0,
        "BLiMP - # trials":1,
        "LegalSupport - # eval":489,
        "LegalSupport - # train":0,
        "LegalSupport - truncated":0,
        "LegalSupport - # prompt tokens":92.348,
        "LegalSupport - # output tokens":0.0,
        "LegalSupport - # trials":1,
        "LSAT - # eval":230,
        "LSAT - # train":0,
        "LSAT - truncated":0,
        "LSAT - # prompt tokens":167.439,
        "LSAT - # output tokens":0.0,
        "LSAT - # trials":1,
        "BBQ - # eval":1000,
        "BBQ - # train":0,
        "BBQ - truncated":0,
        "BBQ - # prompt tokens":58.624,
        "BBQ - # output tokens":0.0,
        "BBQ - # trials":1
    },
    {
        "Model":"OPT (175B) [method: multiple_choice_joint]",
        "Mean win rate":"-",
        "HellaSwag - # eval":1000,
        "HellaSwag - # train":5,
        "HellaSwag - truncated":0,
        "HellaSwag - # prompt tokens":614.348,
        "HellaSwag - # output tokens":1.0,
        "HellaSwag - # trials":3,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":248.21,
        "OpenbookQA - # output tokens":1.0,
        "OpenbookQA - # trials":3,
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":511.121,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BLiMP - # eval":1000,
        "BLiMP - # train":0,
        "BLiMP - truncated":0,
        "BLiMP - # prompt tokens":33.936,
        "BLiMP - # output tokens":1.0,
        "BLiMP - # trials":3,
        "LegalSupport - # eval":489,
        "LegalSupport - # train":3,
        "LegalSupport - truncated":0,
        "LegalSupport - # prompt tokens":605.753,
        "LegalSupport - # output tokens":1.0,
        "LegalSupport - # trials":3,
        "LSAT - # eval":230,
        "LSAT - # train":5,
        "LSAT - truncated":0,
        "LSAT - # prompt tokens":1190.129,
        "LSAT - # output tokens":1.0,
        "LSAT - # trials":3,
        "BBQ - # eval":1000,
        "BBQ - # train":5,
        "BBQ - truncated":0,
        "BBQ - # prompt tokens":424.437,
        "BBQ - # output tokens":1.0,
        "BBQ - # trials":3
    },
    {
        "Model":"OPT (175B) [method: multiple_choice_separate_calibrated]",
        "Mean win rate":"-",
        "HellaSwag - # eval":1000,
        "HellaSwag - # train":0,
        "HellaSwag - truncated":0,
        "HellaSwag - # prompt tokens":33.273,
        "HellaSwag - # output tokens":0.0,
        "HellaSwag - # trials":1,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":0,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":5.27,
        "OpenbookQA - # output tokens":0.0,
        "OpenbookQA - # trials":1,
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":12.908,
        "TruthfulQA - # output tokens":0.0,
        "TruthfulQA - # trials":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":9.217,
        "MMLU - # output tokens":0.0,
        "MMLU - # trials":1,
        "BLiMP - # eval":1000,
        "BLiMP - # train":0,
        "BLiMP - truncated":0,
        "BLiMP - # prompt tokens":10.608,
        "BLiMP - # output tokens":0.0,
        "BLiMP - # trials":1,
        "LegalSupport - # eval":489,
        "LegalSupport - # train":0,
        "LegalSupport - truncated":0,
        "LegalSupport - # prompt tokens":32.62,
        "LegalSupport - # output tokens":0.0,
        "LegalSupport - # trials":1,
        "LSAT - # eval":230,
        "LSAT - # train":0,
        "LSAT - truncated":0,
        "LSAT - # prompt tokens":11.717,
        "LSAT - # output tokens":0.0,
        "LSAT - # trials":1,
        "BBQ - # eval":1000,
        "BBQ - # train":0,
        "BBQ - truncated":0,
        "BBQ - # prompt tokens":5.405,
        "BBQ - # output tokens":0.0,
        "BBQ - # trials":1
    },
    {
        "Model":"OPT (175B) [method: multiple_choice_separate_original]",
        "Mean win rate":"-",
        "HellaSwag - # eval":1000,
        "HellaSwag - # train":0,
        "HellaSwag - truncated":0,
        "HellaSwag - # prompt tokens":87.888,
        "HellaSwag - # output tokens":0.0,
        "HellaSwag - # trials":1,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":0,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":15.024,
        "OpenbookQA - # output tokens":0.0,
        "OpenbookQA - # trials":1,
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":24.203,
        "TruthfulQA - # output tokens":0.0,
        "TruthfulQA - # trials":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":38.647,
        "MMLU - # output tokens":0.0,
        "MMLU - # trials":1,
        "BLiMP - # eval":1000,
        "BLiMP - # train":0,
        "BLiMP - truncated":0,
        "BLiMP - # prompt tokens":8.608,
        "BLiMP - # output tokens":0.0,
        "BLiMP - # trials":1,
        "LegalSupport - # eval":489,
        "LegalSupport - # train":0,
        "LegalSupport - truncated":0,
        "LegalSupport - # prompt tokens":92.671,
        "LegalSupport - # output tokens":0.0,
        "LegalSupport - # trials":1,
        "LSAT - # eval":230,
        "LSAT - # train":0,
        "LSAT - truncated":0,
        "LSAT - # prompt tokens":165.309,
        "LSAT - # output tokens":0.0,
        "LSAT - # trials":1,
        "BBQ - # eval":1000,
        "BBQ - # train":0,
        "BBQ - truncated":0,
        "BBQ - # prompt tokens":57.788,
        "BBQ - # output tokens":0.0,
        "BBQ - # trials":1
    },
    {
        "Model":"OPT (66B) [method: multiple_choice_joint]",
        "Mean win rate":"-",
        "HellaSwag - # eval":1000,
        "HellaSwag - # train":5,
        "HellaSwag - truncated":0,
        "HellaSwag - # prompt tokens":614.348,
        "HellaSwag - # output tokens":1.0,
        "HellaSwag - # trials":3,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":5,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":248.21,
        "OpenbookQA - # output tokens":1.0,
        "OpenbookQA - # trials":3,
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":5,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":511.121,
        "TruthfulQA - # output tokens":1.0,
        "TruthfulQA - # trials":3,
        "MMLU - # eval":102.8,
        "MMLU - # train":5,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":472.274,
        "MMLU - # output tokens":1.0,
        "MMLU - # trials":3,
        "BLiMP - # eval":1000,
        "BLiMP - # train":0,
        "BLiMP - truncated":0,
        "BLiMP - # prompt tokens":33.936,
        "BLiMP - # output tokens":1.0,
        "BLiMP - # trials":3,
        "LegalSupport - # eval":489,
        "LegalSupport - # train":3,
        "LegalSupport - truncated":0,
        "LegalSupport - # prompt tokens":605.753,
        "LegalSupport - # output tokens":1.0,
        "LegalSupport - # trials":3,
        "LSAT - # eval":230,
        "LSAT - # train":5,
        "LSAT - truncated":0,
        "LSAT - # prompt tokens":1190.129,
        "LSAT - # output tokens":1.0,
        "LSAT - # trials":3,
        "BBQ - # eval":1000,
        "BBQ - # train":5,
        "BBQ - truncated":0,
        "BBQ - # prompt tokens":424.437,
        "BBQ - # output tokens":1.0,
        "BBQ - # trials":3
    },
    {
        "Model":"OPT (66B) [method: multiple_choice_separate_calibrated]",
        "Mean win rate":"-",
        "HellaSwag - # eval":1000,
        "HellaSwag - # train":0,
        "HellaSwag - truncated":0,
        "HellaSwag - # prompt tokens":33.273,
        "HellaSwag - # output tokens":0.846,
        "HellaSwag - # trials":1,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":0,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":5.27,
        "OpenbookQA - # output tokens":1.0,
        "OpenbookQA - # trials":1,
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":12.908,
        "TruthfulQA - # output tokens":0.0,
        "TruthfulQA - # trials":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":9.217,
        "MMLU - # output tokens":0.006,
        "MMLU - # trials":1,
        "BLiMP - # eval":1000,
        "BLiMP - # train":0,
        "BLiMP - truncated":0,
        "BLiMP - # prompt tokens":10.608,
        "BLiMP - # output tokens":0.0,
        "BLiMP - # trials":1,
        "LegalSupport - # eval":489,
        "LegalSupport - # train":0,
        "LegalSupport - truncated":0,
        "LegalSupport - # prompt tokens":32.62,
        "LegalSupport - # output tokens":0.0,
        "LegalSupport - # trials":1,
        "LSAT - # eval":230,
        "LSAT - # train":0,
        "LSAT - truncated":0,
        "LSAT - # prompt tokens":11.717,
        "LSAT - # output tokens":0.0,
        "LSAT - # trials":1,
        "BBQ - # eval":1000,
        "BBQ - # train":0,
        "BBQ - truncated":0,
        "BBQ - # prompt tokens":5.405,
        "BBQ - # output tokens":0.0,
        "BBQ - # trials":1
    },
    {
        "Model":"OPT (66B) [method: multiple_choice_separate_original]",
        "Mean win rate":"-",
        "HellaSwag - # eval":1000,
        "HellaSwag - # train":0,
        "HellaSwag - truncated":0,
        "HellaSwag - # prompt tokens":87.888,
        "HellaSwag - # output tokens":0.2,
        "HellaSwag - # trials":1,
        "OpenbookQA - # eval":500,
        "OpenbookQA - # train":0,
        "OpenbookQA - truncated":0,
        "OpenbookQA - # prompt tokens":15.024,
        "OpenbookQA - # output tokens":0.98,
        "OpenbookQA - # trials":1,
        "TruthfulQA - # eval":654,
        "TruthfulQA - # train":0,
        "TruthfulQA - truncated":0,
        "TruthfulQA - # prompt tokens":24.203,
        "TruthfulQA - # output tokens":0.0,
        "TruthfulQA - # trials":1,
        "MMLU - # eval":102.8,
        "MMLU - # train":0,
        "MMLU - truncated":0,
        "MMLU - # prompt tokens":38.647,
        "MMLU - # output tokens":0.0,
        "MMLU - # trials":1,
        "BLiMP - # eval":1000,
        "BLiMP - # train":0,
        "BLiMP - truncated":0,
        "BLiMP - # prompt tokens":8.608,
        "BLiMP - # output tokens":0.0,
        "BLiMP - # trials":1,
        "LegalSupport - # eval":489,
        "LegalSupport - # train":0,
        "LegalSupport - truncated":0,
        "LegalSupport - # prompt tokens":92.671,
        "LegalSupport - # output tokens":0.0,
        "LegalSupport - # trials":1,
        "LSAT - # eval":230,
        "LSAT - # train":0,
        "LSAT - truncated":0,
        "LSAT - # prompt tokens":165.309,
        "LSAT - # output tokens":0.0,
        "LSAT - # trials":1,
        "BBQ - # eval":1000,
        "BBQ - # train":0,
        "BBQ - truncated":0,
        "BBQ - # prompt tokens":57.788,
        "BBQ - # output tokens":0.0,
        "BBQ - # trials":1
    }
]