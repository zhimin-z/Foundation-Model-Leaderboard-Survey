[
    {
        "table_id":957,
        "row_id":28430,
        "rank":1,
        "Model":"ALBERT (Ensemble)",
        "mlmodel":{

        },
        "method_short":"ALBERT ",
        "method_details":"Ensemble",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-11-06",
        "metrics":{
            "Accuracy":"91.4",
            "Accuracy (Middle)":null,
            "Accuracy (High)":null
        },
        "raw_metrics":{
            "Accuracy":91.4,
            "Accuracy (Middle)":null,
            "Accuracy (High)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":233251,
            "title":"Improving Machine Reading Comprehension with Single-choice Decision and Transfer Learning",
            "url":"\/paper\/improving-machine-reading-comprehension-with-2",
            "published":"2020-11-06T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/improving-machine-reading-comprehension-with-2\/review\/?hl=28430"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":957,
        "row_id":17076,
        "rank":2,
        "Model":"Megatron-BERT (ensemble)",
        "mlmodel":{

        },
        "method_short":"Megatron-BERT ",
        "method_details":"ensemble",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-09-17",
        "metrics":{
            "Accuracy":"90.9",
            "Accuracy (Middle)":"93.1",
            "Accuracy (High)":"90.0"
        },
        "raw_metrics":{
            "Accuracy":90.9,
            "Accuracy (Middle)":93.1,
            "Accuracy (High)":90.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":154243,
            "title":"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
            "url":"\/paper\/megatron-lm-training-multi-billion-parameter",
            "published":"2019-09-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/megatron-lm-training-multi-billion-parameter\/review\/?hl=17076"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":957,
        "row_id":17075,
        "rank":3,
        "Model":"ALBERTxxlarge+DUMA(ensemble)",
        "mlmodel":{

        },
        "method_short":"ALBERTxxlarge+DUMA",
        "method_details":"ensemble",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-18",
        "metrics":{
            "Accuracy":"89.8",
            "Accuracy (Middle)":"88.7",
            "Accuracy (High)":"92.6"
        },
        "raw_metrics":{
            "Accuracy":89.8,
            "Accuracy (Middle)":88.7,
            "Accuracy (High)":92.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":181103,
            "title":"DUMA: Reading Comprehension with Transposition Thinking",
            "url":"\/paper\/dual-multi-head-co-attention-for-multi-choice",
            "published":"2020-01-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dual-multi-head-co-attention-for-multi-choice\/review\/?hl=17075"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":957,
        "row_id":17074,
        "rank":4,
        "Model":"Megatron-BERT",
        "mlmodel":{

        },
        "method_short":"Megatron-BERT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-09-17",
        "metrics":{
            "Accuracy":"89.5",
            "Accuracy (Middle)":"91.8",
            "Accuracy (High)":"88.6"
        },
        "raw_metrics":{
            "Accuracy":89.5,
            "Accuracy (Middle)":91.8,
            "Accuracy (High)":88.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":154243,
            "title":"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
            "url":"\/paper\/megatron-lm-training-multi-billion-parameter",
            "published":"2019-09-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/megatron-lm-training-multi-billion-parameter\/review\/?hl=17074"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":957,
        "row_id":24557,
        "rank":5,
        "Model":"DeBERTalarge",
        "mlmodel":{

        },
        "method_short":"DeBERTalarge",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-05",
        "metrics":{
            "Accuracy":"86.8",
            "Accuracy (Middle)":null,
            "Accuracy (High)":null
        },
        "raw_metrics":{
            "Accuracy":86.8,
            "Accuracy (Middle)":null,
            "Accuracy (High)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":201217,
            "title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention",
            "url":"\/paper\/deberta-decoding-enhanced-bert-with",
            "published":"2020-06-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deberta-decoding-enhanced-bert-with\/review\/?hl=24557"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":957,
        "row_id":17073,
        "rank":6,
        "Model":"B10-10-10",
        "mlmodel":{

        },
        "method_short":"B10-10-10",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-05",
        "metrics":{
            "Accuracy":"85.7",
            "Accuracy (Middle)":"88.8",
            "Accuracy (High)":"84.4"
        },
        "raw_metrics":{
            "Accuracy":85.7,
            "Accuracy (Middle)":88.8,
            "Accuracy (High)":84.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":200794,
            "title":"Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing",
            "url":"\/paper\/funnel-transformer-filtering-out-sequential",
            "published":"2020-06-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/funnel-transformer-filtering-out-sequential\/review\/?hl=17073"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":957,
        "row_id":6065,
        "rank":7,
        "Model":"RoBERTa",
        "mlmodel":{

        },
        "method_short":"RoBERTa",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-07-26",
        "metrics":{
            "Accuracy":"83.2",
            "Accuracy (Middle)":"86.5",
            "Accuracy (High)":"81.3"
        },
        "raw_metrics":{
            "Accuracy":83.2,
            "Accuracy (Middle)":86.5,
            "Accuracy (High)":81.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":148282,
            "title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "url":"\/paper\/roberta-a-robustly-optimized-bert-pretraining",
            "published":"2019-07-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/roberta-a-robustly-optimized-bert-pretraining\/review\/?hl=6065"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":957,
        "row_id":112808,
        "rank":8,
        "Model":"Orca 2-13B",
        "mlmodel":{

        },
        "method_short":"Orca 2-13B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-18",
        "metrics":{
            "Accuracy":"82.87",
            "Accuracy (Middle)":null,
            "Accuracy (High)":null
        },
        "raw_metrics":{
            "Accuracy":82.87,
            "Accuracy (Middle)":null,
            "Accuracy (High)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1323875,
            "title":"Orca 2: Teaching Small Language Models How to Reason",
            "url":"\/paper\/orca-2-teaching-small-language-models-how-to",
            "published":"2023-11-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/orca-2-teaching-small-language-models-how-to\/review\/?hl=112808"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":957,
        "row_id":112802,
        "rank":9,
        "Model":"Orca 2-7B",
        "mlmodel":{

        },
        "method_short":"Orca 2-7B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-18",
        "metrics":{
            "Accuracy":"80.79",
            "Accuracy (Middle)":null,
            "Accuracy (High)":null
        },
        "raw_metrics":{
            "Accuracy":80.79,
            "Accuracy (Middle)":null,
            "Accuracy (High)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1323875,
            "title":"Orca 2: Teaching Small Language Models How to Reason",
            "url":"\/paper\/orca-2-teaching-small-language-models-how-to",
            "published":"2023-11-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/orca-2-teaching-small-language-models-how-to\/review\/?hl=112802"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":957,
        "row_id":35550,
        "rank":10,
        "Model":"HAT (Encoder)",
        "mlmodel":{

        },
        "method_short":"HAT ",
        "method_details":"Encoder",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-15",
        "metrics":{
            "Accuracy":"67.3",
            "Accuracy (Middle)":null,
            "Accuracy (High)":null
        },
        "raw_metrics":{
            "Accuracy":67.3,
            "Accuracy (Middle)":null,
            "Accuracy (High)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":782858,
            "title":"Hierarchical Learning for Generation with Long Source Sequences",
            "url":"\/paper\/hierarchical-learning-for-generation-with",
            "published":"2021-04-15T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/hierarchical-learning-for-generation-with\/review\/?hl=35550"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":957,
        "row_id":12699,
        "rank":11,
        "Model":"XLNet",
        "mlmodel":{

        },
        "method_short":"XLNet",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-19",
        "metrics":{
            "Accuracy":null,
            "Accuracy (Middle)":"88.6",
            "Accuracy (High)":"84.0"
        },
        "raw_metrics":{
            "Accuracy":null,
            "Accuracy (Middle)":88.6,
            "Accuracy (High)":84.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":143172,
            "title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding",
            "url":"\/paper\/xlnet-generalized-autoregressive-pretraining",
            "published":"2019-06-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/xlnet-generalized-autoregressive-pretraining\/review\/?hl=12699"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":957,
        "row_id":97655,
        "rank":12,
        "Model":"PaLM 540B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"PaLM 540B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-05",
        "metrics":{
            "Accuracy":null,
            "Accuracy (Middle)":"68.1",
            "Accuracy (High)":"49.1"
        },
        "raw_metrics":{
            "Accuracy":null,
            "Accuracy (Middle)":68.1,
            "Accuracy (High)":49.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":989558,
            "title":"PaLM: Scaling Language Modeling with Pathways",
            "url":"\/paper\/palm-scaling-language-modeling-with-pathways-1",
            "published":"2022-04-05T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":957,
        "row_id":97651,
        "rank":13,
        "Model":"LLaMA 65B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"LLaMA 65B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-27",
        "metrics":{
            "Accuracy":null,
            "Accuracy (Middle)":"67.9",
            "Accuracy (High)":"51.6"
        },
        "raw_metrics":{
            "Accuracy":null,
            "Accuracy (Middle)":67.9,
            "Accuracy (High)":51.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":1164350,
            "title":"LLaMA: Open and Efficient Foundation Language Models",
            "url":"\/paper\/llama-open-and-efficient-foundation-language-1",
            "published":"2023-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-open-and-efficient-foundation-language-1\/review\/?hl=97651"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":957,
        "row_id":97654,
        "rank":14,
        "Model":"PaLM 62B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"PaLM 62B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-05",
        "metrics":{
            "Accuracy":null,
            "Accuracy (Middle)":"64.3",
            "Accuracy (High)":"47.5"
        },
        "raw_metrics":{
            "Accuracy":null,
            "Accuracy (Middle)":64.3,
            "Accuracy (High)":47.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":989558,
            "title":"PaLM: Scaling Language Modeling with Pathways",
            "url":"\/paper\/palm-scaling-language-modeling-with-pathways-1",
            "published":"2022-04-05T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":957,
        "row_id":97650,
        "rank":15,
        "Model":"LLaMA 33B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"LLaMA 33B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-27",
        "metrics":{
            "Accuracy":null,
            "Accuracy (Middle)":"64.1",
            "Accuracy (High)":"48.3"
        },
        "raw_metrics":{
            "Accuracy":null,
            "Accuracy (Middle)":64.1,
            "Accuracy (High)":48.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1164350,
            "title":"LLaMA: Open and Efficient Foundation Language Models",
            "url":"\/paper\/llama-open-and-efficient-foundation-language-1",
            "published":"2023-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-open-and-efficient-foundation-language-1\/review\/?hl=97650"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":957,
        "row_id":97649,
        "rank":16,
        "Model":"LLaMA 13B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"LLaMA 13B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-27",
        "metrics":{
            "Accuracy":null,
            "Accuracy (Middle)":"61.6",
            "Accuracy (High)":"47.2"
        },
        "raw_metrics":{
            "Accuracy":null,
            "Accuracy (Middle)":61.6,
            "Accuracy (High)":47.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":1164350,
            "title":"LLaMA: Open and Efficient Foundation Language Models",
            "url":"\/paper\/llama-open-and-efficient-foundation-language-1",
            "published":"2023-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-open-and-efficient-foundation-language-1\/review\/?hl=97649"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":957,
        "row_id":97648,
        "rank":17,
        "Model":"LLaMA 7B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"LLaMA 7B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-27",
        "metrics":{
            "Accuracy":null,
            "Accuracy (Middle)":"61.1",
            "Accuracy (High)":"46.9"
        },
        "raw_metrics":{
            "Accuracy":null,
            "Accuracy (Middle)":61.1,
            "Accuracy (High)":46.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":1164350,
            "title":"LLaMA: Open and Efficient Foundation Language Models",
            "url":"\/paper\/llama-open-and-efficient-foundation-language-1",
            "published":"2023-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-open-and-efficient-foundation-language-1\/review\/?hl=97648"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":957,
        "row_id":97652,
        "rank":18,
        "Model":"GPT-3 175B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"GPT-3 175B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-05-28",
        "metrics":{
            "Accuracy":null,
            "Accuracy (Middle)":"58.4",
            "Accuracy (High)":"45.5"
        },
        "raw_metrics":{
            "Accuracy":null,
            "Accuracy (Middle)":58.4,
            "Accuracy (High)":45.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":198147,
            "title":"Language Models are Few-Shot Learners",
            "url":"\/paper\/language-models-are-few-shot-learners",
            "published":"2020-05-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/language-models-are-few-shot-learners\/review\/?hl=97652"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":957,
        "row_id":97653,
        "rank":19,
        "Model":"PaLM 8B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"PaLM 8B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-05",
        "metrics":{
            "Accuracy":null,
            "Accuracy (Middle)":"57.9",
            "Accuracy (High)":"42.3"
        },
        "raw_metrics":{
            "Accuracy":null,
            "Accuracy (Middle)":57.9,
            "Accuracy (High)":42.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":989558,
            "title":"PaLM: Scaling Language Modeling with Pathways",
            "url":"\/paper\/palm-scaling-language-modeling-with-pathways-1",
            "published":"2022-04-05T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":957,
        "row_id":100792,
        "rank":20,
        "Model":"Bloomberg GPT (one-shot)",
        "mlmodel":{

        },
        "method_short":"Bloomberg GPT ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "Accuracy":null,
            "Accuracy (Middle)":"54.32",
            "Accuracy (High)":"41.74"
        },
        "raw_metrics":{
            "Accuracy":null,
            "Accuracy (Middle)":54.32,
            "Accuracy (High)":41.74
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100792"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":957,
        "row_id":100795,
        "rank":21,
        "Model":"BLOOM 176B (one-shot)",
        "mlmodel":{

        },
        "method_short":"BLOOM 176B ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "Accuracy":null,
            "Accuracy (Middle)":"52.3",
            "Accuracy (High)":"39.14"
        },
        "raw_metrics":{
            "Accuracy":null,
            "Accuracy (Middle)":52.3,
            "Accuracy (High)":39.14
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100795"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":957,
        "row_id":100794,
        "rank":22,
        "Model":"OPT 66B (one-shot)",
        "mlmodel":{

        },
        "method_short":"OPT 66B ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "Accuracy":null,
            "Accuracy (Middle)":"47.42",
            "Accuracy (High)":"37.02"
        },
        "raw_metrics":{
            "Accuracy":null,
            "Accuracy (Middle)":47.42,
            "Accuracy (High)":37.02
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100794"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":957,
        "row_id":100793,
        "rank":23,
        "Model":"GPT-NeoX (one-shot)",
        "mlmodel":{

        },
        "method_short":"GPT-NeoX ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "Accuracy":null,
            "Accuracy (Middle)":"41.23",
            "Accuracy (High)":"34.33"
        },
        "raw_metrics":{
            "Accuracy":null,
            "Accuracy (Middle)":41.23,
            "Accuracy (High)":34.33
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100793"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]