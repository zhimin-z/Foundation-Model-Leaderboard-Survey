[
    {
        "Model":"Mistral v0.1 (7B)",
        "EM":"0.715",
        "Observed inference time (s)":"0.709",
        "# eval":"165",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"3089.109",
        "# output tokens":"1"
    },
    {
        "Model":"Llama 2 (13B)",
        "EM":"0.655",
        "Observed inference time (s)":"1.697",
        "# eval":"165",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"3159.636",
        "# output tokens":"1"
    },
    {
        "Model":"Llama 2 (70B)",
        "EM":"0.812",
        "Observed inference time (s)":"0.981",
        "# eval":"165",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"3159.636",
        "# output tokens":"1"
    },
    {
        "Model":"Llama 2 (7B)",
        "EM":"0.606",
        "Observed inference time (s)":"0.947",
        "# eval":"165",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"3159.636",
        "# output tokens":"1"
    },
    {
        "Model":"Mixtral (8x7B 32K seqlen)",
        "EM":"0.818",
        "Observed inference time (s)":"0.641",
        "# eval":"165",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"3089.109",
        "# output tokens":"1"
    },
    {
        "Model":"Phi-2",
        "EM":"0.648",
        "Observed inference time (s)":"0.409",
        "# eval":"165",
        "# train":"2.945",
        "truncated":"0",
        "# prompt tokens":"1826.103",
        "# output tokens":"1"
    },
    {
        "Model":"Qwen1.5 (14B)",
        "EM":"0.848",
        "Observed inference time (s)":"0.549",
        "# eval":"165",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"2807.903",
        "# output tokens":"1"
    },
    {
        "Model":"Qwen1.5 (72B)",
        "EM":"0.861",
        "Observed inference time (s)":"0.713",
        "# eval":"165",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"2807.903",
        "# output tokens":"1"
    },
    {
        "Model":"Qwen1.5 (7B)",
        "EM":"0.733",
        "Observed inference time (s)":"0.42",
        "# eval":"165",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"2807.903",
        "# output tokens":"1"
    },
    {
        "Model":"Yi (34B)",
        "EM":"0.867",
        "Observed inference time (s)":"2.359",
        "# eval":"165",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"2957.412",
        "# output tokens":"1"
    },
    {
        "Model":"Yi (6B)",
        "EM":"0.752",
        "Observed inference time (s)":"0.912",
        "# eval":"165",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"2957.412",
        "# output tokens":"1"
    },
    {
        "Model":"Anthropic Claude Instant 1.2",
        "EM":"0.818",
        "Observed inference time (s)":"1.62",
        "# eval":"165",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"2952.576",
        "# output tokens":"1"
    },
    {
        "Model":"Anthropic Claude 2.1",
        "EM":"0.848",
        "Observed inference time (s)":"3.916",
        "# eval":"165",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"2952.576",
        "# output tokens":"1"
    },
    {
        "Model":"Claude 3 Haiku (20240307)",
        "EM":"0.818",
        "Observed inference time (s)":"0.966",
        "# eval":"165",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"2887.576",
        "# output tokens":"1"
    },
    {
        "Model":"Claude 3 Sonnet (20240229)",
        "EM":"0.848",
        "Observed inference time (s)":"2.456",
        "# eval":"165",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"2887.576",
        "# output tokens":"1"
    },
    {
        "Model":"Claude 3 Opus (20240229)",
        "EM":"0.885",
        "Observed inference time (s)":"5.005",
        "# eval":"165",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"2887.576",
        "# output tokens":"1"
    },
    {
        "Model":"Gemini 1.0 Pro",
        "EM":"0.824",
        "Observed inference time (s)":"0.836",
        "# eval":"165",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"2789.424",
        "# output tokens":"0"
    },
    {
        "Model":"PaLM-2 (Bison)",
        "EM":"0.848",
        "Observed inference time (s)":"1.13",
        "# eval":"165",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"2823.23",
        "# output tokens":"1"
    },
    {
        "Model":"PaLM-2 (Unicorn)",
        "EM":"0.861",
        "Observed inference time (s)":"2.108",
        "# eval":"165",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"2823.23",
        "# output tokens":"0"
    },
    {
        "Model":"GPT-3.5 Turbo (0613)",
        "EM":"0.8",
        "Observed inference time (s)":"0.539",
        "# eval":"165",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"2791.073",
        "# output tokens":"1"
    },
    {
        "Model":"GPT-4 Turbo (1106 preview)",
        "EM":"0.897",
        "Observed inference time (s)":"0.64",
        "# eval":"165",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"2791.073",
        "# output tokens":"1"
    },
    {
        "Model":"GPT-4 (0613)",
        "EM":"0.879",
        "Observed inference time (s)":"0.579",
        "# eval":"165",
        "# train":"5",
        "truncated":"0",
        "# prompt tokens":"2791.073",
        "# output tokens":"1"
    }
]