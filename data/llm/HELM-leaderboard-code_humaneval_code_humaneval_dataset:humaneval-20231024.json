{
  "title": "dataset: humaneval",
  "header": [
    {
      "value": "Model/adapter",
      "markdown": false,
      "metadata": {}
    },
    {
      "value": "pass@1",
      "description": "The HumanEval benchmark for measuring functional correctness for synthesizing programs from docstrings [(Chen et al., 2021)](https://arxiv.org/pdf/2107.03374.pdf).\n\npass@1: Fraction of model outputs that pass the associated test cases.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "pass@1",
        "run_group": "HumanEval (Code)"
      }
    },
    {
      "value": "Denoised inference time (s)",
      "description": "The HumanEval benchmark for measuring functional correctness for synthesizing programs from docstrings [(Chen et al., 2021)](https://arxiv.org/pdf/2107.03374.pdf).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Denoised inference time (s)",
        "run_group": "HumanEval (Code)"
      }
    },
    {
      "value": "# eval",
      "description": "The HumanEval benchmark for measuring functional correctness for synthesizing programs from docstrings [(Chen et al., 2021)](https://arxiv.org/pdf/2107.03374.pdf).\n\n# eval: Number of evaluation instances.",
      "markdown": false,
      "metadata": {
        "metric": "# eval",
        "run_group": "HumanEval (Code)"
      }
    },
    {
      "value": "# train",
      "description": "The HumanEval benchmark for measuring functional correctness for synthesizing programs from docstrings [(Chen et al., 2021)](https://arxiv.org/pdf/2107.03374.pdf).\n\n# train: Number of training instances (e.g., in-context examples).",
      "markdown": false,
      "metadata": {
        "metric": "# train",
        "run_group": "HumanEval (Code)"
      }
    },
    {
      "value": "truncated",
      "description": "The HumanEval benchmark for measuring functional correctness for synthesizing programs from docstrings [(Chen et al., 2021)](https://arxiv.org/pdf/2107.03374.pdf).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
      "markdown": false,
      "metadata": {
        "metric": "truncated",
        "run_group": "HumanEval (Code)"
      }
    },
    {
      "value": "# prompt tokens",
      "description": "The HumanEval benchmark for measuring functional correctness for synthesizing programs from docstrings [(Chen et al., 2021)](https://arxiv.org/pdf/2107.03374.pdf).\n\n# prompt tokens: Number of tokens in the prompt.",
      "markdown": false,
      "metadata": {
        "metric": "# prompt tokens",
        "run_group": "HumanEval (Code)"
      }
    },
    {
      "value": "# output tokens",
      "description": "The HumanEval benchmark for measuring functional correctness for synthesizing programs from docstrings [(Chen et al., 2021)](https://arxiv.org/pdf/2107.03374.pdf).\n\n# output tokens: Actual number of output tokens.",
      "markdown": false,
      "metadata": {
        "metric": "# output tokens",
        "run_group": "HumanEval (Code)"
      }
    },
    {
      "value": "# trials",
      "description": "The HumanEval benchmark for measuring functional correctness for synthesizing programs from docstrings [(Chen et al., 2021)](https://arxiv.org/pdf/2107.03374.pdf).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
      "markdown": false,
      "metadata": {
        "metric": "# trials",
        "run_group": "HumanEval (Code)"
      }
    }
  ],
  "rows": [
    [
      {
        "value": "code-davinci-002",
        "description": "",
        "href": "?group=code_humaneval&subgroup=dataset%3A%20humaneval&runSpecs=%5B%22code%3Adataset%3Dhumaneval%2Cmodel%3Dopenai_code-davinci-002%22%5D",
        "markdown": false
      },
      {
        "value": 0.4634146341463415,
        "description": "min=0.463, mean=0.463, max=0.463, sum=0.463 (1)",
        "style": {
          "font-weight": "bold"
        },
        "markdown": false
      },
      {
        "value": 2.002510170541158,
        "description": "min=2.003, mean=2.003, max=2.003, sum=2.003 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 164.0,
        "description": "min=164, mean=164, max=164, sum=164 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 0.0,
        "description": "min=0, mean=0, max=0, sum=0 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 0.0,
        "description": "min=0, mean=0, max=0, sum=0 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 170.34756097560975,
        "description": "min=170.348, mean=170.348, max=170.348, sum=170.348 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 74.8109756097561,
        "description": "min=74.811, mean=74.811, max=74.811, sum=74.811 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 1.0,
        "description": "min=1, mean=1, max=1, sum=1 (1)",
        "style": {},
        "markdown": false
      }
    ],
    [
      {
        "value": "code-cushman-001 (12B)",
        "description": "",
        "href": "?group=code_humaneval&subgroup=dataset%3A%20humaneval&runSpecs=%5B%22code%3Adataset%3Dhumaneval%2Cmodel%3Dopenai_code-cushman-001%22%5D",
        "markdown": false
      },
      {
        "value": 0.3170731707317073,
        "description": "min=0.317, mean=0.317, max=0.317, sum=0.317 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 0.7948615901295731,
        "description": "min=0.795, mean=0.795, max=0.795, sum=0.795 (1)",
        "style": {
          "font-weight": "bold"
        },
        "markdown": false
      },
      {
        "value": 164.0,
        "description": "min=164, mean=164, max=164, sum=164 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 0.0,
        "description": "min=0, mean=0, max=0, sum=0 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 0.0,
        "description": "min=0, mean=0, max=0, sum=0 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 170.34756097560975,
        "description": "min=170.348, mean=170.348, max=170.348, sum=170.348 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 88.77439024390245,
        "description": "min=88.774, mean=88.774, max=88.774, sum=88.774 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 1.0,
        "description": "min=1, mean=1, max=1, sum=1 (1)",
        "style": {},
        "markdown": false
      }
    ]
  ],
  "links": [
    {
      "text": "compare all",
      "href": "?group=code_humaneval&subgroup=dataset%3A%20humaneval&runSpecs=%5B%22code%3Adataset%3Dhumaneval%2Cmodel%3Dopenai_code-cushman-001%22%2C%20%22code%3Adataset%3Dhumaneval%2Cmodel%3Dopenai_code-davinci-002%22%5D"
    },
    {
      "text": "LaTeX",
      "href": "/nlp/scr4/nlp/crfm/yifanmai/helm-release/benchmark_output/releases/v0.3.0/groups/latex/code_humaneval_code_humaneval_dataset:humaneval.tex"
    },
    {
      "text": "JSON",
      "href": "/nlp/scr4/nlp/crfm/yifanmai/helm-release/benchmark_output/releases/v0.3.0/groups/json/code_humaneval_code_humaneval_dataset:humaneval.json"
    }
  ],
  "name": "code_humaneval_dataset:humaneval"
}