[
    {
        "table_id":7966,
        "row_id":57553,
        "rank":1,
        "method":"Model soups (BASIC-L)",
        "mlmodel":{

        },
        "method_short":"Model soups ",
        "method_details":"BASIC-L",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-10",
        "metrics":{
            "Top 1 Accuracy":"84.63"
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.63
        },
        "uses_additional_data":true,
        "paper":{
            "id":974897,
            "title":"Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
            "url":"\/paper\/model-soups-averaging-weights-of-multiple",
            "published":"2022-03-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/model-soups-averaging-weights-of-multiple\/review\/?hl=57553"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7966,
        "row_id":70416,
        "rank":2,
        "method":"ViT-e",
        "mlmodel":{

        },
        "method_short":"ViT-e",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-14",
        "metrics":{
            "Top 1 Accuracy":"84.3"
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1074922,
            "title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model",
            "url":"\/paper\/pali-a-jointly-scaled-multilingual-language",
            "published":"2022-09-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/pali-a-jointly-scaled-multilingual-language\/review\/?hl=70416"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7966,
        "row_id":48975,
        "rank":3,
        "method":"Model soups (ViT-G\/14)",
        "mlmodel":{

        },
        "method_short":"Model soups ",
        "method_details":"ViT-G\/14",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-10",
        "metrics":{
            "Top 1 Accuracy":"84.22"
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.22
        },
        "uses_additional_data":true,
        "paper":{
            "id":974897,
            "title":"Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
            "url":"\/paper\/model-soups-averaging-weights-of-multiple",
            "published":"2022-03-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/model-soups-averaging-weights-of-multiple\/review\/?hl=48975"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7966,
        "row_id":42926,
        "rank":4,
        "method":"SwinV2-G",
        "mlmodel":{

        },
        "method_short":"SwinV2-G",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-18",
        "metrics":{
            "Top 1 Accuracy":"84.00%"
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":912369,
            "title":"Swin Transformer V2: Scaling Up Capacity and Resolution",
            "url":"\/paper\/swin-transformer-v2-scaling-up-capacity-and",
            "published":"2021-11-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/swin-transformer-v2-scaling-up-capacity-and\/review\/?hl=42926"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7966,
        "row_id":34459,
        "rank":5,
        "method":"ViT-G\/14",
        "mlmodel":{

        },
        "method_short":"ViT-G\/14",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-08",
        "metrics":{
            "Top 1 Accuracy":"83.33"
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.33
        },
        "uses_additional_data":true,
        "paper":{
            "id":813674,
            "title":"Scaling Vision Transformers",
            "url":"\/paper\/scaling-vision-transformers",
            "published":"2021-06-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-vision-transformers\/review\/?hl=34459"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7966,
        "row_id":100963,
        "rank":6,
        "method":"MAWS (ViT-2B)",
        "mlmodel":{

        },
        "method_short":"MAWS ",
        "method_details":"ViT-2B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-23",
        "metrics":{
            "Top 1 Accuracy":"83.0"
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":1179156,
            "title":"The effectiveness of MAE pre-pretraining for billion-scale pretraining",
            "url":"\/paper\/the-effectiveness-of-mae-pre-pretraining-for",
            "published":"2023-03-23T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7966,
        "row_id":101770,
        "rank":7,
        "method":"MOAT-4 (IN-22K pretraining)",
        "mlmodel":{

        },
        "method_short":"MOAT-4 ",
        "method_details":"IN-22K pretraining",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-04",
        "metrics":{
            "Top 1 Accuracy":"81.5"
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":1087041,
            "title":"MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models",
            "url":"\/paper\/moat-alternating-mobile-convolution-and",
            "published":"2022-10-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/moat-alternating-mobile-convolution-and\/review\/?hl=101770"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7966,
        "row_id":45651,
        "rank":8,
        "method":"SWAG (ViT H\/14)",
        "mlmodel":{

        },
        "method_short":"SWAG ",
        "method_details":"ViT H\/14",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-20",
        "metrics":{
            "Top 1 Accuracy":"81.1"
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.1
        },
        "uses_additional_data":true,
        "paper":{
            "id":948291,
            "title":"Revisiting Weakly Supervised Pre-Training of Visual Perception Models",
            "url":"\/paper\/revisiting-weakly-supervised-pre-training-of",
            "published":"2022-01-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/revisiting-weakly-supervised-pre-training-of\/review\/?hl=45651"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7966,
        "row_id":101771,
        "rank":9,
        "method":"MOAT-3 (IN-22K pretraining)",
        "mlmodel":{

        },
        "method_short":"MOAT-3 ",
        "method_details":"IN-22K pretraining",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-04",
        "metrics":{
            "Top 1 Accuracy":"80.6"
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":1087041,
            "title":"MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models",
            "url":"\/paper\/moat-alternating-mobile-convolution-and",
            "published":"2022-10-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/moat-alternating-mobile-convolution-and\/review\/?hl=101771"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7966,
        "row_id":101772,
        "rank":10,
        "method":"MOAT-2 (IN-22K pretraining)",
        "mlmodel":{

        },
        "method_short":"MOAT-2 ",
        "method_details":"IN-22K pretraining",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-04",
        "metrics":{
            "Top 1 Accuracy":"79.3"
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1087041,
            "title":"MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models",
            "url":"\/paper\/moat-alternating-mobile-convolution-and",
            "published":"2022-10-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/moat-alternating-mobile-convolution-and\/review\/?hl=101772"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7966,
        "row_id":101773,
        "rank":11,
        "method":"MOAT-1 (IN-22K pretraining)",
        "mlmodel":{

        },
        "method_short":"MOAT-1 ",
        "method_details":"IN-22K pretraining",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-04",
        "metrics":{
            "Top 1 Accuracy":"78.4"
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":1087041,
            "title":"MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models",
            "url":"\/paper\/moat-alternating-mobile-convolution-and",
            "published":"2022-10-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/moat-alternating-mobile-convolution-and\/review\/?hl=101773"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7966,
        "row_id":61471,
        "rank":12,
        "method":"SwinV2-B",
        "mlmodel":{

        },
        "method_short":"SwinV2-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-18",
        "metrics":{
            "Top 1 Accuracy":"78.08"
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.08
        },
        "uses_additional_data":false,
        "paper":{
            "id":912369,
            "title":"Swin Transformer V2: Scaling Up Capacity and Resolution",
            "url":"\/paper\/swin-transformer-v2-scaling-up-capacity-and",
            "published":"2021-11-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/swin-transformer-v2-scaling-up-capacity-and\/review\/?hl=61471"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7966,
        "row_id":35536,
        "rank":13,
        "method":"VOLO-D5",
        "mlmodel":{

        },
        "method_short":"VOLO-D5",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-24",
        "metrics":{
            "Top 1 Accuracy":"78"
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":824294,
            "title":"VOLO: Vision Outlooker for Visual Recognition",
            "url":"\/paper\/volo-vision-outlooker-for-visual-recognition",
            "published":"2021-06-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/volo-vision-outlooker-for-visual-recognition\/review\/?hl=35536"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7966,
        "row_id":35538,
        "rank":14,
        "method":"VOLO-D4",
        "mlmodel":{

        },
        "method_short":"VOLO-D4",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-24",
        "metrics":{
            "Top 1 Accuracy":"77.8"
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":824294,
            "title":"VOLO: Vision Outlooker for Visual Recognition",
            "url":"\/paper\/volo-vision-outlooker-for-visual-recognition",
            "published":"2021-06-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/volo-vision-outlooker-for-visual-recognition\/review\/?hl=35538"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7966,
        "row_id":29442,
        "rank":15,
        "method":"CAIT-M36-448",
        "mlmodel":{

        },
        "method_short":"CAIT-M36-448",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-31",
        "metrics":{
            "Top 1 Accuracy":"76.7"
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":772635,
            "title":"Going deeper with Image Transformers",
            "url":"\/paper\/going-deeper-with-image-transformers",
            "published":"2021-03-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/going-deeper-with-image-transformers\/review\/?hl=29442"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7966,
        "row_id":47971,
        "rank":16,
        "method":"SEER (RegNet10B)",
        "mlmodel":{

        },
        "method_short":"SEER ",
        "method_details":"RegNet10B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-16",
        "metrics":{
            "Top 1 Accuracy":"76.2"
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.2
        },
        "uses_additional_data":true,
        "paper":{
            "id":963673,
            "title":"Vision Models Are More Robust And Fair When Pretrained On Uncurated Images Without Supervision",
            "url":"\/paper\/vision-models-are-more-robust-and-fair-when",
            "published":"2022-02-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vision-models-are-more-robust-and-fair-when\/review\/?hl=47971"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":107,
                "name":"IG-1B",
                "color":"#2771D3"
            },
            {
                "id":108,
                "name":"RegNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7966,
        "row_id":34924,
        "rank":17,
        "method":"ResMLP-B24\/8 22k",
        "mlmodel":{

        },
        "method_short":"ResMLP-B24\/8 22k",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-07",
        "metrics":{
            "Top 1 Accuracy":"74.2"
        },
        "raw_metrics":{
            "Top 1 Accuracy":74.2
        },
        "uses_additional_data":true,
        "paper":{
            "id":795412,
            "title":"ResMLP: Feedforward networks for image classification with data-efficient training",
            "url":"\/paper\/resmlp-feedforward-networks-for-image",
            "published":"2021-05-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/resmlp-feedforward-networks-for-image\/review\/?hl=34924"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7966,
        "row_id":49546,
        "rank":18,
        "method":"ViT-B-36x1",
        "mlmodel":{

        },
        "method_short":"ViT-B-36x1",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-18",
        "metrics":{
            "Top 1 Accuracy":"73.9"
        },
        "raw_metrics":{
            "Top 1 Accuracy":73.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":979672,
            "title":"Three things everyone should know about Vision Transformers",
            "url":"\/paper\/three-things-everyone-should-know-about",
            "published":"2022-03-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/three-things-everyone-should-know-about\/review\/?hl=49546"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7966,
        "row_id":34923,
        "rank":19,
        "method":"ResMLP-B24\/8",
        "mlmodel":{

        },
        "method_short":"ResMLP-B24\/8",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-07",
        "metrics":{
            "Top 1 Accuracy":"73.4"
        },
        "raw_metrics":{
            "Top 1 Accuracy":73.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":795412,
            "title":"ResMLP: Feedforward networks for image classification with data-efficient training",
            "url":"\/paper\/resmlp-feedforward-networks-for-image",
            "published":"2021-05-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/resmlp-feedforward-networks-for-image\/review\/?hl=34923"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7966,
        "row_id":53861,
        "rank":20,
        "method":"Sequencer2D-L",
        "mlmodel":{

        },
        "method_short":"Sequencer2D-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-04",
        "metrics":{
            "Top 1 Accuracy":"73.4"
        },
        "raw_metrics":{
            "Top 1 Accuracy":73.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":1004136,
            "title":"Sequencer: Deep LSTM for Image Classification",
            "url":"\/paper\/sequencer-deep-lstm-for-image-classification",
            "published":"2022-05-04T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7966,
        "row_id":29463,
        "rank":21,
        "method":"LeViT-384",
        "mlmodel":{

        },
        "method_short":"LeViT-384",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-02",
        "metrics":{
            "Top 1 Accuracy":"71.4"
        },
        "raw_metrics":{
            "Top 1 Accuracy":71.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":774248,
            "title":"LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference",
            "url":"\/paper\/levit-a-vision-transformer-in-convnet-s",
            "published":"2021-04-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/levit-a-vision-transformer-in-convnet-s\/review\/?hl=29463"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7966,
        "row_id":29472,
        "rank":22,
        "method":"LeViT-256",
        "mlmodel":{

        },
        "method_short":"LeViT-256",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-02",
        "metrics":{
            "Top 1 Accuracy":"69.9"
        },
        "raw_metrics":{
            "Top 1 Accuracy":69.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":774248,
            "title":"LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference",
            "url":"\/paper\/levit-a-vision-transformer-in-convnet-s",
            "published":"2021-04-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/levit-a-vision-transformer-in-convnet-s\/review\/?hl=29472"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7966,
        "row_id":34925,
        "rank":23,
        "method":"ResMLP-S24\/16",
        "mlmodel":{

        },
        "method_short":"ResMLP-S24\/16",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-07",
        "metrics":{
            "Top 1 Accuracy":"69.8"
        },
        "raw_metrics":{
            "Top 1 Accuracy":69.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":795412,
            "title":"ResMLP: Feedforward networks for image classification with data-efficient training",
            "url":"\/paper\/resmlp-feedforward-networks-for-image",
            "published":"2021-05-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/resmlp-feedforward-networks-for-image\/review\/?hl=34925"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7966,
        "row_id":34307,
        "rank":24,
        "method":"ResNet-152x2-SAM",
        "mlmodel":{

        },
        "method_short":"ResNet-152x2-SAM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-03",
        "metrics":{
            "Top 1 Accuracy":"69.6"
        },
        "raw_metrics":{
            "Top 1 Accuracy":69.6
        },
        "uses_additional_data":true,
        "paper":{
            "id":810994,
            "title":"When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations",
            "url":"\/paper\/when-vision-transformers-outperform-resnets",
            "published":"2021-06-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/when-vision-transformers-outperform-resnets\/review\/?hl=34307"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7966,
        "row_id":29481,
        "rank":25,
        "method":"LeViT-192",
        "mlmodel":{

        },
        "method_short":"LeViT-192",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-02",
        "metrics":{
            "Top 1 Accuracy":"68.7"
        },
        "raw_metrics":{
            "Top 1 Accuracy":68.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":774248,
            "title":"LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference",
            "url":"\/paper\/levit-a-vision-transformer-in-convnet-s",
            "published":"2021-04-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/levit-a-vision-transformer-in-convnet-s\/review\/?hl=29481"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7966,
        "row_id":40195,
        "rank":26,
        "method":"ResNet50 (A1)",
        "mlmodel":{

        },
        "method_short":"ResNet50 ",
        "method_details":"A1",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-01",
        "metrics":{
            "Top 1 Accuracy":"68.7"
        },
        "raw_metrics":{
            "Top 1 Accuracy":68.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":877343,
            "title":"ResNet strikes back: An improved training procedure in timm",
            "url":"\/paper\/resnet-strikes-back-an-improved-training",
            "published":"2021-10-01T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7966,
        "row_id":29490,
        "rank":27,
        "method":"LeViT-128",
        "mlmodel":{

        },
        "method_short":"LeViT-128",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-02",
        "metrics":{
            "Top 1 Accuracy":"67.5"
        },
        "raw_metrics":{
            "Top 1 Accuracy":67.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":774248,
            "title":"LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference",
            "url":"\/paper\/levit-a-vision-transformer-in-convnet-s",
            "published":"2021-04-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/levit-a-vision-transformer-in-convnet-s\/review\/?hl=29490"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7966,
        "row_id":34312,
        "rank":28,
        "method":"ViT-B\/16-SAM",
        "mlmodel":{

        },
        "method_short":"ViT-B\/16-SAM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-03",
        "metrics":{
            "Top 1 Accuracy":"67.5"
        },
        "raw_metrics":{
            "Top 1 Accuracy":67.5
        },
        "uses_additional_data":true,
        "paper":{
            "id":810994,
            "title":"When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations",
            "url":"\/paper\/when-vision-transformers-outperform-resnets",
            "published":"2021-06-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/when-vision-transformers-outperform-resnets\/review\/?hl=34312"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7966,
        "row_id":34926,
        "rank":29,
        "method":"ResMLP-S12\/16",
        "mlmodel":{

        },
        "method_short":"ResMLP-S12\/16",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-07",
        "metrics":{
            "Top 1 Accuracy":"66.0"
        },
        "raw_metrics":{
            "Top 1 Accuracy":66.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":795412,
            "title":"ResMLP: Feedforward networks for image classification with data-efficient training",
            "url":"\/paper\/resmlp-feedforward-networks-for-image",
            "published":"2021-05-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/resmlp-feedforward-networks-for-image\/review\/?hl=34926"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7966,
        "row_id":34317,
        "rank":30,
        "method":"Mixer-B\/8-SAM",
        "mlmodel":{

        },
        "method_short":"Mixer-B\/8-SAM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-03",
        "metrics":{
            "Top 1 Accuracy":"65.5"
        },
        "raw_metrics":{
            "Top 1 Accuracy":65.5
        },
        "uses_additional_data":true,
        "paper":{
            "id":810994,
            "title":"When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations",
            "url":"\/paper\/when-vision-transformers-outperform-resnets",
            "published":"2021-06-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/when-vision-transformers-outperform-resnets\/review\/?hl=34317"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7966,
        "row_id":29499,
        "rank":31,
        "method":"LeViT-128S",
        "mlmodel":{

        },
        "method_short":"LeViT-128S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-02",
        "metrics":{
            "Top 1 Accuracy":"63.9"
        },
        "raw_metrics":{
            "Top 1 Accuracy":63.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":774248,
            "title":"LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference",
            "url":"\/paper\/levit-a-vision-transformer-in-convnet-s",
            "published":"2021-04-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/levit-a-vision-transformer-in-convnet-s\/review\/?hl=29499"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    }
]