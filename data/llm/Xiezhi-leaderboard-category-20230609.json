[
    {
        "Model\/Category":"Human-Average",
        "Philosophy":0.453,
        "Economics":0.52,
        "Jurisprudence":0.46,
        "Pedagogy":0.51,
        "Literature":0.56,
        "History":0.46,
        "Science":0.394,
        "Engineering":0.38,
        "Agronomy":0.333,
        "Medicine":0.43,
        "Management":0.513,
        "Art Studies":0.4,
        "Xiezhi Overall":null,
        "MMLU Overall":null,
        "C-Eval Overall":null,
        "M3KE Overall":null
    },
    {
        "Model\/Category":"Human-Top",
        "Philosophy":0.856,
        "Economics":0.871,
        "Jurisprudence":0.761,
        "Pedagogy":0.854,
        "Literature":0.825,
        "History":0.854,
        "Science":0.926,
        "Engineering":0.928,
        "Agronomy":0.902,
        "Medicine":0.805,
        "Management":0.857,
        "Art Studies":0.821,
        "Xiezhi Overall":null,
        "MMLU Overall":null,
        "C-Eval Overall":null,
        "M3KE Overall":null
    },
    {
        "Model\/Category":"Alpaca-7b",
        "Philosophy":null,
        "Economics":null,
        "Jurisprudence":null,
        "Pedagogy":null,
        "Literature":null,
        "History":null,
        "Science":null,
        "Engineering":null,
        "Agronomy":null,
        "Medicine":null,
        "Management":null,
        "Art Studies":null,
        "Xiezhi Overall":null,
        "MMLU Overall":null,
        "C-Eval Overall":null,
        "M3KE Overall":0.142
    },
    {
        "Model\/Category":"Alpaca-lora-7b",
        "Philosophy":null,
        "Economics":null,
        "Jurisprudence":0.192,
        "Pedagogy":null,
        "Literature":0.206,
        "History":0.192,
        "Science":null,
        "Engineering":0.224,
        "Agronomy":null,
        "Medicine":null,
        "Management":null,
        "Art Studies":0.227,
        "Xiezhi Overall":0.194,
        "MMLU Overall":null,
        "C-Eval Overall":null,
        "M3KE Overall":null
    },
    {
        "Model\/Category":"Baize-healthcare-lora-7B",
        "Philosophy":null,
        "Economics":null,
        "Jurisprudence":null,
        "Pedagogy":null,
        "Literature":0.284,
        "History":0.181,
        "Science":null,
        "Engineering":null,
        "Agronomy":null,
        "Medicine":0.279,
        "Management":null,
        "Art Studies":0.417,
        "Xiezhi Overall":null,
        "MMLU Overall":null,
        "C-Eval Overall":null,
        "M3KE Overall":null
    },
    {
        "Model\/Category":"Baize-lora-13B",
        "Philosophy":null,
        "Economics":null,
        "Jurisprudence":null,
        "Pedagogy":0.244,
        "Literature":0.249,
        "History":null,
        "Science":null,
        "Engineering":null,
        "Agronomy":null,
        "Medicine":null,
        "Management":null,
        "Art Studies":0.229,
        "Xiezhi Overall":null,
        "MMLU Overall":null,
        "C-Eval Overall":null,
        "M3KE Overall":null
    },
    {
        "Model\/Category":"Baize-lora-30B",
        "Philosophy":null,
        "Economics":null,
        "Jurisprudence":null,
        "Pedagogy":null,
        "Literature":null,
        "History":null,
        "Science":null,
        "Engineering":null,
        "Agronomy":null,
        "Medicine":null,
        "Management":0.375,
        "Art Studies":null,
        "Xiezhi Overall":null,
        "MMLU Overall":null,
        "C-Eval Overall":null,
        "M3KE Overall":null
    },
    {
        "Model\/Category":"Baize-lora-7B",
        "Philosophy":null,
        "Economics":0.222,
        "Jurisprudence":0.23,
        "Pedagogy":null,
        "Literature":0.213,
        "History":0.202,
        "Science":null,
        "Engineering":null,
        "Agronomy":null,
        "Medicine":null,
        "Management":0.268,
        "Art Studies":null,
        "Xiezhi Overall":0.2,
        "MMLU Overall":null,
        "C-Eval Overall":null,
        "M3KE Overall":null
    },
    {
        "Model\/Category":"Belle-7B-0.2M",
        "Philosophy":0.228,
        "Economics":null,
        "Jurisprudence":0.217,
        "Pedagogy":0.251,
        "Literature":0.194,
        "History":0.214,
        "Science":0.191,
        "Engineering":null,
        "Agronomy":0.216,
        "Medicine":0.223,
        "Management":0.268,
        "Art Studies":0.238,
        "Xiezhi Overall":0.211,
        "MMLU Overall":null,
        "C-Eval Overall":null,
        "M3KE Overall":0.203
    },
    {
        "Model\/Category":"Belle-7B-0.6M",
        "Philosophy":null,
        "Economics":null,
        "Jurisprudence":null,
        "Pedagogy":null,
        "Literature":null,
        "History":null,
        "Science":0.197,
        "Engineering":null,
        "Agronomy":null,
        "Medicine":null,
        "Management":null,
        "Art Studies":null,
        "Xiezhi Overall":null,
        "MMLU Overall":null,
        "C-Eval Overall":null,
        "M3KE Overall":null
    },
    {
        "Model\/Category":"Belle-7B-1M",
        "Philosophy":0.226,
        "Economics":0.255,
        "Jurisprudence":0.199,
        "Pedagogy":0.237,
        "Literature":null,
        "History":0.207,
        "Science":0.21,
        "Engineering":0.215,
        "Agronomy":null,
        "Medicine":0.21,
        "Management":0.259,
        "Art Studies":null,
        "Xiezhi Overall":0.209,
        "MMLU Overall":null,
        "C-Eval Overall":null,
        "M3KE Overall":null
    },
    {
        "Model\/Category":"Bloomz-3b",
        "Philosophy":null,
        "Economics":null,
        "Jurisprudence":null,
        "Pedagogy":null,
        "Literature":null,
        "History":null,
        "Science":null,
        "Engineering":null,
        "Agronomy":null,
        "Medicine":null,
        "Management":null,
        "Art Studies":null,
        "Xiezhi Overall":null,
        "MMLU Overall":null,
        "C-Eval Overall":null,
        "M3KE Overall":null
    },
    {
        "Model\/Category":"Bloomz-7b1",
        "Philosophy":null,
        "Economics":null,
        "Jurisprudence":null,
        "Pedagogy":null,
        "Literature":null,
        "History":null,
        "Science":null,
        "Engineering":null,
        "Agronomy":null,
        "Medicine":null,
        "Management":null,
        "Art Studies":null,
        "Xiezhi Overall":null,
        "MMLU Overall":null,
        "C-Eval Overall":null,
        "M3KE Overall":null
    },
    {
        "Model\/Category":"Bloomz-7b1-mt",
        "Philosophy":null,
        "Economics":null,
        "Jurisprudence":null,
        "Pedagogy":null,
        "Literature":null,
        "History":null,
        "Science":null,
        "Engineering":null,
        "Agronomy":null,
        "Medicine":null,
        "Management":null,
        "Art Studies":null,
        "Xiezhi Overall":null,
        "MMLU Overall":null,
        "C-Eval Overall":null,
        "M3KE Overall":null
    },
    {
        "Model\/Category":"Bloomz-mt",
        "Philosophy":0.453,
        "Economics":0.31,
        "Jurisprudence":null,
        "Pedagogy":0.442,
        "Literature":0.405,
        "History":0.272,
        "Science":0.408,
        "Engineering":0.387,
        "Agronomy":0.366,
        "Medicine":null,
        "Management":null,
        "Art Studies":0.377,
        "Xiezhi Overall":0.337,
        "MMLU Overall":0.266,
        "C-Eval Overall":0.204,
        "M3KE Overall":0.161
    },
    {
        "Model\/Category":"ChatGPT",
        "Philosophy":0.477,
        "Economics":null,
        "Jurisprudence":0.213,
        "Pedagogy":0.28,
        "Literature":null,
        "History":0.233,
        "Science":0.22,
        "Engineering":0.412,
        "Agronomy":0.311,
        "Medicine":0.265,
        "Management":null,
        "Art Studies":0.339,
        "Xiezhi Overall":0.267,
        "MMLU Overall":0.24,
        "C-Eval Overall":0.286,
        "M3KE Overall":0.29
    },
    {
        "Model\/Category":"Doctorglm-6b",
        "Philosophy":null,
        "Economics":null,
        "Jurisprudence":null,
        "Pedagogy":null,
        "Literature":null,
        "History":null,
        "Science":null,
        "Engineering":null,
        "Agronomy":null,
        "Medicine":0.253,
        "Management":null,
        "Art Studies":null,
        "Xiezhi Overall":null,
        "MMLU Overall":null,
        "C-Eval Overall":null,
        "M3KE Overall":null
    },
    {
        "Model\/Category":"Falcon-40b-instruct",
        "Philosophy":null,
        "Economics":null,
        "Jurisprudence":null,
        "Pedagogy":null,
        "Literature":null,
        "History":null,
        "Science":null,
        "Engineering":null,
        "Agronomy":null,
        "Medicine":null,
        "Management":null,
        "Art Studies":null,
        "Xiezhi Overall":null,
        "MMLU Overall":null,
        "C-Eval Overall":null,
        "M3KE Overall":0.141
    },
    {
        "Model\/Category":"Falcon-7b",
        "Philosophy":null,
        "Economics":0.233,
        "Jurisprudence":null,
        "Pedagogy":null,
        "Literature":null,
        "History":null,
        "Science":null,
        "Engineering":0.228,
        "Agronomy":null,
        "Medicine":null,
        "Management":null,
        "Art Studies":null,
        "Xiezhi Overall":null,
        "MMLU Overall":null,
        "C-Eval Overall":null,
        "M3KE Overall":null
    },
    {
        "Model\/Category":"Falcon-7b-instruct",
        "Philosophy":null,
        "Economics":0.214,
        "Jurisprudence":null,
        "Pedagogy":null,
        "Literature":null,
        "History":null,
        "Science":null,
        "Engineering":null,
        "Agronomy":null,
        "Medicine":null,
        "Management":null,
        "Art Studies":null,
        "Xiezhi Overall":null,
        "MMLU Overall":null,
        "C-Eval Overall":null,
        "M3KE Overall":null
    },
    {
        "Model\/Category":"GPT-4",
        "Philosophy":0.413,
        "Economics":0.419,
        "Jurisprudence":0.368,
        "Pedagogy":0.472,
        "Literature":0.417,
        "History":0.437,
        "Science":0.436,
        "Engineering":0.42,
        "Agronomy":0.515,
        "Medicine":0.469,
        "Management":0.39,
        "Art Studies":0.437,
        "Xiezhi Overall":0.431,
        "MMLU Overall":0.402,
        "C-Eval Overall":0.413,
        "M3KE Overall":0.404
    },
    {
        "Model\/Category":"Llama-13b",
        "Philosophy":null,
        "Economics":null,
        "Jurisprudence":null,
        "Pedagogy":null,
        "Literature":null,
        "History":null,
        "Science":null,
        "Engineering":null,
        "Agronomy":null,
        "Medicine":null,
        "Management":null,
        "Art Studies":null,
        "Xiezhi Overall":null,
        "MMLU Overall":0.166,
        "C-Eval Overall":0.152,
        "M3KE Overall":null
    },
    {
        "Model\/Category":"Llama-65b",
        "Philosophy":null,
        "Economics":null,
        "Jurisprudence":null,
        "Pedagogy":null,
        "Literature":null,
        "History":null,
        "Science":null,
        "Engineering":null,
        "Agronomy":null,
        "Medicine":null,
        "Management":null,
        "Art Studies":null,
        "Xiezhi Overall":null,
        "MMLU Overall":0.143,
        "C-Eval Overall":0.154,
        "M3KE Overall":null
    },
    {
        "Model\/Category":"Llama-65b-hf",
        "Philosophy":null,
        "Economics":0.29,
        "Jurisprudence":0.323,
        "Pedagogy":0.237,
        "Literature":null,
        "History":null,
        "Science":null,
        "Engineering":null,
        "Agronomy":null,
        "Medicine":null,
        "Management":null,
        "Art Studies":null,
        "Xiezhi Overall":null,
        "MMLU Overall":null,
        "C-Eval Overall":null,
        "M3KE Overall":null
    },
    {
        "Model\/Category":"Llama-7b",
        "Philosophy":null,
        "Economics":null,
        "Jurisprudence":null,
        "Pedagogy":null,
        "Literature":null,
        "History":null,
        "Science":null,
        "Engineering":null,
        "Agronomy":null,
        "Medicine":null,
        "Management":null,
        "Art Studies":null,
        "Xiezhi Overall":null,
        "MMLU Overall":null,
        "C-Eval Overall":null,
        "M3KE Overall":0.158
    },
    {
        "Model\/Category":"Llama-7b-hf",
        "Philosophy":0.241,
        "Economics":0.234,
        "Jurisprudence":0.21,
        "Pedagogy":null,
        "Literature":null,
        "History":null,
        "Science":null,
        "Engineering":null,
        "Agronomy":null,
        "Medicine":null,
        "Management":null,
        "Art Studies":null,
        "Xiezhi Overall":null,
        "MMLU Overall":null,
        "C-Eval Overall":null,
        "M3KE Overall":0.158
    },
    {
        "Model\/Category":"Moss-moon-003-base",
        "Philosophy":null,
        "Economics":null,
        "Jurisprudence":null,
        "Pedagogy":null,
        "Literature":null,
        "History":null,
        "Science":null,
        "Engineering":null,
        "Agronomy":null,
        "Medicine":null,
        "Management":null,
        "Art Studies":0.224,
        "Xiezhi Overall":null,
        "MMLU Overall":null,
        "C-Eval Overall":null,
        "M3KE Overall":null
    },
    {
        "Model\/Category":"Pythia-1.4b",
        "Philosophy":0.321,
        "Economics":null,
        "Jurisprudence":null,
        "Pedagogy":0.241,
        "Literature":null,
        "History":null,
        "Science":null,
        "Engineering":null,
        "Agronomy":0.193,
        "Medicine":null,
        "Management":null,
        "Art Studies":null,
        "Xiezhi Overall":null,
        "MMLU Overall":null,
        "C-Eval Overall":null,
        "M3KE Overall":null
    },
    {
        "Model\/Category":"Pythia-2.8b",
        "Philosophy":null,
        "Economics":null,
        "Jurisprudence":null,
        "Pedagogy":null,
        "Literature":null,
        "History":null,
        "Science":null,
        "Engineering":null,
        "Agronomy":null,
        "Medicine":null,
        "Management":0.367,
        "Art Studies":null,
        "Xiezhi Overall":null,
        "MMLU Overall":null,
        "C-Eval Overall":null,
        "M3KE Overall":null
    },
    {
        "Model\/Category":"Stablelm-7b",
        "Philosophy":null,
        "Economics":null,
        "Jurisprudence":null,
        "Pedagogy":null,
        "Literature":null,
        "History":null,
        "Science":null,
        "Engineering":null,
        "Agronomy":null,
        "Medicine":null,
        "Management":null,
        "Art Studies":null,
        "Xiezhi Overall":null,
        "MMLU Overall":0.158,
        "C-Eval Overall":null,
        "M3KE Overall":0.14
    },
    {
        "Model\/Category":"Vicuna-13b-delta-v1.1",
        "Philosophy":0.223,
        "Economics":null,
        "Jurisprudence":null,
        "Pedagogy":null,
        "Literature":null,
        "History":null,
        "Science":null,
        "Engineering":null,
        "Agronomy":null,
        "Medicine":null,
        "Management":null,
        "Art Studies":null,
        "Xiezhi Overall":null,
        "MMLU Overall":null,
        "C-Eval Overall":null,
        "M3KE Overall":null
    },
    {
        "Model\/Category":"Vicuna-7b-delta-v1.1",
        "Philosophy":null,
        "Economics":null,
        "Jurisprudence":null,
        "Pedagogy":null,
        "Literature":null,
        "History":null,
        "Science":0.188,
        "Engineering":null,
        "Agronomy":null,
        "Medicine":null,
        "Management":null,
        "Art Studies":null,
        "Xiezhi Overall":0.191,
        "MMLU Overall":null,
        "C-Eval Overall":null,
        "M3KE Overall":null
    }
]