[
    {
        "Model":"GPT-4 (few-shot, k=10)",
        "Accuracy":95.3,
        "Paper":"https:\/\/paperswithcode.com\/paper\/gpt-4-technical-report-1",
        "Date":"2023-03-15"
    },
    {
        "Model":"TheBloke\/llama-2-70b-Guanaco-QLoRA-fp16 (10-shot)",
        "Accuracy":88.3,
        "Paper":"https:\/\/paperswithcode.com\/paper\/qlora-efficient-finetuning-of-quantized-llms",
        "Date":"2023-05-23"
    },
    {
        "Model":"PaLM 2-L (one-shot)",
        "Accuracy":87.4,
        "Paper":"https:\/\/paperswithcode.com\/paper\/palm-2-technical-report-1",
        "Date":"2023-05-17"
    },
    {
        "Model":"PaLM 2-M (one-shot)",
        "Accuracy":86.7,
        "Paper":"https:\/\/paperswithcode.com\/paper\/palm-2-technical-report-1",
        "Date":"2023-05-17"
    },
    {
        "Model":"MUPPET Roberta Large",
        "Accuracy":86.4,
        "Paper":"https:\/\/paperswithcode.com\/paper\/muppet-massive-multi-task-representations",
        "Date":"2021-01-26"
    },
    {
        "Model":"LLaMA-65B+CFG (zero-shot)",
        "Accuracy":86.3,
        "Paper":"https:\/\/paperswithcode.com\/paper\/stay-on-topic-with-classifier-free-guidance",
        "Date":"2023-06-30"
    },
    {
        "Model":"PaLM 2-S (one-shot)",
        "Accuracy":85.6,
        "Paper":"https:\/\/paperswithcode.com\/paper\/palm-2-technical-report-1",
        "Date":"2023-05-17"
    },
    {
        "Model":"GPT-3.5 (few-shot, k=10)",
        "Accuracy":85.5,
        "Paper":"https:\/\/paperswithcode.com\/paper\/gpt-4-technical-report-1",
        "Date":"2023-03-15"
    },
    {
        "Model":"LLaMA-30B+CFG (zero-shot)",
        "Accuracy":85.3,
        "Paper":"https:\/\/paperswithcode.com\/paper\/stay-on-topic-with-classifier-free-guidance",
        "Date":"2023-06-30"
    },
    {
        "Model":"LLaMA 2 70B (zero-shot)",
        "Accuracy":85.3,
        "Paper":"https:\/\/paperswithcode.com\/paper\/llama-2-open-foundation-and-fine-tuned-chat",
        "Date":"2023-07-18"
    },
    {
        "Model":"LLaMA 65B (zero-shot)",
        "Accuracy":84.2,
        "Paper":"https:\/\/paperswithcode.com\/paper\/llama-open-and-efficient-foundation-language-1",
        "Date":"2023-02-27"
    },
    {
        "Model":"PaLM-540B (Few-Shot)",
        "Accuracy":83.8,
        "Paper":"https:\/\/paperswithcode.com\/paper\/palm-scaling-language-modeling-with-pathways-1",
        "Date":"2022-04-05"
    },
    {
        "Model":"PaLM-540B (One-Shot)",
        "Accuracy":83.6,
        "Paper":"https:\/\/paperswithcode.com\/paper\/palm-scaling-language-modeling-with-pathways-1",
        "Date":"2022-04-05"
    },
    {
        "Model":"PaLM-540B (Zero-Shot)",
        "Accuracy":83.4,
        "Paper":"https:\/\/paperswithcode.com\/paper\/palm-scaling-language-modeling-with-pathways-1",
        "Date":"2022-04-05"
    },
    {
        "Model":"LLaMA 2 34B (zero-shot)",
        "Accuracy":83.3,
        "Paper":"https:\/\/paperswithcode.com\/paper\/llama-2-open-foundation-and-fine-tuned-chat",
        "Date":"2023-07-18"
    },
    {
        "Model":"LLaMA 33B (zero-shot)",
        "Accuracy":82.8,
        "Paper":"https:\/\/paperswithcode.com\/paper\/llama-open-and-efficient-foundation-language-1",
        "Date":"2023-02-27"
    },
    {
        "Model":"Megatron-Turing NLG 530B (Few-Shot)",
        "Accuracy":82.4,
        "Paper":"https:\/\/paperswithcode.com\/paper\/using-deepspeed-and-megatron-to-train",
        "Date":"2022-01-28"
    },
    {
        "Model":"LLaMA-13B+CFG (zero-shot)",
        "Accuracy":82.1,
        "Paper":"https:\/\/paperswithcode.com\/paper\/stay-on-topic-with-classifier-free-guidance",
        "Date":"2023-06-30"
    },
    {
        "Model":"Chinchilla (Zero-Shot)",
        "Accuracy":80.8,
        "Paper":"https:\/\/paperswithcode.com\/paper\/training-compute-optimal-large-language",
        "Date":"2022-03-29"
    },
    {
        "Model":"LLaMA 2 13B (zero-shot)",
        "Accuracy":80.7,
        "Paper":"https:\/\/paperswithcode.com\/paper\/llama-2-open-foundation-and-fine-tuned-chat",
        "Date":"2023-07-18"
    },
    {
        "Model":"Megatron-Turing NLG 530B (One-Shot)",
        "Accuracy":80.2,
        "Paper":"https:\/\/paperswithcode.com\/paper\/using-deepspeed-and-megatron-to-train",
        "Date":"2022-01-28"
    },
    {
        "Model":"GPT-3 175B (Few-Shot)",
        "Accuracy":79.3,
        "Paper":"https:\/\/paperswithcode.com\/paper\/language-models-are-few-shot-learners",
        "Date":"2020-05-28"
    },
    {
        "Model":"Gopher (zero-shot)",
        "Accuracy":79.2,
        "Paper":"https:\/\/paperswithcode.com\/paper\/scaling-language-models-methods-analysis-1",
        "Date":"2021-12-08"
    },
    {
        "Model":"LLaMA 13B (zero-shot)",
        "Accuracy":79.2,
        "Paper":"https:\/\/paperswithcode.com\/paper\/llama-open-and-efficient-foundation-language-1",
        "Date":"2023-02-27"
    },
    {
        "Model":"GPT-3 (zero-shot)",
        "Accuracy":78.9,
        "Paper":"https:\/\/paperswithcode.com\/paper\/language-models-are-few-shot-learners",
        "Date":"2020-05-28"
    },
    {
        "Model":"LLaMA 2 7B (zero-shot)",
        "Accuracy":77.2,
        "Paper":"https:\/\/paperswithcode.com\/paper\/llama-2-open-foundation-and-fine-tuned-chat",
        "Date":"2023-07-18"
    },
    {
        "Model":"LLaMA 7B (zero-shot)",
        "Accuracy":76.1,
        "Paper":"https:\/\/paperswithcode.com\/paper\/llama-open-and-efficient-foundation-language-1",
        "Date":"2023-02-27"
    },
    {
        "Model":"Blooomberg GPT (one-shot)",
        "Accuracy":73.92,
        "Paper":"https:\/\/paperswithcode.com\/paper\/bloomberggpt-a-large-language-model-for",
        "Date":"2023-03-30"
    },
    {
        "Model":"OPT 66B (one-shot)",
        "Accuracy":73.47,
        "Paper":"https:\/\/paperswithcode.com\/paper\/bloomberggpt-a-large-language-model-for",
        "Date":"2023-03-30"
    },
    {
        "Model":"BLOOM 176B (one-shot)",
        "Accuracy":73.21,
        "Paper":"https:\/\/paperswithcode.com\/paper\/bloomberggpt-a-large-language-model-for",
        "Date":"2023-03-30"
    },
    {
        "Model":"Sheared-LLaMA-2.7B (50B)",
        "Accuracy":70.8,
        "Paper":"https:\/\/paperswithcode.com\/paper\/sheared-llama-accelerating-language-model-pre",
        "Date":"2023-10-10"
    },
    {
        "Model":"GPT-NeoX (one-shot)",
        "Accuracy":68.37,
        "Paper":"https:\/\/paperswithcode.com\/paper\/bloomberggpt-a-large-language-model-for",
        "Date":"2023-03-30"
    },
    {
        "Model":"Open-LLaMA-3B-v2",
        "Accuracy":67.6,
        "Paper":"https:\/\/paperswithcode.com\/paper\/sheared-llama-accelerating-language-model-pre",
        "Date":"2023-10-10"
    },
    {
        "Model":"Sheared-LLaMA-1.3B (50B)",
        "Accuracy":60.7,
        "Paper":"https:\/\/paperswithcode.com\/paper\/sheared-llama-accelerating-language-model-pre",
        "Date":"2023-10-10"
    },
    {
        "Model":"FLAN 137B (zero-shot)",
        "Accuracy":56.7,
        "Paper":"https:\/\/paperswithcode.com\/paper\/finetuned-language-models-are-zero-shot",
        "Date":"2021-09-03"
    }
]