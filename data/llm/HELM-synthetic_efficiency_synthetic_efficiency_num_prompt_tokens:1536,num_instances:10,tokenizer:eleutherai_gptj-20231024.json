{
  "title": "num_prompt_tokens: 1536, num_instances: 10, tokenizer: eleutherai/gptj",
  "header": [
    {
      "value": "Model/adapter",
      "markdown": false,
      "metadata": {}
    },
    {
      "value": "# eval",
      "description": "Scenario introduced in this work to better understand inference runtime performance of various models.\n\n# eval: Number of evaluation instances.",
      "markdown": false,
      "metadata": {
        "metric": "# eval",
        "run_group": "Synthetic efficiency"
      }
    },
    {
      "value": "# train",
      "description": "Scenario introduced in this work to better understand inference runtime performance of various models.\n\n# train: Number of training instances (e.g., in-context examples).",
      "markdown": false,
      "metadata": {
        "metric": "# train",
        "run_group": "Synthetic efficiency"
      }
    },
    {
      "value": "truncated",
      "description": "Scenario introduced in this work to better understand inference runtime performance of various models.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
      "markdown": false,
      "metadata": {
        "metric": "truncated",
        "run_group": "Synthetic efficiency"
      }
    },
    {
      "value": "# prompt tokens",
      "description": "Scenario introduced in this work to better understand inference runtime performance of various models.\n\n# prompt tokens: Number of tokens in the prompt.",
      "markdown": false,
      "metadata": {
        "metric": "# prompt tokens",
        "run_group": "Synthetic efficiency"
      }
    },
    {
      "value": "# output tokens",
      "description": "Scenario introduced in this work to better understand inference runtime performance of various models.\n\n# output tokens: Actual number of output tokens.",
      "markdown": false,
      "metadata": {
        "metric": "# output tokens",
        "run_group": "Synthetic efficiency"
      }
    },
    {
      "value": "# trials",
      "description": "Scenario introduced in this work to better understand inference runtime performance of various models.\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
      "markdown": false,
      "metadata": {
        "metric": "# trials",
        "run_group": "Synthetic efficiency"
      }
    }
  ],
  "rows": [
    [
      {
        "value": "GPT-J (6B) [max_tokens: 1]",
        "description": "",
        "href": "?group=synthetic_efficiency&subgroup=num_prompt_tokens%3A%201536%2C%20num_instances%3A%2010%2C%20tokenizer%3A%20eleutherai%2Fgptj&runSpecs=%5B%22synthetic_efficiency%3Arandom%3DNone%2Cmodel%3Dtogether_gpt-j-6b%2Ctokenizer%3Deleutherai_gptj%2Cnum_prompt_tokens%3D1536%2Cnum_output_tokens%3D1%22%5D",
        "markdown": false
      },
      {
        "value": 10.0,
        "description": "min=10, mean=10, max=10, sum=10 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 0.0,
        "description": "min=0, mean=0, max=0, sum=0 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 0.0,
        "description": "min=0, mean=0, max=0, sum=0 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 1536.0,
        "description": "min=1536, mean=1536, max=1536, sum=1536 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 1.0,
        "description": "min=1, mean=1, max=1, sum=1 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 1.0,
        "description": "min=1, mean=1, max=1, sum=1 (1)",
        "style": {},
        "markdown": false
      }
    ],
    [
      {
        "value": "GPT-J (6B) [max_tokens: 16]",
        "description": "",
        "href": "?group=synthetic_efficiency&subgroup=num_prompt_tokens%3A%201536%2C%20num_instances%3A%2010%2C%20tokenizer%3A%20eleutherai%2Fgptj&runSpecs=%5B%22synthetic_efficiency%3Arandom%3DNone%2Cmodel%3Dtogether_gpt-j-6b%2Ctokenizer%3Deleutherai_gptj%2Cnum_prompt_tokens%3D1536%2Cnum_output_tokens%3D16%22%5D",
        "markdown": false
      },
      {
        "value": 10.0,
        "description": "min=10, mean=10, max=10, sum=10 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 0.0,
        "description": "min=0, mean=0, max=0, sum=0 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 0.0,
        "description": "min=0, mean=0, max=0, sum=0 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 1536.0,
        "description": "min=1536, mean=1536, max=1536, sum=1536 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 16.0,
        "description": "min=16, mean=16, max=16, sum=16 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 1.0,
        "description": "min=1, mean=1, max=1, sum=1 (1)",
        "style": {},
        "markdown": false
      }
    ],
    [
      {
        "value": "GPT-J (6B) [max_tokens: 2]",
        "description": "",
        "href": "?group=synthetic_efficiency&subgroup=num_prompt_tokens%3A%201536%2C%20num_instances%3A%2010%2C%20tokenizer%3A%20eleutherai%2Fgptj&runSpecs=%5B%22synthetic_efficiency%3Arandom%3DNone%2Cmodel%3Dtogether_gpt-j-6b%2Ctokenizer%3Deleutherai_gptj%2Cnum_prompt_tokens%3D1536%2Cnum_output_tokens%3D2%22%5D",
        "markdown": false
      },
      {
        "value": 10.0,
        "description": "min=10, mean=10, max=10, sum=10 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 0.0,
        "description": "min=0, mean=0, max=0, sum=0 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 0.0,
        "description": "min=0, mean=0, max=0, sum=0 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 1536.0,
        "description": "min=1536, mean=1536, max=1536, sum=1536 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 2.0,
        "description": "min=2, mean=2, max=2, sum=2 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 1.0,
        "description": "min=1, mean=1, max=1, sum=1 (1)",
        "style": {},
        "markdown": false
      }
    ],
    [
      {
        "value": "GPT-J (6B) [max_tokens: 32]",
        "description": "",
        "href": "?group=synthetic_efficiency&subgroup=num_prompt_tokens%3A%201536%2C%20num_instances%3A%2010%2C%20tokenizer%3A%20eleutherai%2Fgptj&runSpecs=%5B%22synthetic_efficiency%3Arandom%3DNone%2Cmodel%3Dtogether_gpt-j-6b%2Ctokenizer%3Deleutherai_gptj%2Cnum_prompt_tokens%3D1536%2Cnum_output_tokens%3D32%22%5D",
        "markdown": false
      },
      {
        "value": 10.0,
        "description": "min=10, mean=10, max=10, sum=10 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 0.0,
        "description": "min=0, mean=0, max=0, sum=0 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 0.0,
        "description": "min=0, mean=0, max=0, sum=0 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 1536.0,
        "description": "min=1536, mean=1536, max=1536, sum=1536 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 32.0,
        "description": "min=32, mean=32, max=32, sum=32 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 1.0,
        "description": "min=1, mean=1, max=1, sum=1 (1)",
        "style": {},
        "markdown": false
      }
    ],
    [
      {
        "value": "GPT-J (6B) [max_tokens: 4]",
        "description": "",
        "href": "?group=synthetic_efficiency&subgroup=num_prompt_tokens%3A%201536%2C%20num_instances%3A%2010%2C%20tokenizer%3A%20eleutherai%2Fgptj&runSpecs=%5B%22synthetic_efficiency%3Arandom%3DNone%2Cmodel%3Dtogether_gpt-j-6b%2Ctokenizer%3Deleutherai_gptj%2Cnum_prompt_tokens%3D1536%2Cnum_output_tokens%3D4%22%5D",
        "markdown": false
      },
      {
        "value": 10.0,
        "description": "min=10, mean=10, max=10, sum=10 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 0.0,
        "description": "min=0, mean=0, max=0, sum=0 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 0.0,
        "description": "min=0, mean=0, max=0, sum=0 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 1536.0,
        "description": "min=1536, mean=1536, max=1536, sum=1536 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 4.0,
        "description": "min=4, mean=4, max=4, sum=4 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 1.0,
        "description": "min=1, mean=1, max=1, sum=1 (1)",
        "style": {},
        "markdown": false
      }
    ],
    [
      {
        "value": "GPT-J (6B) [max_tokens: 64]",
        "description": "",
        "href": "?group=synthetic_efficiency&subgroup=num_prompt_tokens%3A%201536%2C%20num_instances%3A%2010%2C%20tokenizer%3A%20eleutherai%2Fgptj&runSpecs=%5B%22synthetic_efficiency%3Arandom%3DNone%2Cmodel%3Dtogether_gpt-j-6b%2Ctokenizer%3Deleutherai_gptj%2Cnum_prompt_tokens%3D1536%2Cnum_output_tokens%3D64%22%5D",
        "markdown": false
      },
      {
        "value": 10.0,
        "description": "min=10, mean=10, max=10, sum=10 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 0.0,
        "description": "min=0, mean=0, max=0, sum=0 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 0.0,
        "description": "min=0, mean=0, max=0, sum=0 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 1536.0,
        "description": "min=1536, mean=1536, max=1536, sum=1536 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 64.0,
        "description": "min=64, mean=64, max=64, sum=64 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 1.0,
        "description": "min=1, mean=1, max=1, sum=1 (1)",
        "style": {},
        "markdown": false
      }
    ],
    [
      {
        "value": "GPT-J (6B) [max_tokens: 8]",
        "description": "",
        "href": "?group=synthetic_efficiency&subgroup=num_prompt_tokens%3A%201536%2C%20num_instances%3A%2010%2C%20tokenizer%3A%20eleutherai%2Fgptj&runSpecs=%5B%22synthetic_efficiency%3Arandom%3DNone%2Cmodel%3Dtogether_gpt-j-6b%2Ctokenizer%3Deleutherai_gptj%2Cnum_prompt_tokens%3D1536%2Cnum_output_tokens%3D8%22%5D",
        "markdown": false
      },
      {
        "value": 10.0,
        "description": "min=10, mean=10, max=10, sum=10 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 0.0,
        "description": "min=0, mean=0, max=0, sum=0 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 0.0,
        "description": "min=0, mean=0, max=0, sum=0 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 1536.0,
        "description": "min=1536, mean=1536, max=1536, sum=1536 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 8.0,
        "description": "min=8, mean=8, max=8, sum=8 (1)",
        "style": {},
        "markdown": false
      },
      {
        "value": 1.0,
        "description": "min=1, mean=1, max=1, sum=1 (1)",
        "style": {},
        "markdown": false
      }
    ]
  ],
  "links": [
    {
      "text": "LaTeX",
      "href": "/nlp/scr4/nlp/crfm/yifanmai/helm-release/benchmark_output/releases/v0.3.0/groups/latex/synthetic_efficiency_synthetic_efficiency_num_prompt_tokens:1536,num_instances:10,tokenizer:eleutherai_gptj.tex"
    },
    {
      "text": "JSON",
      "href": "/nlp/scr4/nlp/crfm/yifanmai/helm-release/benchmark_output/releases/v0.3.0/groups/json/synthetic_efficiency_synthetic_efficiency_num_prompt_tokens:1536,num_instances:10,tokenizer:eleutherai_gptj.json"
    }
  ],
  "name": "synthetic_efficiency_num_prompt_tokens:1536,num_instances:10,tokenizer:eleutherai_gptj"
}