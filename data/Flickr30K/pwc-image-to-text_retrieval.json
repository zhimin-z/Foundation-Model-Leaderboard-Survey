[
    {
        "table_id":9727,
        "row_id":96190,
        "rank":1,
        "method":"BLIP-2 ViT-G (zero-shot, 1K test set)",
        "mlmodel":{

        },
        "method_short":"BLIP-2 ViT-G ",
        "method_details":"zero-shot, 1K test set",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Recall@1":"97.6",
            "Recall@5":"100",
            "Recall@10":"100",
            "Recall@Sum":null
        },
        "raw_metrics":{
            "Recall@1":97.6,
            "Recall@5":100.0,
            "Recall@10":100.0,
            "Recall@Sum":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96190"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":9727,
        "row_id":103016,
        "rank":2,
        "method":"ONE-PEACE(finetuned, w\/o ranking)",
        "mlmodel":{

        },
        "method_short":"ONE-PEACE",
        "method_details":"finetuned, w\/o ranking",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-18",
        "metrics":{
            "Recall@1":"97.6",
            "Recall@5":"100",
            "Recall@10":"100",
            "Recall@Sum":null
        },
        "raw_metrics":{
            "Recall@1":97.6,
            "Recall@5":100.0,
            "Recall@10":100.0,
            "Recall@Sum":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1211430,
            "title":"ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities",
            "url":"\/paper\/one-peace-exploring-one-general",
            "published":"2023-05-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/one-peace-exploring-one-general\/review\/?hl=103016"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":9727,
        "row_id":96189,
        "rank":3,
        "method":"BLIP-2 ViT-L (zero-shot, 1K test set)",
        "mlmodel":{

        },
        "method_short":"BLIP-2 ViT-L ",
        "method_details":"zero-shot, 1K test set",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-30",
        "metrics":{
            "Recall@1":"96.9",
            "Recall@5":"100",
            "Recall@10":"100",
            "Recall@Sum":null
        },
        "raw_metrics":{
            "Recall@1":96.9,
            "Recall@5":100.0,
            "Recall@10":100.0,
            "Recall@Sum":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1149122,
            "title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "url":"\/paper\/blip-2-bootstrapping-language-image-pre",
            "published":"2023-01-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/blip-2-bootstrapping-language-image-pre\/review\/?hl=96189"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":9727,
        "row_id":74111,
        "rank":4,
        "method":"ERNIE-ViL 2.0",
        "mlmodel":{

        },
        "method_short":"ERNIE-ViL 2.0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-30",
        "metrics":{
            "Recall@1":"96.1",
            "Recall@5":"99.9",
            "Recall@10":"100.0",
            "Recall@Sum":null
        },
        "raw_metrics":{
            "Recall@1":96.1,
            "Recall@5":99.9,
            "Recall@10":100.0,
            "Recall@Sum":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1084534,
            "title":"ERNIE-ViL 2.0: Multi-view Contrastive Learning for Image-Text Pre-training",
            "url":"\/paper\/ernie-vil-2-0-multi-view-contrastive-learning",
            "published":"2022-09-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ernie-vil-2-0-multi-view-contrastive-learning\/review\/?hl=74111"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":9727,
        "row_id":37027,
        "rank":5,
        "method":"ALBEF",
        "mlmodel":{

        },
        "method_short":"ALBEF",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-16",
        "metrics":{
            "Recall@1":"95.9",
            "Recall@5":"99.8",
            "Recall@10":"100.0",
            "Recall@Sum":null
        },
        "raw_metrics":{
            "Recall@1":95.9,
            "Recall@5":99.8,
            "Recall@10":100.0,
            "Recall@Sum":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":836928,
            "title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation",
            "url":"\/paper\/align-before-fuse-vision-and-language",
            "published":"2021-07-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/align-before-fuse-vision-and-language\/review\/?hl=37027"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":9727,
        "row_id":34383,
        "rank":6,
        "method":"GSMN",
        "mlmodel":{

        },
        "method_short":"GSMN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-04",
        "metrics":{
            "Recall@1":"76.4",
            "Recall@5":"94.3",
            "Recall@10":"97.3",
            "Recall@Sum":"268"
        },
        "raw_metrics":{
            "Recall@1":76.4,
            "Recall@5":94.3,
            "Recall@10":97.3,
            "Recall@Sum":268.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":811688,
            "title":"A Deep Local and Global Scene-Graph Matching for Image-Text Retrieval",
            "url":"\/paper\/a-deep-local-and-global-scene-graph-matching",
            "published":"2021-06-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/a-deep-local-and-global-scene-graph-matching\/review\/?hl=34383"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":9727,
        "row_id":34380,
        "rank":7,
        "method":"LGSGM",
        "mlmodel":{

        },
        "method_short":"LGSGM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-04",
        "metrics":{
            "Recall@1":"71",
            "Recall@5":"91.9",
            "Recall@10":"96.1",
            "Recall@Sum":"259"
        },
        "raw_metrics":{
            "Recall@1":71.0,
            "Recall@5":91.9,
            "Recall@10":96.1,
            "Recall@Sum":259.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":811688,
            "title":"A Deep Local and Global Scene-Graph Matching for Image-Text Retrieval",
            "url":"\/paper\/a-deep-local-and-global-scene-graph-matching",
            "published":"2021-06-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/a-deep-local-and-global-scene-graph-matching\/review\/?hl=34380"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]