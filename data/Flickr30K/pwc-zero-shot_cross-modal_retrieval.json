[
    {
        "table_id":10070,
        "row_id":64394,
        "rank":1,
        "Model":"BEiT-3",
        "mlmodel":{

        },
        "method_short":"BEiT-3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-08-22",
        "metrics":{
            "Image-to-text R@1":"94.9",
            "Image-to-text R@5":"99.9",
            "Image-to-text R@10":"100.0",
            "Text-to-image R@1":"81.5",
            "Text-to-image R@5":"95.6",
            "Text-to-image R@10":"97.8"
        },
        "raw_metrics":{
            "Image-to-text R@1":94.9,
            "Image-to-text R@5":99.9,
            "Image-to-text R@10":100.0,
            "Text-to-image R@1":81.5,
            "Text-to-image R@5":95.6,
            "Text-to-image R@10":97.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":1062207,
            "title":"Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks",
            "url":"\/paper\/image-as-a-foreign-language-beit-pretraining",
            "published":"2022-08-22T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":10070,
        "row_id":54005,
        "rank":2,
        "Model":"CoCa",
        "mlmodel":{

        },
        "method_short":"CoCa",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-04",
        "metrics":{
            "Image-to-text R@1":"92.5",
            "Image-to-text R@5":"99.5",
            "Image-to-text R@10":"99.9",
            "Text-to-image R@1":"80.4",
            "Text-to-image R@5":"95.7",
            "Text-to-image R@10":"97.7"
        },
        "raw_metrics":{
            "Image-to-text R@1":92.5,
            "Image-to-text R@5":99.5,
            "Image-to-text R@10":99.9,
            "Text-to-image R@1":80.4,
            "Text-to-image R@5":95.7,
            "Text-to-image R@10":97.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":1004211,
            "title":"CoCa: Contrastive Captioners are Image-Text Foundation Models",
            "url":"\/paper\/coca-contrastive-captioners-are-image-text",
            "published":"2022-05-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/coca-contrastive-captioners-are-image-text\/review\/?hl=54005"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":10070,
        "row_id":105532,
        "rank":3,
        "Model":"RO-ViT",
        "mlmodel":{

        },
        "method_short":"RO-ViT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-11",
        "metrics":{
            "Image-to-text R@1":"92.1",
            "Image-to-text R@5":"99.4",
            "Image-to-text R@10":"99.7",
            "Text-to-image R@1":"80.7",
            "Text-to-image R@5":"96.1",
            "Text-to-image R@10":"97.7"
        },
        "raw_metrics":{
            "Image-to-text R@1":92.1,
            "Image-to-text R@5":99.4,
            "Image-to-text R@10":99.7,
            "Text-to-image R@1":80.7,
            "Text-to-image R@5":96.1,
            "Text-to-image R@10":97.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":1206407,
            "title":"Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers",
            "url":"\/paper\/region-aware-pretraining-for-open-vocabulary",
            "published":"2023-05-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/region-aware-pretraining-for-open-vocabulary\/review\/?hl=105532"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":10070,
        "row_id":74112,
        "rank":4,
        "Model":"ERNIE-ViL 2.0",
        "mlmodel":{

        },
        "method_short":"ERNIE-ViL 2.0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-30",
        "metrics":{
            "Image-to-text R@1":"91.2",
            "Image-to-text R@5":"99.1",
            "Image-to-text R@10":"99.8",
            "Text-to-image R@1":"77.4",
            "Text-to-image R@5":"93.8",
            "Text-to-image R@10":"96.4"
        },
        "raw_metrics":{
            "Image-to-text R@1":91.2,
            "Image-to-text R@5":99.1,
            "Image-to-text R@10":99.8,
            "Text-to-image R@1":77.4,
            "Text-to-image R@5":93.8,
            "Text-to-image R@10":96.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":1084534,
            "title":"ERNIE-ViL 2.0: Multi-view Contrastive Learning for Image-Text Pre-training",
            "url":"\/paper\/ernie-vil-2-0-multi-view-contrastive-learning",
            "published":"2022-09-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ernie-vil-2-0-multi-view-contrastive-learning\/review\/?hl=74112"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":10070,
        "row_id":48602,
        "rank":5,
        "Model":"Florence",
        "mlmodel":{

        },
        "method_short":"Florence",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-22",
        "metrics":{
            "Image-to-text R@1":"90.9",
            "Image-to-text R@5":"99.1",
            "Image-to-text R@10":null,
            "Text-to-image R@1":"76.7",
            "Text-to-image R@5":"93.6",
            "Text-to-image R@10":null
        },
        "raw_metrics":{
            "Image-to-text R@1":90.9,
            "Image-to-text R@5":99.1,
            "Image-to-text R@10":null,
            "Text-to-image R@1":76.7,
            "Text-to-image R@5":93.6,
            "Text-to-image R@10":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":914419,
            "title":"Florence: A New Foundation Model for Computer Vision",
            "url":"\/paper\/florence-a-new-foundation-model-for-computer",
            "published":"2021-11-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/florence-a-new-foundation-model-for-computer\/review\/?hl=48602"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":10070,
        "row_id":48597,
        "rank":6,
        "Model":"ALBEF",
        "mlmodel":{

        },
        "method_short":"ALBEF",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-16",
        "metrics":{
            "Image-to-text R@1":"90.5",
            "Image-to-text R@5":"98.8",
            "Image-to-text R@10":"99.7",
            "Text-to-image R@1":"76.8",
            "Text-to-image R@5":"93.7",
            "Text-to-image R@10":"96.7"
        },
        "raw_metrics":{
            "Image-to-text R@1":90.5,
            "Image-to-text R@5":98.8,
            "Image-to-text R@10":99.7,
            "Text-to-image R@1":76.8,
            "Text-to-image R@5":93.7,
            "Text-to-image R@10":96.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":836928,
            "title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation",
            "url":"\/paper\/align-before-fuse-vision-and-language",
            "published":"2021-07-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/align-before-fuse-vision-and-language\/review\/?hl=48597"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":10070,
        "row_id":53742,
        "rank":7,
        "Model":"Flamingo",
        "mlmodel":{

        },
        "method_short":"Flamingo",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-29",
        "metrics":{
            "Image-to-text R@1":"89.3",
            "Image-to-text R@5":"98.8",
            "Image-to-text R@10":"99.7",
            "Text-to-image R@1":"79.5",
            "Text-to-image R@5":"95.3",
            "Text-to-image R@10":"97.9"
        },
        "raw_metrics":{
            "Image-to-text R@1":89.3,
            "Image-to-text R@5":98.8,
            "Image-to-text R@10":99.7,
            "Text-to-image R@1":79.5,
            "Text-to-image R@5":95.3,
            "Text-to-image R@10":97.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":1001838,
            "title":"Flamingo: a Visual Language Model for Few-Shot Learning",
            "url":"\/paper\/flamingo-a-visual-language-model-for-few-shot-1",
            "published":"2022-04-29T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":10070,
        "row_id":48596,
        "rank":8,
        "Model":"ALIGN",
        "mlmodel":{

        },
        "method_short":"ALIGN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-11",
        "metrics":{
            "Image-to-text R@1":"88.6",
            "Image-to-text R@5":"98.7",
            "Image-to-text R@10":"99.7",
            "Text-to-image R@1":"75.7",
            "Text-to-image R@5":"93.8",
            "Text-to-image R@10":"96.8"
        },
        "raw_metrics":{
            "Image-to-text R@1":88.6,
            "Image-to-text R@5":98.7,
            "Image-to-text R@10":99.7,
            "Text-to-image R@1":75.7,
            "Text-to-image R@5":93.8,
            "Text-to-image R@10":96.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":744362,
            "title":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision",
            "url":"\/paper\/scaling-up-visual-and-vision-language",
            "published":"2021-02-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-up-visual-and-vision-language\/review\/?hl=48596"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":10070,
        "row_id":48599,
        "rank":9,
        "Model":"CLIP",
        "mlmodel":{

        },
        "method_short":"CLIP",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-26",
        "metrics":{
            "Image-to-text R@1":"88.0",
            "Image-to-text R@5":"98.7",
            "Image-to-text R@10":"99.4",
            "Text-to-image R@1":"68.7",
            "Text-to-image R@5":"90.6",
            "Text-to-image R@10":"95.2"
        },
        "raw_metrics":{
            "Image-to-text R@1":88.0,
            "Image-to-text R@5":98.7,
            "Image-to-text R@10":99.4,
            "Text-to-image R@1":68.7,
            "Text-to-image R@5":90.6,
            "Text-to-image R@10":95.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":749733,
            "title":"Learning Transferable Visual Models From Natural Language Supervision",
            "url":"\/paper\/learning-transferable-visual-models-from",
            "published":"2021-02-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learning-transferable-visual-models-from\/review\/?hl=48599"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":10070,
        "row_id":87862,
        "rank":10,
        "Model":"PTP-BLIP (14M)",
        "mlmodel":{

        },
        "method_short":"PTP-BLIP ",
        "method_details":"14M",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-19",
        "metrics":{
            "Image-to-text R@1":"87.1",
            "Image-to-text R@5":"98.4",
            "Image-to-text R@10":"99.3",
            "Text-to-image R@1":"73.1",
            "Text-to-image R@5":"91.0",
            "Text-to-image R@10":"94.8"
        },
        "raw_metrics":{
            "Image-to-text R@1":87.1,
            "Image-to-text R@5":98.4,
            "Image-to-text R@10":99.3,
            "Text-to-image R@1":73.1,
            "Text-to-image R@5":91.0,
            "Text-to-image R@10":94.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":1130454,
            "title":"Position-guided Text Prompt for Vision-Language Pre-training",
            "url":"\/paper\/position-guided-text-prompt-for-vision",
            "published":"2022-12-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/position-guided-text-prompt-for-vision\/review\/?hl=87862"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":10070,
        "row_id":79796,
        "rank":11,
        "Model":"AltCLIP",
        "mlmodel":{

        },
        "method_short":"AltCLIP",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-12",
        "metrics":{
            "Image-to-text R@1":"86",
            "Image-to-text R@5":"98",
            "Image-to-text R@10":"99.1",
            "Text-to-image R@1":"72.5",
            "Text-to-image R@5":"91.6",
            "Text-to-image R@10":"95.4"
        },
        "raw_metrics":{
            "Image-to-text R@1":86.0,
            "Image-to-text R@5":98.0,
            "Image-to-text R@10":99.1,
            "Text-to-image R@1":72.5,
            "Text-to-image R@5":91.6,
            "Text-to-image R@10":95.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":1110840,
            "title":"AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities",
            "url":"\/paper\/altclip-altering-the-language-encoder-in-clip",
            "published":"2022-11-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/altclip-altering-the-language-encoder-in-clip\/review\/?hl=79796"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":10070,
        "row_id":48601,
        "rank":12,
        "Model":"UNITER",
        "mlmodel":{

        },
        "method_short":"UNITER",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-09-25",
        "metrics":{
            "Image-to-text R@1":"80.7",
            "Image-to-text R@5":"95.7",
            "Image-to-text R@10":"98.0",
            "Text-to-image R@1":"66.2",
            "Text-to-image R@5":"88.4",
            "Text-to-image R@10":"92.9"
        },
        "raw_metrics":{
            "Image-to-text R@1":80.7,
            "Image-to-text R@5":95.7,
            "Image-to-text R@10":98.0,
            "Text-to-image R@1":66.2,
            "Text-to-image R@5":88.4,
            "Text-to-image R@10":92.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":156206,
            "title":"UNITER: UNiversal Image-TExt Representation Learning",
            "url":"\/paper\/uniter-learning-universal-image-text-1",
            "published":"2019-09-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/uniter-learning-universal-image-text-1\/review\/?hl=48601"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":10070,
        "row_id":35252,
        "rank":13,
        "Model":"ViLT-B\/32",
        "mlmodel":{

        },
        "method_short":"ViLT-B\/32",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-05",
        "metrics":{
            "Image-to-text R@1":"73.2",
            "Image-to-text R@5":"93.6",
            "Image-to-text R@10":"96.5",
            "Text-to-image R@1":"55",
            "Text-to-image R@5":"82.5",
            "Text-to-image R@10":"89.8"
        },
        "raw_metrics":{
            "Image-to-text R@1":73.2,
            "Image-to-text R@5":93.6,
            "Image-to-text R@10":96.5,
            "Text-to-image R@1":55.0,
            "Text-to-image R@5":82.5,
            "Text-to-image R@10":89.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":742872,
            "title":"ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision",
            "url":"\/paper\/vilt-vision-and-language-transformer-without",
            "published":"2021-02-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vilt-vision-and-language-transformer-without\/review\/?hl=35252"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":10070,
        "row_id":48600,
        "rank":14,
        "Model":"ImageBERT",
        "mlmodel":{

        },
        "method_short":"ImageBERT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-01-22",
        "metrics":{
            "Image-to-text R@1":"70.7",
            "Image-to-text R@5":"90.2",
            "Image-to-text R@10":"94.0",
            "Text-to-image R@1":"54.3",
            "Text-to-image R@5":"79.6",
            "Text-to-image R@10":"87.5"
        },
        "raw_metrics":{
            "Image-to-text R@1":70.7,
            "Image-to-text R@5":90.2,
            "Image-to-text R@10":94.0,
            "Text-to-image R@1":54.3,
            "Text-to-image R@5":79.6,
            "Text-to-image R@10":87.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":180489,
            "title":"ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data",
            "url":"\/paper\/imagebert-cross-modal-pre-training-with-large",
            "published":"2020-01-22T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/imagebert-cross-modal-pre-training-with-large\/review\/?hl=48600"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":10070,
        "row_id":103941,
        "rank":15,
        "Model":"OpenCLIP VIT-H\/14",
        "mlmodel":{

        },
        "method_short":"OpenCLIP VIT-H\/14",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-14",
        "metrics":{
            "Image-to-text R@1":null,
            "Image-to-text R@5":"99.3",
            "Image-to-text R@10":null,
            "Text-to-image R@1":null,
            "Text-to-image R@5":"94.1",
            "Text-to-image R@10":null
        },
        "raw_metrics":{
            "Image-to-text R@1":null,
            "Image-to-text R@5":99.3,
            "Image-to-text R@10":null,
            "Text-to-image R@1":null,
            "Text-to-image R@5":94.1,
            "Text-to-image R@10":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1128601,
            "title":"Reproducible scaling laws for contrastive language-image learning",
            "url":"\/paper\/reproducible-scaling-laws-for-contrastive",
            "published":"2022-12-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/reproducible-scaling-laws-for-contrastive\/review\/?hl=103941"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":10070,
        "row_id":104616,
        "rank":16,
        "Model":"VAST",
        "mlmodel":{

        },
        "method_short":"VAST",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "Image-to-text R@1":null,
            "Image-to-text R@5":null,
            "Image-to-text R@10":null,
            "Text-to-image R@1":"90.4",
            "Text-to-image R@5":null,
            "Text-to-image R@10":null
        },
        "raw_metrics":{
            "Image-to-text R@1":null,
            "Image-to-text R@5":null,
            "Image-to-text R@10":null,
            "Text-to-image R@1":90.4,
            "Text-to-image R@5":null,
            "Text-to-image R@10":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]