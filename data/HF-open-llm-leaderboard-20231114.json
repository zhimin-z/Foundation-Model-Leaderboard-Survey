[
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":68.68,
        "ARC":64.59,
        "HellaSwag":85.69,
        "MMLU":76.35,
        "TruthfulQA":56.23,
        "Winogrande":83.03,
        "GSM8K":50.64,
        "DROP":64.2,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"custom",
        "#Params (B)":34.0,
        "Hub \u2764\ufe0f":15.0,
        "Available on the hub":true,
        "Model sha":"cd8d59de87ea11c6453ee287ac82e5523f08c8ec",
        "model_name_for_query":"01-ai\/Yi-34B"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":67.01,
        "ARC":71.42,
        "HellaSwag":87.53,
        "MMLU":69.88,
        "TruthfulQA":61.54,
        "Winogrande":83.19,
        "GSM8K":43.21,
        "DROP":52.31,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.98,
        "Hub \u2764\ufe0f":17.0,
        "Available on the hub":true,
        "Model sha":"7b78087db07eec97f7b461d10758ece76d685543",
        "model_name_for_query":"MayaPH\/GodziLLa2-70B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":66.98,
        "ARC":72.95,
        "HellaSwag":87.82,
        "MMLU":71.17,
        "TruthfulQA":64.46,
        "Winogrande":83.27,
        "GSM8K":39.5,
        "DROP":49.66,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":69.24,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"43efad8bfdb47139934e810906c1e59c25b5e269",
        "model_name_for_query":"sequelbox\/StellarBright"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":66.89,
        "ARC":71.84,
        "HellaSwag":87.94,
        "MMLU":70.48,
        "TruthfulQA":62.26,
        "Winogrande":82.72,
        "GSM8K":40.56,
        "DROP":52.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":148.0,
        "Available on the hub":true,
        "Model sha":"a66378c15f89756215ccc64572ba69b161173703",
        "model_name_for_query":"garage-bAInd\/Platypus2-70B-instruct"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":66.88,
        "ARC":71.08,
        "HellaSwag":87.89,
        "MMLU":70.58,
        "TruthfulQA":62.25,
        "Winogrande":83.58,
        "GSM8K":45.26,
        "DROP":47.49,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":214.0,
        "Available on the hub":true,
        "Model sha":"5f9c77b2c0397cf83d2f97740483f107c7109e8c",
        "model_name_for_query":"upstage\/SOLAR-0-70b-16bit"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":66.58,
        "ARC":70.82,
        "HellaSwag":87.92,
        "MMLU":70.39,
        "TruthfulQA":59.85,
        "Winogrande":82.79,
        "GSM8K":34.19,
        "DROP":60.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"6e3ce78eb5346bf3a5ee88cd60c25dc0d73de639",
        "model_name_for_query":"Sao10K\/Euryale-1.3-L2-70B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":66.55,
        "ARC":68.69,
        "HellaSwag":86.42,
        "MMLU":69.92,
        "TruthfulQA":58.85,
        "Winogrande":82.08,
        "GSM8K":44.81,
        "DROP":55.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"884c53a64a3c5faf7b0706d36a587ca1532ed8f5",
        "model_name_for_query":"psmathur\/model_101"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":66.47,
        "ARC":61.86,
        "HellaSwag":83.13,
        "MMLU":67.41,
        "TruthfulQA":56.18,
        "Winogrande":80.11,
        "GSM8K":60.27,
        "DROP":56.3,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":68.76,
        "Hub \u2764\ufe0f":41.0,
        "Available on the hub":true,
        "Model sha":"a6ee90d262ac729f90ed8de97127766df070074c",
        "model_name_for_query":"OpenBuddy\/openbuddy-llama2-70b-v10.1-bf16"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":66.34,
        "ARC":71.42,
        "HellaSwag":87.99,
        "MMLU":70.78,
        "TruthfulQA":62.66,
        "Winogrande":83.5,
        "GSM8K":33.74,
        "DROP":54.28,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":26.0,
        "Available on the hub":true,
        "Model sha":"32110b4f33e5e80073ca1f47638482fdc0e19297",
        "model_name_for_query":"budecosystem\/genz-70b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":66.1,
        "ARC":70.9,
        "HellaSwag":87.48,
        "MMLU":69.8,
        "TruthfulQA":60.97,
        "Winogrande":82.87,
        "GSM8K":32.22,
        "DROP":58.42,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":53.0,
        "Available on the hub":true,
        "Model sha":"8469429924dc2e1a9394b8095753985668a4052e",
        "model_name_for_query":"upstage\/Llama-2-70b-instruct"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":65.9,
        "ARC":70.05,
        "HellaSwag":87.55,
        "MMLU":67.82,
        "TruthfulQA":65.02,
        "Winogrande":83.27,
        "GSM8K":29.95,
        "DROP":57.68,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":30.0,
        "Available on the hub":true,
        "Model sha":"49e5b5ee0bed2864f0b38ba8bf9e01ccc5e0ba5f",
        "model_name_for_query":"ehartford\/Samantha-1.11-70b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":65.88,
        "ARC":71.76,
        "HellaSwag":88.2,
        "MMLU":70.99,
        "TruthfulQA":65.26,
        "Winogrande":82.64,
        "GSM8K":41.47,
        "DROP":40.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":38.0,
        "Available on the hub":true,
        "Model sha":"05941a3eaacff0dead79b09d2175b5d7b98c525b",
        "model_name_for_query":"ICBU-NPU\/FashionGPT-70B-V1.1"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":65.8,
        "ARC":71.67,
        "HellaSwag":87.5,
        "MMLU":70.03,
        "TruthfulQA":59.36,
        "Winogrande":83.5,
        "GSM8K":30.63,
        "DROP":57.92,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.98,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"08239fb1e30b1e42b14370f23e942bc51e76027c",
        "model_name_for_query":"chargoddard\/MelangeB-70b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":65.74,
        "ARC":68.77,
        "HellaSwag":87.46,
        "MMLU":68.6,
        "TruthfulQA":64.85,
        "Winogrande":83.27,
        "GSM8K":31.61,
        "DROP":55.59,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"a3819d186f5b4d52ced7ddeb7fa16bf66e8a2ea7",
        "model_name_for_query":"ehartford\/Samantha-1.1-70b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":65.52,
        "ARC":73.55,
        "HellaSwag":87.62,
        "MMLU":70.67,
        "TruthfulQA":64.41,
        "Winogrande":83.43,
        "GSM8K":33.28,
        "DROP":45.65,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":16.0,
        "Available on the hub":true,
        "Model sha":"55a30d29db194832c0b5de1392a6598a63582144",
        "model_name_for_query":"AIDC-ai-business\/Marcoroni-70B-v1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":65.41,
        "ARC":72.1,
        "HellaSwag":87.46,
        "MMLU":71.02,
        "TruthfulQA":61.18,
        "Winogrande":82.87,
        "GSM8K":30.78,
        "DROP":52.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"b9b8560832276f60ba6bf37ac913b230a85ac19b",
        "model_name_for_query":"fangloveskari\/Platypus_QLoRA_LLaMA_70b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":65.4,
        "ARC":67.58,
        "HellaSwag":87.52,
        "MMLU":69.11,
        "TruthfulQA":61.79,
        "Winogrande":82.32,
        "GSM8K":30.48,
        "DROP":59.03,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":70.0,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":false,
        "Model sha":"4bff676fe29f56d31961794c062aebc36312446e",
        "model_name_for_query":"v2ray\/LLaMA-2-Wizard-70B-QLoRA"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":65.32,
        "ARC":76.79,
        "HellaSwag":87.76,
        "MMLU":66.35,
        "TruthfulQA":55.09,
        "Winogrande":77.58,
        "GSM8K":45.64,
        "DROP":47.99,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":68.95,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":false,
        "Model sha":"7e506c4a056821e5d151a0e46572cd74d04194be",
        "model_name_for_query":"TigerResearch\/tigerbot-70b-chat"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":65.24,
        "ARC":76.79,
        "HellaSwag":87.83,
        "MMLU":66.08,
        "TruthfulQA":55.1,
        "Winogrande":77.9,
        "GSM8K":44.96,
        "DROP":48.02,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":68.95,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":false,
        "Model sha":"7e506c4a056821e5d151a0e46572cd74d04194be",
        "model_name_for_query":"TigerResearch\/tigerbot-70b-chat"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":65.03,
        "ARC":71.59,
        "HellaSwag":87.7,
        "MMLU":69.43,
        "TruthfulQA":60.72,
        "Winogrande":82.32,
        "GSM8K":39.42,
        "DROP":44.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"5020869e6394b1ac039bf80a0a1d2bed6be6707e",
        "model_name_for_query":"psmathur\/model_009"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":65.01,
        "ARC":69.71,
        "HellaSwag":87.95,
        "MMLU":69.79,
        "TruthfulQA":59.49,
        "Winogrande":82.95,
        "GSM8K":44.88,
        "DROP":40.27,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":true,
        "Model sha":"eadc78a4a9e173bccdca7dc8d12a34e80317c66c",
        "model_name_for_query":"jondurbin\/airoboros-l2-70b-2.2.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":64.97,
        "ARC":71.08,
        "HellaSwag":86.37,
        "MMLU":68.79,
        "TruthfulQA":59.44,
        "Winogrande":82.95,
        "GSM8K":35.86,
        "DROP":50.28,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":836.0,
        "Available on the hub":true,
        "Model sha":"e4944caa6ece819413b140b8dcecea79fe7e22cf",
        "model_name_for_query":"stabilityai\/StableBeluga2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":64.9,
        "ARC":71.25,
        "HellaSwag":87.85,
        "MMLU":70.18,
        "TruthfulQA":61.27,
        "Winogrande":82.72,
        "GSM8K":40.86,
        "DROP":40.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":16.0,
        "Available on the hub":true,
        "Model sha":"c1d4f997f8ed685a6efc72229523b2e56fd0774b",
        "model_name_for_query":"psmathur\/orca_mini_v3_70b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":64.88,
        "ARC":68.43,
        "HellaSwag":86.71,
        "MMLU":69.31,
        "TruthfulQA":57.18,
        "Winogrande":81.77,
        "GSM8K":32.37,
        "DROP":58.43,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"9542702011bf4d282f4b0f0bd79229f5822b6313",
        "model_name_for_query":"psmathur\/model_51"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":64.76,
        "ARC":68.94,
        "HellaSwag":86.9,
        "MMLU":69.37,
        "TruthfulQA":53.67,
        "Winogrande":82.95,
        "GSM8K":31.77,
        "DROP":59.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":70.0,
        "Hub \u2764\ufe0f":14.0,
        "Available on the hub":false,
        "Model sha":"e552ddca841a2b86e36bbe5f99840afedfdbcd14",
        "model_name_for_query":"v2ray\/LLaMA-2-Jannie-70B-QLoRA"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":64.67,
        "ARC":72.27,
        "HellaSwag":87.74,
        "MMLU":70.23,
        "TruthfulQA":63.37,
        "Winogrande":83.66,
        "GSM8K":28.35,
        "DROP":47.04,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":49.0,
        "Available on the hub":true,
        "Model sha":"ef9b04ef02ccc4d96f1181467da92bb6b5baf835",
        "model_name_for_query":"fangloveskari\/ORCA_LLaMA_70B_QLoRA"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":64.63,
        "ARC":68.77,
        "HellaSwag":87.57,
        "MMLU":68.81,
        "TruthfulQA":57.69,
        "Winogrande":83.9,
        "GSM8K":35.25,
        "DROP":50.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":17.0,
        "Available on the hub":true,
        "Model sha":"7b687d6e4101b8bb8cc4062f8a318d639098a55d",
        "model_name_for_query":"migtissera\/Synthia-70B-v1.2b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":64.3,
        "ARC":65.1,
        "HellaSwag":86.19,
        "MMLU":64.6,
        "TruthfulQA":54.97,
        "Winogrande":82.64,
        "GSM8K":41.62,
        "DROP":54.98,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"?",
        "#Params (B)":178.64,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"7d7b93ffd67d1b0c39f3503050dbbcc951948120",
        "model_name_for_query":"OpenBuddy\/openbuddy-falcon-180b-v13-preview0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":64.23,
        "ARC":71.08,
        "HellaSwag":87.6,
        "MMLU":70.04,
        "TruthfulQA":58.09,
        "Winogrande":83.82,
        "GSM8K":22.9,
        "DROP":56.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":12.0,
        "Available on the hub":true,
        "Model sha":"b9f8de09ab860ee8ba570db7227c5444020ea056",
        "model_name_for_query":"garage-bAInd\/Camel-Platypus2-70B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":64.16,
        "ARC":70.65,
        "HellaSwag":87.15,
        "MMLU":70.08,
        "TruthfulQA":52.37,
        "Winogrande":84.37,
        "GSM8K":33.06,
        "DROP":51.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":19.0,
        "Available on the hub":true,
        "Model sha":"16b6583ad58313331f86be18e531ab03f1857695",
        "model_name_for_query":"garage-bAInd\/Platypus2-70B"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":64.13,
        "ARC":70.31,
        "HellaSwag":86.39,
        "MMLU":69.29,
        "TruthfulQA":54.02,
        "Winogrande":82.87,
        "GSM8K":28.89,
        "DROP":57.15,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"08115ee077953e9c01c6a40f5086def3ecf9f5f0",
        "model_name_for_query":"liuxiang886\/llama2-70B-qlora-gpt4"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":64.05,
        "ARC":70.14,
        "HellaSwag":87.71,
        "MMLU":69.83,
        "TruthfulQA":57.77,
        "Winogrande":82.95,
        "GSM8K":23.96,
        "DROP":55.97,
        "Type":"instruction-tuned",
        "Precision":"8bit",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":12.0,
        "Available on the hub":true,
        "Model sha":"6f958a1063fe1e6075f6e379fae621ff5a1d98c6",
        "model_name_for_query":"garage-bAInd\/Camel-Platypus2-70B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":63.58,
        "ARC":69.03,
        "HellaSwag":87.17,
        "MMLU":71.04,
        "TruthfulQA":52.41,
        "Winogrande":84.21,
        "GSM8K":32.07,
        "DROP":49.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":70.0,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":false,
        "Model sha":"d55e05e9d67418c639933c85a5b9d17c6f531a92",
        "model_name_for_query":"s1ghhh\/medllama-2-70b-qlora-1.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":63.41,
        "ARC":70.48,
        "HellaSwag":86.98,
        "MMLU":70.13,
        "TruthfulQA":58.64,
        "Winogrande":83.27,
        "GSM8K":31.92,
        "DROP":42.42,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":16.0,
        "Available on the hub":true,
        "Model sha":"9b92ee1093b125035ba1649dca6f4ceb9d86a656",
        "model_name_for_query":"migtissera\/Synthia-70B-v1.2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":63.2,
        "ARC":71.08,
        "HellaSwag":87.65,
        "MMLU":69.04,
        "TruthfulQA":63.12,
        "Winogrande":83.35,
        "GSM8K":37.15,
        "DROP":31.05,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":19.0,
        "Available on the hub":true,
        "Model sha":"0f5d81b13718a866cb078bd8762ab80a41972663",
        "model_name_for_query":"psmathur\/model_007"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":63.1,
        "ARC":68.86,
        "HellaSwag":86.43,
        "MMLU":64.77,
        "TruthfulQA":59.7,
        "Winogrande":81.06,
        "GSM8K":26.23,
        "DROP":54.69,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":9.0,
        "Available on the hub":true,
        "Model sha":"b95668861dfb7b0abca44ccdbef2db49b2dd8917",
        "model_name_for_query":"upstage\/llama-65b-instruct"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":63.01,
        "ARC":69.28,
        "HellaSwag":87.59,
        "MMLU":69.51,
        "TruthfulQA":59.05,
        "Winogrande":84.06,
        "GSM8K":34.65,
        "DROP":36.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":69.24,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"a87cb1756d7b7389cc5a6d4647cf53377e962aea",
        "model_name_for_query":"sequelbox\/SharpBalance"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":62.84,
        "ARC":70.05,
        "HellaSwag":87.12,
        "MMLU":70.34,
        "TruthfulQA":57.84,
        "Winogrande":83.66,
        "GSM8K":31.84,
        "DROP":39.02,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"05a13f6adfe95a713dff04dc2eaa214c77c2512a",
        "model_name_for_query":"migtissera\/Synthia-70B-v1.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":62.78,
        "ARC":72.1,
        "HellaSwag":87.4,
        "MMLU":69.91,
        "TruthfulQA":65.81,
        "Winogrande":82.32,
        "GSM8K":22.14,
        "DROP":39.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":48.0,
        "Available on the hub":true,
        "Model sha":"46b78b9a10e78283e59c28b56cb59c2f33b0816a",
        "model_name_for_query":"uni-tianyan\/Uni-TianYan"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":62.62,
        "ARC":62.8,
        "HellaSwag":83.6,
        "MMLU":62.01,
        "TruthfulQA":55.09,
        "Winogrande":79.95,
        "GSM8K":43.37,
        "DROP":51.5,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":65.07,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"445b77821fac8e6cfb77d0399fb827400b5bb71e",
        "model_name_for_query":"OpenBuddy\/openbuddy-llama-65b-v8-bf16"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":62.1,
        "ARC":62.46,
        "HellaSwag":83.61,
        "MMLU":65.49,
        "TruthfulQA":52.76,
        "Winogrande":80.19,
        "GSM8K":37.76,
        "DROP":52.45,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":68.95,
        "Hub \u2764\ufe0f":11.0,
        "Available on the hub":true,
        "Model sha":"8af85526293eb8625375f3f7a1bab69825176e48",
        "model_name_for_query":"TigerResearch\/tigerbot-70b-base"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":62.02,
        "ARC":71.42,
        "HellaSwag":87.31,
        "MMLU":68.58,
        "TruthfulQA":62.65,
        "Winogrande":84.14,
        "GSM8K":28.66,
        "DROP":31.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"3d95e0f3598f7a76ab97cb2cc0e4aae957d77479",
        "model_name_for_query":"psmathur\/model_007_v2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":61.76,
        "ARC":70.39,
        "HellaSwag":86.54,
        "MMLU":68.89,
        "TruthfulQA":55.55,
        "Winogrande":81.61,
        "GSM8K":15.24,
        "DROP":54.1,
        "Type":"fine-tuned",
        "Precision":"GPTQ",
        "Hub License":"llama2",
        "#Params (B)":72.82,
        "Hub \u2764\ufe0f":14.0,
        "Available on the hub":true,
        "Model sha":"23ed580cb77ebaee49ea11eb4538fd3ab3795b76",
        "model_name_for_query":"TheBloke\/Airoboros-L2-70B-2.1-GPTQ"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":61.54,
        "ARC":62.29,
        "HellaSwag":86.07,
        "MMLU":64.25,
        "TruthfulQA":53.75,
        "Winogrande":80.66,
        "GSM8K":34.57,
        "DROP":49.21,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"1da59e150f1d0bae67f66400738a01d408a8c45d",
        "model_name_for_query":"luffycodes\/higgs-llama-vicuna-ep25-70b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":61.22,
        "ARC":71.67,
        "HellaSwag":87.6,
        "MMLU":70.37,
        "TruthfulQA":58.13,
        "Winogrande":83.98,
        "GSM8K":0.0,
        "DROP":56.81,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.98,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"e54a2b924dec135f3fa2373933ab8485178cde1b",
        "model_name_for_query":"chargoddard\/MelangeC-70b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":60.97,
        "ARC":68.34,
        "HellaSwag":87.21,
        "MMLU":69.52,
        "TruthfulQA":46.46,
        "Winogrande":84.29,
        "GSM8K":42.68,
        "DROP":28.26,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"45444ac60488594e0700e6c7313ff444b4468240",
        "model_name_for_query":"Brillibits\/Instruct_Llama70B_Dolly15k"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":60.78,
        "ARC":68.52,
        "HellaSwag":87.89,
        "MMLU":70.41,
        "TruthfulQA":49.79,
        "Winogrande":83.5,
        "GSM8K":24.72,
        "DROP":40.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":12.0,
        "Available on the hub":true,
        "Model sha":"f16526d9bb814dc10adc911f94e8c7a520beb5b6",
        "model_name_for_query":"jondurbin\/airoboros-l2-70b-gpt4-2.0"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":60.72,
        "ARC":70.14,
        "HellaSwag":87.52,
        "MMLU":69.33,
        "TruthfulQA":57.65,
        "Winogrande":82.24,
        "GSM8K":42.61,
        "DROP":15.52,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.98,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"c775f87a56f00725de4263f8d527995d40f611c4",
        "model_name_for_query":"elinas\/chronos007-70b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":60.54,
        "ARC":62.29,
        "HellaSwag":83.8,
        "MMLU":55.92,
        "TruthfulQA":53.05,
        "Winogrande":82.08,
        "GSM8K":41.24,
        "DROP":45.41,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":178.64,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"4f1aeb136860ee3216f23faec0c598014e5c40a6",
        "model_name_for_query":"OpenBuddy\/openbuddy-falcon-180b-v12-preview0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":60.53,
        "ARC":63.91,
        "HellaSwag":86.21,
        "MMLU":64.75,
        "TruthfulQA":51.32,
        "Winogrande":82.08,
        "GSM8K":28.13,
        "DROP":47.29,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":68.98,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"6e7fdcd20626786dd744ea86c664a3c088ced39f",
        "model_name_for_query":"Faradaylab\/ARIA-70B-V3"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":60.29,
        "ARC":69.45,
        "HellaSwag":87.11,
        "MMLU":68.91,
        "TruthfulQA":59.79,
        "Winogrande":83.66,
        "GSM8K":31.39,
        "DROP":21.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"d63dfdd0baed756981f5f78f7419fd822c572362",
        "model_name_for_query":"migtissera\/Synthia-70B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":60.12,
        "ARC":67.75,
        "HellaSwag":85.83,
        "MMLU":69.22,
        "TruthfulQA":51.79,
        "Winogrande":81.93,
        "GSM8K":23.81,
        "DROP":40.52,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"e95fd7daf017e7c414ec07ebef4ddf013c16f9a4",
        "model_name_for_query":"llm-agents\/tora-70b-v1.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":60.05,
        "ARC":68.6,
        "HellaSwag":87.53,
        "MMLU":69.37,
        "TruthfulQA":48.52,
        "Winogrande":83.9,
        "GSM8K":17.66,
        "DROP":44.74,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":12.0,
        "Available on the hub":true,
        "Model sha":"f16526d9bb814dc10adc911f94e8c7a520beb5b6",
        "model_name_for_query":"jondurbin\/airoboros-l2-70b-gpt4-2.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":59.93,
        "ARC":71.84,
        "HellaSwag":86.89,
        "MMLU":69.37,
        "TruthfulQA":64.79,
        "Winogrande":81.22,
        "GSM8K":14.25,
        "DROP":31.14,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"7feeb5b665ab1ecdfd9cc4fe45fadb86b7b91b5b",
        "model_name_for_query":"oh-yeontaek\/llama-2-70B-LoRA-assemble-v2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":59.77,
        "ARC":68.26,
        "HellaSwag":87.65,
        "MMLU":70.0,
        "TruthfulQA":48.76,
        "Winogrande":83.66,
        "GSM8K":45.94,
        "DROP":14.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"ca3789cd6b683e97dcd6a5f0367f90a63d7a4e7b",
        "model_name_for_query":"psmathur\/test_42_70b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":59.35,
        "ARC":68.0,
        "HellaSwag":86.85,
        "MMLU":69.31,
        "TruthfulQA":50.98,
        "Winogrande":82.32,
        "GSM8K":44.66,
        "DROP":13.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"783a3c7d5d0a75e6e11074f2577b90dd219ef7b1",
        "model_name_for_query":"meta-math\/MetaMath-70B-V1.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":59.31,
        "ARC":65.02,
        "HellaSwag":84.88,
        "MMLU":62.19,
        "TruthfulQA":46.06,
        "Winogrande":80.51,
        "GSM8K":26.69,
        "DROP":49.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":65.0,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":false,
        "Model sha":"aa5bd88bd132925cf2dd5c44eceafdb5ed5e5be4",
        "model_name_for_query":"HiTZ\/alpaca-lora-65b-en-pt-es-ca"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":59.1,
        "ARC":69.45,
        "HellaSwag":88.86,
        "MMLU":70.5,
        "TruthfulQA":45.47,
        "Winogrande":86.9,
        "GSM8K":45.94,
        "DROP":6.57,
        "Type":"pretrained",
        "Precision":"8bit",
        "Hub License":"unknown",
        "#Params (B)":179.52,
        "Hub \u2764\ufe0f":830.0,
        "Available on the hub":false,
        "Model sha":"71a1a70b629e9963f7b4601e82f3f9079d48011e",
        "model_name_for_query":"tiiuae\/falcon-180B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":59.08,
        "ARC":70.05,
        "HellaSwag":87.83,
        "MMLU":70.67,
        "TruthfulQA":49.79,
        "Winogrande":83.58,
        "GSM8K":25.4,
        "DROP":26.2,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":true,
        "Model sha":"1cccd0b60a988bf6ddc4e2688895837845afa076",
        "model_name_for_query":"jondurbin\/airoboros-l2-70b-gpt4-m2.0"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":59.06,
        "ARC":70.14,
        "HellaSwag":87.54,
        "MMLU":70.23,
        "TruthfulQA":60.49,
        "Winogrande":83.43,
        "GSM8K":30.93,
        "DROP":10.68,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"b366c0bb318ae592023cca894cc6b4421a607a0d",
        "model_name_for_query":"lizpreciatior\/lzlv_70b_fp16_hf"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":58.99,
        "ARC":67.06,
        "HellaSwag":84.98,
        "MMLU":63.48,
        "TruthfulQA":53.51,
        "Winogrande":81.14,
        "GSM8K":32.75,
        "DROP":30.0,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":false,
        "Model sha":"",
        "model_name_for_query":"openbmb\/UltraLM-65b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":58.89,
        "ARC":62.37,
        "HellaSwag":82.29,
        "MMLU":58.18,
        "TruthfulQA":52.6,
        "Winogrande":77.51,
        "GSM8K":31.61,
        "DROP":47.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.35,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":true,
        "Model sha":"85f7ad9d6ff016312262a47d45ffd07dee54aab0",
        "model_name_for_query":"OpenBuddyEA\/openbuddy-llama-30b-v7.1-bf16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":58.87,
        "ARC":70.22,
        "HellaSwag":87.25,
        "MMLU":69.77,
        "TruthfulQA":59.86,
        "Winogrande":82.87,
        "GSM8K":27.22,
        "DROP":14.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":159.0,
        "Available on the hub":true,
        "Model sha":"d6c803a180e3d46c371f8d3cb3848b861596ccbc",
        "model_name_for_query":"Xwin-LM\/Xwin-LM-70B-V0.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":58.87,
        "ARC":65.78,
        "HellaSwag":84.79,
        "MMLU":63.49,
        "TruthfulQA":52.45,
        "Winogrande":80.98,
        "GSM8K":13.87,
        "DROP":50.73,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":18.0,
        "Available on the hub":true,
        "Model sha":"836cf4dcd60ebe2ff09415c72f809d94639e8d35",
        "model_name_for_query":"lilloukas\/GPlatty-30B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":58.85,
        "ARC":71.08,
        "HellaSwag":87.32,
        "MMLU":70.7,
        "TruthfulQA":63.92,
        "Winogrande":83.66,
        "GSM8K":28.13,
        "DROP":7.13,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"060c096af49700760f734c0102250a524d46b3eb",
        "model_name_for_query":"ICBU-NPU\/FashionGPT-70B-V1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":58.85,
        "ARC":73.04,
        "HellaSwag":88.15,
        "MMLU":70.11,
        "TruthfulQA":65.15,
        "Winogrande":82.56,
        "GSM8K":24.03,
        "DROP":8.9,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"990a1664fc058de6ee2406af62c0a817d7047304",
        "model_name_for_query":"ICBU-NPU\/FashionGPT-70B-V1.2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":58.83,
        "ARC":62.46,
        "HellaSwag":82.3,
        "MMLU":58.15,
        "TruthfulQA":52.57,
        "Winogrande":77.82,
        "GSM8K":30.93,
        "DROP":47.55,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":32.35,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":true,
        "Model sha":"85f7ad9d6ff016312262a47d45ffd07dee54aab0",
        "model_name_for_query":"OpenBuddyEA\/openbuddy-llama-30b-v7.1-bf16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":58.57,
        "ARC":66.13,
        "HellaSwag":85.99,
        "MMLU":63.89,
        "TruthfulQA":51.32,
        "Winogrande":79.95,
        "GSM8K":13.65,
        "DROP":49.07,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"4373e66135c6fb4a6063777c4270a34509e7e932",
        "model_name_for_query":"jondurbin\/airoboros-65b-gpt4-1.3"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":58.56,
        "ARC":64.93,
        "HellaSwag":84.94,
        "MMLU":61.9,
        "TruthfulQA":56.3,
        "Winogrande":79.56,
        "GSM8K":17.82,
        "DROP":44.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":102.0,
        "Available on the hub":true,
        "Model sha":"be44a37814a20e790063086703f570732597887a",
        "model_name_for_query":"upstage\/llama-30b-instruct-2048"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":58.48,
        "ARC":68.43,
        "HellaSwag":86.77,
        "MMLU":68.76,
        "TruthfulQA":52.5,
        "Winogrande":82.56,
        "GSM8K":30.25,
        "DROP":20.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":37.0,
        "Available on the hub":true,
        "Model sha":"34b23982a9a996adc8f45c4c2eac7245c4e251b3",
        "model_name_for_query":"jarradh\/llama2_70b_chat_uncensored"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":58.45,
        "ARC":69.2,
        "HellaSwag":87.46,
        "MMLU":70.14,
        "TruthfulQA":55.86,
        "Winogrande":82.72,
        "GSM8K":32.22,
        "DROP":11.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"agpl-3.0",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"ff29fed2a33fc050fd20d0e25b5b23c4a101b074",
        "model_name_for_query":"Doctor-Shotgun\/mythospice-limarp-70b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":58.41,
        "ARC":70.14,
        "HellaSwag":87.73,
        "MMLU":70.35,
        "TruthfulQA":54.0,
        "Winogrande":83.74,
        "GSM8K":28.58,
        "DROP":14.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"13c7b5f403c0f2af9bf7fce2d4a32deb9054c083",
        "model_name_for_query":"psmathur\/model_420"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":58.32,
        "ARC":68.26,
        "HellaSwag":88.32,
        "MMLU":70.23,
        "TruthfulQA":55.69,
        "Winogrande":83.98,
        "GSM8K":29.8,
        "DROP":11.98,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":53.0,
        "Available on the hub":true,
        "Model sha":"54b0e39d5e9aee7b323f50b0a26db15295c3d5c9",
        "model_name_for_query":"TheBloke\/llama-2-70b-Guanaco-QLoRA-fp16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":58.32,
        "ARC":68.34,
        "HellaSwag":87.87,
        "MMLU":70.18,
        "TruthfulQA":57.47,
        "Winogrande":84.29,
        "GSM8K":29.72,
        "DROP":10.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":20.0,
        "Available on the hub":true,
        "Model sha":"cf1e917e42fd1e56ee1edef7ee1a98cbe705c18c",
        "model_name_for_query":"augtoma\/qCammel-70x"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":58.32,
        "ARC":68.34,
        "HellaSwag":87.87,
        "MMLU":70.18,
        "TruthfulQA":57.47,
        "Winogrande":84.29,
        "GSM8K":29.72,
        "DROP":10.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":20.0,
        "Available on the hub":true,
        "Model sha":"cf1e917e42fd1e56ee1edef7ee1a98cbe705c18c",
        "model_name_for_query":"augtoma\/qCammel-70"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":58.32,
        "ARC":68.34,
        "HellaSwag":87.87,
        "MMLU":70.18,
        "TruthfulQA":57.47,
        "Winogrande":84.29,
        "GSM8K":29.72,
        "DROP":10.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":20.0,
        "Available on the hub":true,
        "Model sha":"cf1e917e42fd1e56ee1edef7ee1a98cbe705c18c",
        "model_name_for_query":"augtoma\/qCammel-70-x"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":58.32,
        "ARC":68.34,
        "HellaSwag":87.87,
        "MMLU":70.18,
        "TruthfulQA":57.47,
        "Winogrande":84.29,
        "GSM8K":29.72,
        "DROP":10.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":20.0,
        "Available on the hub":true,
        "Model sha":"cf1e917e42fd1e56ee1edef7ee1a98cbe705c18c",
        "model_name_for_query":"augtoma\/qCammel-70v1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":58.32,
        "ARC":68.34,
        "HellaSwag":87.87,
        "MMLU":70.18,
        "TruthfulQA":57.47,
        "Winogrande":84.29,
        "GSM8K":29.72,
        "DROP":10.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":20.0,
        "Available on the hub":true,
        "Model sha":"cf1e917e42fd1e56ee1edef7ee1a98cbe705c18c",
        "model_name_for_query":"augtoma\/qCammel70"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":58.2,
        "ARC":68.26,
        "HellaSwag":87.65,
        "MMLU":70.0,
        "TruthfulQA":48.76,
        "Winogrande":83.66,
        "GSM8K":34.72,
        "DROP":14.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"ca3789cd6b683e97dcd6a5f0367f90a63d7a4e7b",
        "model_name_for_query":"psmathur\/model_42_70b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":58.2,
        "ARC":68.26,
        "HellaSwag":87.65,
        "MMLU":70.0,
        "TruthfulQA":48.76,
        "Winogrande":83.66,
        "GSM8K":34.72,
        "DROP":14.37,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"7dadf059a03bdfec2eb4f4a47666545875c68e49",
        "model_name_for_query":"pankajmathur\/Lima_Unchained_70b"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":57.98,
        "ARC":65.27,
        "HellaSwag":85.75,
        "MMLU":63.42,
        "TruthfulQA":47.32,
        "Winogrande":81.37,
        "GSM8K":29.04,
        "DROP":33.69,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"51ce30a69b3c3363c8cfcd6395bf1df974ba2977",
        "model_name_for_query":"Aeala\/Alpaca-elina-65b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":57.98,
        "ARC":62.12,
        "HellaSwag":82.78,
        "MMLU":56.19,
        "TruthfulQA":52.68,
        "Winogrande":78.69,
        "GSM8K":30.1,
        "DROP":43.28,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"ee76c821f861f0ab0276f9f429dd06565f1f2051",
        "model_name_for_query":"Aeala\/GPT4-x-AlpacaDente-30b"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":57.98,
        "ARC":60.49,
        "HellaSwag":82.13,
        "MMLU":61.85,
        "TruthfulQA":52.61,
        "Winogrande":76.72,
        "GSM8K":23.5,
        "DROP":48.53,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.0,
        "Hub \u2764\ufe0f":47.0,
        "Available on the hub":true,
        "Model sha":"b8825fe3394608fe84f0f5eb6471454384fb83aa",
        "model_name_for_query":"internlm\/internlm-20b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":57.89,
        "ARC":65.78,
        "HellaSwag":83.95,
        "MMLU":62.57,
        "TruthfulQA":53.52,
        "Winogrande":80.35,
        "GSM8K":9.63,
        "DROP":49.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":9.0,
        "Available on the hub":true,
        "Model sha":"017e1c32bca060107337dbf26db2044a7caa56f2",
        "model_name_for_query":"ariellee\/SuperPlatty-30B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":57.85,
        "ARC":63.05,
        "HellaSwag":83.56,
        "MMLU":57.71,
        "TruthfulQA":51.52,
        "Winogrande":78.22,
        "GSM8K":30.48,
        "DROP":40.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":64.0,
        "Available on the hub":true,
        "Model sha":"1a0d1d72a40946463fb4a9780207da19bfecc38b",
        "model_name_for_query":"MetaIX\/GPT4-X-Alpasta-30b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":57.73,
        "ARC":68.09,
        "HellaSwag":87.07,
        "MMLU":69.21,
        "TruthfulQA":61.56,
        "Winogrande":84.14,
        "GSM8K":26.91,
        "DROP":7.11,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.98,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"697aaeb8eb9905c9b25bebb736d1905444c774a6",
        "model_name_for_query":"quantumaikr\/llama-2-70b-fb16-orca-chat-10k"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":57.71,
        "ARC":69.28,
        "HellaSwag":87.53,
        "MMLU":70.1,
        "TruthfulQA":56.76,
        "Winogrande":83.27,
        "GSM8K":30.1,
        "DROP":6.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"b00992c26604c9cd496bc41472a05e4c01cd2008",
        "model_name_for_query":"Doctor-Shotgun\/mythospice-70b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":57.69,
        "ARC":70.39,
        "HellaSwag":87.82,
        "MMLU":70.31,
        "TruthfulQA":55.2,
        "Winogrande":83.58,
        "GSM8K":22.52,
        "DROP":14.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":46.0,
        "Available on the hub":true,
        "Model sha":"ea98153fa721ed7110c77e73388e3b6f3996f2bb",
        "model_name_for_query":"jondurbin\/airoboros-l2-70b-gpt4-1.4.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":57.58,
        "ARC":67.06,
        "HellaSwag":86.38,
        "MMLU":67.7,
        "TruthfulQA":56.45,
        "Winogrande":82.0,
        "GSM8K":27.22,
        "DROP":16.28,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":64.0,
        "Available on the hub":true,
        "Model sha":"e68a8a2888097def3c7f4fe5d443866a18d05c6c",
        "model_name_for_query":"OpenAssistant\/llama2-70b-oasst-sft-v10"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":57.34,
        "ARC":69.62,
        "HellaSwag":86.82,
        "MMLU":69.18,
        "TruthfulQA":57.43,
        "Winogrande":83.9,
        "GSM8K":27.37,
        "DROP":7.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":70.0,
        "Hub \u2764\ufe0f":12.0,
        "Available on the hub":false,
        "Model sha":"a1190dee60b5854e80d340958dc3cc956bc56f68",
        "model_name_for_query":"dfurman\/llama-2-70b-dolphin-peft"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":57.21,
        "ARC":61.52,
        "HellaSwag":83.5,
        "MMLU":57.43,
        "TruthfulQA":50.7,
        "Winogrande":79.08,
        "GSM8K":30.48,
        "DROP":37.78,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"62f92ddab8b37eaeda15cf5ecb5605141a0525eb",
        "model_name_for_query":"TehVenom\/oasst-sft-6-llama-33b-xor-MERGED-16bit"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":57.17,
        "ARC":65.44,
        "HellaSwag":84.41,
        "MMLU":64.05,
        "TruthfulQA":54.81,
        "Winogrande":80.82,
        "GSM8K":17.97,
        "DROP":32.71,
        "Type":"instruction-tuned",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":112.0,
        "Available on the hub":true,
        "Model sha":"6dae38060d70b82dcfe787a612d04aaf0adf0738",
        "model_name_for_query":"WizardLM\/WizardLM-70B-V1.0"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":57.12,
        "ARC":64.59,
        "HellaSwag":84.26,
        "MMLU":64.23,
        "TruthfulQA":45.35,
        "Winogrande":81.37,
        "GSM8K":14.4,
        "DROP":45.65,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":16.0,
        "Available on the hub":true,
        "Model sha":"c5d21054f8dd71099696bd7790df07ac54990f29",
        "model_name_for_query":"garage-bAInd\/Platypus-30B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":57.12,
        "ARC":64.59,
        "HellaSwag":84.24,
        "MMLU":64.19,
        "TruthfulQA":45.35,
        "Winogrande":81.37,
        "GSM8K":14.4,
        "DROP":45.65,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":16.0,
        "Available on the hub":true,
        "Model sha":"979ad39b58a8e4a9419b7bc7a0dc8419f3912e71",
        "model_name_for_query":"lilloukas\/Platypus-30B"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":57.11,
        "ARC":69.2,
        "HellaSwag":88.89,
        "MMLU":69.59,
        "TruthfulQA":45.16,
        "Winogrande":86.74,
        "GSM8K":33.21,
        "DROP":7.02,
        "Type":"pretrained",
        "Precision":"4bit",
        "Hub License":"unknown",
        "#Params (B)":179.52,
        "Hub \u2764\ufe0f":830.0,
        "Available on the hub":false,
        "Model sha":"71a1a70b629e9963f7b4601e82f3f9079d48011e",
        "model_name_for_query":"tiiuae\/falcon-180B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":57.11,
        "ARC":62.12,
        "HellaSwag":83.45,
        "MMLU":62.65,
        "TruthfulQA":51.37,
        "Winogrande":78.85,
        "GSM8K":17.59,
        "DROP":43.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":115.0,
        "Available on the hub":true,
        "Model sha":"8e6d0b18be876e0ebfff47d6c4f33d776f189971",
        "model_name_for_query":"migtissera\/SynthIA-7B-v1.3"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":57.11,
        "ARC":67.66,
        "HellaSwag":87.24,
        "MMLU":69.95,
        "TruthfulQA":51.28,
        "Winogrande":84.14,
        "GSM8K":32.75,
        "DROP":6.73,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"153b209007e688d713cd670c9972f2827c597b45",
        "model_name_for_query":"jordiclive\/Llama-2-70b-oasst-1-200"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":57.1,
        "ARC":66.98,
        "HellaSwag":85.73,
        "MMLU":65.99,
        "TruthfulQA":56.58,
        "Winogrande":81.69,
        "GSM8K":35.33,
        "DROP":7.4,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":46.0,
        "Available on the hub":true,
        "Model sha":"33da87ba6d90662c6a00535bd628e5b39b3afd3b",
        "model_name_for_query":"OpenLemur\/lemur-70b-chat-v1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":57.08,
        "ARC":67.66,
        "HellaSwag":87.65,
        "MMLU":69.82,
        "TruthfulQA":49.28,
        "Winogrande":83.9,
        "GSM8K":34.57,
        "DROP":6.69,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"6b0c2cb654133cad2d4920e7da2e3f6cb1c4f7fd",
        "model_name_for_query":"TheBloke\/fiction.live-Kimiko-V2-70B-fp16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":57.05,
        "ARC":60.58,
        "HellaSwag":81.81,
        "MMLU":56.63,
        "TruthfulQA":48.38,
        "Winogrande":78.14,
        "GSM8K":26.76,
        "DROP":47.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":30.0,
        "Available on the hub":true,
        "Model sha":"9fe5a8dada738f44e7ee9293b2140ae0be021787",
        "model_name_for_query":"Aeala\/GPT4-x-AlpacaDente2-30b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":56.97,
        "ARC":67.15,
        "HellaSwag":86.78,
        "MMLU":69.29,
        "TruthfulQA":56.5,
        "Winogrande":82.64,
        "GSM8K":29.04,
        "DROP":7.42,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.98,
        "Hub \u2764\ufe0f":28.0,
        "Available on the hub":true,
        "Model sha":"fd57855006c15c4121feccab1cbeee8107de5b5a",
        "model_name_for_query":"quantumaikr\/llama-2-70b-fb16-korean"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":56.96,
        "ARC":65.87,
        "HellaSwag":86.08,
        "MMLU":63.37,
        "TruthfulQA":52.72,
        "Winogrande":79.56,
        "GSM8K":26.54,
        "DROP":24.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":21.0,
        "Available on the hub":true,
        "Model sha":"50ab86e198e1c82ec81aefc628f23501c101d390",
        "model_name_for_query":"jondurbin\/airoboros-65b-gpt4-1.2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":56.92,
        "ARC":62.03,
        "HellaSwag":84.23,
        "MMLU":64.19,
        "TruthfulQA":46.49,
        "Winogrande":78.69,
        "GSM8K":13.27,
        "DROP":49.54,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"824e3a4738818142374721306ce85b83770de24b",
        "model_name_for_query":"PocketDoc\/Dans-TotSirocco-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":56.9,
        "ARC":62.2,
        "HellaSwag":84.28,
        "MMLU":63.8,
        "TruthfulQA":46.04,
        "Winogrande":79.48,
        "GSM8K":13.19,
        "DROP":49.3,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"?",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"824e3a4738818142374721306ce85b83770de24b",
        "model_name_for_query":"PocketDoc\/Dans-TotSirocco-7b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":56.89,
        "ARC":63.05,
        "HellaSwag":84.15,
        "MMLU":64.11,
        "TruthfulQA":45.07,
        "Winogrande":78.53,
        "GSM8K":17.36,
        "DROP":45.92,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"d836a261afa0871d3734a7dfd1a28dc23c173ea7",
        "model_name_for_query":"bhenrym14\/mistral-7b-platypus-fp16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":56.68,
        "ARC":67.75,
        "HellaSwag":85.48,
        "MMLU":58.98,
        "TruthfulQA":61.05,
        "Winogrande":76.8,
        "GSM8K":2.81,
        "DROP":43.88,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":19.0,
        "Available on the hub":true,
        "Model sha":"305eea72fb9fe2ac5929a62483ea51f152bcc060",
        "model_name_for_query":"ajibawa-2023\/scarlett-33b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":56.64,
        "ARC":63.05,
        "HellaSwag":85.0,
        "MMLU":58.32,
        "TruthfulQA":52.1,
        "Winogrande":78.85,
        "GSM8K":11.14,
        "DROP":47.99,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"openrail",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"9c2b558b888e0ef8b4a72e0771db72a06a5c8474",
        "model_name_for_query":"bofenghuang\/vigogne-33b-instruct"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":56.58,
        "ARC":67.41,
        "HellaSwag":87.37,
        "MMLU":69.77,
        "TruthfulQA":46.77,
        "Winogrande":83.9,
        "GSM8K":34.27,
        "DROP":6.6,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":16.0,
        "Available on the hub":true,
        "Model sha":"129e0af93d04b1b9cc85ea48bbb300f1ccb44210",
        "model_name_for_query":"NousResearch\/Nous-Puffin-70B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":56.42,
        "ARC":63.48,
        "HellaSwag":84.37,
        "MMLU":58.99,
        "TruthfulQA":46.98,
        "Winogrande":80.98,
        "GSM8K":15.54,
        "DROP":44.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"1990b46a2e2ac1f6282d961bce691ceceafed514",
        "model_name_for_query":"PocketDoc\/Dans-PersonalityEngine-30b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":56.39,
        "ARC":68.94,
        "HellaSwag":87.07,
        "MMLU":68.84,
        "TruthfulQA":54.49,
        "Winogrande":82.08,
        "GSM8K":26.54,
        "DROP":6.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"6589310a57ce5d9d6877f353f3d00cda8fa9101c",
        "model_name_for_query":"Sao10K\/Euryale-L2-70B"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":56.36,
        "ARC":59.04,
        "HellaSwag":80.66,
        "MMLU":56.72,
        "TruthfulQA":52.18,
        "Winogrande":79.64,
        "GSM8K":13.8,
        "DROP":52.45,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":12.95,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"2d439187efd6edd91a0c0146f08dff52d92aa7bc",
        "model_name_for_query":"yulan-team\/YuLan-Chat-2-13b-fp16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":56.32,
        "ARC":64.51,
        "HellaSwag":85.2,
        "MMLU":59.09,
        "TruthfulQA":48.42,
        "Winogrande":80.82,
        "GSM8K":10.24,
        "DROP":45.99,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":21.0,
        "Available on the hub":true,
        "Model sha":"64c10edf5312cd13704925b07413882d9e94c7a0",
        "model_name_for_query":"openaccess-ai-collective\/hippogriff-30b-chat"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":56.25,
        "ARC":67.32,
        "HellaSwag":87.33,
        "MMLU":69.83,
        "TruthfulQA":44.92,
        "Winogrande":83.74,
        "GSM8K":33.97,
        "DROP":6.62,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.98,
        "Hub \u2764\ufe0f":623.0,
        "Available on the hub":false,
        "Model sha":"ed7b07231238f836b99bf45701b9a0063576b194",
        "model_name_for_query":"meta-llama\/Llama-2-70b-hf"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":56.25,
        "ARC":67.32,
        "HellaSwag":87.33,
        "MMLU":69.83,
        "TruthfulQA":44.92,
        "Winogrande":83.74,
        "GSM8K":33.97,
        "DROP":6.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":68.98,
        "Hub \u2764\ufe0f":40.0,
        "Available on the hub":true,
        "Model sha":"b25061ef1b440e970d15d4ac99bc42937cd442a2",
        "model_name_for_query":"TheBloke\/Llama-2-70B-fp16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":56.21,
        "ARC":68.09,
        "HellaSwag":86.5,
        "MMLU":68.28,
        "TruthfulQA":53.7,
        "Winogrande":81.22,
        "GSM8K":28.66,
        "DROP":7.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":9.0,
        "Available on the hub":true,
        "Model sha":"373af41ca0b2855972b8d471fd63e72b63e4c9fc",
        "model_name_for_query":"elinas\/chronos-70b-v2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":56.2,
        "ARC":59.9,
        "HellaSwag":79.99,
        "MMLU":78.66,
        "TruthfulQA":45.56,
        "Winogrande":74.35,
        "GSM8K":12.21,
        "DROP":42.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"17f57642165e30a4025d6817bd47dcd80d0c5c4d",
        "model_name_for_query":"Aspik101\/trurl-2-13b-pl-instruct_unload"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":56.19,
        "ARC":62.54,
        "HellaSwag":83.28,
        "MMLU":59.03,
        "TruthfulQA":52.49,
        "Winogrande":77.51,
        "GSM8K":22.21,
        "DROP":36.25,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":true,
        "Model sha":"465f87a243969963f25ae6cf8f8d2de6c0898bbe",
        "model_name_for_query":"TheBloke\/WizardLM-30B-fp16"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":56.13,
        "ARC":62.54,
        "HellaSwag":83.27,
        "MMLU":59.05,
        "TruthfulQA":52.49,
        "Winogrande":77.51,
        "GSM8K":21.83,
        "DROP":36.21,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"gpl-3.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"75318440dba949804d6263d368e1f29a94ea7c5f",
        "model_name_for_query":"LLMs\/WizardLM-30B-V1.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":56.12,
        "ARC":60.58,
        "HellaSwag":82.17,
        "MMLU":57.93,
        "TruthfulQA":46.94,
        "Winogrande":78.61,
        "GSM8K":29.8,
        "DROP":36.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":13.0,
        "Available on the hub":true,
        "Model sha":"a7a2306b9a63de2c545f35b24735f4540baf5903",
        "model_name_for_query":"TheBloke\/OpenAssistant-SFT-7-Llama-30B-HF"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":56.06,
        "ARC":64.25,
        "HellaSwag":83.64,
        "MMLU":58.23,
        "TruthfulQA":53.2,
        "Winogrande":77.43,
        "GSM8K":15.85,
        "DROP":39.83,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"7f035eabd1d0e7b38ace395847a623f475d90da8",
        "model_name_for_query":"YeungNLP\/firefly-llama-30b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":56.03,
        "ARC":63.14,
        "HellaSwag":83.64,
        "MMLU":59.91,
        "TruthfulQA":56.21,
        "Winogrande":76.72,
        "GSM8K":9.4,
        "DROP":43.23,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"67107327d09c2f9bf3e4b316d97767c97f5a0804",
        "model_name_for_query":"42MARU\/sitebunny-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":55.99,
        "ARC":67.06,
        "HellaSwag":87.26,
        "MMLU":69.85,
        "TruthfulQA":44.57,
        "Winogrande":83.35,
        "GSM8K":33.21,
        "DROP":6.6,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"5095384f1b7bb6e23a987f95589e66e21ae854ef",
        "model_name_for_query":"psmathur\/model_420_preview"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":55.94,
        "ARC":65.02,
        "HellaSwag":86.13,
        "MMLU":62.73,
        "TruthfulQA":59.16,
        "Winogrande":80.66,
        "GSM8K":28.28,
        "DROP":9.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"664ff8e3e1d446971a16a6c9018ab24de7664684",
        "model_name_for_query":"TheBloke\/gpt4-alpaca-lora_mlp-65B-HF"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":55.92,
        "ARC":71.25,
        "HellaSwag":87.3,
        "MMLU":70.56,
        "TruthfulQA":60.61,
        "Winogrande":81.53,
        "GSM8K":5.69,
        "DROP":14.53,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.98,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"d48cf79d1ead50154b1e70120779ae91bc5fafb4",
        "model_name_for_query":"chargoddard\/MelangeA-70b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":55.85,
        "ARC":59.22,
        "HellaSwag":80.26,
        "MMLU":56.9,
        "TruthfulQA":61.09,
        "Winogrande":75.37,
        "GSM8K":18.65,
        "DROP":39.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":58.0,
        "Available on the hub":true,
        "Model sha":"c673387016c622fd0a707426953c03957398bc37",
        "model_name_for_query":"ehartford\/dolphin-2.0-mistral-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":55.79,
        "ARC":64.51,
        "HellaSwag":84.15,
        "MMLU":57.37,
        "TruthfulQA":53.87,
        "Winogrande":80.19,
        "GSM8K":23.58,
        "DROP":26.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.53,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"679aae34440d576456b283070371b2a15dbb948b",
        "model_name_for_query":"Sao10K\/Zephyrus-L1-33B"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":55.74,
        "ARC":59.98,
        "HellaSwag":83.4,
        "MMLU":56.1,
        "TruthfulQA":45.14,
        "Winogrande":80.82,
        "GSM8K":19.71,
        "DROP":45.01,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"37c3655676c37662f60c68dacfce3f0e861be846",
        "model_name_for_query":"TheBloke\/tulu-30B-fp16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":55.66,
        "ARC":62.12,
        "HellaSwag":85.68,
        "MMLU":63.49,
        "TruthfulQA":49.8,
        "Winogrande":81.69,
        "GSM8K":28.81,
        "DROP":18.04,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"2bf026af438d522268533484a85a3e54178e7809",
        "model_name_for_query":"Faradaylab\/ARIA-70B-V2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":55.44,
        "ARC":60.84,
        "HellaSwag":82.29,
        "MMLU":60.8,
        "TruthfulQA":52.38,
        "Winogrande":77.03,
        "GSM8K":11.07,
        "DROP":43.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":17.0,
        "Available on the hub":true,
        "Model sha":"ebf138de4fb7a57f0d187ad0ab43abd6b35bfb62",
        "model_name_for_query":"SkunkworksAI\/Mistralic-7B-1"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":55.42,
        "ARC":68.52,
        "HellaSwag":85.67,
        "MMLU":67.03,
        "TruthfulQA":43.47,
        "Winogrande":82.24,
        "GSM8K":28.73,
        "DROP":12.27,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"e3230df22d065b6699096494d1151fa337dde9e8",
        "model_name_for_query":"yeontaek\/llama-2-70b-IA3-guanaco"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":55.33,
        "ARC":59.64,
        "HellaSwag":82.25,
        "MMLU":61.33,
        "TruthfulQA":48.45,
        "Winogrande":77.51,
        "GSM8K":8.26,
        "DROP":49.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"f7db67fe6c82657b35d0ffcf8b7ff1568d979482",
        "model_name_for_query":"uukuguy\/speechless-code-mistral-orca-7b-v1.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":55.31,
        "ARC":55.97,
        "HellaSwag":79.79,
        "MMLU":54.95,
        "TruthfulQA":51.16,
        "Winogrande":74.35,
        "GSM8K":30.33,
        "DROP":40.58,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.88,
        "Hub \u2764\ufe0f":62.0,
        "Available on the hub":true,
        "Model sha":"b51c6b29abdf7c420cb5e5f4f309ff83179c7bb8",
        "model_name_for_query":"OpenBuddy\/openbuddy-llama2-13b-v8.1-fp16"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":55.3,
        "ARC":63.65,
        "HellaSwag":84.97,
        "MMLU":57.37,
        "TruthfulQA":52.17,
        "Winogrande":78.22,
        "GSM8K":6.6,
        "DROP":44.12,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"12ccd0e6c9ef12c7d3c2eab8266cd32c0b2f7683",
        "model_name_for_query":"jondurbin\/airoboros-33b-2.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":55.28,
        "ARC":63.82,
        "HellaSwag":83.85,
        "MMLU":63.68,
        "TruthfulQA":54.54,
        "Winogrande":78.61,
        "GSM8K":18.5,
        "DROP":23.97,
        "Type":"fine-tuned",
        "Precision":"GPTQ",
        "Hub License":"llama2",
        "#Params (B)":72.82,
        "Hub \u2764\ufe0f":26.0,
        "Available on the hub":true,
        "Model sha":"c234d7c9c0fd26efb55757fdbfb604d549539fe0",
        "model_name_for_query":"TheBloke\/WizardLM-70B-V1.0-GPTQ"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":55.25,
        "ARC":63.05,
        "HellaSwag":83.59,
        "MMLU":56.89,
        "TruthfulQA":59.03,
        "Winogrande":77.66,
        "GSM8K":10.69,
        "DROP":35.82,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":9.0,
        "Available on the hub":true,
        "Model sha":"6962638c2b0368ad496af6e20e46e3de97a7772b",
        "model_name_for_query":"CalderaAI\/30B-Epsilon"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":55.2,
        "ARC":64.16,
        "HellaSwag":84.38,
        "MMLU":57.49,
        "TruthfulQA":51.57,
        "Winogrande":79.48,
        "GSM8K":16.07,
        "DROP":33.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.53,
        "Hub \u2764\ufe0f":13.0,
        "Available on the hub":true,
        "Model sha":"0cff8e9718e57202171003d556d2e6630061879d",
        "model_name_for_query":"openaccess-ai-collective\/manticore-30b-chat-pyg-alpha"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":55.1,
        "ARC":62.54,
        "HellaSwag":83.9,
        "MMLU":56.57,
        "TruthfulQA":48.14,
        "Winogrande":76.95,
        "GSM8K":10.99,
        "DROP":46.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":11.0,
        "Available on the hub":true,
        "Model sha":"a3033ac5825662f1c66418d7543648dc76980185",
        "model_name_for_query":"Undi95\/MLewdBoros-L2-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":55.01,
        "ARC":64.59,
        "HellaSwag":85.27,
        "MMLU":58.38,
        "TruthfulQA":45.32,
        "Winogrande":76.24,
        "GSM8K":6.37,
        "DROP":48.92,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"5f80b372b493d901cab4490b4f23c71499023615",
        "model_name_for_query":"ajibawa-2023\/carl-33b"
    },
    {
        "T":"\ud83d\udfe6",
        "Average \u2b06\ufe0f":54.98,
        "ARC":64.59,
        "HellaSwag":85.88,
        "MMLU":63.91,
        "TruthfulQA":52.8,
        "Winogrande":80.51,
        "GSM8K":26.69,
        "DROP":10.5,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":68.98,
        "Hub \u2764\ufe0f":1453.0,
        "Available on the hub":false,
        "Model sha":"7f54101c0fbb67a8143ca23eb8bd09b71f269c74",
        "model_name_for_query":"meta-llama\/Llama-2-70b-chat-hf"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":54.96,
        "ARC":64.51,
        "HellaSwag":85.87,
        "MMLU":63.88,
        "TruthfulQA":52.8,
        "Winogrande":80.51,
        "GSM8K":26.69,
        "DROP":10.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"d8580d360c51e71fddd27897445e2aa9d1888585",
        "model_name_for_query":"willyninja30\/ARIA-70B-French"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":54.86,
        "ARC":64.85,
        "HellaSwag":85.59,
        "MMLU":63.11,
        "TruthfulQA":45.15,
        "Winogrande":81.22,
        "GSM8K":28.05,
        "DROP":16.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"113b61b37a2862b950ada68620e57acafbcefe13",
        "model_name_for_query":"TheBloke\/alpaca-lora-65B-HF"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":54.85,
        "ARC":61.95,
        "HellaSwag":83.83,
        "MMLU":61.74,
        "TruthfulQA":46.63,
        "Winogrande":78.45,
        "GSM8K":17.29,
        "DROP":34.07,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"6439444e2c0b61253d3e61ae04fe0436717acc2f",
        "model_name_for_query":"CobraMamba\/mamba-gpt-7b-v2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":54.8,
        "ARC":60.75,
        "HellaSwag":83.67,
        "MMLU":56.27,
        "TruthfulQA":50.32,
        "Winogrande":74.98,
        "GSM8K":10.92,
        "DROP":46.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"e76f35fe771ef142d6629092bd4a93301fd6cd4a",
        "model_name_for_query":"Sao10K\/Stheno-1.2-L2-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":54.8,
        "ARC":62.71,
        "HellaSwag":83.37,
        "MMLU":63.48,
        "TruthfulQA":51.32,
        "Winogrande":79.24,
        "GSM8K":17.44,
        "DROP":26.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"5a9912ef90a0efc1aaea327e5cf3e9554c8bd897",
        "model_name_for_query":"migtissera\/SynthIA-7B-v1.5"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":54.77,
        "ARC":61.26,
        "HellaSwag":84.1,
        "MMLU":63.46,
        "TruthfulQA":46.34,
        "Winogrande":79.16,
        "GSM8K":17.36,
        "DROP":31.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"e64d658b397748e409d9633fd24fc5a6df429600",
        "model_name_for_query":"CobraMamba\/mamba-gpt-7b-v1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":54.74,
        "ARC":65.61,
        "HellaSwag":85.15,
        "MMLU":63.13,
        "TruthfulQA":52.47,
        "Winogrande":81.29,
        "GSM8K":27.82,
        "DROP":7.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":9.0,
        "Available on the hub":true,
        "Model sha":"6cdacfda96970aa144e316b108ab9bc17c99a573",
        "model_name_for_query":"TheBloke\/VicUnlocked-alpaca-65B-QLoRA-fp16"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":54.74,
        "ARC":62.12,
        "HellaSwag":83.0,
        "MMLU":59.22,
        "TruthfulQA":56.16,
        "Winogrande":77.03,
        "GSM8K":13.72,
        "DROP":31.92,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":230.0,
        "Available on the hub":true,
        "Model sha":"ef8d6becf883fb3ce52e3706885f761819477ab4",
        "model_name_for_query":"lmsys\/vicuna-33b-v1.3"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":54.68,
        "ARC":65.44,
        "HellaSwag":86.47,
        "MMLU":62.92,
        "TruthfulQA":52.81,
        "Winogrande":82.4,
        "GSM8K":26.0,
        "DROP":6.69,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":26.0,
        "Available on the hub":true,
        "Model sha":"7f83ae526f8b83705ca8434535da8fd8c692f9d0",
        "model_name_for_query":"TheBloke\/guanaco-65B-HF"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":54.65,
        "ARC":61.69,
        "HellaSwag":83.83,
        "MMLU":55.1,
        "TruthfulQA":53.34,
        "Winogrande":74.51,
        "GSM8K":9.78,
        "DROP":44.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"6f6ec6024ee054020e49fd96f149919692848f0b",
        "model_name_for_query":"Undi95\/MLewd-v2.4-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":54.63,
        "ARC":61.01,
        "HellaSwag":83.47,
        "MMLU":63.69,
        "TruthfulQA":42.65,
        "Winogrande":78.22,
        "GSM8K":15.69,
        "DROP":37.65,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"ddc7e4fcbbb5c666a3fe1bbe4a47b4477151b699",
        "model_name_for_query":"PocketDoc\/Dans-AdventurousWinds-7b"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":54.62,
        "ARC":63.48,
        "HellaSwag":86.09,
        "MMLU":63.93,
        "TruthfulQA":43.43,
        "Winogrande":82.56,
        "GSM8K":37.23,
        "DROP":5.63,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":65.29,
        "Hub \u2764\ufe0f":64.0,
        "Available on the hub":true,
        "Model sha":"49707c5313d34d1c5a846e29cf2a2a650c22c8ee",
        "model_name_for_query":"huggyllama\/llama-65b"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":54.59,
        "ARC":67.83,
        "HellaSwag":85.55,
        "MMLU":58.79,
        "TruthfulQA":61.19,
        "Winogrande":76.48,
        "GSM8K":4.02,
        "DROP":28.29,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":13.0,
        "Available on the hub":true,
        "Model sha":"ad8892a17be1372f611203a4cf71560cc337e458",
        "model_name_for_query":"ehartford\/samantha-1.1-llama-33b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":54.56,
        "ARC":62.46,
        "HellaSwag":83.13,
        "MMLU":63.47,
        "TruthfulQA":55.69,
        "Winogrande":76.4,
        "GSM8K":11.9,
        "DROP":28.88,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"19694dc88e74a018d54bac6070cf521dff6d4397",
        "model_name_for_query":"NeverSleep\/Mistral-11B-SynthIAirOmniMix"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":54.53,
        "ARC":62.29,
        "HellaSwag":82.46,
        "MMLU":57.09,
        "TruthfulQA":51.41,
        "Winogrande":76.56,
        "GSM8K":3.56,
        "DROP":48.35,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"0c15b8540335b3e21a976a5fc5c33b47927fea6c",
        "model_name_for_query":"TFLai\/Stable-Platypus2-13B-QLoRA-0.80-epoch"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":54.51,
        "ARC":64.08,
        "HellaSwag":83.99,
        "MMLU":62.24,
        "TruthfulQA":53.05,
        "Winogrande":77.74,
        "GSM8K":19.94,
        "DROP":20.53,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":314.0,
        "Available on the hub":true,
        "Model sha":"7233ac83317946d05c474b71cc1379f49eb74c14",
        "model_name_for_query":"Open-Orca\/Mistral-7B-OpenOrca"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":54.46,
        "ARC":66.64,
        "HellaSwag":86.66,
        "MMLU":63.18,
        "TruthfulQA":49.11,
        "Winogrande":80.74,
        "GSM8K":20.85,
        "DROP":14.05,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"ea4bdd0221f77de9b0343cd8291cbd0fd6033ca8",
        "model_name_for_query":"jondurbin\/airoboros-65b-gpt4-2.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":54.43,
        "ARC":60.75,
        "HellaSwag":83.64,
        "MMLU":56.39,
        "TruthfulQA":50.3,
        "Winogrande":75.22,
        "GSM8K":7.96,
        "DROP":46.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"0f45a9f834dd216ce25ffa606b3b1ef2c99e7acd",
        "model_name_for_query":"Sao10K\/Stheno-1.1-L2-13B"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":54.41,
        "ARC":68.17,
        "HellaSwag":86.49,
        "MMLU":68.89,
        "TruthfulQA":52.69,
        "Winogrande":82.32,
        "GSM8K":3.94,
        "DROP":18.37,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":95.0,
        "Available on the hub":true,
        "Model sha":"e85b43e53c5379e35393b970c66d76c2d1060381",
        "model_name_for_query":"WizardLM\/WizardMath-70B-V1.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":54.41,
        "ARC":62.46,
        "HellaSwag":86.23,
        "MMLU":59.37,
        "TruthfulQA":52.78,
        "Winogrande":80.51,
        "GSM8K":12.13,
        "DROP":27.39,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":21.0,
        "Available on the hub":true,
        "Model sha":"fea4312379557e8a1e8073965f560798de369edd",
        "model_name_for_query":"upstage\/llama-30b-instruct"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":54.41,
        "ARC":63.65,
        "HellaSwag":83.84,
        "MMLU":59.36,
        "TruthfulQA":56.8,
        "Winogrande":77.66,
        "GSM8K":18.65,
        "DROP":20.89,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":44.0,
        "Available on the hub":true,
        "Model sha":"3eca9fdee0ce28d6a4a635a6f19d9a413caee3e7",
        "model_name_for_query":"ehartford\/WizardLM-33B-V1.0-Uncensored"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":54.4,
        "ARC":63.14,
        "HellaSwag":85.19,
        "MMLU":57.28,
        "TruthfulQA":48.07,
        "Winogrande":78.45,
        "GSM8K":9.7,
        "DROP":38.94,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"96af3dc6c9f2248d964cf14cef6e5f2e5894583a",
        "model_name_for_query":"jondurbin\/airoboros-33b-gpt4-m2.0"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":54.39,
        "ARC":61.09,
        "HellaSwag":82.4,
        "MMLU":56.46,
        "TruthfulQA":49.9,
        "Winogrande":77.66,
        "GSM8K":23.28,
        "DROP":29.96,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":35.58,
        "Hub \u2764\ufe0f":380.0,
        "Available on the hub":true,
        "Model sha":"56a82ece7a9309189561a590e8f4d2fe0d4be92b",
        "model_name_for_query":"TheBloke\/Wizard-Vicuna-30B-Uncensored-GPTQ"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":54.37,
        "ARC":61.01,
        "HellaSwag":83.61,
        "MMLU":57.07,
        "TruthfulQA":47.81,
        "Winogrande":75.93,
        "GSM8K":10.01,
        "DROP":45.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"ba253c52eb85e24987c81e5d36b5a9a00e276ce7",
        "model_name_for_query":"sauce1337\/AppleSauce-L2-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":54.35,
        "ARC":56.14,
        "HellaSwag":79.78,
        "MMLU":60.01,
        "TruthfulQA":47.02,
        "Winogrande":76.48,
        "GSM8K":26.99,
        "DROP":33.99,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":14.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"e878e1f1f7b533c32beb8e06ebcf0cfa23f3fe9b",
        "model_name_for_query":"JosephusCheung\/Pwen-14B-Chat-20_30"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":54.34,
        "ARC":67.92,
        "HellaSwag":86.46,
        "MMLU":68.92,
        "TruthfulQA":52.77,
        "Winogrande":82.32,
        "GSM8K":4.09,
        "DROP":17.87,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":95.0,
        "Available on the hub":true,
        "Model sha":"e85b43e53c5379e35393b970c66d76c2d1060381",
        "model_name_for_query":"WizardLM\/WizardMath-70B-V1.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":54.27,
        "ARC":63.4,
        "HellaSwag":85.19,
        "MMLU":57.46,
        "TruthfulQA":48.15,
        "Winogrande":78.37,
        "GSM8K":9.63,
        "DROP":37.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"84a89dee5bf3447079f115a3ef4d58ef8f924798",
        "model_name_for_query":"jondurbin\/airoboros-33b-gpt4-m2.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":54.27,
        "ARC":66.81,
        "HellaSwag":86.66,
        "MMLU":63.41,
        "TruthfulQA":49.17,
        "Winogrande":80.27,
        "GSM8K":20.55,
        "DROP":13.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"ea4bdd0221f77de9b0343cd8291cbd0fd6033ca8",
        "model_name_for_query":"jondurbin\/airoboros-65b-gpt4-2.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":54.25,
        "ARC":62.71,
        "HellaSwag":82.29,
        "MMLU":58.3,
        "TruthfulQA":52.52,
        "Winogrande":76.87,
        "GSM8K":1.82,
        "DROP":45.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":19.0,
        "Available on the hub":true,
        "Model sha":"0e54aa49c24617e30a23a20c0c5da61419b9fe68",
        "model_name_for_query":"garage-bAInd\/Stable-Platypus2-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":54.19,
        "ARC":65.02,
        "HellaSwag":86.35,
        "MMLU":64.37,
        "TruthfulQA":46.66,
        "Winogrande":80.19,
        "GSM8K":22.14,
        "DROP":14.58,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"fa081d52619b35d7016fb40ce855187d6a8e7e4c",
        "model_name_for_query":"jondurbin\/airoboros-65b-gpt4-m2.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":54.18,
        "ARC":59.9,
        "HellaSwag":80.76,
        "MMLU":58.34,
        "TruthfulQA":47.97,
        "Winogrande":77.9,
        "GSM8K":7.51,
        "DROP":46.85,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"65214c9923d55795ecd6e7f9e0fcee5ba5f26929",
        "model_name_for_query":"uukuguy\/speechless-orca-platypus-coig-lite-2k-0.6e-13b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":54.08,
        "ARC":60.84,
        "HellaSwag":82.56,
        "MMLU":56.42,
        "TruthfulQA":53.32,
        "Winogrande":75.93,
        "GSM8K":2.27,
        "DROP":47.24,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"1f81c0439f60d848e3cbc7f06fcd58b5161a8557",
        "model_name_for_query":"TFLai\/OrcaMini-Platypus2-13B-QLoRA-0.80-epoch"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":54.03,
        "ARC":64.33,
        "HellaSwag":85.72,
        "MMLU":65.85,
        "TruthfulQA":44.78,
        "Winogrande":83.03,
        "GSM8K":28.73,
        "DROP":5.74,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":31.0,
        "Available on the hub":true,
        "Model sha":"74432ae16ef50207fe17fb88b2f1c1d32ef3b481",
        "model_name_for_query":"OpenLemur\/lemur-70b-v1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":54.01,
        "ARC":63.91,
        "HellaSwag":85.67,
        "MMLU":57.95,
        "TruthfulQA":45.54,
        "Winogrande":77.98,
        "GSM8K":11.07,
        "DROP":35.94,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"a4e1b721add286900c5a6f529c3d7a3e0049b2e0",
        "model_name_for_query":"jondurbin\/airoboros-33b-gpt4-2.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.94,
        "ARC":65.1,
        "HellaSwag":86.34,
        "MMLU":64.32,
        "TruthfulQA":46.63,
        "Winogrande":80.11,
        "GSM8K":21.61,
        "DROP":13.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"fa081d52619b35d7016fb40ce855187d6a8e7e4c",
        "model_name_for_query":"jondurbin\/airoboros-65b-gpt4-m2.0"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":53.93,
        "ARC":59.3,
        "HellaSwag":83.46,
        "MMLU":57.0,
        "TruthfulQA":45.56,
        "Winogrande":76.4,
        "GSM8K":14.86,
        "DROP":40.96,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"569f848698a468fb03d37033c67f3734bbaec127",
        "model_name_for_query":"PulsarAI\/Nebula-7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.88,
        "ARC":64.68,
        "HellaSwag":84.95,
        "MMLU":57.77,
        "TruthfulQA":47.44,
        "Winogrande":77.74,
        "GSM8K":10.39,
        "DROP":34.17,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"57bd88e24d603dc4bbe4016ed0871db7c0e529d5",
        "model_name_for_query":"jondurbin\/airoboros-33b-gpt4-m2.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.87,
        "ARC":61.6,
        "HellaSwag":82.2,
        "MMLU":57.55,
        "TruthfulQA":53.58,
        "Winogrande":77.51,
        "GSM8K":1.44,
        "DROP":43.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"ee2ceaae9cb806bc30df84ba4d598fdf32e53b17",
        "model_name_for_query":"Enno-Ai\/ennodata-13b-8bit-raw-15epoch"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":53.87,
        "ARC":62.12,
        "HellaSwag":84.17,
        "MMLU":62.35,
        "TruthfulQA":57.62,
        "Winogrande":75.37,
        "GSM8K":15.62,
        "DROP":19.85,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":45.0,
        "Available on the hub":true,
        "Model sha":"5f57f70ec99450c70da2540e94dd7fd67be4b23c",
        "model_name_for_query":"teknium\/CollectiveCognition-v1.1-Mistral-7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.87,
        "ARC":50.0,
        "HellaSwag":71.19,
        "MMLU":55.71,
        "TruthfulQA":53.01,
        "Winogrande":70.8,
        "GSM8K":34.57,
        "DROP":41.81,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":33.53,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":true,
        "Model sha":"1b361b3634bf59913b47c9dad1b138e99833472b",
        "model_name_for_query":"OpenBuddy\/openbuddy-codellama2-34b-v11.1-bf16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.87,
        "ARC":50.0,
        "HellaSwag":71.19,
        "MMLU":55.71,
        "TruthfulQA":53.01,
        "Winogrande":70.8,
        "GSM8K":34.57,
        "DROP":41.81,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":33.53,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":true,
        "Model sha":"21ac0d26c0097e5ac5b4a757493574b156da7731",
        "model_name_for_query":"openBuddy\/openbuddy-llama2-34b-v11.1-bf16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.86,
        "ARC":62.29,
        "HellaSwag":83.78,
        "MMLU":57.1,
        "TruthfulQA":48.3,
        "Winogrande":76.09,
        "GSM8K":11.75,
        "DROP":37.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"c8788874b78c84bc5593586d16fbd8ae7b5b2991",
        "model_name_for_query":"sauce1337\/BerrySauce-L2-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.82,
        "ARC":63.82,
        "HellaSwag":85.65,
        "MMLU":58.44,
        "TruthfulQA":45.57,
        "Winogrande":77.9,
        "GSM8K":10.69,
        "DROP":34.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"ddc598f492f5098a8e308f51a82834f98f29a4ce",
        "model_name_for_query":"jondurbin\/airoboros-33b-gpt4-2.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.79,
        "ARC":62.12,
        "HellaSwag":83.3,
        "MMLU":57.57,
        "TruthfulQA":54.03,
        "Winogrande":76.56,
        "GSM8K":16.68,
        "DROP":26.28,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"1c1f4e9256ac2be145a9106863ee9f2e9d701e74",
        "model_name_for_query":"ajibawa-2023\/Uncensored-Frank-33B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.79,
        "ARC":58.11,
        "HellaSwag":82.39,
        "MMLU":57.03,
        "TruthfulQA":53.53,
        "Winogrande":73.72,
        "GSM8K":9.55,
        "DROP":42.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"c41d373a2d49b79236d6c4d0dfc4086e709c07eb",
        "model_name_for_query":"PulsarAI\/CollectiveCognition-v1.1-Nebula-7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.78,
        "ARC":65.53,
        "HellaSwag":85.77,
        "MMLU":61.95,
        "TruthfulQA":52.43,
        "Winogrande":79.79,
        "GSM8K":18.04,
        "DROP":12.94,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":16.0,
        "Available on the hub":true,
        "Model sha":"ae256799615c16443f9c423c653ed9f60577e99e",
        "model_name_for_query":"jondurbin\/airoboros-65b-gpt4-1.4"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.78,
        "ARC":61.95,
        "HellaSwag":82.48,
        "MMLU":57.32,
        "TruthfulQA":53.5,
        "Winogrande":75.85,
        "GSM8K":1.36,
        "DROP":43.97,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"1c959d4b5d5b8683b051f07475bb5c1ab24c8bb0",
        "model_name_for_query":"psmathur\/model_007_13b_v2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.77,
        "ARC":60.67,
        "HellaSwag":82.31,
        "MMLU":55.94,
        "TruthfulQA":50.85,
        "Winogrande":75.53,
        "GSM8K":14.4,
        "DROP":36.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":27.0,
        "Available on the hub":true,
        "Model sha":"1776feacbf1052cff02eb3d7531a854555d3f6dc",
        "model_name_for_query":"BELLE-2\/BELLE-Llama2-13B-chat-0.4M"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.76,
        "ARC":62.97,
        "HellaSwag":83.68,
        "MMLU":58.16,
        "TruthfulQA":52.27,
        "Winogrande":77.11,
        "GSM8K":15.39,
        "DROP":26.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":11.0,
        "Available on the hub":true,
        "Model sha":"e2329c05a6e59660ba3cbcc01adf30a78f852594",
        "model_name_for_query":"concedo\/Vicuzard-30B-Uncensored"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.72,
        "ARC":61.95,
        "HellaSwag":82.21,
        "MMLU":57.44,
        "TruthfulQA":53.57,
        "Winogrande":75.93,
        "GSM8K":1.29,
        "DROP":43.65,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"206553873db96a6730d36477837335dbbcc906fc",
        "model_name_for_query":"Enno-Ai\/ennodata-raw-pankajmathur-13b-peft"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.68,
        "ARC":65.78,
        "HellaSwag":85.83,
        "MMLU":62.27,
        "TruthfulQA":52.45,
        "Winogrande":79.64,
        "GSM8K":18.04,
        "DROP":11.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":65.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"85ae3b595c6b8415df87000c22bc14ea18c174f5",
        "model_name_for_query":"jondurbin\/airoboros-65b-gpt4-1.4-peft"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":53.68,
        "ARC":65.78,
        "HellaSwag":85.83,
        "MMLU":62.27,
        "TruthfulQA":52.45,
        "Winogrande":79.64,
        "GSM8K":18.04,
        "DROP":11.76,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":16.0,
        "Available on the hub":true,
        "Model sha":"ae256799615c16443f9c423c653ed9f60577e99e",
        "model_name_for_query":"jondurbin\/airoboros-65b-gpt4-1.4"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":53.64,
        "ARC":62.37,
        "HellaSwag":85.08,
        "MMLU":63.79,
        "TruthfulQA":47.33,
        "Winogrande":77.66,
        "GSM8K":17.29,
        "DROP":21.93,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"aa2c84e89c4c8a10e0569e45021b59e6d1c08bda",
        "model_name_for_query":"akjindal53244\/Mistral-7B-v0.1-Open-Platypus"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.61,
        "ARC":61.95,
        "HellaSwag":84.6,
        "MMLU":62.51,
        "TruthfulQA":52.31,
        "Winogrande":80.51,
        "GSM8K":26.99,
        "DROP":6.42,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"40edb31ba93045d673735361bc98f56125bbc77b",
        "model_name_for_query":"TheBloke\/robin-65b-v2-fp16"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":53.6,
        "ARC":67.49,
        "HellaSwag":86.03,
        "MMLU":68.44,
        "TruthfulQA":52.23,
        "Winogrande":81.77,
        "GSM8K":2.88,
        "DROP":16.36,
        "Type":"instruction-tuned",
        "Precision":"8bit",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":95.0,
        "Available on the hub":true,
        "Model sha":"97e5913edd2c593c3eef12070024674e7ee4e16c",
        "model_name_for_query":"WizardLM\/WizardMath-70B-V1.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.57,
        "ARC":55.97,
        "HellaSwag":82.05,
        "MMLU":54.74,
        "TruthfulQA":48.9,
        "Winogrande":76.16,
        "GSM8K":12.59,
        "DROP":44.6,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":206.0,
        "Available on the hub":true,
        "Model sha":"cb69cda10a72bc9736b1c10181ac41f28b69ff9b",
        "model_name_for_query":"FlagAlpha\/Llama2-Chinese-13b-Chat"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":53.56,
        "ARC":64.59,
        "HellaSwag":86.17,
        "MMLU":60.5,
        "TruthfulQA":44.12,
        "Winogrande":79.32,
        "GSM8K":14.4,
        "DROP":25.81,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"gpl-3.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"7c40caaea4fe3264fd469dac428b0f9450e574a6",
        "model_name_for_query":"Secbone\/llama-33B-instructed"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.54,
        "ARC":55.29,
        "HellaSwag":76.63,
        "MMLU":55.29,
        "TruthfulQA":55.76,
        "Winogrande":72.77,
        "GSM8K":19.41,
        "DROP":39.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":12.0,
        "Available on the hub":true,
        "Model sha":"7ee3416f31a3c7e8d5ab4295ac1b641075f36345",
        "model_name_for_query":"maywell\/Synatra-V0.1-7B"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":53.54,
        "ARC":55.29,
        "HellaSwag":76.63,
        "MMLU":55.29,
        "TruthfulQA":55.76,
        "Winogrande":72.77,
        "GSM8K":19.41,
        "DROP":39.63,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":12.0,
        "Available on the hub":true,
        "Model sha":"7ee3416f31a3c7e8d5ab4295ac1b641075f36345",
        "model_name_for_query":"maywell\/Synatra-V0.1-7B-Instruct"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.53,
        "ARC":63.4,
        "HellaSwag":83.29,
        "MMLU":63.5,
        "TruthfulQA":50.06,
        "Winogrande":78.06,
        "GSM8K":28.66,
        "DROP":7.79,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":16.0,
        "Available on the hub":true,
        "Model sha":"5ba23522319a51d0af23b336a6a83c72ae3780e7",
        "model_name_for_query":"openaccess-ai-collective\/jackalope-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.51,
        "ARC":51.79,
        "HellaSwag":76.23,
        "MMLU":56.13,
        "TruthfulQA":49.7,
        "Winogrande":73.48,
        "GSM8K":24.34,
        "DROP":42.89,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":12.88,
        "Hub \u2764\ufe0f":16.0,
        "Available on the hub":true,
        "Model sha":"76fb7d00836eb2f1d9c9605d8881d73b782cf324",
        "model_name_for_query":"OpenBuddy\/openbuddy-llama2-13b-v11.1-bf16"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":53.5,
        "ARC":62.88,
        "HellaSwag":83.57,
        "MMLU":56.95,
        "TruthfulQA":49.52,
        "Winogrande":74.51,
        "GSM8K":0.91,
        "DROP":46.13,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"adc5e7befcc3d0a26f46198fdda4a098a2742fe6",
        "model_name_for_query":"CalderaAI\/13B-Thorns-l2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.48,
        "ARC":61.01,
        "HellaSwag":83.95,
        "MMLU":56.33,
        "TruthfulQA":50.18,
        "Winogrande":75.14,
        "GSM8K":11.98,
        "DROP":35.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"c4e7b771e30fdbfd6bd2e66a6928024bd5692bbd",
        "model_name_for_query":"Sao10K\/Stheno-L2-13B"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":53.47,
        "ARC":64.42,
        "HellaSwag":84.92,
        "MMLU":63.32,
        "TruthfulQA":55.56,
        "Winogrande":77.74,
        "GSM8K":20.77,
        "DROP":7.56,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":54.0,
        "Available on the hub":true,
        "Model sha":"aa5bd48c8b3040d1155a8fd59328df160aa63680",
        "model_name_for_query":"ehartford\/dolphin-2.1-mistral-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.47,
        "ARC":60.58,
        "HellaSwag":83.75,
        "MMLU":62.98,
        "TruthfulQA":47.9,
        "Winogrande":78.69,
        "GSM8K":19.18,
        "DROP":21.19,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"753852b8cb52dc5f0411568e98c0cb445a7835dc",
        "model_name_for_query":"uukuguy\/speechless-code-mistral-7b-v1.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.46,
        "ARC":63.91,
        "HellaSwag":85.67,
        "MMLU":58.28,
        "TruthfulQA":35.7,
        "Winogrande":80.11,
        "GSM8K":0.3,
        "DROP":50.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":39.0,
        "Available on the hub":true,
        "Model sha":"5818a6344f48dc5a324589b57cb288a9d54c0b79",
        "model_name_for_query":"ehartford\/based-30b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.44,
        "ARC":62.12,
        "HellaSwag":83.45,
        "MMLU":58.24,
        "TruthfulQA":50.81,
        "Winogrande":78.45,
        "GSM8K":14.25,
        "DROP":26.74,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":16.0,
        "Available on the hub":true,
        "Model sha":"c7b7cecb5a314fc66deebabcb67c230a3fbe84f7",
        "model_name_for_query":"TheBloke\/Wizard-Vicuna-30B-Uncensored-fp16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.44,
        "ARC":62.12,
        "HellaSwag":83.45,
        "MMLU":58.24,
        "TruthfulQA":50.81,
        "Winogrande":78.45,
        "GSM8K":14.25,
        "DROP":26.74,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":72.0,
        "Available on the hub":true,
        "Model sha":"6374baef4cedd41f85c111b8eec3eb38ee24c4b9",
        "model_name_for_query":"ehartford\/Wizard-Vicuna-30B-Uncensored"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.43,
        "ARC":62.97,
        "HellaSwag":83.49,
        "MMLU":62.3,
        "TruthfulQA":57.39,
        "Winogrande":77.43,
        "GSM8K":21.46,
        "DROP":9.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"b0134a7512444dfbb60a2e2d81469a5bbbb18026",
        "model_name_for_query":"Weyaxi\/SlimOpenOrca-Mistral-7B"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":53.39,
        "ARC":60.49,
        "HellaSwag":84.03,
        "MMLU":57.83,
        "TruthfulQA":54.52,
        "Winogrande":75.77,
        "GSM8K":2.96,
        "DROP":38.12,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"5a54eb9d5a66df4720ec52422f5627ccd94d5fd6",
        "model_name_for_query":"PulsarAI\/Chat-AYB-Platypus2-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.38,
        "ARC":62.97,
        "HellaSwag":84.6,
        "MMLU":63.29,
        "TruthfulQA":57.77,
        "Winogrande":77.51,
        "GSM8K":18.42,
        "DROP":9.13,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"41e912e0f79094a80687f88ca5555f84aa9d307f",
        "model_name_for_query":"uukuguy\/speechless-mistral-six-in-one-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.37,
        "ARC":63.99,
        "HellaSwag":85.0,
        "MMLU":63.44,
        "TruthfulQA":55.57,
        "Winogrande":77.9,
        "GSM8K":20.09,
        "DROP":7.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":54.0,
        "Available on the hub":true,
        "Model sha":"aa5bd48c8b3040d1155a8fd59328df160aa63680",
        "model_name_for_query":"ehartford\/dolphin-2.1-mistral-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.34,
        "ARC":62.54,
        "HellaSwag":83.86,
        "MMLU":62.77,
        "TruthfulQA":54.23,
        "Winogrande":77.43,
        "GSM8K":21.38,
        "DROP":11.2,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"a9744d8cf9ce4230678a891bcf8bba7cbc0aaece",
        "model_name_for_query":"Open-Orca\/Mistral-7B-SlimOrca"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.34,
        "ARC":64.33,
        "HellaSwag":84.4,
        "MMLU":63.72,
        "TruthfulQA":52.52,
        "Winogrande":78.37,
        "GSM8K":21.38,
        "DROP":8.66,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"d4039b40e842df7f6b8de50532444c8944ea5791",
        "model_name_for_query":"uukuguy\/speechless-mistral-dolphin-orca-platypus-samantha-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.33,
        "ARC":64.93,
        "HellaSwag":84.27,
        "MMLU":56.47,
        "TruthfulQA":58.65,
        "Winogrande":78.37,
        "GSM8K":7.73,
        "DROP":22.9,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":116.0,
        "Available on the hub":true,
        "Model sha":"24da9e88f2b2b7946bc6fe9412d6728b9adc2c3d",
        "model_name_for_query":"CalderaAI\/30B-Lazarus"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.33,
        "ARC":62.46,
        "HellaSwag":85.62,
        "MMLU":59.13,
        "TruthfulQA":55.63,
        "Winogrande":77.19,
        "GSM8K":10.92,
        "DROP":22.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":19.99,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":true,
        "Model sha":"cda06630a1d8173541431e5ce8bc17dcfaa37e5e",
        "model_name_for_query":"Undi95\/MLewd-ReMM-L2-Chat-20B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.31,
        "ARC":59.47,
        "HellaSwag":82.28,
        "MMLU":55.18,
        "TruthfulQA":47.6,
        "Winogrande":78.61,
        "GSM8K":10.77,
        "DROP":39.25,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"2ee11d9c7acaefb723796227e2ad099b165f0dd9",
        "model_name_for_query":"uukuguy\/speechless-hermes-coig-lite-13b"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":53.31,
        "ARC":63.48,
        "HellaSwag":86.09,
        "MMLU":63.93,
        "TruthfulQA":43.43,
        "Winogrande":82.56,
        "GSM8K":27.67,
        "DROP":5.98,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":65.29,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"4ae2e56610e8b9b9a78472708390668e9096b4f9",
        "model_name_for_query":"huggingface\/llama-65b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":53.27,
        "ARC":54.52,
        "HellaSwag":75.63,
        "MMLU":55.38,
        "TruthfulQA":56.28,
        "Winogrande":73.72,
        "GSM8K":14.25,
        "DROP":43.1,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":698.0,
        "Available on the hub":true,
        "Model sha":"7961f5aa9b736bf8e364b2e6f201190f97a27931",
        "model_name_for_query":"mistralai\/Mistral-7B-Instruct-v0.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.27,
        "ARC":63.23,
        "HellaSwag":83.68,
        "MMLU":54.9,
        "TruthfulQA":53.14,
        "Winogrande":77.51,
        "GSM8K":6.29,
        "DROP":34.13,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"763b3fd5afc3e7fb6c7c8768d40f06901c8d5913",
        "model_name_for_query":"royallab\/Pygmalion-2-13b-SuperCOT"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.25,
        "ARC":62.12,
        "HellaSwag":82.6,
        "MMLU":57.5,
        "TruthfulQA":48.29,
        "Winogrande":76.01,
        "GSM8K":12.89,
        "DROP":33.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"d11815287c51ef51485fb003f8f72773cf6f19a4",
        "model_name_for_query":"Undi95\/OpenRP-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.22,
        "ARC":59.56,
        "HellaSwag":82.26,
        "MMLU":55.3,
        "TruthfulQA":47.56,
        "Winogrande":78.53,
        "GSM8K":9.86,
        "DROP":39.5,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"2ee11d9c7acaefb723796227e2ad099b165f0dd9",
        "model_name_for_query":"uukuguy\/speechless-hermes-coig-lite-13b"
    },
    {
        "T":"\ud83d\udfe6",
        "Average \u2b06\ufe0f":53.18,
        "ARC":61.35,
        "HellaSwag":83.8,
        "MMLU":57.89,
        "TruthfulQA":51.18,
        "Winogrande":78.77,
        "GSM8K":31.46,
        "DROP":7.78,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":true,
        "Model sha":"e04207847429af03c4780f5ac85c726536217981",
        "model_name_for_query":"Yhyu13\/oasst-rlhf-2-llama-30b-7k-steps-hf"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":53.09,
        "ARC":59.3,
        "HellaSwag":78.43,
        "MMLU":57.69,
        "TruthfulQA":52.45,
        "Winogrande":76.09,
        "GSM8K":8.04,
        "DROP":39.67,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.44,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":true,
        "Model sha":"fc2535104c0b48afc42575f9fe10bbcbb7612ec3",
        "model_name_for_query":"minlik\/chinese-alpaca-33b-merged"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.06,
        "ARC":64.08,
        "HellaSwag":84.24,
        "MMLU":64.0,
        "TruthfulQA":56.19,
        "Winogrande":78.45,
        "GSM8K":16.15,
        "DROP":8.35,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"4ff48527af8c3907129c06160c7f7b7b786a5a79",
        "model_name_for_query":"Undi95\/Mistral-11B-TestBench9"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":53.06,
        "ARC":55.55,
        "HellaSwag":76.42,
        "MMLU":63.85,
        "TruthfulQA":41.86,
        "Winogrande":73.8,
        "GSM8K":12.66,
        "DROP":47.32,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"custom",
        "#Params (B)":6.0,
        "Hub \u2764\ufe0f":15.0,
        "Available on the hub":true,
        "Model sha":"d8029c814d8faa68e1aef2e488f668a3af5d1a8a",
        "model_name_for_query":"01-ai\/Yi-6B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.06,
        "ARC":64.85,
        "HellaSwag":85.08,
        "MMLU":56.56,
        "TruthfulQA":53.96,
        "Winogrande":80.03,
        "GSM8K":11.9,
        "DROP":19.07,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":124.0,
        "Available on the hub":true,
        "Model sha":"dc9d81f454d286ea040c5cd45b058aecaa51c13e",
        "model_name_for_query":"ausboss\/llama-30b-supercot"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.04,
        "ARC":62.8,
        "HellaSwag":83.23,
        "MMLU":60.01,
        "TruthfulQA":55.95,
        "Winogrande":75.93,
        "GSM8K":12.13,
        "DROP":21.25,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"66037b5ee553f7b878d796d2b2d5ada5734cc164",
        "model_name_for_query":"posicube\/Llama-chat-AY-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.02,
        "ARC":62.46,
        "HellaSwag":82.75,
        "MMLU":55.54,
        "TruthfulQA":50.11,
        "Winogrande":76.4,
        "GSM8K":12.28,
        "DROP":31.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":11.0,
        "Available on the hub":true,
        "Model sha":"8ea1fb205553cadbc90069d80a7e58281b6281c3",
        "model_name_for_query":"Open-Orca\/LlongOrca-13B-16k"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.01,
        "ARC":64.42,
        "HellaSwag":83.93,
        "MMLU":63.82,
        "TruthfulQA":56.68,
        "Winogrande":77.74,
        "GSM8K":14.94,
        "DROP":9.57,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"9aae2b156b24557bb98e515f3a90c7865529d2e9",
        "model_name_for_query":"Undi95\/Mistral-11B-TestBench11"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.01,
        "ARC":63.4,
        "HellaSwag":84.79,
        "MMLU":59.34,
        "TruthfulQA":55.62,
        "Winogrande":76.24,
        "GSM8K":11.3,
        "DROP":20.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":true,
        "Model sha":"cc7ca1b8f906b9f62ace094540f4ff4124dd581a",
        "model_name_for_query":"posicube\/Llama2-chat-AYB-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.0,
        "ARC":63.91,
        "HellaSwag":84.26,
        "MMLU":62.66,
        "TruthfulQA":53.84,
        "Winogrande":78.22,
        "GSM8K":19.94,
        "DROP":8.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"076c0f7de93307e8fb3ad3bd820fb5f73325ca70",
        "model_name_for_query":"Weyaxi\/Dolphin2.1-OpenOrca-7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":53.0,
        "ARC":58.7,
        "HellaSwag":79.74,
        "MMLU":55.1,
        "TruthfulQA":50.22,
        "Winogrande":75.69,
        "GSM8K":10.46,
        "DROP":41.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.97,
        "Hub \u2764\ufe0f":64.0,
        "Available on the hub":true,
        "Model sha":"576094cbf4988baf88b3bb66678be1db70bd720a",
        "model_name_for_query":"ziqingyang\/chinese-alpaca-2-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.97,
        "ARC":60.58,
        "HellaSwag":82.53,
        "MMLU":53.71,
        "TruthfulQA":54.46,
        "Winogrande":73.72,
        "GSM8K":4.32,
        "DROP":41.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":11.0,
        "Available on the hub":true,
        "Model sha":"69615d9a8e1547f2407afd3380868a99f780e008",
        "model_name_for_query":"The-Face-Of-Goonery\/Huginn-13b-FP16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.96,
        "ARC":62.88,
        "HellaSwag":83.41,
        "MMLU":62.05,
        "TruthfulQA":56.65,
        "Winogrande":77.58,
        "GSM8K":18.95,
        "DROP":9.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"7cd030ccdb169c2685fe028bb4380b91ad74920f",
        "model_name_for_query":"PulsarAI\/SlimOpenOrca-Mistral-7B-v2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.92,
        "ARC":64.16,
        "HellaSwag":84.25,
        "MMLU":62.7,
        "TruthfulQA":53.83,
        "Winogrande":77.66,
        "GSM8K":19.71,
        "DROP":8.15,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"076c0f7de93307e8fb3ad3bd820fb5f73325ca70",
        "model_name_for_query":"Weyaxi\/Dolphin2.1-OpenOrca-7B"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":52.91,
        "ARC":60.32,
        "HellaSwag":83.72,
        "MMLU":55.74,
        "TruthfulQA":52.18,
        "Winogrande":75.53,
        "GSM8K":0.91,
        "DROP":41.98,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"3d91f63d82abd598d5b80d24d74feb6b00b7d80f",
        "model_name_for_query":"TFLai\/MythoMix-Platypus2-13B-QLoRA-0.80-epoch"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":52.89,
        "ARC":59.9,
        "HellaSwag":83.29,
        "MMLU":56.69,
        "TruthfulQA":51.08,
        "Winogrande":75.22,
        "GSM8K":1.44,
        "DROP":42.65,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"6e49d3d205e7f2e15c01ace0901da8931bbaab3b",
        "model_name_for_query":"TFLai\/Nous-Hermes-Platypus2-13B-QLoRA-0.80-epoch"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":52.87,
        "ARC":57.0,
        "HellaSwag":82.25,
        "MMLU":54.21,
        "TruthfulQA":49.58,
        "Winogrande":73.09,
        "GSM8K":11.37,
        "DROP":42.57,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"a7d4b8a1683e33dd3c60064d7dd9d5c35691323f",
        "model_name_for_query":"Weyaxi\/Samantha-Nebula-7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.85,
        "ARC":62.63,
        "HellaSwag":81.49,
        "MMLU":56.17,
        "TruthfulQA":49.48,
        "Winogrande":76.48,
        "GSM8K":10.99,
        "DROP":32.69,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"a452045c96ae62379a98ef0d85666616a66e78a6",
        "model_name_for_query":"openbmb\/UltraLM-13b-v2.0"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":52.82,
        "ARC":57.17,
        "HellaSwag":81.72,
        "MMLU":55.25,
        "TruthfulQA":51.64,
        "Winogrande":73.24,
        "GSM8K":9.4,
        "DROP":41.33,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"ef964d514cc25a600b0de78fc469d1acbec34591",
        "model_name_for_query":"Weyaxi\/TekniumAiroboros-Nebula-7B"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":52.8,
        "ARC":60.58,
        "HellaSwag":82.04,
        "MMLU":55.57,
        "TruthfulQA":48.41,
        "Winogrande":76.32,
        "GSM8K":0.08,
        "DROP":46.63,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":19.2,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"a4e1f8f62740d676c25eedb4f29f4e776dcc0c22",
        "model_name_for_query":"Envoid\/Libra-19B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.79,
        "ARC":57.25,
        "HellaSwag":81.73,
        "MMLU":55.72,
        "TruthfulQA":41.53,
        "Winogrande":77.58,
        "GSM8K":14.03,
        "DROP":41.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"209da26cff560ab34064f277190ab63f8c970b93",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE5_4w-r8-q_k_v_o"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.76,
        "ARC":62.2,
        "HellaSwag":83.82,
        "MMLU":55.43,
        "TruthfulQA":53.32,
        "Winogrande":74.51,
        "GSM8K":12.05,
        "DROP":27.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"a5ef9385d9430a81778183d71b58eb2b869d6a7e",
        "model_name_for_query":"Undi95\/ReMM-Mistral-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.75,
        "ARC":63.82,
        "HellaSwag":84.7,
        "MMLU":61.49,
        "TruthfulQA":52.49,
        "Winogrande":79.79,
        "GSM8K":17.89,
        "DROP":9.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"b15f4310ea37fef99e4f16372a4b1f2342e27613",
        "model_name_for_query":"Aspik101\/llama-30b-2048-instruct-PL-lora_unload"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.74,
        "ARC":61.18,
        "HellaSwag":83.25,
        "MMLU":55.92,
        "TruthfulQA":51.08,
        "Winogrande":77.35,
        "GSM8K":2.05,
        "DROP":38.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":11.0,
        "Available on the hub":true,
        "Model sha":"ac1f326ea75a28197c4b8e7c015071e8eef64485",
        "model_name_for_query":"bofenghuang\/vigogne-2-13b-instruct"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.72,
        "ARC":61.86,
        "HellaSwag":83.81,
        "MMLU":57.0,
        "TruthfulQA":54.51,
        "Winogrande":75.77,
        "GSM8K":10.46,
        "DROP":25.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"f6181961a6a2f9ca534e1a8907b4a4459be6b6bd",
        "model_name_for_query":"Undi95\/MLewd-Chat-v2-13B"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":52.7,
        "ARC":59.22,
        "HellaSwag":81.92,
        "MMLU":56.67,
        "TruthfulQA":48.23,
        "Winogrande":77.19,
        "GSM8K":1.29,
        "DROP":44.41,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"c0d3c0a5d4e9001ea933c6b71ca3adc99d1f71a2",
        "model_name_for_query":"yeontaek\/llama-2-13b-Beluga-QLoRA"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":52.7,
        "ARC":57.94,
        "HellaSwag":79.55,
        "MMLU":55.2,
        "TruthfulQA":43.46,
        "Winogrande":76.56,
        "GSM8K":10.92,
        "DROP":45.28,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"2e95049edf02368bbd4b4f6ffb50bc8821e919bb",
        "model_name_for_query":"Voicelab\/trurl-2-13b-academic"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.68,
        "ARC":60.07,
        "HellaSwag":82.64,
        "MMLU":55.61,
        "TruthfulQA":46.58,
        "Winogrande":74.82,
        "GSM8K":4.93,
        "DROP":44.11,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":11.0,
        "Available on the hub":true,
        "Model sha":"8f96e561c8c795e383ca0faeb1696fa1e33e87de",
        "model_name_for_query":"IkariDev\/Athena-v1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.65,
        "ARC":58.79,
        "HellaSwag":79.93,
        "MMLU":56.77,
        "TruthfulQA":48.29,
        "Winogrande":75.93,
        "GSM8K":4.25,
        "DROP":44.59,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"6bf4cf6211489bdbea70585a4a5c0f39deefb4e5",
        "model_name_for_query":"uukuguy\/speechless-orca-platypus-coig-lite-4k-0.6e-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.58,
        "ARC":59.73,
        "HellaSwag":83.1,
        "MMLU":54.11,
        "TruthfulQA":49.94,
        "Winogrande":74.51,
        "GSM8K":2.96,
        "DROP":43.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"c4710577003a23ca8e9040d16dfb8f3e9bc5d636",
        "model_name_for_query":"Undi95\/ReMM-L2-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.58,
        "ARC":59.73,
        "HellaSwag":83.12,
        "MMLU":54.1,
        "TruthfulQA":49.94,
        "Winogrande":74.51,
        "GSM8K":2.96,
        "DROP":43.69,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"79e711178c6881496ae1f5635b08bc193f370709",
        "model_name_for_query":"Undi95\/ReMM-L2-13B-PIPPA"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.56,
        "ARC":59.47,
        "HellaSwag":82.45,
        "MMLU":55.83,
        "TruthfulQA":49.78,
        "Winogrande":75.45,
        "GSM8K":10.01,
        "DROP":34.95,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"fd311f52648825d6988d2f945918468ceb32289f",
        "model_name_for_query":"Undi95\/UndiMix-v1-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.56,
        "ARC":62.63,
        "HellaSwag":84.81,
        "MMLU":62.74,
        "TruthfulQA":50.98,
        "Winogrande":78.69,
        "GSM8K":18.65,
        "DROP":9.4,
        "Type":"fine-tuned",
        "Precision":"GPTQ",
        "Hub License":"llama2",
        "#Params (B)":72.82,
        "Hub \u2764\ufe0f":204.0,
        "Available on the hub":true,
        "Model sha":"054fbf6f65e7ab7691ec07ec9ad366acf2dd90bf",
        "model_name_for_query":"TheBloke\/Llama-2-70B-chat-GPTQ"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.55,
        "ARC":63.31,
        "HellaSwag":84.66,
        "MMLU":61.66,
        "TruthfulQA":53.35,
        "Winogrande":79.08,
        "GSM8K":16.83,
        "DROP":8.94,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"1a076bce564f03bd47951eecab628c541fb1a6ad",
        "model_name_for_query":"Aspik101\/llama-30b-instruct-2048-PL-lora"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":52.55,
        "ARC":62.37,
        "HellaSwag":85.5,
        "MMLU":62.76,
        "TruthfulQA":54.48,
        "Winogrande":77.58,
        "GSM8K":17.89,
        "DROP":7.22,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"58777f0563610fa770c4fa252c0350de71d4ab9d",
        "model_name_for_query":"teknium\/CollectiveCognition-v1-Mistral-7B"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":52.54,
        "ARC":63.74,
        "HellaSwag":84.87,
        "MMLU":58.54,
        "TruthfulQA":47.06,
        "Winogrande":77.03,
        "GSM8K":12.66,
        "DROP":23.9,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"5b6bd680b1c008e52521dc8c663dbc87820da3d0",
        "model_name_for_query":"jondurbin\/airoboros-33b-gpt4"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.54,
        "ARC":59.64,
        "HellaSwag":82.65,
        "MMLU":57.9,
        "TruthfulQA":43.44,
        "Winogrande":77.19,
        "GSM8K":9.7,
        "DROP":37.24,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"fd23b7d052eb7c18ecd2acc1be77c66b7b8d6dad",
        "model_name_for_query":"speechlessai\/speechless-llama2-dolphin-orca-platypus-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.5,
        "ARC":61.69,
        "HellaSwag":84.1,
        "MMLU":56.77,
        "TruthfulQA":48.05,
        "Winogrande":76.4,
        "GSM8K":12.51,
        "DROP":28.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"7ff32abd17851a769a031659e91e660f219be363",
        "model_name_for_query":"Brouz\/Slerpeno"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.4,
        "ARC":61.01,
        "HellaSwag":84.04,
        "MMLU":61.39,
        "TruthfulQA":57.9,
        "Winogrande":78.61,
        "GSM8K":14.03,
        "DROP":9.82,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":377.0,
        "Available on the hub":true,
        "Model sha":"2cd2cd16a6ab22585d643cf264fac73b18e7852a",
        "model_name_for_query":"HuggingFaceH4\/zephyr-7b-alpha"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.37,
        "ARC":63.31,
        "HellaSwag":84.25,
        "MMLU":58.15,
        "TruthfulQA":49.15,
        "Winogrande":76.48,
        "GSM8K":14.48,
        "DROP":20.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":true,
        "Model sha":"a58124cdc9f39ccd59d4290a8bdfda93ff3690dc",
        "model_name_for_query":"boomerchan\/magpie-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.35,
        "ARC":61.52,
        "HellaSwag":83.29,
        "MMLU":55.11,
        "TruthfulQA":50.38,
        "Winogrande":75.45,
        "GSM8K":10.08,
        "DROP":30.61,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":221.0,
        "Available on the hub":true,
        "Model sha":"8f95aa9cd207db7b24179fc779c2b8973e71bee2",
        "model_name_for_query":"NousResearch\/Nous-Hermes-Llama2-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.32,
        "ARC":60.24,
        "HellaSwag":82.93,
        "MMLU":56.8,
        "TruthfulQA":51.57,
        "Winogrande":74.35,
        "GSM8K":12.89,
        "DROP":27.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":118.0,
        "Available on the hub":true,
        "Model sha":"761783745fcb97831ad8035d3cbd5de484aca3ce",
        "model_name_for_query":"ehartford\/WizardLM-30B-Uncensored"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.29,
        "ARC":59.73,
        "HellaSwag":81.06,
        "MMLU":54.53,
        "TruthfulQA":38.64,
        "Winogrande":78.14,
        "GSM8K":14.03,
        "DROP":39.9,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"aeeded8db9eea97e2e6a2e19a006ce1acd110a82",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-huangyt_Fintune_1_17w-q_k_v_o_proj"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.25,
        "ARC":59.13,
        "HellaSwag":81.99,
        "MMLU":55.49,
        "TruthfulQA":51.57,
        "Winogrande":74.66,
        "GSM8K":11.22,
        "DROP":31.69,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":20.0,
        "Available on the hub":true,
        "Model sha":"6e918dc8beb1e764def5938fdb8e3f64ba40a456",
        "model_name_for_query":"YeungNLP\/firefly-llama2-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.23,
        "ARC":63.14,
        "HellaSwag":82.35,
        "MMLU":56.52,
        "TruthfulQA":51.81,
        "Winogrande":76.48,
        "GSM8K":13.12,
        "DROP":22.23,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":27.0,
        "Available on the hub":true,
        "Model sha":"99904e4119575f2c1606ca1e31d288f38a9f20b5",
        "model_name_for_query":"psmathur\/orca_mini_v3_13b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":52.23,
        "ARC":63.14,
        "HellaSwag":82.35,
        "MMLU":56.52,
        "TruthfulQA":51.81,
        "Winogrande":76.48,
        "GSM8K":13.12,
        "DROP":22.23,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":27.0,
        "Available on the hub":true,
        "Model sha":"72eec98f68d240a71d3da8a266917b6e754ae831",
        "model_name_for_query":"pankajmathur\/orca_mini_v3_13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.22,
        "ARC":61.01,
        "HellaSwag":83.93,
        "MMLU":55.7,
        "TruthfulQA":48.64,
        "Winogrande":76.09,
        "GSM8K":11.75,
        "DROP":28.43,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":15.0,
        "Available on the hub":true,
        "Model sha":"665948fc79acc2bcce3e9e7d2b0689ca43ae62d4",
        "model_name_for_query":"Gryphe\/MythoLogic-L2-13b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":52.21,
        "ARC":57.68,
        "HellaSwag":82.44,
        "MMLU":55.33,
        "TruthfulQA":43.61,
        "Winogrande":77.35,
        "GSM8K":6.6,
        "DROP":42.46,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":21.83,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"15bca3e9b25cc2f280fec21686ef3bc445217503",
        "model_name_for_query":"chargoddard\/platypus-2-22b-relora"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.21,
        "ARC":57.68,
        "HellaSwag":81.91,
        "MMLU":54.95,
        "TruthfulQA":41.31,
        "Winogrande":76.48,
        "GSM8K":12.05,
        "DROP":41.07,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"f76f93dad8408523e69c59abbb96ce6b1b9b9f69",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r8-q_k_v_o"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.2,
        "ARC":58.7,
        "HellaSwag":81.66,
        "MMLU":53.87,
        "TruthfulQA":43.02,
        "Winogrande":76.72,
        "GSM8K":13.8,
        "DROP":37.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"33fd8a46a711ab8c45698dae9601678dfd7b3d33",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE5_4w-r16-q_k_v_o"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.2,
        "ARC":64.42,
        "HellaSwag":84.93,
        "MMLU":60.35,
        "TruthfulQA":49.18,
        "Winogrande":77.51,
        "GSM8K":9.78,
        "DROP":19.21,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"b3254a827fb1dfe0d4e428bf5ab1c3a2bac82d68",
        "model_name_for_query":"jondurbin\/airoboros-33b-gpt4-1.2"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":52.19,
        "ARC":57.59,
        "HellaSwag":81.5,
        "MMLU":49.86,
        "TruthfulQA":52.59,
        "Winogrande":77.27,
        "GSM8K":10.69,
        "DROP":35.84,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":true,
        "Model sha":"85cfe8e6db2bee804873cfdb48955696cc5b0689",
        "model_name_for_query":"digitous\/13B-Chimera"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.17,
        "ARC":61.52,
        "HellaSwag":82.13,
        "MMLU":54.21,
        "TruthfulQA":55.91,
        "Winogrande":76.16,
        "GSM8K":0.38,
        "DROP":34.86,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"aa9912a2ac60abeac28b4566731cd903dcc582ac",
        "model_name_for_query":"MayaPH\/GodziLLa-30B"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":52.16,
        "ARC":64.08,
        "HellaSwag":85.08,
        "MMLU":63.91,
        "TruthfulQA":50.4,
        "Winogrande":78.53,
        "GSM8K":16.98,
        "DROP":6.13,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"5574a021f55a446a756dcbc776f1765aefc280a1",
        "model_name_for_query":"ehartford\/samantha-1.2-mistral-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.12,
        "ARC":60.75,
        "HellaSwag":83.61,
        "MMLU":56.51,
        "TruthfulQA":49.6,
        "Winogrande":75.37,
        "GSM8K":0.08,
        "DROP":38.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"0480a52799cb8e8de73bb41994df8b6b793937c7",
        "model_name_for_query":"garage-bAInd\/Camel-Platypus2-13B"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":52.11,
        "ARC":53.84,
        "HellaSwag":77.05,
        "MMLU":53.57,
        "TruthfulQA":44.06,
        "Winogrande":74.98,
        "GSM8K":17.06,
        "DROP":44.21,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"2df5ed76be7eff0962f2d816a64eca1e78e1cbf3",
        "model_name_for_query":"TigerResearch\/tigerbot-13b-base"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":52.11,
        "ARC":57.51,
        "HellaSwag":82.36,
        "MMLU":54.94,
        "TruthfulQA":43.62,
        "Winogrande":77.11,
        "GSM8K":6.29,
        "DROP":42.9,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":21.83,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"15bca3e9b25cc2f280fec21686ef3bc445217503",
        "model_name_for_query":"chargoddard\/platypus2-22b-relora"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.09,
        "ARC":58.02,
        "HellaSwag":80.15,
        "MMLU":57.26,
        "TruthfulQA":48.04,
        "Winogrande":75.45,
        "GSM8K":5.84,
        "DROP":39.88,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"081d1da5cfa2f6ad43abdf4fb5e41f8ec5846224",
        "model_name_for_query":"uukuguy\/speechless-orca-platypus-coig-lite-4k-0.5e-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.08,
        "ARC":59.3,
        "HellaSwag":81.2,
        "MMLU":55.58,
        "TruthfulQA":38.13,
        "Winogrande":76.8,
        "GSM8K":13.5,
        "DROP":40.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"71224344025dbfada6821c6a89cade1d8358dad1",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r16-q_k_v_o"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.07,
        "ARC":59.3,
        "HellaSwag":82.42,
        "MMLU":53.55,
        "TruthfulQA":52.46,
        "Winogrande":74.19,
        "GSM8K":9.55,
        "DROP":32.98,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"7870cc50b82b5cbebfa9935b6d73a9d20170299a",
        "model_name_for_query":"Undi95\/CreativityEngine"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.06,
        "ARC":56.06,
        "HellaSwag":81.89,
        "MMLU":55.04,
        "TruthfulQA":40.12,
        "Winogrande":76.56,
        "GSM8K":14.25,
        "DROP":40.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"f907fffbb08698040325b3f2e47200a1b48b3ed9",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r8-q_k_v_o"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.03,
        "ARC":61.26,
        "HellaSwag":83.26,
        "MMLU":55.04,
        "TruthfulQA":50.41,
        "Winogrande":75.37,
        "GSM8K":9.17,
        "DROP":29.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":221.0,
        "Available on the hub":true,
        "Model sha":"8f95aa9cd207db7b24179fc779c2b8973e71bee2",
        "model_name_for_query":"NousResearch\/Nous-Hermes-Llama2-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.03,
        "ARC":60.67,
        "HellaSwag":80.46,
        "MMLU":56.51,
        "TruthfulQA":51.03,
        "Winogrande":74.82,
        "GSM8K":11.75,
        "DROP":28.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"97279d20a8c7e2d0576c9ff4b2e15a421c40d58a",
        "model_name_for_query":"YeungNLP\/firefly-llama2-13b-v1.2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":52.0,
        "ARC":63.48,
        "HellaSwag":84.12,
        "MMLU":58.57,
        "TruthfulQA":52.86,
        "Winogrande":76.4,
        "GSM8K":13.27,
        "DROP":15.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"fe054ab749a69375285df40913a88bd40f1e2bf6",
        "model_name_for_query":"Sao10K\/Stheno-1.8-L2-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.99,
        "ARC":62.2,
        "HellaSwag":82.32,
        "MMLU":57.67,
        "TruthfulQA":49.6,
        "Winogrande":76.8,
        "GSM8K":12.89,
        "DROP":22.47,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"e77ec90f432bdffa210a0e4310d117e5d1c662df",
        "model_name_for_query":"circulus\/Llama-2-13b-orca-v1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.98,
        "ARC":61.18,
        "HellaSwag":83.21,
        "MMLU":55.13,
        "TruthfulQA":50.56,
        "Winogrande":75.14,
        "GSM8K":10.39,
        "DROP":28.28,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"5a45cb2a6442581ce32cc19c561c49cec1db4ebb",
        "model_name_for_query":"Undi95\/Nous-Hermes-13B-Code"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.96,
        "ARC":59.47,
        "HellaSwag":81.0,
        "MMLU":54.31,
        "TruthfulQA":38.17,
        "Winogrande":77.27,
        "GSM8K":13.27,
        "DROP":40.25,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"aa5b161b39900c5e80d5bb39d098f6333ad964f7",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-huangyt_Fintune_1_17w"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.95,
        "ARC":62.8,
        "HellaSwag":84.14,
        "MMLU":56.14,
        "TruthfulQA":51.06,
        "Winogrande":76.01,
        "GSM8K":12.59,
        "DROP":20.92,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"e63d24870c840d47e82b029e7f405baa10ad9ea4",
        "model_name_for_query":"Doctor-Shotgun\/CalliopeDS-v2-L2-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.94,
        "ARC":62.03,
        "HellaSwag":82.27,
        "MMLU":57.71,
        "TruthfulQA":49.61,
        "Winogrande":76.87,
        "GSM8K":13.8,
        "DROP":21.26,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"e77ec90f432bdffa210a0e4310d117e5d1c662df",
        "model_name_for_query":"circulus\/Llama-2-13b-orca-v1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.94,
        "ARC":62.03,
        "HellaSwag":82.27,
        "MMLU":57.71,
        "TruthfulQA":49.61,
        "Winogrande":76.87,
        "GSM8K":13.8,
        "DROP":21.26,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":107.0,
        "Available on the hub":true,
        "Model sha":"1d6eef4cc2b73f39600a568803ad8183f2da4514",
        "model_name_for_query":"stabilityai\/StableBeluga-13B"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":51.93,
        "ARC":64.85,
        "HellaSwag":85.72,
        "MMLU":58.51,
        "TruthfulQA":52.24,
        "Winogrande":80.19,
        "GSM8K":15.54,
        "DROP":6.44,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":9.0,
        "Available on the hub":true,
        "Model sha":"3c8007467a081dc72ae09b9d358416b056b38920",
        "model_name_for_query":"TheBloke\/gpt4-alpaca-lora-30b-HF"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.9,
        "ARC":61.09,
        "HellaSwag":84.03,
        "MMLU":55.73,
        "TruthfulQA":44.96,
        "Winogrande":74.98,
        "GSM8K":15.85,
        "DROP":26.66,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"f587f4a31de6818f4200d9cdc7f116ca8ba1cdc2",
        "model_name_for_query":"WhoTookMyAmogusNickname\/NewHope_HF_not_official"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.9,
        "ARC":58.53,
        "HellaSwag":82.47,
        "MMLU":53.9,
        "TruthfulQA":37.92,
        "Winogrande":76.8,
        "GSM8K":12.81,
        "DROP":40.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"d74752b931bfddaa063a292e7ea85dfb1d7a4998",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-huangyt_FINETUNE2_3w-q_k_v_o_proj"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.89,
        "ARC":56.23,
        "HellaSwag":81.98,
        "MMLU":55.87,
        "TruthfulQA":39.76,
        "Winogrande":76.72,
        "GSM8K":11.52,
        "DROP":41.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"cc3c5e5a874cf4ff4f94ea919e819f8a914c8acb",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r16-q_k_v_o"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.89,
        "ARC":57.25,
        "HellaSwag":81.49,
        "MMLU":55.9,
        "TruthfulQA":39.79,
        "Winogrande":75.77,
        "GSM8K":12.05,
        "DROP":40.95,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"a12fb5937e6904977e8123b0d5ef21283b6895d4",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r16-q_k_v_o_gate_up_down"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":51.88,
        "ARC":62.97,
        "HellaSwag":84.66,
        "MMLU":62.2,
        "TruthfulQA":52.96,
        "Winogrande":78.61,
        "GSM8K":11.98,
        "DROP":9.74,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"ae9e37811a54ffe45f41a572c7e68363aa11b062",
        "model_name_for_query":"mncai\/Mistral-7B-OpenOrca-1k"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.87,
        "ARC":61.52,
        "HellaSwag":84.06,
        "MMLU":60.23,
        "TruthfulQA":51.05,
        "Winogrande":80.82,
        "GSM8K":17.66,
        "DROP":7.75,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"1114ff08ed15ef417502da58f0237d2f6650c9ce",
        "model_name_for_query":"gaodrew\/gaodrew-llama-30b-instruct-2048-Open-Platypus-100steps"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.85,
        "ARC":61.69,
        "HellaSwag":84.98,
        "MMLU":56.98,
        "TruthfulQA":54.16,
        "Winogrande":76.09,
        "GSM8K":8.49,
        "DROP":20.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":19.99,
        "Hub \u2764\ufe0f":16.0,
        "Available on the hub":true,
        "Model sha":"e4c23af4f5dd88cb27d245e2bfc3b81db652632c",
        "model_name_for_query":"Undi95\/Emerhyst-20B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.85,
        "ARC":59.64,
        "HellaSwag":82.7,
        "MMLU":58.3,
        "TruthfulQA":56.0,
        "Winogrande":75.37,
        "GSM8K":13.12,
        "DROP":17.81,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":24.0,
        "Available on the hub":true,
        "Model sha":"4410d8a20871927e9fe981c01bc8314b451b2fcd",
        "model_name_for_query":"uukuguy\/speechless-llama2-hermes-orca-platypus-wizardlm-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.83,
        "ARC":59.04,
        "HellaSwag":81.15,
        "MMLU":53.0,
        "TruthfulQA":40.16,
        "Winogrande":76.48,
        "GSM8K":11.9,
        "DROP":41.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"ac40ecf48cf5f7168e8c3929632c654bc834c3d7",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r4-q_k_v_o"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.82,
        "ARC":61.35,
        "HellaSwag":83.44,
        "MMLU":58.49,
        "TruthfulQA":48.19,
        "Winogrande":76.09,
        "GSM8K":24.11,
        "DROP":11.04,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"55d31300f8972b56320855bb40efb5e3d1e1a6fc",
        "model_name_for_query":"gradientputri\/MegaMix-T1-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.8,
        "ARC":54.27,
        "HellaSwag":75.2,
        "MMLU":56.12,
        "TruthfulQA":43.92,
        "Winogrande":73.56,
        "GSM8K":24.79,
        "DROP":34.74,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"68aad9f8452b2abf7d5415d48c09bd55d5b7ca05",
        "model_name_for_query":"uukuguy\/speechless-codellama-34b-v1.9"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":51.79,
        "ARC":62.03,
        "HellaSwag":82.65,
        "MMLU":54.11,
        "TruthfulQA":42.98,
        "Winogrande":76.95,
        "GSM8K":0.15,
        "DROP":43.65,
        "Type":"instruction-tuned",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"2a1b03977395eee44742abda63a4787ea5371d06",
        "model_name_for_query":"Enno-Ai\/vigogne2-enno-13b-sft-lora-4bit"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.78,
        "ARC":59.47,
        "HellaSwag":82.47,
        "MMLU":54.83,
        "TruthfulQA":44.65,
        "Winogrande":75.06,
        "GSM8K":3.56,
        "DROP":42.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":true,
        "Model sha":"172e30e56e939f73d7d00a165c2d49cbd284481f",
        "model_name_for_query":"jondurbin\/airoboros-l2-13b-2.1"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":51.77,
        "ARC":58.36,
        "HellaSwag":81.03,
        "MMLU":54.7,
        "TruthfulQA":52.98,
        "Winogrande":72.85,
        "GSM8K":5.53,
        "DROP":36.94,
        "Type":"instruction-tuned",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"7a2eed5038addcf4fa3b8dd358b45eb96134e749",
        "model_name_for_query":"totally-not-an-llm\/EverythingLM-13b-V3-peft"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.77,
        "ARC":61.95,
        "HellaSwag":83.88,
        "MMLU":56.9,
        "TruthfulQA":48.96,
        "Winogrande":76.16,
        "GSM8K":13.72,
        "DROP":20.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"6dd97c74cfe1d22432d5c993814e230f333ba401",
        "model_name_for_query":"Undi95\/UndiMix-v4-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.76,
        "ARC":59.64,
        "HellaSwag":82.07,
        "MMLU":50.34,
        "TruthfulQA":47.74,
        "Winogrande":77.11,
        "GSM8K":7.81,
        "DROP":37.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"315aa0924dd42840b8cced581c9db1240f9bae1d",
        "model_name_for_query":"CalderaAI\/13B-BlueMethod"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":51.76,
        "ARC":61.26,
        "HellaSwag":82.31,
        "MMLU":55.21,
        "TruthfulQA":41.91,
        "Winogrande":75.77,
        "GSM8K":0.91,
        "DROP":44.96,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"71aa919fc15fa9d9def9185791b15a3f76e7bd8d",
        "model_name_for_query":"pe-nlp\/llama-2-13b-platypus-vicuna-wizard"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":51.76,
        "ARC":65.02,
        "HellaSwag":86.2,
        "MMLU":58.73,
        "TruthfulQA":49.75,
        "Winogrande":80.03,
        "GSM8K":16.22,
        "DROP":6.36,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"gpl",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":11.0,
        "Available on the hub":true,
        "Model sha":"17114520801da7b9599fe7a9fdf238915713a59b",
        "model_name_for_query":"bavest\/fin-llama-33b-merged"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.71,
        "ARC":58.96,
        "HellaSwag":81.72,
        "MMLU":53.16,
        "TruthfulQA":44.68,
        "Winogrande":74.35,
        "GSM8K":5.99,
        "DROP":43.14,
        "Type":"fine-tuned",
        "Precision":"GPTQ",
        "Hub License":"llama2",
        "#Params (B)":16.23,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":true,
        "Model sha":"d90d96e40b9359cb5c35e6b6c8f0eb24896e827b",
        "model_name_for_query":"TheBloke\/Airoboros-L2-13B-2.1-GPTQ"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.69,
        "ARC":58.36,
        "HellaSwag":81.1,
        "MMLU":54.53,
        "TruthfulQA":37.02,
        "Winogrande":76.64,
        "GSM8K":12.28,
        "DROP":41.86,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"5cbcd9c0a6b9a19f0d099e653cde18e11bf95303",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE5_4w-r4-q_k_v_o"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":51.68,
        "ARC":59.22,
        "HellaSwag":80.66,
        "MMLU":54.52,
        "TruthfulQA":40.42,
        "Winogrande":76.32,
        "GSM8K":5.38,
        "DROP":45.24,
        "Type":"instruction-tuned",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":22.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"26fdd8fa420d72ed835c7d17086f0441db0985d4",
        "model_name_for_query":"chargoddard\/ypotryll-22b-epoch2-qlora"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.68,
        "ARC":56.57,
        "HellaSwag":82.11,
        "MMLU":50.44,
        "TruthfulQA":51.5,
        "Winogrande":75.3,
        "GSM8K":8.34,
        "DROP":37.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"gpl",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":388.0,
        "Available on the hub":true,
        "Model sha":"24e8c03148ffd1f3e469744dfc24ad2ad82848f8",
        "model_name_for_query":"NousResearch\/Nous-Hermes-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.68,
        "ARC":51.45,
        "HellaSwag":73.99,
        "MMLU":62.08,
        "TruthfulQA":47.01,
        "Winogrande":68.43,
        "GSM8K":20.62,
        "DROP":38.14,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"e6c38a7d2f4ba7b867fff421c08c02ba1908224e",
        "model_name_for_query":"JosephusCheung\/Pwen-7B-Chat-20_30"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.67,
        "ARC":62.03,
        "HellaSwag":81.85,
        "MMLU":58.52,
        "TruthfulQA":55.7,
        "Winogrande":76.56,
        "GSM8K":13.95,
        "DROP":13.12,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"c6362c4fc0dc03420e3c08454b2e7689e4e32d3a",
        "model_name_for_query":"uukuguy\/speechless-llama2-13b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":51.62,
        "ARC":62.2,
        "HellaSwag":84.1,
        "MMLU":64.14,
        "TruthfulQA":46.94,
        "Winogrande":78.69,
        "GSM8K":18.5,
        "DROP":6.76,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"9609a969ba6429b84e538d96afac55eb133a9983",
        "model_name_for_query":"PeanutJar\/Mistral-v0.1-PeanutButter-v0.0.0-7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.58,
        "ARC":61.77,
        "HellaSwag":83.51,
        "MMLU":63.99,
        "TruthfulQA":47.46,
        "Winogrande":78.3,
        "GSM8K":19.03,
        "DROP":7.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"b66724f8195e7b76289f8f3f72a98392557c46ad",
        "model_name_for_query":"unaidedelf87777\/wizard-mistral-v0.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.58,
        "ARC":60.24,
        "HellaSwag":81.39,
        "MMLU":50.92,
        "TruthfulQA":54.56,
        "Winogrande":75.06,
        "GSM8K":8.11,
        "DROP":30.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":69.0,
        "Available on the hub":true,
        "Model sha":"badd80f8a6f46fb15310fedf6d4db54959854897",
        "model_name_for_query":"WizardLM\/WizardLM-13B-V1.1"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":51.57,
        "ARC":59.73,
        "HellaSwag":82.91,
        "MMLU":54.77,
        "TruthfulQA":45.14,
        "Winogrande":74.03,
        "GSM8K":2.81,
        "DROP":41.62,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"ebf991c8d34314caab6ccc6b078c681d20bac39a",
        "model_name_for_query":"yeontaek\/airoboros-2.1-llama-2-13B-QLoRa"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.56,
        "ARC":61.26,
        "HellaSwag":82.93,
        "MMLU":56.47,
        "TruthfulQA":47.27,
        "Winogrande":76.48,
        "GSM8K":10.99,
        "DROP":25.48,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":true,
        "Model sha":"60d4937ac3c4dcb84c40bbf7265c5cc7f5f3d4f9",
        "model_name_for_query":"migtissera\/Synthia-13B-v1.2"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":51.53,
        "ARC":62.29,
        "HellaSwag":83.27,
        "MMLU":59.47,
        "TruthfulQA":51.79,
        "Winogrande":77.35,
        "GSM8K":7.73,
        "DROP":18.82,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"0ce62a64ca53cd5feb18f523a96dd3be86e6513d",
        "model_name_for_query":"PulsarAI\/GenAI-Nova-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.52,
        "ARC":63.65,
        "HellaSwag":83.47,
        "MMLU":59.82,
        "TruthfulQA":55.94,
        "Winogrande":76.48,
        "GSM8K":9.25,
        "DROP":12.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"1aca45d37eade21eb381aaefc9245b58ec3b7b26",
        "model_name_for_query":"PulsarAI\/2x-LoRA-Assemble-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.51,
        "ARC":61.6,
        "HellaSwag":83.49,
        "MMLU":58.26,
        "TruthfulQA":47.48,
        "Winogrande":76.16,
        "GSM8K":24.11,
        "DROP":9.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"14e0756c210bcf420fbf825e6b8087ee5c716e7f",
        "model_name_for_query":"gradientputri\/MegaMix-A1-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.46,
        "ARC":57.08,
        "HellaSwag":81.24,
        "MMLU":56.67,
        "TruthfulQA":51.51,
        "Winogrande":74.66,
        "GSM8K":11.3,
        "DROP":27.73,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":100.0,
        "Available on the hub":true,
        "Model sha":"3deb0106f72a3a433f0c6ea0cb978bdf14bcd3a6",
        "model_name_for_query":"lmsys\/vicuna-13b-v1.5"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.45,
        "ARC":63.91,
        "HellaSwag":85.0,
        "MMLU":59.44,
        "TruthfulQA":49.83,
        "Winogrande":80.35,
        "GSM8K":15.01,
        "DROP":6.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":9.0,
        "Available on the hub":true,
        "Model sha":"a4deca117c5fa48f2cdc49ed2e2596046201d688",
        "model_name_for_query":"Henk717\/chronoboros-33B"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":51.43,
        "ARC":64.42,
        "HellaSwag":85.21,
        "MMLU":59.79,
        "TruthfulQA":50.59,
        "Winogrande":79.32,
        "GSM8K":13.72,
        "DROP":6.93,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.53,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":true,
        "Model sha":"06843c6693cc265dabb464c818a3d3713239721a",
        "model_name_for_query":"Henk717\/airochronos-33B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.41,
        "ARC":63.31,
        "HellaSwag":83.53,
        "MMLU":59.67,
        "TruthfulQA":55.8,
        "Winogrande":76.09,
        "GSM8K":8.87,
        "DROP":12.62,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":12.0,
        "Available on the hub":true,
        "Model sha":"dd12dced8076a959c03b8b5c4a4266f234d6639a",
        "model_name_for_query":"posicube\/Llama2-chat-AYT-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.41,
        "ARC":63.14,
        "HellaSwag":84.52,
        "MMLU":59.89,
        "TruthfulQA":55.48,
        "Winogrande":76.95,
        "GSM8K":9.17,
        "DROP":10.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"71edf22c49677d0239caf5f87d8139dd9cc79078",
        "model_name_for_query":"Riiid\/sheep-duck-llama-2-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.39,
        "ARC":62.29,
        "HellaSwag":83.69,
        "MMLU":55.7,
        "TruthfulQA":50.94,
        "Winogrande":75.93,
        "GSM8K":12.81,
        "DROP":18.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"f7696299463d8ec402a4e1eb001f3a447f1c5552",
        "model_name_for_query":"Undi95\/Emerald-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.36,
        "ARC":63.57,
        "HellaSwag":83.51,
        "MMLU":59.82,
        "TruthfulQA":55.96,
        "Winogrande":76.16,
        "GSM8K":8.42,
        "DROP":12.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"85bb49d333dba4a08b051418663d16853ce30cee",
        "model_name_for_query":"oh-yeontaek\/llama-2-13B-LoRA-assemble"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.36,
        "ARC":64.25,
        "HellaSwag":85.2,
        "MMLU":59.83,
        "TruthfulQA":50.56,
        "Winogrande":79.08,
        "GSM8K":13.57,
        "DROP":7.01,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":32.53,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":true,
        "Model sha":"06843c6693cc265dabb464c818a3d3713239721a",
        "model_name_for_query":"Henk717\/airochronos-33B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.33,
        "ARC":59.13,
        "HellaSwag":80.64,
        "MMLU":56.12,
        "TruthfulQA":51.29,
        "Winogrande":74.66,
        "GSM8K":10.54,
        "DROP":26.89,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"848ef91ab46a72260542283918a971347c6bfa93",
        "model_name_for_query":"luffycodes\/nash-vicuna-13b-v1dot5-ep2-w-rag-w-simple"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.32,
        "ARC":52.99,
        "HellaSwag":75.38,
        "MMLU":51.36,
        "TruthfulQA":47.94,
        "Winogrande":71.03,
        "GSM8K":18.88,
        "DROP":41.63,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":12.88,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"4d4e72c553e9d60fdc208663b0a1c0364caa2f30",
        "model_name_for_query":"OpenBuddy\/openbuddy-llama2-13b-v11-bf16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.31,
        "ARC":56.4,
        "HellaSwag":79.13,
        "MMLU":49.61,
        "TruthfulQA":49.62,
        "Winogrande":76.56,
        "GSM8K":12.51,
        "DROP":35.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":9.0,
        "Available on the hub":true,
        "Model sha":"b5ae4519d4c8f4559a0aa80b6efe2008413ece01",
        "model_name_for_query":"openaccess-ai-collective\/minotaur-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.29,
        "ARC":62.03,
        "HellaSwag":84.19,
        "MMLU":58.75,
        "TruthfulQA":52.84,
        "Winogrande":77.43,
        "GSM8K":11.3,
        "DROP":12.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":13.0,
        "Available on the hub":true,
        "Model sha":"6c66622a99c1bc73498aa6a15a59da825d875310",
        "model_name_for_query":"Undi95\/MLewd-L2-Chat-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.29,
        "ARC":63.23,
        "HellaSwag":85.33,
        "MMLU":57.36,
        "TruthfulQA":51.65,
        "Winogrande":76.09,
        "GSM8K":10.92,
        "DROP":14.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":19.99,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"ac279478abd9ddb8d1f5adcc548be0287b963adf",
        "model_name_for_query":"Undi95\/MXLewd-L2-20B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.29,
        "ARC":59.47,
        "HellaSwag":83.02,
        "MMLU":62.25,
        "TruthfulQA":53.39,
        "Winogrande":78.77,
        "GSM8K":14.78,
        "DROP":7.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.98,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"e13dd23ae5e611e959b6c8d5bc47bf4fd37cd9d7",
        "model_name_for_query":"quantumaikr\/QuantumLM-70B-hf"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":51.28,
        "ARC":63.4,
        "HellaSwag":84.1,
        "MMLU":61.36,
        "TruthfulQA":46.08,
        "Winogrande":76.8,
        "GSM8K":16.0,
        "DROP":11.22,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":17.0,
        "Available on the hub":true,
        "Model sha":"7f9e40543fdff8c3e58eca0390c8a631829c1206",
        "model_name_for_query":"ehartford\/samantha-mistral-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.27,
        "ARC":55.03,
        "HellaSwag":81.97,
        "MMLU":56.64,
        "TruthfulQA":38.07,
        "Winogrande":77.19,
        "GSM8K":12.21,
        "DROP":37.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"555486843f613276b6edb480f6d37b9203daa226",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r16-gate_up_down"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.25,
        "ARC":56.31,
        "HellaSwag":81.43,
        "MMLU":55.3,
        "TruthfulQA":39.11,
        "Winogrande":76.8,
        "GSM8K":10.46,
        "DROP":39.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"0d8d502e4e5ef89592dd0d3bc7223eaf7f77f78b",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r4-q_k_v_o_gate_up_down"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.24,
        "ARC":58.45,
        "HellaSwag":84.31,
        "MMLU":49.15,
        "TruthfulQA":38.05,
        "Winogrande":75.14,
        "GSM8K":15.31,
        "DROP":38.28,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-3.0",
        "#Params (B)":29.96,
        "Hub \u2764\ufe0f":93.0,
        "Available on the hub":true,
        "Model sha":"2abf1163dd8c9b11f07d805c06e6ec90a1f2037e",
        "model_name_for_query":"mosaicml\/mpt-30b-instruct"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.23,
        "ARC":57.94,
        "HellaSwag":81.19,
        "MMLU":53.43,
        "TruthfulQA":40.48,
        "Winogrande":76.72,
        "GSM8K":10.84,
        "DROP":37.99,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"15f1b122d60631091419cb8e668a28737b92a0e0",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r8-q_k_v_o_gate_up_down"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":51.22,
        "ARC":61.77,
        "HellaSwag":84.11,
        "MMLU":64.38,
        "TruthfulQA":45.92,
        "Winogrande":78.37,
        "GSM8K":17.44,
        "DROP":6.53,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"f4d471d7a9447d0969a58d5b3146d50cfa3005b3",
        "model_name_for_query":"PeanutJar\/Mistral-v0.1-PeanutButter-v0.0.2-7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.2,
        "ARC":62.63,
        "HellaSwag":83.17,
        "MMLU":55.91,
        "TruthfulQA":52.43,
        "Winogrande":74.74,
        "GSM8K":10.84,
        "DROP":18.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"4328809e568f01e3f0a05764e3bb58e901310415",
        "model_name_for_query":"Undi95\/Amethyst-13B-Mistral"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.2,
        "ARC":62.63,
        "HellaSwag":83.17,
        "MMLU":55.91,
        "TruthfulQA":52.43,
        "Winogrande":74.74,
        "GSM8K":10.84,
        "DROP":18.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"d4a85b1006f0b9439e64f0e7400533a7b867c24d",
        "model_name_for_query":"Undi95\/Amethyst-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.19,
        "ARC":54.78,
        "HellaSwag":81.4,
        "MMLU":54.73,
        "TruthfulQA":41.02,
        "Winogrande":76.64,
        "GSM8K":10.54,
        "DROP":39.24,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"8702b433008a62e9f8bf15e70ba15fa7100e991c",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r4-q_k_v_o"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.19,
        "ARC":61.6,
        "HellaSwag":82.53,
        "MMLU":63.08,
        "TruthfulQA":38.82,
        "Winogrande":78.93,
        "GSM8K":27.45,
        "DROP":5.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":19.0,
        "Available on the hub":true,
        "Model sha":"3fa4546259d6bbd6b5d637484c325ab19181a73c",
        "model_name_for_query":"TheBloke\/dromedary-65b-lora-HF"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.18,
        "ARC":59.81,
        "HellaSwag":83.71,
        "MMLU":54.86,
        "TruthfulQA":47.79,
        "Winogrande":76.16,
        "GSM8K":8.95,
        "DROP":26.99,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"2fcef275782b2c1061cf671d889aea652d13236c",
        "model_name_for_query":"jondurbin\/airoboros-l2-13b-3.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.18,
        "ARC":60.92,
        "HellaSwag":81.94,
        "MMLU":58.9,
        "TruthfulQA":57.19,
        "Winogrande":75.93,
        "GSM8K":9.02,
        "DROP":14.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"948ee7af94a8b092807df4becfc0a8c1cd042878",
        "model_name_for_query":"Sao10K\/BrainDerp2"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":51.17,
        "ARC":58.96,
        "HellaSwag":82.46,
        "MMLU":54.62,
        "TruthfulQA":47.71,
        "Winogrande":75.14,
        "GSM8K":0.0,
        "DROP":39.32,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"45bd1e47218ba2e075e03f6407980eb839e67eb3",
        "model_name_for_query":"TFLai\/Airboros2.1-Platypus2-13B-QLora-0.80-epoch"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.17,
        "ARC":53.92,
        "HellaSwag":80.66,
        "MMLU":53.19,
        "TruthfulQA":43.84,
        "Winogrande":75.61,
        "GSM8K":14.25,
        "DROP":36.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"532aeb363b0ceee155b3cf9479ef635b797cee7c",
        "model_name_for_query":"TheBloke\/tulu-13B-fp16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.17,
        "ARC":62.63,
        "HellaSwag":85.59,
        "MMLU":57.77,
        "TruthfulQA":51.02,
        "Winogrande":81.45,
        "GSM8K":13.34,
        "DROP":6.36,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":40.0,
        "Hub \u2764\ufe0f":38.0,
        "Available on the hub":false,
        "Model sha":"3d5084b6fbcb9f9f36493d9fd1e3795b0b9860f0",
        "model_name_for_query":"dfurman\/falcon-40b-openassistant-peft"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.16,
        "ARC":60.75,
        "HellaSwag":82.1,
        "MMLU":58.81,
        "TruthfulQA":56.9,
        "Winogrande":75.85,
        "GSM8K":8.26,
        "DROP":15.48,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"ba21a7ed5458b3fa2b05ce6aab431acd1f857516",
        "model_name_for_query":"Sao10K\/BrainDerp"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":51.16,
        "ARC":63.65,
        "HellaSwag":82.92,
        "MMLU":58.7,
        "TruthfulQA":55.55,
        "Winogrande":77.03,
        "GSM8K":10.01,
        "DROP":10.25,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"bf152c36935acd67a9029c017f0c1ff2d7a92314",
        "model_name_for_query":"Weyaxi\/Luban-Marcoroni-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.14,
        "ARC":60.84,
        "HellaSwag":85.18,
        "MMLU":56.45,
        "TruthfulQA":53.33,
        "Winogrande":75.77,
        "GSM8K":7.73,
        "DROP":18.66,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":19.99,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"37869800c15fb37d017ea83bb50fec6d6141f6ba",
        "model_name_for_query":"Undi95\/MM-ReMM-L2-20B"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":51.13,
        "ARC":60.58,
        "HellaSwag":82.56,
        "MMLU":58.25,
        "TruthfulQA":54.77,
        "Winogrande":74.9,
        "GSM8K":0.91,
        "DROP":25.96,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"f147bf8428c174d1dc0332da626d4b039690ceab",
        "model_name_for_query":"PulsarAI\/2x-LoRA-Assemble-Platypus2-13B"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":51.13,
        "ARC":63.74,
        "HellaSwag":82.88,
        "MMLU":58.64,
        "TruthfulQA":55.56,
        "Winogrande":76.87,
        "GSM8K":9.93,
        "DROP":10.25,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"9b68680ed8351ef8ef6948169e69a888af40002e",
        "model_name_for_query":"Weyaxi\/Luban-Marcoroni-13B-v3"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.12,
        "ARC":59.04,
        "HellaSwag":82.82,
        "MMLU":54.71,
        "TruthfulQA":36.47,
        "Winogrande":74.19,
        "GSM8K":7.73,
        "DROP":42.9,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":14.0,
        "Available on the hub":true,
        "Model sha":"ec556571acc6783fea4414e4ca72d291c563b6dc",
        "model_name_for_query":"jondurbin\/airoboros-l2-13b-gpt4-2.0"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":51.11,
        "ARC":63.48,
        "HellaSwag":82.89,
        "MMLU":58.72,
        "TruthfulQA":55.56,
        "Winogrande":76.95,
        "GSM8K":9.93,
        "DROP":10.25,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"d7c704a08218dcc03963bc08e9113e281c056f53",
        "model_name_for_query":"Weyaxi\/Luban-Marcoroni-13B-v2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.1,
        "ARC":61.09,
        "HellaSwag":83.86,
        "MMLU":55.42,
        "TruthfulQA":52.08,
        "Winogrande":75.45,
        "GSM8K":9.93,
        "DROP":19.86,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":15.0,
        "Available on the hub":true,
        "Model sha":"eca790fb9394c9c61be27ef709080b3b92783a45",
        "model_name_for_query":"Gryphe\/MythoMix-L2-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.1,
        "ARC":55.89,
        "HellaSwag":81.38,
        "MMLU":53.77,
        "TruthfulQA":40.25,
        "Winogrande":76.72,
        "GSM8K":12.28,
        "DROP":37.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"a8b15badead658df6ec5b884b813962b9fd29cfb",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE5_4w-r4-q_k_v_o_gate_up_down"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.1,
        "ARC":60.92,
        "HellaSwag":82.1,
        "MMLU":58.91,
        "TruthfulQA":57.18,
        "Winogrande":75.61,
        "GSM8K":8.04,
        "DROP":14.92,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"0b575b9245406cca92942ce2ababb5b868109bed",
        "model_name_for_query":"Sao10K\/BrainDerp3"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.08,
        "ARC":62.8,
        "HellaSwag":84.13,
        "MMLU":56.87,
        "TruthfulQA":55.49,
        "Winogrande":79.08,
        "GSM8K":11.37,
        "DROP":7.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"eeb29b35ceb6dd5c532f1e4e1235f1cdd3f51f23",
        "model_name_for_query":"Aspik101\/30B-Lazarus-instruct-PL-lora_unload"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.07,
        "ARC":52.47,
        "HellaSwag":74.13,
        "MMLU":53.47,
        "TruthfulQA":47.14,
        "Winogrande":73.24,
        "GSM8K":14.71,
        "DROP":42.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"1d64d871cd56da3031e19bc267ef8bd0b85b9936",
        "model_name_for_query":"speechlessai\/speechless-codellama-34b-v1.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":51.07,
        "ARC":52.47,
        "HellaSwag":74.13,
        "MMLU":53.47,
        "TruthfulQA":47.14,
        "Winogrande":73.24,
        "GSM8K":14.71,
        "DROP":42.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":true,
        "Model sha":"57e18e617b4fd7ab61bd7da8ee9516513ad76842",
        "model_name_for_query":"uukuguy\/speechless-codellama-dolphin-orca-platypus-34b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":51.02,
        "ARC":53.5,
        "HellaSwag":75.14,
        "MMLU":51.72,
        "TruthfulQA":58.81,
        "Winogrande":70.4,
        "GSM8K":10.84,
        "DROP":36.73,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":14.0,
        "Available on the hub":true,
        "Model sha":"3a33eea0858d411617c472c3c0ae39f17d2b3f5d",
        "model_name_for_query":"ehartford\/samantha-mistral-instruct-7b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":51.0,
        "ARC":62.46,
        "HellaSwag":83.05,
        "MMLU":58.72,
        "TruthfulQA":56.12,
        "Winogrande":77.35,
        "GSM8K":8.87,
        "DROP":10.46,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"51c9b600023cd26c4eb3754b9a89c60dde959ccc",
        "model_name_for_query":"Weyaxi\/ChatAYT-Lora-Assamble-Marcoroni"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.99,
        "ARC":60.92,
        "HellaSwag":83.56,
        "MMLU":55.33,
        "TruthfulQA":51.97,
        "Winogrande":75.22,
        "GSM8K":9.17,
        "DROP":20.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"27baccf242bc1dc34fc39661a40bbf867cbea8b5",
        "model_name_for_query":"Undi95\/ReMM-SLERP-L2-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.99,
        "ARC":60.92,
        "HellaSwag":83.56,
        "MMLU":55.33,
        "TruthfulQA":51.97,
        "Winogrande":75.22,
        "GSM8K":9.17,
        "DROP":20.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":true,
        "Model sha":"cb3562e7aae05a95fe61610b7b8f4957d3529ce7",
        "model_name_for_query":"The-Face-Of-Goonery\/Huginn-13b-v1.2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.98,
        "ARC":63.05,
        "HellaSwag":82.8,
        "MMLU":58.73,
        "TruthfulQA":55.53,
        "Winogrande":76.56,
        "GSM8K":9.7,
        "DROP":10.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":13.0,
        "Available on the hub":true,
        "Model sha":"01b0f2046083dd8d9d8f9e626d78d83eaa1d57dd",
        "model_name_for_query":"ai-business\/Luban-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.97,
        "ARC":58.62,
        "HellaSwag":80.85,
        "MMLU":47.76,
        "TruthfulQA":48.73,
        "Winogrande":76.72,
        "GSM8K":8.34,
        "DROP":35.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"openrail",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"27002e974774c3599e6a4d731dd44e68b9e41f92",
        "model_name_for_query":"bofenghuang\/vigogne-13b-chat"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.97,
        "ARC":62.03,
        "HellaSwag":83.8,
        "MMLU":58.39,
        "TruthfulQA":49.92,
        "Winogrande":77.27,
        "GSM8K":12.43,
        "DROP":12.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"a6790d83337578f38d2bcd51038a779eaa8d0fac",
        "model_name_for_query":"adonlee\/LLaMA_2_13B_SFT_v0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.97,
        "ARC":60.92,
        "HellaSwag":83.56,
        "MMLU":55.33,
        "TruthfulQA":51.97,
        "Winogrande":75.22,
        "GSM8K":9.02,
        "DROP":20.73,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":119.0,
        "Available on the hub":true,
        "Model sha":"faa4ef8c87dbb00d447904ceb048d49b6a463d07",
        "model_name_for_query":"Gryphe\/MythoMax-L2-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.96,
        "ARC":54.35,
        "HellaSwag":75.65,
        "MMLU":54.67,
        "TruthfulQA":45.21,
        "Winogrande":73.56,
        "GSM8K":11.6,
        "DROP":41.71,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"cb81174d72dbe06f8db1c406ef97981532de6f09",
        "model_name_for_query":"uukuguy\/speechless-codellama-34b-v2.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.96,
        "ARC":64.42,
        "HellaSwag":85.13,
        "MMLU":59.53,
        "TruthfulQA":50.47,
        "Winogrande":77.9,
        "GSM8K":11.75,
        "DROP":7.54,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":13.0,
        "Available on the hub":true,
        "Model sha":"04e1e194247a95cc60ba3cd70d026bc94c1f1764",
        "model_name_for_query":"jondurbin\/airoboros-33b-gpt4-1.4"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.96,
        "ARC":58.36,
        "HellaSwag":82.27,
        "MMLU":54.18,
        "TruthfulQA":45.18,
        "Winogrande":74.59,
        "GSM8K":1.52,
        "DROP":40.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"f57879831c39f2dcb656cb2c9e9ce5878e92bb44",
        "model_name_for_query":"Undi95\/CodeEngine"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":50.95,
        "ARC":62.37,
        "HellaSwag":84.28,
        "MMLU":57.02,
        "TruthfulQA":47.81,
        "Winogrande":75.22,
        "GSM8K":9.17,
        "DROP":20.76,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":true,
        "Model sha":"aed4ddc951c657993939fa5b87a4088550569a3b",
        "model_name_for_query":"The-Face-Of-Goonery\/huginnv1.2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.94,
        "ARC":52.3,
        "HellaSwag":75.09,
        "MMLU":56.34,
        "TruthfulQA":50.81,
        "Winogrande":71.74,
        "GSM8K":14.71,
        "DROP":35.55,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.13,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"e6c4cc00e1bb2aa2082c2b8fd93c949aa36ce300",
        "model_name_for_query":"OpenBuddy\/openbuddy-mistral-7b-v13"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.92,
        "ARC":62.54,
        "HellaSwag":84.19,
        "MMLU":57.33,
        "TruthfulQA":50.87,
        "Winogrande":76.48,
        "GSM8K":11.98,
        "DROP":13.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":11.0,
        "Available on the hub":true,
        "Model sha":"dde640538a44a08f6f456a2b7634e31a5d7a1245",
        "model_name_for_query":"IkariDev\/Athena-v4"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.86,
        "ARC":62.46,
        "HellaSwag":83.66,
        "MMLU":57.82,
        "TruthfulQA":50.94,
        "Winogrande":78.37,
        "GSM8K":15.31,
        "DROP":7.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"652f03ac67b4293198d98b618e64285fb32a28e9",
        "model_name_for_query":"Aspik101\/Vicuzard-30B-Uncensored-instruct-PL-lora_unload"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.84,
        "ARC":56.31,
        "HellaSwag":79.01,
        "MMLU":52.55,
        "TruthfulQA":51.68,
        "Winogrande":73.88,
        "GSM8K":2.73,
        "DROP":39.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":22.0,
        "Available on the hub":true,
        "Model sha":"d7c2bc52a3ae13571357f51273ae948caf84400e",
        "model_name_for_query":"HyperbeeAI\/Tulpar-7b-v0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.84,
        "ARC":60.92,
        "HellaSwag":83.5,
        "MMLU":59.39,
        "TruthfulQA":54.29,
        "Winogrande":75.22,
        "GSM8K":9.7,
        "DROP":12.84,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"f227ad33b16726b099e35e5dc47f4db1f22665a7",
        "model_name_for_query":"uukuguy\/speechless-llama2-hermes-orca-platypus-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.84,
        "ARC":59.73,
        "HellaSwag":83.08,
        "MMLU":61.29,
        "TruthfulQA":50.81,
        "Winogrande":76.56,
        "GSM8K":16.0,
        "DROP":8.38,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"e83b8c1887c45473961a4ff36ae202ada1ca3d42",
        "model_name_for_query":"caisarl76\/Mistral-7B-OpenOrca-Guanaco-accu16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.82,
        "ARC":59.22,
        "HellaSwag":81.52,
        "MMLU":54.94,
        "TruthfulQA":42.83,
        "Winogrande":76.87,
        "GSM8K":11.6,
        "DROP":28.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"a759c4fae8dc5fcd264bf58b89b9fd13d06784ae",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r16-q_k_v_o_gate_up_down"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.81,
        "ARC":62.54,
        "HellaSwag":82.76,
        "MMLU":59.23,
        "TruthfulQA":54.66,
        "Winogrande":77.11,
        "GSM8K":8.19,
        "DROP":11.19,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"908cfb670611875b52045c4bab81cff53f0279a7",
        "model_name_for_query":"uukuguy\/speechless-llama2-luban-orca-platypus-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.81,
        "ARC":57.76,
        "HellaSwag":80.78,
        "MMLU":54.32,
        "TruthfulQA":40.8,
        "Winogrande":76.72,
        "GSM8K":7.96,
        "DROP":37.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"ebe1b75fa315a9b55f686368070a0bcd0245ee39",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r4-q_k_v_o_gate_up_down"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.81,
        "ARC":61.69,
        "HellaSwag":85.32,
        "MMLU":58.0,
        "TruthfulQA":53.77,
        "Winogrande":75.61,
        "GSM8K":9.1,
        "DROP":12.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":19.99,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"b5b501b4d23ec7ab24b827f79e48b2c67e548ddb",
        "model_name_for_query":"Undi95\/MLewd-ReMM-L2-Chat-20B-Inverted"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":50.79,
        "ARC":57.76,
        "HellaSwag":82.16,
        "MMLU":54.68,
        "TruthfulQA":41.11,
        "Winogrande":74.98,
        "GSM8K":0.91,
        "DROP":43.94,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"b51bf8c4e132308751cc8b9d9c1131539f79f07f",
        "model_name_for_query":"pe-nlp\/llama-2-13b-vicuna-wizard"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.79,
        "ARC":62.97,
        "HellaSwag":83.83,
        "MMLU":58.98,
        "TruthfulQA":50.21,
        "Winogrande":78.3,
        "GSM8K":14.1,
        "DROP":7.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"62c74e7531625c1383bbbdc7c8346a996e9d1e21",
        "model_name_for_query":"camel-ai\/CAMEL-33B-Combined-Data"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.76,
        "ARC":60.07,
        "HellaSwag":82.76,
        "MMLU":61.5,
        "TruthfulQA":54.4,
        "Winogrande":78.06,
        "GSM8K":11.98,
        "DROP":6.54,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"9c9f31f213b69da7797c2c0630c17cf8f785fc13",
        "model_name_for_query":"caisarl76\/Mistral-7B-guanaco1k-ep2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.76,
        "ARC":60.07,
        "HellaSwag":82.76,
        "MMLU":61.5,
        "TruthfulQA":54.4,
        "Winogrande":78.06,
        "GSM8K":11.98,
        "DROP":6.54,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"9c9f31f213b69da7797c2c0630c17cf8f785fc13",
        "model_name_for_query":"caisarl76\/mistral-guanaco1k-ep2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.76,
        "ARC":62.37,
        "HellaSwag":82.96,
        "MMLU":58.68,
        "TruthfulQA":51.23,
        "Winogrande":77.19,
        "GSM8K":14.1,
        "DROP":8.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":94.0,
        "Available on the hub":true,
        "Model sha":"26d1bc5c54c1f60a5de0b1ed4d0b16f285aee230",
        "model_name_for_query":"Open-Orca\/OpenOrcaxOpenChat-Preview2-13B"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":50.74,
        "ARC":61.6,
        "HellaSwag":82.93,
        "MMLU":63.16,
        "TruthfulQA":46.96,
        "Winogrande":78.14,
        "GSM8K":16.38,
        "DROP":5.99,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"c34c4a249ecf0cc391beba142a1f9cb23154fcd1",
        "model_name_for_query":"lgaalves\/mistral-7b-platypus1k"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.73,
        "ARC":55.89,
        "HellaSwag":80.95,
        "MMLU":53.73,
        "TruthfulQA":42.72,
        "Winogrande":73.09,
        "GSM8K":10.92,
        "DROP":37.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"645ede9d6ec60d8fa051bc7ad32ab5f7bfdc066d",
        "model_name_for_query":"wei123602\/llama2-13b-fintune2-4E"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":50.73,
        "ARC":62.97,
        "HellaSwag":84.28,
        "MMLU":58.58,
        "TruthfulQA":51.28,
        "Winogrande":77.58,
        "GSM8K":12.36,
        "DROP":8.03,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"942af4d59533af09cf9ba13d1e369b8e871a0a4b",
        "model_name_for_query":"PulsarAI\/Chat-AYB-Nova-13B"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":50.71,
        "ARC":60.15,
        "HellaSwag":84.25,
        "MMLU":59.84,
        "TruthfulQA":49.86,
        "Winogrande":76.87,
        "GSM8K":17.44,
        "DROP":6.54,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"dad401175da3782475a122008720ddc3338e2632",
        "model_name_for_query":"mncai\/Mistral-7B-openplatypus-1k"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.65,
        "ARC":63.57,
        "HellaSwag":83.75,
        "MMLU":58.08,
        "TruthfulQA":51.09,
        "Winogrande":77.27,
        "GSM8K":11.07,
        "DROP":9.73,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":25.0,
        "Available on the hub":true,
        "Model sha":"ee25c078f08b0812d82597afa3f5e877c19a5c83",
        "model_name_for_query":"Undi95\/Unholy-v1-12L-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.65,
        "ARC":61.86,
        "HellaSwag":83.79,
        "MMLU":57.64,
        "TruthfulQA":51.03,
        "Winogrande":78.22,
        "GSM8K":14.63,
        "DROP":7.36,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"c63d117d1ec5794766dd6dc5e1469769df8aba1d",
        "model_name_for_query":"Aeala\/VicUnlocked-alpaca-30b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":50.62,
        "ARC":57.42,
        "HellaSwag":80.59,
        "MMLU":55.99,
        "TruthfulQA":53.45,
        "Winogrande":74.66,
        "GSM8K":8.11,
        "DROP":24.13,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":16.19,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"6fb7f82d486b3eee53d750f83cc7eae434349809",
        "model_name_for_query":"chargoddard\/llama-2-16b-nastychat"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.57,
        "ARC":61.95,
        "HellaSwag":84.0,
        "MMLU":56.14,
        "TruthfulQA":50.81,
        "Winogrande":75.85,
        "GSM8K":13.19,
        "DROP":12.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"bc42c77f88482c37c72c85c66135e99972bbca1b",
        "model_name_for_query":"Undi95\/ReMM-v2-L2-13B"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":50.55,
        "ARC":62.03,
        "HellaSwag":83.96,
        "MMLU":57.48,
        "TruthfulQA":52.5,
        "Winogrande":75.53,
        "GSM8K":8.34,
        "DROP":14.0,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":12.0,
        "Available on the hub":true,
        "Model sha":"233568319a636b6a7b02a4def2c51d08a3e0fbfc",
        "model_name_for_query":"chargoddard\/storytime-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.52,
        "ARC":60.92,
        "HellaSwag":84.18,
        "MMLU":58.3,
        "TruthfulQA":49.02,
        "Winogrande":78.77,
        "GSM8K":16.22,
        "DROP":6.23,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"d72dc1f05eaf1beb6373fd53fd22eb90f293a5c4",
        "model_name_for_query":"Aeala\/Enterredaas-33b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":50.49,
        "ARC":62.37,
        "HellaSwag":82.99,
        "MMLU":59.38,
        "TruthfulQA":52.2,
        "Winogrande":75.77,
        "GSM8K":11.14,
        "DROP":9.58,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"39ae03b77b4f1d453b02468ce6bb4ddeb6526b77",
        "model_name_for_query":"TFLai\/OpenOrca-Platypus2-13B-QLoRA-0.80-epoch"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.49,
        "ARC":56.31,
        "HellaSwag":79.19,
        "MMLU":51.36,
        "TruthfulQA":51.26,
        "Winogrande":74.51,
        "GSM8K":1.74,
        "DROP":39.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"6032c5106970f98d59925959fbd330ae4b1d1a7e",
        "model_name_for_query":"zarakiquemparte\/zararp-l2-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.48,
        "ARC":59.56,
        "HellaSwag":82.55,
        "MMLU":55.89,
        "TruthfulQA":42.67,
        "Winogrande":77.27,
        "GSM8K":12.43,
        "DROP":23.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"c75073d7545a4d222f40dc519021c55a81850d75",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-dolphin_20w"
    },
    {
        "T":"\ud83d\udfe6",
        "Average \u2b06\ufe0f":50.48,
        "ARC":59.04,
        "HellaSwag":81.94,
        "MMLU":54.64,
        "TruthfulQA":44.12,
        "Winogrande":74.51,
        "GSM8K":15.24,
        "DROP":23.87,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":617.0,
        "Available on the hub":false,
        "Model sha":"f848cf15ab9a51ae5735ab28120a9a0773eeb541",
        "model_name_for_query":"meta-llama\/Llama-2-13b-chat-hf"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.48,
        "ARC":59.04,
        "HellaSwag":81.93,
        "MMLU":54.63,
        "TruthfulQA":44.12,
        "Winogrande":74.51,
        "GSM8K":15.24,
        "DROP":23.87,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"2605b5b3b0ecba906ac26d39aab40f33c2ec81c9",
        "model_name_for_query":"NewstaR\/Morningstar-13b-hf"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.48,
        "ARC":59.04,
        "HellaSwag":81.93,
        "MMLU":54.63,
        "TruthfulQA":44.12,
        "Winogrande":74.51,
        "GSM8K":15.24,
        "DROP":23.87,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"openrail++",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":25.0,
        "Available on the hub":true,
        "Model sha":"d4af0b233a5b6a214e96582e103396e99dcf5f95",
        "model_name_for_query":"deepse\/CodeUp-Llama-2-13b-chat-hf"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.47,
        "ARC":62.8,
        "HellaSwag":83.15,
        "MMLU":59.39,
        "TruthfulQA":53.08,
        "Winogrande":76.24,
        "GSM8K":9.02,
        "DROP":9.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":204.0,
        "Available on the hub":true,
        "Model sha":"e7a40134f7eb687c6ab66d445dc7251257f8d391",
        "model_name_for_query":"Open-Orca\/OpenOrca-Platypus2-13B"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":50.47,
        "ARC":63.91,
        "HellaSwag":85.04,
        "MMLU":58.53,
        "TruthfulQA":45.36,
        "Winogrande":78.69,
        "GSM8K":13.04,
        "DROP":8.73,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"f94e5249d2b998933466d42e08fa9551e3238205",
        "model_name_for_query":"jondurbin\/airoboros-33b-gpt4-1.3"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.46,
        "ARC":58.53,
        "HellaSwag":79.92,
        "MMLU":46.03,
        "TruthfulQA":53.06,
        "Winogrande":73.95,
        "GSM8K":8.79,
        "DROP":32.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"50af05b015446110a2dc52a1b4b341142c98e62b",
        "model_name_for_query":"Aeala\/GPT4-x-Alpasta-13b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":50.46,
        "ARC":58.02,
        "HellaSwag":82.33,
        "MMLU":55.8,
        "TruthfulQA":46.23,
        "Winogrande":77.58,
        "GSM8K":3.26,
        "DROP":29.98,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"d1a41d83c6bcc14378ee4859d65ef77a261d39d7",
        "model_name_for_query":"yeontaek\/llama-2-13b-QLoRA"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.45,
        "ARC":61.26,
        "HellaSwag":84.16,
        "MMLU":56.22,
        "TruthfulQA":51.35,
        "Winogrande":75.61,
        "GSM8K":14.03,
        "DROP":10.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"d55031fbcd41d749bc0c0ffbcd85636718d373b6",
        "model_name_for_query":"Undi95\/ReMM-v2.2-L2-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.45,
        "ARC":55.8,
        "HellaSwag":81.74,
        "MMLU":55.09,
        "TruthfulQA":39.12,
        "Winogrande":76.32,
        "GSM8K":12.81,
        "DROP":32.29,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"aefc3a122cb054b070a212d1127600775aded4be",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r4-gate_up_down"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.45,
        "ARC":63.82,
        "HellaSwag":85.09,
        "MMLU":58.94,
        "TruthfulQA":45.33,
        "Winogrande":79.01,
        "GSM8K":12.74,
        "DROP":8.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"f94e5249d2b998933466d42e08fa9551e3238205",
        "model_name_for_query":"jondurbin\/airoboros-33b-gpt4-1.3"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.43,
        "ARC":55.97,
        "HellaSwag":81.53,
        "MMLU":54.42,
        "TruthfulQA":40.72,
        "Winogrande":75.06,
        "GSM8K":9.55,
        "DROP":35.77,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"905fc0b26dcb9e1fc5be99e73596e0884f9b71df",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r8-q_k_v_o_gate_up_down"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.41,
        "ARC":59.04,
        "HellaSwag":81.65,
        "MMLU":56.37,
        "TruthfulQA":39.98,
        "Winogrande":75.45,
        "GSM8K":11.22,
        "DROP":29.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"e81b5d4550224711929fdea4effdd990cc0c7404",
        "model_name_for_query":"wei123602\/Llama-2-13b-FINETUNE4_TEST3"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.41,
        "ARC":61.43,
        "HellaSwag":83.92,
        "MMLU":55.95,
        "TruthfulQA":50.3,
        "Winogrande":75.93,
        "GSM8K":12.74,
        "DROP":12.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"e6b5ac97f74355cb281a621261debe5720fb4da2",
        "model_name_for_query":"Undi95\/ReMM-v2.1-L2-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.39,
        "ARC":57.0,
        "HellaSwag":80.35,
        "MMLU":52.06,
        "TruthfulQA":45.0,
        "Winogrande":74.82,
        "GSM8K":0.0,
        "DROP":43.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"c38a9df20162455b53eb35d38a9b67fb824559e8",
        "model_name_for_query":"PocketDoc\/Dans-MysteryModel-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.38,
        "ARC":57.34,
        "HellaSwag":78.81,
        "MMLU":50.75,
        "TruthfulQA":53.18,
        "Winogrande":73.48,
        "GSM8K":0.0,
        "DROP":39.14,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"72e866a96a2e9afc6527c8d757c69088c3a069c8",
        "model_name_for_query":"oh-yeontaek\/llama-2-7B-LoRA-assemble"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.38,
        "ARC":62.71,
        "HellaSwag":85.04,
        "MMLU":58.48,
        "TruthfulQA":44.23,
        "Winogrande":79.79,
        "GSM8K":15.77,
        "DROP":6.65,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":67.0,
        "Available on the hub":true,
        "Model sha":"300bc5f3dc129a3d17adf059394e381eff7fbd55",
        "model_name_for_query":"digitous\/Alpacino30b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.38,
        "ARC":59.9,
        "HellaSwag":82.51,
        "MMLU":56.3,
        "TruthfulQA":43.14,
        "Winogrande":77.19,
        "GSM8K":12.66,
        "DROP":20.98,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"f01882672e89b164f76093cf3bd26cfc6ecf72ed",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-OpenOrca_20w"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.38,
        "ARC":62.2,
        "HellaSwag":83.11,
        "MMLU":55.88,
        "TruthfulQA":53.2,
        "Winogrande":74.19,
        "GSM8K":5.31,
        "DROP":18.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":19.99,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"c0cbe0b3c88041bb6beef27dbe85146af8dddec9",
        "model_name_for_query":"Undi95\/U-Amethyst-20B"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":50.34,
        "ARC":62.63,
        "HellaSwag":83.24,
        "MMLU":58.64,
        "TruthfulQA":51.88,
        "Winogrande":76.95,
        "GSM8K":10.24,
        "DROP":8.8,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"2a344b91b28ce4d0bd48b9b5a6cc87b71123eab5",
        "model_name_for_query":"PulsarAI\/2x-LoRA-Assemble-Nova-13B"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":50.32,
        "ARC":59.98,
        "HellaSwag":83.31,
        "MMLU":64.16,
        "TruthfulQA":42.15,
        "Winogrande":78.37,
        "GSM8K":18.12,
        "DROP":6.14,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":1173.0,
        "Available on the hub":true,
        "Model sha":"e836d8f71b5812f9fee65618453dc537c66bd82a",
        "model_name_for_query":"mistralai\/Mistral-7B-v0.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.32,
        "ARC":54.69,
        "HellaSwag":81.48,
        "MMLU":56.8,
        "TruthfulQA":39.93,
        "Winogrande":76.24,
        "GSM8K":12.59,
        "DROP":30.48,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"9e6431061bd13852a7435f5fe7a6eb0bbd148e14",
        "model_name_for_query":"wei123602\/llama2-13b-FINETUNE3_TEST2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.31,
        "ARC":56.48,
        "HellaSwag":78.85,
        "MMLU":51.49,
        "TruthfulQA":51.99,
        "Winogrande":73.4,
        "GSM8K":1.14,
        "DROP":38.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"31fa6527a3285d5fd320219d7c2dadde07b83718",
        "model_name_for_query":"zarakiquemparte\/zararp-1.1-l2-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.23,
        "ARC":58.7,
        "HellaSwag":81.18,
        "MMLU":58.25,
        "TruthfulQA":56.44,
        "Winogrande":72.77,
        "GSM8K":3.34,
        "DROP":20.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":true,
        "Model sha":"f3a8a475ff0c6ae37ac8ae0690980be11cac731a",
        "model_name_for_query":"totally-not-an-llm\/PuddleJumper-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.23,
        "ARC":52.56,
        "HellaSwag":75.73,
        "MMLU":56.68,
        "TruthfulQA":50.44,
        "Winogrande":71.59,
        "GSM8K":8.72,
        "DROP":35.89,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.13,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"b64386bde3d7850a01df763f5c777c74888d34fc",
        "model_name_for_query":"OpenBuddy\/openbuddy-mistral-7b-v13.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.22,
        "ARC":62.54,
        "HellaSwag":82.67,
        "MMLU":58.56,
        "TruthfulQA":51.93,
        "Winogrande":76.8,
        "GSM8K":9.4,
        "DROP":9.61,
        "Type":"fine-tuned",
        "Precision":"GPTQ",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":16.24,
        "Hub \u2764\ufe0f":49.0,
        "Available on the hub":true,
        "Model sha":"0fa9a56066656fbc94e3ec088bc900fd1d4d38e8",
        "model_name_for_query":"TheBloke\/OpenOrca-Platypus2-13B-GPTQ"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.21,
        "ARC":61.6,
        "HellaSwag":82.62,
        "MMLU":54.55,
        "TruthfulQA":48.34,
        "Winogrande":74.74,
        "GSM8K":11.98,
        "DROP":17.63,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"73a27445e5e5a72857626e551c70542ec607f60c",
        "model_name_for_query":"ajibawa-2023\/Uncensored-Frank-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.2,
        "ARC":54.78,
        "HellaSwag":81.52,
        "MMLU":56.03,
        "TruthfulQA":39.14,
        "Winogrande":77.03,
        "GSM8K":13.19,
        "DROP":29.73,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"0ed198a814192b06e60715112d2a4b6bfd630806",
        "model_name_for_query":"wei123602\/Llama-2-13b-FINETUNE4_TEST"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.18,
        "ARC":56.14,
        "HellaSwag":79.34,
        "MMLU":52.1,
        "TruthfulQA":50.66,
        "Winogrande":74.43,
        "GSM8K":7.81,
        "DROP":30.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"3268ff5291934a14f3f5e7013bbb408f33adb542",
        "model_name_for_query":"zarakiquemparte\/zarafusionex-1.1-l2-7b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":50.13,
        "ARC":54.1,
        "HellaSwag":78.57,
        "MMLU":51.66,
        "TruthfulQA":46.84,
        "Winogrande":74.35,
        "GSM8K":4.78,
        "DROP":40.61,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"8c71cdb481ce6bbda3b2042e5526a232ab23825c",
        "model_name_for_query":"Azure99\/blossom-v2-llama2-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.13,
        "ARC":58.7,
        "HellaSwag":82.0,
        "MMLU":57.66,
        "TruthfulQA":56.35,
        "Winogrande":74.66,
        "GSM8K":8.95,
        "DROP":12.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"7c87376b201b1c30c4e12c0b7bc2f28f017ce7bc",
        "model_name_for_query":"Sao10K\/Mythical-Destroyer-L2-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.12,
        "ARC":61.77,
        "HellaSwag":85.06,
        "MMLU":57.52,
        "TruthfulQA":51.49,
        "Winogrande":77.35,
        "GSM8K":7.73,
        "DROP":9.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"2424b6346e9e8fd749b9a6734f5d7125b5926daf",
        "model_name_for_query":"dsvv-cair\/alpaca-cleaned-llama-30b-bf16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.1,
        "ARC":61.69,
        "HellaSwag":84.34,
        "MMLU":57.87,
        "TruthfulQA":51.26,
        "Winogrande":75.77,
        "GSM8K":11.6,
        "DROP":8.21,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"5e4024b6694bb13f1a81ce4277ac9141f0b226df",
        "model_name_for_query":"IkariDev\/Athena-v3"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":50.1,
        "ARC":62.12,
        "HellaSwag":83.92,
        "MMLU":57.53,
        "TruthfulQA":52.33,
        "Winogrande":75.06,
        "GSM8K":8.79,
        "DROP":10.98,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"88dc61b7afebf2220ca42898e1286c59961ed440",
        "model_name_for_query":"chargoddard\/duplicitous-slurpbeast-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.1,
        "ARC":56.66,
        "HellaSwag":81.09,
        "MMLU":53.3,
        "TruthfulQA":43.99,
        "Winogrande":73.01,
        "GSM8K":8.04,
        "DROP":34.62,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"f769a92cfeffe8ee07beee8814ce7eca7cd62805",
        "model_name_for_query":"luffycodes\/mcq-vicuna-13b-v1.5"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.1,
        "ARC":58.96,
        "HellaSwag":80.32,
        "MMLU":47.25,
        "TruthfulQA":47.41,
        "Winogrande":75.53,
        "GSM8K":9.25,
        "DROP":31.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":true,
        "Model sha":"6cb016f5bfcbc24ee08312b52f08ef5e8f860871",
        "model_name_for_query":"dvruette\/llama-13b-pretrained-sft-do2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.08,
        "ARC":60.41,
        "HellaSwag":83.27,
        "MMLU":57.17,
        "TruthfulQA":51.79,
        "Winogrande":76.87,
        "GSM8K":12.89,
        "DROP":8.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"67f6e669d7a15c1104a1478057f3752a503e83c0",
        "model_name_for_query":"FelixChao\/vicuna-33b-coder"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":50.06,
        "ARC":59.73,
        "HellaSwag":84.02,
        "MMLU":57.81,
        "TruthfulQA":48.54,
        "Winogrande":79.48,
        "GSM8K":14.4,
        "DROP":6.45,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"3259cb3c2a10cfb429fb51c4a76fffa049f4c44d",
        "model_name_for_query":"TheBloke\/VicUnlocked-30B-LoRA-HF"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.06,
        "ARC":57.94,
        "HellaSwag":81.32,
        "MMLU":47.62,
        "TruthfulQA":50.23,
        "Winogrande":77.11,
        "GSM8K":11.83,
        "DROP":24.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"openrail",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":13.0,
        "Available on the hub":true,
        "Model sha":"a13e08a36c355d64fae59f28162e5fa542a8d235",
        "model_name_for_query":"bofenghuang\/vigogne-13b-instruct"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":50.0,
        "ARC":60.24,
        "HellaSwag":83.69,
        "MMLU":53.17,
        "TruthfulQA":41.81,
        "Winogrande":73.24,
        "GSM8K":5.76,
        "DROP":32.13,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"ed3535921eb24e0737f9a6cda70b1a3fd71532cd",
        "model_name_for_query":"heegyu\/LIMA2-13b-hf"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":50.0,
        "ARC":55.2,
        "HellaSwag":80.76,
        "MMLU":48.8,
        "TruthfulQA":51.07,
        "Winogrande":73.4,
        "GSM8K":6.9,
        "DROP":33.82,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":15.0,
        "Available on the hub":true,
        "Model sha":"42dfc6f7d735670e2f3e30b0919708a81f9a0df9",
        "model_name_for_query":"NousResearch\/Capybara-7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.98,
        "ARC":55.46,
        "HellaSwag":77.63,
        "MMLU":53.12,
        "TruthfulQA":59.01,
        "Winogrande":73.48,
        "GSM8K":14.94,
        "DROP":16.25,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":true,
        "Model sha":"a8e18f970f7ca994740177d6c228adee9e17aba9",
        "model_name_for_query":"Severian\/ANIMA-Phi-Neptune-Mistral-7B-v4"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.95,
        "ARC":62.97,
        "HellaSwag":82.75,
        "MMLU":56.86,
        "TruthfulQA":42.93,
        "Winogrande":76.32,
        "GSM8K":2.81,
        "DROP":25.02,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"e35bb473156d74c8b5ad23a5e9df815891e8139a",
        "model_name_for_query":"chickencaesar\/llama2-platypus-llama2-chat-13B-hf"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.94,
        "ARC":55.29,
        "HellaSwag":80.73,
        "MMLU":48.72,
        "TruthfulQA":51.13,
        "Winogrande":73.32,
        "GSM8K":6.97,
        "DROP":33.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":15.0,
        "Available on the hub":true,
        "Model sha":"42dfc6f7d735670e2f3e30b0919708a81f9a0df9",
        "model_name_for_query":"NousResearch\/Nous-Capybara-7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.94,
        "ARC":57.0,
        "HellaSwag":79.69,
        "MMLU":51.33,
        "TruthfulQA":51.83,
        "Winogrande":72.45,
        "GSM8K":0.68,
        "DROP":36.58,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"719d8e1eb4a820f01e0a92ef6220d041964bb472",
        "model_name_for_query":"HyperbeeAI\/Tulpar-7b-v1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.93,
        "ARC":55.97,
        "HellaSwag":76.22,
        "MMLU":52.89,
        "TruthfulQA":59.76,
        "Winogrande":73.48,
        "GSM8K":14.94,
        "DROP":16.25,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":false,
        "Model sha":"e8e9a4804c842b84def9e9aaae38236d4754f277",
        "model_name_for_query":"Severian\/ANIMA-Phi-Neptune-Mistral-7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.92,
        "ARC":56.57,
        "HellaSwag":80.09,
        "MMLU":48.47,
        "TruthfulQA":47.22,
        "Winogrande":74.51,
        "GSM8K":4.85,
        "DROP":37.74,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":20.0,
        "Available on the hub":true,
        "Model sha":"fdf075081555f3ed84c037e8dd3fe85c3b3609d7",
        "model_name_for_query":"jondurbin\/spicyboros-7b-2.2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.92,
        "ARC":58.62,
        "HellaSwag":81.07,
        "MMLU":48.32,
        "TruthfulQA":54.19,
        "Winogrande":76.01,
        "GSM8K":0.76,
        "DROP":30.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"83905656ca3e63877b8d9f3a74118da0c9bc6939",
        "model_name_for_query":"TheBloke\/WizardLM-13B-V1-1-SuperHOT-8K-fp16"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":49.92,
        "ARC":60.58,
        "HellaSwag":81.26,
        "MMLU":57.92,
        "TruthfulQA":48.89,
        "Winogrande":76.95,
        "GSM8K":15.92,
        "DROP":7.89,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"1f5609ffd40bc3af2dcbc5c88e9312d47a73c4b4",
        "model_name_for_query":"rombodawg\/LosslessMegaCoder-llama2-13b-mini"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.92,
        "ARC":60.58,
        "HellaSwag":81.26,
        "MMLU":57.92,
        "TruthfulQA":48.89,
        "Winogrande":76.95,
        "GSM8K":15.92,
        "DROP":7.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"b38d1b53c358a0313c69bcceebe97628327ada82",
        "model_name_for_query":"andreaskoepf\/llama2-13b-megacode2_min100"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.91,
        "ARC":56.23,
        "HellaSwag":81.15,
        "MMLU":53.38,
        "TruthfulQA":44.08,
        "Winogrande":72.93,
        "GSM8K":7.51,
        "DROP":34.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"f769a92cfeffe8ee07beee8814ce7eca7cd62805",
        "model_name_for_query":"luffycodes\/mcq-vicuna-13b-v1.5"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.91,
        "ARC":61.26,
        "HellaSwag":82.14,
        "MMLU":57.85,
        "TruthfulQA":50.22,
        "Winogrande":77.11,
        "GSM8K":12.43,
        "DROP":8.35,
        "Type":"fine-tuned",
        "Precision":"GPTQ",
        "Hub License":"llama2",
        "#Params (B)":16.24,
        "Hub \u2764\ufe0f":21.0,
        "Available on the hub":true,
        "Model sha":"ec9eb4f471b5bb6a7e5e505369628586c0c72252",
        "model_name_for_query":"TheBloke\/OpenOrcaxOpenChat-Preview2-13B-GPTQ"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.9,
        "ARC":56.31,
        "HellaSwag":79.32,
        "MMLU":47.03,
        "TruthfulQA":48.42,
        "Winogrande":76.95,
        "GSM8K":16.07,
        "DROP":25.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"c28cc0cf5a1a1bf4de96b23d06b02129dca85eb9",
        "model_name_for_query":"dvruette\/llama-13b-pretrained"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.89,
        "ARC":62.46,
        "HellaSwag":83.65,
        "MMLU":57.88,
        "TruthfulQA":44.52,
        "Winogrande":75.85,
        "GSM8K":18.35,
        "DROP":6.51,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"afca2c9488cf8738faec4db6721f6a4c755a5d81",
        "model_name_for_query":"gradientputri\/MegaMix-S1-13B"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":49.88,
        "ARC":59.9,
        "HellaSwag":82.75,
        "MMLU":58.45,
        "TruthfulQA":51.9,
        "Winogrande":74.43,
        "GSM8K":3.87,
        "DROP":17.89,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":true,
        "Model sha":"75c1bf5f4b40cf61873ff6487ccd3efc4f684330",
        "model_name_for_query":"chargoddard\/Chronorctypus-Limarobormes-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.88,
        "ARC":50.94,
        "HellaSwag":83.47,
        "MMLU":53.52,
        "TruthfulQA":46.09,
        "Winogrande":73.16,
        "GSM8K":4.78,
        "DROP":37.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"4d70cf0047a7a5cd2c864bc2606e81f0830e4405",
        "model_name_for_query":"JosephusCheung\/Qwen-LLaMAfied-7B-Chat"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.87,
        "ARC":55.8,
        "HellaSwag":79.53,
        "MMLU":53.01,
        "TruthfulQA":38.24,
        "Winogrande":75.69,
        "GSM8K":3.94,
        "DROP":42.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.97,
        "Hub \u2764\ufe0f":22.0,
        "Available on the hub":true,
        "Model sha":"484c8a18b02f95eb2b6f6302105cf9a329e76ec8",
        "model_name_for_query":"ziqingyang\/chinese-llama-2-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.87,
        "ARC":55.38,
        "HellaSwag":81.92,
        "MMLU":55.28,
        "TruthfulQA":40.76,
        "Winogrande":76.09,
        "GSM8K":13.72,
        "DROP":25.92,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"2ca747d779feaa99c475b8015c9b4a50aea41cd2",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE5_4w-r4-gate_up_down"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.86,
        "ARC":55.38,
        "HellaSwag":79.21,
        "MMLU":50.5,
        "TruthfulQA":52.75,
        "Winogrande":73.24,
        "GSM8K":0.61,
        "DROP":37.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"70e38a7424544193f0ad6a93ae26a5bfd15e4e90",
        "model_name_for_query":"xxyyy123\/10k_v1_lora_qkvo_rank28_v2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.86,
        "ARC":55.8,
        "HellaSwag":82.27,
        "MMLU":55.63,
        "TruthfulQA":38.15,
        "Winogrande":77.43,
        "GSM8K":12.66,
        "DROP":27.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"48b8ceeb62e5ca897f284bbc0923201689af7c89",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r16-gate_up_down-test1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.85,
        "ARC":56.4,
        "HellaSwag":81.93,
        "MMLU":53.63,
        "TruthfulQA":39.23,
        "Winogrande":76.95,
        "GSM8K":11.98,
        "DROP":28.83,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"dd61a482fa2f71efe6f22aae6949355ca4b06ccc",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r4-gate_up_down"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.85,
        "ARC":58.19,
        "HellaSwag":81.35,
        "MMLU":57.39,
        "TruthfulQA":51.24,
        "Winogrande":73.32,
        "GSM8K":6.82,
        "DROP":20.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"be755c9eef8233ca59e0178db75de878f5859222",
        "model_name_for_query":"Sao10K\/Medusa-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.85,
        "ARC":62.71,
        "HellaSwag":81.99,
        "MMLU":57.51,
        "TruthfulQA":47.45,
        "Winogrande":76.8,
        "GSM8K":13.72,
        "DROP":8.74,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":94.0,
        "Available on the hub":true,
        "Model sha":"26d1bc5c54c1f60a5de0b1ed4d0b16f285aee230",
        "model_name_for_query":"Open-Orca\/OpenOrcaxOpenChat-Preview2-13B"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":49.83,
        "ARC":60.92,
        "HellaSwag":83.77,
        "MMLU":56.47,
        "TruthfulQA":49.42,
        "Winogrande":76.01,
        "GSM8K":11.6,
        "DROP":10.6,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"9b2dbc1f6f17a162228799df6e9449c903ddf04d",
        "model_name_for_query":"jondurbin\/airoboros-l2-13b-2.2.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.81,
        "ARC":57.25,
        "HellaSwag":81.79,
        "MMLU":53.96,
        "TruthfulQA":39.66,
        "Winogrande":77.82,
        "GSM8K":11.75,
        "DROP":26.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"8a75b17d4b60f820159bb0100f26f438727bb199",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r8-gate_up_down"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.81,
        "ARC":58.28,
        "HellaSwag":81.39,
        "MMLU":56.87,
        "TruthfulQA":39.86,
        "Winogrande":76.01,
        "GSM8K":11.9,
        "DROP":24.36,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"fe1b604097aad9408ce63fa7ffc9c320cdd06e4f",
        "model_name_for_query":"wei123602\/Llama-2-13b-FINETUNE4_compare8k2"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":49.81,
        "ARC":58.53,
        "HellaSwag":80.66,
        "MMLU":49.59,
        "TruthfulQA":54.35,
        "Winogrande":74.43,
        "GSM8K":8.11,
        "DROP":22.96,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":16.22,
        "Hub \u2764\ufe0f":26.0,
        "Available on the hub":true,
        "Model sha":"9df807ac64034bc6e7387326689d6e39656ce5e0",
        "model_name_for_query":"TheBloke\/WizardLM-13B-V1.1-GPTQ"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":49.8,
        "ARC":61.69,
        "HellaSwag":83.79,
        "MMLU":57.5,
        "TruthfulQA":52.27,
        "Winogrande":75.06,
        "GSM8K":9.1,
        "DROP":9.2,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"a05d0562b8da2ac2e76aa65984e8063249bc85c8",
        "model_name_for_query":"chargoddard\/duplicitous-mammal-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.79,
        "ARC":58.7,
        "HellaSwag":81.89,
        "MMLU":56.08,
        "TruthfulQA":38.95,
        "Winogrande":77.35,
        "GSM8K":12.96,
        "DROP":22.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"4c3a4cb54c0487666bd58589b50f90c22de80969",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r16-gate_up_down"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.78,
        "ARC":55.55,
        "HellaSwag":79.4,
        "MMLU":51.21,
        "TruthfulQA":51.05,
        "Winogrande":74.66,
        "GSM8K":7.2,
        "DROP":29.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"13d0e2498a4b5f53f6dc2464f20e093b07a4bd4b",
        "model_name_for_query":"zarakiquemparte\/zarafusionix-l2-7b"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":49.73,
        "ARC":61.43,
        "HellaSwag":84.73,
        "MMLU":58.45,
        "TruthfulQA":42.27,
        "Winogrande":80.03,
        "GSM8K":14.86,
        "DROP":6.33,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.53,
        "Hub \u2764\ufe0f":37.0,
        "Available on the hub":true,
        "Model sha":"2b1edcdb3c7ced7bce6c1aa75c94545777c3118b",
        "model_name_for_query":"huggyllama\/llama-30b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.72,
        "ARC":55.97,
        "HellaSwag":79.98,
        "MMLU":54.3,
        "TruthfulQA":48.09,
        "Winogrande":74.59,
        "GSM8K":12.28,
        "DROP":22.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"98e0e2086df11b9f80e1571110540a657e52c2e8",
        "model_name_for_query":"budecosystem\/genz-13b-v2"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":49.72,
        "ARC":62.54,
        "HellaSwag":82.1,
        "MMLU":58.67,
        "TruthfulQA":46.96,
        "Winogrande":77.82,
        "GSM8K":12.36,
        "DROP":7.6,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"17493c1f2e4620a44d7947edad0386d338e805ce",
        "model_name_for_query":"yeontaek\/Platypus2xOpenOrca-13B-IA3-v3"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":49.71,
        "ARC":59.81,
        "HellaSwag":82.69,
        "MMLU":56.96,
        "TruthfulQA":52.92,
        "Winogrande":74.43,
        "GSM8K":2.35,
        "DROP":18.83,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"5427ceec420f943a0b011a4d96f3efc292306933",
        "model_name_for_query":"TFLai\/OpenOrcaPlatypus2-Platypus2-13B-QLora-0.80-epoch"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.71,
        "ARC":61.26,
        "HellaSwag":84.73,
        "MMLU":58.47,
        "TruthfulQA":42.27,
        "Winogrande":80.03,
        "GSM8K":14.86,
        "DROP":6.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"fba493af11a73cf5a2ee7857dd7aecb98c659dc4",
        "model_name_for_query":"Yhyu13\/llama-30B-hf-openassitant"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":49.71,
        "ARC":61.26,
        "HellaSwag":84.73,
        "MMLU":58.47,
        "TruthfulQA":42.27,
        "Winogrande":80.03,
        "GSM8K":14.86,
        "DROP":6.33,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.53,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"13c77caa472bfa79d4f3f0ec82cbdc9dd88e5d22",
        "model_name_for_query":"huggingface\/llama-30b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":49.69,
        "ARC":57.0,
        "HellaSwag":81.06,
        "MMLU":58.3,
        "TruthfulQA":52.66,
        "Winogrande":72.45,
        "GSM8K":3.64,
        "DROP":22.74,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"1fe9494e334a32ba73dc2926f58246450850c534",
        "model_name_for_query":"totally-not-an-llm\/PuddleJumper-13b-V2"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":49.69,
        "ARC":61.43,
        "HellaSwag":81.84,
        "MMLU":59.02,
        "TruthfulQA":48.64,
        "Winogrande":77.19,
        "GSM8K":10.84,
        "DROP":8.88,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"3aa9abe9cb2e5c699f80935e04fbb351cdfbf21b",
        "model_name_for_query":"yeontaek\/Platypus2xOpenOrca-13B-IA3-v4"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":49.69,
        "ARC":62.37,
        "HellaSwag":82.47,
        "MMLU":57.44,
        "TruthfulQA":45.97,
        "Winogrande":77.58,
        "GSM8K":14.48,
        "DROP":7.52,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"5a6c3686749ecb76971a915403da8c07a98078a6",
        "model_name_for_query":"TFLai\/Orca-Nova-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.67,
        "ARC":60.67,
        "HellaSwag":82.0,
        "MMLU":56.87,
        "TruthfulQA":42.59,
        "Winogrande":77.19,
        "GSM8K":10.69,
        "DROP":17.72,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"c3bcafd7f6133a7e7c069f8765a99fe84989d926",
        "model_name_for_query":"duliadotio\/dulia-13b-8k-alpha"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.67,
        "ARC":60.75,
        "HellaSwag":81.91,
        "MMLU":57.06,
        "TruthfulQA":42.64,
        "Winogrande":77.19,
        "GSM8K":10.99,
        "DROP":17.14,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":115.0,
        "Available on the hub":true,
        "Model sha":"160f58ec85ef25ad935eb583f14c7e8c7f7e7839",
        "model_name_for_query":"OpenAssistant\/llama2-13b-orca-8k-3319"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.67,
        "ARC":52.9,
        "HellaSwag":76.12,
        "MMLU":57.54,
        "TruthfulQA":52.82,
        "Winogrande":71.35,
        "GSM8K":1.21,
        "DROP":35.72,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.13,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"8ff18d61b1c8295ecd73153b8e0b63934187a50e",
        "model_name_for_query":"OpenBuddy\/openbuddy-mistral-7b-v13-base"
    },
    {
        "T":"\ud83d\udfe6",
        "Average \u2b06\ufe0f":49.66,
        "ARC":57.17,
        "HellaSwag":79.57,
        "MMLU":50.24,
        "TruthfulQA":52.51,
        "Winogrande":72.93,
        "GSM8K":0.38,
        "DROP":34.85,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"9c4a7444d6fb12931e50f111053e016531fe60b7",
        "model_name_for_query":"xxyyy123\/mc_data_30k_from_platpus_orca_7b_10k_v1_lora_qkvo_rank14_v2"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":49.65,
        "ARC":62.71,
        "HellaSwag":82.55,
        "MMLU":56.79,
        "TruthfulQA":49.86,
        "Winogrande":76.24,
        "GSM8K":10.77,
        "DROP":8.64,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"3e25556187ba576082a85c270d2d4b4ea6ea9f6f",
        "model_name_for_query":"PulsarAI\/EnsembleV5-Nova-13B"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":49.65,
        "ARC":62.71,
        "HellaSwag":82.55,
        "MMLU":56.79,
        "TruthfulQA":49.86,
        "Winogrande":76.24,
        "GSM8K":10.77,
        "DROP":8.64,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"7ba38d309709d35149b4a18f94096875885035ae",
        "model_name_for_query":"TFLai\/EnsembleV5-Nova-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.65,
        "ARC":55.63,
        "HellaSwag":78.71,
        "MMLU":50.98,
        "TruthfulQA":47.21,
        "Winogrande":74.43,
        "GSM8K":7.73,
        "DROP":32.83,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":14.0,
        "Available on the hub":true,
        "Model sha":"7a1b76feabe3e0ed007ea83ee93f7644156d3b23",
        "model_name_for_query":"bofenghuang\/vigogne-2-7b-chat"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":49.64,
        "ARC":62.71,
        "HellaSwag":82.57,
        "MMLU":57.98,
        "TruthfulQA":51.34,
        "Winogrande":77.27,
        "GSM8K":6.75,
        "DROP":8.84,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"ae1145f9fa846ab8d39d8b7da888287ef917efb5",
        "model_name_for_query":"TFLai\/Nova-13B"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":49.63,
        "ARC":62.12,
        "HellaSwag":82.1,
        "MMLU":58.84,
        "TruthfulQA":47.88,
        "Winogrande":77.11,
        "GSM8K":11.83,
        "DROP":7.57,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"5ca46029dd22c007d4dc1706f6284a32be4546c2",
        "model_name_for_query":"yeontaek\/Platypus2xOpenOrca-13B-IA3"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.63,
        "ARC":57.25,
        "HellaSwag":79.99,
        "MMLU":45.52,
        "TruthfulQA":44.45,
        "Winogrande":77.58,
        "GSM8K":13.87,
        "DROP":28.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"1f839c019153789c15bbc45ecbb512d0f5015881",
        "model_name_for_query":"dvruette\/llama-13b-pretrained-sft-epoch-1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.62,
        "ARC":56.48,
        "HellaSwag":78.57,
        "MMLU":51.56,
        "TruthfulQA":47.7,
        "Winogrande":75.06,
        "GSM8K":1.44,
        "DROP":36.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"df23c3d22bc546dbce0267415e94bdb482446c06",
        "model_name_for_query":"Sao10K\/Medusa-1.1-L2-7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.62,
        "ARC":56.23,
        "HellaSwag":79.97,
        "MMLU":47.17,
        "TruthfulQA":49.51,
        "Winogrande":75.45,
        "GSM8K":3.79,
        "DROP":35.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":17.0,
        "Available on the hub":true,
        "Model sha":"8f4dd9c870f748322989168af5c109e16b01c63d",
        "model_name_for_query":"bofenghuang\/vigogne-2-7b-instruct"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.61,
        "ARC":60.67,
        "HellaSwag":81.93,
        "MMLU":57.38,
        "TruthfulQA":47.85,
        "Winogrande":76.16,
        "GSM8K":15.54,
        "DROP":7.74,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":9.0,
        "Available on the hub":true,
        "Model sha":"2c45ecf161da2ff2aa984900f2e4d2b7a7311ab8",
        "model_name_for_query":"OpenAssistant\/llama2-13b-megacode2-oasst"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.59,
        "ARC":61.18,
        "HellaSwag":82.86,
        "MMLU":55.19,
        "TruthfulQA":43.2,
        "Winogrande":76.16,
        "GSM8K":8.04,
        "DROP":20.5,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"6b5418b69e8270df659eacb192f469e7c3af70b3",
        "model_name_for_query":"bhenrym14\/airophin-13b-pntk-16k-fp16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.58,
        "ARC":60.75,
        "HellaSwag":81.94,
        "MMLU":54.08,
        "TruthfulQA":53.23,
        "Winogrande":73.8,
        "GSM8K":4.62,
        "DROP":18.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":true,
        "Model sha":"6d50e6681bc26c9bc0c8377c26c438e295ee0c2f",
        "model_name_for_query":"The-Face-Of-Goonery\/Chronos-Beluga-v2-13bfp16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.57,
        "ARC":60.49,
        "HellaSwag":83.38,
        "MMLU":55.8,
        "TruthfulQA":51.32,
        "Winogrande":77.03,
        "GSM8K":10.01,
        "DROP":8.98,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"agpl-3.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"b373eda586a6527e62382eda5480204652a82499",
        "model_name_for_query":"Doctor-Shotgun\/CalliopeDS-L2-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.57,
        "ARC":59.04,
        "HellaSwag":81.66,
        "MMLU":50.1,
        "TruthfulQA":50.36,
        "Winogrande":76.87,
        "GSM8K":13.12,
        "DROP":15.83,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":true,
        "Model sha":"5dac6f7559dba1c6fb59fee18c3e713cc3c83db7",
        "model_name_for_query":"openaccess-ai-collective\/minotaur-13b-fixed"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.57,
        "ARC":59.3,
        "HellaSwag":82.9,
        "MMLU":56.45,
        "TruthfulQA":52.04,
        "Winogrande":74.74,
        "GSM8K":13.19,
        "DROP":8.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"efaf592c95ae8e769e0d56d36ba4ed23e3bf4059",
        "model_name_for_query":"Sao10K\/Stheno-Inverted-L2-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.55,
        "ARC":55.03,
        "HellaSwag":78.81,
        "MMLU":51.35,
        "TruthfulQA":44.05,
        "Winogrande":74.9,
        "GSM8K":10.84,
        "DROP":31.86,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"0b8e61d3325cddbad207cbf885c2b5db6a83a059",
        "model_name_for_query":"lvkaokao\/llama2-7b-hf-chat-lora-v2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.55,
        "ARC":58.45,
        "HellaSwag":81.7,
        "MMLU":56.61,
        "TruthfulQA":40.19,
        "Winogrande":76.64,
        "GSM8K":13.19,
        "DROP":20.05,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"e312c4c59cab9d130c33288c92aad7c0cb5331d5",
        "model_name_for_query":"wei123602\/Llama-2-13b-FINETUNE4_TEST2"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":49.54,
        "ARC":60.07,
        "HellaSwag":82.42,
        "MMLU":55.87,
        "TruthfulQA":45.57,
        "Winogrande":76.24,
        "GSM8K":12.05,
        "DROP":14.53,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":false,
        "Model sha":"22c83f7d68e547fb0b59acfa01c60b108c59fe55",
        "model_name_for_query":"ehartford\/minotaur-llama2-13b-qlora"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.52,
        "ARC":50.17,
        "HellaSwag":72.21,
        "MMLU":56.34,
        "TruthfulQA":42.52,
        "Winogrande":68.35,
        "GSM8K":19.11,
        "DROP":37.96,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"64a9b89fb18140fc1af1f11471dc9fe34ebc7446",
        "model_name_for_query":"JosephusCheung\/Pwen-VL-Chat-20_30"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.52,
        "ARC":58.96,
        "HellaSwag":81.95,
        "MMLU":47.92,
        "TruthfulQA":51.69,
        "Winogrande":75.69,
        "GSM8K":8.64,
        "DROP":21.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":197.0,
        "Available on the hub":true,
        "Model sha":"fff9ac7f0e2e7b340f2301f5f089d989fc03be67",
        "model_name_for_query":"TheBloke\/Wizard-Vicuna-13B-Uncensored-HF"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.52,
        "ARC":58.96,
        "HellaSwag":81.95,
        "MMLU":47.92,
        "TruthfulQA":51.69,
        "Winogrande":75.69,
        "GSM8K":8.64,
        "DROP":21.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":218.0,
        "Available on the hub":true,
        "Model sha":"95bfd1640a54e76b3e857c2462fd3a77eca0b275",
        "model_name_for_query":"ehartford\/Wizard-Vicuna-13B-Uncensored"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.51,
        "ARC":57.76,
        "HellaSwag":81.04,
        "MMLU":48.38,
        "TruthfulQA":43.48,
        "Winogrande":76.16,
        "GSM8K":0.0,
        "DROP":39.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":true,
        "Model sha":"3553d84037addc97678f99a3464be4c866a0c268",
        "model_name_for_query":"64bits\/LexPodLM-13B"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":49.5,
        "ARC":59.73,
        "HellaSwag":82.66,
        "MMLU":56.94,
        "TruthfulQA":52.92,
        "Winogrande":74.43,
        "GSM8K":1.9,
        "DROP":17.93,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"2af03c3287c60c4ba2fb6afa86c26cf722ab001d",
        "model_name_for_query":"TFLai\/Ensemble5-Platypus2-13B-QLora-0.80-epoch"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.49,
        "ARC":56.74,
        "HellaSwag":80.37,
        "MMLU":55.28,
        "TruthfulQA":51.96,
        "Winogrande":72.38,
        "GSM8K":13.12,
        "DROP":16.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":167.0,
        "Available on the hub":true,
        "Model sha":"277697af19d4b267626ebc9f4e078d19a9a0fddf",
        "model_name_for_query":"lmsys\/vicuna-13b-v1.5-16k"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":49.49,
        "ARC":55.38,
        "HellaSwag":78.57,
        "MMLU":49.39,
        "TruthfulQA":41.83,
        "Winogrande":74.19,
        "GSM8K":9.86,
        "DROP":37.21,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"f660a40323b29040e78097acca320517ed242512",
        "model_name_for_query":"lvkaokao\/llama2-7b-hf-instruction-lora"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.46,
        "ARC":58.62,
        "HellaSwag":82.32,
        "MMLU":54.25,
        "TruthfulQA":38.17,
        "Winogrande":76.8,
        "GSM8K":11.98,
        "DROP":24.05,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"08bc7112a775dd4223d441355f3d619694013789",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-huangyt_FINETUNE2_3w"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.43,
        "ARC":56.74,
        "HellaSwag":80.34,
        "MMLU":48.9,
        "TruthfulQA":51.0,
        "Winogrande":75.93,
        "GSM8K":8.04,
        "DROP":25.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"c0a56d9f5a15bea07493191b5a6295f6797a9b2c",
        "model_name_for_query":"YeungNLP\/firefly-llama-13b-v1.2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.42,
        "ARC":60.92,
        "HellaSwag":82.13,
        "MMLU":56.99,
        "TruthfulQA":48.64,
        "Winogrande":76.56,
        "GSM8K":12.21,
        "DROP":8.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"6e1a6e1f91f6ac97b643be1bd24be6096e2e7dd3",
        "model_name_for_query":"Aspik101\/StableBeluga-13B-instruct-PL-lora_unload"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.42,
        "ARC":62.2,
        "HellaSwag":83.48,
        "MMLU":55.87,
        "TruthfulQA":46.67,
        "Winogrande":78.3,
        "GSM8K":13.04,
        "DROP":6.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":23.0,
        "Available on the hub":true,
        "Model sha":"3c11f81d9180618f13777276b1eb0eb70ab99cf0",
        "model_name_for_query":"elinas\/chronos-33b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":49.41,
        "ARC":61.77,
        "HellaSwag":82.68,
        "MMLU":57.75,
        "TruthfulQA":51.44,
        "Winogrande":77.43,
        "GSM8K":5.76,
        "DROP":9.05,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"fbe6f0e32b5ecf9d75510d0b11a286466f46d79e",
        "model_name_for_query":"TFLai\/SpeechlessV1-Nova-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.41,
        "ARC":61.26,
        "HellaSwag":83.81,
        "MMLU":56.53,
        "TruthfulQA":46.56,
        "Winogrande":77.43,
        "GSM8K":13.27,
        "DROP":7.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":58.0,
        "Available on the hub":true,
        "Model sha":"24916f62b8243a7e4646ea53eeb45d890cbd308f",
        "model_name_for_query":"PygmalionAI\/mythalion-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.41,
        "ARC":56.74,
        "HellaSwag":82.27,
        "MMLU":56.18,
        "TruthfulQA":39.65,
        "Winogrande":77.03,
        "GSM8K":13.19,
        "DROP":20.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"7e0046627fabb0f23ace4b71f279d459ec4a0ff1",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE1_17w-r4"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.39,
        "ARC":55.8,
        "HellaSwag":82.1,
        "MMLU":55.33,
        "TruthfulQA":39.82,
        "Winogrande":76.24,
        "GSM8K":11.37,
        "DROP":25.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"86f255afabc8986c73376cafd98628a068649022",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE5_4w-r16-gate_up_down"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":49.38,
        "ARC":62.29,
        "HellaSwag":82.09,
        "MMLU":57.91,
        "TruthfulQA":47.03,
        "Winogrande":77.43,
        "GSM8K":10.99,
        "DROP":7.93,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"31e1e3235515717a151915131bc970be188d964e",
        "model_name_for_query":"yeontaek\/Platypus2xOpenOrca-13B-IA3-v2.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.35,
        "ARC":57.17,
        "HellaSwag":79.34,
        "MMLU":51.0,
        "TruthfulQA":49.11,
        "Winogrande":73.48,
        "GSM8K":7.58,
        "DROP":27.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"0875bf202aedeef7a58d7382fd6f55f5bca12968",
        "model_name_for_query":"zarakiquemparte\/zaraxe-l2-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.34,
        "ARC":60.32,
        "HellaSwag":83.21,
        "MMLU":55.05,
        "TruthfulQA":50.91,
        "Winogrande":75.37,
        "GSM8K":11.75,
        "DROP":8.73,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":11.0,
        "Available on the hub":true,
        "Model sha":"2f0e2cb734685a6ce0736a9f3e909a795d7592cc",
        "model_name_for_query":"Austism\/chronos-hermes-13b-v2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.33,
        "ARC":53.84,
        "HellaSwag":78.86,
        "MMLU":71.54,
        "TruthfulQA":42.58,
        "Winogrande":75.3,
        "GSM8K":10.01,
        "DROP":13.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"a53fbe358d4cb546916847d861ccfaf7c724a103",
        "model_name_for_query":"gaodrew\/gaodrew-gorgonzola-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.33,
        "ARC":54.95,
        "HellaSwag":78.19,
        "MMLU":50.12,
        "TruthfulQA":49.05,
        "Winogrande":71.03,
        "GSM8K":4.93,
        "DROP":37.04,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":143.0,
        "Available on the hub":true,
        "Model sha":"d120381b03051b60a7c77ec3fb1be6c3c1546466",
        "model_name_for_query":"Open-Orca\/OpenOrca-Preview1-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.33,
        "ARC":59.9,
        "HellaSwag":81.12,
        "MMLU":47.18,
        "TruthfulQA":51.18,
        "Winogrande":76.8,
        "GSM8K":8.72,
        "DROP":20.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":true,
        "Model sha":"90c02cc338afcdd890a948af06432674743363ad",
        "model_name_for_query":"TehVenom\/Metharme-13b-Merged"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.33,
        "ARC":60.58,
        "HellaSwag":82.96,
        "MMLU":56.75,
        "TruthfulQA":40.14,
        "Winogrande":76.64,
        "GSM8K":7.35,
        "DROP":20.86,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"26b7edfd282af223d86d5e539451357bb114247b",
        "model_name_for_query":"bhenrym14\/airophin-v2-13b-PI-8k-fp16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.32,
        "ARC":56.83,
        "HellaSwag":81.7,
        "MMLU":52.79,
        "TruthfulQA":50.23,
        "Winogrande":71.11,
        "GSM8K":0.23,
        "DROP":32.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"45ba2f603769aa6b97639962f522b8d7398c2393",
        "model_name_for_query":"Sao10K\/Stheno-1.3-L2-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.31,
        "ARC":55.72,
        "HellaSwag":80.34,
        "MMLU":55.4,
        "TruthfulQA":51.44,
        "Winogrande":74.66,
        "GSM8K":13.27,
        "DROP":14.35,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":36.0,
        "Available on the hub":true,
        "Model sha":"134cea14627fd875f6f277cad92f988024855478",
        "model_name_for_query":"ehartford\/WizardLM-1.0-Uncensored-Llama2-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.3,
        "ARC":53.5,
        "HellaSwag":70.5,
        "MMLU":54.4,
        "TruthfulQA":50.19,
        "Winogrande":70.01,
        "GSM8K":9.7,
        "DROP":36.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.76,
        "Hub \u2764\ufe0f":54.0,
        "Available on the hub":true,
        "Model sha":"1f30e4f2037e1e30122667639b8ef37138e85057",
        "model_name_for_query":"hpcai-tech\/Colossal-LLaMA-2-7b-base"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.3,
        "ARC":55.29,
        "HellaSwag":81.87,
        "MMLU":48.23,
        "TruthfulQA":51.19,
        "Winogrande":75.3,
        "GSM8K":1.21,
        "DROP":32.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"b407c1ece029ad5693d38e6e0931e9482962ed15",
        "model_name_for_query":"TheBloke\/Nous-Hermes-13B-SuperHOT-8K-fp16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.29,
        "ARC":58.53,
        "HellaSwag":82.2,
        "MMLU":50.61,
        "TruthfulQA":47.5,
        "Winogrande":76.24,
        "GSM8K":10.39,
        "DROP":19.55,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":26.0,
        "Available on the hub":true,
        "Model sha":"aa828ef92c363a5577ffd7d29e678277b9d2eb3c",
        "model_name_for_query":"digitous\/13B-HyperMantis"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":49.28,
        "ARC":60.24,
        "HellaSwag":82.22,
        "MMLU":58.03,
        "TruthfulQA":55.26,
        "Winogrande":75.37,
        "GSM8K":0.91,
        "DROP":12.95,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"15a99bc147cf9b744cbab7a7c8c5f232cd0c8d10",
        "model_name_for_query":"TFLai\/Luban-Platypus2-13B-QLora-0.80-epoch"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.28,
        "ARC":56.4,
        "HellaSwag":79.34,
        "MMLU":46.59,
        "TruthfulQA":48.6,
        "Winogrande":75.22,
        "GSM8K":11.83,
        "DROP":27.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"045c84727d495bfb4b612a2482ce0d807c067b46",
        "model_name_for_query":"dvruette\/llama-13b-pretrained-dropout"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":49.27,
        "ARC":54.69,
        "HellaSwag":76.45,
        "MMLU":55.08,
        "TruthfulQA":46.15,
        "Winogrande":68.43,
        "GSM8K":8.34,
        "DROP":35.76,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":13.0,
        "Available on the hub":true,
        "Model sha":"2caa8ce3aab012bf34c7c531827f6befc7cc1c98",
        "model_name_for_query":"jondurbin\/airoboros-c34b-2.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.27,
        "ARC":56.66,
        "HellaSwag":79.16,
        "MMLU":51.94,
        "TruthfulQA":51.29,
        "Winogrande":74.74,
        "GSM8K":8.57,
        "DROP":22.55,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"68ca01427848528ab21263fd06720a081b09d063",
        "model_name_for_query":"zarakiquemparte\/zarafusionex-1.2-l2-7b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":49.26,
        "ARC":57.34,
        "HellaSwag":78.66,
        "MMLU":55.56,
        "TruthfulQA":51.97,
        "Winogrande":75.77,
        "GSM8K":17.74,
        "DROP":7.77,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":11.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"9399ea6c2a1d955e31d6b4d68b2b86115aea0e59",
        "model_name_for_query":"maywell\/Synatra-11B-Testbench"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.25,
        "ARC":59.04,
        "HellaSwag":82.21,
        "MMLU":54.64,
        "TruthfulQA":47.27,
        "Winogrande":71.9,
        "GSM8K":13.5,
        "DROP":16.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":157.0,
        "Available on the hub":true,
        "Model sha":"6760d0c07ffdc2405295ed7a29437cf4dc414bac",
        "model_name_for_query":"WizardLM\/WizardLM-13B-V1.2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.22,
        "ARC":55.8,
        "HellaSwag":80.41,
        "MMLU":55.59,
        "TruthfulQA":51.42,
        "Winogrande":74.11,
        "GSM8K":13.27,
        "DROP":13.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":36.0,
        "Available on the hub":true,
        "Model sha":"134cea14627fd875f6f277cad92f988024855478",
        "model_name_for_query":"ehartford\/WizardLM-1.0-Uncensored-Llama2-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.19,
        "ARC":55.12,
        "HellaSwag":80.24,
        "MMLU":50.89,
        "TruthfulQA":44.62,
        "Winogrande":71.9,
        "GSM8K":2.27,
        "DROP":39.31,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":true,
        "Model sha":"172e30e56e939f73d7d00a165c2d49cbd284481f",
        "model_name_for_query":"jondurbin\/airoboros-l2-13b-2.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.19,
        "ARC":53.67,
        "HellaSwag":79.66,
        "MMLU":54.48,
        "TruthfulQA":40.22,
        "Winogrande":75.93,
        "GSM8K":14.56,
        "DROP":25.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"22cea7bf138eb0d6c962812df2b2235290acbee2",
        "model_name_for_query":"wei123602\/llama2-13b-FINETUNE3_TEST"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.18,
        "ARC":59.81,
        "HellaSwag":82.8,
        "MMLU":56.76,
        "TruthfulQA":44.45,
        "Winogrande":76.24,
        "GSM8K":18.12,
        "DROP":6.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"a95be7130d32da99bcd484f6f436b2dd49341110",
        "model_name_for_query":"openchat\/openchat_v3.1"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":49.16,
        "ARC":60.75,
        "HellaSwag":82.09,
        "MMLU":58.77,
        "TruthfulQA":45.15,
        "Winogrande":77.03,
        "GSM8K":7.13,
        "DROP":13.23,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"8b2f5d65c03d415b7c43530def622e133e1ef014",
        "model_name_for_query":"yeontaek\/Platypus2xOpenOrca-13B-LoRa"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.16,
        "ARC":58.7,
        "HellaSwag":81.93,
        "MMLU":57.21,
        "TruthfulQA":43.26,
        "Winogrande":76.95,
        "GSM8K":12.51,
        "DROP":13.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"939d06081210fa943c60210a47583f43b60901ad",
        "model_name_for_query":"wei123602\/Llama-2-13b-FINETUNE4"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.13,
        "ARC":57.25,
        "HellaSwag":80.88,
        "MMLU":52.92,
        "TruthfulQA":50.55,
        "Winogrande":74.11,
        "GSM8K":14.1,
        "DROP":14.07,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"gpl-3.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"f802ea7c01e2da27b0f7091c70d3ecfd8fc042b9",
        "model_name_for_query":"LLMs\/WizardLM-13B-V1.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.12,
        "ARC":55.38,
        "HellaSwag":78.58,
        "MMLU":58.53,
        "TruthfulQA":43.22,
        "Winogrande":78.77,
        "GSM8K":18.73,
        "DROP":10.65,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.0,
        "Hub \u2764\ufe0f":114.0,
        "Available on the hub":true,
        "Model sha":"79946225fa7a215e0ebcf4440a9cce88e475deaa",
        "model_name_for_query":"internlm\/internlm-20b-chat"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.12,
        "ARC":58.96,
        "HellaSwag":79.71,
        "MMLU":49.1,
        "TruthfulQA":49.59,
        "Winogrande":75.61,
        "GSM8K":8.19,
        "DROP":22.69,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"dd326f89ce885844d714d9ab33603e0d17f56cc5",
        "model_name_for_query":"YeungNLP\/firefly-llama-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.09,
        "ARC":57.25,
        "HellaSwag":80.88,
        "MMLU":52.9,
        "TruthfulQA":50.55,
        "Winogrande":74.11,
        "GSM8K":13.87,
        "DROP":14.07,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":true,
        "Model sha":"b79733805e98e668ff9a459975c259881b1b8014",
        "model_name_for_query":"TheBloke\/wizardLM-13B-1.0-fp16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.09,
        "ARC":57.25,
        "HellaSwag":82.27,
        "MMLU":56.16,
        "TruthfulQA":39.75,
        "Winogrande":77.43,
        "GSM8K":13.34,
        "DROP":17.43,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"5da5c92f3cf85a62c1be90a0bb2ae8dffce64a7d",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE1_17w-r16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.08,
        "ARC":59.3,
        "HellaSwag":81.53,
        "MMLU":57.46,
        "TruthfulQA":41.63,
        "Winogrande":76.72,
        "GSM8K":12.13,
        "DROP":14.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"bacd035db122dafaf86bf52bb9ca8c613070cc58",
        "model_name_for_query":"wei123602\/llama-13b-FINETUNE3"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.07,
        "ARC":54.86,
        "HellaSwag":78.58,
        "MMLU":47.89,
        "TruthfulQA":49.0,
        "Winogrande":72.61,
        "GSM8K":4.55,
        "DROP":35.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"e46bfa43829cbea7608192a6d07bcc147387fdb7",
        "model_name_for_query":"zarakiquemparte\/zarablend-1.1-l2-7b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":49.05,
        "ARC":62.8,
        "HellaSwag":84.04,
        "MMLU":55.13,
        "TruthfulQA":45.66,
        "Winogrande":75.14,
        "GSM8K":11.75,
        "DROP":8.82,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"ad086aacf0176911133b6cccfb34364afce9de5a",
        "model_name_for_query":"lu-vae\/llama2-13B-sharegpt4-orca-openplatypus-8w"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.03,
        "ARC":54.35,
        "HellaSwag":78.44,
        "MMLU":47.74,
        "TruthfulQA":49.88,
        "Winogrande":73.09,
        "GSM8K":4.47,
        "DROP":35.26,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"745c34e70aa92056e8cd79c1d16e8fcfe1797645",
        "model_name_for_query":"zarakiquemparte\/kuchiki-l2-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.03,
        "ARC":54.44,
        "HellaSwag":78.62,
        "MMLU":47.61,
        "TruthfulQA":49.38,
        "Winogrande":73.32,
        "GSM8K":4.4,
        "DROP":35.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":true,
        "Model sha":"8b14e71ae3f52c409a25e1ac98dd05e0bb91eaff",
        "model_name_for_query":"zarakiquemparte\/zarablend-l2-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":49.01,
        "ARC":56.4,
        "HellaSwag":79.33,
        "MMLU":48.4,
        "TruthfulQA":48.38,
        "Winogrande":73.95,
        "GSM8K":5.53,
        "DROP":31.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"cc5ee2231066c147423f89e9df40f7364c3275a5",
        "model_name_for_query":"haonan-li\/bactrian-x-llama-13b-merged"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.98,
        "ARC":60.84,
        "HellaSwag":83.66,
        "MMLU":56.73,
        "TruthfulQA":47.54,
        "Winogrande":76.16,
        "GSM8K":11.37,
        "DROP":6.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"af473e64f6a4fa02a7e24ee7679eea9505eb179d",
        "model_name_for_query":"augtoma\/qCammel-13"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.96,
        "ARC":61.01,
        "HellaSwag":82.82,
        "MMLU":56.09,
        "TruthfulQA":44.87,
        "Winogrande":77.74,
        "GSM8K":12.28,
        "DROP":7.92,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"0ddd810c9150492d7318656acac44849651edbf2",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-OpenOrca_5w"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.96,
        "ARC":58.28,
        "HellaSwag":81.05,
        "MMLU":50.03,
        "TruthfulQA":51.57,
        "Winogrande":76.24,
        "GSM8K":7.13,
        "DROP":18.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":12.0,
        "Available on the hub":true,
        "Model sha":"9219b61a0e8bc880e4cd0f8bebc48a97ee0950c7",
        "model_name_for_query":"TheBloke\/airoboros-13B-HF"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.94,
        "ARC":58.28,
        "HellaSwag":81.05,
        "MMLU":50.03,
        "TruthfulQA":51.57,
        "Winogrande":76.24,
        "GSM8K":6.97,
        "DROP":18.42,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":93.0,
        "Available on the hub":true,
        "Model sha":"44830f9e1559f318f5dad875bab40d1d1beddbfc",
        "model_name_for_query":"jondurbin\/airoboros-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.94,
        "ARC":61.26,
        "HellaSwag":82.13,
        "MMLU":56.25,
        "TruthfulQA":46.67,
        "Winogrande":76.32,
        "GSM8K":12.36,
        "DROP":7.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"fa988ba73f67ad0c8e7fa8f408106ea040070258",
        "model_name_for_query":"Danielbrdz\/Barcenas-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.93,
        "ARC":61.6,
        "HellaSwag":82.31,
        "MMLU":57.27,
        "TruthfulQA":51.53,
        "Winogrande":76.56,
        "GSM8K":4.4,
        "DROP":8.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"1a827ccb7f00157b3cc9ce538d61a6ba8d5a65db",
        "model_name_for_query":"TFLai\/Nova-13B-50-step"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.91,
        "ARC":57.51,
        "HellaSwag":77.94,
        "MMLU":52.56,
        "TruthfulQA":48.18,
        "Winogrande":74.74,
        "GSM8K":9.86,
        "DROP":21.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.97,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"9497e3bd12e19e1300bc7b1980fbe232420134b9",
        "model_name_for_query":"YeungNLP\/firefly-llama2-13b-chat"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.9,
        "ARC":62.54,
        "HellaSwag":82.8,
        "MMLU":56.53,
        "TruthfulQA":45.96,
        "Winogrande":74.27,
        "GSM8K":9.63,
        "DROP":10.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":57.0,
        "Available on the hub":true,
        "Model sha":"32938856dc3d713dcba706aded7c82791b6ff647",
        "model_name_for_query":"Xwin-LM\/Xwin-LM-13B-V0.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.87,
        "ARC":54.35,
        "HellaSwag":82.13,
        "MMLU":55.33,
        "TruthfulQA":39.6,
        "Winogrande":77.19,
        "GSM8K":12.89,
        "DROP":20.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"1646a2b77ddeaf0f848c96ed68726556c7539729",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r8-gate_up_down"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.87,
        "ARC":60.84,
        "HellaSwag":82.99,
        "MMLU":55.96,
        "TruthfulQA":47.72,
        "Winogrande":76.01,
        "GSM8K":12.28,
        "DROP":6.29,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":true,
        "Model sha":"e355ead3a939f471fe2586201156fb972fad0f4b",
        "model_name_for_query":"ehartford\/Samantha-1.11-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.82,
        "ARC":54.18,
        "HellaSwag":78.0,
        "MMLU":48.14,
        "TruthfulQA":49.96,
        "Winogrande":73.16,
        "GSM8K":4.7,
        "DROP":33.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"10fe70fec0df5c4dcbdfd2e9ec74830c41b3cfd2",
        "model_name_for_query":"zarakiquemparte\/kuchiki-1.1-l2-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.8,
        "ARC":59.39,
        "HellaSwag":83.01,
        "MMLU":55.77,
        "TruthfulQA":51.22,
        "Winogrande":74.66,
        "GSM8K":8.95,
        "DROP":8.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"8d2e9087093eef1c9173e167beb40b9d034a4655",
        "model_name_for_query":"Sao10K\/Stheno-Inverted-1.2-L2-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.79,
        "ARC":59.3,
        "HellaSwag":82.66,
        "MMLU":57.39,
        "TruthfulQA":57.09,
        "Winogrande":74.74,
        "GSM8K":0.0,
        "DROP":10.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":true,
        "Model sha":"cbc8b2e4a3beafc311b9e61f8fa9f7526a77c360",
        "model_name_for_query":"Sao10K\/Mythical-Destroyer-V2-L2-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.79,
        "ARC":57.0,
        "HellaSwag":80.32,
        "MMLU":47.08,
        "TruthfulQA":53.46,
        "Winogrande":74.35,
        "GSM8K":0.68,
        "DROP":28.62,
        "Type":"fine-tuned",
        "Precision":"GPTQ",
        "Hub License":"other",
        "#Params (B)":16.22,
        "Hub \u2764\ufe0f":44.0,
        "Available on the hub":true,
        "Model sha":"085eb5cd394f30d72bf5efcf83a580e87264b3e8",
        "model_name_for_query":"TheBloke\/WizardLM-13B-V1-1-SuperHOT-8K-GPTQ"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.78,
        "ARC":59.56,
        "HellaSwag":83.47,
        "MMLU":55.8,
        "TruthfulQA":44.58,
        "Winogrande":75.61,
        "GSM8K":13.95,
        "DROP":8.51,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"agpl-3.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"9195bd6ea775daf347a275e190665e10bf1fb54b",
        "model_name_for_query":"kingbri\/chronolima-airo-grad-l2-13B"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":48.78,
        "ARC":56.66,
        "HellaSwag":80.56,
        "MMLU":55.43,
        "TruthfulQA":53.62,
        "Winogrande":72.61,
        "GSM8K":0.08,
        "DROP":22.51,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"f7b6c11b4df16079dfdd1e8dd8c489a8835c7cc4",
        "model_name_for_query":"TFLai\/Athena-Platypus2-13B-QLora-0.80-epoch"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":48.76,
        "ARC":56.14,
        "HellaSwag":78.32,
        "MMLU":48.62,
        "TruthfulQA":45.0,
        "Winogrande":74.51,
        "GSM8K":5.0,
        "DROP":33.7,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"mit",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"74edb1ad58d3d517ef46c4e2a31081084ecbc473",
        "model_name_for_query":"teknium\/OpenHermes-7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.72,
        "ARC":56.91,
        "HellaSwag":80.71,
        "MMLU":53.21,
        "TruthfulQA":48.25,
        "Winogrande":74.74,
        "GSM8K":11.3,
        "DROP":15.9,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"b05b4c22893e950e8e33acb67087a9acc8f0ab97",
        "model_name_for_query":"FelixChao\/llama2-13b-math1.2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.69,
        "ARC":59.56,
        "HellaSwag":83.5,
        "MMLU":55.78,
        "TruthfulQA":44.67,
        "Winogrande":75.85,
        "GSM8K":13.65,
        "DROP":7.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"d2ad57b2b50361485b2b04e59a989161599cb08b",
        "model_name_for_query":"kingbri\/airolima-chronos-grad-l2-13B"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":48.68,
        "ARC":58.02,
        "HellaSwag":82.65,
        "MMLU":55.99,
        "TruthfulQA":48.27,
        "Winogrande":76.09,
        "GSM8K":13.12,
        "DROP":6.61,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"2be36a2dab4ed0f97727a1508367f53d59950818",
        "model_name_for_query":"lu-vae\/llama2-13b-sharegpt4-test"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.65,
        "ARC":57.08,
        "HellaSwag":80.61,
        "MMLU":53.05,
        "TruthfulQA":48.3,
        "Winogrande":74.27,
        "GSM8K":10.99,
        "DROP":16.24,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"b05b4c22893e950e8e33acb67087a9acc8f0ab97",
        "model_name_for_query":"FelixChao\/llama2-13b-math1.2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.65,
        "ARC":60.15,
        "HellaSwag":82.84,
        "MMLU":56.84,
        "TruthfulQA":44.38,
        "Winogrande":76.24,
        "GSM8K":13.8,
        "DROP":6.26,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"cc708183e430234b8718c08d9f90474569eabeac",
        "model_name_for_query":"openchat\/openchat_v3.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.62,
        "ARC":52.39,
        "HellaSwag":77.52,
        "MMLU":47.72,
        "TruthfulQA":46.87,
        "Winogrande":74.27,
        "GSM8K":8.04,
        "DROP":33.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":104.0,
        "Available on the hub":true,
        "Model sha":"4c3bc725f71898c6a1acd4ea98a2f8d74d1b1b6b",
        "model_name_for_query":"FlagAlpha\/Llama2-Chinese-7b-Chat"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.62,
        "ARC":59.64,
        "HellaSwag":82.68,
        "MMLU":56.68,
        "TruthfulQA":44.49,
        "Winogrande":76.95,
        "GSM8K":13.65,
        "DROP":6.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":36.0,
        "Available on the hub":true,
        "Model sha":"65320bf6dbe0cb4682d45a9e55dbc876502f8b66",
        "model_name_for_query":"openchat\/openchat_v3.2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.61,
        "ARC":58.19,
        "HellaSwag":81.89,
        "MMLU":52.02,
        "TruthfulQA":39.96,
        "Winogrande":74.82,
        "GSM8K":0.76,
        "DROP":32.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":13.0,
        "Available on the hub":true,
        "Model sha":"2768cf6f955b65868ccbb20658e2cc444b2f3be9",
        "model_name_for_query":"hyunseoki\/ko-en-llama2-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.6,
        "ARC":55.55,
        "HellaSwag":77.11,
        "MMLU":52.16,
        "TruthfulQA":52.23,
        "Winogrande":69.93,
        "GSM8K":14.4,
        "DROP":18.83,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":56.0,
        "Available on the hub":true,
        "Model sha":"b6d16c3e1cffef5e914863f41fd96152dafddd6f",
        "model_name_for_query":"ehartford\/dolphin-llama-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.59,
        "ARC":60.67,
        "HellaSwag":82.69,
        "MMLU":56.23,
        "TruthfulQA":44.41,
        "Winogrande":77.35,
        "GSM8K":11.83,
        "DROP":6.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"0ec406128968b41a9b7a5f18c358f7638d696b56",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-dolphin_5w"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.57,
        "ARC":60.07,
        "HellaSwag":84.05,
        "MMLU":55.61,
        "TruthfulQA":46.12,
        "Winogrande":75.61,
        "GSM8K":10.99,
        "DROP":7.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"ea321257d81e0f41c985f5155297b7fbd6ac375a",
        "model_name_for_query":"Expert68\/llama2_13b_instructed_version2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.57,
        "ARC":57.17,
        "HellaSwag":82.26,
        "MMLU":55.89,
        "TruthfulQA":39.93,
        "Winogrande":76.56,
        "GSM8K":12.89,
        "DROP":15.27,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"c1a5ad1b5e490ed860eeb1b449a02e14da10717f",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-huangyt_Fintune_1_17w-gate_up_down_proj"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.56,
        "ARC":59.98,
        "HellaSwag":81.86,
        "MMLU":56.11,
        "TruthfulQA":47.41,
        "Winogrande":76.09,
        "GSM8K":10.99,
        "DROP":7.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":true,
        "Model sha":"fbb23bc41438b016f1df1e9180c6c350a03557ea",
        "model_name_for_query":"migtissera\/Synthia-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.51,
        "ARC":50.51,
        "HellaSwag":83.65,
        "MMLU":51.53,
        "TruthfulQA":44.23,
        "Winogrande":71.43,
        "GSM8K":2.5,
        "DROP":35.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.1,
        "Hub \u2764\ufe0f":19.0,
        "Available on the hub":true,
        "Model sha":"b8d5c09c83b1ef23668cb9209dbc43c0df2de8ae",
        "model_name_for_query":"vonjack\/Qwen-LLaMAfied-HFTok-7B-Chat"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.45,
        "ARC":58.53,
        "HellaSwag":82.27,
        "MMLU":55.9,
        "TruthfulQA":40.26,
        "Winogrande":76.95,
        "GSM8K":15.39,
        "DROP":9.86,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"fdc145fe1b47cdda483535c018e35a5ab249a552",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE4_addto15k_4.5w-r16-gate_up_down"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.44,
        "ARC":59.47,
        "HellaSwag":82.6,
        "MMLU":56.82,
        "TruthfulQA":44.51,
        "Winogrande":76.09,
        "GSM8K":13.42,
        "DROP":6.15,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":36.0,
        "Available on the hub":true,
        "Model sha":"bc771c901529dedbf04864d0b81452f62301f882",
        "model_name_for_query":"openchat\/openchat_v3.2"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":48.42,
        "ARC":54.69,
        "HellaSwag":76.84,
        "MMLU":55.43,
        "TruthfulQA":51.36,
        "Winogrande":72.53,
        "GSM8K":20.02,
        "DROP":8.05,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"79d9761af231fecbfaf6066d6d405a0f8c04f4ba",
        "model_name_for_query":"jondurbin\/airoboros-c34b-2.2.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.4,
        "ARC":52.82,
        "HellaSwag":78.03,
        "MMLU":50.03,
        "TruthfulQA":50.19,
        "Winogrande":73.48,
        "GSM8K":5.61,
        "DROP":28.65,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"aa63a32154923034fb89b1408d3d7ffa994d3327",
        "model_name_for_query":"jphme\/em_german_leo_mistral"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.39,
        "ARC":58.7,
        "HellaSwag":81.63,
        "MMLU":50.84,
        "TruthfulQA":49.17,
        "Winogrande":76.64,
        "GSM8K":12.21,
        "DROP":9.58,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":108.0,
        "Available on the hub":true,
        "Model sha":"aed786b0200251c9962ac200c50f7e367f264b46",
        "model_name_for_query":"openaccess-ai-collective\/manticore-13b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":48.39,
        "ARC":60.67,
        "HellaSwag":82.34,
        "MMLU":52.32,
        "TruthfulQA":50.62,
        "Winogrande":73.64,
        "GSM8K":4.62,
        "DROP":14.48,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":11.0,
        "Available on the hub":true,
        "Model sha":"6c2faf828c5380d28c51fcb4d3d0f1a420fb9a9a",
        "model_name_for_query":"The-Face-Of-Goonery\/Huginn-v3-13b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":48.39,
        "ARC":60.67,
        "HellaSwag":82.34,
        "MMLU":52.32,
        "TruthfulQA":50.62,
        "Winogrande":73.64,
        "GSM8K":4.62,
        "DROP":14.48,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"6186feee849e0c2b7e62d4cbdc4cdc48260ac684",
        "model_name_for_query":"The-Face-Of-Goonery\/Huginn-13b-V4"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":48.39,
        "ARC":60.67,
        "HellaSwag":82.34,
        "MMLU":52.32,
        "TruthfulQA":50.62,
        "Winogrande":73.64,
        "GSM8K":4.62,
        "DROP":14.48,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"f3be56d8bf71a8d3905974b1e5fcba7336b02159",
        "model_name_for_query":"The-Face-Of-Goonery\/Huginn-13b-v4.5"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.38,
        "ARC":56.91,
        "HellaSwag":81.22,
        "MMLU":56.06,
        "TruthfulQA":49.76,
        "Winogrande":75.22,
        "GSM8K":12.28,
        "DROP":7.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"5c8aeb722e11d1c7258abd45f9f2840f57976c28",
        "model_name_for_query":"Aspik101\/vicuna-13b-v1.5-PL-lora_unload"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.36,
        "ARC":56.83,
        "HellaSwag":80.69,
        "MMLU":53.43,
        "TruthfulQA":48.48,
        "Winogrande":74.74,
        "GSM8K":10.69,
        "DROP":13.62,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"3c4d83d3525e54a493ff510443fdcca44bf63b59",
        "model_name_for_query":"FelixChao\/llama2-13b-math1.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.35,
        "ARC":62.29,
        "HellaSwag":83.28,
        "MMLU":56.14,
        "TruthfulQA":44.72,
        "Winogrande":74.35,
        "GSM8K":11.22,
        "DROP":6.48,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"a6d9e26ab765eb170cc0aa428ee5e25b08524657",
        "model_name_for_query":"Sao10K\/SthenoWriter-L2-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.34,
        "ARC":58.7,
        "HellaSwag":82.54,
        "MMLU":51.16,
        "TruthfulQA":52.42,
        "Winogrande":75.3,
        "GSM8K":12.13,
        "DROP":6.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":29.96,
        "Hub \u2764\ufe0f":183.0,
        "Available on the hub":true,
        "Model sha":"54f33278a04aa4e612bca482b82f801ab658e890",
        "model_name_for_query":"mosaicml\/mpt-30b-chat"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.34,
        "ARC":57.25,
        "HellaSwag":80.74,
        "MMLU":53.56,
        "TruthfulQA":48.43,
        "Winogrande":74.43,
        "GSM8K":10.69,
        "DROP":13.26,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"3c4d83d3525e54a493ff510443fdcca44bf63b59",
        "model_name_for_query":"FelixChao\/llama2-13b-math1.1"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":48.33,
        "ARC":61.09,
        "HellaSwag":82.99,
        "MMLU":55.47,
        "TruthfulQA":44.12,
        "Winogrande":77.19,
        "GSM8K":10.99,
        "DROP":6.43,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"67e68284234538d3851d5c0c334383daffec57a2",
        "model_name_for_query":"yeontaek\/llama-2-13b-Guanaco-QLoRA"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.32,
        "ARC":58.7,
        "HellaSwag":82.52,
        "MMLU":53.39,
        "TruthfulQA":50.55,
        "Winogrande":75.06,
        "GSM8K":11.3,
        "DROP":6.74,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":17.0,
        "Available on the hub":true,
        "Model sha":"e5d411138e72370c5613dfea0f66ded99f6e62f9",
        "model_name_for_query":"elinas\/chronos-13b-v2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.31,
        "ARC":55.97,
        "HellaSwag":80.72,
        "MMLU":52.85,
        "TruthfulQA":45.03,
        "Winogrande":72.77,
        "GSM8K":8.87,
        "DROP":21.93,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"bb3029bce8347b09c2fd6908475b195bcabe53e3",
        "model_name_for_query":"luffycodes\/mcq-hal-vicuna-13b-v1.5"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.3,
        "ARC":59.81,
        "HellaSwag":82.24,
        "MMLU":56.35,
        "TruthfulQA":46.01,
        "Winogrande":75.45,
        "GSM8K":11.6,
        "DROP":6.6,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":28.0,
        "Available on the hub":true,
        "Model sha":"f09d0fe655ad57cce9179b7b40ea6f81e07db18c",
        "model_name_for_query":"teknium\/OpenHermes-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.28,
        "ARC":56.23,
        "HellaSwag":82.7,
        "MMLU":55.35,
        "TruthfulQA":39.55,
        "Winogrande":76.72,
        "GSM8K":8.64,
        "DROP":18.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"3be177b35f1b44d147751ab38ca6d8a008eb6b7f",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE2_TEST_2.2w"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.26,
        "ARC":56.57,
        "HellaSwag":75.47,
        "MMLU":53.51,
        "TruthfulQA":50.46,
        "Winogrande":73.48,
        "GSM8K":19.33,
        "DROP":8.97,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":41.0,
        "Available on the hub":true,
        "Model sha":"3fd110de9282e52f56f999bf1da1a76425f00e29",
        "model_name_for_query":"ehartford\/Samantha-1.11-CodeLlama-34b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.23,
        "ARC":60.58,
        "HellaSwag":82.19,
        "MMLU":55.45,
        "TruthfulQA":45.11,
        "Winogrande":76.64,
        "GSM8K":11.37,
        "DROP":6.24,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":37.0,
        "Available on the hub":false,
        "Model sha":"31103acf93479d5c3865fb9b51dcb38e10d8b801",
        "model_name_for_query":"shareAI\/llama2-13b-Chinese-chat"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.22,
        "ARC":56.06,
        "HellaSwag":81.71,
        "MMLU":45.36,
        "TruthfulQA":48.55,
        "Winogrande":75.77,
        "GSM8K":7.2,
        "DROP":22.92,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"f6953fa162b487a3d4c6bdc7b7951e09576c2ae5",
        "model_name_for_query":"ausboss\/llama-13b-supercot"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.22,
        "ARC":59.22,
        "HellaSwag":81.02,
        "MMLU":57.04,
        "TruthfulQA":48.43,
        "Winogrande":73.09,
        "GSM8K":8.57,
        "DROP":10.17,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"b1c2ebcda387211732e87911e39edca503502a33",
        "model_name_for_query":"gaodrew\/OpenOrca-Platypus2-13B-thera-1250"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.19,
        "ARC":61.77,
        "HellaSwag":84.53,
        "MMLU":55.21,
        "TruthfulQA":45.94,
        "Winogrande":75.22,
        "GSM8K":8.79,
        "DROP":5.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"294c40349bf0c5377f71d92e7539bf5de3176a74",
        "model_name_for_query":"beaugogh\/Llama2-13b-sharegpt4"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.19,
        "ARC":58.36,
        "HellaSwag":81.61,
        "MMLU":48.84,
        "TruthfulQA":47.54,
        "Winogrande":73.64,
        "GSM8K":3.87,
        "DROP":23.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"482bd38b65e73fde13f5d03fed2bee7acda8fadd",
        "model_name_for_query":"jondurbin\/airoboros-13b-gpt4-1.2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.17,
        "ARC":56.31,
        "HellaSwag":79.14,
        "MMLU":52.71,
        "TruthfulQA":50.19,
        "Winogrande":75.22,
        "GSM8K":7.81,
        "DROP":15.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":112.0,
        "Available on the hub":true,
        "Model sha":"329adcfc39f48dce183eb0b155b732dbe03c6304",
        "model_name_for_query":"stabilityai\/StableBeluga-7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.17,
        "ARC":56.31,
        "HellaSwag":79.14,
        "MMLU":52.71,
        "TruthfulQA":50.19,
        "Winogrande":75.22,
        "GSM8K":7.81,
        "DROP":15.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"e501f231277671710384ba0397da2c4486865958",
        "model_name_for_query":"circulus\/Llama-2-7b-orca-v1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.14,
        "ARC":57.42,
        "HellaSwag":82.42,
        "MMLU":55.57,
        "TruthfulQA":39.19,
        "Winogrande":77.03,
        "GSM8K":12.05,
        "DROP":13.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"469c6674ad2190b639d6f5ce6bfecc1463825dfb",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-huangyt_FINETUNE2_3w-gate_up_down_proj"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.14,
        "ARC":61.26,
        "HellaSwag":83.26,
        "MMLU":56.0,
        "TruthfulQA":41.99,
        "Winogrande":75.22,
        "GSM8K":13.04,
        "DROP":6.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":true,
        "Model sha":"d6624ce1bcc6b50c86b86e879a8c9822218b84d2",
        "model_name_for_query":"CalderaAI\/13B-Legerdemain-L2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.14,
        "ARC":57.85,
        "HellaSwag":81.66,
        "MMLU":54.45,
        "TruthfulQA":46.32,
        "Winogrande":76.48,
        "GSM8K":13.65,
        "DROP":6.55,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":46.0,
        "Available on the hub":true,
        "Model sha":"d72667bd92fd6f76835466d302563d213e0b1ee1",
        "model_name_for_query":"jphme\/Llama-2-13b-chat-german"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.13,
        "ARC":60.32,
        "HellaSwag":82.37,
        "MMLU":56.02,
        "TruthfulQA":42.22,
        "Winogrande":78.06,
        "GSM8K":11.75,
        "DROP":6.13,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":29.0,
        "Available on the hub":true,
        "Model sha":"3cdc103995ccd5fc7fd2cb5f51f71b510466f5fc",
        "model_name_for_query":"PygmalionAI\/pygmalion-2-13b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":48.08,
        "ARC":60.41,
        "HellaSwag":82.58,
        "MMLU":55.86,
        "TruthfulQA":43.61,
        "Winogrande":76.72,
        "GSM8K":8.49,
        "DROP":8.92,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"891be2d8f205baa04c8a92f6ab1225f0d0c3e5bd",
        "model_name_for_query":"dhmeltzer\/Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.08,
        "ARC":55.63,
        "HellaSwag":81.31,
        "MMLU":52.13,
        "TruthfulQA":41.14,
        "Winogrande":76.72,
        "GSM8K":11.22,
        "DROP":18.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"5195e87bb34317c5aaf201faa476aae78ecc9f1b",
        "model_name_for_query":"wei123602\/FINETUNE3_TEST4"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.07,
        "ARC":55.63,
        "HellaSwag":78.96,
        "MMLU":50.3,
        "TruthfulQA":44.72,
        "Winogrande":74.11,
        "GSM8K":0.0,
        "DROP":32.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"df3a2c77a63a370405c7711b323e7ffa550cdd9e",
        "model_name_for_query":"DopeorNope\/LaOT"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.07,
        "ARC":57.17,
        "HellaSwag":82.15,
        "MMLU":54.88,
        "TruthfulQA":40.23,
        "Winogrande":76.32,
        "GSM8K":13.34,
        "DROP":12.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"86adab5c098c9338e098a8e5b0188b0aa39b2478",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE5_4w-r8-gate_up_down"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.05,
        "ARC":53.41,
        "HellaSwag":75.29,
        "MMLU":50.0,
        "TruthfulQA":45.42,
        "Winogrande":72.22,
        "GSM8K":7.13,
        "DROP":32.9,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"e26ca5f157c60fc527170cc04db7fc0ea04ad26f",
        "model_name_for_query":"Voicelab\/trurl-2-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.05,
        "ARC":57.59,
        "HellaSwag":80.2,
        "MMLU":51.85,
        "TruthfulQA":51.56,
        "Winogrande":75.85,
        "GSM8K":10.69,
        "DROP":8.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"734f5641f6c548474517d1536c46024517f120e0",
        "model_name_for_query":"TheBloke\/UltraLM-13B-fp16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.04,
        "ARC":61.26,
        "HellaSwag":82.56,
        "MMLU":56.7,
        "TruthfulQA":44.86,
        "Winogrande":76.87,
        "GSM8K":7.05,
        "DROP":6.95,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":15.0,
        "Available on the hub":true,
        "Model sha":"b5e926e3d6c03e83c7983e87eb71098b5e80a62e",
        "model_name_for_query":"garage-bAInd\/Platypus2-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":48.02,
        "ARC":56.23,
        "HellaSwag":79.12,
        "MMLU":52.7,
        "TruthfulQA":50.19,
        "Winogrande":75.22,
        "GSM8K":7.81,
        "DROP":14.86,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"eeb22ca9481a5ed7e131a329324494f234300a45",
        "model_name_for_query":"Lajonbot\/tableBeluga-7B-instruct-pl-lora_unload"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":48.01,
        "ARC":60.67,
        "HellaSwag":82.5,
        "MMLU":56.34,
        "TruthfulQA":43.91,
        "Winogrande":75.93,
        "GSM8K":7.51,
        "DROP":9.24,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"1450c541cf9e378e81862fabeb234b8e0a2bdf5a",
        "model_name_for_query":"yeontaek\/Platypus2-13B-LoRa"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":47.98,
        "ARC":56.91,
        "HellaSwag":79.64,
        "MMLU":52.37,
        "TruthfulQA":50.51,
        "Winogrande":74.27,
        "GSM8K":7.13,
        "DROP":15.06,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":37.0,
        "Available on the hub":true,
        "Model sha":"f9849ea6bf0f6ebb78dca1cea1c7a3ef8f7d715c",
        "model_name_for_query":"pankajmathur\/orca_mini_v3_7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.98,
        "ARC":56.91,
        "HellaSwag":79.64,
        "MMLU":52.37,
        "TruthfulQA":50.51,
        "Winogrande":74.27,
        "GSM8K":7.13,
        "DROP":15.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":37.0,
        "Available on the hub":true,
        "Model sha":"a1583d2f02041fb37df28eeae4da644d8dff33eb",
        "model_name_for_query":"psmathur\/orca_mini_v3_7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.98,
        "ARC":61.01,
        "HellaSwag":83.32,
        "MMLU":55.17,
        "TruthfulQA":40.65,
        "Winogrande":76.8,
        "GSM8K":12.51,
        "DROP":6.39,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"0fed305667508e50330e71a2d43e9cee5ea73783",
        "model_name_for_query":"TheBloke\/Kimiko-v2-13B-fp16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.97,
        "ARC":59.81,
        "HellaSwag":82.5,
        "MMLU":55.9,
        "TruthfulQA":42.3,
        "Winogrande":75.93,
        "GSM8K":13.5,
        "DROP":5.88,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":24.0,
        "Available on the hub":true,
        "Model sha":"aab7ce4d48b31a295a0116b61569d8e87a09bb7a",
        "model_name_for_query":"openchat\/openchat_v3.2_super"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":47.97,
        "ARC":53.92,
        "HellaSwag":74.64,
        "MMLU":49.74,
        "TruthfulQA":45.43,
        "Winogrande":71.59,
        "GSM8K":2.2,
        "DROP":38.28,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.94,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"8f6b11ca4344ac230d6b55defa4e04e60a39f9b5",
        "model_name_for_query":"gywy\/llama2-13b-chinese-v2"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":47.96,
        "ARC":60.92,
        "HellaSwag":82.43,
        "MMLU":55.61,
        "TruthfulQA":44.26,
        "Winogrande":75.69,
        "GSM8K":11.07,
        "DROP":5.74,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"b933009635299bca32c694336aa2007d756a2dda",
        "model_name_for_query":"Aspik101\/Redmond-Puffin-13B-instruct-PL-lora_unload"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.96,
        "ARC":60.92,
        "HellaSwag":83.18,
        "MMLU":54.58,
        "TruthfulQA":44.0,
        "Winogrande":74.9,
        "GSM8K":11.6,
        "DROP":6.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":11.0,
        "Available on the hub":true,
        "Model sha":"feb7ef47ceca6aec9548264a39622b63fdcb853c",
        "model_name_for_query":"Mikael110\/llama-2-13b-guanaco-fp16"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":47.95,
        "ARC":57.34,
        "HellaSwag":81.24,
        "MMLU":55.64,
        "TruthfulQA":55.98,
        "Winogrande":73.88,
        "GSM8K":0.0,
        "DROP":11.55,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"ada55b32fe8ed55b7691d997ad2e86f232c91aad",
        "model_name_for_query":"TFLai\/MythicalDestroyerV2-Platypus2-13B-QLora-0.80-epoch"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.92,
        "ARC":60.49,
        "HellaSwag":83.08,
        "MMLU":54.84,
        "TruthfulQA":43.63,
        "Winogrande":74.9,
        "GSM8K":12.36,
        "DROP":6.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"6e918ff9f563552af4ad66f4308f6d040e24af4b",
        "model_name_for_query":"Undi95\/LewdEngine"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.92,
        "ARC":52.47,
        "HellaSwag":74.04,
        "MMLU":53.88,
        "TruthfulQA":48.04,
        "Winogrande":69.14,
        "GSM8K":10.92,
        "DROP":26.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.99,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"da2cd76e2d61bf0247bd67a4f2835319c54a7d62",
        "model_name_for_query":"hiyouga\/Baichuan2-7B-Chat-LLaMAfied"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.89,
        "ARC":54.44,
        "HellaSwag":78.94,
        "MMLU":50.39,
        "TruthfulQA":46.51,
        "Winogrande":73.16,
        "GSM8K":0.23,
        "DROP":31.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"cc1dad50689b3ebcc1c9c67f275da6b4bb63e2ce",
        "model_name_for_query":"zarakiquemparte\/zaraxls-l2-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.89,
        "ARC":50.17,
        "HellaSwag":77.04,
        "MMLU":47.63,
        "TruthfulQA":41.61,
        "Winogrande":73.8,
        "GSM8K":11.22,
        "DROP":33.74,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"8a026683f79119643f4007da4e9155c7849792cc",
        "model_name_for_query":"TheBloke\/tulu-7B-fp16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.88,
        "ARC":60.41,
        "HellaSwag":83.2,
        "MMLU":55.36,
        "TruthfulQA":42.12,
        "Winogrande":76.64,
        "GSM8K":11.45,
        "DROP":5.98,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":99.0,
        "Available on the hub":true,
        "Model sha":"12af25fa7ea02c4fc636952ea8b9dc9cf48e35be",
        "model_name_for_query":"NousResearch\/Redmond-Puffin-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.87,
        "ARC":60.58,
        "HellaSwag":82.97,
        "MMLU":52.1,
        "TruthfulQA":46.1,
        "Winogrande":73.64,
        "GSM8K":8.11,
        "DROP":11.55,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.53,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"24ebae726954e4c1f24a8b2cbe0ca863012a7338",
        "model_name_for_query":"bhenrym14\/airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-fp16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.86,
        "ARC":59.64,
        "HellaSwag":83.22,
        "MMLU":47.56,
        "TruthfulQA":48.82,
        "Winogrande":76.24,
        "GSM8K":7.73,
        "DROP":11.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"037e369be06a8a0eef87f2cddfd3469670483f29",
        "model_name_for_query":"jondurbin\/airoboros-13b-gpt4-1.4-fp16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.86,
        "ARC":59.64,
        "HellaSwag":83.22,
        "MMLU":47.56,
        "TruthfulQA":48.82,
        "Winogrande":76.24,
        "GSM8K":7.73,
        "DROP":11.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":18.0,
        "Available on the hub":true,
        "Model sha":"d0d2687ed2b4a63a644ed6c5b3f6401844718659",
        "model_name_for_query":"jondurbin\/airoboros-13b-gpt4-1.4"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":47.85,
        "ARC":54.52,
        "HellaSwag":79.36,
        "MMLU":55.15,
        "TruthfulQA":54.32,
        "Winogrande":71.11,
        "GSM8K":0.0,
        "DROP":20.49,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"4b5aabc51907e4cba49f373c6dc09a2634f2fb8a",
        "model_name_for_query":"TFLai\/PuddleJumper-Platypus2-13B-QLoRA-0.80-epoch"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.84,
        "ARC":50.94,
        "HellaSwag":77.65,
        "MMLU":68.93,
        "TruthfulQA":40.63,
        "Winogrande":75.45,
        "GSM8K":8.57,
        "DROP":12.74,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"a53fbe358d4cb546916847d861ccfaf7c724a103",
        "model_name_for_query":"gaodrew\/gaodrew-gorgonzola-13b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":47.84,
        "ARC":59.39,
        "HellaSwag":83.88,
        "MMLU":55.57,
        "TruthfulQA":46.89,
        "Winogrande":74.03,
        "GSM8K":8.04,
        "DROP":7.06,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"e676fbd9015beacfba5d71426beace7605200477",
        "model_name_for_query":"Secbone\/llama-2-13B-instructed"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.83,
        "ARC":56.14,
        "HellaSwag":80.27,
        "MMLU":47.89,
        "TruthfulQA":36.97,
        "Winogrande":73.56,
        "GSM8K":2.27,
        "DROP":37.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"agpl-3.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"142e198df473fd0cd4370b0d50be5f57e1da399b",
        "model_name_for_query":"llama-anon\/instruct-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.82,
        "ARC":54.44,
        "HellaSwag":78.68,
        "MMLU":44.45,
        "TruthfulQA":43.95,
        "Winogrande":74.11,
        "GSM8K":2.2,
        "DROP":36.89,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"699491e2e73cc2936205db143f59c1a686b88f14",
        "model_name_for_query":"jondurbin\/airoboros-l2-7b-2.1"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":47.8,
        "ARC":59.13,
        "HellaSwag":82.13,
        "MMLU":54.98,
        "TruthfulQA":44.23,
        "Winogrande":76.4,
        "GSM8K":8.11,
        "DROP":9.6,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"aad13bce3b243721e52e9cda479f1102dda99f12",
        "model_name_for_query":"dhmeltzer\/Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16_merged"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":47.78,
        "ARC":54.18,
        "HellaSwag":73.84,
        "MMLU":50.67,
        "TruthfulQA":40.7,
        "Winogrande":69.93,
        "GSM8K":8.34,
        "DROP":36.82,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"f66e783ac783837b3f59f274ecf55f18a9221cd0",
        "model_name_for_query":"jondurbin\/airocoder-34b-2.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.78,
        "ARC":60.49,
        "HellaSwag":83.21,
        "MMLU":54.95,
        "TruthfulQA":42.08,
        "Winogrande":76.48,
        "GSM8K":11.22,
        "DROP":6.03,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":99.0,
        "Available on the hub":true,
        "Model sha":"12af25fa7ea02c4fc636952ea8b9dc9cf48e35be",
        "model_name_for_query":"NousResearch\/Redmond-Puffin-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.76,
        "ARC":55.72,
        "HellaSwag":81.55,
        "MMLU":53.9,
        "TruthfulQA":41.89,
        "Winogrande":77.19,
        "GSM8K":11.9,
        "DROP":12.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"eb934db4644738a74143b381445213979c8858ed",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE5_4w-r8-q_k_v_o_gate_up_down"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":47.74,
        "ARC":60.49,
        "HellaSwag":82.76,
        "MMLU":56.52,
        "TruthfulQA":44.14,
        "Winogrande":76.8,
        "GSM8K":6.07,
        "DROP":7.4,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"0a8560232ff73ca3c3f8e217b4517fa6c4f55558",
        "model_name_for_query":"TFLai\/Limarp-Platypus2-13B-QLoRA-0.80-epoch"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.73,
        "ARC":54.35,
        "HellaSwag":78.6,
        "MMLU":46.7,
        "TruthfulQA":45.5,
        "Winogrande":72.77,
        "GSM8K":9.86,
        "DROP":26.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":101.0,
        "Available on the hub":true,
        "Model sha":"6b5e1067e412cc5750aec7415a065671df3618be",
        "model_name_for_query":"Tap-M\/Luna-AI-Llama2-Uncensored"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.71,
        "ARC":61.52,
        "HellaSwag":79.21,
        "MMLU":57.01,
        "TruthfulQA":42.72,
        "Winogrande":75.93,
        "GSM8K":9.63,
        "DROP":7.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"6cd644049de2b944beaefcc6aa34965c00e08529",
        "model_name_for_query":"anhnv125\/llama-op-v4"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.71,
        "ARC":58.53,
        "HellaSwag":81.96,
        "MMLU":48.76,
        "TruthfulQA":48.76,
        "Winogrande":77.19,
        "GSM8K":9.55,
        "DROP":9.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":24.0,
        "Available on the hub":true,
        "Model sha":"f9ef65a3cf50e3c09ccb443f99225148e08517aa",
        "model_name_for_query":"openaccess-ai-collective\/manticore-13b-chat-pyg"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.69,
        "ARC":54.61,
        "HellaSwag":80.41,
        "MMLU":52.88,
        "TruthfulQA":52.14,
        "Winogrande":74.82,
        "GSM8K":10.77,
        "DROP":8.24,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":160.0,
        "Available on the hub":true,
        "Model sha":"7900eeb715a49affee9e6390f824e62eea3f3fb1",
        "model_name_for_query":"lmsys\/vicuna-13b-v1.3"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.68,
        "ARC":59.39,
        "HellaSwag":81.87,
        "MMLU":47.75,
        "TruthfulQA":52.59,
        "Winogrande":77.35,
        "GSM8K":8.11,
        "DROP":6.74,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":false,
        "Model sha":"7aedafea409de07a997d70a84e30242c7b86877c",
        "model_name_for_query":"chansung\/gpt4-alpaca-lora-13b-decapoda-1024"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":47.6,
        "ARC":60.49,
        "HellaSwag":82.86,
        "MMLU":54.67,
        "TruthfulQA":42.97,
        "Winogrande":74.66,
        "GSM8K":11.45,
        "DROP":6.07,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":12.0,
        "Available on the hub":true,
        "Model sha":"2c4fddeb097636d6462b7628a8e053ad3ff4678c",
        "model_name_for_query":"KoboldAI\/LLaMA2-13B-Holomax"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":47.59,
        "ARC":55.89,
        "HellaSwag":78.55,
        "MMLU":52.31,
        "TruthfulQA":50.68,
        "Winogrande":71.51,
        "GSM8K":0.45,
        "DROP":23.71,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":21.83,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"ae8f93963266d31000433f1a52d43435e1473e2b",
        "model_name_for_query":"Envoid\/Yousei-22B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.58,
        "ARC":55.38,
        "HellaSwag":78.47,
        "MMLU":45.18,
        "TruthfulQA":49.29,
        "Winogrande":74.82,
        "GSM8K":6.9,
        "DROP":23.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.89,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"9a21051ae490d2f8ab8b1181c1b45e0412d71a90",
        "model_name_for_query":"YeungNLP\/firefly-ziya-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.52,
        "ARC":55.12,
        "HellaSwag":78.94,
        "MMLU":48.34,
        "TruthfulQA":49.01,
        "Winogrande":74.03,
        "GSM8K":5.76,
        "DROP":21.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":48.0,
        "Available on the hub":true,
        "Model sha":"60e58acecdc1552e1b1752a38d1d91d942d1c3f0",
        "model_name_for_query":"NousResearch\/Nous-Hermes-llama-2-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.51,
        "ARC":57.51,
        "HellaSwag":82.49,
        "MMLU":54.83,
        "TruthfulQA":43.81,
        "Winogrande":77.27,
        "GSM8K":10.46,
        "DROP":6.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"f65029ea8f030731ace568e40bab33a7097a13de",
        "model_name_for_query":"NekoPunchBBB\/Llama-2-13b-hf_Open-Platypus-QLoRA-multigpu"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.51,
        "ARC":59.56,
        "HellaSwag":82.46,
        "MMLU":56.06,
        "TruthfulQA":42.45,
        "Winogrande":76.8,
        "GSM8K":8.57,
        "DROP":6.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"bc55678af8226e1323305f743a4882da31994e0c",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-Open-Platypus_2.5w"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.5,
        "ARC":54.35,
        "HellaSwag":79.29,
        "MMLU":49.33,
        "TruthfulQA":48.92,
        "Winogrande":73.56,
        "GSM8K":10.84,
        "DROP":16.24,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":true,
        "Model sha":"85ea4f4818478084eedd01e958ac5cc7cf64b3bb",
        "model_name_for_query":"migtissera\/Synthia-7B-v1.2"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":47.48,
        "ARC":57.76,
        "HellaSwag":81.63,
        "MMLU":55.63,
        "TruthfulQA":39.7,
        "Winogrande":75.93,
        "GSM8K":2.96,
        "DROP":18.76,
        "Type":"instruction-tuned",
        "Precision":"4bit",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"114eb8efd2de1c9eae85d92de490b95c854dfae9",
        "model_name_for_query":"TFLai\/Platypus2-13B-QLoRA-0.80-epoch"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.47,
        "ARC":59.64,
        "HellaSwag":82.52,
        "MMLU":56.56,
        "TruthfulQA":42.14,
        "Winogrande":76.24,
        "GSM8K":1.59,
        "DROP":13.58,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"ff4eeb0f876c41553c302020041a0e78a15f9aa7",
        "model_name_for_query":"jjaaaww\/posi_13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.45,
        "ARC":61.09,
        "HellaSwag":82.46,
        "MMLU":55.27,
        "TruthfulQA":38.53,
        "Winogrande":77.35,
        "GSM8K":11.14,
        "DROP":6.27,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"dacbafa40716a2d87e593240cc5c1dc883b5066a",
        "model_name_for_query":"StudentLLM\/Alpagasus-2-13b-QLoRA-merged"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.44,
        "ARC":57.94,
        "HellaSwag":82.4,
        "MMLU":48.56,
        "TruthfulQA":47.27,
        "Winogrande":76.87,
        "GSM8K":8.26,
        "DROP":10.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"0e3796192f7edf43968541b9454ea35da4a2b1c5",
        "model_name_for_query":"dvruette\/oasst-llama-13b-2-epochs"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.44,
        "ARC":54.44,
        "HellaSwag":75.78,
        "MMLU":51.36,
        "TruthfulQA":51.17,
        "Winogrande":72.61,
        "GSM8K":0.38,
        "DROP":26.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"5d8e7e5764ace89e6ccd1deece33b0e8a4b4587b",
        "model_name_for_query":"formulae\/Dorflan"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.44,
        "ARC":57.34,
        "HellaSwag":81.09,
        "MMLU":50.59,
        "TruthfulQA":50.22,
        "Winogrande":76.32,
        "GSM8K":10.08,
        "DROP":6.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":103.0,
        "Available on the hub":true,
        "Model sha":"76e90314541be6cfa2b55208831c99f1351c1a33",
        "model_name_for_query":"openaccess-ai-collective\/wizard-mega-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.44,
        "ARC":59.04,
        "HellaSwag":83.05,
        "MMLU":49.41,
        "TruthfulQA":46.62,
        "Winogrande":75.77,
        "GSM8K":8.19,
        "DROP":9.98,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"19c7060adcb34d42e742fe51dd36b8657ac069b7",
        "model_name_for_query":"jondurbin\/airoboros-13b-gpt4-1.1"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":47.43,
        "ARC":52.47,
        "HellaSwag":79.08,
        "MMLU":47.58,
        "TruthfulQA":37.14,
        "Winogrande":74.74,
        "GSM8K":6.82,
        "DROP":34.16,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"819f3f384e37f8906a62a8048556c9e58e495c02",
        "model_name_for_query":"TaylorAI\/FLAN-Llama-7B-2_Llama2-7B-Flash_868_full_model"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.41,
        "ARC":59.47,
        "HellaSwag":82.16,
        "MMLU":54.83,
        "TruthfulQA":41.45,
        "Winogrande":76.24,
        "GSM8K":11.9,
        "DROP":5.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"4ef2c736641c2983996c4662bf481782a9de5055",
        "model_name_for_query":"Lajonbot\/Llama-2-13b-hf-instruct-pl-lora_unload"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.41,
        "ARC":60.84,
        "HellaSwag":82.14,
        "MMLU":55.93,
        "TruthfulQA":38.27,
        "Winogrande":76.4,
        "GSM8K":11.9,
        "DROP":6.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"aa1d543fe3391fe9f0e6143ef785fffe9c871225",
        "model_name_for_query":"layoric\/llama-2-13b-code-alpaca"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":47.39,
        "ARC":59.98,
        "HellaSwag":82.43,
        "MMLU":55.41,
        "TruthfulQA":39.9,
        "Winogrande":76.56,
        "GSM8K":10.54,
        "DROP":6.89,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"6a0a2b6672c7b36c714a66c4a836e0b50c6cb5e6",
        "model_name_for_query":"dhmeltzer\/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.36,
        "ARC":57.08,
        "HellaSwag":81.49,
        "MMLU":49.17,
        "TruthfulQA":48.3,
        "Winogrande":76.4,
        "GSM8K":9.25,
        "DROP":9.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"d0ef3991a11c4dc2ea2f832d4082c89c3c5e810c",
        "model_name_for_query":"Aspik101\/Nous-Hermes-13b-pl-lora_unload"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":47.35,
        "ARC":60.84,
        "HellaSwag":82.43,
        "MMLU":55.55,
        "TruthfulQA":38.65,
        "Winogrande":76.87,
        "GSM8K":10.84,
        "DROP":6.28,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"e324e828c8d68aa8510f50dfab133388a44fd821",
        "model_name_for_query":"StudentLLM\/Alpagasus-2-13b-QLoRA-merged"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.35,
        "ARC":59.39,
        "HellaSwag":81.4,
        "MMLU":53.73,
        "TruthfulQA":45.63,
        "Winogrande":76.01,
        "GSM8K":9.17,
        "DROP":6.13,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":true,
        "Model sha":"5474ecbccd1f2a2cda9f77a157993f55c97377ed",
        "model_name_for_query":"FPHam\/Free_Sydney_13b_HF"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":47.33,
        "ARC":58.87,
        "HellaSwag":82.14,
        "MMLU":54.98,
        "TruthfulQA":42.84,
        "Winogrande":77.11,
        "GSM8K":9.4,
        "DROP":5.99,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"39e07f6213a64d79cf31e9c0773dea6224f7f021",
        "model_name_for_query":"lgaalves\/llama-2-13b-hf-platypus"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.33,
        "ARC":58.87,
        "HellaSwag":82.14,
        "MMLU":54.98,
        "TruthfulQA":42.84,
        "Winogrande":77.11,
        "GSM8K":9.4,
        "DROP":5.99,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"c318a24121bd69509f395e17a9636093213ece21",
        "model_name_for_query":"NekoPunchBBB\/Llama-2-13b-hf_Open-Platypus"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":47.31,
        "ARC":61.09,
        "HellaSwag":82.65,
        "MMLU":56.32,
        "TruthfulQA":38.35,
        "Winogrande":75.69,
        "GSM8K":11.3,
        "DROP":5.8,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"b738c64d536df02f5c137a94bc7a32a4c486012b",
        "model_name_for_query":"yeontaek\/Platypus2-13B-IA3"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":47.27,
        "ARC":60.07,
        "HellaSwag":82.01,
        "MMLU":54.8,
        "TruthfulQA":42.7,
        "Winogrande":71.9,
        "GSM8K":12.36,
        "DROP":7.08,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":16.0,
        "Available on the hub":true,
        "Model sha":"209316bea6eab73d8b18fca2a730b1dff3dcf999",
        "model_name_for_query":"WizardLM\/WizardMath-13B-V1.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.27,
        "ARC":58.19,
        "HellaSwag":81.75,
        "MMLU":50.13,
        "TruthfulQA":48.93,
        "Winogrande":75.77,
        "GSM8K":8.64,
        "DROP":7.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":12.0,
        "Available on the hub":true,
        "Model sha":"67695d15e6610bc8055fbcde82f298e48ad2d374",
        "model_name_for_query":"Gryphe\/MythoBoros-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.23,
        "ARC":59.56,
        "HellaSwag":82.7,
        "MMLU":53.65,
        "TruthfulQA":43.26,
        "Winogrande":76.32,
        "GSM8K":9.1,
        "DROP":6.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":9.0,
        "Available on the hub":true,
        "Model sha":"cf315234979f5924ad73399bcdcdf51b05a1fc98",
        "model_name_for_query":"Fredithefish\/Guanaco-13B-Uncensored"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.23,
        "ARC":55.12,
        "HellaSwag":79.69,
        "MMLU":50.07,
        "TruthfulQA":52.56,
        "Winogrande":72.69,
        "GSM8K":6.37,
        "DROP":14.13,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":30.0,
        "Available on the hub":true,
        "Model sha":"1058709314f7ca090937d0a2b7b37b0b3a8f12a3",
        "model_name_for_query":"psmathur\/orca_mini_v2_13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.23,
        "ARC":58.45,
        "HellaSwag":81.56,
        "MMLU":49.36,
        "TruthfulQA":49.47,
        "Winogrande":75.61,
        "GSM8K":8.64,
        "DROP":7.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":17.0,
        "Available on the hub":true,
        "Model sha":"d89d925ad1eeaee465c4de3e5c74240a5a40b585",
        "model_name_for_query":"Gryphe\/MythoLogic-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.23,
        "ARC":58.36,
        "HellaSwag":82.33,
        "MMLU":56.14,
        "TruthfulQA":39.51,
        "Winogrande":76.4,
        "GSM8K":10.92,
        "DROP":6.92,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"d824054153586d58139b7c3527ba211f33a81382",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-FINETUNE4_compare15k_4.5w-r16-gate_up_down"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.19,
        "ARC":58.62,
        "HellaSwag":82.56,
        "MMLU":55.84,
        "TruthfulQA":42.09,
        "Winogrande":76.64,
        "GSM8K":7.05,
        "DROP":7.54,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"001a5f96daea57b5f256c2df270b35653b439f6f",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-Open_Platypus_and_ccp_2.6w-3_epoch"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":47.18,
        "ARC":59.9,
        "HellaSwag":84.11,
        "MMLU":54.67,
        "TruthfulQA":41.94,
        "Winogrande":74.03,
        "GSM8K":7.81,
        "DROP":7.77,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"f3d421aadb29830345bf392f793ce3c33e7d68c5",
        "model_name_for_query":"NobodyExistsOnTheInternet\/GiftedConvo13bLoraNoEconsE4"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.17,
        "ARC":59.56,
        "HellaSwag":82.09,
        "MMLU":47.48,
        "TruthfulQA":48.96,
        "Winogrande":76.72,
        "GSM8K":9.1,
        "DROP":6.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"49678a2dd15fb4e1f1b99616ccc1ffd269912833",
        "model_name_for_query":"TheBloke\/gpt4-alpaca-lora-13B-HF"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.16,
        "ARC":57.17,
        "HellaSwag":81.14,
        "MMLU":50.58,
        "TruthfulQA":49.54,
        "Winogrande":76.24,
        "GSM8K":9.1,
        "DROP":6.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":12.0,
        "Available on the hub":true,
        "Model sha":"bd2a0968964c0f2dfae8f5a8950b43e35142f830",
        "model_name_for_query":"openchat\/openchat_v2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.16,
        "ARC":57.34,
        "HellaSwag":81.23,
        "MMLU":50.17,
        "TruthfulQA":50.7,
        "Winogrande":75.93,
        "GSM8K":8.42,
        "DROP":6.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":29.0,
        "Available on the hub":true,
        "Model sha":"0eb53946b8fac30606dc72541f2fc073cb6a0e12",
        "model_name_for_query":"openchat\/openchat_v2_w"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.15,
        "ARC":58.11,
        "HellaSwag":81.52,
        "MMLU":48.65,
        "TruthfulQA":35.99,
        "Winogrande":77.51,
        "GSM8K":11.3,
        "DROP":16.97,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"d2cd599cc40db3370009f45d6caa7e486cb6d31f",
        "model_name_for_query":"dvruette\/oasst-llama-13b-1000-steps"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.14,
        "ARC":59.9,
        "HellaSwag":84.39,
        "MMLU":53.68,
        "TruthfulQA":39.9,
        "Winogrande":75.22,
        "GSM8K":8.72,
        "DROP":8.21,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"7da6d235d625e16c850ccd0b947dee40071b1f89",
        "model_name_for_query":"NobodyExistsOnTheInternet\/PuffedLIMA13bQLORA"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.14,
        "ARC":59.22,
        "HellaSwag":81.02,
        "MMLU":53.73,
        "TruthfulQA":39.7,
        "Winogrande":73.64,
        "GSM8K":8.64,
        "DROP":14.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":26.0,
        "Available on the hub":true,
        "Model sha":"a852b77f7d0777092c76898bc83f8e657ca2af3e",
        "model_name_for_query":"jondurbin\/airoboros-l2-13b-gpt4-m2.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.14,
        "ARC":59.81,
        "HellaSwag":84.39,
        "MMLU":53.62,
        "TruthfulQA":39.87,
        "Winogrande":75.22,
        "GSM8K":8.79,
        "DROP":8.26,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"40e4fce0c25bd23f6011b424748ee2b5374b98d5",
        "model_name_for_query":"NobodyExistsOnTheInternet\/PuffedConvo13bLoraE4"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.12,
        "ARC":58.53,
        "HellaSwag":81.1,
        "MMLU":55.15,
        "TruthfulQA":46.18,
        "Winogrande":71.03,
        "GSM8K":11.14,
        "DROP":6.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"5f14e6f5ea67fd2840791c46b3e00846cbdb32cf",
        "model_name_for_query":"Lajonbot\/WizardLM-13B-V1.2-PL-lora_unload"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":47.12,
        "ARC":53.84,
        "HellaSwag":80.67,
        "MMLU":54.44,
        "TruthfulQA":46.23,
        "Winogrande":76.01,
        "GSM8K":12.36,
        "DROP":6.26,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"828aa1020fc7d394fe8ee2c596e3211df7656eac",
        "model_name_for_query":"lgaalves\/llama-2-13b-chat-platypus"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.11,
        "ARC":49.57,
        "HellaSwag":72.62,
        "MMLU":46.5,
        "TruthfulQA":48.63,
        "Winogrande":70.01,
        "GSM8K":5.76,
        "DROP":36.66,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.7,
        "Hub \u2764\ufe0f":93.0,
        "Available on the hub":true,
        "Model sha":"ab2476bffedeed752daedd77e71900578e136e7c",
        "model_name_for_query":"ziqingyang\/chinese-alpaca-2-7b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":47.05,
        "ARC":55.38,
        "HellaSwag":79.14,
        "MMLU":48.46,
        "TruthfulQA":42.43,
        "Winogrande":73.48,
        "GSM8K":7.43,
        "DROP":23.03,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"6cfd95e2dcdb6996afa9eb5c63273a1a3524c6c6",
        "model_name_for_query":"heegyu\/WizardVicuna2-13b-hf"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":47.04,
        "ARC":59.22,
        "HellaSwag":81.03,
        "MMLU":55.73,
        "TruthfulQA":41.15,
        "Winogrande":76.4,
        "GSM8K":4.4,
        "DROP":11.35,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":19.36,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"d2c8cc15c57da217ff29ebaaae4bc4f57d6b21b0",
        "model_name_for_query":"The-Face-Of-Goonery\/Huginn-19b-prototype"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.04,
        "ARC":59.39,
        "HellaSwag":83.29,
        "MMLU":47.89,
        "TruthfulQA":47.65,
        "Winogrande":75.77,
        "GSM8K":7.88,
        "DROP":7.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":17.0,
        "Available on the hub":true,
        "Model sha":"c0eef6e6f63d4b11953539308717cea0079b44f9",
        "model_name_for_query":"jondurbin\/airoboros-13b-gpt4"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":47.03,
        "ARC":57.51,
        "HellaSwag":82.55,
        "MMLU":57.34,
        "TruthfulQA":43.38,
        "Winogrande":76.64,
        "GSM8K":5.0,
        "DROP":6.79,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"e62a8fafce0d64ac03d465a4e915bc1f50776a08",
        "model_name_for_query":"yeontaek\/Platypus2-13B-QLoRa"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.0,
        "ARC":52.73,
        "HellaSwag":80.13,
        "MMLU":51.94,
        "TruthfulQA":52.08,
        "Winogrande":74.19,
        "GSM8K":8.64,
        "DROP":9.31,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":96.0,
        "Available on the hub":true,
        "Model sha":"8c71dbe9221e83d2ec72e4dc08beccfc78b563c0",
        "model_name_for_query":"TheBloke\/vicuna-13B-1.1-HF"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.0,
        "ARC":52.73,
        "HellaSwag":80.13,
        "MMLU":51.94,
        "TruthfulQA":52.08,
        "Winogrande":74.19,
        "GSM8K":8.64,
        "DROP":9.31,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":134.0,
        "Available on the hub":true,
        "Model sha":"bfcc6ca66694310be6c85ba0638597f4256c4143",
        "model_name_for_query":"eachadea\/vicuna-13b-1.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.0,
        "ARC":52.73,
        "HellaSwag":80.13,
        "MMLU":51.94,
        "TruthfulQA":52.08,
        "Winogrande":74.19,
        "GSM8K":8.64,
        "DROP":9.31,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"83fa0860990df1db35550f973ba4306449e35412",
        "model_name_for_query":"pillowtalks-ai\/delta13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.0,
        "ARC":52.73,
        "HellaSwag":80.13,
        "MMLU":51.94,
        "TruthfulQA":52.08,
        "Winogrande":74.19,
        "GSM8K":8.64,
        "DROP":9.31,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"346e3c46959cf9f1e03feffa761afe020c0fb6a8",
        "model_name_for_query":"kevinpro\/Vicuna-13B-CoT"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.0,
        "ARC":52.73,
        "HellaSwag":80.14,
        "MMLU":51.9,
        "TruthfulQA":52.08,
        "Winogrande":74.19,
        "GSM8K":8.64,
        "DROP":9.31,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":402.0,
        "Available on the hub":true,
        "Model sha":"ffed4c7cf1b9814812078efbe29ec3f610ea39e7",
        "model_name_for_query":"lmsys\/vicuna-13b-delta-v1.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.0,
        "ARC":52.73,
        "HellaSwag":80.14,
        "MMLU":51.9,
        "TruthfulQA":52.08,
        "Winogrande":74.19,
        "GSM8K":8.64,
        "DROP":9.31,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"fe74a0ece9089828b301bd0f067ae5f257516179",
        "model_name_for_query":"TheBloke\/Vicuna-13B-CoT-fp16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":47.0,
        "ARC":52.73,
        "HellaSwag":80.14,
        "MMLU":51.9,
        "TruthfulQA":52.08,
        "Winogrande":74.19,
        "GSM8K":8.64,
        "DROP":9.31,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":96.0,
        "Available on the hub":true,
        "Model sha":"8c71dbe9221e83d2ec72e4dc08beccfc78b563c0",
        "model_name_for_query":"lmsys\/vicuna-13b-v1.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.98,
        "ARC":59.22,
        "HellaSwag":82.35,
        "MMLU":55.85,
        "TruthfulQA":39.55,
        "Winogrande":76.72,
        "GSM8K":8.79,
        "DROP":6.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"27868769e2d6b1af46337f0997c71b0577952a3d",
        "model_name_for_query":"TheBloke\/Kimiko-13B-fp16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.97,
        "ARC":57.51,
        "HellaSwag":82.14,
        "MMLU":54.56,
        "TruthfulQA":42.21,
        "Winogrande":76.56,
        "GSM8K":9.55,
        "DROP":6.26,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"83a8e51d0a72dcfbe5de13dc7ee10dc20e91602e",
        "model_name_for_query":"NekoPunchBBB\/Llama-2-13b-hf_Open-Platypus-8bit-att"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.97,
        "ARC":54.86,
        "HellaSwag":80.41,
        "MMLU":52.2,
        "TruthfulQA":49.62,
        "Winogrande":76.09,
        "GSM8K":9.02,
        "DROP":6.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"5582369752583b02df3cba4bd2a733d12265cddb",
        "model_name_for_query":"Lajonbot\/vicuna-13b-v1.3-PL-lora_unload"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.96,
        "ARC":53.75,
        "HellaSwag":79.47,
        "MMLU":51.5,
        "TruthfulQA":49.54,
        "Winogrande":72.61,
        "GSM8K":8.42,
        "DROP":13.43,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"388bc2f82a1ee8b963c7f94f9c7b6743f7214306",
        "model_name_for_query":"wahaha1987\/llama_13b_sharegpt94k_fastchat"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.95,
        "ARC":56.31,
        "HellaSwag":79.51,
        "MMLU":45.71,
        "TruthfulQA":40.98,
        "Winogrande":72.06,
        "GSM8K":2.58,
        "DROP":31.53,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.67,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"03d23910fa0f9b0542ce7634cbcd36983321f55a",
        "model_name_for_query":"jb723\/llama2-ko-7B-model"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":46.93,
        "ARC":58.96,
        "HellaSwag":81.94,
        "MMLU":55.0,
        "TruthfulQA":40.26,
        "Winogrande":76.56,
        "GSM8K":8.72,
        "DROP":7.05,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"30edbe648df2661dd779cd19ef613e6914dcc8e0",
        "model_name_for_query":"dhmeltzer\/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16_merged"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":46.91,
        "ARC":59.3,
        "HellaSwag":81.45,
        "MMLU":55.82,
        "TruthfulQA":38.23,
        "Winogrande":76.64,
        "GSM8K":10.69,
        "DROP":6.28,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":true,
        "Model sha":"428508a0cf288c0f5b7891c9b2f758ddf4d62c26",
        "model_name_for_query":"BramVanroy\/Llama-2-13b-chat-dutch"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":46.89,
        "ARC":59.39,
        "HellaSwag":82.13,
        "MMLU":55.77,
        "TruthfulQA":37.38,
        "Winogrande":76.64,
        "GSM8K":10.84,
        "DROP":6.08,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":389.0,
        "Available on the hub":false,
        "Model sha":"7da18fb10421c3ae2a1eb92815bad75e84816e35",
        "model_name_for_query":"meta-llama\/Llama-2-13b-hf"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.87,
        "ARC":59.3,
        "HellaSwag":82.15,
        "MMLU":55.67,
        "TruthfulQA":37.39,
        "Winogrande":76.64,
        "GSM8K":10.84,
        "DROP":6.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"cb9fced568b1abd881133c642c427aaa488f00cc",
        "model_name_for_query":"NewstaR\/Starlight-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.87,
        "ARC":59.3,
        "HellaSwag":82.15,
        "MMLU":55.67,
        "TruthfulQA":37.39,
        "Winogrande":76.64,
        "GSM8K":10.84,
        "DROP":6.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"81b40096471a8980e3e1a8998f358bd363033783",
        "model_name_for_query":"TaylorAI\/Flash-Llama-13B"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":46.87,
        "ARC":59.3,
        "HellaSwag":82.15,
        "MMLU":55.67,
        "TruthfulQA":37.39,
        "Winogrande":76.64,
        "GSM8K":10.84,
        "DROP":6.08,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":51.0,
        "Available on the hub":true,
        "Model sha":"b2e65e8ad4bb35e5abaee0170ebd5fc2134a50bb",
        "model_name_for_query":"TheBloke\/Llama-2-13B-fp16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.86,
        "ARC":58.28,
        "HellaSwag":82.69,
        "MMLU":54.53,
        "TruthfulQA":39.23,
        "Winogrande":75.93,
        "GSM8K":11.22,
        "DROP":6.17,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":21.62,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"7adbaa5b8e122bb93bf510d8655ec4132d7b4a8a",
        "model_name_for_query":"chargoddard\/llama2-22b-blocktriangular"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":46.85,
        "ARC":58.96,
        "HellaSwag":82.31,
        "MMLU":54.73,
        "TruthfulQA":40.25,
        "Winogrande":75.61,
        "GSM8K":9.86,
        "DROP":6.22,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"0636c1f582c979a5a292cc5f3dc293800b1494e2",
        "model_name_for_query":"llm-agents\/tora-13b-v1.0"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":46.85,
        "ARC":58.53,
        "HellaSwag":82.55,
        "MMLU":54.68,
        "TruthfulQA":39.84,
        "Winogrande":76.32,
        "GSM8K":9.93,
        "DROP":6.08,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":21.62,
        "Hub \u2764\ufe0f":35.0,
        "Available on the hub":true,
        "Model sha":"2bece0787009b4b584f49d0e0d1b49ecf4a52da9",
        "model_name_for_query":"chargoddard\/llama2-22b"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":46.84,
        "ARC":58.79,
        "HellaSwag":82.08,
        "MMLU":55.6,
        "TruthfulQA":37.82,
        "Winogrande":76.48,
        "GSM8K":11.3,
        "DROP":5.84,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"openrail",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":false,
        "Model sha":"c29b67965ea55da3e2ac678eef7ffdf36f8ef5ab",
        "model_name_for_query":"shareAI\/bimoGPT-llama2-13b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":46.84,
        "ARC":59.39,
        "HellaSwag":83.19,
        "MMLU":55.15,
        "TruthfulQA":40.56,
        "Winogrande":74.03,
        "GSM8K":7.81,
        "DROP":7.77,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"9d7031e7d956dd2d25c61d85f594d115ce65b172",
        "model_name_for_query":"NobodyExistsOnTheInternet\/GiftedConvo13bLoraNoEcons"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.84,
        "ARC":58.28,
        "HellaSwag":82.32,
        "MMLU":54.67,
        "TruthfulQA":48.66,
        "Winogrande":73.48,
        "GSM8K":1.29,
        "DROP":9.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"feb1fa71e0b24261d3ca428b4aed881dd31f166e",
        "model_name_for_query":"Undi95\/MLewd-L2-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.83,
        "ARC":56.23,
        "HellaSwag":80.39,
        "MMLU":53.62,
        "TruthfulQA":45.76,
        "Winogrande":70.24,
        "GSM8K":11.14,
        "DROP":10.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":21.83,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"90cffebc8f530161505b84740ff6c8f646299d6c",
        "model_name_for_query":"nkpz\/llama2-22b-chat-wizard-uncensored"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":46.83,
        "ARC":52.13,
        "HellaSwag":74.78,
        "MMLU":49.15,
        "TruthfulQA":48.85,
        "Winogrande":68.35,
        "GSM8K":9.48,
        "DROP":25.06,
        "Type":"instruction-tuned",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":616.0,
        "Available on the hub":true,
        "Model sha":"5cdc34e4a81d202f1d4a3b5d60e028aab895dfeb",
        "model_name_for_query":"WizardLM\/WizardCoder-Python-34B-V1.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.81,
        "ARC":59.3,
        "HellaSwag":82.04,
        "MMLU":54.67,
        "TruthfulQA":38.03,
        "Winogrande":77.27,
        "GSM8K":10.31,
        "DROP":6.08,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"b23fe7d174653b87dc08507d9b83504a8dddbc45",
        "model_name_for_query":"BramVanroy\/llama2-13b-ft-mc4_nl_cleaned_tiny"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.8,
        "ARC":57.85,
        "HellaSwag":83.84,
        "MMLU":48.28,
        "TruthfulQA":46.73,
        "Winogrande":75.85,
        "GSM8K":8.72,
        "DROP":6.36,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"bd59c700815124df616a17f5b49a0bc51590b231",
        "model_name_for_query":"TheBloke\/guanaco-13B-HF"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.8,
        "ARC":58.36,
        "HellaSwag":81.69,
        "MMLU":47.89,
        "TruthfulQA":45.42,
        "Winogrande":76.95,
        "GSM8K":7.51,
        "DROP":9.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"3a82b04684fe99d59556421c3f96a187049a3cec",
        "model_name_for_query":"xzuyn\/Alpacino-SuperCOT-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.79,
        "ARC":52.47,
        "HellaSwag":78.35,
        "MMLU":39.51,
        "TruthfulQA":44.52,
        "Winogrande":73.16,
        "GSM8K":7.58,
        "DROP":31.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"openrail",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"9af636df9c8693ea857b62442bd1c6c73d657dc6",
        "model_name_for_query":"bofenghuang\/vigogne-7b-chat"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":46.78,
        "ARC":57.85,
        "HellaSwag":82.63,
        "MMLU":55.25,
        "TruthfulQA":39.33,
        "Winogrande":76.32,
        "GSM8K":10.39,
        "DROP":5.67,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"6c3a002f6c9e8a481a7375d91856d603bf6dd040",
        "model_name_for_query":"KnutJaegersberg\/deacon-13b"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":46.78,
        "ARC":52.82,
        "HellaSwag":79.59,
        "MMLU":48.19,
        "TruthfulQA":48.88,
        "Winogrande":70.17,
        "GSM8K":2.81,
        "DROP":24.99,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":456.0,
        "Available on the hub":true,
        "Model sha":"6a571f458cab9a23d14324ec63e0abd1744c8353",
        "model_name_for_query":"chavinlo\/gpt4-x-alpaca"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.76,
        "ARC":58.96,
        "HellaSwag":82.51,
        "MMLU":56.12,
        "TruthfulQA":40.07,
        "Winogrande":76.64,
        "GSM8K":6.82,
        "DROP":6.23,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"2929bfa1049db46df94f5710755178d18a981665",
        "model_name_for_query":"CHIH-HUNG\/llama-2-13b-Open_Platypus_and_ccp_2.6w"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.76,
        "ARC":58.53,
        "HellaSwag":82.59,
        "MMLU":54.64,
        "TruthfulQA":39.3,
        "Winogrande":76.32,
        "GSM8K":9.78,
        "DROP":6.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":21.62,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"40a51343ae776b5cb39f2b4343ae8f9b676ffd58",
        "model_name_for_query":"chargoddard\/llama2-22b-blocktriangular"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.74,
        "ARC":60.49,
        "HellaSwag":82.97,
        "MMLU":54.44,
        "TruthfulQA":37.34,
        "Winogrande":75.69,
        "GSM8K":10.08,
        "DROP":6.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"f364d000bedac80e72aa103c08b77aee1b61b7da",
        "model_name_for_query":"IGeniusDev\/llama13B-quant8-testv1-openorca-customdataset"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":46.73,
        "ARC":55.8,
        "HellaSwag":79.74,
        "MMLU":54.17,
        "TruthfulQA":46.71,
        "Winogrande":74.19,
        "GSM8K":9.86,
        "DROP":6.65,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"9058130b416355b37f5f78777748aa56d98a4da0",
        "model_name_for_query":"quantumaikr\/QuantumLM"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":46.72,
        "ARC":56.4,
        "HellaSwag":75.45,
        "MMLU":54.51,
        "TruthfulQA":43.06,
        "Winogrande":72.45,
        "GSM8K":19.64,
        "DROP":5.51,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":20.0,
        "Available on the hub":true,
        "Model sha":"3e8df2cf4a4ee1c0b2d079cb7be70024d425ea8c",
        "model_name_for_query":"ehartford\/WizardLM-1.0-Uncensored-CodeLlama-34b"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":46.72,
        "ARC":54.35,
        "HellaSwag":79.47,
        "MMLU":51.97,
        "TruthfulQA":50.88,
        "Winogrande":74.66,
        "GSM8K":8.42,
        "DROP":7.26,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":16.22,
        "Hub \u2764\ufe0f":19.0,
        "Available on the hub":true,
        "Model sha":"6ef1f8d8638ea2d6681a8e3da73be57c501d847b",
        "model_name_for_query":"TheBloke\/vicuna-13b-v1.3.0-GPTQ"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.72,
        "ARC":50.68,
        "HellaSwag":75.36,
        "MMLU":49.33,
        "TruthfulQA":44.7,
        "Winogrande":72.38,
        "GSM8K":17.36,
        "DROP":17.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":53.9,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"01305dc473ba231519fe71e7f4b2d1e3f6aa9bc8",
        "model_name_for_query":"elliotthwang\/Elliott-Chinese-LLaMa-GPTQ-V1.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.66,
        "ARC":54.69,
        "HellaSwag":79.18,
        "MMLU":48.88,
        "TruthfulQA":49.62,
        "Winogrande":74.82,
        "GSM8K":9.33,
        "DROP":10.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":48.0,
        "Available on the hub":true,
        "Model sha":"12dc8aacb474522ae2a83c18cb0fdf0907987f8f",
        "model_name_for_query":"TheBloke\/wizard-vicuna-13B-HF"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.64,
        "ARC":54.69,
        "HellaSwag":79.18,
        "MMLU":48.88,
        "TruthfulQA":49.62,
        "Winogrande":74.82,
        "GSM8K":9.17,
        "DROP":10.13,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":70.0,
        "Available on the hub":true,
        "Model sha":"419dc5acc391de54a60d0b041e94e767d1ef2032",
        "model_name_for_query":"junelee\/wizard-vicuna-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.63,
        "ARC":53.16,
        "HellaSwag":73.51,
        "MMLU":48.81,
        "TruthfulQA":45.32,
        "Winogrande":75.06,
        "GSM8K":2.12,
        "DROP":28.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.94,
        "Hub \u2764\ufe0f":32.0,
        "Available on the hub":true,
        "Model sha":"a118d2c35573b9a70c6f5b56fba4b657f74ce00c",
        "model_name_for_query":"shibing624\/chinese-alpaca-plus-13b-hf"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":46.6,
        "ARC":57.51,
        "HellaSwag":79.44,
        "MMLU":49.35,
        "TruthfulQA":49.84,
        "Winogrande":74.51,
        "GSM8K":7.51,
        "DROP":8.06,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":39.0,
        "Available on the hub":true,
        "Model sha":"1370c7c595e6c8394e6332bc535ae25e21def85b",
        "model_name_for_query":"Open-Orca\/LlongOrca-7B-16k"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.57,
        "ARC":52.9,
        "HellaSwag":78.53,
        "MMLU":45.09,
        "TruthfulQA":39.45,
        "Winogrande":71.11,
        "GSM8K":3.18,
        "DROP":35.69,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"8432fe95c426ca7709cf2d31a64eee612c4dea42",
        "model_name_for_query":"jondurbin\/airoboros-l2-7b-gpt4-2.0"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":46.52,
        "ARC":57.68,
        "HellaSwag":80.69,
        "MMLU":49.81,
        "TruthfulQA":52.11,
        "Winogrande":71.59,
        "GSM8K":2.27,
        "DROP":11.5,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":21.83,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"29222b05794abb862ad0aaaf3020696c9f599810",
        "model_name_for_query":"The-Face-Of-Goonery\/Huginn-22b-Prototype"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.51,
        "ARC":59.13,
        "HellaSwag":81.48,
        "MMLU":54.45,
        "TruthfulQA":37.07,
        "Winogrande":76.16,
        "GSM8K":11.3,
        "DROP":6.01,
        "Type":"fine-tuned",
        "Precision":"GPTQ",
        "Hub License":"llama2",
        "#Params (B)":16.23,
        "Hub \u2764\ufe0f":99.0,
        "Available on the hub":true,
        "Model sha":"b7db471d1789802a3a8e3b93cdd66a9f046f17c3",
        "model_name_for_query":"TheBloke\/Llama-2-13B-GPTQ"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.5,
        "ARC":56.14,
        "HellaSwag":78.6,
        "MMLU":50.35,
        "TruthfulQA":45.03,
        "Winogrande":74.27,
        "GSM8K":6.6,
        "DROP":14.51,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"4f9e95665d95b4c692910190ff77257216e476f1",
        "model_name_for_query":"migtissera\/Synthia-7B"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":46.49,
        "ARC":55.38,
        "HellaSwag":78.93,
        "MMLU":50.6,
        "TruthfulQA":50.12,
        "Winogrande":73.95,
        "GSM8K":8.19,
        "DROP":8.3,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"a6943d2d30d0af904b3321559157d589e60f9e0f",
        "model_name_for_query":"Yhyu13\/chimera-inst-chat-13b-hf"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":46.49,
        "ARC":57.85,
        "HellaSwag":81.07,
        "MMLU":47.56,
        "TruthfulQA":47.77,
        "Winogrande":75.93,
        "GSM8K":8.49,
        "DROP":6.8,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":16.22,
        "Hub \u2764\ufe0f":33.0,
        "Available on the hub":true,
        "Model sha":"923f27245d13058c9c1b3ab0eab6c6c93ffc162e",
        "model_name_for_query":"TheBloke\/manticore-13b-chat-pyg-GPTQ"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":46.49,
        "ARC":54.78,
        "HellaSwag":78.98,
        "MMLU":51.29,
        "TruthfulQA":49.17,
        "Winogrande":74.74,
        "GSM8K":7.28,
        "DROP":9.16,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"443cb81ce988ea6c0b1e20132c170463d559367e",
        "model_name_for_query":"Yehoon\/yehoon_llama2"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":46.47,
        "ARC":59.04,
        "HellaSwag":82.33,
        "MMLU":55.36,
        "TruthfulQA":35.75,
        "Winogrande":76.32,
        "GSM8K":10.01,
        "DROP":6.48,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"a3ed7416156963f49bf4dc056188e006c0c214d2",
        "model_name_for_query":"dhmeltzer\/Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.47,
        "ARC":51.19,
        "HellaSwag":75.99,
        "MMLU":49.33,
        "TruthfulQA":48.66,
        "Winogrande":73.32,
        "GSM8K":15.39,
        "DROP":11.4,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.94,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"35bb2c73953f6ea40be6f0c8c6b2dfa7ecbaa0df",
        "model_name_for_query":"OpenBuddy\/openbuddy-atom-13b-v9-bf16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.46,
        "ARC":59.13,
        "HellaSwag":82.78,
        "MMLU":55.62,
        "TruthfulQA":40.27,
        "Winogrande":73.32,
        "GSM8K":6.97,
        "DROP":7.13,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":12.0,
        "Available on the hub":true,
        "Model sha":"35ff51ebe5668269dfd33a9ed94412d88f1f4b65",
        "model_name_for_query":"jondurbin\/airoboros-l2-13b-gpt4-1.4.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.46,
        "ARC":53.16,
        "HellaSwag":74.64,
        "MMLU":49.89,
        "TruthfulQA":45.74,
        "Winogrande":72.3,
        "GSM8K":7.43,
        "DROP":22.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"768d800e4dbe3fc95334f30ca7cd02113d3e3fd3",
        "model_name_for_query":"Aspik101\/trurl-2-7b-pl-instruct_unload"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.36,
        "ARC":68.17,
        "HellaSwag":85.88,
        "MMLU":64.83,
        "TruthfulQA":55.81,
        "Winogrande":49.8,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":65.29,
        "Hub \u2764\ufe0f":54.0,
        "Available on the hub":false,
        "Model sha":"40a78d91d43ad9aef6663ff15ddc15be9922bce5",
        "model_name_for_query":"stabilityai\/StableBeluga1-Delta"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.34,
        "ARC":52.99,
        "HellaSwag":75.64,
        "MMLU":50.74,
        "TruthfulQA":48.94,
        "Winogrande":72.77,
        "GSM8K":14.48,
        "DROP":8.83,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"openrail",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":261.0,
        "Available on the hub":true,
        "Model sha":"72efd71d7f89d9c46008b7a574faf90300ed9ba8",
        "model_name_for_query":"LinkSoul\/Chinese-Llama-2-7b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":46.33,
        "ARC":58.45,
        "HellaSwag":81.97,
        "MMLU":55.02,
        "TruthfulQA":35.85,
        "Winogrande":75.69,
        "GSM8K":10.69,
        "DROP":6.63,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"5a89844b1aea3f0573e696143ec66727df4b5d79",
        "model_name_for_query":"dhmeltzer\/Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16_merged"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.32,
        "ARC":53.58,
        "HellaSwag":77.67,
        "MMLU":45.24,
        "TruthfulQA":47.07,
        "Winogrande":70.09,
        "GSM8K":4.17,
        "DROP":26.42,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":123.0,
        "Available on the hub":true,
        "Model sha":"70e2e38b82f1e25d8b90b50fbfc2361123bef45f",
        "model_name_for_query":"lmsys\/longchat-13b-16k"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.31,
        "ARC":56.14,
        "HellaSwag":79.13,
        "MMLU":60.04,
        "TruthfulQA":40.95,
        "Winogrande":74.43,
        "GSM8K":7.88,
        "DROP":5.59,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"553178f8d5d69eb1dfa5b9503d2ce0c1e481e5b1",
        "model_name_for_query":"itsliupeng\/llama2_7b_mmlu"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.3,
        "ARC":56.91,
        "HellaSwag":79.29,
        "MMLU":49.72,
        "TruthfulQA":47.88,
        "Winogrande":74.9,
        "GSM8K":8.95,
        "DROP":6.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":22.0,
        "Available on the hub":true,
        "Model sha":"a3c4bbccca8b650700a49a225582c17bb49b446b",
        "model_name_for_query":"project-baize\/baize-v2-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.3,
        "ARC":50.77,
        "HellaSwag":75.36,
        "MMLU":49.41,
        "TruthfulQA":44.7,
        "Winogrande":72.61,
        "GSM8K":16.0,
        "DROP":15.25,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":52.86,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"ebffe57ba6cc70b60ff5295889abc62d91eeb4dd",
        "model_name_for_query":"elliotthwang\/Elliott-Chinese-LLaMa-GPTQ-V2.0"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":46.27,
        "ARC":59.39,
        "HellaSwag":81.51,
        "MMLU":54.31,
        "TruthfulQA":37.81,
        "Winogrande":75.77,
        "GSM8K":8.57,
        "DROP":6.55,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":true,
        "Model sha":"772b53f64f484fa0d651d453bcefc35a0f52f251",
        "model_name_for_query":"clibrain\/Llama-2-13b-ft-instruct-es"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":46.17,
        "ARC":54.1,
        "HellaSwag":77.91,
        "MMLU":54.49,
        "TruthfulQA":49.36,
        "Winogrande":70.17,
        "GSM8K":9.93,
        "DROP":7.24,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":59.0,
        "Available on the hub":true,
        "Model sha":"0a5752d096ebab21759dbe203f6b7c7f6092faf2",
        "model_name_for_query":"teknium\/Mistral-Trismegistus-7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.08,
        "ARC":58.7,
        "HellaSwag":80.88,
        "MMLU":49.69,
        "TruthfulQA":47.37,
        "Winogrande":73.01,
        "GSM8K":6.82,
        "DROP":6.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":31.0,
        "Available on the hub":true,
        "Model sha":"943f932ae1ae462389e6d2db5273158530749fff",
        "model_name_for_query":"totally-not-an-llm\/EverythingLM-13b-V2-16k"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.07,
        "ARC":55.63,
        "HellaSwag":79.25,
        "MMLU":49.74,
        "TruthfulQA":47.42,
        "Winogrande":75.45,
        "GSM8K":7.13,
        "DROP":7.86,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":11.0,
        "Available on the hub":true,
        "Model sha":"6d98f2801f13d89de7978ee9f348a52ea46a24ec",
        "model_name_for_query":"camel-ai\/CAMEL-13B-Combined-Data"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":46.07,
        "ARC":59.56,
        "HellaSwag":81.44,
        "MMLU":46.26,
        "TruthfulQA":46.7,
        "Winogrande":74.98,
        "GSM8K":7.35,
        "DROP":6.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":220.0,
        "Available on the hub":true,
        "Model sha":"f661da5af278fbda8a43b19ff0250e4efc103e3e",
        "model_name_for_query":"openchat\/openchat_8192"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":46.0,
        "ARC":58.45,
        "HellaSwag":82.3,
        "MMLU":47.58,
        "TruthfulQA":41.12,
        "Winogrande":77.51,
        "GSM8K":9.33,
        "DROP":5.74,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"3b37c31e04419adcc91eddb57f24fd6f9ac91938",
        "model_name_for_query":"PocketDoc\/Dans-PersonalityEngine-13b"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":45.98,
        "ARC":55.89,
        "HellaSwag":79.75,
        "MMLU":44.99,
        "TruthfulQA":54.72,
        "Winogrande":72.69,
        "GSM8K":7.28,
        "DROP":6.58,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"24f58beb9ed4cf635fc962853ed71d0f4b1909ba",
        "model_name_for_query":"frank098\/Wizard-Vicuna-13B-juniper"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.97,
        "ARC":57.25,
        "HellaSwag":81.94,
        "MMLU":53.65,
        "TruthfulQA":38.03,
        "Winogrande":76.09,
        "GSM8K":8.95,
        "DROP":5.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":16.0,
        "Available on the hub":true,
        "Model sha":"a947965cb07ca12a38ff981fe65b618d7dea28d3",
        "model_name_for_query":"LeoLM\/leo-hessianai-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.95,
        "ARC":51.62,
        "HellaSwag":77.55,
        "MMLU":48.49,
        "TruthfulQA":43.88,
        "Winogrande":73.16,
        "GSM8K":11.75,
        "DROP":15.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"679d17809939a0bf9b79bbb027898cbea64045b2",
        "model_name_for_query":"abhishek\/llama2guanacotest"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.95,
        "ARC":49.49,
        "HellaSwag":76.48,
        "MMLU":47.74,
        "TruthfulQA":41.58,
        "Winogrande":72.45,
        "GSM8K":28.51,
        "DROP":5.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":true,
        "Model sha":"0b448f6f64808f8bca94dc871e96a3eae7e95621",
        "model_name_for_query":"meta-math\/MetaMath-13B-V1.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.94,
        "ARC":57.59,
        "HellaSwag":80.53,
        "MMLU":48.0,
        "TruthfulQA":44.54,
        "Winogrande":76.64,
        "GSM8K":7.43,
        "DROP":6.87,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"379cb8f080110f3418155029f534f67a79e25db4",
        "model_name_for_query":"lizhuang144\/llama_mirror_13b_v1.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.94,
        "ARC":56.57,
        "HellaSwag":79.4,
        "MMLU":49.98,
        "TruthfulQA":47.89,
        "Winogrande":73.32,
        "GSM8K":5.31,
        "DROP":9.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":73.0,
        "Available on the hub":true,
        "Model sha":"470e680120a7249d6e8a875345015ddba1711100",
        "model_name_for_query":"Xwin-LM\/Xwin-LM-7B-V0.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.92,
        "ARC":57.42,
        "HellaSwag":81.68,
        "MMLU":48.72,
        "TruthfulQA":41.76,
        "Winogrande":77.19,
        "GSM8K":8.87,
        "DROP":5.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"98faa74a9b41cbd9033904cd58420705936849eb",
        "model_name_for_query":"heegyu\/LIMA-13b-hf"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.9,
        "ARC":53.24,
        "HellaSwag":77.39,
        "MMLU":51.04,
        "TruthfulQA":50.34,
        "Winogrande":72.14,
        "GSM8K":8.19,
        "DROP":8.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":70.0,
        "Available on the hub":true,
        "Model sha":"de56c35b1763eaae20f4d60efd64af0a9091ebe5",
        "model_name_for_query":"lmsys\/vicuna-7b-v1.5"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.9,
        "ARC":50.94,
        "HellaSwag":76.64,
        "MMLU":43.96,
        "TruthfulQA":46.73,
        "Winogrande":70.56,
        "GSM8K":2.05,
        "DROP":30.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":433.0,
        "Available on the hub":true,
        "Model sha":"9025c5f96fef9525da9238369ad082961b0e9494",
        "model_name_for_query":"ehartford\/WizardLM-13B-Uncensored"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.88,
        "ARC":55.55,
        "HellaSwag":81.26,
        "MMLU":48.3,
        "TruthfulQA":51.49,
        "Winogrande":72.85,
        "GSM8K":5.38,
        "DROP":6.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"1e74a9cca843cdeb8591d4e4f4320dc1870adf1b",
        "model_name_for_query":"beaugogh\/Llama2-7b-openorca-mc-v2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.88,
        "ARC":53.24,
        "HellaSwag":77.39,
        "MMLU":50.82,
        "TruthfulQA":50.33,
        "Winogrande":72.06,
        "GSM8K":8.11,
        "DROP":9.16,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":70.0,
        "Available on the hub":true,
        "Model sha":"de56c35b1763eaae20f4d60efd64af0a9091ebe5",
        "model_name_for_query":"lmsys\/vicuna-7b-v1.5"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.8,
        "ARC":56.74,
        "HellaSwag":81.63,
        "MMLU":48.69,
        "TruthfulQA":46.94,
        "Winogrande":74.27,
        "GSM8K":5.99,
        "DROP":6.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"afe00170f084f773e401ba7d738d692533cca6b4",
        "model_name_for_query":"mncai\/Llama2-7B-guanaco-dolphin-500"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.78,
        "ARC":58.53,
        "HellaSwag":81.31,
        "MMLU":47.92,
        "TruthfulQA":41.66,
        "Winogrande":76.95,
        "GSM8K":7.96,
        "DROP":6.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":29.0,
        "Available on the hub":true,
        "Model sha":"7092a5c8dec649694dd66ff8cfe5452ce52e6a40",
        "model_name_for_query":"digitous\/Alpacino13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.77,
        "ARC":59.47,
        "HellaSwag":82.61,
        "MMLU":52.13,
        "TruthfulQA":37.44,
        "Winogrande":75.53,
        "GSM8K":7.73,
        "DROP":5.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"6d17c854025b0bd54ce572ac803f1bb052875dbf",
        "model_name_for_query":"Yukang\/Llama-2-13b-longlora-32k-ft"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.76,
        "ARC":53.5,
        "HellaSwag":78.39,
        "MMLU":44.61,
        "TruthfulQA":41.32,
        "Winogrande":70.56,
        "GSM8K":4.93,
        "DROP":27.02,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":13.0,
        "Available on the hub":true,
        "Model sha":"0b3ef975fb5e8ac1eae775160ab54c98221889df",
        "model_name_for_query":"xzuyn\/MedicWizard-7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.76,
        "ARC":58.79,
        "HellaSwag":81.79,
        "MMLU":48.12,
        "TruthfulQA":41.24,
        "Winogrande":76.16,
        "GSM8K":8.49,
        "DROP":5.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"a7e5484df8aceae7800ae9301a3954cf74b527e9",
        "model_name_for_query":"PocketDoc\/Dans-PileOfSets-Mk1-llama-13b-merged"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.76,
        "ARC":57.76,
        "HellaSwag":79.63,
        "MMLU":52.51,
        "TruthfulQA":51.8,
        "Winogrande":68.98,
        "GSM8K":0.08,
        "DROP":9.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":20.63,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"6f9dcdaae6ef9071effe63d2107abe8b9712345b",
        "model_name_for_query":"Sao10K\/Stheno-Mix-L2-20B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.74,
        "ARC":57.25,
        "HellaSwag":78.62,
        "MMLU":50.57,
        "TruthfulQA":50.62,
        "Winogrande":76.32,
        "GSM8K":1.52,
        "DROP":5.31,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"79047f667253c878ad3143b016e3dcb3df707572",
        "model_name_for_query":"lvkaokao\/llama2-7b-hf-chat-lora-v3"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.74,
        "ARC":45.9,
        "HellaSwag":76.36,
        "MMLU":50.04,
        "TruthfulQA":40.32,
        "Winogrande":71.74,
        "GSM8K":0.0,
        "DROP":35.79,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"082cf758aa3f6d8f956056003b5b3b6cde447d88",
        "model_name_for_query":"luffycodes\/vicuna-shishya-7b-ep3-v1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.7,
        "ARC":56.57,
        "HellaSwag":80.58,
        "MMLU":50.18,
        "TruthfulQA":47.46,
        "Winogrande":72.77,
        "GSM8K":6.44,
        "DROP":5.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":29.0,
        "Available on the hub":true,
        "Model sha":"8456a856a8b115b05e76a7d0d945853b10ac71e2",
        "model_name_for_query":"totally-not-an-llm\/EverythingLM-13b-16k"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.7,
        "ARC":51.71,
        "HellaSwag":79.94,
        "MMLU":50.84,
        "TruthfulQA":52.68,
        "Winogrande":71.03,
        "GSM8K":7.58,
        "DROP":6.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":95.0,
        "Available on the hub":true,
        "Model sha":"ac4218770a58baaaaf25201076fe082abb6ffd13",
        "model_name_for_query":"eachadea\/vicuna-13b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":45.69,
        "ARC":55.2,
        "HellaSwag":78.84,
        "MMLU":49.83,
        "TruthfulQA":40.64,
        "Winogrande":73.48,
        "GSM8K":1.82,
        "DROP":20.02,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"f784afa7887b0738d92ea470797582756f02e630",
        "model_name_for_query":"garage-bAInd\/Platypus2-7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.68,
        "ARC":53.75,
        "HellaSwag":78.38,
        "MMLU":51.57,
        "TruthfulQA":45.76,
        "Winogrande":74.9,
        "GSM8K":7.88,
        "DROP":7.52,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"b90c207e248c0ad541274c2eb5ef76da1181802f",
        "model_name_for_query":"LTC-AI-Labs\/L2-7b-Beluga-WVG-Test"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":45.65,
        "ARC":54.78,
        "HellaSwag":81.48,
        "MMLU":47.2,
        "TruthfulQA":53.13,
        "Winogrande":72.85,
        "GSM8K":4.47,
        "DROP":5.64,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"734a6f0c69e1e53b988c107926bc17cb0536f851",
        "model_name_for_query":"beaugogh\/Llama2-7b-openorca-mc-v2-dpo"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.65,
        "ARC":55.12,
        "HellaSwag":80.53,
        "MMLU":47.93,
        "TruthfulQA":47.69,
        "Winogrande":74.82,
        "GSM8K":7.58,
        "DROP":5.87,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"5f3194b779897bbc4c4218a9dddc44a9b5faea15",
        "model_name_for_query":"mncai\/Llama2-7B-guanaco-1k"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.6,
        "ARC":53.67,
        "HellaSwag":77.5,
        "MMLU":45.61,
        "TruthfulQA":48.95,
        "Winogrande":70.96,
        "GSM8K":5.53,
        "DROP":16.98,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":197.0,
        "Available on the hub":true,
        "Model sha":"24fb8e1e9cc78e0aa7ef154b026c4a83296e3fc4",
        "model_name_for_query":"lmsys\/vicuna-7b-delta-v1.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.6,
        "ARC":53.67,
        "HellaSwag":77.46,
        "MMLU":45.63,
        "TruthfulQA":48.94,
        "Winogrande":70.96,
        "GSM8K":5.53,
        "DROP":16.98,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"d971d788db19648ad16bf77ec3f1de35ebf9a8e0",
        "model_name_for_query":"Ejafa\/vicuna_7B_vanilla_1.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.6,
        "ARC":53.67,
        "HellaSwag":77.46,
        "MMLU":45.63,
        "TruthfulQA":48.94,
        "Winogrande":70.96,
        "GSM8K":5.53,
        "DROP":16.98,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":109.0,
        "Available on the hub":true,
        "Model sha":"9d8eea215e00b388a22e8f050768ea8911d41f1d",
        "model_name_for_query":"eachadea\/vicuna-7b-1.1"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":45.59,
        "ARC":58.62,
        "HellaSwag":81.17,
        "MMLU":50.23,
        "TruthfulQA":43.43,
        "Winogrande":76.16,
        "GSM8K":0.08,
        "DROP":9.43,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"568ac6a5f1a9f5eb6bc09efb2188740d771ed0e9",
        "model_name_for_query":"yeontaek\/Platypus2xOpenOrca-13B-LoRa-v2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.56,
        "ARC":54.18,
        "HellaSwag":79.34,
        "MMLU":49.7,
        "TruthfulQA":46.5,
        "Winogrande":74.11,
        "GSM8K":8.49,
        "DROP":6.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"733016abcd2abee63eb45ed63d2bba14b91da217",
        "model_name_for_query":"l3utterfly\/llama2-7b-layla"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":45.55,
        "ARC":55.63,
        "HellaSwag":80.17,
        "MMLU":48.44,
        "TruthfulQA":51.62,
        "Winogrande":73.48,
        "GSM8K":4.09,
        "DROP":5.42,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"2c4096fa2129665fb127f1c2a1302f30565a5265",
        "model_name_for_query":"beaugogh\/Llama2-7b-openorca-mc-v1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.55,
        "ARC":58.28,
        "HellaSwag":80.98,
        "MMLU":54.14,
        "TruthfulQA":34.21,
        "Winogrande":75.93,
        "GSM8K":9.25,
        "DROP":6.07,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":false,
        "Model sha":"86329885e029c1f4fb6ff6b6f3409007158499e7",
        "model_name_for_query":"StudentLLM\/Alpagasus-2-13B-QLoRA-pipeline"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":45.54,
        "ARC":52.82,
        "HellaSwag":79.63,
        "MMLU":39.83,
        "TruthfulQA":52.55,
        "Winogrande":71.82,
        "GSM8K":0.15,
        "DROP":22.02,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":16.22,
        "Hub \u2764\ufe0f":15.0,
        "Available on the hub":true,
        "Model sha":"bd3c66e626c81de4977f197e1534bd3dfa2f569d",
        "model_name_for_query":"TheBloke\/Manticore-13B-Chat-Pyg-Guanaco-SuperHOT-8K-GPTQ"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.53,
        "ARC":55.72,
        "HellaSwag":78.75,
        "MMLU":47.99,
        "TruthfulQA":43.11,
        "Winogrande":75.85,
        "GSM8K":10.77,
        "DROP":6.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"e92a1439ac8d2edb5e311b8a42e13ed7c5e70db5",
        "model_name_for_query":"lvkaokao\/llama2-7b-hf-chat-lora"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.52,
        "ARC":58.53,
        "HellaSwag":81.6,
        "MMLU":46.96,
        "TruthfulQA":45.29,
        "Winogrande":75.85,
        "GSM8K":2.35,
        "DROP":8.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"32a474742c2a235ca12c96afaea57dcb6b46ef56",
        "model_name_for_query":"jondurbin\/airoboros-13b-gpt4-1.3"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":45.44,
        "ARC":52.9,
        "HellaSwag":77.71,
        "MMLU":48.83,
        "TruthfulQA":48.93,
        "Winogrande":71.9,
        "GSM8K":7.05,
        "DROP":10.78,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"5e66b59c145586266b2351a63f0cf1b4f62f5454",
        "model_name_for_query":"Harshvir\/Llama-2-7B-physics"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.44,
        "ARC":54.1,
        "HellaSwag":79.55,
        "MMLU":45.97,
        "TruthfulQA":43.65,
        "Winogrande":72.69,
        "GSM8K":2.73,
        "DROP":19.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":32.0,
        "Available on the hub":true,
        "Model sha":"06dbd3e0da08255c575e585cb82e0554c1d2707a",
        "model_name_for_query":"WizardLM\/WizardMath-7B-V1.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.39,
        "ARC":54.86,
        "HellaSwag":78.25,
        "MMLU":51.13,
        "TruthfulQA":43.68,
        "Winogrande":74.35,
        "GSM8K":8.04,
        "DROP":7.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"6073a87872eb36149404bfb7d60e0108074ee1c3",
        "model_name_for_query":"Lazycuber\/L2-7b-Orca-WVG-Test"
    },
    {
        "T":"\ud83d\udfe6",
        "Average \u2b06\ufe0f":45.36,
        "ARC":53.33,
        "HellaSwag":78.5,
        "MMLU":50.29,
        "TruthfulQA":48.38,
        "Winogrande":75.22,
        "GSM8K":4.09,
        "DROP":7.74,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":95.0,
        "Available on the hub":true,
        "Model sha":"2b099b2be0dafb2606ae9808c0f6183fe4bff7bc",
        "model_name_for_query":"TheBloke\/stable-vicuna-13B-HF"
    },
    {
        "T":"\ud83d\udfe6",
        "Average \u2b06\ufe0f":45.36,
        "ARC":52.9,
        "HellaSwag":78.55,
        "MMLU":48.32,
        "TruthfulQA":45.57,
        "Winogrande":71.74,
        "GSM8K":7.35,
        "DROP":13.09,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":1415.0,
        "Available on the hub":false,
        "Model sha":"b7701a9e825e79a5ab18b5801be113c2160cc627",
        "model_name_for_query":"meta-llama\/Llama-2-7b-chat-hf"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":45.35,
        "ARC":49.57,
        "HellaSwag":76.25,
        "MMLU":45.99,
        "TruthfulQA":42.17,
        "Winogrande":71.82,
        "GSM8K":1.52,
        "DROP":30.12,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"825506858e4603745a479215b8dea1524bfab6a0",
        "model_name_for_query":"synapsoft\/Llama-2-7b-chat-hf-flan2022-1.2M"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.35,
        "ARC":54.27,
        "HellaSwag":77.81,
        "MMLU":51.07,
        "TruthfulQA":46.28,
        "Winogrande":73.56,
        "GSM8K":6.97,
        "DROP":7.48,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"2491546f1219c3e9bb1a8cf37fbecf0b299c2177",
        "model_name_for_query":"LTC-AI-Labs\/L2-7b-Base-test-WVG"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":45.34,
        "ARC":52.73,
        "HellaSwag":78.58,
        "MMLU":48.3,
        "TruthfulQA":45.58,
        "Winogrande":71.9,
        "GSM8K":8.49,
        "DROP":11.77,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"163a5bec7b6f5aaa4667aa6a95746deff50ceab1",
        "model_name_for_query":"davzoku\/cria-llama2-7b-v1.3"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":45.33,
        "ARC":53.5,
        "HellaSwag":77.38,
        "MMLU":49.72,
        "TruthfulQA":45.77,
        "Winogrande":74.03,
        "GSM8K":9.55,
        "DROP":7.34,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":true,
        "Model sha":"186b105d61054611d0b921a55c220d41c6aefe43",
        "model_name_for_query":"rombodawg\/LosslessMegaCoder-llama2-7b-mini"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.31,
        "ARC":56.06,
        "HellaSwag":80.07,
        "MMLU":52.49,
        "TruthfulQA":42.43,
        "Winogrande":73.48,
        "GSM8K":3.79,
        "DROP":8.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":21.62,
        "Hub \u2764\ufe0f":11.0,
        "Available on the hub":true,
        "Model sha":"e6c74222958328e50712aa00294dc818c24075b2",
        "model_name_for_query":"nkpz\/llama2-22b-daydreamer-v3"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.29,
        "ARC":54.52,
        "HellaSwag":78.95,
        "MMLU":49.26,
        "TruthfulQA":46.77,
        "Winogrande":74.51,
        "GSM8K":7.05,
        "DROP":5.99,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"e66094c43ffe6c5b3f4164cd4ba048d3bc422fd0",
        "model_name_for_query":"ashercn97\/manatee-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.28,
        "ARC":45.65,
        "HellaSwag":75.65,
        "MMLU":49.27,
        "TruthfulQA":43.12,
        "Winogrande":69.93,
        "GSM8K":4.47,
        "DROP":28.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"883b0fa4158de8207d0a94f4b8cb188e6250aa9d",
        "model_name_for_query":"Charlie911\/vicuna-7b-v1.5-lora-mctaco"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.26,
        "ARC":54.61,
        "HellaSwag":80.15,
        "MMLU":39.25,
        "TruthfulQA":41.22,
        "Winogrande":73.09,
        "GSM8K":3.11,
        "DROP":25.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"5a45a16bac51ed9529a6dc2eab7355cc61eefb5b",
        "model_name_for_query":"jondurbin\/airoboros-7b-gpt4-1.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.24,
        "ARC":54.69,
        "HellaSwag":77.32,
        "MMLU":49.51,
        "TruthfulQA":50.41,
        "Winogrande":71.11,
        "GSM8K":6.44,
        "DROP":7.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":55.0,
        "Available on the hub":true,
        "Model sha":"9a93d7d11fac7f3f9074510b80092b53bc1a5bec",
        "model_name_for_query":"lmsys\/vicuna-7b-v1.5-16k"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.23,
        "ARC":53.07,
        "HellaSwag":78.69,
        "MMLU":38.9,
        "TruthfulQA":40.72,
        "Winogrande":73.09,
        "GSM8K":1.74,
        "DROP":30.39,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"d9bcb0ad365bfacdf95128bc1272b4106aff7be9",
        "model_name_for_query":"jondurbin\/airoboros-7b-gpt4"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.22,
        "ARC":53.07,
        "HellaSwag":78.67,
        "MMLU":38.88,
        "TruthfulQA":40.73,
        "Winogrande":73.09,
        "GSM8K":1.74,
        "DROP":30.39,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"14aa50fba9f6418c0d5e2d24087eb802931040ef",
        "model_name_for_query":"TheBloke\/airoboros-7b-gpt4-fp16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.22,
        "ARC":50.43,
        "HellaSwag":76.92,
        "MMLU":48.14,
        "TruthfulQA":47.01,
        "Winogrande":70.48,
        "GSM8K":5.69,
        "DROP":17.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":112.0,
        "Available on the hub":true,
        "Model sha":"ac066c83424c4a7221aa10c0ebe074b24d3bcdb6",
        "model_name_for_query":"lmsys\/vicuna-7b-v1.3"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.21,
        "ARC":51.02,
        "HellaSwag":75.23,
        "MMLU":49.58,
        "TruthfulQA":45.09,
        "Winogrande":72.61,
        "GSM8K":17.21,
        "DROP":5.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":53.9,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"bbbca62bb340b4ae0a19ba93dae38fc9f9787c16",
        "model_name_for_query":"elliotthwang\/Elliott-Chinese-LLaMa-GPTQ"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.16,
        "ARC":55.38,
        "HellaSwag":77.2,
        "MMLU":45.46,
        "TruthfulQA":51.5,
        "Winogrande":71.11,
        "GSM8K":8.04,
        "DROP":7.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"2204970fc0d96b071e2b1b003fbc5c87cfc46840",
        "model_name_for_query":"frank098\/WizardLM_13B_juniper"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.16,
        "ARC":52.99,
        "HellaSwag":77.59,
        "MMLU":45.32,
        "TruthfulQA":50.23,
        "Winogrande":74.03,
        "GSM8K":6.82,
        "DROP":9.11,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":40.0,
        "Available on the hub":true,
        "Model sha":"b20f96a0171ce4c0fa27d6048215ebe710521587",
        "model_name_for_query":"TheBloke\/koala-13B-HF"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.11,
        "ARC":54.18,
        "HellaSwag":77.31,
        "MMLU":49.3,
        "TruthfulQA":50.35,
        "Winogrande":71.03,
        "GSM8K":6.37,
        "DROP":7.24,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":55.0,
        "Available on the hub":true,
        "Model sha":"9a93d7d11fac7f3f9074510b80092b53bc1a5bec",
        "model_name_for_query":"lmsys\/vicuna-7b-v1.5-16k"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":45.1,
        "ARC":55.03,
        "HellaSwag":80.06,
        "MMLU":47.64,
        "TruthfulQA":44.65,
        "Winogrande":73.8,
        "GSM8K":6.14,
        "DROP":8.4,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"eafbba6fec094a17ca7bce6d9605cac97b90a483",
        "model_name_for_query":"jondurbin\/airoboros-l2-7b-2.2.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.1,
        "ARC":56.91,
        "HellaSwag":80.15,
        "MMLU":50.31,
        "TruthfulQA":37.44,
        "Winogrande":76.4,
        "GSM8K":8.87,
        "DROP":5.58,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"69e9e12d8bab66dffdcb15fa534fc3f0dc34acec",
        "model_name_for_query":"haoranxu\/ALMA-13B-Pretrain"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":45.1,
        "ARC":49.74,
        "HellaSwag":71.9,
        "MMLU":42.96,
        "TruthfulQA":47.66,
        "Winogrande":69.61,
        "GSM8K":3.94,
        "DROP":29.85,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.76,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"385f2d164e7fe780e053276d95d36240f2368c21",
        "model_name_for_query":"golaxy\/gowizardlm"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":45.06,
        "ARC":43.86,
        "HellaSwag":72.39,
        "MMLU":41.09,
        "TruthfulQA":38.16,
        "Winogrande":73.72,
        "GSM8K":2.5,
        "DROP":43.7,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":29.98,
        "Hub \u2764\ufe0f":34.0,
        "Available on the hub":true,
        "Model sha":"291753b04817a31a742631053ee361874d6db8a4",
        "model_name_for_query":"facebook\/opt-iml-max-30b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.06,
        "ARC":53.67,
        "HellaSwag":78.79,
        "MMLU":46.78,
        "TruthfulQA":43.97,
        "Winogrande":71.74,
        "GSM8K":7.35,
        "DROP":13.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"ec98429034fc84a4555dd4e3db4d6af534a03832",
        "model_name_for_query":"TheTravellingEngineer\/llama2-7b-chat-hf-dpo"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.05,
        "ARC":47.61,
        "HellaSwag":72.24,
        "MMLU":47.74,
        "TruthfulQA":48.73,
        "Winogrande":69.69,
        "GSM8K":9.86,
        "DROP":19.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.89,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"8690c065bccd3e897ccbf3d8aa24b0216a6f5dba",
        "model_name_for_query":"OpenBuddy\/openbuddy-openllama-13b-v7-fp16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.05,
        "ARC":53.5,
        "HellaSwag":76.74,
        "MMLU":49.69,
        "TruthfulQA":49.68,
        "Winogrande":71.98,
        "GSM8K":7.2,
        "DROP":6.54,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"92bf763ce7ae0bfe155bfd60190eed64582e5080",
        "model_name_for_query":"Lajonbot\/vicuna-7b-v1.5-PL-lora_unload"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":45.03,
        "ARC":54.95,
        "HellaSwag":79.25,
        "MMLU":46.61,
        "TruthfulQA":46.35,
        "Winogrande":74.03,
        "GSM8K":7.35,
        "DROP":6.66,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":12.0,
        "Available on the hub":true,
        "Model sha":"762ecb0d85572c8f8bcbca06d27f7f64a4d74615",
        "model_name_for_query":"camel-ai\/CAMEL-13B-Role-Playing-Data"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.98,
        "ARC":54.1,
        "HellaSwag":80.42,
        "MMLU":41.47,
        "TruthfulQA":40.46,
        "Winogrande":71.19,
        "GSM8K":3.03,
        "DROP":24.21,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":30.0,
        "Available on the hub":true,
        "Model sha":"b57b9f5ff34059e485b769973d023021fc66a8f7",
        "model_name_for_query":"medalpaca\/medalpaca-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.98,
        "ARC":53.67,
        "HellaSwag":78.79,
        "MMLU":46.72,
        "TruthfulQA":43.97,
        "Winogrande":71.74,
        "GSM8K":7.35,
        "DROP":12.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"b1ab836d9ebf7029fafa07949b51d3838501d537",
        "model_name_for_query":"NewstaR\/Koss-7B-chat"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.97,
        "ARC":54.95,
        "HellaSwag":78.48,
        "MMLU":48.36,
        "TruthfulQA":45.72,
        "Winogrande":74.74,
        "GSM8K":5.84,
        "DROP":6.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"eb5b1d65fdf916ca71f89a46eb91175c1c630a57",
        "model_name_for_query":"LTC-AI-Labs\/L2-7b-Hermes-WVG-Test"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.95,
        "ARC":55.97,
        "HellaSwag":77.89,
        "MMLU":49.48,
        "TruthfulQA":44.11,
        "Winogrande":74.11,
        "GSM8K":5.91,
        "DROP":7.14,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"23ae02efba01c37abe3cff0fedc7d2d9644fe98e",
        "model_name_for_query":"LTC-AI-Labs\/L2-7b-Synthia-WVG-Test"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":44.91,
        "ARC":53.33,
        "HellaSwag":78.5,
        "MMLU":43.61,
        "TruthfulQA":46.37,
        "Winogrande":75.61,
        "GSM8K":6.52,
        "DROP":10.4,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"d5b6e9d5b882d4f6ba322396e027925ed915f848",
        "model_name_for_query":"psyche\/kollama2-7b-v2"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":44.87,
        "ARC":53.75,
        "HellaSwag":78.69,
        "MMLU":46.65,
        "TruthfulQA":43.93,
        "Winogrande":72.61,
        "GSM8K":7.81,
        "DROP":10.65,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"fc7a3abbc3b9a9b3e163ef3c4844307ac270fca7",
        "model_name_for_query":"guardrail\/llama-2-7b-guanaco-instruct-sharded"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":44.83,
        "ARC":56.23,
        "HellaSwag":80.93,
        "MMLU":47.67,
        "TruthfulQA":39.48,
        "Winogrande":76.24,
        "GSM8K":7.58,
        "DROP":5.66,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"4022c52fcc7473ce7364bb5ac166195903ea1efb",
        "model_name_for_query":"huggingface\/llama-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.82,
        "ARC":58.19,
        "HellaSwag":80.12,
        "MMLU":50.48,
        "TruthfulQA":45.18,
        "Winogrande":70.72,
        "GSM8K":1.97,
        "DROP":7.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"1de9244bfadb947f80872727f76790cbc76e7142",
        "model_name_for_query":"totally-not-an-llm\/EverythingLM-13b-V3-16k"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":44.81,
        "ARC":56.14,
        "HellaSwag":80.92,
        "MMLU":47.61,
        "TruthfulQA":39.48,
        "Winogrande":76.24,
        "GSM8K":7.58,
        "DROP":5.66,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":119.0,
        "Available on the hub":true,
        "Model sha":"bf57045473f207bb1de1ed035ace226f4d9f9bba",
        "model_name_for_query":"huggyllama\/llama-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.8,
        "ARC":51.79,
        "HellaSwag":75.79,
        "MMLU":50.23,
        "TruthfulQA":49.61,
        "Winogrande":71.82,
        "GSM8K":6.6,
        "DROP":7.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"91ce25dbdb67793ad1fcfdfd59f7603c2be65aea",
        "model_name_for_query":"AlekseyKorshuk\/vic15-exp-syn-fight-cp3838"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.8,
        "ARC":56.14,
        "HellaSwag":80.93,
        "MMLU":47.66,
        "TruthfulQA":39.48,
        "Winogrande":76.16,
        "GSM8K":7.58,
        "DROP":5.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":11.0,
        "Available on the hub":false,
        "Model sha":"13443d633eaa5b7e1a90ac9cdb4a4d51b1c8d0d1",
        "model_name_for_query":"jordiclive\/gpt4all-alpaca-oa-codealpaca-lora-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.8,
        "ARC":56.48,
        "HellaSwag":80.02,
        "MMLU":42.93,
        "TruthfulQA":35.86,
        "Winogrande":75.53,
        "GSM8K":0.08,
        "DROP":22.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":26.0,
        "Available on the hub":true,
        "Model sha":"f96308083033c84db47b6c093da3817c085c87c7",
        "model_name_for_query":"TehVenom\/Pygmalion-13b-Merged"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.78,
        "ARC":54.95,
        "HellaSwag":80.05,
        "MMLU":47.03,
        "TruthfulQA":43.47,
        "Winogrande":74.74,
        "GSM8K":7.28,
        "DROP":5.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"4309eedebe8ba5709e0cc7cf186cb783f3bc8060",
        "model_name_for_query":"Norquinal\/llama-2-7b-claude-chat-rp"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.77,
        "ARC":53.41,
        "HellaSwag":78.85,
        "MMLU":37.09,
        "TruthfulQA":43.48,
        "Winogrande":72.22,
        "GSM8K":4.55,
        "DROP":23.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":19.0,
        "Available on the hub":true,
        "Model sha":"b802f1b4401d0b2242137160c20cc11b9ffd3a4c",
        "model_name_for_query":"TheBloke\/Wizard-Vicuna-7B-Uncensored-HF"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.77,
        "ARC":53.41,
        "HellaSwag":78.85,
        "MMLU":37.09,
        "TruthfulQA":43.48,
        "Winogrande":72.22,
        "GSM8K":4.55,
        "DROP":23.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":63.0,
        "Available on the hub":true,
        "Model sha":"1097285acd9c48a1d09bc0a9844d365384732111",
        "model_name_for_query":"ehartford\/Wizard-Vicuna-7B-Uncensored"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.77,
        "ARC":55.12,
        "HellaSwag":77.4,
        "MMLU":49.27,
        "TruthfulQA":43.64,
        "Winogrande":73.64,
        "GSM8K":6.14,
        "DROP":8.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"770fa73981a599e935c21a95b1817a553c726694",
        "model_name_for_query":"Danielbrdz\/Barcenas-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.77,
        "ARC":52.73,
        "HellaSwag":79.1,
        "MMLU":47.88,
        "TruthfulQA":47.21,
        "Winogrande":68.43,
        "GSM8K":7.96,
        "DROP":10.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"de4cfe99e9e3db62733b40f48b2b11faf9abe4bf",
        "model_name_for_query":"maximuslee07\/llama-2-7b-rockwell-final"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":44.74,
        "ARC":51.45,
        "HellaSwag":77.35,
        "MMLU":46.47,
        "TruthfulQA":45.52,
        "Winogrande":70.8,
        "GSM8K":6.75,
        "DROP":14.82,
        "Type":"instruction-tuned",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"6864fa8ee43fa4d6b4f3ae055bbf464a5dcca570",
        "model_name_for_query":"davzoku\/cria-llama2-7b-v1.3_peft"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.7,
        "ARC":51.96,
        "HellaSwag":78.11,
        "MMLU":38.43,
        "TruthfulQA":42.47,
        "Winogrande":72.85,
        "GSM8K":2.73,
        "DROP":26.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"openrail",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":21.0,
        "Available on the hub":true,
        "Model sha":"c6e2f515a0b289478118b5b75ff74107002ad962",
        "model_name_for_query":"bofenghuang\/vigogne-7b-instruct"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.67,
        "ARC":45.82,
        "HellaSwag":67.71,
        "MMLU":45.88,
        "TruthfulQA":44.67,
        "Winogrande":65.35,
        "GSM8K":8.49,
        "DROP":34.77,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"25e1c346c2a01588a728307d5c35fbeecd58b51b",
        "model_name_for_query":"speechlessai\/speechless-codellama-dolphin-orca-platypus-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.66,
        "ARC":54.01,
        "HellaSwag":78.23,
        "MMLU":49.11,
        "TruthfulQA":43.78,
        "Winogrande":75.14,
        "GSM8K":6.37,
        "DROP":5.98,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":25.0,
        "Available on the hub":true,
        "Model sha":"983f8ad5c156f4a0e4d2b7b5f1146981ad2e8a8b",
        "model_name_for_query":"PygmalionAI\/pygmalion-2-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.66,
        "ARC":57.42,
        "HellaSwag":82.11,
        "MMLU":51.43,
        "TruthfulQA":47.99,
        "Winogrande":57.85,
        "GSM8K":0.45,
        "DROP":15.36,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":true,
        "Model sha":"97981254d4b0ac0d1472376f602c004670070fdd",
        "model_name_for_query":"CalderaAI\/13B-Ouroboros"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.65,
        "ARC":42.83,
        "HellaSwag":71.47,
        "MMLU":47.47,
        "TruthfulQA":47.24,
        "Winogrande":67.4,
        "GSM8K":9.48,
        "DROP":26.69,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.94,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"259ab0967975012a546f2362d6cd03ab10768157",
        "model_name_for_query":"keyfan\/vicuna-chinese-replication-v1.1"
    },
    {
        "T":"\ud83d\udfe6",
        "Average \u2b06\ufe0f":44.65,
        "ARC":55.03,
        "HellaSwag":79.12,
        "MMLU":40.51,
        "TruthfulQA":50.37,
        "Winogrande":74.19,
        "GSM8K":7.2,
        "DROP":6.1,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"730cbd8f3077f3d24001aab714def991f1e4e7e8",
        "model_name_for_query":"ehartford\/Samantha-1.11-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.64,
        "ARC":55.72,
        "HellaSwag":80.94,
        "MMLU":47.47,
        "TruthfulQA":48.34,
        "Winogrande":71.19,
        "GSM8K":2.65,
        "DROP":6.14,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"8ecaba5dd0e9929f5858cfe9f5f8cd8ba285c9e5",
        "model_name_for_query":"HWERI\/Llama2-7b-sharegpt4"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.64,
        "ARC":55.72,
        "HellaSwag":80.94,
        "MMLU":47.47,
        "TruthfulQA":48.34,
        "Winogrande":71.19,
        "GSM8K":2.65,
        "DROP":6.14,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":false,
        "Model sha":"922d1d963ad1b042c30b774a818d9f6180c28075",
        "model_name_for_query":"beaugogh\/Llama2-7b-sharegpt4"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.6,
        "ARC":54.86,
        "HellaSwag":79.65,
        "MMLU":46.38,
        "TruthfulQA":43.83,
        "Winogrande":75.22,
        "GSM8K":6.29,
        "DROP":5.99,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":9.0,
        "Available on the hub":true,
        "Model sha":"f769fed10874af73ad12115efd044cb4a64506b0",
        "model_name_for_query":"Mikael110\/llama-2-7b-guanaco-fp16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.6,
        "ARC":50.51,
        "HellaSwag":76.87,
        "MMLU":45.35,
        "TruthfulQA":41.34,
        "Winogrande":69.53,
        "GSM8K":4.09,
        "DROP":24.48,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":9.0,
        "Available on the hub":true,
        "Model sha":"67729407add902e3d4d36bb105d7c011fb368ea5",
        "model_name_for_query":"jondurbin\/airoboros-l2-7b-gpt4-m2.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.58,
        "ARC":54.44,
        "HellaSwag":78.48,
        "MMLU":49.23,
        "TruthfulQA":41.82,
        "Winogrande":75.3,
        "GSM8K":6.82,
        "DROP":5.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"4e5fa9ae7f572b4841b02c3f96d8a3c7a7e59521",
        "model_name_for_query":"Delcos\/Mistral-Pygmalion-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.56,
        "ARC":53.75,
        "HellaSwag":78.54,
        "MMLU":35.95,
        "TruthfulQA":43.55,
        "Winogrande":69.93,
        "GSM8K":0.0,
        "DROP":30.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"8fdcfdda6877d7f21173dfac48b2c14499ba8264",
        "model_name_for_query":"mncai\/chatdoctor"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.55,
        "ARC":50.77,
        "HellaSwag":74.63,
        "MMLU":48.13,
        "TruthfulQA":49.36,
        "Winogrande":72.38,
        "GSM8K":6.9,
        "DROP":9.72,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"dc71924cfb214b91461d35178e6ea6fef7946f13",
        "model_name_for_query":"joehuangx\/spatial-vicuna-7b-v1.5-LoRA"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.54,
        "ARC":54.44,
        "HellaSwag":80.66,
        "MMLU":46.74,
        "TruthfulQA":41.39,
        "Winogrande":74.9,
        "GSM8K":7.73,
        "DROP":5.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"e65d34ed31cdcd2637f6284aa0605f30ef5a9381",
        "model_name_for_query":"Norquinal\/llama-2-7b-claude-chat"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":44.53,
        "ARC":54.61,
        "HellaSwag":81.0,
        "MMLU":47.07,
        "TruthfulQA":41.93,
        "Winogrande":74.51,
        "GSM8K":6.52,
        "DROP":6.1,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"bc8c239cacf1e3211f05e27be67a74d84c12aea9",
        "model_name_for_query":"PeanutJar\/LLaMa-2-PeanutButter_v18_B-7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.5,
        "ARC":51.19,
        "HellaSwag":78.92,
        "MMLU":46.63,
        "TruthfulQA":48.5,
        "Winogrande":74.43,
        "GSM8K":5.99,
        "DROP":5.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"unknown",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":false,
        "Model sha":"0fc43413117187e0723cdac133068ab527c80fe2",
        "model_name_for_query":"dfurman\/llama-2-7b-instruct-peft"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.47,
        "ARC":52.82,
        "HellaSwag":76.07,
        "MMLU":50.47,
        "TruthfulQA":43.54,
        "Winogrande":73.72,
        "GSM8K":7.81,
        "DROP":6.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"372c90543ebb2a317fb9b51ff3890cc270e5ce3a",
        "model_name_for_query":"Charlie911\/vicuna-7b-v1.5-lora-timedial-unit-080082"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.47,
        "ARC":48.38,
        "HellaSwag":71.78,
        "MMLU":44.5,
        "TruthfulQA":44.73,
        "Winogrande":67.88,
        "GSM8K":2.05,
        "DROP":32.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.04,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"16d4c4214fa8d5a962b9064a8b958076b7c79a17",
        "model_name_for_query":"golaxy\/gogpt2-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.47,
        "ARC":51.28,
        "HellaSwag":78.75,
        "MMLU":44.68,
        "TruthfulQA":45.83,
        "Winogrande":74.11,
        "GSM8K":2.73,
        "DROP":13.88,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"openrail",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"a893ebef4b818de1968dd9e932da2f513d16386a",
        "model_name_for_query":"sia-ai\/llama-2-7b-1-percent-open-orca-1000-steps-v0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.46,
        "ARC":46.76,
        "HellaSwag":71.53,
        "MMLU":42.85,
        "TruthfulQA":47.85,
        "Winogrande":68.67,
        "GSM8K":2.27,
        "DROP":31.28,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.76,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"ee60ed402dedf24b6154aef05df54512e02fc9e2",
        "model_name_for_query":"golaxy\/gogpt2-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.44,
        "ARC":48.81,
        "HellaSwag":73.79,
        "MMLU":43.03,
        "TruthfulQA":41.0,
        "Winogrande":69.77,
        "GSM8K":1.9,
        "DROP":32.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.76,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"7eb70c0e330b7d3ff490047ddbb153bb96294882",
        "model_name_for_query":"golaxy\/gogpt-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.44,
        "ARC":52.82,
        "HellaSwag":76.1,
        "MMLU":50.58,
        "TruthfulQA":43.4,
        "Winogrande":73.72,
        "GSM8K":7.66,
        "DROP":6.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"ae7e0fb58f4201bb14fd4e641d0d6dcc22674e0e",
        "model_name_for_query":"Charlie911\/vicuna-7b-v1.5-lora-timedial-unit-080091"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.43,
        "ARC":46.33,
        "HellaSwag":67.71,
        "MMLU":47.19,
        "TruthfulQA":46.66,
        "Winogrande":63.77,
        "GSM8K":5.99,
        "DROP":33.34,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"a82467de3cb9438aa8f9e0ea8ea692f16a5724b2",
        "model_name_for_query":"uukuguy\/speechless-codellama-orca-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.41,
        "ARC":53.92,
        "HellaSwag":79.1,
        "MMLU":51.25,
        "TruthfulQA":36.24,
        "Winogrande":75.53,
        "GSM8K":8.57,
        "DROP":6.27,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.97,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"f87d66f9c4541c575a6fad3c19a31b11568e0dfb",
        "model_name_for_query":"YeungNLP\/firefly-llama2-13b-pretrain"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.41,
        "ARC":44.8,
        "HellaSwag":68.6,
        "MMLU":44.03,
        "TruthfulQA":46.28,
        "Winogrande":66.93,
        "GSM8K":9.55,
        "DROP":30.68,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"0c41023f8f665946a2c46c3823afee431408bcbd",
        "model_name_for_query":"uukuguy\/speechless-codellama-dolphin-orca-platypus-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.41,
        "ARC":53.07,
        "HellaSwag":75.59,
        "MMLU":48.8,
        "TruthfulQA":44.73,
        "Winogrande":73.24,
        "GSM8K":8.64,
        "DROP":6.77,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"55862462a23ab43fb73d4c784f1518ab4645764c",
        "model_name_for_query":"revolutionarybukhari\/Llama-2-7b-chat-finetune-AUTOMATE"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.38,
        "ARC":53.33,
        "HellaSwag":78.81,
        "MMLU":45.58,
        "TruthfulQA":42.0,
        "Winogrande":75.14,
        "GSM8K":6.22,
        "DROP":9.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"4ede4a6f8a8d6cc3bfff8b98837116c74c280f63",
        "model_name_for_query":"edor\/Platypus2-mini-7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.38,
        "ARC":53.24,
        "HellaSwag":78.89,
        "MMLU":46.77,
        "TruthfulQA":42.75,
        "Winogrande":75.37,
        "GSM8K":7.96,
        "DROP":5.68,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"ba8e755feab0bbf90675dcb9f8875a42f92112a5",
        "model_name_for_query":"LTC-AI-Labs\/Guanaco-Vicuna-7B-L2"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":44.35,
        "ARC":55.29,
        "HellaSwag":81.69,
        "MMLU":46.97,
        "TruthfulQA":43.78,
        "Winogrande":70.88,
        "GSM8K":5.91,
        "DROP":5.93,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"f98bb987216448aa3aa89e575a7494fae8b68066",
        "model_name_for_query":"PeanutJar\/LLaMa-2-PeanutButter_v10-7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.27,
        "ARC":53.24,
        "HellaSwag":76.92,
        "MMLU":35.92,
        "TruthfulQA":39.44,
        "Winogrande":72.22,
        "GSM8K":1.21,
        "DROP":30.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"1f61442e1238062095b31b4909c5e9ab26105794",
        "model_name_for_query":"TehVenom\/Pygmalion_AlpacaLora-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.25,
        "ARC":53.24,
        "HellaSwag":76.94,
        "MMLU":44.64,
        "TruthfulQA":45.34,
        "Winogrande":70.64,
        "GSM8K":4.32,
        "DROP":14.66,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"2d82abff150b7a5ae484f9cd7c64c72fd4eaf7f5",
        "model_name_for_query":"wahaha1987\/llama_7b_sharegpt94k_fastchat"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.24,
        "ARC":50.77,
        "HellaSwag":76.02,
        "MMLU":39.5,
        "TruthfulQA":43.86,
        "Winogrande":71.43,
        "GSM8K":2.88,
        "DROP":25.23,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":34.0,
        "Available on the hub":true,
        "Model sha":"165850882991d7fa4eabab577a03ed84e0713bfa",
        "model_name_for_query":"psmathur\/orca_mini_v2_7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.22,
        "ARC":50.94,
        "HellaSwag":74.55,
        "MMLU":38.56,
        "TruthfulQA":46.89,
        "Winogrande":72.06,
        "GSM8K":1.36,
        "DROP":25.15,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"f5cc642a10160a014e2afeefcd57d4781994c51e",
        "model_name_for_query":"jxhong\/CAlign-alpaca-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.2,
        "ARC":53.24,
        "HellaSwag":79.13,
        "MMLU":46.65,
        "TruthfulQA":42.59,
        "Winogrande":75.14,
        "GSM8K":7.05,
        "DROP":5.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"67ede9be6ceffdf574294351cca937d88d7d448d",
        "model_name_for_query":"LTC-AI-Labs\/L2-7b-Base-WVG-Uncensored"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.14,
        "ARC":50.6,
        "HellaSwag":76.99,
        "MMLU":48.93,
        "TruthfulQA":43.42,
        "Winogrande":75.37,
        "GSM8K":7.96,
        "DROP":5.68,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"9d49378c69c00113cf7f6e66d1ddb9d9b003dddc",
        "model_name_for_query":"Lazycuber\/L2-7b-Guanaco-Uncensored"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":44.13,
        "ARC":53.67,
        "HellaSwag":78.21,
        "MMLU":45.9,
        "TruthfulQA":46.13,
        "Winogrande":73.8,
        "GSM8K":4.7,
        "DROP":6.53,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"1295069e9fef63aed87d36fe108d6c934cb34ded",
        "model_name_for_query":"dhmeltzer\/Llama-2-7b-hf-eli5-cleaned-1024_qlora_merged"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.13,
        "ARC":52.9,
        "HellaSwag":76.29,
        "MMLU":50.47,
        "TruthfulQA":41.6,
        "Winogrande":73.56,
        "GSM8K":7.28,
        "DROP":6.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"1e1709818cca48af4cd31c07c493f996854aa10f",
        "model_name_for_query":"Charlie911\/vicuna-7b-v1.5-lora-timedial"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.06,
        "ARC":52.22,
        "HellaSwag":79.08,
        "MMLU":46.63,
        "TruthfulQA":42.97,
        "Winogrande":74.51,
        "GSM8K":7.28,
        "DROP":5.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"dd51a3b26ad378e2953c947a1e4c2f8febe0cb52",
        "model_name_for_query":"Lazycuber\/L2-7b-Base-Guanaco-Uncensored"
    },
    {
        "T":"\ud83d\udfe6",
        "Average \u2b06\ufe0f":44.03,
        "ARC":54.1,
        "HellaSwag":78.74,
        "MMLU":45.44,
        "TruthfulQA":43.4,
        "Winogrande":73.64,
        "GSM8K":4.55,
        "DROP":8.35,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"f1f3b9fdb1e2d8d8fa913d57a8fe15d7bdf72c20",
        "model_name_for_query":"dhmeltzer\/llama-7b-SFT-qlora-eli5-wiki_DPO_ds_RM_top_2_1024_r_64_alpha_16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.02,
        "ARC":48.04,
        "HellaSwag":76.63,
        "MMLU":46.12,
        "TruthfulQA":30.9,
        "Winogrande":69.46,
        "GSM8K":0.0,
        "DROP":37.0,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"8dc109f45ef36cc7bbd0f5d83fb65ac8e768d1bd",
        "model_name_for_query":"luffycodes\/llama-shishya-7b-ep3-v1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":44.01,
        "ARC":53.07,
        "HellaSwag":77.65,
        "MMLU":37.23,
        "TruthfulQA":43.39,
        "Winogrande":70.96,
        "GSM8K":2.12,
        "DROP":23.66,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":14.0,
        "Available on the hub":true,
        "Model sha":"7ea67f85ff3a7a8ec77f1819dec3e56779b764b1",
        "model_name_for_query":"jondurbin\/airoboros-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.99,
        "ARC":51.45,
        "HellaSwag":76.77,
        "MMLU":40.61,
        "TruthfulQA":44.34,
        "Winogrande":69.77,
        "GSM8K":3.41,
        "DROP":21.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"a7261e7ae6ee76c78e1ba1ac8c59bcc3e0868bf9",
        "model_name_for_query":"quantumaikr\/KoreanLM-hf"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.96,
        "ARC":53.75,
        "HellaSwag":78.76,
        "MMLU":46.02,
        "TruthfulQA":43.31,
        "Winogrande":73.48,
        "GSM8K":4.7,
        "DROP":7.72,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"6ba5416f618ed3e11b409326e84c36fa542f0951",
        "model_name_for_query":"dhmeltzer\/llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.91,
        "ARC":48.55,
        "HellaSwag":76.03,
        "MMLU":43.15,
        "TruthfulQA":49.4,
        "Winogrande":69.77,
        "GSM8K":3.03,
        "DROP":17.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"ff8e15fd68119d36ae1f0cebaa87f16e2ad3c732",
        "model_name_for_query":"Monero\/WizardLM-13b-OpenAssistant-Uncensored"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.91,
        "ARC":53.07,
        "HellaSwag":77.57,
        "MMLU":46.03,
        "TruthfulQA":44.57,
        "Winogrande":74.19,
        "GSM8K":6.29,
        "DROP":5.65,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"afca346816726b83e331bb4d93246ed5146e1675",
        "model_name_for_query":"willnguyen\/lacda-2-7B-chat-v0.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.85,
        "ARC":53.75,
        "HellaSwag":78.34,
        "MMLU":46.8,
        "TruthfulQA":42.34,
        "Winogrande":73.95,
        "GSM8K":6.22,
        "DROP":5.54,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"3dfef350be9c8ce92c2d314dbe96a002bd6ca97d",
        "model_name_for_query":"Aspik101\/Llama-2-7b-hf-instruct-pl-lora_unload"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.83,
        "ARC":52.47,
        "HellaSwag":78.75,
        "MMLU":45.33,
        "TruthfulQA":43.9,
        "Winogrande":74.19,
        "GSM8K":6.07,
        "DROP":6.11,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"6c1fc95e67b11f1011a3b2fc1aa05c7b83251e40",
        "model_name_for_query":"TheTravellingEngineer\/llama2-7b-hf-guanaco"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.83,
        "ARC":54.27,
        "HellaSwag":79.63,
        "MMLU":50.97,
        "TruthfulQA":37.71,
        "Winogrande":72.77,
        "GSM8K":5.99,
        "DROP":5.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"c2defe28e2f3f10460baf8f778b00986a53aa7a2",
        "model_name_for_query":"conceptofmind\/LLongMA-2-13b-16k"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.83,
        "ARC":53.16,
        "HellaSwag":78.33,
        "MMLU":47.09,
        "TruthfulQA":42.11,
        "Winogrande":73.64,
        "GSM8K":6.9,
        "DROP":5.58,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"ac5d22e14c2c7a400519da5d12d88e4fe683ccfa",
        "model_name_for_query":"elliotthwang\/elliott_Llama-2-7b-hf"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.82,
        "ARC":51.11,
        "HellaSwag":78.51,
        "MMLU":46.11,
        "TruthfulQA":44.86,
        "Winogrande":73.88,
        "GSM8K":5.69,
        "DROP":6.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":111.0,
        "Available on the hub":true,
        "Model sha":"35696b9a7ab330dcbe240ff76fb44ab1eccf45bf",
        "model_name_for_query":"togethercomputer\/Llama-2-7B-32K-Instruct"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.81,
        "ARC":53.07,
        "HellaSwag":78.57,
        "MMLU":46.8,
        "TruthfulQA":38.75,
        "Winogrande":74.03,
        "GSM8K":10.01,
        "DROP":5.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"bigscience-openrail-m",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"28a206689c0097738177840a40e455a308db2d7d",
        "model_name_for_query":"kashif\/stack-llama-2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.81,
        "ARC":57.17,
        "HellaSwag":80.27,
        "MMLU":36.11,
        "TruthfulQA":48.52,
        "Winogrande":72.14,
        "GSM8K":0.3,
        "DROP":12.16,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"0715b738e750830ba7213f26fe32fa1cc1bb15b3",
        "model_name_for_query":"ajibawa-2023\/scarlett-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.78,
        "ARC":51.19,
        "HellaSwag":75.4,
        "MMLU":47.47,
        "TruthfulQA":42.06,
        "Winogrande":71.67,
        "GSM8K":11.98,
        "DROP":6.72,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"cb0b04b1bff7921614efbd87d5b87bac04c58d13",
        "model_name_for_query":"CobraMamba\/mamba-gpt-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.78,
        "ARC":51.62,
        "HellaSwag":76.73,
        "MMLU":47.45,
        "TruthfulQA":44.79,
        "Winogrande":72.77,
        "GSM8K":7.43,
        "DROP":5.66,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"bdb57c5c992872ced47f48cb2177a5fa159f926a",
        "model_name_for_query":"quantumaikr\/llama-2-7b-hf-guanaco-1k"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.75,
        "ARC":52.13,
        "HellaSwag":78.14,
        "MMLU":38.64,
        "TruthfulQA":41.79,
        "Winogrande":71.67,
        "GSM8K":2.12,
        "DROP":21.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":28.0,
        "Available on the hub":true,
        "Model sha":"431fda60009d9b37a73211123ffb9c797764e182",
        "model_name_for_query":"jondurbin\/airoboros-7b-gpt4-1.2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.74,
        "ARC":53.16,
        "HellaSwag":78.98,
        "MMLU":47.04,
        "TruthfulQA":39.51,
        "Winogrande":74.35,
        "GSM8K":7.51,
        "DROP":5.65,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"aa8f81624d897aa493474bcd96dc3feae9f7a535",
        "model_name_for_query":"TinyPixel\/elm-test"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.71,
        "ARC":54.27,
        "HellaSwag":79.66,
        "MMLU":50.86,
        "TruthfulQA":37.68,
        "Winogrande":72.61,
        "GSM8K":5.46,
        "DROP":5.45,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"c2defe28e2f3f10460baf8f778b00986a53aa7a2",
        "model_name_for_query":"conceptofmind\/LLongMA-2-13b-16k"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.71,
        "ARC":53.24,
        "HellaSwag":78.78,
        "MMLU":42.31,
        "TruthfulQA":44.56,
        "Winogrande":73.95,
        "GSM8K":5.99,
        "DROP":7.11,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"48fca4ba1e2d31ff4fbe6856b9b93ad2d97da8b7",
        "model_name_for_query":"psyche\/kollama2-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.7,
        "ARC":50.51,
        "HellaSwag":76.72,
        "MMLU":48.03,
        "TruthfulQA":43.36,
        "Winogrande":72.93,
        "GSM8K":8.57,
        "DROP":5.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"5d33696ee324899d52fc43794b46009fea08a9af",
        "model_name_for_query":"TheTravellingEngineer\/llama2-7b-chat-hf-guanaco"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.66,
        "ARC":54.01,
        "HellaSwag":78.35,
        "MMLU":46.25,
        "TruthfulQA":38.49,
        "Winogrande":75.45,
        "GSM8K":7.13,
        "DROP":5.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":36.0,
        "Available on the hub":true,
        "Model sha":"f21c0d5e3f9f8c5addf093358e6885afa9602296",
        "model_name_for_query":"mrm8488\/llama-2-coder-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.65,
        "ARC":52.47,
        "HellaSwag":80.59,
        "MMLU":42.85,
        "TruthfulQA":47.22,
        "Winogrande":69.69,
        "GSM8K":5.16,
        "DROP":7.54,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"bigcode-openrail-m",
        "#Params (B)":15.52,
        "Hub \u2764\ufe0f":222.0,
        "Available on the hub":true,
        "Model sha":"b1bcda690655777373f57ea6614eb095ec2c886f",
        "model_name_for_query":"HuggingFaceH4\/starchat-beta"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.63,
        "ARC":53.07,
        "HellaSwag":78.88,
        "MMLU":46.42,
        "TruthfulQA":39.4,
        "Winogrande":74.03,
        "GSM8K":7.96,
        "DROP":5.65,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"4d6a006c6341f29b11c02f19bf9535f51b4da1b5",
        "model_name_for_query":"TinyPixel\/lima-test"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.62,
        "ARC":54.78,
        "HellaSwag":77.94,
        "MMLU":41.35,
        "TruthfulQA":44.02,
        "Winogrande":74.51,
        "GSM8K":6.67,
        "DROP":6.04,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"18a4ed38285c732efc583a4bd883b3a681f8d005",
        "model_name_for_query":"wenge-research\/yayi-7b-llama2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.61,
        "ARC":51.79,
        "HellaSwag":76.41,
        "MMLU":49.58,
        "TruthfulQA":40.33,
        "Winogrande":73.4,
        "GSM8K":7.13,
        "DROP":6.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"26626ea669172be6bc8e6b2b0bc5f14aef8061aa",
        "model_name_for_query":"Charlie911\/vicuna-7b-v1.5-lora-mixed-datasets-time-unit"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.6,
        "ARC":54.27,
        "HellaSwag":76.52,
        "MMLU":37.5,
        "TruthfulQA":43.86,
        "Winogrande":70.24,
        "GSM8K":5.0,
        "DROP":17.8,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"65bbcb80158a6d2e133bba99a90142caf4e2e242",
        "model_name_for_query":"ajibawa-2023\/Uncensored-Frank-7B"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":43.58,
        "ARC":53.16,
        "HellaSwag":78.11,
        "MMLU":45.54,
        "TruthfulQA":40.37,
        "Winogrande":74.9,
        "GSM8K":7.2,
        "DROP":5.78,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"15b2fa81418792841014f589e61d1d9e30457040",
        "model_name_for_query":"PeanutJar\/LLaMa-2-PeanutButter_v18_A-7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.57,
        "ARC":53.16,
        "HellaSwag":77.71,
        "MMLU":43.47,
        "TruthfulQA":45.28,
        "Winogrande":73.8,
        "GSM8K":6.37,
        "DROP":5.22,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"c39cee3821269e7fdffa690c2d0836c74dfebd25",
        "model_name_for_query":"RoversX\/llama-2-7b-hf-small-shards-Samantha-V1-SFT"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.57,
        "ARC":52.99,
        "HellaSwag":77.49,
        "MMLU":47.12,
        "TruthfulQA":42.61,
        "Winogrande":72.06,
        "GSM8K":6.9,
        "DROP":5.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"f838fda8d2b97effae1e8af4dbb6217eab14fb7e",
        "model_name_for_query":"Lajonbot\/Llama-2-7b-chat-hf-instruct-pl-lora_unload"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.56,
        "ARC":53.24,
        "HellaSwag":78.78,
        "MMLU":46.61,
        "TruthfulQA":39.17,
        "Winogrande":73.8,
        "GSM8K":7.66,
        "DROP":5.66,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"cb1111653997cee2818ffcf13a1c37237ea2934d",
        "model_name_for_query":"TinyPixel\/testmodel2"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":43.55,
        "ARC":53.67,
        "HellaSwag":78.09,
        "MMLU":45.63,
        "TruthfulQA":41.72,
        "Winogrande":73.56,
        "GSM8K":5.61,
        "DROP":6.6,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"2af3d3acb0466fef466512bc17b9bf57024629e8",
        "model_name_for_query":"dhmeltzer\/Llama-2-7b-hf-eli5-cleaned-wiki65k-1024_qlora_merged"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.51,
        "ARC":51.71,
        "HellaSwag":76.44,
        "MMLU":50.13,
        "TruthfulQA":39.57,
        "Winogrande":73.24,
        "GSM8K":7.13,
        "DROP":6.39,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"9c74b9396ff6b33e7a7622e59aa1f46103d993fe",
        "model_name_for_query":"Charlie911\/vicuna-7b-v1.5-lora-mixed-datasets"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":43.51,
        "ARC":51.37,
        "HellaSwag":78.47,
        "MMLU":45.53,
        "TruthfulQA":45.01,
        "Winogrande":72.85,
        "GSM8K":4.7,
        "DROP":6.64,
        "Type":"instruction-tuned",
        "Precision":"8bit",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":111.0,
        "Available on the hub":true,
        "Model sha":"b050a6f17d46e32c4b90a30492f14746589f74b7",
        "model_name_for_query":"togethercomputer\/Llama-2-7B-32K-Instruct"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.51,
        "ARC":55.03,
        "HellaSwag":77.84,
        "MMLU":40.92,
        "TruthfulQA":44.02,
        "Winogrande":73.72,
        "GSM8K":6.97,
        "DROP":6.04,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"f1a9e8d91e5b636cde3ea7fcf752a9f0234bd92a",
        "model_name_for_query":"wenge-research\/yayi-7b-llama2"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":43.49,
        "ARC":51.45,
        "HellaSwag":78.63,
        "MMLU":43.6,
        "TruthfulQA":43.71,
        "Winogrande":74.43,
        "GSM8K":6.6,
        "DROP":5.99,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"c7e776f3f3afc0fa22cb7aff0d00522e571e9b29",
        "model_name_for_query":"lgaalves\/llama-2-7b-hf_open-platypus"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":43.48,
        "ARC":53.16,
        "HellaSwag":78.25,
        "MMLU":47.07,
        "TruthfulQA":39.08,
        "Winogrande":73.24,
        "GSM8K":7.88,
        "DROP":5.71,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":32.0,
        "Available on the hub":true,
        "Model sha":"48fa08b3098a23d3671e09565499a4cfbaff1923",
        "model_name_for_query":"elyza\/ELYZA-japanese-Llama-2-7b-instruct"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.48,
        "ARC":53.24,
        "HellaSwag":78.72,
        "MMLU":46.57,
        "TruthfulQA":38.75,
        "Winogrande":73.88,
        "GSM8K":7.58,
        "DROP":5.64,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"a1fbc4d8a2c1a3d211325bdff9e7f0539fa7a2b1",
        "model_name_for_query":"TinyPixel\/testmodel-3"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.48,
        "ARC":42.92,
        "HellaSwag":73.97,
        "MMLU":48.49,
        "TruthfulQA":40.43,
        "Winogrande":69.69,
        "GSM8K":0.68,
        "DROP":28.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"8e1930bbbbdeb4f6f4639e837f09d9878bbf7831",
        "model_name_for_query":"Charlie911\/vicuna-7b-v1.5-lora-mctaco-modified2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.47,
        "ARC":42.06,
        "HellaSwag":62.01,
        "MMLU":46.53,
        "TruthfulQA":45.18,
        "Winogrande":65.04,
        "GSM8K":10.84,
        "DROP":32.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.63,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"bb94ff691996484b1a9d899a6c0956ef6750d86a",
        "model_name_for_query":"OpenBuddy\/openbuddy-openllama-7b-v12-bf16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.47,
        "ARC":53.41,
        "HellaSwag":78.56,
        "MMLU":46.43,
        "TruthfulQA":38.71,
        "Winogrande":74.03,
        "GSM8K":7.51,
        "DROP":5.61,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":false,
        "Model sha":"16c279c5e7d12b8a6ff7771881808ef253a406b9",
        "model_name_for_query":"undi95\/llama2-to-mistral-diff"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.47,
        "ARC":53.41,
        "HellaSwag":78.56,
        "MMLU":46.43,
        "TruthfulQA":38.71,
        "Winogrande":74.03,
        "GSM8K":7.51,
        "DROP":5.61,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"405c54ec7aea0735996ef5ff6ede6c35ab930381",
        "model_name_for_query":"TheTravellingEngineer\/llama2-7b-chat-hf-v4"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.46,
        "ARC":55.12,
        "HellaSwag":79.6,
        "MMLU":45.17,
        "TruthfulQA":40.29,
        "Winogrande":74.27,
        "GSM8K":2.81,
        "DROP":6.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":true,
        "Model sha":"77bdd1f049f27876c38b68782fc240518208f391",
        "model_name_for_query":"jondurbin\/airoboros-l2-7b-gpt4-1.4.1"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":43.43,
        "ARC":53.07,
        "HellaSwag":78.59,
        "MMLU":46.87,
        "TruthfulQA":38.76,
        "Winogrande":74.03,
        "GSM8K":7.13,
        "DROP":5.59,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":679.0,
        "Available on the hub":false,
        "Model sha":"e8f058fa738b6b308540024e9aa12e274e291f75",
        "model_name_for_query":"meta-llama\/Llama-2-7b-hf"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.43,
        "ARC":53.07,
        "HellaSwag":78.57,
        "MMLU":46.86,
        "TruthfulQA":38.75,
        "Winogrande":74.03,
        "GSM8K":7.13,
        "DROP":5.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"69a4886f51ed752216cdd7f41a584d14240126f9",
        "model_name_for_query":"yeen214\/test_llama2_7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.43,
        "ARC":53.07,
        "HellaSwag":78.57,
        "MMLU":46.86,
        "TruthfulQA":38.75,
        "Winogrande":74.03,
        "GSM8K":7.13,
        "DROP":5.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"ebe2e68699cb7ab6bb22688f265c89be2ac0fa6d",
        "model_name_for_query":"bongchoi\/test-llama2-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.42,
        "ARC":53.07,
        "HellaSwag":78.57,
        "MMLU":46.8,
        "TruthfulQA":38.75,
        "Winogrande":74.03,
        "GSM8K":7.13,
        "DROP":5.6,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"27c84ef23d850582453e1cc2dcea13de48da090f",
        "model_name_for_query":"TaylorAI\/Flash-Llama-7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.42,
        "ARC":53.07,
        "HellaSwag":78.57,
        "MMLU":46.8,
        "TruthfulQA":38.75,
        "Winogrande":74.03,
        "GSM8K":7.13,
        "DROP":5.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"7fe54f507e762b0f62265813aef908765b1298c0",
        "model_name_for_query":"ibranze\/araproje-llama2-7b-hf"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.42,
        "ARC":53.07,
        "HellaSwag":78.57,
        "MMLU":46.8,
        "TruthfulQA":38.75,
        "Winogrande":74.03,
        "GSM8K":7.13,
        "DROP":5.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"405c54ec7aea0735996ef5ff6ede6c35ab930381",
        "model_name_for_query":"TheTravellingEngineer\/llama2-7b-chat-hf-v4"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.42,
        "ARC":53.07,
        "HellaSwag":78.57,
        "MMLU":46.8,
        "TruthfulQA":38.75,
        "Winogrande":74.03,
        "GSM8K":7.13,
        "DROP":5.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"1c97650d4b919e2c6a2829778caa3a109935a58c",
        "model_name_for_query":"TheTravellingEngineer\/llama2-7b-chat-hf-v2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.42,
        "ARC":53.07,
        "HellaSwag":78.57,
        "MMLU":46.8,
        "TruthfulQA":38.75,
        "Winogrande":74.03,
        "GSM8K":7.13,
        "DROP":5.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"1f7436c458ebc3d8d31b91091c1a7a48e942cd3b",
        "model_name_for_query":"NewstaR\/Starlight-7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.42,
        "ARC":52.99,
        "HellaSwag":78.62,
        "MMLU":46.87,
        "TruthfulQA":38.67,
        "Winogrande":74.35,
        "GSM8K":6.82,
        "DROP":5.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":false,
        "Model sha":"67f2e8af850049a86fb9ee8ef581deb0f51e58e6",
        "model_name_for_query":"ToolBench\/ToolLLaMA-7b-LoRA"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.4,
        "ARC":53.67,
        "HellaSwag":77.83,
        "MMLU":46.58,
        "TruthfulQA":38.82,
        "Winogrande":75.22,
        "GSM8K":5.69,
        "DROP":5.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":13.0,
        "Available on the hub":true,
        "Model sha":"b62f431c88b232204ea7046f9d906ae1daa68437",
        "model_name_for_query":"clibrain\/Llama-2-7b-ft-instruct-es"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.39,
        "ARC":43.52,
        "HellaSwag":71.12,
        "MMLU":46.87,
        "TruthfulQA":42.45,
        "Winogrande":66.85,
        "GSM8K":5.53,
        "DROP":27.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":61.0,
        "Available on the hub":true,
        "Model sha":"6cdb9e75cd473e31e87067c2a0b646083247d9ab",
        "model_name_for_query":"fireballoon\/baichuan-vicuna-chinese-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.39,
        "ARC":47.35,
        "HellaSwag":69.97,
        "MMLU":44.12,
        "TruthfulQA":42.87,
        "Winogrande":65.67,
        "GSM8K":0.0,
        "DROP":33.74,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"ccbd599ac46bcfbf7020be393afeecef404bce2b",
        "model_name_for_query":"JosephusCheung\/Qwen-VL-LLaMAfied-7B-Chat"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.39,
        "ARC":53.58,
        "HellaSwag":78.66,
        "MMLU":44.49,
        "TruthfulQA":41.34,
        "Winogrande":74.11,
        "GSM8K":5.84,
        "DROP":5.69,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":152.0,
        "Available on the hub":true,
        "Model sha":"e9a972b12c6b59bfbcf30fe3779c2c933ce755bd",
        "model_name_for_query":"georgesung\/llama2_7b_chat_uncensored"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":43.35,
        "ARC":52.82,
        "HellaSwag":78.66,
        "MMLU":43.61,
        "TruthfulQA":42.21,
        "Winogrande":71.98,
        "GSM8K":6.22,
        "DROP":7.97,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":25.0,
        "Available on the hub":true,
        "Model sha":"bdac596568769d1ba4af8df9a611eee9723adf29",
        "model_name_for_query":"TehVenom\/Pygmalion-Vicuna-1.1-7b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":43.32,
        "ARC":53.24,
        "HellaSwag":80.6,
        "MMLU":43.22,
        "TruthfulQA":44.74,
        "Winogrande":69.93,
        "GSM8K":3.87,
        "DROP":7.63,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"6a1aa59cb7624f059728840ce68b20b1070ebdcb",
        "model_name_for_query":"heegyu\/LIMA2-7b-hf"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.3,
        "ARC":51.79,
        "HellaSwag":76.43,
        "MMLU":46.18,
        "TruthfulQA":40.08,
        "Winogrande":72.53,
        "GSM8K":10.31,
        "DROP":5.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"f35ae37b436637cd3e14d086324ccdaccfd69045",
        "model_name_for_query":"dotvignesh\/perry-7b"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":43.29,
        "ARC":50.26,
        "HellaSwag":76.1,
        "MMLU":45.27,
        "TruthfulQA":46.25,
        "Winogrande":71.51,
        "GSM8K":7.66,
        "DROP":5.99,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"f44998432fb90d88094ddf42e57ec458877a197f",
        "model_name_for_query":"quantumaikr\/QuantumLM-7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.26,
        "ARC":50.94,
        "HellaSwag":75.56,
        "MMLU":43.78,
        "TruthfulQA":41.96,
        "Winogrande":69.14,
        "GSM8K":14.71,
        "DROP":6.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"c5072a762070c6b3756385c63805348c155004b5",
        "model_name_for_query":"rameshm\/llama-2-13b-mathgpt-v4"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.25,
        "ARC":53.41,
        "HellaSwag":77.9,
        "MMLU":43.56,
        "TruthfulQA":40.81,
        "Winogrande":74.59,
        "GSM8K":5.08,
        "DROP":7.37,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"6ca41503b383c654aee8d5496e70fbdfaa33db10",
        "model_name_for_query":"dhmeltzer\/llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.22,
        "ARC":52.13,
        "HellaSwag":78.55,
        "MMLU":46.25,
        "TruthfulQA":37.69,
        "Winogrande":74.98,
        "GSM8K":6.22,
        "DROP":6.69,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"aed968a4b3f3b716064eb8b50c5ae24b38007627",
        "model_name_for_query":"qualis2006\/llama-2-7b-int4-python-code-18k"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":43.18,
        "ARC":52.56,
        "HellaSwag":77.61,
        "MMLU":45.58,
        "TruthfulQA":44.89,
        "Winogrande":69.93,
        "GSM8K":5.16,
        "DROP":6.53,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"7c343a501f5cd3b768d2f78d9941b760fd66815d",
        "model_name_for_query":"LeoLM\/leo-hessianai-7b-chat"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.17,
        "ARC":47.35,
        "HellaSwag":75.88,
        "MMLU":43.84,
        "TruthfulQA":30.16,
        "Winogrande":68.75,
        "GSM8K":0.0,
        "DROP":36.21,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"679c6cb9e869df686b1ae415ed440e6cfc05f80b",
        "model_name_for_query":"luffycodes\/llama-shishya-7b-ep3-v2"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":43.17,
        "ARC":50.6,
        "HellaSwag":78.22,
        "MMLU":42.73,
        "TruthfulQA":50.72,
        "Winogrande":66.14,
        "GSM8K":4.62,
        "DROP":9.14,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":15.52,
        "Hub \u2764\ufe0f":100.0,
        "Available on the hub":true,
        "Model sha":"845e9e4452dd4440760b3d5f680400fc014e91b5",
        "model_name_for_query":"openchat\/opencoderplus"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.15,
        "ARC":53.84,
        "HellaSwag":79.63,
        "MMLU":48.93,
        "TruthfulQA":38.73,
        "Winogrande":73.8,
        "GSM8K":0.0,
        "DROP":7.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"102f9fdad903f5eaffe1ed8173ae56081072e429",
        "model_name_for_query":"PocketDoc\/Dans-RetroRodeo-13b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":43.14,
        "ARC":44.37,
        "HellaSwag":65.2,
        "MMLU":43.46,
        "TruthfulQA":45.94,
        "Winogrande":64.01,
        "GSM8K":5.99,
        "DROP":33.06,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"6fdfeabe817235df3d560a6e6465c3722bc3a4ba",
        "model_name_for_query":"uukuguy\/speechless-codellama-orca-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.13,
        "ARC":52.13,
        "HellaSwag":78.77,
        "MMLU":43.42,
        "TruthfulQA":44.45,
        "Winogrande":73.09,
        "GSM8K":4.25,
        "DROP":5.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"db068e363e66e5d4b131e1d7a42a3a849e406a9b",
        "model_name_for_query":"Fredithefish\/Guanaco-7B-Uncensored"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.06,
        "ARC":51.71,
        "HellaSwag":74.97,
        "MMLU":43.16,
        "TruthfulQA":44.42,
        "Winogrande":68.67,
        "GSM8K":4.78,
        "DROP":13.73,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":44.0,
        "Available on the hub":true,
        "Model sha":"16deb633ef4d6a18d5750239edc5a85ffeaf3918",
        "model_name_for_query":"lmsys\/longchat-7b-v1.5-32k"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":43.05,
        "ARC":49.49,
        "HellaSwag":75.88,
        "MMLU":46.58,
        "TruthfulQA":49.31,
        "Winogrande":69.38,
        "GSM8K":4.25,
        "DROP":6.49,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"2147983e9493347c3424c07403f65e7a81c0b19f",
        "model_name_for_query":"FelixChao\/vicuna-7B-physics"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":42.99,
        "ARC":49.83,
        "HellaSwag":77.33,
        "MMLU":44.41,
        "TruthfulQA":47.96,
        "Winogrande":71.74,
        "GSM8K":3.87,
        "DROP":5.81,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":false,
        "Model sha":"0ae2abdc539a79ad84b141f894d614adf3754882",
        "model_name_for_query":"venkycs\/llama-v2-7b-32kC-Security"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":42.96,
        "ARC":53.75,
        "HellaSwag":77.55,
        "MMLU":46.85,
        "TruthfulQA":38.84,
        "Winogrande":71.59,
        "GSM8K":6.29,
        "DROP":5.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.66,
        "Hub \u2764\ufe0f":42.0,
        "Available on the hub":true,
        "Model sha":"89de33d1ad568855853196802aeaecd799c6586f",
        "model_name_for_query":"elyza\/ELYZA-japanese-Llama-2-7b-fast-instruct"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":42.94,
        "ARC":53.92,
        "HellaSwag":80.33,
        "MMLU":38.61,
        "TruthfulQA":41.05,
        "Winogrande":72.77,
        "GSM8K":3.71,
        "DROP":10.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"cae1ab8991f66bbe66ae95ed23a87846e7343047",
        "model_name_for_query":"jondurbin\/airoboros-7b-gpt4-1.4"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":42.89,
        "ARC":52.47,
        "HellaSwag":78.68,
        "MMLU":45.9,
        "TruthfulQA":37.9,
        "Winogrande":73.56,
        "GSM8K":2.5,
        "DROP":9.24,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"717edbee98945192b1a396fc9c337c5b32d6c79c",
        "model_name_for_query":"llm-agents\/tora-7b-v1.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":42.84,
        "ARC":52.39,
        "HellaSwag":75.39,
        "MMLU":39.77,
        "TruthfulQA":42.89,
        "Winogrande":71.19,
        "GSM8K":5.91,
        "DROP":12.3,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"81d424a431ab7fa4ff725925b6d0e4269d4563e4",
        "model_name_for_query":"vibhorag101\/llama-2-7b-chat-hf-phr_mental_health-2048"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":42.83,
        "ARC":49.57,
        "HellaSwag":73.45,
        "MMLU":54.86,
        "TruthfulQA":37.54,
        "Winogrande":70.72,
        "GSM8K":7.81,
        "DROP":5.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.99,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"dc5bda435771212fc73a8c6556fbdf4fcd87f96d",
        "model_name_for_query":"hiyouga\/Baichuan2-7B-Base-LLaMAfied"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":42.81,
        "ARC":52.13,
        "HellaSwag":75.71,
        "MMLU":48.05,
        "TruthfulQA":38.76,
        "Winogrande":71.51,
        "GSM8K":8.11,
        "DROP":5.39,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"0e6d1edd87c8753b55d280179c8fb0e65ebf5fa2",
        "model_name_for_query":"itsliupeng\/llama2_7b_code"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":42.79,
        "ARC":51.45,
        "HellaSwag":76.92,
        "MMLU":33.35,
        "TruthfulQA":48.13,
        "Winogrande":68.9,
        "GSM8K":3.26,
        "DROP":17.51,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":15.0,
        "Available on the hub":true,
        "Model sha":"c03ac527360663d17bb142405251028eec843ed9",
        "model_name_for_query":"notstoic\/PygmalionCoT-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":42.74,
        "ARC":48.81,
        "HellaSwag":74.63,
        "MMLU":49.58,
        "TruthfulQA":42.48,
        "Winogrande":72.3,
        "GSM8K":4.47,
        "DROP":6.91,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":34.0,
        "Available on the hub":true,
        "Model sha":"a7073a0f5142ce04aaa1603b0812b358f62a8de8",
        "model_name_for_query":"GOAT-AI\/GOAT-7B-Community"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":42.74,
        "ARC":54.35,
        "HellaSwag":78.06,
        "MMLU":45.35,
        "TruthfulQA":37.11,
        "Winogrande":73.4,
        "GSM8K":4.62,
        "DROP":6.28,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"684c4f4612fadae47c2c7db9fe9e9be4aaafc7e2",
        "model_name_for_query":"dhmeltzer\/llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":42.67,
        "ARC":53.07,
        "HellaSwag":77.74,
        "MMLU":43.8,
        "TruthfulQA":38.98,
        "Winogrande":74.59,
        "GSM8K":5.38,
        "DROP":5.13,
        "Type":"",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9",
        "model_name_for_query":"meta-llama\/Llama-2-7b-hf"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":42.65,
        "ARC":52.22,
        "HellaSwag":76.78,
        "MMLU":45.89,
        "TruthfulQA":38.38,
        "Winogrande":73.4,
        "GSM8K":6.22,
        "DROP":5.66,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"a5269bc93a7f98e192e34553cec1302877ca4327",
        "model_name_for_query":"TheTravellingEngineer\/llama2-7b-chat-hf-v3"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":42.65,
        "ARC":40.87,
        "HellaSwag":73.4,
        "MMLU":47.42,
        "TruthfulQA":39.87,
        "Winogrande":69.46,
        "GSM8K":1.29,
        "DROP":26.25,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"a7749ff092ef03900de34b69d41c767a6a48ea9e",
        "model_name_for_query":"Charlie911\/vicuna-7b-v1.5-lora-mctaco-modified1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":42.62,
        "ARC":50.34,
        "HellaSwag":77.91,
        "MMLU":32.35,
        "TruthfulQA":35.08,
        "Winogrande":70.48,
        "GSM8K":2.81,
        "DROP":29.4,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-3.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":437.0,
        "Available on the hub":true,
        "Model sha":"925e0d80e50e77aaddaf9c3ced41ca4ea23a1025",
        "model_name_for_query":"mosaicml\/mpt-7b-instruct"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":42.61,
        "ARC":50.43,
        "HellaSwag":75.54,
        "MMLU":46.78,
        "TruthfulQA":39.66,
        "Winogrande":68.19,
        "GSM8K":13.12,
        "DROP":4.58,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"cbb33eea774cc03d4363c424d81e8c9d58332274",
        "model_name_for_query":"llm-agents\/tora-code-34b-v1.0"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":42.58,
        "ARC":51.02,
        "HellaSwag":76.03,
        "MMLU":44.68,
        "TruthfulQA":47.16,
        "Winogrande":70.72,
        "GSM8K":2.73,
        "DROP":5.72,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"5ee98fd03b310e3081f0c9986c5153b27ec5dce6",
        "model_name_for_query":"LeoLM\/leo-hessianai-7b-chat-bilingual"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":42.57,
        "ARC":48.04,
        "HellaSwag":76.28,
        "MMLU":47.42,
        "TruthfulQA":44.4,
        "Winogrande":70.09,
        "GSM8K":6.22,
        "DROP":5.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"e4b19d9d6168b32402da4ab2b5ec7ff27cf40d9b",
        "model_name_for_query":"Aspik101\/vicuna-7b-v1.3-instruct-pl-lora_unload"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":42.55,
        "ARC":44.88,
        "HellaSwag":67.7,
        "MMLU":43.16,
        "TruthfulQA":40.88,
        "Winogrande":66.14,
        "GSM8K":1.82,
        "DROP":33.26,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"f01d3ab70cc23e31dcf5d6418406b08dc2003153",
        "model_name_for_query":"speechlessai\/speechless-codellama-airoboros-orca-platypus-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":42.54,
        "ARC":40.7,
        "HellaSwag":73.08,
        "MMLU":47.26,
        "TruthfulQA":41.59,
        "Winogrande":67.88,
        "GSM8K":0.08,
        "DROP":27.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"715b03c8573df06f3825d1c08b307e2a83fa8bf9",
        "model_name_for_query":"Charlie911\/vicuna-7b-v1.5-lora-mctaco-modified4"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":42.53,
        "ARC":52.22,
        "HellaSwag":76.42,
        "MMLU":44.6,
        "TruthfulQA":37.92,
        "Winogrande":72.69,
        "GSM8K":8.34,
        "DROP":5.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":32.0,
        "Available on the hub":true,
        "Model sha":"976887c5891284db204320860bb84b71d598063e",
        "model_name_for_query":"elyza\/ELYZA-japanese-Llama-2-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":42.51,
        "ARC":51.96,
        "HellaSwag":76.7,
        "MMLU":45.36,
        "TruthfulQA":38.31,
        "Winogrande":73.56,
        "GSM8K":5.99,
        "DROP":5.7,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"a5269bc93a7f98e192e34553cec1302877ca4327",
        "model_name_for_query":"TheTravellingEngineer\/llama2-7b-chat-hf-v3"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":42.48,
        "ARC":53.33,
        "HellaSwag":78.9,
        "MMLU":48.09,
        "TruthfulQA":37.84,
        "Winogrande":73.32,
        "GSM8K":0.0,
        "DROP":5.9,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"efc7cbc5d0461c137e8ea0c83e54bc5357188783",
        "model_name_for_query":"PocketDoc\/Dans-CreepingSenseOfDoom"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":42.46,
        "ARC":49.23,
        "HellaSwag":70.48,
        "MMLU":38.39,
        "TruthfulQA":39.72,
        "Winogrande":70.09,
        "GSM8K":0.68,
        "DROP":28.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.68,
        "Hub \u2764\ufe0f":46.0,
        "Available on the hub":true,
        "Model sha":"0deb5a13732f1e3e3240ea83f403c57283fe2dc8",
        "model_name_for_query":"shibing624\/chinese-alpaca-plus-7b-hf"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":42.44,
        "ARC":48.04,
        "HellaSwag":73.25,
        "MMLU":35.04,
        "TruthfulQA":39.92,
        "Winogrande":70.17,
        "GSM8K":6.22,
        "DROP":24.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.64,
        "Hub \u2764\ufe0f":15.0,
        "Available on the hub":true,
        "Model sha":"a2d55220b3d0693825fe69e1174653dc6cc4a920",
        "model_name_for_query":"Linly-AI\/Chinese-LLaMA-2-7B-hf"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":42.4,
        "ARC":49.83,
        "HellaSwag":74.42,
        "MMLU":44.1,
        "TruthfulQA":51.7,
        "Winogrande":67.17,
        "GSM8K":3.34,
        "DROP":6.26,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"fbf6476ebfa856ffe743e41f8d4413c15b2127c9",
        "model_name_for_query":"FelixChao\/vicuna-7B-chemical"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":42.33,
        "ARC":49.83,
        "HellaSwag":75.5,
        "MMLU":39.1,
        "TruthfulQA":45.74,
        "Winogrande":71.59,
        "GSM8K":4.17,
        "DROP":10.42,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"175965f50907c6a8cd40f1a4b10d28342969c066",
        "model_name_for_query":"jphme\/orca_mini_v2_ger_7b"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":42.33,
        "ARC":52.05,
        "HellaSwag":77.59,
        "MMLU":43.99,
        "TruthfulQA":39.32,
        "Winogrande":72.93,
        "GSM8K":5.0,
        "DROP":5.45,
        "Type":"pretrained",
        "Precision":"GPTQ",
        "Hub License":"llama2",
        "#Params (B)":9.05,
        "Hub \u2764\ufe0f":58.0,
        "Available on the hub":true,
        "Model sha":"ecd7ab9f6adc36ecbe0d751eeea0d90ae1863c3b",
        "model_name_for_query":"TheBloke\/Llama-2-7B-GPTQ"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":42.3,
        "ARC":55.03,
        "HellaSwag":78.79,
        "MMLU":37.5,
        "TruthfulQA":41.53,
        "Winogrande":72.69,
        "GSM8K":4.55,
        "DROP":6.02,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"bbece5e3f8ee9be09c8defc536a95c6ef780c681",
        "model_name_for_query":"LLMs\/AlpacaGPT4-7B-elina"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":42.3,
        "ARC":52.47,
        "HellaSwag":77.98,
        "MMLU":41.97,
        "TruthfulQA":35.73,
        "Winogrande":72.3,
        "GSM8K":0.99,
        "DROP":14.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"7b5f77827636bbf3174c48ca16e774c89d71d7bd",
        "model_name_for_query":"jondurbin\/airoboros-7b-gpt4-1.3"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":42.2,
        "ARC":51.37,
        "HellaSwag":77.74,
        "MMLU":41.34,
        "TruthfulQA":41.21,
        "Winogrande":73.32,
        "GSM8K":4.85,
        "DROP":5.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"05ec8f4a568777e1e543acdf8a587e080fb18fba",
        "model_name_for_query":"Juniplayground\/Mist_LLaMA-2-7B-1024_V3"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":42.13,
        "ARC":53.07,
        "HellaSwag":76.16,
        "MMLU":33.63,
        "TruthfulQA":45.07,
        "Winogrande":70.8,
        "GSM8K":3.56,
        "DROP":12.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":11.0,
        "Available on the hub":true,
        "Model sha":"53887996c0f17f7711d182537505a895fb404542",
        "model_name_for_query":"jondurbin\/airoboros-gpt-3.5-turbo-100k-7b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":42.09,
        "ARC":53.41,
        "HellaSwag":78.57,
        "MMLU":50.37,
        "TruthfulQA":48.36,
        "Winogrande":56.99,
        "GSM8K":0.0,
        "DROP":6.94,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"gpl-3.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"51f3d9eaa71de287c96195abd0ff954839857b19",
        "model_name_for_query":"LLMs\/Stable-Vicuna-13B"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":42.05,
        "ARC":50.85,
        "HellaSwag":74.89,
        "MMLU":40.02,
        "TruthfulQA":47.23,
        "Winogrande":69.06,
        "GSM8K":3.87,
        "DROP":8.42,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"cdffb3488c5cb1a9aa5039a6b3bc72af24827db0",
        "model_name_for_query":"openthaigpt\/openthaigpt-1.0.0-alpha-7b-chat-ckpt-hf"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":42.01,
        "ARC":45.9,
        "HellaSwag":74.43,
        "MMLU":42.09,
        "TruthfulQA":35.13,
        "Winogrande":65.9,
        "GSM8K":10.08,
        "DROP":20.54,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-3.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":23.0,
        "Available on the hub":true,
        "Model sha":"736f68aceeb61298a5de3cf5ae81d0bc2697edf4",
        "model_name_for_query":"mosaicml\/mpt-7b-8k-instruct"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":41.99,
        "ARC":47.7,
        "HellaSwag":72.08,
        "MMLU":45.11,
        "TruthfulQA":42.27,
        "Winogrande":69.61,
        "GSM8K":10.84,
        "DROP":6.3,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.73,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"300831494aa1eb16e59799310a09531f60dcc904",
        "model_name_for_query":"TigerResearch\/tigerbot-7b-base"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":41.96,
        "ARC":52.3,
        "HellaSwag":77.09,
        "MMLU":41.6,
        "TruthfulQA":37.58,
        "Winogrande":69.46,
        "GSM8K":1.44,
        "DROP":14.23,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":249.0,
        "Available on the hub":true,
        "Model sha":"cc7773cac2478231807c56ef2f02292d98f85cf5",
        "model_name_for_query":"chavinlo\/alpaca-native"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":41.91,
        "ARC":54.1,
        "HellaSwag":78.47,
        "MMLU":34.98,
        "TruthfulQA":36.74,
        "Winogrande":70.48,
        "GSM8K":3.56,
        "DROP":15.02,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":13.0,
        "Available on the hub":true,
        "Model sha":"50e61b8e6cc17cb3fbcb490fe3dc7e2c8b248378",
        "model_name_for_query":"jerryjalapeno\/nart-100k-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":41.9,
        "ARC":47.87,
        "HellaSwag":73.08,
        "MMLU":35.42,
        "TruthfulQA":41.49,
        "Winogrande":68.43,
        "GSM8K":3.26,
        "DROP":23.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":339.0,
        "Available on the hub":true,
        "Model sha":"14c23f9fa775ab5ce49010418f00df06d92b0b13",
        "model_name_for_query":"ehartford\/WizardLM-7B-Uncensored"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":41.88,
        "ARC":46.59,
        "HellaSwag":67.52,
        "MMLU":48.37,
        "TruthfulQA":49.72,
        "Winogrande":63.77,
        "GSM8K":5.69,
        "DROP":11.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":64.0,
        "Available on the hub":true,
        "Model sha":"85aa4f67191fd016ab7ea8c389fddb5d9e5a9a52",
        "model_name_for_query":"ehartford\/dolphin-llama2-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":41.87,
        "ARC":53.92,
        "HellaSwag":78.13,
        "MMLU":32.98,
        "TruthfulQA":45.6,
        "Winogrande":72.61,
        "GSM8K":4.17,
        "DROP":5.66,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"d20419e1d9e9a6a59ced3edf5169e8e7b3e8394c",
        "model_name_for_query":"DevaMalla\/llama_7b_qlora_pds-eval"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":41.85,
        "ARC":49.49,
        "HellaSwag":72.67,
        "MMLU":43.85,
        "TruthfulQA":44.8,
        "Winogrande":69.69,
        "GSM8K":6.29,
        "DROP":6.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.76,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"9ef1045ca31f670d9cbf820af904b33a097cd787",
        "model_name_for_query":"golaxy\/goims"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":41.8,
        "ARC":48.04,
        "HellaSwag":77.62,
        "MMLU":41.88,
        "TruthfulQA":43.68,
        "Winogrande":71.03,
        "GSM8K":4.4,
        "DROP":5.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":26.0,
        "Available on the hub":true,
        "Model sha":"ef97b878a279cd1765fbed7b8321fb3cff1aa5b5",
        "model_name_for_query":"mosaicml\/mpt-7b-8k-chat"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":41.79,
        "ARC":49.06,
        "HellaSwag":75.71,
        "MMLU":33.76,
        "TruthfulQA":36.28,
        "Winogrande":71.51,
        "GSM8K":0.15,
        "DROP":26.04,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"be5cb84a84a859dd6e5e3efc4648d6d5d1a5d188",
        "model_name_for_query":"WeOpenML\/Alpaca-7B-v1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":41.72,
        "ARC":51.88,
        "HellaSwag":75.46,
        "MMLU":44.34,
        "TruthfulQA":36.45,
        "Winogrande":71.59,
        "GSM8K":6.29,
        "DROP":6.04,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.66,
        "Hub \u2764\ufe0f":15.0,
        "Available on the hub":true,
        "Model sha":"e326078aa122fb1c4973997952d7b8630071776a",
        "model_name_for_query":"elyza\/ELYZA-japanese-Llama-2-7b-fast"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":41.71,
        "ARC":51.96,
        "HellaSwag":75.84,
        "MMLU":42.85,
        "TruthfulQA":37.94,
        "Winogrande":72.14,
        "GSM8K":5.61,
        "DROP":5.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":22.0,
        "Available on the hub":true,
        "Model sha":"88c5ac07006ea8f1b5d10aa4f03f0d624dd27e56",
        "model_name_for_query":"LeoLM\/leo-hessianai-7b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":41.68,
        "ARC":23.29,
        "HellaSwag":78.46,
        "MMLU":42.33,
        "TruthfulQA":37.97,
        "Winogrande":75.53,
        "GSM8K":4.47,
        "DROP":29.66,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"792f946a1413a7c58378d7a350b7d75b9df80561",
        "model_name_for_query":"synapsoft\/Llama-2-7b-hf-flan2022-1.2M"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":41.66,
        "ARC":54.1,
        "HellaSwag":77.32,
        "MMLU":37.09,
        "TruthfulQA":39.96,
        "Winogrande":72.85,
        "GSM8K":4.4,
        "DROP":5.93,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":16.0,
        "Available on the hub":false,
        "Model sha":"e3eb8bb0d8840431afe24760d964f8ba57edd83e",
        "model_name_for_query":"project-baize\/baize-healthcare-lora-7B"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":41.62,
        "ARC":52.56,
        "HellaSwag":77.65,
        "MMLU":35.94,
        "TruthfulQA":42.13,
        "Winogrande":72.06,
        "GSM8K":5.08,
        "DROP":5.89,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"1665b271316dfee05b2a8daf8b9d6c22ed0aef60",
        "model_name_for_query":"AlpinDale\/pygmalion-instruct"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":41.6,
        "ARC":52.9,
        "HellaSwag":63.79,
        "MMLU":43.89,
        "TruthfulQA":40.89,
        "Winogrande":72.22,
        "GSM8K":12.43,
        "DROP":5.04,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"other",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":916.0,
        "Available on the hub":true,
        "Model sha":"ea95720a352172db6fcbcd89032bfb1cb8481797",
        "model_name_for_query":"microsoft\/phi-1_5"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":41.58,
        "ARC":48.72,
        "HellaSwag":77.3,
        "MMLU":43.72,
        "TruthfulQA":37.85,
        "Winogrande":70.01,
        "GSM8K":8.04,
        "DROP":5.43,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":15.52,
        "Hub \u2764\ufe0f":181.0,
        "Available on the hub":false,
        "Model sha":"95be82087c33f14ee9941c812a154a9dd66efe72",
        "model_name_for_query":"bigcode\/starcoderplus"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":41.53,
        "ARC":55.12,
        "HellaSwag":78.07,
        "MMLU":35.91,
        "TruthfulQA":39.71,
        "Winogrande":72.53,
        "GSM8K":3.18,
        "DROP":6.15,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":false,
        "Model sha":"d58c62ee27cae60392bd0bd53e1fd05ea82e273b",
        "model_name_for_query":"aiplanet\/effi-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":41.51,
        "ARC":52.47,
        "HellaSwag":77.76,
        "MMLU":32.38,
        "TruthfulQA":46.14,
        "Winogrande":71.74,
        "GSM8K":4.09,
        "DROP":5.95,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"b6b5c65c5c1cce34d24c8f790bb0cc011e0f0808",
        "model_name_for_query":"DevaMalla\/llama_7b_qlora_cds"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":41.5,
        "ARC":53.67,
        "HellaSwag":78.62,
        "MMLU":35.91,
        "TruthfulQA":39.16,
        "Winogrande":72.53,
        "GSM8K":5.0,
        "DROP":5.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":12.0,
        "Available on the hub":true,
        "Model sha":"62ca156891feead8db117be8f5f35687b6274e6e",
        "model_name_for_query":"Neko-Institute-of-Science\/metharme-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":41.37,
        "ARC":52.99,
        "HellaSwag":80.05,
        "MMLU":35.32,
        "TruthfulQA":39.2,
        "Winogrande":71.43,
        "GSM8K":5.08,
        "DROP":5.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"293c24105fa15afa127a2ec3905fdc2a0a3a6dac",
        "model_name_for_query":"TheBloke\/guanaco-7B-HF"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":41.37,
        "ARC":51.19,
        "HellaSwag":75.23,
        "MMLU":43.75,
        "TruthfulQA":38.08,
        "Winogrande":72.06,
        "GSM8K":3.26,
        "DROP":5.99,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":434.0,
        "Available on the hub":true,
        "Model sha":"b6d7fde8392250730d24cc2fcfa3b7e5f9a03ce8",
        "model_name_for_query":"openlm-research\/open_llama_13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":41.32,
        "ARC":48.38,
        "HellaSwag":73.56,
        "MMLU":34.83,
        "TruthfulQA":35.82,
        "Winogrande":69.14,
        "GSM8K":0.0,
        "DROP":27.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"c5d09631c88ab5012b48187ecd90ae773cd4bbd9",
        "model_name_for_query":"hyunseoki\/ko-ref-llama2-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":41.32,
        "ARC":51.19,
        "HellaSwag":75.24,
        "MMLU":43.76,
        "TruthfulQA":38.4,
        "Winogrande":71.74,
        "GSM8K":2.96,
        "DROP":5.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"42ed3023ae1afe861f533570be881a03b10fc860",
        "model_name_for_query":"titan087\/OpenLlama13B-Guanaco"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":41.31,
        "ARC":50.85,
        "HellaSwag":77.36,
        "MMLU":35.91,
        "TruthfulQA":36.63,
        "Winogrande":71.9,
        "GSM8K":0.91,
        "DROP":15.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"7fe5cb1a7009fdade8dfcfec335527997a730fcf",
        "model_name_for_query":"WeOpenML\/PandaLM-Alpaca-7B-v1"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":41.3,
        "ARC":50.17,
        "HellaSwag":72.69,
        "MMLU":30.3,
        "TruthfulQA":37.64,
        "Winogrande":68.67,
        "GSM8K":0.0,
        "DROP":29.61,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"gpl-3.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":213.0,
        "Available on the hub":true,
        "Model sha":"bed6f3bd18f07a4a379525645cbd86d622b12836",
        "model_name_for_query":"JosephusCheung\/Guanaco"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":41.24,
        "ARC":45.82,
        "HellaSwag":69.36,
        "MMLU":45.05,
        "TruthfulQA":44.97,
        "Winogrande":66.93,
        "GSM8K":10.99,
        "DROP":5.53,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"40ff78ce37efcaf83718534c494829a573b9d719",
        "model_name_for_query":"FelixChao\/CodeLlama13B-Finetune-v1"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":41.21,
        "ARC":52.99,
        "HellaSwag":77.89,
        "MMLU":36.41,
        "TruthfulQA":37.75,
        "Winogrande":72.3,
        "GSM8K":4.32,
        "DROP":6.78,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"2123beec77083c414b2ae51dd25b7a870b0b936c",
        "model_name_for_query":"ausboss\/llama7b-wizardlm-unfiltered"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":41.13,
        "ARC":47.53,
        "HellaSwag":76.14,
        "MMLU":43.33,
        "TruthfulQA":39.23,
        "Winogrande":71.9,
        "GSM8K":4.32,
        "DROP":5.48,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":465.0,
        "Available on the hub":true,
        "Model sha":"aef6d8946ae1015bdb65c478a2dd73b58daaef47",
        "model_name_for_query":"togethercomputer\/LLaMA-2-7B-32K"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":41.08,
        "ARC":47.1,
        "HellaSwag":73.58,
        "MMLU":25.53,
        "TruthfulQA":45.96,
        "Winogrande":69.93,
        "GSM8K":3.64,
        "DROP":21.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":18.0,
        "Available on the hub":true,
        "Model sha":"d102fe3b68f1a5a50d547e4fd1c8b33b783c993b",
        "model_name_for_query":"TheBloke\/koala-7B-HF"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":41.06,
        "ARC":46.25,
        "HellaSwag":71.88,
        "MMLU":40.74,
        "TruthfulQA":39.89,
        "Winogrande":73.09,
        "GSM8K":0.53,
        "DROP":15.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.94,
        "Hub \u2764\ufe0f":17.0,
        "Available on the hub":true,
        "Model sha":"f17a52b8067d551a814069d2c710e1f5c487a3ce",
        "model_name_for_query":"shibing624\/chinese-llama-plus-13b-hf"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":41.01,
        "ARC":41.64,
        "HellaSwag":60.56,
        "MMLU":29.89,
        "TruthfulQA":58.18,
        "Winogrande":63.54,
        "GSM8K":6.29,
        "DROP":27.0,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.07,
        "Hub \u2764\ufe0f":13.0,
        "Available on the hub":false,
        "Model sha":"98b847905d63f74624e834db1ff95ee2814cbbd3",
        "model_name_for_query":"TigerResearch\/tigerbot-7b-sft"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":40.99,
        "ARC":49.57,
        "HellaSwag":58.2,
        "MMLU":43.78,
        "TruthfulQA":41.16,
        "Winogrande":62.51,
        "GSM8K":2.81,
        "DROP":28.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":29.97,
        "Hub \u2764\ufe0f":55.0,
        "Available on the hub":true,
        "Model sha":"a1f0c4bedd65b485a0d4d3a3bd60d7a4599f1eaf",
        "model_name_for_query":"GeorgiaTechResearchInstitute\/galpaca-30b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":40.97,
        "ARC":54.86,
        "HellaSwag":79.1,
        "MMLU":33.63,
        "TruthfulQA":34.74,
        "Winogrande":72.77,
        "GSM8K":5.53,
        "DROP":6.12,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"7f4cbd810b4bef0d75c1fd3f551146b4ea97d9fd",
        "model_name_for_query":"DevaMalla\/llama_7b_lora"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":40.9,
        "ARC":49.74,
        "HellaSwag":73.67,
        "MMLU":31.52,
        "TruthfulQA":34.65,
        "Winogrande":65.43,
        "GSM8K":0.53,
        "DROP":30.75,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-3.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":25.0,
        "Available on the hub":true,
        "Model sha":"fdf9f034163cce67e04d55172155f0e07b1b19a0",
        "model_name_for_query":"VMware\/open-llama-7b-open-instruct"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":40.87,
        "ARC":53.5,
        "HellaSwag":78.29,
        "MMLU":33.96,
        "TruthfulQA":40.29,
        "Winogrande":68.59,
        "GSM8K":2.35,
        "DROP":9.13,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"de4c7af9598bebc47dd43253c972be719f3195d6",
        "model_name_for_query":"ajibawa-2023\/carl-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":40.86,
        "ARC":48.98,
        "HellaSwag":75.06,
        "MMLU":39.6,
        "TruthfulQA":41.39,
        "Winogrande":71.11,
        "GSM8K":4.17,
        "DROP":5.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":23.0,
        "Available on the hub":true,
        "Model sha":"e4731c2c2671e2d0b47b5eba08c753ca21671fab",
        "model_name_for_query":"project-baize\/baize-v2-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":40.81,
        "ARC":46.16,
        "HellaSwag":68.88,
        "MMLU":44.55,
        "TruthfulQA":44.98,
        "Winogrande":66.14,
        "GSM8K":9.4,
        "DROP":5.54,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"7a771bd8899b9ef4ba9680e96f84dc85810a67d6",
        "model_name_for_query":"uukuguy\/speechless-codellama-platypus-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":40.74,
        "ARC":45.31,
        "HellaSwag":75.1,
        "MMLU":31.18,
        "TruthfulQA":40.6,
        "Winogrande":70.96,
        "GSM8K":0.0,
        "DROP":22.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":17.99,
        "Hub \u2764\ufe0f":15.0,
        "Available on the hub":true,
        "Model sha":"f2968552d2f522023f3289747234aea5508980e2",
        "model_name_for_query":"TheBloke\/BigTranslate-13B-GPTQ"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":40.73,
        "ARC":55.12,
        "HellaSwag":78.26,
        "MMLU":35.71,
        "TruthfulQA":33.98,
        "Winogrande":72.06,
        "GSM8K":4.55,
        "DROP":5.46,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"7f94b0be78193abc54722cf723541c3800426f7b",
        "model_name_for_query":"DevaMalla\/llama_7b_qlora"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":40.72,
        "ARC":52.73,
        "HellaSwag":77.89,
        "MMLU":38.77,
        "TruthfulQA":36.07,
        "Winogrande":70.32,
        "GSM8K":2.27,
        "DROP":6.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"91ffa900ed637cf5fd904d96e6985b6f7857ad64",
        "model_name_for_query":"jondurbin\/airoboros-7b-gpt4-1.4.1-qlora"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":40.71,
        "ARC":52.73,
        "HellaSwag":78.78,
        "MMLU":36.26,
        "TruthfulQA":33.71,
        "Winogrande":72.93,
        "GSM8K":4.55,
        "DROP":5.98,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"305683c1b95f6888b8668dbc6b56d9efa5d07fef",
        "model_name_for_query":"DevaMalla\/llama7b_alpaca_1gpu_bf16"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":40.69,
        "ARC":46.59,
        "HellaSwag":75.94,
        "MMLU":45.23,
        "TruthfulQA":37.2,
        "Winogrande":71.19,
        "GSM8K":3.34,
        "DROP":5.36,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":2.8,
        "Hub \u2764\ufe0f":192.0,
        "Available on the hub":false,
        "Model sha":"a4750ace0db6f08d7bbba0aa52a585f231ea3cde",
        "model_name_for_query":"stabilityai\/stablelm-3b-4e1t"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":40.69,
        "ARC":44.71,
        "HellaSwag":63.23,
        "MMLU":39.06,
        "TruthfulQA":47.08,
        "Winogrande":62.83,
        "GSM8K":1.29,
        "DROP":26.62,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.07,
        "Hub \u2764\ufe0f":44.0,
        "Available on the hub":true,
        "Model sha":"5ed4d9570e0f76e1becb05bf467a7b4ff7b66055",
        "model_name_for_query":"FreedomIntelligence\/phoenix-inst-chat-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":40.65,
        "ARC":46.5,
        "HellaSwag":75.51,
        "MMLU":37.62,
        "TruthfulQA":40.16,
        "Winogrande":68.43,
        "GSM8K":4.09,
        "DROP":12.2,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":485.0,
        "Available on the hub":true,
        "Model sha":"64e5c9c9fb53a8e89690c2dee75a5add37f7113e",
        "model_name_for_query":"mosaicml\/mpt-7b-chat"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":40.55,
        "ARC":48.55,
        "HellaSwag":74.82,
        "MMLU":38.68,
        "TruthfulQA":42.19,
        "Winogrande":69.69,
        "GSM8K":4.02,
        "DROP":5.92,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":true,
        "Model sha":"9fc1bc4409b9e71f54213245a91c2742fbf7b3d0",
        "model_name_for_query":"wenge-research\/yayi-13b-llama2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":40.53,
        "ARC":46.5,
        "HellaSwag":76.91,
        "MMLU":42.32,
        "TruthfulQA":40.33,
        "Winogrande":69.3,
        "GSM8K":2.05,
        "DROP":6.29,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"cabb47abd422a2d67161e2d038265ee23be45fb8",
        "model_name_for_query":"yhyhy3\/open_llama_7b_v2_med_instruct"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":40.39,
        "ARC":44.03,
        "HellaSwag":62.6,
        "MMLU":38.64,
        "TruthfulQA":44.34,
        "Winogrande":63.3,
        "GSM8K":0.53,
        "DROP":29.33,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":7.07,
        "Hub \u2764\ufe0f":9.0,
        "Available on the hub":true,
        "Model sha":"8c2dc302780fe320ee3428f3db2ee7ff3684dcef",
        "model_name_for_query":"cmarkea\/bloomz-7b1-mt-sft-chat"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":40.37,
        "ARC":52.3,
        "HellaSwag":77.49,
        "MMLU":36.61,
        "TruthfulQA":33.81,
        "Winogrande":72.06,
        "GSM8K":4.02,
        "DROP":6.31,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"d985610bef080473e40f01c53266083c5f0c3169",
        "model_name_for_query":"illuin\/test-custom-llama"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":40.34,
        "ARC":39.76,
        "HellaSwag":70.31,
        "MMLU":35.16,
        "TruthfulQA":39.53,
        "Winogrande":64.33,
        "GSM8K":7.43,
        "DROP":25.88,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"b8fbe09571a71603ab517fe897a1281005060b62",
        "model_name_for_query":"Vmware\/open-llama-7b-v2-open-instruct"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":40.31,
        "ARC":47.35,
        "HellaSwag":77.08,
        "MMLU":45.1,
        "TruthfulQA":36.46,
        "Winogrande":68.51,
        "GSM8K":2.58,
        "DROP":5.08,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":6.89,
        "Hub \u2764\ufe0f":36.0,
        "Available on the hub":true,
        "Model sha":"eb3b56fee1ad4b1efe6625bbbc7a277df8ab5b96",
        "model_name_for_query":"stabilityai\/stablelm-base-alpha-7b-v2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":40.29,
        "ARC":51.37,
        "HellaSwag":77.81,
        "MMLU":35.68,
        "TruthfulQA":34.54,
        "Winogrande":72.22,
        "GSM8K":4.62,
        "DROP":5.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":38.0,
        "Available on the hub":true,
        "Model sha":"6473f9996d758fde48a181f37cc5de575aff1606",
        "model_name_for_query":"Neko-Institute-of-Science\/pygmalion-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":40.26,
        "ARC":48.63,
        "HellaSwag":74.83,
        "MMLU":41.04,
        "TruthfulQA":39.08,
        "Winogrande":70.24,
        "GSM8K":3.26,
        "DROP":4.74,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.7,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"444c85ef809f8793d84b0813ab78bec50700cfcf",
        "model_name_for_query":"YeungNLP\/firefly-llama2-7b-pretrain"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":40.1,
        "ARC":42.66,
        "HellaSwag":65.16,
        "MMLU":38.56,
        "TruthfulQA":42.06,
        "Winogrande":62.9,
        "GSM8K":0.91,
        "DROP":28.48,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"f7b1f87a096045f1bba8f68c62e062102218717b",
        "model_name_for_query":"uukuguy\/speechless-tora-code-7b-v1.0"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":40.02,
        "ARC":46.93,
        "HellaSwag":73.51,
        "MMLU":44.34,
        "TruthfulQA":35.47,
        "Winogrande":65.35,
        "GSM8K":2.65,
        "DROP":11.88,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.26,
        "Hub \u2764\ufe0f":14.0,
        "Available on the hub":true,
        "Model sha":"407810f75698c95000dc0ae1a9a0457be625e972",
        "model_name_for_query":"Writer\/palmyra-med-20b"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":40.01,
        "ARC":42.49,
        "HellaSwag":63.01,
        "MMLU":37.85,
        "TruthfulQA":45.2,
        "Winogrande":64.64,
        "GSM8K":0.08,
        "DROP":26.8,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":7.07,
        "Hub \u2764\ufe0f":110.0,
        "Available on the hub":true,
        "Model sha":"2f4c4f3ebcf171dbbe2bae989ea2d2f3d3486a97",
        "model_name_for_query":"bigscience\/bloomz-7b1"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":40.01,
        "ARC":44.54,
        "HellaSwag":64.93,
        "MMLU":38.89,
        "TruthfulQA":45.88,
        "Winogrande":68.03,
        "GSM8K":12.66,
        "DROP":5.14,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":66.0,
        "Available on the hub":true,
        "Model sha":"b9f91b7351ecd589118d883afa23d5c93a38c612",
        "model_name_for_query":"codellama\/CodeLlama-13b-Instruct-hf"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":40.01,
        "ARC":44.62,
        "HellaSwag":64.94,
        "MMLU":38.77,
        "TruthfulQA":45.88,
        "Winogrande":68.03,
        "GSM8K":12.66,
        "DROP":5.14,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":28.0,
        "Available on the hub":true,
        "Model sha":"521c208c7251ccd3e44ccd9500b6bed419bca565",
        "model_name_for_query":"TheBloke\/CodeLlama-13B-Instruct-fp16"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":39.96,
        "ARC":45.31,
        "HellaSwag":68.63,
        "MMLU":42.82,
        "TruthfulQA":42.38,
        "Winogrande":65.59,
        "GSM8K":9.1,
        "DROP":5.91,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"81cb1bca46ce646b8339501537837e02116de1b8",
        "model_name_for_query":"uukuguy\/speechless-codellama-platypus-13b"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":39.96,
        "ARC":43.86,
        "HellaSwag":62.91,
        "MMLU":37.35,
        "TruthfulQA":45.65,
        "Winogrande":63.06,
        "GSM8K":0.0,
        "DROP":26.9,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":7.07,
        "Hub \u2764\ufe0f":120.0,
        "Available on the hub":true,
        "Model sha":"76875e6ea8df98157fb032c48ad6e354fd6a077b",
        "model_name_for_query":"bigscience\/bloomz-7b1-mt"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":39.93,
        "ARC":51.02,
        "HellaSwag":77.82,
        "MMLU":35.71,
        "TruthfulQA":34.33,
        "Winogrande":71.43,
        "GSM8K":3.56,
        "DROP":5.62,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"f356572651e58fb337d610470d4b36976e7fb802",
        "model_name_for_query":"huggingface\/llama-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":39.93,
        "ARC":51.02,
        "HellaSwag":77.82,
        "MMLU":35.71,
        "TruthfulQA":34.33,
        "Winogrande":71.43,
        "GSM8K":3.56,
        "DROP":5.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"afb4604a06c8541960fb51240259777764c4ce7e",
        "model_name_for_query":"TheBloke\/Planner-7B-fp16"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":39.91,
        "ARC":50.94,
        "HellaSwag":77.8,
        "MMLU":35.67,
        "TruthfulQA":34.34,
        "Winogrande":71.43,
        "GSM8K":3.56,
        "DROP":5.62,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"e01d89d8e444f7d751ea58feaf22ff8c9af69d2a",
        "model_name_for_query":"DevaMalla\/llama-base-7b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":39.78,
        "ARC":47.78,
        "HellaSwag":69.6,
        "MMLU":38.76,
        "TruthfulQA":43.97,
        "Winogrande":65.43,
        "GSM8K":7.81,
        "DROP":5.13,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"32ffc44ffdf1adfe2d8ef219327fbd534f3d5955",
        "model_name_for_query":"yeontaek\/WizardCoder-Python-13B-LoRa"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":39.73,
        "ARC":47.18,
        "HellaSwag":75.53,
        "MMLU":38.89,
        "TruthfulQA":38.48,
        "Winogrande":68.98,
        "GSM8K":2.65,
        "DROP":6.39,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"9af88449bed5be4709befcfbbba123ee75805479",
        "model_name_for_query":"ashercn97\/giraffe-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":39.52,
        "ARC":47.35,
        "HellaSwag":72.16,
        "MMLU":41.76,
        "TruthfulQA":41.49,
        "Winogrande":64.48,
        "GSM8K":3.41,
        "DROP":6.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"43f3de8bcef63eec03a1b00079c08b5932c1a429",
        "model_name_for_query":"jlevin\/guanaco-unchained-llama-2-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":39.48,
        "ARC":51.02,
        "HellaSwag":77.62,
        "MMLU":33.95,
        "TruthfulQA":33.53,
        "Winogrande":70.96,
        "GSM8K":3.71,
        "DROP":5.59,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"7872a492ebbb3c6a899f9acbd34dfd5f7e674fdd",
        "model_name_for_query":"Enno-Ai\/ennodata-7b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":39.47,
        "ARC":45.39,
        "HellaSwag":62.36,
        "MMLU":35.36,
        "TruthfulQA":45.02,
        "Winogrande":67.8,
        "GSM8K":13.19,
        "DROP":7.17,
        "Type":"instruction-tuned",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":42.0,
        "Available on the hub":true,
        "Model sha":"612dab2a8b2d77edb4fd36cfc28b3ffbbb20ffc1",
        "model_name_for_query":"OpenAssistant\/codellama-13b-oasst-sft-v10"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":39.43,
        "ARC":48.46,
        "HellaSwag":75.28,
        "MMLU":39.56,
        "TruthfulQA":34.49,
        "Winogrande":72.14,
        "GSM8K":1.97,
        "DROP":4.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.86,
        "Hub \u2764\ufe0f":85.0,
        "Available on the hub":true,
        "Model sha":"d5c58cc2cae21b4fb96aaad2658acc898ab22d99",
        "model_name_for_query":"beomi\/llama-2-ko-7b"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":39.37,
        "ARC":47.01,
        "HellaSwag":72.56,
        "MMLU":38.93,
        "TruthfulQA":43.63,
        "Winogrande":67.56,
        "GSM8K":0.0,
        "DROP":5.92,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"8e9c93c09e6a6c7d504c88d6ca598144829bced8",
        "model_name_for_query":"csitfun\/llama-7b-logicot"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":39.33,
        "ARC":46.67,
        "HellaSwag":67.67,
        "MMLU":28.55,
        "TruthfulQA":37.6,
        "Winogrande":65.43,
        "GSM8K":0.76,
        "DROP":28.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"75741b55ad462330e3498d1506f438f835152177",
        "model_name_for_query":"VMware\/open-llama-0.7T-7B-open-instruct-v1.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":39.21,
        "ARC":43.26,
        "HellaSwag":63.87,
        "MMLU":34.29,
        "TruthfulQA":48.97,
        "Winogrande":67.88,
        "GSM8K":10.77,
        "DROP":5.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"openrail",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":13.0,
        "Available on the hub":true,
        "Model sha":"675b3e35a9601683c2cb4ec7f1b11d2869842f36",
        "model_name_for_query":"shareAI\/CodeLLaMA-chat-13b-Chinese"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":39.16,
        "ARC":42.58,
        "HellaSwag":72.03,
        "MMLU":34.91,
        "TruthfulQA":36.85,
        "Winogrande":64.09,
        "GSM8K":0.0,
        "DROP":23.66,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"e80966ae720de9a844441a4a2bbc661106969915",
        "model_name_for_query":"Yukang\/LongAlpaca-13B"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":38.97,
        "ARC":43.52,
        "HellaSwag":72.83,
        "MMLU":35.18,
        "TruthfulQA":43.17,
        "Winogrande":66.46,
        "GSM8K":3.94,
        "DROP":7.7,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":20.26,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"3b7442b7e2240846bc9cfac545bd8861c1660aa2",
        "model_name_for_query":"Writer\/palmyra-20b-chat"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":38.96,
        "ARC":45.99,
        "HellaSwag":73.44,
        "MMLU":35.46,
        "TruthfulQA":39.92,
        "Winogrande":69.69,
        "GSM8K":2.5,
        "DROP":5.74,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":9.04,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"5dc039834e1ea42ac334458b2e3090fe3705cc59",
        "model_name_for_query":"TheBloke\/Project-Baize-v2-7B-GPTQ"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":38.77,
        "ARC":40.78,
        "HellaSwag":35.66,
        "MMLU":39.72,
        "TruthfulQA":44.29,
        "Winogrande":74.51,
        "GSM8K":31.01,
        "DROP":5.44,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.74,
        "Hub \u2764\ufe0f":154.0,
        "Available on the hub":true,
        "Model sha":"c109b9dde086b31725fa09ff7effdc04c03c033d",
        "model_name_for_query":"codellama\/CodeLlama-34b-Instruct-hf"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":38.74,
        "ARC":47.7,
        "HellaSwag":77.57,
        "MMLU":30.8,
        "TruthfulQA":33.44,
        "Winogrande":72.14,
        "GSM8K":4.02,
        "DROP":5.55,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":1083.0,
        "Available on the hub":true,
        "Model sha":"72e5f594ce36f9cabfa2a9fd8f58b491eb467ee7",
        "model_name_for_query":"mosaicml\/mpt-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":38.74,
        "ARC":47.7,
        "HellaSwag":77.57,
        "MMLU":30.8,
        "TruthfulQA":33.44,
        "Winogrande":72.14,
        "GSM8K":4.02,
        "DROP":5.55,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"b772e556c8e8a17d087db6935e7cd019e5eefb0f",
        "model_name_for_query":"anas-awadalla\/mpt-7b"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":38.72,
        "ARC":43.69,
        "HellaSwag":72.2,
        "MMLU":41.29,
        "TruthfulQA":35.54,
        "Winogrande":69.38,
        "GSM8K":3.49,
        "DROP":5.49,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":86.0,
        "Available on the hub":true,
        "Model sha":"e5961def23172a2384543940e773ab676033c963",
        "model_name_for_query":"openlm-research\/open_llama_7b_v2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":38.61,
        "ARC":44.62,
        "HellaSwag":62.56,
        "MMLU":33.81,
        "TruthfulQA":40.61,
        "Winogrande":62.9,
        "GSM8K":0.0,
        "DROP":25.77,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.07,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"8f9996f852db583b982efbd671465d18ad13ffae",
        "model_name_for_query":"golaxy\/gogpt-7b-bloom"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":38.37,
        "ARC":42.06,
        "HellaSwag":67.96,
        "MMLU":49.34,
        "TruthfulQA":38.89,
        "Winogrande":64.8,
        "GSM8K":1.21,
        "DROP":4.31,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"41bd1937dbc51f9e589d310bddab5b4c1409e783",
        "model_name_for_query":"togethercomputer\/GPT-JT-6B-v0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":38.36,
        "ARC":42.66,
        "HellaSwag":66.58,
        "MMLU":30.41,
        "TruthfulQA":38.62,
        "Winogrande":66.22,
        "GSM8K":0.0,
        "DROP":24.05,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"1ee08c79ae7393473754b77e82b1472ef63d5dd2",
        "model_name_for_query":"hyunseoki\/ko-ref-llama2-7b"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":38.27,
        "ARC":47.87,
        "HellaSwag":78.13,
        "MMLU":27.79,
        "TruthfulQA":34.26,
        "Winogrande":72.38,
        "GSM8K":2.65,
        "DROP":4.82,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.92,
        "Hub \u2764\ufe0f":885.0,
        "Available on the hub":true,
        "Model sha":"378337427557d1df3e742264a2901a49f25d4eb1",
        "model_name_for_query":"tiiuae\/falcon-7b"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":38.24,
        "ARC":28.84,
        "HellaSwag":26.08,
        "MMLU":24.62,
        "TruthfulQA":49.14,
        "Winogrande":76.32,
        "GSM8K":34.42,
        "DROP":28.29,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":35.58,
        "Hub \u2764\ufe0f":19.0,
        "Available on the hub":true,
        "Model sha":"e2e97475a9775d2fe7afba098aee37e694b9220f",
        "model_name_for_query":"TheBloke\/WizardLM-30B-GPTQ"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":38.23,
        "ARC":44.45,
        "HellaSwag":69.5,
        "MMLU":37.47,
        "TruthfulQA":37.0,
        "Winogrande":68.98,
        "GSM8K":1.44,
        "DROP":8.77,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.7,
        "Hub \u2764\ufe0f":64.0,
        "Available on the hub":true,
        "Model sha":"557b5cbd48a4a4eb5a08e975c4b6e11ac1ed4cbc",
        "model_name_for_query":"ziqingyang\/chinese-llama-2-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":37.99,
        "ARC":46.33,
        "HellaSwag":61.72,
        "MMLU":36.34,
        "TruthfulQA":43.7,
        "Winogrande":62.27,
        "GSM8K":0.91,
        "DROP":14.67,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":7.07,
        "Hub \u2764\ufe0f":28.0,
        "Available on the hub":true,
        "Model sha":"00be6c9e41a8367a855c6f18ebfa08f5ecdb2cc4",
        "model_name_for_query":"wenge-research\/yayi-7b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":37.98,
        "ARC":47.1,
        "HellaSwag":73.0,
        "MMLU":28.26,
        "TruthfulQA":41.81,
        "Winogrande":64.72,
        "GSM8K":2.58,
        "DROP":8.36,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.26,
        "Hub \u2764\ufe0f":36.0,
        "Available on the hub":true,
        "Model sha":"c78df447c70d4677b128b1df864b9fff8338d900",
        "model_name_for_query":"Writer\/InstructPalmyra-20b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":37.91,
        "ARC":40.87,
        "HellaSwag":63.35,
        "MMLU":32.81,
        "TruthfulQA":43.79,
        "Winogrande":67.17,
        "GSM8K":12.13,
        "DROP":5.25,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"b7cfbbce945b966607d15ae275704922a6d04afc",
        "model_name_for_query":"NousResearch\/CodeLlama-13b-hf"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":37.91,
        "ARC":40.87,
        "HellaSwag":63.35,
        "MMLU":32.81,
        "TruthfulQA":43.79,
        "Winogrande":67.17,
        "GSM8K":12.13,
        "DROP":5.25,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":48.0,
        "Available on the hub":true,
        "Model sha":"55876f398020b287ac845b34ca08089acf4f4bc3",
        "model_name_for_query":"codellama\/CodeLlama-13b-hf"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":37.76,
        "ARC":40.1,
        "HellaSwag":65.3,
        "MMLU":26.66,
        "TruthfulQA":36.79,
        "Winogrande":64.09,
        "GSM8K":1.82,
        "DROP":29.56,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.47,
        "Hub \u2764\ufe0f":68.0,
        "Available on the hub":true,
        "Model sha":"112a5ad9f556078ab14a5cd93511b9db4a0d4413",
        "model_name_for_query":"matsuo-lab\/weblab-10b-instruction-sft"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":37.74,
        "ARC":44.62,
        "HellaSwag":71.25,
        "MMLU":25.92,
        "TruthfulQA":41.93,
        "Winogrande":66.69,
        "GSM8K":2.12,
        "DROP":11.61,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.89,
        "Hub \u2764\ufe0f":47.0,
        "Available on the hub":true,
        "Model sha":"359c0649b4f1d10a26ebea32908035bc00d152ee",
        "model_name_for_query":"RWKV\/rwkv-raven-14b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":37.69,
        "ARC":40.78,
        "HellaSwag":35.66,
        "MMLU":39.72,
        "TruthfulQA":44.29,
        "Winogrande":74.51,
        "GSM8K":23.05,
        "DROP":5.78,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.74,
        "Hub \u2764\ufe0f":9.0,
        "Available on the hub":true,
        "Model sha":"a4d0ce949de4d5b5f74691641efb5b70736a32a8",
        "model_name_for_query":"TheBloke\/CodeLlama-34B-Instruct-fp16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":37.66,
        "ARC":46.59,
        "HellaSwag":72.28,
        "MMLU":33.4,
        "TruthfulQA":37.84,
        "Winogrande":67.09,
        "GSM8K":0.68,
        "DROP":5.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":14.0,
        "Available on the hub":true,
        "Model sha":"e303cf09e553c38ca5e0c0816d83631801ca5776",
        "model_name_for_query":"lyogavin\/Anima-7B-100K"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":37.62,
        "ARC":48.81,
        "HellaSwag":74.44,
        "MMLU":26.16,
        "TruthfulQA":36.89,
        "Winogrande":68.27,
        "GSM8K":2.65,
        "DROP":6.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":20.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"20b347273d90da7c2c9eb4c32d4173dba862a0d2",
        "model_name_for_query":"dvruette\/gpt-neox-20b-full-precision"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":37.6,
        "ARC":40.87,
        "HellaSwag":67.15,
        "MMLU":47.19,
        "TruthfulQA":37.07,
        "Winogrande":65.27,
        "GSM8K":1.21,
        "DROP":4.42,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":302.0,
        "Available on the hub":true,
        "Model sha":"f34aa35f906895602c1f86f5685e598afdea8051",
        "model_name_for_query":"togethercomputer\/GPT-JT-6B-v1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":37.59,
        "ARC":45.65,
        "HellaSwag":74.03,
        "MMLU":29.92,
        "TruthfulQA":34.51,
        "Winogrande":67.09,
        "GSM8K":6.9,
        "DROP":5.05,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.24,
        "Hub \u2764\ufe0f":691.0,
        "Available on the hub":true,
        "Model sha":"d386708e84d862a65f7d2b4989f64750cb657227",
        "model_name_for_query":"togethercomputer\/GPT-NeoXT-Chat-Base-20B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":37.53,
        "ARC":40.36,
        "HellaSwag":75.51,
        "MMLU":27.07,
        "TruthfulQA":32.83,
        "Winogrande":67.96,
        "GSM8K":0.0,
        "DROP":18.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.84,
        "Hub \u2764\ufe0f":13.0,
        "Available on the hub":true,
        "Model sha":"785793f6b216afd9fc664fc63e8e6c776a016825",
        "model_name_for_query":"KoboldAI\/fairseq-dense-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":37.52,
        "ARC":41.55,
        "HellaSwag":71.48,
        "MMLU":25.82,
        "TruthfulQA":35.73,
        "Winogrande":65.27,
        "GSM8K":0.68,
        "DROP":22.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"933fb9db10f131f7ea54f4e6024ed2acf41c711a",
        "model_name_for_query":"Xilabs\/calypso-3b-alpha-v2"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":37.47,
        "ARC":46.16,
        "HellaSwag":70.85,
        "MMLU":25.84,
        "TruthfulQA":44.08,
        "Winogrande":67.96,
        "GSM8K":1.9,
        "DROP":5.54,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.92,
        "Hub \u2764\ufe0f":676.0,
        "Available on the hub":true,
        "Model sha":"cf4b3c42ce2fdfe24f753f0f0d179202fea59c99",
        "model_name_for_query":"tiiuae\/falcon-7b-instruct"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":37.47,
        "ARC":40.02,
        "HellaSwag":62.55,
        "MMLU":30.37,
        "TruthfulQA":37.59,
        "Winogrande":62.35,
        "GSM8K":2.27,
        "DROP":27.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"c92bf022cddc3f57b4552ec3391df487295a2f87",
        "model_name_for_query":"heegyu\/RedTulu-Uncensored-3B-0719"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":37.44,
        "ARC":48.55,
        "HellaSwag":74.61,
        "MMLU":26.39,
        "TruthfulQA":35.63,
        "Winogrande":66.77,
        "GSM8K":3.11,
        "DROP":6.99,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":20.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"4aec11ef19103796fb21387ce925b63c9d61dae1",
        "model_name_for_query":"dvruette\/oasst-gpt-neox-20b-1000-steps"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":37.41,
        "ARC":41.98,
        "HellaSwag":64.06,
        "MMLU":28.2,
        "TruthfulQA":42.78,
        "Winogrande":64.72,
        "GSM8K":7.2,
        "DROP":12.94,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":250.0,
        "Available on the hub":true,
        "Model sha":"73c15208cb608be2949b7c6e4ba6d88f0176c267",
        "model_name_for_query":"nomic-ai\/gpt4all-j"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":37.41,
        "ARC":44.97,
        "HellaSwag":68.81,
        "MMLU":34.44,
        "TruthfulQA":41.39,
        "Winogrande":64.09,
        "GSM8K":0.61,
        "DROP":7.56,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":6.64,
        "Hub \u2764\ufe0f":34.0,
        "Available on the hub":true,
        "Model sha":"9b31bbf38a43d41eaf166fb3573f706b23cb1c13",
        "model_name_for_query":"JosephusCheung\/LL7M"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":37.39,
        "ARC":43.94,
        "HellaSwag":65.22,
        "MMLU":29.97,
        "TruthfulQA":42.03,
        "Winogrande":66.06,
        "GSM8K":0.38,
        "DROP":14.14,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":17.0,
        "Available on the hub":true,
        "Model sha":"6ed0dca683685cb5b9e7df599f87d311f00ba6db",
        "model_name_for_query":"psmathur\/orca_mini_7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":37.39,
        "ARC":47.35,
        "HellaSwag":65.81,
        "MMLU":31.59,
        "TruthfulQA":42.63,
        "Winogrande":68.03,
        "GSM8K":1.59,
        "DROP":4.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":false,
        "Model sha":"bf8bdcb0c30cceb0ceda33cf5fde683807e39a58",
        "model_name_for_query":"TheBloke\/landmark-attention-llama7b-fp16"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":37.38,
        "ARC":45.82,
        "HellaSwag":70.78,
        "MMLU":25.66,
        "TruthfulQA":44.07,
        "Winogrande":68.03,
        "GSM8K":1.74,
        "DROP":5.56,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.92,
        "Hub \u2764\ufe0f":676.0,
        "Available on the hub":true,
        "Model sha":"eb410fb6ffa9028e97adb801f0d6ec46d02f8b07",
        "model_name_for_query":"tiiuae\/falcon-7b-instruct"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":37.34,
        "ARC":48.04,
        "HellaSwag":72.76,
        "MMLU":25.96,
        "TruthfulQA":39.92,
        "Winogrande":66.3,
        "GSM8K":2.5,
        "DROP":5.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.24,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"1a5b8d25587eab67d837621a6c9423e7ef6df289",
        "model_name_for_query":"h2oai\/h2ogpt-gm-oasst1-en-1024-20b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":37.34,
        "ARC":27.13,
        "HellaSwag":28.28,
        "MMLU":28.94,
        "TruthfulQA":44.94,
        "Winogrande":72.61,
        "GSM8K":20.47,
        "DROP":39.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":317.0,
        "Available on the hub":true,
        "Model sha":"b073c9bb418ae52ca76b4ab48ac2dfbc8622f434",
        "model_name_for_query":"Phind\/Phind-CodeLlama-34B-v1"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":37.31,
        "ARC":46.33,
        "HellaSwag":76.25,
        "MMLU":26.99,
        "TruthfulQA":35.43,
        "Winogrande":70.01,
        "GSM8K":1.14,
        "DROP":5.01,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":65.72,
        "Hub \u2764\ufe0f":171.0,
        "Available on the hub":true,
        "Model sha":"7259969061237fe940036d22bea0fd349e4485e9",
        "model_name_for_query":"facebook\/opt-66b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":37.24,
        "ARC":44.45,
        "HellaSwag":69.29,
        "MMLU":36.67,
        "TruthfulQA":34.98,
        "Winogrande":62.59,
        "GSM8K":8.19,
        "DROP":4.5,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"4bf5b528d95a507b435c24a8986afe80d5951782",
        "model_name_for_query":"llm-agents\/tora-code-13b-v1.0"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":37.22,
        "ARC":46.76,
        "HellaSwag":71.87,
        "MMLU":32.35,
        "TruthfulQA":33.95,
        "Winogrande":67.96,
        "GSM8K":2.65,
        "DROP":5.01,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"bsd-3-clause",
        "#Params (B)":15.72,
        "Hub \u2764\ufe0f":17.0,
        "Available on the hub":true,
        "Model sha":"b65951b0cf7c5639f73caea801a892788608ed69",
        "model_name_for_query":"Salesforce\/codegen-16B-nl"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":37.18,
        "ARC":46.42,
        "HellaSwag":72.08,
        "MMLU":26.16,
        "TruthfulQA":35.53,
        "Winogrande":68.75,
        "GSM8K":2.88,
        "DROP":8.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":20.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"f0462a8b7908f61202d86e6a9a2996d8339363b5",
        "model_name_for_query":"dvruette\/oasst-gpt-neox-20b-3000-steps"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":37.15,
        "ARC":24.57,
        "HellaSwag":27.6,
        "MMLU":25.76,
        "TruthfulQA":48.37,
        "Winogrande":71.82,
        "GSM8K":23.2,
        "DROP":38.7,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":365.0,
        "Available on the hub":true,
        "Model sha":"949f61e203f91b412efe8f679c798f09f0ff4b0c",
        "model_name_for_query":"Phind\/Phind-CodeLlama-34B-v2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":37.12,
        "ARC":46.93,
        "HellaSwag":72.77,
        "MMLU":26.25,
        "TruthfulQA":37.5,
        "Winogrande":68.03,
        "GSM8K":3.18,
        "DROP":5.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.24,
        "Hub \u2764\ufe0f":38.0,
        "Available on the hub":true,
        "Model sha":"3bdf6f870ca14bcc5587b666fbe57488f7854d30",
        "model_name_for_query":"h2oai\/h2ogpt-oasst1-512-20b"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":37.06,
        "ARC":42.06,
        "HellaSwag":63.4,
        "MMLU":35.43,
        "TruthfulQA":43.1,
        "Winogrande":64.17,
        "GSM8K":0.0,
        "DROP":11.23,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":95.0,
        "Available on the hub":true,
        "Model sha":"ca900c8f3145de40cd188c559b2901a2e4711546",
        "model_name_for_query":"psmathur\/orca_mini_13b"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":37.05,
        "ARC":47.01,
        "HellaSwag":71.98,
        "MMLU":30.49,
        "TruthfulQA":34.85,
        "Winogrande":67.96,
        "GSM8K":1.59,
        "DROP":5.5,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":99.0,
        "Available on the hub":true,
        "Model sha":"6fb184ff23774c25bf84b3628e49c8b78372c7be",
        "model_name_for_query":"openlm-research\/open_llama_7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":36.94,
        "ARC":44.28,
        "HellaSwag":72.43,
        "MMLU":31.47,
        "TruthfulQA":34.66,
        "Winogrande":68.43,
        "GSM8K":1.97,
        "DROP":5.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"3465eaca4d293ccc6ce66888e6c8bd9032ae7071",
        "model_name_for_query":"klosax\/open_llama_13b_600bt_preview"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":36.92,
        "ARC":44.11,
        "HellaSwag":72.02,
        "MMLU":37.62,
        "TruthfulQA":33.96,
        "Winogrande":64.96,
        "GSM8K":1.59,
        "DROP":4.21,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":103.0,
        "Available on the hub":true,
        "Model sha":"95667a602ff2646bf67fe3a57c4eb9a1edec87fe",
        "model_name_for_query":"togethercomputer\/RedPajama-INCITE-7B-Instruct"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":36.92,
        "ARC":44.11,
        "HellaSwag":72.02,
        "MMLU":37.62,
        "TruthfulQA":33.96,
        "Winogrande":64.96,
        "GSM8K":1.59,
        "DROP":4.21,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":103.0,
        "Available on the hub":true,
        "Model sha":"95667a602ff2646bf67fe3a57c4eb9a1edec87fe",
        "model_name_for_query":"togethercomputer\/RedPajama-INCITE-Instruct-7B-v0.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":36.9,
        "ARC":40.61,
        "HellaSwag":67.73,
        "MMLU":33.92,
        "TruthfulQA":42.76,
        "Winogrande":63.06,
        "GSM8K":2.5,
        "DROP":7.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"ee3ada91a69a194cedfabbfeab98f1499b75cb44",
        "model_name_for_query":"AlekseyKorshuk\/pygmalion-6b-vicuna-chatml"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":36.89,
        "ARC":44.03,
        "HellaSwag":72.92,
        "MMLU":27.84,
        "TruthfulQA":39.92,
        "Winogrande":66.54,
        "GSM8K":1.21,
        "DROP":5.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":21.0,
        "Available on the hub":true,
        "Model sha":"5fba568304f6f876f5b9e42026f986ea245b836b",
        "model_name_for_query":"acrastt\/Marx-3B-V2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":36.87,
        "ARC":44.03,
        "HellaSwag":70.28,
        "MMLU":26.55,
        "TruthfulQA":36.53,
        "Winogrande":65.27,
        "GSM8K":10.61,
        "DROP":4.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.58,
        "Hub \u2764\ufe0f":21.0,
        "Available on the hub":true,
        "Model sha":"275c9b71bfab4e271d1ed85515c61e317b6ef65e",
        "model_name_for_query":"OpenAssistant\/pythia-12b-sft-v8-7k-steps"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":36.8,
        "ARC":44.97,
        "HellaSwag":71.85,
        "MMLU":28.54,
        "TruthfulQA":35.93,
        "Winogrande":67.88,
        "GSM8K":3.41,
        "DROP":5.02,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.26,
        "Hub \u2764\ufe0f":17.0,
        "Available on the hub":true,
        "Model sha":"40086d791942cb28f55e679cd3fb6f6b5ba4effd",
        "model_name_for_query":"Writer\/palmyra-large"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":36.79,
        "ARC":43.69,
        "HellaSwag":70.77,
        "MMLU":35.61,
        "TruthfulQA":36.05,
        "Winogrande":65.59,
        "GSM8K":1.29,
        "DROP":4.56,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"f2b7cda25f6965c1551fa78e9e38676994bc6638",
        "model_name_for_query":"TehVenom\/Moderator-Chan_GPT-JT-6b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":36.76,
        "ARC":41.21,
        "HellaSwag":71.67,
        "MMLU":29.86,
        "TruthfulQA":36.45,
        "Winogrande":65.98,
        "GSM8K":1.14,
        "DROP":11.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"d3a0bf8e1181be02cc9c4c4cdfedaedacaefbfac",
        "model_name_for_query":"RobbeD\/OpenLlama-Platypus-3B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":36.71,
        "ARC":47.44,
        "HellaSwag":72.58,
        "MMLU":26.37,
        "TruthfulQA":34.39,
        "Winogrande":68.43,
        "GSM8K":2.2,
        "DROP":5.6,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.24,
        "Hub \u2764\ufe0f":9.0,
        "Available on the hub":true,
        "Model sha":"b3a6bf4250a037c09e451344e2a4e987011b79de",
        "model_name_for_query":"h2oai\/h2ogpt-gm-oasst1-multilang-1024-20b"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":36.71,
        "ARC":39.25,
        "HellaSwag":63.47,
        "MMLU":26.09,
        "TruthfulQA":55.42,
        "Winogrande":63.69,
        "GSM8K":0.53,
        "DROP":8.52,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.91,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"fa87c904b5921231b9f6f94b9c537cdda8783b96",
        "model_name_for_query":"Fredithefish\/ReasonixPajama-3B-HF"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":36.67,
        "ARC":42.32,
        "HellaSwag":70.15,
        "MMLU":27.36,
        "TruthfulQA":36.75,
        "Winogrande":65.67,
        "GSM8K":9.55,
        "DROP":4.92,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.58,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"142e306db8e279a07c557ea5a919ab7e7a4af17c",
        "model_name_for_query":"OpenAssistant\/pythia-12b-sft-v8-2.5k-steps"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":36.57,
        "ARC":43.26,
        "HellaSwag":74.07,
        "MMLU":26.66,
        "TruthfulQA":35.16,
        "Winogrande":70.64,
        "GSM8K":1.14,
        "DROP":5.05,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":29.98,
        "Hub \u2764\ufe0f":133.0,
        "Available on the hub":true,
        "Model sha":"ceea0a90ac0f6fae7c2c34bcb40477438c152546",
        "model_name_for_query":"facebook\/opt-30b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":36.56,
        "ARC":37.88,
        "HellaSwag":63.29,
        "MMLU":32.77,
        "TruthfulQA":42.61,
        "Winogrande":62.51,
        "GSM8K":5.16,
        "DROP":11.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"79d5a4d53953ca1c26bc2155f168b7e2108f377f",
        "model_name_for_query":"AlekseyKorshuk\/chatml-pyg-v1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":36.5,
        "ARC":43.17,
        "HellaSwag":72.68,
        "MMLU":28.46,
        "TruthfulQA":39.09,
        "Winogrande":65.59,
        "GSM8K":1.29,
        "DROP":5.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":11.0,
        "Available on the hub":true,
        "Model sha":"c0dcc44989cf4e006efae31abbcef7e8be8547c0",
        "model_name_for_query":"acrastt\/Marx-3B"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":36.42,
        "ARC":40.53,
        "HellaSwag":67.66,
        "MMLU":41.63,
        "TruthfulQA":37.33,
        "Winogrande":62.67,
        "GSM8K":0.99,
        "DROP":4.15,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":30.0,
        "Available on the hub":true,
        "Model sha":"1297870783f6091294769014afddf94499966a78",
        "model_name_for_query":"togethercomputer\/GPT-JT-Moderation-6B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":36.42,
        "ARC":42.66,
        "HellaSwag":64.69,
        "MMLU":37.15,
        "TruthfulQA":39.88,
        "Winogrande":59.75,
        "GSM8K":5.23,
        "DROP":5.55,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":43.0,
        "Available on the hub":true,
        "Model sha":"72a255a58480ef0713eed988312fe82f77f94f37",
        "model_name_for_query":"glaiveai\/glaive-coder-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":36.41,
        "ARC":45.31,
        "HellaSwag":67.67,
        "MMLU":27.81,
        "TruthfulQA":38.16,
        "Winogrande":65.9,
        "GSM8K":4.02,
        "DROP":5.98,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":11.58,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"c21fbece4253841f2d6e15f04f60fe1ba6f990dd",
        "model_name_for_query":"dvruette\/oasst-pythia-12b-pretrained-sft"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":36.37,
        "ARC":40.53,
        "HellaSwag":67.47,
        "MMLU":25.73,
        "TruthfulQA":32.53,
        "Winogrande":62.51,
        "GSM8K":2.05,
        "DROP":23.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":690.0,
        "Available on the hub":true,
        "Model sha":"30e2405100eac6bd53f75964cc7345eeafd19f7d",
        "model_name_for_query":"PygmalionAI\/pygmalion-6b"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":36.36,
        "ARC":43.43,
        "HellaSwag":70.08,
        "MMLU":26.12,
        "TruthfulQA":36.06,
        "Winogrande":64.64,
        "GSM8K":9.55,
        "DROP":4.63,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.58,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"a0debfed4a020d449e3d00f4e75f2c2aefb68db3",
        "model_name_for_query":"OpenAssistant\/pythia-12b-sft-v8-rlhf-2k-steps"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":36.33,
        "ARC":41.81,
        "HellaSwag":73.01,
        "MMLU":26.36,
        "TruthfulQA":38.99,
        "Winogrande":66.69,
        "GSM8K":1.9,
        "DROP":5.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"4da0c661e6df1235c9997b996c8e395b87248406",
        "model_name_for_query":"harborwater\/open-llama-3b-v2-wizard-evol-instuct-v2-196k"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":36.31,
        "ARC":42.58,
        "HellaSwag":66.76,
        "MMLU":26.79,
        "TruthfulQA":31.96,
        "Winogrande":63.46,
        "GSM8K":0.23,
        "DROP":22.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.58,
        "Hub \u2764\ufe0f":17.0,
        "Available on the hub":true,
        "Model sha":"ff4699b502b79c716330b6f761002588a65dcba6",
        "model_name_for_query":"hakurei\/instruct-12b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":36.29,
        "ARC":42.83,
        "HellaSwag":73.28,
        "MMLU":26.87,
        "TruthfulQA":37.26,
        "Winogrande":66.61,
        "GSM8K":1.59,
        "DROP":5.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"31ce2c1611d9f7d56184ceb5bff6a7e95a180c03",
        "model_name_for_query":"harborwater\/open-llama-3b-everything-v2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":36.26,
        "ARC":45.73,
        "HellaSwag":68.59,
        "MMLU":26.82,
        "TruthfulQA":37.81,
        "Winogrande":65.9,
        "GSM8K":3.03,
        "DROP":5.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.58,
        "Hub \u2764\ufe0f":335.0,
        "Available on the hub":true,
        "Model sha":"626b8c140cfdedb119dfb78c626cd772283dee33",
        "model_name_for_query":"OpenAssistant\/oasst-sft-4-pythia-12b-epoch-3.5"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":36.24,
        "ARC":42.58,
        "HellaSwag":71.04,
        "MMLU":30.04,
        "TruthfulQA":37.26,
        "Winogrande":65.82,
        "GSM8K":0.68,
        "DROP":6.28,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"49cdf710c1a9178ddf616da79211fdcdb2170c3f",
        "model_name_for_query":"CobraMamba\/mamba-gpt-3b-v4"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":36.2,
        "ARC":39.51,
        "HellaSwag":33.9,
        "MMLU":38.49,
        "TruthfulQA":40.94,
        "Winogrande":74.35,
        "GSM8K":20.77,
        "DROP":5.43,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":33.74,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"d434d06249feb6ca511b0a09162130bcc59d84e3",
        "model_name_for_query":"chargoddard\/llama-2-34b-uncode"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":36.13,
        "ARC":41.72,
        "HellaSwag":71.05,
        "MMLU":27.31,
        "TruthfulQA":37.86,
        "Winogrande":67.48,
        "GSM8K":1.21,
        "DROP":6.27,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":12.0,
        "Available on the hub":true,
        "Model sha":"d860a90ef6b30c695b985dd2ff382d4bbb80e857",
        "model_name_for_query":"CobraMamba\/mamba-gpt-3b-v3"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":36.12,
        "ARC":40.44,
        "HellaSwag":67.16,
        "MMLU":30.4,
        "TruthfulQA":35.48,
        "Winogrande":66.85,
        "GSM8K":1.29,
        "DROP":11.21,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.67,
        "Hub \u2764\ufe0f":51.0,
        "Available on the hub":true,
        "Model sha":"3293b98cd8204371988f898dafa9b5a297555cbe",
        "model_name_for_query":"kfkas\/Llama-2-ko-7b-Chat"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":36.11,
        "ARC":40.44,
        "HellaSwag":67.12,
        "MMLU":30.19,
        "TruthfulQA":35.45,
        "Winogrande":66.61,
        "GSM8K":1.67,
        "DROP":11.28,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":6.67,
        "Hub \u2764\ufe0f":51.0,
        "Available on the hub":true,
        "Model sha":"3293b98cd8204371988f898dafa9b5a297555cbe",
        "model_name_for_query":"kfkas\/Llama-2-ko-7b-Chat"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":36.09,
        "ARC":45.48,
        "HellaSwag":72.79,
        "MMLU":26.77,
        "TruthfulQA":32.15,
        "Winogrande":68.11,
        "GSM8K":2.27,
        "DROP":5.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.24,
        "Hub \u2764\ufe0f":70.0,
        "Available on the hub":true,
        "Model sha":"1a80940a290452af71caf17a8e520955eb338e0f",
        "model_name_for_query":"KoboldAI\/GPT-NeoX-20B-Erebus"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":36.09,
        "ARC":39.42,
        "HellaSwag":71.26,
        "MMLU":26.91,
        "TruthfulQA":32.73,
        "Winogrande":65.27,
        "GSM8K":0.0,
        "DROP":17.05,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"d62d83b8eb7a6ba012a762752a5b5679add3b40c",
        "model_name_for_query":"KoboldAI\/fairseq-dense-6.7B"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":36.09,
        "ARC":46.25,
        "HellaSwag":71.63,
        "MMLU":27.68,
        "TruthfulQA":33.03,
        "Winogrande":67.32,
        "GSM8K":1.59,
        "DROP":5.11,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":89.0,
        "Available on the hub":true,
        "Model sha":"78f7e482443971f4873ba3239f0ac810a367833b",
        "model_name_for_query":"togethercomputer\/RedPajama-INCITE-Base-7B-v0.1"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":36.09,
        "ARC":46.25,
        "HellaSwag":71.63,
        "MMLU":27.68,
        "TruthfulQA":33.03,
        "Winogrande":67.32,
        "GSM8K":1.59,
        "DROP":5.11,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":89.0,
        "Available on the hub":true,
        "Model sha":"78f7e482443971f4873ba3239f0ac810a367833b",
        "model_name_for_query":"togethercomputer\/RedPajama-INCITE-7B-Base"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":36.03,
        "ARC":41.21,
        "HellaSwag":72.88,
        "MMLU":25.39,
        "TruthfulQA":38.87,
        "Winogrande":66.61,
        "GSM8K":1.59,
        "DROP":5.68,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"4da0c661e6df1235c9997b996c8e395b87248406",
        "model_name_for_query":"harborwater\/open-llama-3b-v2-wizard-evol-instuct-v2-196k"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":36.03,
        "ARC":41.81,
        "HellaSwag":72.3,
        "MMLU":26.36,
        "TruthfulQA":38.33,
        "Winogrande":67.01,
        "GSM8K":0.99,
        "DROP":5.39,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"edbea6fe86d0bc2673c10269828008a1cb451919",
        "model_name_for_query":"acrastt\/Griffin-3B"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":36.02,
        "ARC":45.73,
        "HellaSwag":73.45,
        "MMLU":25.0,
        "TruthfulQA":31.61,
        "Winogrande":68.9,
        "GSM8K":2.43,
        "DROP":5.04,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.74,
        "Hub \u2764\ufe0f":433.0,
        "Available on the hub":true,
        "Model sha":"9369f145ca7b66ef62760f9351af951b2d53b77f",
        "model_name_for_query":"EleutherAI\/gpt-neox-20b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":35.93,
        "ARC":41.72,
        "HellaSwag":71.78,
        "MMLU":24.49,
        "TruthfulQA":40.04,
        "Winogrande":66.93,
        "GSM8K":1.06,
        "DROP":5.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"ffc81b58375342f12e38a67272d95458a72e8d09",
        "model_name_for_query":"harborwater\/wizard-orca-3b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":35.93,
        "ARC":41.3,
        "HellaSwag":71.85,
        "MMLU":27.51,
        "TruthfulQA":38.34,
        "Winogrande":66.38,
        "GSM8K":0.76,
        "DROP":5.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"1159e9cdd05c03d31331f329ba58e4e3444943be",
        "model_name_for_query":"acrastt\/Puma-3B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":35.93,
        "ARC":41.47,
        "HellaSwag":68.8,
        "MMLU":26.58,
        "TruthfulQA":36.82,
        "Winogrande":65.27,
        "GSM8K":7.66,
        "DROP":4.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.58,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":true,
        "Model sha":"37ca702e957a4b740689d67c58c284224e2fbae2",
        "model_name_for_query":"OpenAssistant\/pythia-12b-pre-v8-12.5k-steps"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":35.93,
        "ARC":44.97,
        "HellaSwag":72.68,
        "MMLU":25.99,
        "TruthfulQA":31.64,
        "Winogrande":68.43,
        "GSM8K":2.88,
        "DROP":4.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.24,
        "Hub \u2764\ufe0f":9.0,
        "Available on the hub":true,
        "Model sha":"dd98d514b5aff4e820922c88a73d6d5bf17f332e",
        "model_name_for_query":"KoboldAI\/GPT-NeoX-20B-Skein"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":35.84,
        "ARC":46.42,
        "HellaSwag":70.0,
        "MMLU":26.19,
        "TruthfulQA":39.19,
        "Winogrande":62.19,
        "GSM8K":0.61,
        "DROP":6.3,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.58,
        "Hub \u2764\ufe0f":277.0,
        "Available on the hub":true,
        "Model sha":"293df535fe7711a5726987fc2f17dfc87de452a1",
        "model_name_for_query":"OpenAssistant\/oasst-sft-1-pythia-12b"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":35.79,
        "ARC":45.39,
        "HellaSwag":69.68,
        "MMLU":25.97,
        "TruthfulQA":39.85,
        "Winogrande":63.22,
        "GSM8K":0.53,
        "DROP":5.9,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":11.58,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"e2ccc0ef8d1cc5ffc8b0e2e885f03ef50597ea8a",
        "model_name_for_query":"dvruette\/oasst-pythia-12b-6000-steps"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":35.75,
        "ARC":40.96,
        "HellaSwag":70.72,
        "MMLU":26.21,
        "TruthfulQA":38.78,
        "Winogrande":66.93,
        "GSM8K":1.21,
        "DROP":5.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"4bcf1610eb1f3959568d5acee74833c41502bf04",
        "model_name_for_query":"AtAndDev\/ShortKing-3b-v0.3"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":35.71,
        "ARC":41.21,
        "HellaSwag":66.89,
        "MMLU":36.5,
        "TruthfulQA":34.22,
        "Winogrande":64.4,
        "GSM8K":1.59,
        "DROP":5.14,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"bigscience-openrail-m",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":9.0,
        "Available on the hub":true,
        "Model sha":"92b955a3ff74aa577fa0d8517dfc314847ef60af",
        "model_name_for_query":"digitous\/GPT-R"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":35.69,
        "ARC":24.66,
        "HellaSwag":29.77,
        "MMLU":27.95,
        "TruthfulQA":45.27,
        "Winogrande":68.82,
        "GSM8K":21.53,
        "DROP":31.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":231.0,
        "Available on the hub":true,
        "Model sha":"3aabef8c9bc1b3ec2fffed053645bc1e2d829b6c",
        "model_name_for_query":"Phind\/Phind-CodeLlama-34B-Python-v1"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":35.69,
        "ARC":44.97,
        "HellaSwag":69.75,
        "MMLU":26.64,
        "TruthfulQA":38.89,
        "Winogrande":63.14,
        "GSM8K":0.99,
        "DROP":5.48,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":11.58,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"5227ec9c9def4b0bdf6c7ad95d9f77cbf458283d",
        "model_name_for_query":"dvruette\/oasst-pythia-12b-flash-attn-5000-steps"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":35.66,
        "ARC":41.72,
        "HellaSwag":71.01,
        "MMLU":26.92,
        "TruthfulQA":37.32,
        "Winogrande":67.01,
        "GSM8K":1.06,
        "DROP":4.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.62,
        "Hub \u2764\ufe0f":24.0,
        "Available on the hub":true,
        "Model sha":"16347024c4df6cd114720958964a850fc287cac0",
        "model_name_for_query":"princeton-nlp\/Sheared-LLaMA-2.7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":35.63,
        "ARC":38.23,
        "HellaSwag":66.43,
        "MMLU":28.56,
        "TruthfulQA":44.4,
        "Winogrande":62.83,
        "GSM8K":1.06,
        "DROP":7.88,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"465669ddafad25393ac3cfe94d3726cced112b30",
        "model_name_for_query":"l3utterfly\/open-llama-3b-v2-layla"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":35.6,
        "ARC":43.09,
        "HellaSwag":69.75,
        "MMLU":25.87,
        "TruthfulQA":38.0,
        "Winogrande":66.14,
        "GSM8K":1.06,
        "DROP":5.27,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.59,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"e547fffafb382fd39ef5de35ba3b5afc1b43e74d",
        "model_name_for_query":"h2oai\/h2ogpt-gm-oasst1-en-1024-12b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":35.58,
        "ARC":42.75,
        "HellaSwag":71.72,
        "MMLU":27.16,
        "TruthfulQA":34.26,
        "Winogrande":66.3,
        "GSM8K":1.52,
        "DROP":5.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"1f9e8d48163feb63ed190eaa982f393542a75d30",
        "model_name_for_query":"harborwater\/open-llama-3b-everythingLM-2048"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":35.5,
        "ARC":41.55,
        "HellaSwag":61.52,
        "MMLU":26.79,
        "TruthfulQA":42.42,
        "Winogrande":61.8,
        "GSM8K":0.08,
        "DROP":14.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":129.0,
        "Available on the hub":true,
        "Model sha":"fd2754e80ce80757a3a68a840d7d287dd7def676",
        "model_name_for_query":"psmathur\/orca_mini_3b"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":35.49,
        "ARC":39.93,
        "HellaSwag":63.82,
        "MMLU":28.45,
        "TruthfulQA":36.69,
        "Winogrande":64.48,
        "GSM8K":3.87,
        "DROP":11.19,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":64.0,
        "Available on the hub":true,
        "Model sha":"1f6ffd8f162030396a3bc1ca2e3504896dbe6434",
        "model_name_for_query":"amazon\/LightGPT"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":35.46,
        "ARC":42.58,
        "HellaSwag":69.91,
        "MMLU":26.53,
        "TruthfulQA":36.42,
        "Winogrande":67.17,
        "GSM8K":0.76,
        "DROP":4.86,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"9a3f69a1eba3618930f222d4e013d534102a2af5",
        "model_name_for_query":"Rallio67\/7B-redpajama-conditional-alpha"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":35.42,
        "ARC":41.72,
        "HellaSwag":68.02,
        "MMLU":30.81,
        "TruthfulQA":34.44,
        "Winogrande":65.43,
        "GSM8K":2.65,
        "DROP":4.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"b881231ab6ea85da2a9a139f282df85d1d18b002",
        "model_name_for_query":"digitous\/Javalion-R"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":35.41,
        "ARC":43.0,
        "HellaSwag":67.91,
        "MMLU":28.33,
        "TruthfulQA":36.57,
        "Winogrande":64.96,
        "GSM8K":1.21,
        "DROP":5.91,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":11.58,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"c5a9b7fad884e6c45ce5d2ca551aa1c03db6865f",
        "model_name_for_query":"dvruette\/oasst-pythia-12b-reference"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":35.41,
        "ARC":42.32,
        "HellaSwag":70.24,
        "MMLU":26.01,
        "TruthfulQA":36.41,
        "Winogrande":66.22,
        "GSM8K":1.67,
        "DROP":5.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.59,
        "Hub \u2764\ufe0f":26.0,
        "Available on the hub":true,
        "Model sha":"c6bb0fe363e0105839d34ca757793b61c9606f95",
        "model_name_for_query":"h2oai\/h2ogpt-oasst1-512-12b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":35.35,
        "ARC":36.86,
        "HellaSwag":54.34,
        "MMLU":31.49,
        "TruthfulQA":39.69,
        "Winogrande":58.88,
        "GSM8K":0.38,
        "DROP":25.82,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":3.0,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"a35b6ae6809891e253b45fb5795979c33992e548",
        "model_name_for_query":"cmarkea\/bloomz-3b-sft-chat"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":35.33,
        "ARC":41.64,
        "HellaSwag":69.01,
        "MMLU":30.7,
        "TruthfulQA":34.5,
        "Winogrande":64.8,
        "GSM8K":1.67,
        "DROP":5.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"4c4a5caf5d9049a47f5565b72e5a53dede08ac8b",
        "model_name_for_query":"digitous\/Javelin-R"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":35.31,
        "ARC":39.42,
        "HellaSwag":66.48,
        "MMLU":23.64,
        "TruthfulQA":38.56,
        "Winogrande":62.9,
        "GSM8K":0.3,
        "DROP":15.84,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.19,
        "Hub \u2764\ufe0f":16.0,
        "Available on the hub":true,
        "Model sha":"a2dfc9f659be13556a25d9e38da642c6f67aeee3",
        "model_name_for_query":"RWKV\/rwkv-raven-7b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":35.3,
        "ARC":41.04,
        "HellaSwag":71.19,
        "MMLU":24.32,
        "TruthfulQA":36.66,
        "Winogrande":66.93,
        "GSM8K":1.59,
        "DROP":5.39,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"920609897049f674bc4a9678579f6869f6cbed13",
        "model_name_for_query":"vihangd\/smartyplats-3b-v2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":35.29,
        "ARC":27.39,
        "HellaSwag":26.03,
        "MMLU":25.81,
        "TruthfulQA":48.9,
        "Winogrande":77.9,
        "GSM8K":24.56,
        "DROP":16.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":35.58,
        "Hub \u2764\ufe0f":37.0,
        "Available on the hub":true,
        "Model sha":"1c65902c620fcdf6b9c8e36ce17f21360e186a1e",
        "model_name_for_query":"TheBloke\/WizardLM-33B-V1.0-Uncensored-GPTQ"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":35.26,
        "ARC":40.27,
        "HellaSwag":71.6,
        "MMLU":27.12,
        "TruthfulQA":34.78,
        "Winogrande":67.01,
        "GSM8K":0.91,
        "DROP":5.13,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":61.0,
        "Available on the hub":true,
        "Model sha":"bce5d60d3b0c68318862270ec4e794d83308d80a",
        "model_name_for_query":"openlm-research\/open_llama_3b_v2"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":35.26,
        "ARC":40.27,
        "HellaSwag":71.6,
        "MMLU":27.12,
        "TruthfulQA":34.78,
        "Winogrande":67.01,
        "GSM8K":0.91,
        "DROP":5.13,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"c55e3e114951346f273c519d266170e4d52781e9",
        "model_name_for_query":"KnutJaegersberg\/openllama_3b_EvolInstruct_lora_merged"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":35.26,
        "ARC":41.81,
        "HellaSwag":65.06,
        "MMLU":32.29,
        "TruthfulQA":36.32,
        "Winogrande":61.72,
        "GSM8K":4.7,
        "DROP":4.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":44.0,
        "Available on the hub":true,
        "Model sha":"e40673a27a4aefcff2c6d2b3b1e0681a38703e4e",
        "model_name_for_query":"WizardLM\/WizardCoder-Python-7B-V1.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":35.24,
        "ARC":42.32,
        "HellaSwag":68.89,
        "MMLU":26.01,
        "TruthfulQA":35.56,
        "Winogrande":66.93,
        "GSM8K":2.12,
        "DROP":4.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":11.58,
        "Hub \u2764\ufe0f":9.0,
        "Available on the hub":true,
        "Model sha":"7faeb395c26189eeab9bf3a98994696687ad31a3",
        "model_name_for_query":"Pirr\/pythia-13b-deduped-green_devil"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":35.22,
        "ARC":36.86,
        "HellaSwag":54.95,
        "MMLU":32.91,
        "TruthfulQA":40.34,
        "Winogrande":57.14,
        "GSM8K":0.0,
        "DROP":24.36,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":3.0,
        "Hub \u2764\ufe0f":70.0,
        "Available on the hub":true,
        "Model sha":"31eefcb2bcd69632925adf07e090debafe95436d",
        "model_name_for_query":"bigscience\/bloomz-3b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":35.2,
        "ARC":40.36,
        "HellaSwag":72.0,
        "MMLU":26.43,
        "TruthfulQA":36.11,
        "Winogrande":65.67,
        "GSM8K":0.53,
        "DROP":5.28,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"4a1ce189a3fb1d58b3fa47ebe30b3c037592670c",
        "model_name_for_query":"acrastt\/Bean-3B"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":35.19,
        "ARC":38.14,
        "HellaSwag":34.8,
        "MMLU":32.95,
        "TruthfulQA":43.57,
        "Winogrande":72.14,
        "GSM8K":20.02,
        "DROP":4.75,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.74,
        "Hub \u2764\ufe0f":11.0,
        "Available on the hub":true,
        "Model sha":"875f9d97fb6c9619d8867887dd1d80918ff0f593",
        "model_name_for_query":"TheBloke\/CodeLlama-34B-Python-fp16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":35.16,
        "ARC":42.66,
        "HellaSwag":70.45,
        "MMLU":26.2,
        "TruthfulQA":36.08,
        "Winogrande":64.17,
        "GSM8K":1.82,
        "DROP":4.77,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"bee7068ab002784420a1a30170db3906185359f2",
        "model_name_for_query":"digitous\/Javelin-GPTJ"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":35.16,
        "ARC":40.7,
        "HellaSwag":65.86,
        "MMLU":33.34,
        "TruthfulQA":34.84,
        "Winogrande":61.56,
        "GSM8K":4.93,
        "DROP":4.9,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"777501b69bb0ba2675abdcaf7b1309ab05320c2e",
        "model_name_for_query":"llm-agents\/tora-code-7b-v1.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":35.14,
        "ARC":40.44,
        "HellaSwag":67.36,
        "MMLU":31.24,
        "TruthfulQA":34.49,
        "Winogrande":65.35,
        "GSM8K":2.27,
        "DROP":4.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"f6963f77098d8421ff4a1cf4d36f1e94c6c8f44b",
        "model_name_for_query":"digitous\/Janin-R"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":35.13,
        "ARC":40.1,
        "HellaSwag":71.56,
        "MMLU":26.88,
        "TruthfulQA":34.74,
        "Winogrande":66.61,
        "GSM8K":0.91,
        "DROP":5.13,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"b4c7bb49171ff6955cfc1f7e33143383c57f7606",
        "model_name_for_query":"TaylorAI\/Flash-Llama-3B"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":35.12,
        "ARC":40.19,
        "HellaSwag":36.82,
        "MMLU":34.79,
        "TruthfulQA":44.28,
        "Winogrande":71.19,
        "GSM8K":14.33,
        "DROP":4.26,
        "Type":"instruction-tuned",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":33.74,
        "Hub \u2764\ufe0f":61.0,
        "Available on the hub":true,
        "Model sha":"3dd8ab05bbd273b9f77088b1d4015b7f1848793d",
        "model_name_for_query":"codellama\/CodeLlama-34b-Python-hf"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":35.12,
        "ARC":41.38,
        "HellaSwag":67.67,
        "MMLU":28.48,
        "TruthfulQA":36.86,
        "Winogrande":64.33,
        "GSM8K":1.97,
        "DROP":5.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"6413b1d9e8b58df9d3aac91a862e8d505d8c6716",
        "model_name_for_query":"TehVenom\/Dolly_Shygmalion-6b-Dev_V8P2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":35.11,
        "ARC":37.54,
        "HellaSwag":31.84,
        "MMLU":37.2,
        "TruthfulQA":38.89,
        "Winogrande":73.4,
        "GSM8K":21.61,
        "DROP":5.31,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"4e61ec70eb258047f5bc689fa6a66f7753da52b8",
        "model_name_for_query":"NousResearch\/CodeLlama-34b-hf"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":35.1,
        "ARC":41.3,
        "HellaSwag":65.97,
        "MMLU":26.78,
        "TruthfulQA":37.91,
        "Winogrande":64.72,
        "GSM8K":0.91,
        "DROP":8.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"83d8c754aac12f838d7c847d4352a09396c383d0",
        "model_name_for_query":"Corianas\/gpt-j-6B-Dolly"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":35.06,
        "ARC":36.52,
        "HellaSwag":55.44,
        "MMLU":34.54,
        "TruthfulQA":41.25,
        "Winogrande":64.56,
        "GSM8K":7.96,
        "DROP":5.17,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":76.0,
        "Available on the hub":true,
        "Model sha":"7affc442e639b8aa1c4b3e98a10a2f45a21b8b4f",
        "model_name_for_query":"codellama\/CodeLlama-7b-Instruct-hf"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":35.06,
        "ARC":40.53,
        "HellaSwag":70.85,
        "MMLU":25.31,
        "TruthfulQA":36.53,
        "Winogrande":65.75,
        "GSM8K":1.06,
        "DROP":5.4,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"89272b9edb323f5ace09e097a6449554c0dcd4e7",
        "model_name_for_query":"vihangd\/smartyplats-3b-v1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":35.03,
        "ARC":42.32,
        "HellaSwag":63.43,
        "MMLU":33.39,
        "TruthfulQA":38.51,
        "Winogrande":60.38,
        "GSM8K":2.5,
        "DROP":4.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"fe7a232baac5394e821f349cb7ef31dbd4ca2078",
        "model_name_for_query":"Danielbrdz\/CodeBarcenas-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":35.0,
        "ARC":42.58,
        "HellaSwag":68.69,
        "MMLU":24.88,
        "TruthfulQA":38.7,
        "Winogrande":63.85,
        "GSM8K":1.44,
        "DROP":4.86,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":12.0,
        "Available on the hub":true,
        "Model sha":"acfe27303f74129930fef5e6fadbc5f58c6b8590",
        "model_name_for_query":"KoboldAI\/GPT-J-6B-Skein"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":34.99,
        "ARC":30.72,
        "HellaSwag":53.81,
        "MMLU":27.61,
        "TruthfulQA":38.34,
        "Winogrande":60.22,
        "GSM8K":0.53,
        "DROP":33.7,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":1.32,
        "Hub \u2764\ufe0f":34.0,
        "Available on the hub":true,
        "Model sha":"d60fa58f50def19751da2075791da359ca19d273",
        "model_name_for_query":"facebook\/opt-iml-max-1.3b"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":34.99,
        "ARC":40.44,
        "HellaSwag":61.2,
        "MMLU":26.83,
        "TruthfulQA":40.83,
        "Winogrande":64.56,
        "GSM8K":0.68,
        "DROP":10.37,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.07,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"6b4385dc45c47d509b6400c41a2ff3665ad1d189",
        "model_name_for_query":"YeungNLP\/firefly-bloom-7b1"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":34.99,
        "ARC":39.93,
        "HellaSwag":71.2,
        "MMLU":24.9,
        "TruthfulQA":34.1,
        "Winogrande":68.51,
        "GSM8K":0.99,
        "DROP":5.28,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":59.0,
        "Available on the hub":true,
        "Model sha":"e515202d1e7750da62d245fbccb2723b9c1790f5",
        "model_name_for_query":"facebook\/opt-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.98,
        "ARC":41.64,
        "HellaSwag":64.24,
        "MMLU":26.26,
        "TruthfulQA":40.43,
        "Winogrande":61.8,
        "GSM8K":0.53,
        "DROP":9.97,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"0e201b6f344ac6382dda40d389e1c9144a87d027",
        "model_name_for_query":"dvruette\/oasst-pythia-6.9b-4000-steps"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.98,
        "ARC":42.66,
        "HellaSwag":65.89,
        "MMLU":27.28,
        "TruthfulQA":40.16,
        "Winogrande":60.14,
        "GSM8K":0.0,
        "DROP":8.69,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":true,
        "Model sha":"bebfcb894b3f5170ce54e3bb98b6e565fae7b6c0",
        "model_name_for_query":"Yukang\/LongAlpaca-7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.96,
        "ARC":41.3,
        "HellaSwag":62.44,
        "MMLU":27.55,
        "TruthfulQA":42.0,
        "Winogrande":64.56,
        "GSM8K":1.52,
        "DROP":5.38,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"29604e6e19822531b0d49d3f19abef603a97d0ec",
        "model_name_for_query":"h2oai\/h2ogpt-gm-oasst1-en-1024-open-llama-7b-preview-400bt"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.96,
        "ARC":41.89,
        "HellaSwag":68.69,
        "MMLU":26.85,
        "TruthfulQA":35.44,
        "Winogrande":65.27,
        "GSM8K":1.67,
        "DROP":4.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"3ce176bc0f91cae416c78e99f964f54b12472de0",
        "model_name_for_query":"digitous\/Javalion-GPTJ"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.94,
        "ARC":40.7,
        "HellaSwag":67.04,
        "MMLU":29.31,
        "TruthfulQA":35.57,
        "Winogrande":63.93,
        "GSM8K":2.58,
        "DROP":5.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"fa3d503bca50c947e7a5bbde4bdd82f699f65c02",
        "model_name_for_query":"TehVenom\/PPO_Shygmalion-V8p4_Dev-6b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.94,
        "ARC":40.36,
        "HellaSwag":67.15,
        "MMLU":29.3,
        "TruthfulQA":35.26,
        "Winogrande":64.4,
        "GSM8K":2.65,
        "DROP":5.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"f30709dba36c665869f9ac8cd0cef5a8a2e7c8df",
        "model_name_for_query":"TehVenom\/PPO_Pygway-V8p4_Dev-6b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.93,
        "ARC":41.98,
        "HellaSwag":66.82,
        "MMLU":25.69,
        "TruthfulQA":39.67,
        "Winogrande":64.88,
        "GSM8K":0.68,
        "DROP":4.82,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"e471ec778771f29992293d1660cc108f29c9c69e",
        "model_name_for_query":"Aspik101\/WizardVicuna-Uncensored-3B-instruct-PL-lora_unload"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":34.92,
        "ARC":42.32,
        "HellaSwag":68.59,
        "MMLU":25.93,
        "TruthfulQA":34.47,
        "Winogrande":66.46,
        "GSM8K":2.2,
        "DROP":4.46,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"bsd-3-clause",
        "#Params (B)":6.85,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"dff91c0aea702edbea3528344d01d8b9aaee6e39",
        "model_name_for_query":"Salesforce\/codegen-6B-nl"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.9,
        "ARC":41.89,
        "HellaSwag":68.48,
        "MMLU":27.58,
        "TruthfulQA":33.91,
        "Winogrande":65.35,
        "GSM8K":2.12,
        "DROP":4.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":14.0,
        "Available on the hub":true,
        "Model sha":"108fabf8a916900525492c294c50998d7c09f10b",
        "model_name_for_query":"TehVenom\/Dolly_Shygmalion-6b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.87,
        "ARC":43.77,
        "HellaSwag":69.22,
        "MMLU":25.37,
        "TruthfulQA":34.67,
        "Winogrande":64.64,
        "GSM8K":1.52,
        "DROP":4.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"4dff006b2ea7e8d9b067dfe8af8ca1a16bc44dce",
        "model_name_for_query":"digitous\/Skegma-GPTJ"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":34.87,
        "ARC":44.45,
        "HellaSwag":71.07,
        "MMLU":26.12,
        "TruthfulQA":32.04,
        "Winogrande":65.43,
        "GSM8K":0.38,
        "DROP":4.58,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.89,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"4effb0fa9d15c2f383a1d159f4a40df0e09eb6d5",
        "model_name_for_query":"RWKV\/rwkv-4-14b-pile"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":34.87,
        "ARC":41.38,
        "HellaSwag":67.54,
        "MMLU":26.78,
        "TruthfulQA":35.96,
        "Winogrande":65.98,
        "GSM8K":1.82,
        "DROP":4.62,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":1290.0,
        "Available on the hub":true,
        "Model sha":"47e169305d2e8376be1d31e765533382721b2cc1",
        "model_name_for_query":"EleutherAI\/gpt-j-6b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.86,
        "ARC":39.85,
        "HellaSwag":59.58,
        "MMLU":30.47,
        "TruthfulQA":38.62,
        "Winogrande":64.88,
        "GSM8K":5.46,
        "DROP":5.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"855c92912ea4a8eb5f0be1db4bf776ffd0815dac",
        "model_name_for_query":"NousResearch\/CodeLlama-7b-hf"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":34.85,
        "ARC":39.93,
        "HellaSwag":60.8,
        "MMLU":31.12,
        "TruthfulQA":37.82,
        "Winogrande":64.01,
        "GSM8K":5.16,
        "DROP":5.12,
        "Type":"pretrained",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":145.0,
        "Available on the hub":true,
        "Model sha":"be52f4ad322f5a47da121c761aeb5ba20ed77b17",
        "model_name_for_query":"codellama\/CodeLlama-7b-hf"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.81,
        "ARC":41.38,
        "HellaSwag":66.19,
        "MMLU":26.53,
        "TruthfulQA":39.35,
        "Winogrande":63.77,
        "GSM8K":1.14,
        "DROP":5.31,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"36841c80535bc3e8403e3cc084e8e65884c75076",
        "model_name_for_query":"heegyu\/WizardVicuna-Uncensored-3B-0719"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.8,
        "ARC":40.02,
        "HellaSwag":68.67,
        "MMLU":27.44,
        "TruthfulQA":34.63,
        "Winogrande":64.01,
        "GSM8K":4.09,
        "DROP":4.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":61.0,
        "Available on the hub":true,
        "Model sha":"97aa918c383820e1a69f042801091d7deb996c20",
        "model_name_for_query":"togethercomputer\/Pythia-Chat-Base-7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.8,
        "ARC":42.83,
        "HellaSwag":68.43,
        "MMLU":27.13,
        "TruthfulQA":33.03,
        "Winogrande":65.43,
        "GSM8K":1.74,
        "DROP":4.97,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"f239eb8d24fe26db3b0a9a69115dc305fc9351af",
        "model_name_for_query":"TehVenom\/Dolly_Malion-6b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.78,
        "ARC":40.19,
        "HellaSwag":66.43,
        "MMLU":30.39,
        "TruthfulQA":34.76,
        "Winogrande":64.01,
        "GSM8K":1.9,
        "DROP":5.8,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"bigscience-openrail-m",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"930dc82245c607ce43558a0e6c0225e77b341ea6",
        "model_name_for_query":"TehVenom\/GPT-J-Pyg_PPO-6B-Dev-V8p4"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.78,
        "ARC":41.64,
        "HellaSwag":66.23,
        "MMLU":27.26,
        "TruthfulQA":36.1,
        "Winogrande":64.4,
        "GSM8K":0.68,
        "DROP":7.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc",
        "#Params (B)":2.91,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"c588a5924749b86a6cb36a687dafa544c189bb6f",
        "model_name_for_query":"Fredithefish\/RedPajama-INCITE-Chat-3B-Instruction-Tuning-with-GPT-4"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":34.77,
        "ARC":29.44,
        "HellaSwag":26.47,
        "MMLU":24.35,
        "TruthfulQA":49.15,
        "Winogrande":73.16,
        "GSM8K":21.08,
        "DROP":19.74,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":35.58,
        "Hub \u2764\ufe0f":107.0,
        "Available on the hub":true,
        "Model sha":"43c701ddbe0bceac26c860307e06763cc5203500",
        "model_name_for_query":"TheBloke\/WizardLM-30B-Uncensored-GPTQ"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.76,
        "ARC":42.83,
        "HellaSwag":67.62,
        "MMLU":26.23,
        "TruthfulQA":34.44,
        "Winogrande":65.51,
        "GSM8K":0.53,
        "DROP":6.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":115.0,
        "Available on the hub":true,
        "Model sha":"f0e0995eba801096ed04cb87931d96a8316871af",
        "model_name_for_query":"togethercomputer\/RedPajama-INCITE-Chat-3B-v1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.74,
        "ARC":41.89,
        "HellaSwag":68.25,
        "MMLU":27.29,
        "TruthfulQA":33.89,
        "Winogrande":65.35,
        "GSM8K":1.67,
        "DROP":4.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":9.0,
        "Available on the hub":true,
        "Model sha":"2667b0e0b705ed23f81f3e2b69673d722e8f4964",
        "model_name_for_query":"TehVenom\/ChanMalion"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.73,
        "ARC":42.06,
        "HellaSwag":67.51,
        "MMLU":28.52,
        "TruthfulQA":31.95,
        "Winogrande":64.72,
        "GSM8K":2.81,
        "DROP":5.57,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"bigscience-openrail-m",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":true,
        "Model sha":"cde5bab3ae16e1704c5fec54a6a7ff1169c935e6",
        "model_name_for_query":"TehVenom\/GPT-J-Pyg_PPO-6B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.72,
        "ARC":42.41,
        "HellaSwag":72.53,
        "MMLU":25.92,
        "TruthfulQA":33.83,
        "Winogrande":60.85,
        "GSM8K":1.21,
        "DROP":6.29,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":11.58,
        "Hub \u2764\ufe0f":1872.0,
        "Available on the hub":true,
        "Model sha":"19308160448536e378e3db21a73a751579ee7fdd",
        "model_name_for_query":"databricks\/dolly-v2-12b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.7,
        "ARC":39.85,
        "HellaSwag":70.6,
        "MMLU":24.9,
        "TruthfulQA":34.02,
        "Winogrande":67.88,
        "GSM8K":0.38,
        "DROP":5.26,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":25.0,
        "Available on the hub":true,
        "Model sha":"c27a7e2360dd313406719980851e89abf46ebb13",
        "model_name_for_query":"KoboldAI\/OPT-13B-Nerybus-Mix"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.69,
        "ARC":40.02,
        "HellaSwag":70.07,
        "MMLU":25.32,
        "TruthfulQA":34.93,
        "Winogrande":66.54,
        "GSM8K":0.76,
        "DROP":5.23,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":149.0,
        "Available on the hub":true,
        "Model sha":"8a949353677d2b971910a6c4afcc70e95d838c2a",
        "model_name_for_query":"KoboldAI\/OPT-13B-Erebus"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.68,
        "ARC":42.06,
        "HellaSwag":70.82,
        "MMLU":26.94,
        "TruthfulQA":36.09,
        "Winogrande":59.83,
        "GSM8K":0.45,
        "DROP":6.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":87.0,
        "Available on the hub":true,
        "Model sha":"47b94a739e2f3164b438501c8684acc5d5acc146",
        "model_name_for_query":"togethercomputer\/RedPajama-INCITE-7B-Chat"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.68,
        "ARC":42.06,
        "HellaSwag":70.82,
        "MMLU":26.94,
        "TruthfulQA":36.09,
        "Winogrande":59.83,
        "GSM8K":0.45,
        "DROP":6.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":87.0,
        "Available on the hub":true,
        "Model sha":"47b94a739e2f3164b438501c8684acc5d5acc146",
        "model_name_for_query":"togethercomputer\/RedPajama-INCITE-Chat-7B-v0.1"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":34.67,
        "ARC":41.38,
        "HellaSwag":70.26,
        "MMLU":25.63,
        "TruthfulQA":33.0,
        "Winogrande":66.46,
        "GSM8K":1.44,
        "DROP":4.55,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.59,
        "Hub \u2764\ufe0f":48.0,
        "Available on the hub":true,
        "Model sha":"39c1bd94f9dbe4ebd1d191f364cb33a2e5c47707",
        "model_name_for_query":"EleutherAI\/pythia-12b-deduped"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.66,
        "ARC":40.87,
        "HellaSwag":67.29,
        "MMLU":27.4,
        "TruthfulQA":36.25,
        "Winogrande":64.25,
        "GSM8K":1.97,
        "DROP":4.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"a6773861798f2abea3849514aa6f60961518af9c",
        "model_name_for_query":"digitous\/Janin-GPTJ"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.63,
        "ARC":39.68,
        "HellaSwag":70.53,
        "MMLU":25.36,
        "TruthfulQA":33.5,
        "Winogrande":67.88,
        "GSM8K":0.23,
        "DROP":5.24,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":9.0,
        "Available on the hub":true,
        "Model sha":"b0aa4f3630356f7801ca083c00b03d03da13b8bb",
        "model_name_for_query":"KoboldAI\/OPT-13B-Nerys-v2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.62,
        "ARC":39.85,
        "HellaSwag":67.06,
        "MMLU":27.72,
        "TruthfulQA":36.94,
        "Winogrande":64.09,
        "GSM8K":1.97,
        "DROP":4.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":15.0,
        "Available on the hub":true,
        "Model sha":"afa5a11b24cb23eee708e17c83b920a788e9e07b",
        "model_name_for_query":"KoboldAI\/GPT-J-6B-Shinen"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.57,
        "ARC":40.7,
        "HellaSwag":65.45,
        "MMLU":25.44,
        "TruthfulQA":40.71,
        "Winogrande":63.85,
        "GSM8K":0.76,
        "DROP":5.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"62d3d450b8ab2bd2fb9f82383b55d1ecae33a401",
        "model_name_for_query":"heegyu\/WizardVicuna-3B-0719"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.57,
        "ARC":40.87,
        "HellaSwag":67.11,
        "MMLU":27.45,
        "TruthfulQA":35.74,
        "Winogrande":64.72,
        "GSM8K":1.36,
        "DROP":4.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":11.0,
        "Available on the hub":true,
        "Model sha":"036bb03496d648ddc8cf932ad91df8ef1287116c",
        "model_name_for_query":"KoboldAI\/GPT-J-6B-Janeway"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.56,
        "ARC":40.53,
        "HellaSwag":64.94,
        "MMLU":25.35,
        "TruthfulQA":37.14,
        "Winogrande":65.04,
        "GSM8K":0.23,
        "DROP":8.68,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"21a8212e3641dd14924d6bdead0774b64dda8ce0",
        "model_name_for_query":"CobraMamba\/mamba-gpt-3b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.53,
        "ARC":41.81,
        "HellaSwag":67.77,
        "MMLU":28.42,
        "TruthfulQA":32.5,
        "Winogrande":64.4,
        "GSM8K":1.67,
        "DROP":5.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":19.0,
        "Available on the hub":true,
        "Model sha":"b31d25819e00d5031ccdb22a9584f0850dcfe39c",
        "model_name_for_query":"KoboldAI\/PPO_Pygway-6b-Mix"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.49,
        "ARC":44.54,
        "HellaSwag":69.64,
        "MMLU":25.18,
        "TruthfulQA":34.88,
        "Winogrande":60.06,
        "GSM8K":1.14,
        "DROP":5.97,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":132.0,
        "Available on the hub":true,
        "Model sha":"d632f0c8b75b1ae5b26b250d25bfba4e99cb7c6f",
        "model_name_for_query":"databricks\/dolly-v2-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.46,
        "ARC":40.27,
        "HellaSwag":66.88,
        "MMLU":27.53,
        "TruthfulQA":34.24,
        "Winogrande":65.35,
        "GSM8K":1.82,
        "DROP":5.13,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"573e4546fdccc5c8a52b9d7cb23a2e10f0f2ef51",
        "model_name_for_query":"TehVenom\/PPO_Shygmalion-6b"
    },
    {
        "T":"\ud83d\udfe6",
        "Average \u2b06\ufe0f":34.44,
        "ARC":41.3,
        "HellaSwag":66.82,
        "MMLU":26.1,
        "TruthfulQA":35.04,
        "Winogrande":65.43,
        "GSM8K":0.3,
        "DROP":6.08,
        "Type":"RL-tuned",
        "Precision":"8bit",
        "Hub License":"apache-2.0",
        "#Params (B)":3.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"a2ee88a9fa1c9ad41e0a8c15217a4b1230ec33c8",
        "model_name_for_query":"DanielSc4\/RedPajama-INCITE-Chat-3B-v1-RL-LoRA-8bit-test1"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":34.39,
        "ARC":27.73,
        "HellaSwag":26.29,
        "MMLU":23.53,
        "TruthfulQA":49.54,
        "Winogrande":79.79,
        "GSM8K":27.75,
        "DROP":6.13,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":35.58,
        "Hub \u2764\ufe0f":13.0,
        "Available on the hub":true,
        "Model sha":"4c2588d65302e9ca634548ed81e8650fb2975686",
        "model_name_for_query":"TheBloke\/robin-33B-v2-GPTQ"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.36,
        "ARC":42.49,
        "HellaSwag":69.21,
        "MMLU":25.4,
        "TruthfulQA":36.95,
        "Winogrande":60.22,
        "GSM8K":1.59,
        "DROP":4.69,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"4fbfe9eae03a1d6ecf60fda8cf39c4123f0438bd",
        "model_name_for_query":"digitous\/Adventien-GPTJ"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.35,
        "ARC":42.15,
        "HellaSwag":66.72,
        "MMLU":26.18,
        "TruthfulQA":35.21,
        "Winogrande":63.3,
        "GSM8K":0.3,
        "DROP":6.6,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.78,
        "Hub \u2764\ufe0f":9.0,
        "Available on the hub":true,
        "Model sha":"e07122091fd4b318dcea105b16c73144d95bc2f6",
        "model_name_for_query":"Fredithefish\/Guanaco-3B-Uncensored-v2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.34,
        "ARC":41.64,
        "HellaSwag":64.76,
        "MMLU":26.25,
        "TruthfulQA":36.58,
        "Winogrande":64.33,
        "GSM8K":0.15,
        "DROP":6.71,
        "Type":"fine-tuned",
        "Precision":"GPTQ",
        "Hub License":"apache-2.0",
        "#Params (B)":4.78,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"c80e2f01377d551ad17c8c9bac3f52578c38d653",
        "model_name_for_query":"TheBloke\/Guanaco-3B-Uncensored-v2-GPTQ"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":34.34,
        "ARC":41.55,
        "HellaSwag":51.01,
        "MMLU":38.03,
        "TruthfulQA":41.65,
        "Winogrande":57.7,
        "GSM8K":3.11,
        "DROP":7.3,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.66,
        "Hub \u2764\ufe0f":34.0,
        "Available on the hub":true,
        "Model sha":"d86db70e16111175ff7900f71d40806ccf4b8491",
        "model_name_for_query":"OpenAssistant\/galactica-6.7b-finetuned"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.33,
        "ARC":36.43,
        "HellaSwag":61.41,
        "MMLU":25.01,
        "TruthfulQA":37.59,
        "Winogrande":64.64,
        "GSM8K":0.23,
        "DROP":15.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"fdc6ff469295d0aaabec8948525b70d6688728ac",
        "model_name_for_query":"h2oai\/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt-v2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.33,
        "ARC":42.58,
        "HellaSwag":67.48,
        "MMLU":25.99,
        "TruthfulQA":33.62,
        "Winogrande":64.8,
        "GSM8K":0.91,
        "DROP":4.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.78,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"e19eef572d57fc734bf3ea07c7d0098b3901ec9b",
        "model_name_for_query":"acrastt\/RedPajama-INCITE-Chat-Instruct-3B-V1"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":34.32,
        "ARC":41.3,
        "HellaSwag":67.05,
        "MMLU":26.48,
        "TruthfulQA":35.19,
        "Winogrande":64.09,
        "GSM8K":1.67,
        "DROP":4.5,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":true,
        "Model sha":"372b1c08d9b5b0fc18ce86bbf294930e26e66ed5",
        "model_name_for_query":"EleutherAI\/pythia-6.9b-deduped"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.32,
        "ARC":39.76,
        "HellaSwag":64.89,
        "MMLU":27.28,
        "TruthfulQA":37.6,
        "Winogrande":64.48,
        "GSM8K":0.23,
        "DROP":5.97,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":9.0,
        "Available on the hub":true,
        "Model sha":"9dd07308b6eb3f270c5762250b6d46abd6f87b6f",
        "model_name_for_query":"Fredithefish\/ScarletPajama-3B-HF"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.31,
        "ARC":38.82,
        "HellaSwag":54.71,
        "MMLU":31.62,
        "TruthfulQA":41.25,
        "Winogrande":58.64,
        "GSM8K":0.45,
        "DROP":14.66,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"bigscience-openrail-m",
        "#Params (B)":3.0,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":true,
        "Model sha":"4ea0ad223a2623fc15e8824c1c4f8e6539bc40b0",
        "model_name_for_query":"ikala\/bloom-zh-3b-chat"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":34.25,
        "ARC":38.14,
        "HellaSwag":66.56,
        "MMLU":25.75,
        "TruthfulQA":37.46,
        "Winogrande":63.93,
        "GSM8K":0.53,
        "DROP":7.38,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"7067f68d4d9e7b10a1aa2c9fa97456bc04678867",
        "model_name_for_query":"KnutJaegersberg\/black_goo_recipe_a"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.25,
        "ARC":40.61,
        "HellaSwag":64.84,
        "MMLU":26.13,
        "TruthfulQA":35.41,
        "Winogrande":63.54,
        "GSM8K":0.3,
        "DROP":8.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"ec33d12d08d61ed821e67b1a55ad404dc3457ebf",
        "model_name_for_query":"Fredithefish\/RedPajama-INCITE-Chat-3B-ShareGPT-11K"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":34.22,
        "ARC":37.71,
        "HellaSwag":65.94,
        "MMLU":26.02,
        "TruthfulQA":37.4,
        "Winogrande":65.75,
        "GSM8K":0.76,
        "DROP":5.96,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"52297e0b6845b3c1b26f336fd2a2c9b2f56ce6ba",
        "model_name_for_query":"health360\/Healix-3B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.2,
        "ARC":39.16,
        "HellaSwag":68.66,
        "MMLU":24.58,
        "TruthfulQA":35.12,
        "Winogrande":65.98,
        "GSM8K":1.06,
        "DROP":4.86,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.66,
        "Hub \u2764\ufe0f":84.0,
        "Available on the hub":true,
        "Model sha":"9c4d1af96f93224e01d2f69c303fc6d6f686bdcc",
        "model_name_for_query":"KoboldAI\/OPT-6.7B-Erebus"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":34.2,
        "ARC":39.68,
        "HellaSwag":66.42,
        "MMLU":27.13,
        "TruthfulQA":36.07,
        "Winogrande":64.64,
        "GSM8K":0.38,
        "DROP":5.06,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"c96b846ce7bacf5ad231957630dc94d59f329339",
        "model_name_for_query":"KnutJaegersberg\/deacon-3b"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":34.19,
        "ARC":39.16,
        "HellaSwag":68.66,
        "MMLU":24.57,
        "TruthfulQA":35.12,
        "Winogrande":65.98,
        "GSM8K":0.99,
        "DROP":4.86,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.66,
        "Hub \u2764\ufe0f":77.0,
        "Available on the hub":true,
        "Model sha":"a45aa65bbeb77c1558bc99bedc6779195462dab0",
        "model_name_for_query":"facebook\/opt-6.7b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":34.19,
        "ARC":38.74,
        "HellaSwag":66.83,
        "MMLU":26.57,
        "TruthfulQA":36.54,
        "Winogrande":64.72,
        "GSM8K":0.68,
        "DROP":5.23,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"78c0a6432ac0a6c2e54a2c3aac4cb70f446eb18b",
        "model_name_for_query":"KnutJaegersberg\/black_goo_recipe_c"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.18,
        "ARC":42.49,
        "HellaSwag":66.99,
        "MMLU":25.55,
        "TruthfulQA":34.71,
        "Winogrande":63.38,
        "GSM8K":0.53,
        "DROP":5.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.78,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"084a12f767b31c1fde681bebb14e9a291e506ea8",
        "model_name_for_query":"Fredithefish\/Guanaco-3B-Uncensored"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.18,
        "ARC":40.19,
        "HellaSwag":65.47,
        "MMLU":25.95,
        "TruthfulQA":33.78,
        "Winogrande":65.19,
        "GSM8K":0.53,
        "DROP":8.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"ff054eeff9e3541464383d40b36d182057d01113",
        "model_name_for_query":"Fredithefish\/CrimsonPajama"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":34.18,
        "ARC":41.13,
        "HellaSwag":62.0,
        "MMLU":26.25,
        "TruthfulQA":38.9,
        "Winogrande":65.43,
        "GSM8K":0.76,
        "DROP":4.8,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":7.07,
        "Hub \u2764\ufe0f":137.0,
        "Available on the hub":true,
        "Model sha":"e83e90ba86f87f74aa2731cdab25ccf33976bd66",
        "model_name_for_query":"bigscience\/bloom-7b1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.12,
        "ARC":41.55,
        "HellaSwag":65.48,
        "MMLU":25.03,
        "TruthfulQA":36.41,
        "Winogrande":64.48,
        "GSM8K":1.36,
        "DROP":4.51,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":84.0,
        "Available on the hub":true,
        "Model sha":"0c66778ee09a036886741707733620b91057909a",
        "model_name_for_query":"togethercomputer\/RedPajama-INCITE-Instruct-3B-v1"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":34.11,
        "ARC":37.71,
        "HellaSwag":66.6,
        "MMLU":27.23,
        "TruthfulQA":36.8,
        "Winogrande":63.3,
        "GSM8K":0.99,
        "DROP":6.12,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"1c69905286171d7d3ef3f95f8e1bbc9150bad3cd",
        "model_name_for_query":"heegyu\/WizardVicuna-open-llama-3b-v2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":34.06,
        "ARC":40.96,
        "HellaSwag":66.71,
        "MMLU":26.33,
        "TruthfulQA":31.93,
        "Winogrande":63.69,
        "GSM8K":1.59,
        "DROP":7.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"f00ef7a7b0cc6f02af2a11ac764270dfd61b9e2f",
        "model_name_for_query":"Lazycuber\/pyg-instruct-wizardlm"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":33.99,
        "ARC":40.87,
        "HellaSwag":61.73,
        "MMLU":26.37,
        "TruthfulQA":43.19,
        "Winogrande":60.3,
        "GSM8K":0.53,
        "DROP":4.97,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"c08749034baa053834f1b709b6e7b88b914cd1fb",
        "model_name_for_query":"frank098\/orca_mini_3b_juniper"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":33.98,
        "ARC":39.16,
        "HellaSwag":68.63,
        "MMLU":24.47,
        "TruthfulQA":34.84,
        "Winogrande":65.11,
        "GSM8K":0.76,
        "DROP":4.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.66,
        "Hub \u2764\ufe0f":18.0,
        "Available on the hub":true,
        "Model sha":"9afe4dca5a9dbd71cb90d1050d142837f4c739f6",
        "model_name_for_query":"KoboldAI\/OPT-6.7B-Nerybus-Mix"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":33.95,
        "ARC":34.56,
        "HellaSwag":58.37,
        "MMLU":23.87,
        "TruthfulQA":39.89,
        "Winogrande":60.46,
        "GSM8K":0.0,
        "DROP":20.52,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"aec2f59879ea6dfa5233611c4cf83cf3cb974d40",
        "model_name_for_query":"euclaise\/falcon_1b_stage3_2"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":33.91,
        "ARC":39.59,
        "HellaSwag":68.82,
        "MMLU":26.76,
        "TruthfulQA":31.85,
        "Winogrande":64.17,
        "GSM8K":1.74,
        "DROP":4.45,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.59,
        "Hub \u2764\ufe0f":111.0,
        "Available on the hub":true,
        "Model sha":"35c9d7f32fbb108fb8b5bdd574eb03369d1eed49",
        "model_name_for_query":"EleutherAI\/pythia-12b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":33.87,
        "ARC":38.4,
        "HellaSwag":68.57,
        "MMLU":24.34,
        "TruthfulQA":34.73,
        "Winogrande":65.59,
        "GSM8K":0.68,
        "DROP":4.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.66,
        "Hub \u2764\ufe0f":21.0,
        "Available on the hub":true,
        "Model sha":"9e1f1498391df2c28ce35a9290a5a24b8022a43b",
        "model_name_for_query":"KoboldAI\/OPT-6B-nerys-v2"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":33.87,
        "ARC":28.16,
        "HellaSwag":26.34,
        "MMLU":24.94,
        "TruthfulQA":48.98,
        "Winogrande":78.85,
        "GSM8K":23.81,
        "DROP":5.98,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":35.58,
        "Hub \u2764\ufe0f":71.0,
        "Available on the hub":true,
        "Model sha":"8e42e031bfc8be3bbf31dc546d7c51fb991ff6e0",
        "model_name_for_query":"TheBloke\/guanaco-33B-GPTQ"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":33.84,
        "ARC":34.13,
        "HellaSwag":60.77,
        "MMLU":27.79,
        "TruthfulQA":36.66,
        "Winogrande":58.72,
        "GSM8K":0.23,
        "DROP":18.58,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":7.49,
        "Hub \u2764\ufe0f":43.0,
        "Available on the hub":true,
        "Model sha":"732d59308a844004bd9a4def972cc7c3896a38e0",
        "model_name_for_query":"facebook\/xglm-7.5B"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":33.83,
        "ARC":37.8,
        "HellaSwag":66.5,
        "MMLU":26.64,
        "TruthfulQA":36.46,
        "Winogrande":63.61,
        "GSM8K":0.38,
        "DROP":5.4,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"fdf7f93837808958f9463d3c683314e7f649a088",
        "model_name_for_query":"KnutJaegersberg\/black_goo_recipe_d"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":33.76,
        "ARC":39.51,
        "HellaSwag":65.76,
        "MMLU":26.29,
        "TruthfulQA":36.02,
        "Winogrande":62.51,
        "GSM8K":1.44,
        "DROP":4.81,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.47,
        "Hub \u2764\ufe0f":55.0,
        "Available on the hub":true,
        "Model sha":"d6fc432983b1633a4c1568d121c60de6b8c3e511",
        "model_name_for_query":"matsuo-lab\/weblab-10b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":33.76,
        "ARC":39.93,
        "HellaSwag":65.42,
        "MMLU":26.39,
        "TruthfulQA":35.0,
        "Winogrande":63.38,
        "GSM8K":1.59,
        "DROP":4.6,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"f1c9bac89b74d3487cb092788ce828fb9520c1a7",
        "model_name_for_query":"h2oai\/h2ogpt-oig-oasst1-256-6_9b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":33.74,
        "ARC":45.65,
        "HellaSwag":74.14,
        "MMLU":28.8,
        "TruthfulQA":36.12,
        "Winogrande":51.14,
        "GSM8K":0.0,
        "DROP":0.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":690.0,
        "Available on the hub":true,
        "Model sha":"a5e85ae1941e31bb705adbcafce9b0dfd6f3a48b",
        "model_name_for_query":"mosaicml\/mpt-7b-storywriter"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":33.73,
        "ARC":40.19,
        "HellaSwag":64.77,
        "MMLU":27.03,
        "TruthfulQA":33.23,
        "Winogrande":64.72,
        "GSM8K":1.29,
        "DROP":4.9,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":80.0,
        "Available on the hub":true,
        "Model sha":"094fbdd0c911feb485ce55de1952ab2e75277e1e",
        "model_name_for_query":"togethercomputer\/RedPajama-INCITE-Base-3B-v1"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":33.72,
        "ARC":37.63,
        "HellaSwag":66.72,
        "MMLU":25.68,
        "TruthfulQA":37.09,
        "Winogrande":63.77,
        "GSM8K":0.08,
        "DROP":5.1,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"42faec8429cee8c9f4f5db58ffa193f6f8e0d498",
        "model_name_for_query":"KnutJaegersberg\/black_goo_recipe_b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":33.7,
        "ARC":35.15,
        "HellaSwag":57.62,
        "MMLU":26.07,
        "TruthfulQA":40.65,
        "Winogrande":61.01,
        "GSM8K":0.38,
        "DROP":15.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.05,
        "Hub \u2764\ufe0f":103.0,
        "Available on the hub":true,
        "Model sha":"d1438e22a33b9115af0e47ab3a0fe844cbf588a6",
        "model_name_for_query":"Writer\/camel-5b-hf"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":33.68,
        "ARC":42.58,
        "HellaSwag":49.3,
        "MMLU":32.96,
        "TruthfulQA":42.1,
        "Winogrande":56.27,
        "GSM8K":0.38,
        "DROP":12.16,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.66,
        "Hub \u2764\ufe0f":16.0,
        "Available on the hub":true,
        "Model sha":"14fa470051d0bc38fd871643186a9edfd3a8a9aa",
        "model_name_for_query":"GeorgiaTechResearchInstitute\/galactica-6.7b-evol-instruct-70k"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":33.67,
        "ARC":33.79,
        "HellaSwag":65.74,
        "MMLU":26.44,
        "TruthfulQA":34.57,
        "Winogrande":63.93,
        "GSM8K":0.0,
        "DROP":11.24,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":2.78,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"4201f4b101bad2992efc8452009317a354ec52d2",
        "model_name_for_query":"KoboldAI\/fairseq-dense-2.7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":33.67,
        "ARC":36.69,
        "HellaSwag":65.6,
        "MMLU":24.8,
        "TruthfulQA":38.76,
        "Winogrande":65.11,
        "GSM8K":0.23,
        "DROP":4.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":29.97,
        "Hub \u2764\ufe0f":40.0,
        "Available on the hub":true,
        "Model sha":"a1041efcf9599c962822274e92040710579a5bf2",
        "model_name_for_query":"KoboldAI\/OPT-30B-Erebus"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":33.66,
        "ARC":40.53,
        "HellaSwag":67.47,
        "MMLU":25.73,
        "TruthfulQA":32.53,
        "Winogrande":62.67,
        "GSM8K":1.14,
        "DROP":5.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"e49ed0bde45de0a436bff678ec4872069e8f230c",
        "model_name_for_query":"anhnv125\/pygmalion-6b-roleplay"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":33.66,
        "ARC":39.08,
        "HellaSwag":67.15,
        "MMLU":26.43,
        "TruthfulQA":34.71,
        "Winogrande":63.38,
        "GSM8K":0.3,
        "DROP":4.57,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"333b8c41e42a46a6f3aecaf8f3fa8a17c6d83990",
        "model_name_for_query":"KnutJaegersberg\/LLongMA-3b-LIMA"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":33.63,
        "ARC":40.53,
        "HellaSwag":67.48,
        "MMLU":25.68,
        "TruthfulQA":32.55,
        "Winogrande":62.51,
        "GSM8K":1.14,
        "DROP":5.53,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"f855780745aa34c3bdbe020e4c51253d538cb21e",
        "model_name_for_query":"TehVenom\/DiffMerge_Pygmalion_Main-onto-V8P4"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":33.6,
        "ARC":40.44,
        "HellaSwag":65.58,
        "MMLU":24.9,
        "TruthfulQA":36.68,
        "Winogrande":62.51,
        "GSM8K":0.99,
        "DROP":4.08,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":16.0,
        "Available on the hub":true,
        "Model sha":"029a787e0d98fcd3fecffbfbeb4a75a425474937",
        "model_name_for_query":"h2oai\/h2ogpt-oig-oasst1-512-6_9b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":33.55,
        "ARC":40.36,
        "HellaSwag":66.13,
        "MMLU":28.0,
        "TruthfulQA":33.31,
        "Winogrande":61.64,
        "GSM8K":0.23,
        "DROP":5.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"520c5f1ceb5c90d4011887e2a8d3becf15e7e66e",
        "model_name_for_query":"acrastt\/OmegLLaMA-3B"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":33.54,
        "ARC":36.95,
        "HellaSwag":61.9,
        "MMLU":26.85,
        "TruthfulQA":34.3,
        "Winogrande":63.85,
        "GSM8K":0.0,
        "DROP":10.95,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":3.83,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"0a79832bd57a8cdadc61626fb77bdc26c85b9fa4",
        "model_name_for_query":"Dampish\/StellarX-4B-V0"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":33.52,
        "ARC":39.85,
        "HellaSwag":62.65,
        "MMLU":26.94,
        "TruthfulQA":34.97,
        "Winogrande":64.72,
        "GSM8K":0.45,
        "DROP":5.06,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":112.0,
        "Available on the hub":true,
        "Model sha":"141067009124b9c0aea62c76b3eb952174864057",
        "model_name_for_query":"openlm-research\/open_llama_3b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":33.48,
        "ARC":40.96,
        "HellaSwag":64.54,
        "MMLU":26.58,
        "TruthfulQA":31.65,
        "Winogrande":64.09,
        "GSM8K":1.14,
        "DROP":5.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":2.91,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"2bb7f3842398b048efa4ae2d1aafb9e2f18a8586",
        "model_name_for_query":"ewof\/koishi-instruct-3b"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":33.33,
        "ARC":36.52,
        "HellaSwag":61.76,
        "MMLU":26.94,
        "TruthfulQA":45.05,
        "Winogrande":60.77,
        "GSM8K":0.0,
        "DROP":2.23,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"c5c60ea656e921e6c5415f6feaebac4dd9b2aa2a",
        "model_name_for_query":"pszemraj\/pythia-6.9b-HC3"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":33.31,
        "ARC":40.1,
        "HellaSwag":65.0,
        "MMLU":24.64,
        "TruthfulQA":32.85,
        "Winogrande":64.72,
        "GSM8K":1.06,
        "DROP":4.78,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"b666a6e46eeade607c73ed1334ecda3b9345e4bf",
        "model_name_for_query":"EleutherAI\/pythia-6.7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":33.21,
        "ARC":34.47,
        "HellaSwag":59.81,
        "MMLU":26.37,
        "TruthfulQA":34.15,
        "Winogrande":58.25,
        "GSM8K":0.23,
        "DROP":19.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":7.49,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"dd22eaea8be3fcb8c28f61b513a89d1adac00ffd",
        "model_name_for_query":"pythainlp\/wangchanglm-7.5B-sft-en-sharded"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":33.19,
        "ARC":39.68,
        "HellaSwag":66.31,
        "MMLU":24.96,
        "TruthfulQA":33.65,
        "Winogrande":62.35,
        "GSM8K":0.76,
        "DROP":4.61,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.19,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"922e22a761427e50d7be457b31a76b1126021b8b",
        "model_name_for_query":"RWKV\/rwkv-4-7b-pile"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":33.13,
        "ARC":38.65,
        "HellaSwag":63.53,
        "MMLU":25.16,
        "TruthfulQA":36.07,
        "Winogrande":60.14,
        "GSM8K":0.08,
        "DROP":8.24,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"apache-2.0",
        "#Params (B)":3.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"f477d24b00e05fe4c5f8d5f933080994cfd90e4e",
        "model_name_for_query":"DanielSc4\/RedPajama-INCITE-Chat-3B-v1-FT-LoRA-8bit-test1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":33.1,
        "ARC":36.26,
        "HellaSwag":58.38,
        "MMLU":23.89,
        "TruthfulQA":42.04,
        "Winogrande":59.67,
        "GSM8K":0.99,
        "DROP":10.5,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.34,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":true,
        "Model sha":"7f24d32de53aa4bc150f04ca2418604475173921",
        "model_name_for_query":"OpenBuddy\/openbuddy-openllama-3b-v10-bf16"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":32.99,
        "ARC":29.61,
        "HellaSwag":25.47,
        "MMLU":25.34,
        "TruthfulQA":50.25,
        "Winogrande":75.77,
        "GSM8K":9.93,
        "DROP":14.55,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":16.22,
        "Hub \u2764\ufe0f":242.0,
        "Available on the hub":true,
        "Model sha":"d9b00ec47ae3546398432f0693fe2d5d92bf143b",
        "model_name_for_query":"TheBloke\/Wizard-Vicuna-13B-Uncensored-GPTQ"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":32.96,
        "ARC":36.69,
        "HellaSwag":59.78,
        "MMLU":24.87,
        "TruthfulQA":35.6,
        "Winogrande":57.46,
        "GSM8K":0.45,
        "DROP":15.84,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":2.86,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":true,
        "Model sha":"1ddeea6a7313c8ba8824645d7aa88d5449458f67",
        "model_name_for_query":"RWKV\/rwkv-raven-3b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":32.9,
        "ARC":35.49,
        "HellaSwag":65.56,
        "MMLU":23.83,
        "TruthfulQA":38.32,
        "Winogrande":62.35,
        "GSM8K":0.0,
        "DROP":4.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"c3ef73a8c9dc06fae4bfe4460d2f293147aecbb0",
        "model_name_for_query":"euclaise\/falcon_1b_stage2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":32.87,
        "ARC":33.19,
        "HellaSwag":44.5,
        "MMLU":25.94,
        "TruthfulQA":43.99,
        "Winogrande":67.4,
        "GSM8K":10.08,
        "DROP":4.94,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":26.0,
        "Available on the hub":true,
        "Model sha":"442282f4207442b828953a72c51a919c332cba5c",
        "model_name_for_query":"TheBloke\/CodeLlama-13B-Python-fp16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":32.86,
        "ARC":36.86,
        "HellaSwag":55.1,
        "MMLU":26.7,
        "TruthfulQA":43.45,
        "Winogrande":58.88,
        "GSM8K":0.38,
        "DROP":8.65,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"3235ee41e3793c98749b7bbd2bb80882a12ac889",
        "model_name_for_query":"Azure99\/blossom-v1-3b"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":32.8,
        "ARC":39.42,
        "HellaSwag":64.51,
        "MMLU":27.13,
        "TruthfulQA":37.13,
        "Winogrande":57.7,
        "GSM8K":0.38,
        "DROP":3.32,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":21.83,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"cd72f5954ab5801dd2c1b499e59265f7504f9ee6",
        "model_name_for_query":"Devio\/test-22B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":32.77,
        "ARC":35.15,
        "HellaSwag":62.4,
        "MMLU":24.47,
        "TruthfulQA":40.0,
        "Winogrande":61.48,
        "GSM8K":0.0,
        "DROP":5.89,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"f85d91ff3f6cadc93f7222a19b9c4930c8842366",
        "model_name_for_query":"euclaise\/falcon_1b_stage1"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":32.68,
        "ARC":38.14,
        "HellaSwag":60.01,
        "MMLU":25.92,
        "TruthfulQA":39.19,
        "Winogrande":59.83,
        "GSM8K":1.29,
        "DROP":4.39,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":632.0,
        "Available on the hub":true,
        "Model sha":"7e97fa4b15edd955094c4395d62e6f4290e365b5",
        "model_name_for_query":"cerebras\/Cerebras-GPT-13B"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":32.63,
        "ARC":28.41,
        "HellaSwag":26.05,
        "MMLU":24.71,
        "TruthfulQA":49.54,
        "Winogrande":68.67,
        "GSM8K":5.31,
        "DROP":25.71,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":35.58,
        "Hub \u2764\ufe0f":69.0,
        "Available on the hub":true,
        "Model sha":"cd07cc7c55b46524f61214012653c25226d24c0d",
        "model_name_for_query":"TheBloke\/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GPTQ"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":32.57,
        "ARC":32.68,
        "HellaSwag":47.6,
        "MMLU":28.63,
        "TruthfulQA":40.41,
        "Winogrande":55.56,
        "GSM8K":0.0,
        "DROP":23.11,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"bigcode-openrail-m",
        "#Params (B)":15.52,
        "Hub \u2764\ufe0f":74.0,
        "Available on the hub":true,
        "Model sha":"d866b68daa719239dc44979dbf39a608ed6f7bce",
        "model_name_for_query":"GeorgiaTechResearchInstitute\/starcoder-gpteacher-code-instruct"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":32.44,
        "ARC":35.07,
        "HellaSwag":63.56,
        "MMLU":25.28,
        "TruthfulQA":35.96,
        "Winogrande":62.04,
        "GSM8K":0.53,
        "DROP":4.64,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub \u2764\ufe0f":58.0,
        "Available on the hub":true,
        "Model sha":"e4b9872bb803165eb22f0a867d4e6a64d34fce19",
        "model_name_for_query":"tiiuae\/falcon-rw-1b"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":32.44,
        "ARC":37.37,
        "HellaSwag":60.74,
        "MMLU":25.86,
        "TruthfulQA":35.4,
        "Winogrande":62.12,
        "GSM8K":1.06,
        "DROP":4.51,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.91,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"b9d8cace80b1a97f5ed380711aea31f2d1b24310",
        "model_name_for_query":"EleutherAI\/pythia-2.7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":32.43,
        "ARC":35.32,
        "HellaSwag":54.1,
        "MMLU":23.99,
        "TruthfulQA":43.11,
        "Winogrande":58.8,
        "GSM8K":0.53,
        "DROP":11.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"1a403344de52ddb7f18548a526a927714adfe4d4",
        "model_name_for_query":"Azure99\/blossom-v2-3b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":32.41,
        "ARC":32.59,
        "HellaSwag":43.94,
        "MMLU":27.23,
        "TruthfulQA":44.59,
        "Winogrande":65.04,
        "GSM8K":8.64,
        "DROP":4.87,
        "Type":"instruction-tuned",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":22.0,
        "Available on the hub":true,
        "Model sha":"ea1b775799b477fe22e64f8ac9107f28950b5c87",
        "model_name_for_query":"codellama\/CodeLlama-13b-Python-hf"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":32.38,
        "ARC":34.39,
        "HellaSwag":60.91,
        "MMLU":26.7,
        "TruthfulQA":37.82,
        "Winogrande":61.64,
        "GSM8K":0.3,
        "DROP":4.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":32.0,
        "Available on the hub":true,
        "Model sha":"39ca914ceb82f7f14a38484023bc04f0cd5d0a8d",
        "model_name_for_query":"KoboldAI\/OPT-2.7B-Erebus"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":32.37,
        "ARC":33.79,
        "HellaSwag":58.99,
        "MMLU":24.52,
        "TruthfulQA":34.9,
        "Winogrande":57.93,
        "GSM8K":0.53,
        "DROP":15.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":7.49,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":true,
        "Model sha":"eeee33ea6778a5e66184eeb4bf4294d4316b1933",
        "model_name_for_query":"pythainlp\/wangchanglm-7.5B-sft-enth"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":32.37,
        "ARC":36.26,
        "HellaSwag":61.9,
        "MMLU":25.42,
        "TruthfulQA":36.31,
        "Winogrande":60.77,
        "GSM8K":0.61,
        "DROP":5.28,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"7e2156c14b4b7981a4cd6db7b878888a98144df0",
        "model_name_for_query":"Rallio67\/3B-redpajama-conditional-alpha"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":32.34,
        "ARC":31.31,
        "HellaSwag":52.86,
        "MMLU":27.32,
        "TruthfulQA":42.21,
        "Winogrande":63.06,
        "GSM8K":4.55,
        "DROP":5.07,
        "Type":"instruction-tuned",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":54.0,
        "Available on the hub":true,
        "Model sha":"ec4dd26f30674fdee00ef161b55f464ce28f9c20",
        "model_name_for_query":"codellama\/CodeLlama-7b-Python-hf"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":32.29,
        "ARC":33.7,
        "HellaSwag":61.21,
        "MMLU":26.6,
        "TruthfulQA":37.57,
        "Winogrande":62.04,
        "GSM8K":0.15,
        "DROP":4.79,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":9.0,
        "Available on the hub":true,
        "Model sha":"b4131723cfff1fa42f6cbab546c5b4bb0d19fd83",
        "model_name_for_query":"KoboldAI\/OPT-2.7B-Nerybus-Mix"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":32.29,
        "ARC":33.11,
        "HellaSwag":63.19,
        "MMLU":24.22,
        "TruthfulQA":38.4,
        "Winogrande":62.35,
        "GSM8K":0.0,
        "DROP":4.75,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"025c77e9ee457c6771c5a36dbacd064c269642a5",
        "model_name_for_query":"euclaise\/falcon_1b_stage2"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":32.27,
        "ARC":30.12,
        "HellaSwag":58.82,
        "MMLU":25.12,
        "TruthfulQA":36.74,
        "Winogrande":64.25,
        "GSM8K":0.0,
        "DROP":10.84,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":6.66,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"cbe8d60db6f3c52e653ca73e23a1c34c08127d02",
        "model_name_for_query":"MayaPH\/opt-flan-iml-6.7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":32.2,
        "ARC":34.73,
        "HellaSwag":58.96,
        "MMLU":25.53,
        "TruthfulQA":39.14,
        "Winogrande":61.64,
        "GSM8K":0.61,
        "DROP":4.82,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":2.8,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":false,
        "Model sha":"40e84b6d38aac92a0302c2a682498794ef0fd901",
        "model_name_for_query":"TFLai\/pythia-2.8b-4bit-alpaca"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":32.17,
        "ARC":33.96,
        "HellaSwag":61.43,
        "MMLU":25.43,
        "TruthfulQA":37.43,
        "Winogrande":61.96,
        "GSM8K":0.23,
        "DROP":4.77,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":46.0,
        "Available on the hub":true,
        "Model sha":"397f71a473a150c00f0fe3fc4a2f78ff3ccaf82d",
        "model_name_for_query":"facebook\/opt-2.7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":32.17,
        "ARC":36.01,
        "HellaSwag":63.38,
        "MMLU":25.44,
        "TruthfulQA":37.71,
        "Winogrande":57.77,
        "GSM8K":0.0,
        "DROP":4.86,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"45f57352c10a1fb1ec13c4bf387a15552ca1fe65",
        "model_name_for_query":"aisquared\/chopt-2_7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":32.16,
        "ARC":33.28,
        "HellaSwag":61.23,
        "MMLU":26.44,
        "TruthfulQA":37.23,
        "Winogrande":62.04,
        "GSM8K":0.3,
        "DROP":4.6,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"91d7afd6dbf3bbd1e4ccc6b9a2618d632a8cbb92",
        "model_name_for_query":"KoboldAI\/OPT-2.7B-Nerys-v2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":32.13,
        "ARC":36.86,
        "HellaSwag":59.96,
        "MMLU":25.97,
        "TruthfulQA":32.81,
        "Winogrande":63.69,
        "GSM8K":0.61,
        "DROP":5.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"d8fddf7651dfcae5aefda59d9e868c9111d8bdb3",
        "model_name_for_query":"danielhanchen\/open_llama_3b_600bt_preview"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":32.11,
        "ARC":36.26,
        "HellaSwag":60.66,
        "MMLU":26.78,
        "TruthfulQA":35.56,
        "Winogrande":60.22,
        "GSM8K":0.83,
        "DROP":4.47,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.91,
        "Hub \u2764\ufe0f":12.0,
        "Available on the hub":true,
        "Model sha":"7d977fed8c4ce9649816af8cd5fe36a639cbe5b2",
        "model_name_for_query":"EleutherAI\/pythia-2.8b-deduped"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":32.11,
        "ARC":36.01,
        "HellaSwag":54.3,
        "MMLU":27.66,
        "TruthfulQA":43.38,
        "Winogrande":55.8,
        "GSM8K":0.0,
        "DROP":7.59,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"openrail",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"636b17d6044189343475d1889f076aba73036905",
        "model_name_for_query":"bertin-project\/bertin-gpt-j-6B-alpaca"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":32.08,
        "ARC":28.67,
        "HellaSwag":25.94,
        "MMLU":25.84,
        "TruthfulQA":48.53,
        "Winogrande":74.74,
        "GSM8K":9.63,
        "DROP":11.21,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":16.22,
        "Hub \u2764\ufe0f":99.0,
        "Available on the hub":true,
        "Model sha":"936a51c0219744d7a9598d0c65a7d18e01660601",
        "model_name_for_query":"TheBloke\/wizard-vicuna-13B-GPTQ"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":32.05,
        "ARC":32.34,
        "HellaSwag":47.2,
        "MMLU":29.43,
        "TruthfulQA":41.56,
        "Winogrande":55.17,
        "GSM8K":2.12,
        "DROP":16.55,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"bigscience-openrail-m",
        "#Params (B)":15.52,
        "Hub \u2764\ufe0f":621.0,
        "Available on the hub":true,
        "Model sha":"926ca1b215c4631bc5f8c3e47173381452c23e5c",
        "model_name_for_query":"WizardLM\/WizardCoder-15B-V1.0"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":32.01,
        "ARC":38.48,
        "HellaSwag":57.41,
        "MMLU":25.64,
        "TruthfulQA":39.98,
        "Winogrande":57.46,
        "GSM8K":0.3,
        "DROP":4.83,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.0,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"53ea8f8862fc1820f0cd31f62953b7290fd79867",
        "model_name_for_query":"PSanni\/Deer-3b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":31.93,
        "ARC":29.27,
        "HellaSwag":50.12,
        "MMLU":28.37,
        "TruthfulQA":41.61,
        "Winogrande":64.01,
        "GSM8K":5.16,
        "DROP":4.99,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":54.0,
        "Available on the hub":true,
        "Model sha":"ec4dd26f30674fdee00ef161b55f464ce28f9c20",
        "model_name_for_query":"codellama\/CodeLlama-7b-Python-hf"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":31.78,
        "ARC":34.04,
        "HellaSwag":50.51,
        "MMLU":24.66,
        "TruthfulQA":41.8,
        "Winogrande":54.93,
        "GSM8K":0.0,
        "DROP":16.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":12.0,
        "Available on the hub":true,
        "Model sha":"754e0c90ed5d9241fdfd5a188572b3ea2152eaa7",
        "model_name_for_query":"h2oai\/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":31.77,
        "ARC":35.07,
        "HellaSwag":59.36,
        "MMLU":25.93,
        "TruthfulQA":38.02,
        "Winogrande":58.72,
        "GSM8K":0.53,
        "DROP":4.73,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.66,
        "Hub \u2764\ufe0f":61.0,
        "Available on the hub":true,
        "Model sha":"4f56c6e28f9a2a1c470626f1a064238806f19f09",
        "model_name_for_query":"cerebras\/Cerebras-GPT-6.7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":31.73,
        "ARC":32.59,
        "HellaSwag":45.42,
        "MMLU":25.88,
        "TruthfulQA":42.33,
        "Winogrande":56.04,
        "GSM8K":2.88,
        "DROP":16.98,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":[
            "apache-2.0"
        ],
        "#Params (B)":15.52,
        "Hub \u2764\ufe0f":11.0,
        "Available on the hub":true,
        "Model sha":"979531c84ec0b4e1712d6a5cec6907126a21e605",
        "model_name_for_query":"LoupGarou\/WizardCoder-Guanaco-15B-V1.1"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":31.71,
        "ARC":33.36,
        "HellaSwag":56.24,
        "MMLU":26.45,
        "TruthfulQA":39.78,
        "Winogrande":60.06,
        "GSM8K":1.29,
        "DROP":4.77,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":2.72,
        "Hub \u2764\ufe0f":361.0,
        "Available on the hub":true,
        "Model sha":"e24fa291132763e59f4a5422741b424fb5d59056",
        "model_name_for_query":"EleutherAI\/gpt-neo-2.7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":31.66,
        "ARC":31.14,
        "HellaSwag":58.39,
        "MMLU":24.98,
        "TruthfulQA":37.43,
        "Winogrande":59.04,
        "GSM8K":0.0,
        "DROP":10.6,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":1.41,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"20bf1732212ea81adb45b782a25ce69e65a01ad2",
        "model_name_for_query":"KoboldAI\/fairseq-dense-1.3B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":31.6,
        "ARC":32.0,
        "HellaSwag":53.88,
        "MMLU":31.43,
        "TruthfulQA":38.59,
        "Winogrande":56.83,
        "GSM8K":0.0,
        "DROP":8.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.53,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"1dd7804dbbb547c1be852652ce74568ba41d4e73",
        "model_name_for_query":"bhenrym14\/airoboros-33b-gpt4-1.4.1-PI-8192-fp16"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":31.56,
        "ARC":34.64,
        "HellaSwag":56.74,
        "MMLU":25.55,
        "TruthfulQA":38.55,
        "Winogrande":61.4,
        "GSM8K":0.0,
        "DROP":4.01,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"605b6812956400dbde24ad7b8649a744a2ddfc8e",
        "model_name_for_query":"Dampish\/StellarX-4B-V0.2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":31.56,
        "ARC":31.4,
        "HellaSwag":48.38,
        "MMLU":29.92,
        "TruthfulQA":42.47,
        "Winogrande":55.88,
        "GSM8K":0.0,
        "DROP":12.85,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.56,
        "Hub \u2764\ufe0f":32.0,
        "Available on the hub":true,
        "Model sha":"88ca6f5abe2335bac317e82684e574afdd6046b5",
        "model_name_for_query":"MBZUAI\/LaMini-GPT-1.5B"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":31.5,
        "ARC":35.75,
        "HellaSwag":54.37,
        "MMLU":26.59,
        "TruthfulQA":40.57,
        "Winogrande":57.62,
        "GSM8K":0.83,
        "DROP":4.74,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":3.0,
        "Hub \u2764\ufe0f":70.0,
        "Available on the hub":true,
        "Model sha":"52bc5b43010b4844513826b8be3f78c7344c37d7",
        "model_name_for_query":"bigscience\/bloom-3b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":31.47,
        "ARC":32.85,
        "HellaSwag":60.91,
        "MMLU":25.71,
        "TruthfulQA":37.14,
        "Winogrande":58.64,
        "GSM8K":0.45,
        "DROP":4.56,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.28,
        "Hub \u2764\ufe0f":28.0,
        "Available on the hub":true,
        "Model sha":"b1c3f74c8495e27b3963d64af0781d4a611794f3",
        "model_name_for_query":"princeton-nlp\/Sheared-LLaMA-1.3B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":31.47,
        "ARC":37.12,
        "HellaSwag":61.26,
        "MMLU":25.94,
        "TruthfulQA":34.56,
        "Winogrande":55.96,
        "GSM8K":0.83,
        "DROP":4.58,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":15.0,
        "Available on the hub":true,
        "Model sha":"e2c00dc99f986f2430f5d34c0214969cee786755",
        "model_name_for_query":"KoboldAI\/GPT-J-6B-Adventure"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":31.46,
        "ARC":27.99,
        "HellaSwag":26.06,
        "MMLU":24.24,
        "TruthfulQA":50.08,
        "Winogrande":70.64,
        "GSM8K":13.27,
        "DROP":7.96,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":16.22,
        "Hub \u2764\ufe0f":14.0,
        "Available on the hub":true,
        "Model sha":"5a4c2ea612b71d7c00118f796db7189bc1a0c930",
        "model_name_for_query":"TheBloke\/openchat_v2_openorca_preview-GPTQ"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":31.43,
        "ARC":31.57,
        "HellaSwag":49.43,
        "MMLU":30.76,
        "TruthfulQA":43.66,
        "Winogrande":55.09,
        "GSM8K":2.43,
        "DROP":7.07,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"bigcode-openrail-m",
        "#Params (B)":15.52,
        "Hub \u2764\ufe0f":220.0,
        "Available on the hub":true,
        "Model sha":"b693a7a7d52bed1cd7cc0fe00399db838b09c74f",
        "model_name_for_query":"HuggingFaceH4\/starchat-alpha"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":31.38,
        "ARC":36.01,
        "HellaSwag":55.81,
        "MMLU":25.01,
        "TruthfulQA":37.02,
        "Winogrande":54.85,
        "GSM8K":0.38,
        "DROP":10.61,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":7.56,
        "Hub \u2764\ufe0f":65.0,
        "Available on the hub":true,
        "Model sha":"4c454bfc0e3618b3d574e28ba71369607e637e91",
        "model_name_for_query":"OpenAssistant\/stablelm-7b-sft-v7-epoch-3"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":31.33,
        "ARC":25.34,
        "HellaSwag":26.66,
        "MMLU":23.36,
        "TruthfulQA":49.51,
        "Winogrande":73.72,
        "GSM8K":8.57,
        "DROP":12.15,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":32.53,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"468225a547a8cb0a62758d813cf9606b58506ab4",
        "model_name_for_query":"bhenrym14\/airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-fp16"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":31.3,
        "ARC":27.99,
        "HellaSwag":26.1,
        "MMLU":25.72,
        "TruthfulQA":49.68,
        "Winogrande":74.51,
        "GSM8K":6.9,
        "DROP":8.2,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":16.22,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":true,
        "Model sha":"c4246e4b8d3fc77b9fe4ebb1ead61cda4b83575b",
        "model_name_for_query":"TheBloke\/chronos-wizardlm-uc-scot-st-13B-GPTQ"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":31.17,
        "ARC":34.22,
        "HellaSwag":54.59,
        "MMLU":25.78,
        "TruthfulQA":41.64,
        "Winogrande":56.04,
        "GSM8K":0.45,
        "DROP":5.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.42,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"6cd9b5bc13ee15b5e7e7cfb46477bc6a7c0b5d47",
        "model_name_for_query":"AtAndDev\/ShortKingv0.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":31.16,
        "ARC":31.23,
        "HellaSwag":47.66,
        "MMLU":29.52,
        "TruthfulQA":41.63,
        "Winogrande":57.77,
        "GSM8K":6.07,
        "DROP":4.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"b21bd307ea7417185e7dc59557c399a3e4e0092b",
        "model_name_for_query":"sartmis1\/starcoder-finetune-selfinstruct"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":31.14,
        "ARC":35.07,
        "HellaSwag":57.7,
        "MMLU":25.53,
        "TruthfulQA":36.67,
        "Winogrande":57.7,
        "GSM8K":0.68,
        "DROP":4.63,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.03,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":false,
        "Model sha":"20dd44d78aa09480bf15ca0ecc0c0780951d49a9",
        "model_name_for_query":"PY007\/TinyLlama-1.1B-Chat-v0.3"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":31.12,
        "ARC":32.76,
        "HellaSwag":49.13,
        "MMLU":28.79,
        "TruthfulQA":41.05,
        "Winogrande":56.51,
        "GSM8K":0.15,
        "DROP":9.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.32,
        "Hub \u2764\ufe0f":11.0,
        "Available on the hub":true,
        "Model sha":"a5c7ecc4d908e7a9469d080308af64ae775c733d",
        "model_name_for_query":"MBZUAI\/lamini-neo-1.3b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":31.1,
        "ARC":32.68,
        "HellaSwag":58.77,
        "MMLU":23.23,
        "TruthfulQA":36.21,
        "Winogrande":59.04,
        "GSM8K":0.08,
        "DROP":7.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"8d5e8bb336cb886e20a7570bc00c2381792338a5",
        "model_name_for_query":"L-R\/LLmRa-1.3B"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":31.08,
        "ARC":27.73,
        "HellaSwag":26.01,
        "MMLU":24.97,
        "TruthfulQA":48.69,
        "Winogrande":74.74,
        "GSM8K":8.95,
        "DROP":6.48,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":16.22,
        "Hub \u2764\ufe0f":102.0,
        "Available on the hub":true,
        "Model sha":"848bf2514f804799dd28c188e5428d497dc983fb",
        "model_name_for_query":"TheBloke\/wizard-mega-13B-GPTQ"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":31.07,
        "ARC":31.31,
        "HellaSwag":45.82,
        "MMLU":29.29,
        "TruthfulQA":43.38,
        "Winogrande":57.22,
        "GSM8K":5.53,
        "DROP":4.9,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"eb5f39bac15ccab9463001aa203e33d49f4ff7cb",
        "model_name_for_query":"lizhuang144\/starcoder_mirror"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":31.05,
        "ARC":31.48,
        "HellaSwag":57.95,
        "MMLU":25.43,
        "TruthfulQA":35.84,
        "Winogrande":54.93,
        "GSM8K":0.23,
        "DROP":11.48,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":5.08,
        "Hub \u2764\ufe0f":11.0,
        "Available on the hub":true,
        "Model sha":"dc6a67fac06c8bca7860b84656a0cb736293a7a8",
        "model_name_for_query":"facebook\/xglm-4.5B"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":31.02,
        "ARC":32.0,
        "HellaSwag":51.78,
        "MMLU":26.21,
        "TruthfulQA":40.19,
        "Winogrande":55.41,
        "GSM8K":0.61,
        "DROP":10.95,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":7.56,
        "Hub \u2764\ufe0f":208.0,
        "Available on the hub":true,
        "Model sha":"38366357b5a45e002af2d254ff3d559444ec2147",
        "model_name_for_query":"stabilityai\/stablelm-base-alpha-7b"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":31.0,
        "ARC":36.01,
        "HellaSwag":59.66,
        "MMLU":24.67,
        "TruthfulQA":32.14,
        "Winogrande":58.33,
        "GSM8K":0.68,
        "DROP":5.52,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":2.86,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"7fdda3c5570d4a9711f8f02cc3a20941a5623cd3",
        "model_name_for_query":"RWKV\/rwkv-4-3b-pile"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":30.94,
        "ARC":31.48,
        "HellaSwag":56.63,
        "MMLU":25.35,
        "TruthfulQA":40.19,
        "Winogrande":58.25,
        "GSM8K":0.0,
        "DROP":4.67,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":1.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"fdd3691978f557baf9d1c20d4ede900c47f7e135",
        "model_name_for_query":"aisquared\/chopt-1_3b"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":30.84,
        "ARC":31.91,
        "HellaSwag":55.39,
        "MMLU":27.15,
        "TruthfulQA":37.57,
        "Winogrande":58.09,
        "GSM8K":0.99,
        "DROP":4.8,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.05,
        "Hub \u2764\ufe0f":33.0,
        "Available on the hub":true,
        "Model sha":"df2f3bdb7cbe4295d69cf0cbc35f3ceaf451de82",
        "model_name_for_query":"Writer\/palmyra-base"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":30.82,
        "ARC":38.05,
        "HellaSwag":34.79,
        "MMLU":32.96,
        "TruthfulQA":43.57,
        "Winogrande":66.14,
        "GSM8K":0.0,
        "DROP":0.19,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"45f38e53a579a2b39298cc57ab04078722bebec0",
        "model_name_for_query":"ehartford\/CodeLlama-34b-Python-hf"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":30.81,
        "ARC":33.11,
        "HellaSwag":54.08,
        "MMLU":25.11,
        "TruthfulQA":37.92,
        "Winogrande":59.51,
        "GSM8K":0.0,
        "DROP":5.95,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"593e48197e91537b203ba288260f6580b9cbcbe6",
        "model_name_for_query":"euclaise\/falcon_1b_stage3"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":30.79,
        "ARC":34.3,
        "HellaSwag":54.49,
        "MMLU":24.0,
        "TruthfulQA":41.81,
        "Winogrande":55.25,
        "GSM8K":0.83,
        "DROP":4.88,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.42,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"5b50336208840f557ef3301d841e7994caaa63bb",
        "model_name_for_query":"HWERI\/pythia-1.4b-deduped-sharegpt"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":30.79,
        "ARC":34.3,
        "HellaSwag":54.49,
        "MMLU":24.0,
        "TruthfulQA":41.81,
        "Winogrande":55.25,
        "GSM8K":0.83,
        "DROP":4.88,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.42,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"03dfdc25c111a6a4a16d3da12190697611936426",
        "model_name_for_query":"beaugogh\/pythia-1.4b-deduped-sharegpt"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":30.71,
        "ARC":34.39,
        "HellaSwag":55.94,
        "MMLU":25.07,
        "TruthfulQA":37.68,
        "Winogrande":56.43,
        "GSM8K":0.76,
        "DROP":4.73,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.52,
        "Hub \u2764\ufe0f":18.0,
        "Available on the hub":true,
        "Model sha":"62ec4ff53042f692ef0661e54f371747214707a4",
        "model_name_for_query":"PygmalionAI\/metharme-1.3b"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":30.62,
        "ARC":32.68,
        "HellaSwag":54.96,
        "MMLU":25.56,
        "TruthfulQA":38.66,
        "Winogrande":57.3,
        "GSM8K":0.83,
        "DROP":4.33,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub \u2764\ufe0f":18.0,
        "Available on the hub":true,
        "Model sha":"77f320b24ccae4aa85a5890dbb9514bd11267bb3",
        "model_name_for_query":"EleutherAI\/pythia-1.4b-deduped"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":30.6,
        "ARC":31.91,
        "HellaSwag":50.32,
        "MMLU":25.2,
        "TruthfulQA":41.79,
        "Winogrande":54.38,
        "GSM8K":0.15,
        "DROP":10.48,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.0,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"fe942d5d0faca8156eaf456ecdf569993eab8062",
        "model_name_for_query":"golaxy\/gogpt-3b-bloom"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":30.42,
        "ARC":26.54,
        "HellaSwag":26.1,
        "MMLU":23.12,
        "TruthfulQA":49.16,
        "Winogrande":64.33,
        "GSM8K":0.0,
        "DROP":23.71,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":19.0,
        "Available on the hub":true,
        "Model sha":"6f2924e354c3ab035aa2ff7c7e28d0e5327e2667",
        "model_name_for_query":"Yukang\/Llama-2-13b-chat-longlora-32k-sft"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":30.38,
        "ARC":29.52,
        "HellaSwag":54.53,
        "MMLU":24.96,
        "TruthfulQA":38.71,
        "Winogrande":59.75,
        "GSM8K":0.15,
        "DROP":5.02,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":1.32,
        "Hub \u2764\ufe0f":117.0,
        "Available on the hub":true,
        "Model sha":"8c7b10754972749675d22364c25c428b29face51",
        "model_name_for_query":"facebook\/opt-1.3b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":30.36,
        "ARC":30.46,
        "HellaSwag":45.59,
        "MMLU":26.79,
        "TruthfulQA":46.39,
        "Winogrande":53.12,
        "GSM8K":1.44,
        "DROP":8.71,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":[
            "apache-2.0"
        ],
        "#Params (B)":15.52,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":true,
        "Model sha":"ab5ea678d63eb2324658dcc8cfae267eabc366ef",
        "model_name_for_query":"LoupGarou\/WizardCoder-Guanaco-15B-V1.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":30.32,
        "ARC":32.76,
        "HellaSwag":54.13,
        "MMLU":23.28,
        "TruthfulQA":37.17,
        "Winogrande":56.51,
        "GSM8K":0.0,
        "DROP":8.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":48.0,
        "Available on the hub":true,
        "Model sha":"9533805293bc48e8ddfe9dc1940d8cbc5662113e",
        "model_name_for_query":"PygmalionAI\/pygmalion-2.7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":30.32,
        "ARC":26.11,
        "HellaSwag":26.17,
        "MMLU":23.12,
        "TruthfulQA":49.07,
        "Winogrande":64.09,
        "GSM8K":0.0,
        "DROP":23.66,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":19.0,
        "Available on the hub":true,
        "Model sha":"6f2924e354c3ab035aa2ff7c7e28d0e5327e2667",
        "model_name_for_query":"Yukang\/Llama-2-13b-chat-longlora-32k-sft"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":30.31,
        "ARC":32.0,
        "HellaSwag":54.21,
        "MMLU":26.71,
        "TruthfulQA":39.03,
        "Winogrande":54.93,
        "GSM8K":0.53,
        "DROP":4.75,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub \u2764\ufe0f":22.0,
        "Available on the hub":true,
        "Model sha":"7abc14e7779eabc3a028bc695342869d0410dea2",
        "model_name_for_query":"PY007\/TinyLlama-1.1B-Chat-v0.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":30.29,
        "ARC":29.27,
        "HellaSwag":26.24,
        "MMLU":25.4,
        "TruthfulQA":48.58,
        "Winogrande":71.35,
        "GSM8K":5.38,
        "DROP":5.83,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":16.23,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":true,
        "Model sha":"f14d3df05577f3e1ac35e2c4ec32ce0d39b97508",
        "model_name_for_query":"TheBloke\/EverythingLM-13B-16K-GPTQ"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":30.28,
        "ARC":30.29,
        "HellaSwag":55.12,
        "MMLU":26.13,
        "TruthfulQA":39.15,
        "Winogrande":55.8,
        "GSM8K":0.53,
        "DROP":4.94,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":1.03,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"0bb6ebe1d41d394bae0ed9107ec8d776d9d76a68",
        "model_name_for_query":"lgaalves\/tinyllama-1.1b-chat-v0.3_platypus"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":30.17,
        "ARC":31.83,
        "HellaSwag":52.6,
        "MMLU":25.96,
        "TruthfulQA":37.09,
        "Winogrande":53.91,
        "GSM8K":0.0,
        "DROP":9.82,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":1.41,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"571a3bd891ce33f2ee3fc6de09218178edb0dae2",
        "model_name_for_query":"RWKV\/rwkv-raven-1b5"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":30.11,
        "ARC":31.14,
        "HellaSwag":51.43,
        "MMLU":26.55,
        "TruthfulQA":39.24,
        "Winogrande":57.38,
        "GSM8K":0.99,
        "DROP":4.06,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"34b668ff0acfe56f2d541aa46b385557ee39eb3f",
        "model_name_for_query":"EleutherAI\/pythia-1.3b"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":30.07,
        "ARC":30.72,
        "HellaSwag":53.49,
        "MMLU":24.73,
        "TruthfulQA":39.03,
        "Winogrande":57.77,
        "GSM8K":0.76,
        "DROP":4.03,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.06,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"c85efce322a0f6d93d64f7b9096525753da6913e",
        "model_name_for_query":"NYTK\/PULI-GPTrio"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":30.06,
        "ARC":30.89,
        "HellaSwag":52.97,
        "MMLU":25.0,
        "TruthfulQA":39.55,
        "Winogrande":57.3,
        "GSM8K":0.53,
        "DROP":4.18,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.03,
        "Hub \u2764\ufe0f":15.0,
        "Available on the hub":true,
        "Model sha":"098830e58452a0a08f90eb0189ec5925803fd48b",
        "model_name_for_query":"PY007\/TinyLlama-1.1B-intermediate-step-480k-1T"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":30.03,
        "ARC":32.59,
        "HellaSwag":53.98,
        "MMLU":24.93,
        "TruthfulQA":38.77,
        "Winogrande":54.7,
        "GSM8K":0.23,
        "DROP":5.04,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.56,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":true,
        "Model sha":"97440ff1b6ef749423758e3495cdce1b5e68ee92",
        "model_name_for_query":"aisquared\/dlite-v2-1_5b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":29.98,
        "ARC":31.91,
        "HellaSwag":53.59,
        "MMLU":24.41,
        "TruthfulQA":40.37,
        "Winogrande":53.12,
        "GSM8K":0.83,
        "DROP":5.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":7.56,
        "Hub \u2764\ufe0f":350.0,
        "Available on the hub":true,
        "Model sha":"25071b093c15c0d1cb2b2876c6deb621b764fcf5",
        "model_name_for_query":"stabilityai\/stablelm-tuned-alpha-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":29.91,
        "ARC":31.23,
        "HellaSwag":53.29,
        "MMLU":24.22,
        "TruthfulQA":38.72,
        "Winogrande":57.46,
        "GSM8K":0.45,
        "DROP":3.98,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":3.37,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"fbba77f9894cf738ad8d7d08fc6874856fb42507",
        "model_name_for_query":"winglian\/llama-2-4b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":29.9,
        "ARC":29.52,
        "HellaSwag":50.62,
        "MMLU":26.79,
        "TruthfulQA":39.12,
        "Winogrande":57.54,
        "GSM8K":0.15,
        "DROP":5.57,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":1.56,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"e20cf5a8c89441f4dc15fd2af12dbe72b7df8e60",
        "model_name_for_query":"lgaalves\/gpt-2-xl_camel-ai-physics"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":29.9,
        "ARC":28.92,
        "HellaSwag":52.77,
        "MMLU":25.39,
        "TruthfulQA":37.44,
        "Winogrande":58.96,
        "GSM8K":0.15,
        "DROP":5.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":1.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"5b12df71b21b6b7d76ca9d56de6751f25022e854",
        "model_name_for_query":"jzjiao\/opt-1.3b-rlhf"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":29.86,
        "ARC":27.05,
        "HellaSwag":51.68,
        "MMLU":26.64,
        "TruthfulQA":34.69,
        "Winogrande":59.75,
        "GSM8K":0.15,
        "DROP":9.07,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.06,
        "Hub \u2764\ufe0f":59.0,
        "Available on the hub":true,
        "Model sha":"09dfc839067bf44e7f52976eca8adbc17f04e1b0",
        "model_name_for_query":"EleutherAI\/polyglot-ko-12.8b"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":29.79,
        "ARC":30.63,
        "HellaSwag":47.6,
        "MMLU":27.48,
        "TruthfulQA":41.31,
        "Winogrande":56.04,
        "GSM8K":0.45,
        "DROP":5.03,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":1.72,
        "Hub \u2764\ufe0f":100.0,
        "Available on the hub":true,
        "Model sha":"cc72a88036c2fb937d65efeacc57a0c2ef5d6fe5",
        "model_name_for_query":"bigscience\/bloom-1b7"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":29.72,
        "ARC":31.06,
        "HellaSwag":47.72,
        "MMLU":24.8,
        "TruthfulQA":40.14,
        "Winogrande":55.49,
        "GSM8K":0.38,
        "DROP":8.43,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.79,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"abe5e0f574d32f3234035b6e8c5d68bbb201e03c",
        "model_name_for_query":"Corianas\/Quokka_2.7b"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":29.59,
        "ARC":27.65,
        "HellaSwag":43.81,
        "MMLU":26.3,
        "TruthfulQA":40.26,
        "Winogrande":56.59,
        "GSM8K":0.0,
        "DROP":12.51,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.77,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"4f3bd4b37d249e6aa335be677afd39f417e05b5d",
        "model_name_for_query":"MBZUAI\/LaMini-GPT-774M"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":29.52,
        "ARC":29.27,
        "HellaSwag":49.71,
        "MMLU":26.26,
        "TruthfulQA":40.17,
        "Winogrande":56.59,
        "GSM8K":0.3,
        "DROP":4.38,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub \u2764\ufe0f":14.0,
        "Available on the hub":true,
        "Model sha":"213ebf60d7fdd3258fa5574840b06c97a7e8cf5d",
        "model_name_for_query":"PY007\/TinyLlama-1.1B-intermediate-step-240k-503b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":29.48,
        "ARC":31.66,
        "HellaSwag":49.69,
        "MMLU":25.62,
        "TruthfulQA":37.08,
        "Winogrande":55.96,
        "GSM8K":0.08,
        "DROP":6.29,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.56,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"4ac21faec255e3544e96aeb3591c27bdee5ebf45",
        "model_name_for_query":"aisquared\/dlite-v1-1_5b"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":29.44,
        "ARC":31.23,
        "HellaSwag":48.47,
        "MMLU":24.82,
        "TruthfulQA":39.63,
        "Winogrande":56.91,
        "GSM8K":0.45,
        "DROP":4.6,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":1.37,
        "Hub \u2764\ufe0f":206.0,
        "Available on the hub":true,
        "Model sha":"8282180b53cba30a1575e49de1530019e5931739",
        "model_name_for_query":"EleutherAI\/gpt-neo-1.3B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":29.4,
        "ARC":30.8,
        "HellaSwag":48.88,
        "MMLU":25.12,
        "TruthfulQA":40.24,
        "Winogrande":55.41,
        "GSM8K":0.53,
        "DROP":4.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":true,
        "Model sha":"ae7f22e90cb968b0a73355aa2001d6bc7df28477",
        "model_name_for_query":"lxe\/Cerebras-GPT-2.7B-Alpaca-SP"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":29.32,
        "ARC":30.03,
        "HellaSwag":49.17,
        "MMLU":25.56,
        "TruthfulQA":38.78,
        "Winogrande":55.56,
        "GSM8K":0.76,
        "DROP":5.35,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.56,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"53250831436460254b7ee9afc4014d4d3156b372",
        "model_name_for_query":"MrNJK\/gpt2-xl-sft"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":29.24,
        "ARC":31.83,
        "HellaSwag":52.25,
        "MMLU":25.77,
        "TruthfulQA":35.8,
        "Winogrande":53.83,
        "GSM8K":0.0,
        "DROP":5.23,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":1.41,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"643585471eaf5821d94dfcb498ab5b94a36b42cf",
        "model_name_for_query":"RWKV\/rwkv-4-1b5-pile"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":29.16,
        "ARC":29.1,
        "HellaSwag":49.29,
        "MMLU":25.17,
        "TruthfulQA":41.37,
        "Winogrande":54.14,
        "GSM8K":0.45,
        "DROP":4.58,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":41.0,
        "Available on the hub":true,
        "Model sha":"4383dfd80aafdbcfd0876419d246de51e6cbf7c1",
        "model_name_for_query":"cerebras\/Cerebras-GPT-2.7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":29.15,
        "ARC":30.55,
        "HellaSwag":51.8,
        "MMLU":24.25,
        "TruthfulQA":39.01,
        "Winogrande":54.46,
        "GSM8K":0.23,
        "DROP":3.74,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"5889ec467cf80a83c4092b55686f8121e81bf001",
        "model_name_for_query":"BEE-spoke-data\/TinyLlama-1.1bee"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":29.14,
        "ARC":27.13,
        "HellaSwag":48.69,
        "MMLU":25.6,
        "TruthfulQA":39.11,
        "Winogrande":58.56,
        "GSM8K":0.08,
        "DROP":4.83,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"06249d582b0cfefac537dd6bee2e578002ffff00",
        "model_name_for_query":"shaohang\/SparseOPT-1.3B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":29.13,
        "ARC":27.13,
        "HellaSwag":48.69,
        "MMLU":25.6,
        "TruthfulQA":39.11,
        "Winogrande":58.56,
        "GSM8K":0.08,
        "DROP":4.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"06249d582b0cfefac537dd6bee2e578002ffff00",
        "model_name_for_query":"shaohang\/Sparse0.5_OPT-1.3"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":29.09,
        "ARC":27.65,
        "HellaSwag":39.23,
        "MMLU":25.24,
        "TruthfulQA":42.27,
        "Winogrande":54.78,
        "GSM8K":1.74,
        "DROP":12.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":2.48,
        "Hub \u2764\ufe0f":9.0,
        "Available on the hub":true,
        "Model sha":"8334b22c39937c0404e09dd22a867e2e2a6fc9e0",
        "model_name_for_query":"YeungNLP\/firefly-bloom-2b6-v2"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":29.08,
        "ARC":24.66,
        "HellaSwag":46.76,
        "MMLU":23.49,
        "TruthfulQA":44.47,
        "Winogrande":58.01,
        "GSM8K":0.3,
        "DROP":5.86,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.26,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":true,
        "Model sha":"ade35fd78ac2c29f7a56ffd3087321d297bb97a9",
        "model_name_for_query":"TurkuNLP\/gpt3-finnish-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":29.01,
        "ARC":30.12,
        "HellaSwag":47.68,
        "MMLU":25.37,
        "TruthfulQA":40.0,
        "Winogrande":53.99,
        "GSM8K":0.0,
        "DROP":5.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.77,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"0ea894a33e491912cd1a65dde47b4af03f03c4f2",
        "model_name_for_query":"aisquared\/dlite-v2-774m"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":28.88,
        "ARC":27.3,
        "HellaSwag":25.85,
        "MMLU":25.31,
        "TruthfulQA":48.06,
        "Winogrande":63.77,
        "GSM8K":0.08,
        "DROP":11.77,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":16.22,
        "Hub \u2764\ufe0f":43.0,
        "Available on the hub":true,
        "Model sha":"8ec18e5c597da86fa123c08b6e6bef7da6ec7440",
        "model_name_for_query":"TheBloke\/orca_mini_13B-GPTQ"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":28.77,
        "ARC":29.1,
        "HellaSwag":49.65,
        "MMLU":24.27,
        "TruthfulQA":38.94,
        "Winogrande":53.59,
        "GSM8K":1.14,
        "DROP":4.71,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.08,
        "Hub \u2764\ufe0f":13.0,
        "Available on the hub":true,
        "Model sha":"7199d8fc61a6d565cd1f3c62bf11525b563e13b2",
        "model_name_for_query":"EleutherAI\/pythia-1b-deduped"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":28.77,
        "ARC":30.63,
        "HellaSwag":52.63,
        "MMLU":25.04,
        "TruthfulQA":34.96,
        "Winogrande":52.8,
        "GSM8K":0.0,
        "DROP":5.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.41,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"657e40fe890c2baa1705b45084a93a70b98842eb",
        "model_name_for_query":"KnutJaegersberg\/RWKV-4-PilePlus-1B5-20230520-2942-486Gtokens-ctx4096"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":28.71,
        "ARC":23.55,
        "HellaSwag":36.31,
        "MMLU":25.1,
        "TruthfulQA":45.69,
        "Winogrande":53.12,
        "GSM8K":0.0,
        "DROP":17.24,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":0.56,
        "Hub \u2764\ufe0f":81.0,
        "Available on the hub":true,
        "Model sha":"a2845d7e13dd12efae154a9f1c63fcc2e0cc4b05",
        "model_name_for_query":"bigscience\/bloomz-560m"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":28.65,
        "ARC":28.24,
        "HellaSwag":46.35,
        "MMLU":25.19,
        "TruthfulQA":39.26,
        "Winogrande":56.2,
        "GSM8K":0.23,
        "DROP":5.12,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":1.3,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":false,
        "Model sha":"137d483d1dc757c81c59bd190016f7c5df01f978",
        "model_name_for_query":"TFLai\/gpt-neo-1.3B-4bit-alpaca"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":28.54,
        "ARC":26.79,
        "HellaSwag":43.85,
        "MMLU":26.31,
        "TruthfulQA":39.4,
        "Winogrande":56.91,
        "GSM8K":0.0,
        "DROP":6.55,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":1.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"a1a19acc0ef161bfa35f460c15ed3015595714d8",
        "model_name_for_query":"Rachneet\/gpt2-xl-alpaca"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":28.54,
        "ARC":28.58,
        "HellaSwag":46.08,
        "MMLU":25.11,
        "TruthfulQA":41.34,
        "Winogrande":53.83,
        "GSM8K":0.61,
        "DROP":4.25,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.91,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"b4b0fd71589e6590089e1ec14a840ecab10894ae",
        "model_name_for_query":"w601sxs\/b1ade-1b"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":28.52,
        "ARC":25.85,
        "HellaSwag":45.68,
        "MMLU":25.1,
        "TruthfulQA":37.21,
        "Winogrande":53.91,
        "GSM8K":0.76,
        "DROP":11.14,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":1.73,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"d23a5e8e2164af31a84a26756b9b17f925143050",
        "model_name_for_query":"facebook\/xglm-1.7B"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":28.5,
        "ARC":28.33,
        "HellaSwag":42.78,
        "MMLU":26.7,
        "TruthfulQA":41.8,
        "Winogrande":55.01,
        "GSM8K":0.08,
        "DROP":4.81,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":1.06,
        "Hub \u2764\ufe0f":41.0,
        "Available on the hub":true,
        "Model sha":"6f4195539db0eef1c9d010289f32e0645d9a2354",
        "model_name_for_query":"bigscience\/bloom-1b1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":28.49,
        "ARC":27.47,
        "HellaSwag":37.05,
        "MMLU":23.93,
        "TruthfulQA":42.35,
        "Winogrande":53.51,
        "GSM8K":0.0,
        "DROP":15.13,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":0.56,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"e2bbcbdd534c7d75b7d2f9408e74f6682cf3a05e",
        "model_name_for_query":"cmarkea\/bloomz-560m-sft-chat"
    },
    {
        "T":"\ud83d\udfe6",
        "Average \u2b06\ufe0f":28.45,
        "ARC":26.96,
        "HellaSwag":44.98,
        "MMLU":26.33,
        "TruthfulQA":39.6,
        "Winogrande":56.04,
        "GSM8K":0.08,
        "DROP":5.19,
        "Type":"RL-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"openrail",
        "#Params (B)":0.77,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"6674ad1ed9f518054561b866172eb88b7a769413",
        "model_name_for_query":"Locutusque\/gpt2-large-conversational"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":28.45,
        "ARC":27.73,
        "HellaSwag":42.83,
        "MMLU":26.28,
        "TruthfulQA":41.82,
        "Winogrande":55.64,
        "GSM8K":0.15,
        "DROP":4.71,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":1.06,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"f31188966c6735bd894edacfee8371a6eaf7dbc7",
        "model_name_for_query":"FabbriSimo01\/Bloom_1b_Quantized"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":28.38,
        "ARC":27.22,
        "HellaSwag":41.11,
        "MMLU":25.71,
        "TruthfulQA":45.65,
        "Winogrande":53.91,
        "GSM8K":0.99,
        "DROP":4.06,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"bsd-3-clause",
        "#Params (B)":6.85,
        "Hub \u2764\ufe0f":18.0,
        "Available on the hub":true,
        "Model sha":"2d58b1e73791e8f0be7ea59c2720dccb6f4d0f06",
        "model_name_for_query":"Salesforce\/codegen-6B-multi"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":28.27,
        "ARC":27.82,
        "HellaSwag":44.06,
        "MMLU":23.08,
        "TruthfulQA":42.33,
        "Winogrande":55.01,
        "GSM8K":0.53,
        "DROP":5.06,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":[
            "cc-by-nc-sa-4.0"
        ],
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":106.0,
        "Available on the hub":true,
        "Model sha":"d1c03d2114451d562416b9efe4281d319ceff99e",
        "model_name_for_query":"stabilityai\/stablelm-tuned-alpha-3b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":28.25,
        "ARC":27.65,
        "HellaSwag":44.79,
        "MMLU":23.53,
        "TruthfulQA":41.42,
        "Winogrande":55.49,
        "GSM8K":0.3,
        "DROP":4.61,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":3.5,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"b81c038ee2fa2addd285acde08b1a7ca3cb2854d",
        "model_name_for_query":"Devio\/test-3b"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":28.23,
        "ARC":29.27,
        "HellaSwag":46.29,
        "MMLU":25.25,
        "TruthfulQA":40.49,
        "Winogrande":52.8,
        "GSM8K":0.53,
        "DROP":2.98,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":5.87,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"caefdf7a7c177905b0b16fbe9d4c7ba08def97c2",
        "model_name_for_query":"Kunhao\/pile-7b-250b-tokens"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":28.07,
        "ARC":29.35,
        "HellaSwag":26.32,
        "MMLU":25.44,
        "TruthfulQA":49.51,
        "Winogrande":53.12,
        "GSM8K":0.0,
        "DROP":12.75,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":16.22,
        "Hub \u2764\ufe0f":28.0,
        "Available on the hub":true,
        "Model sha":"12190f743a19e91dfe1f5c77abc0c1bf486073dd",
        "model_name_for_query":"TheBloke\/medalpaca-13B-GPTQ-4bit"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":28.07,
        "ARC":26.88,
        "HellaSwag":42.17,
        "MMLU":25.53,
        "TruthfulQA":40.84,
        "Winogrande":53.59,
        "GSM8K":0.0,
        "DROP":7.44,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.77,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"82eff3a62116fd589ad7319c9d75ff6b12f42f72",
        "model_name_for_query":"Mikivis\/gpt2-large-lora-stf4"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":28.05,
        "ARC":26.79,
        "HellaSwag":44.15,
        "MMLU":25.82,
        "TruthfulQA":39.06,
        "Winogrande":55.09,
        "GSM8K":0.0,
        "DROP":5.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.77,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"1c0c5a686f3c83692e033416197155557e4d3a0d",
        "model_name_for_query":"Mikivis\/gpt2-large-lora-sft"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":28.04,
        "ARC":26.62,
        "HellaSwag":42.68,
        "MMLU":24.72,
        "TruthfulQA":40.31,
        "Winogrande":53.67,
        "GSM8K":0.0,
        "DROP":8.31,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.77,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"1244efb5d20765beb54f6b4a4e1426cf6d5daf44",
        "model_name_for_query":"Mikivis\/gpt2-large-lora-sft2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":27.99,
        "ARC":25.43,
        "HellaSwag":46.67,
        "MMLU":25.3,
        "TruthfulQA":39.19,
        "Winogrande":52.88,
        "GSM8K":0.0,
        "DROP":6.48,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.4,
        "Hub \u2764\ufe0f":6.0,
        "Available on the hub":true,
        "Model sha":"24da1ea670f0638c2df911596e95c764bcd5fb44",
        "model_name_for_query":"KoboldAI\/fairseq-dense-355M"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":27.95,
        "ARC":28.07,
        "HellaSwag":44.35,
        "MMLU":25.91,
        "TruthfulQA":36.11,
        "Winogrande":54.62,
        "GSM8K":0.0,
        "DROP":6.62,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.77,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"d3f5401d07965fb13c2cb8b458ffaed9a5a79c2d",
        "model_name_for_query":"aisquared\/dlite-v1-774m"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":27.87,
        "ARC":26.45,
        "HellaSwag":42.24,
        "MMLU":25.43,
        "TruthfulQA":40.5,
        "Winogrande":53.91,
        "GSM8K":0.45,
        "DROP":6.14,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":[
            "cc-by-sa-4.0"
        ],
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":82.0,
        "Available on the hub":true,
        "Model sha":"99567ccfe45fabe467c71393aa6716106edb83c2",
        "model_name_for_query":"stabilityai\/stablelm-base-alpha-3b"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":27.87,
        "ARC":25.85,
        "HellaSwag":44.1,
        "MMLU":26.78,
        "TruthfulQA":39.51,
        "Winogrande":54.38,
        "GSM8K":0.53,
        "DROP":3.91,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub \u2764\ufe0f":102.0,
        "Available on the hub":true,
        "Model sha":"c1f1ef67c12e4bb85fe0bdf1747c645a202cc118",
        "model_name_for_query":"PY007\/TinyLlama-1.1B-step-50K-105b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":27.78,
        "ARC":24.66,
        "HellaSwag":42.67,
        "MMLU":24.89,
        "TruthfulQA":39.37,
        "Winogrande":54.46,
        "GSM8K":0.0,
        "DROP":8.43,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.77,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"8e26a8d2dc1661d87a8652c75f00b805d63e7330",
        "model_name_for_query":"Mikivis\/gpt2-large-lora-sft1"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":27.67,
        "ARC":28.58,
        "HellaSwag":43.94,
        "MMLU":25.38,
        "TruthfulQA":47.48,
        "Winogrande":47.99,
        "GSM8K":0.0,
        "DROP":0.31,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":3.95,
        "Hub \u2764\ufe0f":22.0,
        "Available on the hub":true,
        "Model sha":"ad56d7fc86db4ad5a7036bc9f80e11cd6f435a60",
        "model_name_for_query":"rinna\/bilingual-gpt-neox-4b-8k"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":27.64,
        "ARC":28.07,
        "HellaSwag":46.96,
        "MMLU":24.12,
        "TruthfulQA":37.64,
        "Winogrande":50.04,
        "GSM8K":0.0,
        "DROP":6.65,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"agpl-3.0",
        "#Params (B)":1.52,
        "Hub \u2764\ufe0f":51.0,
        "Available on the hub":true,
        "Model sha":"bef2c90128c00ff6f16c0f397463423b7d988e17",
        "model_name_for_query":"PygmalionAI\/pygmalion-1.3b"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":27.58,
        "ARC":29.18,
        "HellaSwag":43.73,
        "MMLU":23.1,
        "TruthfulQA":45.0,
        "Winogrande":51.85,
        "GSM8K":0.0,
        "DROP":0.19,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":3.95,
        "Hub \u2764\ufe0f":21.0,
        "Available on the hub":true,
        "Model sha":"f02f6f3c8da0093f3c1ce59220409bc2fa9fbb17",
        "model_name_for_query":"rinna\/bilingual-gpt-neox-4b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":27.53,
        "ARC":28.33,
        "HellaSwag":40.54,
        "MMLU":26.77,
        "TruthfulQA":38.76,
        "Winogrande":52.8,
        "GSM8K":0.0,
        "DROP":5.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.36,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"f51d310aebc16a9fe0d999d2a437b5faff635716",
        "model_name_for_query":"aisquared\/dlite-v2-355m"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":27.47,
        "ARC":28.75,
        "HellaSwag":40.8,
        "MMLU":25.1,
        "TruthfulQA":41.33,
        "Winogrande":52.01,
        "GSM8K":0.0,
        "DROP":4.26,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.77,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":false,
        "Model sha":"f43044cfe7bf0827a176f0d319c63251c2b29373",
        "model_name_for_query":"nicholasKluge\/Aira-2-774M"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":27.43,
        "ARC":24.91,
        "HellaSwag":37.33,
        "MMLU":25.37,
        "TruthfulQA":42.08,
        "Winogrande":54.22,
        "GSM8K":0.0,
        "DROP":8.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.56,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"20529d47b0a82343014727edd1639a9a6a6b09e6",
        "model_name_for_query":"player1537\/dolphinette"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":27.43,
        "ARC":24.83,
        "HellaSwag":41.29,
        "MMLU":25.99,
        "TruthfulQA":40.95,
        "Winogrande":54.38,
        "GSM8K":0.3,
        "DROP":4.26,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.51,
        "Hub \u2764\ufe0f":13.0,
        "Available on the hub":true,
        "Model sha":"c4fc8d586d62df497f1f9b69d66d3ca419992d3e",
        "model_name_for_query":"EleutherAI\/pythia-410m-deduped"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":27.41,
        "ARC":27.39,
        "HellaSwag":38.46,
        "MMLU":25.67,
        "TruthfulQA":42.76,
        "Winogrande":53.51,
        "GSM8K":0.15,
        "DROP":3.91,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"3e68735b9bfbca5c2e6a8e4367f003ab3d3c1512",
        "model_name_for_query":"KnutJaegersberg\/gpt-2-xl-EvolInstruct"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":27.36,
        "ARC":25.94,
        "HellaSwag":38.56,
        "MMLU":26.79,
        "TruthfulQA":42.67,
        "Winogrande":53.51,
        "GSM8K":0.38,
        "DROP":3.71,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":1.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"e2126a42a1c8a938553dd513e4adafec41cb793e",
        "model_name_for_query":"FabbriSimo01\/Cerebras_1.3b_Quantized"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":27.35,
        "ARC":26.28,
        "HellaSwag":38.54,
        "MMLU":26.59,
        "TruthfulQA":42.7,
        "Winogrande":53.43,
        "GSM8K":0.23,
        "DROP":3.7,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.32,
        "Hub \u2764\ufe0f":43.0,
        "Available on the hub":true,
        "Model sha":"5b95400ee8d1e3cc9f79f0dec7182ed9c1009c34",
        "model_name_for_query":"cerebras\/Cerebras-GPT-1.3B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":27.26,
        "ARC":25.85,
        "HellaSwag":39.6,
        "MMLU":24.61,
        "TruthfulQA":43.74,
        "Winogrande":53.12,
        "GSM8K":0.0,
        "DROP":3.89,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":3.37,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"b91c2e5389f4f0ce2d6042fdce5927343d8dcb06",
        "model_name_for_query":"winglian\/basilisk-4b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":27.1,
        "ARC":27.73,
        "HellaSwag":37.91,
        "MMLU":26.66,
        "TruthfulQA":40.14,
        "Winogrande":52.72,
        "GSM8K":0.0,
        "DROP":4.54,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.42,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"8a8d738e841a524d658897d89b9e39e7b9272ed8",
        "model_name_for_query":"Corianas\/Quokka_1.3b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":27.03,
        "ARC":25.94,
        "HellaSwag":38.55,
        "MMLU":25.76,
        "TruthfulQA":45.25,
        "Winogrande":50.2,
        "GSM8K":0.3,
        "DROP":3.24,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.46,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"5e41660ced3edf13c47e933112efd280b710b977",
        "model_name_for_query":"SummerSigh\/GPTNeo350M-Instruct-SFT"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":27.02,
        "ARC":27.3,
        "HellaSwag":38.3,
        "MMLU":26.77,
        "TruthfulQA":39.02,
        "Winogrande":53.04,
        "GSM8K":0.15,
        "DROP":4.57,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.42,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"9831f95df82155ef95ff46a505506bf6194b131a",
        "model_name_for_query":"Corianas\/1.3b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":27.01,
        "ARC":23.63,
        "HellaSwag":34.38,
        "MMLU":24.41,
        "TruthfulQA":46.48,
        "Winogrande":53.83,
        "GSM8K":0.0,
        "DROP":6.33,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"6a00b371146d4bd2903890814485ee1b775162e7",
        "model_name_for_query":"TehVenom\/DiffMerge-DollyGPT-Pygmalion"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":27.0,
        "ARC":27.56,
        "HellaSwag":38.92,
        "MMLU":27.26,
        "TruthfulQA":38.53,
        "Winogrande":53.75,
        "GSM8K":0.0,
        "DROP":2.99,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.36,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"2479f5b1bb62251ec88e60182ba81390a4c19cf9",
        "model_name_for_query":"nicholasKluge\/Aira-2-355M"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":26.94,
        "ARC":27.13,
        "HellaSwag":39.07,
        "MMLU":27.12,
        "TruthfulQA":37.13,
        "Winogrande":52.8,
        "GSM8K":0.0,
        "DROP":5.34,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.36,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"c5f4b5a61e6a66a5c7613164d99a70db5bf7e9a2",
        "model_name_for_query":"aisquared\/dlite-v1-355m"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":26.91,
        "ARC":26.88,
        "HellaSwag":37.96,
        "MMLU":28.43,
        "TruthfulQA":36.45,
        "Winogrande":50.59,
        "GSM8K":0.0,
        "DROP":8.07,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.32,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"502e70081df53edc8a9156acf5a26a11a9dad8fb",
        "model_name_for_query":"MBZUAI\/lamini-cerebras-1.3b"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":26.89,
        "ARC":26.71,
        "HellaSwag":40.01,
        "MMLU":24.85,
        "TruthfulQA":39.58,
        "Winogrande":51.14,
        "GSM8K":0.38,
        "DROP":5.54,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.38,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"a4f6ec80438d4262d1bbc8f385feb2ef1a4a9d6b",
        "model_name_for_query":"RWKV\/rwkv-4-430m-pile"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":26.78,
        "ARC":24.23,
        "HellaSwag":39.18,
        "MMLU":24.32,
        "TruthfulQA":41.51,
        "Winogrande":52.96,
        "GSM8K":0.23,
        "DROP":5.04,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.38,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"b39f8d00fb9f33da4271be2035da848da896a23b",
        "model_name_for_query":"robowaifudev\/megatron-gpt2-345m"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":26.65,
        "ARC":24.91,
        "HellaSwag":38.47,
        "MMLU":26.17,
        "TruthfulQA":41.59,
        "Winogrande":49.88,
        "GSM8K":0.0,
        "DROP":5.51,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.41,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"c8db281477559f5c969a9be794ce236f8a99e1a0",
        "model_name_for_query":"ahxt\/llama2_xs_460M_experimental"
    },
    {
        "T":"\ud83d\udfe6",
        "Average \u2b06\ufe0f":26.63,
        "ARC":26.45,
        "HellaSwag":37.67,
        "MMLU":23.95,
        "TruthfulQA":43.51,
        "Winogrande":50.91,
        "GSM8K":0.08,
        "DROP":3.88,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.56,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"7128cbfcdaf67f1eff27e45d875c35e7b47618db",
        "model_name_for_query":"TheTravellingEngineer\/bloom-560m-RLHF-v2"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":26.63,
        "ARC":26.37,
        "HellaSwag":38.39,
        "MMLU":23.6,
        "TruthfulQA":41.19,
        "Winogrande":52.33,
        "GSM8K":0.0,
        "DROP":4.54,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.36,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"2866eeaaf62014a7a6e939d18b6e27f44df48428",
        "model_name_for_query":"KnutJaegersberg\/megatron-gpt2-345m-evol_instruct_v2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":26.63,
        "ARC":25.43,
        "HellaSwag":37.59,
        "MMLU":24.79,
        "TruthfulQA":43.05,
        "Winogrande":51.46,
        "GSM8K":0.45,
        "DROP":3.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.36,
        "Hub \u2764\ufe0f":18.0,
        "Available on the hub":true,
        "Model sha":"56ab08aaa6802d0f830d42c352d5d536be72811d",
        "model_name_for_query":"xhyi\/PT_GPTNEO350_ATG"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":26.6,
        "ARC":26.02,
        "HellaSwag":40.39,
        "MMLU":24.45,
        "TruthfulQA":37.57,
        "Winogrande":52.41,
        "GSM8K":0.23,
        "DROP":5.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.38,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"e31777c9d3b8c5c9f803b23f49550c009cbdcf6d",
        "model_name_for_query":"KnutJaegersberg\/RWKV-4-PilePlus-430M-20230520-6162-1018Gtokens-ctx4098"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":26.55,
        "ARC":23.38,
        "HellaSwag":34.16,
        "MMLU":25.98,
        "TruthfulQA":40.32,
        "Winogrande":53.2,
        "GSM8K":0.0,
        "DROP":8.85,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":0.75,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"f98b1f9c1bd358dd837d05d443d992c495497606",
        "model_name_for_query":"WangZeJun\/bloom-820m-chat"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":26.5,
        "ARC":25.09,
        "HellaSwag":32.02,
        "MMLU":24.94,
        "TruthfulQA":44.42,
        "Winogrande":54.14,
        "GSM8K":0.0,
        "DROP":4.91,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":0.35,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"df5a3cec54a0bdd22e1644bfe576c7b58eca6bfd",
        "model_name_for_query":"health360\/Healix-410M"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":26.42,
        "ARC":23.81,
        "HellaSwag":34.35,
        "MMLU":26.23,
        "TruthfulQA":43.58,
        "Winogrande":52.57,
        "GSM8K":0.3,
        "DROP":4.13,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":0.33,
        "Hub \u2764\ufe0f":11.0,
        "Available on the hub":true,
        "Model sha":"83ce2f4e78d308968cf7ecd03d86a1f64aea8336",
        "model_name_for_query":"KoboldAI\/OPT-350M-Erebus"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":26.4,
        "ARC":24.74,
        "HellaSwag":37.15,
        "MMLU":24.22,
        "TruthfulQA":42.44,
        "Winogrande":51.93,
        "GSM8K":0.23,
        "DROP":4.11,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":0.56,
        "Hub \u2764\ufe0f":263.0,
        "Available on the hub":true,
        "Model sha":"4f42c91d806a19ae1a46af6c3fb5f4990d884cd6",
        "model_name_for_query":"bigscience\/bloom-560m"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":26.35,
        "ARC":24.06,
        "HellaSwag":35.12,
        "MMLU":24.48,
        "TruthfulQA":41.25,
        "Winogrande":54.78,
        "GSM8K":0.38,
        "DROP":4.39,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.38,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"dc95fda9f1e51d94870e28751e35410c66563d18",
        "model_name_for_query":"KnutJaegersberg\/megatron-GPT-2-345m-EvolInstruct"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":26.32,
        "ARC":23.55,
        "HellaSwag":36.73,
        "MMLU":26.02,
        "TruthfulQA":40.83,
        "Winogrande":52.64,
        "GSM8K":0.3,
        "DROP":4.16,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":0.33,
        "Hub \u2764\ufe0f":75.0,
        "Available on the hub":true,
        "Model sha":"cb32f77e905cccbca1d970436fb0f5e6b58ee3c5",
        "model_name_for_query":"facebook\/opt-350m"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":26.3,
        "ARC":26.37,
        "HellaSwag":31.86,
        "MMLU":25.29,
        "TruthfulQA":43.12,
        "Winogrande":50.75,
        "GSM8K":0.0,
        "DROP":6.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.56,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"82bd8b88b95068eee614a35b790388c5d2415705",
        "model_name_for_query":"golaxy\/gogpt-560m"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":26.23,
        "ARC":25.0,
        "HellaSwag":37.8,
        "MMLU":25.68,
        "TruthfulQA":40.41,
        "Winogrande":50.28,
        "GSM8K":0.53,
        "DROP":3.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.33,
        "Hub \u2764\ufe0f":48.0,
        "Available on the hub":true,
        "Model sha":"d65832d913f6b396e2ffb64c373d9383c9da9303",
        "model_name_for_query":"PygmalionAI\/pygmalion-350m"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":26.23,
        "ARC":23.63,
        "HellaSwag":35.49,
        "MMLU":25.91,
        "TruthfulQA":42.08,
        "Winogrande":51.62,
        "GSM8K":0.68,
        "DROP":4.16,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":0.33,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"59b1019c35ab17a7d77ea1ad32b45a8375ba6e89",
        "model_name_for_query":"KoboldAI\/OPT-350M-Nerys-v2"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":26.19,
        "ARC":24.57,
        "HellaSwag":34.64,
        "MMLU":25.18,
        "TruthfulQA":40.43,
        "Winogrande":52.25,
        "GSM8K":0.23,
        "DROP":6.04,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.56,
        "Hub \u2764\ufe0f":28.0,
        "Available on the hub":true,
        "Model sha":"f3059f01b98ccc877c673149e0178c0e957660f9",
        "model_name_for_query":"facebook\/xglm-564M"
    },
    {
        "T":"\ud83d\udfe6",
        "Average \u2b06\ufe0f":26.17,
        "ARC":24.4,
        "HellaSwag":36.96,
        "MMLU":23.63,
        "TruthfulQA":40.76,
        "Winogrande":53.12,
        "GSM8K":0.3,
        "DROP":3.99,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.56,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"b1769e92f325d8a28e7db1c21f133e6c85b84e78",
        "model_name_for_query":"TheTravellingEngineer\/bloom-560m-RLHF"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":26.11,
        "ARC":22.87,
        "HellaSwag":30.7,
        "MMLU":27.55,
        "TruthfulQA":46.1,
        "Winogrande":52.01,
        "GSM8K":0.68,
        "DROP":2.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":1.31,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"9fd01ce09da870fc66af88616d43e53db642ef46",
        "model_name_for_query":"vikp\/phi2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":26.07,
        "ARC":22.87,
        "HellaSwag":30.53,
        "MMLU":26.56,
        "TruthfulQA":44.99,
        "Winogrande":52.01,
        "GSM8K":0.0,
        "DROP":5.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"9bf3a0db7f6bc960c51f2c0dc6fb66ed982b0180",
        "model_name_for_query":"huggingtweets\/bladeecity-jerma985"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":26.03,
        "ARC":27.65,
        "HellaSwag":35.58,
        "MMLU":24.72,
        "TruthfulQA":39.74,
        "Winogrande":49.01,
        "GSM8K":0.08,
        "DROP":5.41,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.0,
        "Hub \u2764\ufe0f":48.0,
        "Available on the hub":true,
        "Model sha":"1051dacf82ca9fba0ba4a4ff67f1d98a81ef7a2e",
        "model_name_for_query":"beomi\/KoAlpaca-Polyglot-5.8B"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":26.02,
        "ARC":29.44,
        "HellaSwag":25.71,
        "MMLU":25.43,
        "TruthfulQA":49.64,
        "Winogrande":51.93,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"dbd1d1f7ad7b6b359f8246141650b25ca0bb8cbb",
        "model_name_for_query":"uukuguy\/speechless-codellama-orca-airoboros-13b-0.10e"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":26.02,
        "ARC":24.57,
        "HellaSwag":30.22,
        "MMLU":26.74,
        "TruthfulQA":42.85,
        "Winogrande":52.25,
        "GSM8K":0.0,
        "DROP":5.48,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":13.0,
        "Available on the hub":true,
        "Model sha":"f01e73ba67da96f6645be3067158cc493b0cbbcb",
        "model_name_for_query":"MBZUAI\/lamini-neo-125m"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":26.0,
        "ARC":24.06,
        "HellaSwag":34.14,
        "MMLU":23.98,
        "TruthfulQA":43.72,
        "Winogrande":50.59,
        "GSM8K":0.0,
        "DROP":5.5,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"c8fb975220512b34e7b4a9fc570ca333ddcaf9b5",
        "model_name_for_query":"KoboldAI\/fairseq-dense-125M"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.99,
        "ARC":22.87,
        "HellaSwag":28.66,
        "MMLU":25.96,
        "TruthfulQA":51.64,
        "Winogrande":50.43,
        "GSM8K":0.08,
        "DROP":2.29,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":1.31,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"1e4dd330ca90c0ef6d77ca71bd49cbe3d71f26b8",
        "model_name_for_query":"hoskinson-center\/proofGPT-v0.1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.98,
        "ARC":22.61,
        "HellaSwag":32.84,
        "MMLU":24.9,
        "TruthfulQA":43.39,
        "Winogrande":53.12,
        "GSM8K":0.3,
        "DROP":4.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.76,
        "Hub \u2764\ufe0f":60.0,
        "Available on the hub":true,
        "Model sha":"8201db0de8deb68f25e7309db04d163b71970494",
        "model_name_for_query":"ai-forever\/rugpt3large_based_on_gpt2"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":25.95,
        "ARC":21.76,
        "HellaSwag":32.88,
        "MMLU":24.11,
        "TruthfulQA":44.35,
        "Winogrande":51.54,
        "GSM8K":0.0,
        "DROP":7.03,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.88,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"b9a3dd97387fc70d07010d469888a918842d3449",
        "model_name_for_query":"TurkuNLP\/gpt3-finnish-large"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.9,
        "ARC":29.52,
        "HellaSwag":26.49,
        "MMLU":25.98,
        "TruthfulQA":48.97,
        "Winogrande":50.36,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"06253ee259e6b205c4734ab6ec3fa850737b2110",
        "model_name_for_query":"porkorbeef\/Llama-2-13b-sf"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.9,
        "ARC":29.27,
        "HellaSwag":25.74,
        "MMLU":25.69,
        "TruthfulQA":49.61,
        "Winogrande":50.99,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"dbd1d1f7ad7b6b359f8246141650b25ca0bb8cbb",
        "model_name_for_query":"uukuguy\/speechless-codellama-orca-airoboros-13b-0.10e"
    },
    {
        "T":"\ud83d\udfe6",
        "Average \u2b06\ufe0f":25.83,
        "ARC":27.99,
        "HellaSwag":26.19,
        "MMLU":26.86,
        "TruthfulQA":48.88,
        "Winogrande":50.91,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"65bd72580520a1d4a0c19fcb23f68c1f28464e1b",
        "model_name_for_query":"TheTravellingEngineer\/bloom-1b1-RLHF"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.82,
        "ARC":23.29,
        "HellaSwag":28.45,
        "MMLU":24.57,
        "TruthfulQA":50.87,
        "Winogrande":51.14,
        "GSM8K":0.0,
        "DROP":2.43,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":9.0,
        "Available on the hub":true,
        "Model sha":"02f405f08ca0e5b1aaa90a7c3b11303b5f245102",
        "model_name_for_query":"hoskinson-center\/proofGPT-v0.1-6.7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.82,
        "ARC":28.84,
        "HellaSwag":25.63,
        "MMLU":26.5,
        "TruthfulQA":49.26,
        "Winogrande":50.51,
        "GSM8K":0.0,
        "DROP":0.01,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"1d636881854338e571825226c712180da06be72c",
        "model_name_for_query":"yhyhy3\/med-orca-instruct-33b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.81,
        "ARC":29.35,
        "HellaSwag":26.35,
        "MMLU":24.94,
        "TruthfulQA":48.32,
        "Winogrande":51.7,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"06253ee259e6b205c4734ab6ec3fa850737b2110",
        "model_name_for_query":"porkorbeef\/Llama-2-13b"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":25.79,
        "ARC":22.95,
        "HellaSwag":30.26,
        "MMLU":25.97,
        "TruthfulQA":45.58,
        "Winogrande":51.78,
        "GSM8K":0.3,
        "DROP":3.69,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.15,
        "Hub \u2764\ufe0f":132.0,
        "Available on the hub":true,
        "Model sha":"6cb0d322a3a484e99667e7cb240e22f1ac036b99",
        "model_name_for_query":"EleutherAI\/gpt-neo-125m"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.79,
        "ARC":28.75,
        "HellaSwag":26.13,
        "MMLU":24.46,
        "TruthfulQA":49.71,
        "Winogrande":51.46,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"ea7a283403ec1a40570bfc25f2c4b8fcb089b6bb",
        "model_name_for_query":"marcchew\/Marcoroni-7B-LaMini-80K"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.77,
        "ARC":28.41,
        "HellaSwag":26.63,
        "MMLU":25.36,
        "TruthfulQA":47.34,
        "Winogrande":52.64,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"b0dae937b7137790d8946794375e1affd51c760a",
        "model_name_for_query":"doas\/test5"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.7,
        "ARC":29.95,
        "HellaSwag":26.94,
        "MMLU":25.62,
        "TruthfulQA":49.03,
        "Winogrande":48.38,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.67,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"45901e1d6ccb22f5ed8aec3f9dd366823fdd1c33",
        "model_name_for_query":"yeen214\/test_llama2_ko_7b"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":25.69,
        "ARC":26.96,
        "HellaSwag":26.52,
        "MMLU":23.33,
        "TruthfulQA":50.71,
        "Winogrande":49.64,
        "GSM8K":0.0,
        "DROP":2.63,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":3.37,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"293f071b223efd7959f9e1fac66285369aaa959d",
        "model_name_for_query":"winglian\/Llama-2-3b-hf"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.68,
        "ARC":28.92,
        "HellaSwag":25.76,
        "MMLU":25.28,
        "TruthfulQA":49.22,
        "Winogrande":50.59,
        "GSM8K":0.0,
        "DROP":0.01,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"119abfc73f9ce541a40779f167fe21e95faed4e8",
        "model_name_for_query":"uukuguy\/speechless-codellama-orca-platypus-13b-0.10e"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.68,
        "ARC":27.99,
        "HellaSwag":26.0,
        "MMLU":27.04,
        "TruthfulQA":48.59,
        "Winogrande":50.12,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"gpl-3.0",
        "#Params (B)":12.89,
        "Hub \u2764\ufe0f":20.0,
        "Available on the hub":true,
        "Model sha":"826e83e411df32f358893ab21f5eae680499ae9a",
        "model_name_for_query":"IDEA-CCNL\/Ziya-LLaMA-13B-Pretrain-v1"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":25.66,
        "ARC":24.06,
        "HellaSwag":31.39,
        "MMLU":24.86,
        "TruthfulQA":44.34,
        "Winogrande":51.38,
        "GSM8K":0.23,
        "DROP":3.38,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.21,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"582159a2dfe3e712a8d47ae83dec95ae3bde8e7e",
        "model_name_for_query":"EleutherAI\/pythia-160m-deduped"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":25.65,
        "ARC":22.01,
        "HellaSwag":28.99,
        "MMLU":26.83,
        "TruthfulQA":45.98,
        "Winogrande":52.49,
        "GSM8K":0.0,
        "DROP":3.26,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.26,
        "Hub \u2764\ufe0f":20.0,
        "Available on the hub":true,
        "Model sha":"d77812ac95aece1f1edef6745ae2a1b325ad01a4",
        "model_name_for_query":"cerebras\/Cerebras-GPT-256M"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.64,
        "ARC":21.59,
        "HellaSwag":30.18,
        "MMLU":26.13,
        "TruthfulQA":45.38,
        "Winogrande":52.17,
        "GSM8K":0.61,
        "DROP":3.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.69,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"6bdde9a227da60c2db803024d5b2e3a53a41cf0b",
        "model_name_for_query":"FINDA-FIT\/llama-r"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.64,
        "ARC":22.53,
        "HellaSwag":27.37,
        "MMLU":25.38,
        "TruthfulQA":47.09,
        "Winogrande":50.91,
        "GSM8K":0.0,
        "DROP":6.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.04,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"218e8da522cf6fb5566314f37624f27412ae2259",
        "model_name_for_query":"BreadAi\/gpt-YA-1-1_70M"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.63,
        "ARC":28.5,
        "HellaSwag":26.32,
        "MMLU":27.04,
        "TruthfulQA":47.39,
        "Winogrande":50.2,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"e8c03e43eab479a216b5f4f182a711c3624f38bd",
        "model_name_for_query":"marcchew\/LaMini-40k-Platypus2-7B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.62,
        "ARC":24.23,
        "HellaSwag":32.33,
        "MMLU":24.54,
        "TruthfulQA":43.49,
        "Winogrande":50.83,
        "GSM8K":0.38,
        "DROP":3.55,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"9eac24dad1bd7194e38ce8083a0197cee456456c",
        "model_name_for_query":"klosax\/pythia-160m-deduped-step92k-193bt"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":25.62,
        "ARC":24.15,
        "HellaSwag":31.91,
        "MMLU":26.61,
        "TruthfulQA":42.19,
        "Winogrande":48.38,
        "GSM8K":0.08,
        "DROP":6.03,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.67,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"ec721c97ef0e6ebfc578ab98b3ff6e2bd19b3e27",
        "model_name_for_query":"Corianas\/590m"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.6,
        "ARC":29.61,
        "HellaSwag":26.65,
        "MMLU":24.34,
        "TruthfulQA":48.49,
        "Winogrande":50.12,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"f08d224deae510ebf1408ce38bc2610b1e4c77eb",
        "model_name_for_query":"doas\/test2"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":25.6,
        "ARC":21.16,
        "HellaSwag":31.09,
        "MMLU":24.34,
        "TruthfulQA":47.05,
        "Winogrande":50.83,
        "GSM8K":1.74,
        "DROP":2.98,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.11,
        "Hub \u2764\ufe0f":221.0,
        "Available on the hub":true,
        "Model sha":"af2ef45ef8cbe82eb7eb4074f260412bc14c7b11",
        "model_name_for_query":"Deci\/DeciCoder-1b"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":25.59,
        "ARC":28.5,
        "HellaSwag":25.37,
        "MMLU":24.85,
        "TruthfulQA":50.86,
        "Winogrande":49.57,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":9.04,
        "Hub \u2764\ufe0f":150.0,
        "Available on the hub":true,
        "Model sha":"cc30c031fd795ee3d3a50312ab4549415bfbdb46",
        "model_name_for_query":"TheBloke\/WizardLM-7B-uncensored-GPTQ"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":25.58,
        "ARC":25.43,
        "HellaSwag":31.97,
        "MMLU":23.43,
        "TruthfulQA":47.0,
        "Winogrande":51.07,
        "GSM8K":0.0,
        "DROP":0.19,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"b6d0002b10d43ab48aa14e365d9e7b40655ec160",
        "model_name_for_query":"Panchovix\/WizardLM-33B-V1.0-Uncensored-SuperHOT-8k"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.58,
        "ARC":25.09,
        "HellaSwag":26.45,
        "MMLU":26.14,
        "TruthfulQA":51.36,
        "Winogrande":49.57,
        "GSM8K":0.0,
        "DROP":0.45,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"4edde209eea33af491206f8651c0c47e70e08289",
        "model_name_for_query":"BreadAi\/PM_modelV2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.57,
        "ARC":22.44,
        "HellaSwag":30.36,
        "MMLU":25.14,
        "TruthfulQA":45.64,
        "Winogrande":51.22,
        "GSM8K":0.08,
        "DROP":4.12,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"515fd7753c5fecbf4a2951f7cebb2846d91324b3",
        "model_name_for_query":"ogimgio\/gpt-neo-125m-neurallinguisticpioneers"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":25.57,
        "ARC":28.75,
        "HellaSwag":25.88,
        "MMLU":25.36,
        "TruthfulQA":49.27,
        "Winogrande":49.72,
        "GSM8K":0.0,
        "DROP":0.01,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"119abfc73f9ce541a40779f167fe21e95faed4e8",
        "model_name_for_query":"uukuguy\/speechless-codellama-orca-platypus-13b-0.10e"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.56,
        "ARC":27.73,
        "HellaSwag":25.96,
        "MMLU":27.04,
        "TruthfulQA":48.65,
        "Winogrande":49.57,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"gpl-3.0",
        "#Params (B)":12.89,
        "Hub \u2764\ufe0f":251.0,
        "Available on the hub":true,
        "Model sha":"fccf34387d2c9f2f95ff59ae380e6de3718e41ff",
        "model_name_for_query":"IDEA-CCNL\/Ziya-LLaMA-13B-v1"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":25.56,
        "ARC":21.67,
        "HellaSwag":28.34,
        "MMLU":25.55,
        "TruthfulQA":50.87,
        "Winogrande":50.2,
        "GSM8K":0.23,
        "DROP":2.04,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":1.53,
        "Hub \u2764\ufe0f":87.0,
        "Available on the hub":true,
        "Model sha":"065248a99f051da363b1c2cbf05da943c8b6211b",
        "model_name_for_query":"codeparrot\/codeparrot"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":25.55,
        "ARC":23.72,
        "HellaSwag":32.4,
        "MMLU":25.97,
        "TruthfulQA":44.15,
        "Winogrande":48.15,
        "GSM8K":0.45,
        "DROP":3.99,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.59,
        "Hub \u2764\ufe0f":17.0,
        "Available on the hub":true,
        "Model sha":"67a653304fd782a34906d59f3795a37f9e053397",
        "model_name_for_query":"cerebras\/Cerebras-GPT-590M"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.54,
        "ARC":27.73,
        "HellaSwag":24.91,
        "MMLU":23.12,
        "TruthfulQA":52.4,
        "Winogrande":50.51,
        "GSM8K":0.0,
        "DROP":0.1,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":1.32,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"1b13331834190bfe49a176f1661ba4d8309a5051",
        "model_name_for_query":"MayaPH\/FinOPT-Franklin"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.53,
        "ARC":27.65,
        "HellaSwag":26.23,
        "MMLU":26.92,
        "TruthfulQA":47.4,
        "Winogrande":50.51,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"27868e4faed5d68d059c8c57dbd3e24e4933ca28",
        "model_name_for_query":"marcchew\/Marcoroni-7B-LaMini-40K"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":25.53,
        "ARC":22.7,
        "HellaSwag":30.15,
        "MMLU":25.81,
        "TruthfulQA":44.97,
        "Winogrande":51.46,
        "GSM8K":0.15,
        "DROP":3.45,
        "Type":"instruction-tuned",
        "Precision":"4bit",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"7e75e6f4626437305e4d3e7b2aa36f617c517247",
        "model_name_for_query":"lgaalves\/gpt2-dolly"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":25.47,
        "ARC":22.87,
        "HellaSwag":31.47,
        "MMLU":26.02,
        "TruthfulQA":42.87,
        "Winogrande":51.62,
        "GSM8K":0.08,
        "DROP":3.36,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":81.0,
        "Available on the hub":true,
        "Model sha":"3d2b5f275bdf882b8775f902e1bfdb790e2cfc32",
        "model_name_for_query":"facebook\/opt-125m"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.45,
        "ARC":28.07,
        "HellaSwag":26.3,
        "MMLU":25.17,
        "TruthfulQA":48.96,
        "Winogrande":49.64,
        "GSM8K":0.0,
        "DROP":0.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"f7d38ee654e505ad7a454f192d5e3d85cb60b3b8",
        "model_name_for_query":"danielpark\/gorani-100k-llama2-13b-instruct"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.45,
        "ARC":24.49,
        "HellaSwag":25.08,
        "MMLU":26.59,
        "TruthfulQA":52.3,
        "Winogrande":49.64,
        "GSM8K":0.0,
        "DROP":0.04,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.14,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"4807e7df1dfb9d60c6d98e3cfeff62cb6b9a1579",
        "model_name_for_query":"TFLai\/gpt2-turkish-uncased"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.45,
        "ARC":20.9,
        "HellaSwag":28.34,
        "MMLU":25.02,
        "TruthfulQA":45.12,
        "Winogrande":53.28,
        "GSM8K":0.0,
        "DROP":5.47,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.07,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"b288893319b6cdce499148f4482043c350116560",
        "model_name_for_query":"nthngdy\/pythia-owt2-70m-100k"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.44,
        "ARC":28.58,
        "HellaSwag":26.58,
        "MMLU":20.79,
        "TruthfulQA":49.03,
        "Winogrande":53.12,
        "GSM8K":0.0,
        "DROP":0.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"ee9b0cf26f521b5cb2322d743880e8b6bfadb0b7",
        "model_name_for_query":"porkorbeef\/Llama-2-13b-12_153950"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":25.43,
        "ARC":20.99,
        "HellaSwag":28.77,
        "MMLU":26.79,
        "TruthfulQA":47.68,
        "Winogrande":51.22,
        "GSM8K":0.99,
        "DROP":1.57,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"bigcode-openrail-m",
        "#Params (B)":0.16,
        "Hub \u2764\ufe0f":57.0,
        "Available on the hub":true,
        "Model sha":"8547527bef0bc927268c1653cce6948c5c242dd1",
        "model_name_for_query":"bigcode\/tiny_starcoder_py"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.41,
        "ARC":29.52,
        "HellaSwag":26.15,
        "MMLU":23.13,
        "TruthfulQA":48.29,
        "Winogrande":50.75,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"50199ba51c4d002cc86cf3fb2ac921ec52bf4828",
        "model_name_for_query":"marcchew\/Platypus-2-7B-LaMini-14K"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":25.39,
        "ARC":24.32,
        "HellaSwag":31.58,
        "MMLU":25.57,
        "TruthfulQA":40.72,
        "Winogrande":47.91,
        "GSM8K":0.15,
        "DROP":7.45,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.59,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"bab37eb7ba63f6ff9f0eb36a85727146b82ae5ed",
        "model_name_for_query":"MBZUAI\/lamini-cerebras-590m"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.37,
        "ARC":24.4,
        "HellaSwag":26.05,
        "MMLU":25.87,
        "TruthfulQA":49.46,
        "Winogrande":51.62,
        "GSM8K":0.0,
        "DROP":0.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.11,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"09f1ec782ae2243fc605b24eb13ec8d5e4fd2734",
        "model_name_for_query":"SebastianSchramm\/Cerebras-GPT-111M-instruction"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":25.36,
        "ARC":22.78,
        "HellaSwag":30.34,
        "MMLU":24.95,
        "TruthfulQA":44.26,
        "Winogrande":51.54,
        "GSM8K":0.23,
        "DROP":3.45,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.21,
        "Hub \u2764\ufe0f":12.0,
        "Available on the hub":true,
        "Model sha":"50f5173d932e8e61f858120bcb800b97af589f46",
        "model_name_for_query":"EleutherAI\/pythia-160m"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.36,
        "ARC":21.67,
        "HellaSwag":30.91,
        "MMLU":26.57,
        "TruthfulQA":44.01,
        "Winogrande":50.67,
        "GSM8K":0.0,
        "DROP":3.72,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"816206ad02a397161be78dcb70eeda67e0c53132",
        "model_name_for_query":"huggingtweets\/jerma985"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.36,
        "ARC":23.29,
        "HellaSwag":28.68,
        "MMLU":26.72,
        "TruthfulQA":43.79,
        "Winogrande":50.12,
        "GSM8K":0.0,
        "DROP":4.9,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"377b080cf96e10d50289aa3e1fd79c330265f45a",
        "model_name_for_query":"Tincando\/fiction_story_generator"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":25.35,
        "ARC":24.66,
        "HellaSwag":31.23,
        "MMLU":23.13,
        "TruthfulQA":47.44,
        "Winogrande":50.43,
        "GSM8K":0.0,
        "DROP":0.59,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"47c14f699cbbc9bd24458edd86eb70d87552b623",
        "model_name_for_query":"Panchovix\/airoboros-33b-gpt4-1.2-SuperHOT-8k"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.35,
        "ARC":28.16,
        "HellaSwag":26.13,
        "MMLU":25.96,
        "TruthfulQA":47.91,
        "Winogrande":49.33,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"4b231ae58c15244e6e15f0d2f4e26ec37b846229",
        "model_name_for_query":"openbmb\/UltraRM-13b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.35,
        "ARC":22.27,
        "HellaSwag":28.99,
        "MMLU":26.62,
        "TruthfulQA":41.71,
        "Winogrande":52.72,
        "GSM8K":0.23,
        "DROP":4.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":0.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"b1fe75844a07832acd405a4d989a26f6ab7b1c00",
        "model_name_for_query":"Corianas\/256_5epoch"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.35,
        "ARC":28.07,
        "HellaSwag":25.83,
        "MMLU":25.31,
        "TruthfulQA":48.49,
        "Winogrande":49.72,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"7f22882125208d1f54765c21abf84fd162aa454a",
        "model_name_for_query":"vicgalle\/alpaca-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.34,
        "ARC":25.68,
        "HellaSwag":25.4,
        "MMLU":23.12,
        "TruthfulQA":51.15,
        "Winogrande":52.01,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.07,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"6ea42abd94cb0017918f6fe5e71d78bcb7c75548",
        "model_name_for_query":"HWERI\/pythia-70m-deduped-cleansharegpt"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.33,
        "ARC":26.28,
        "HellaSwag":25.6,
        "MMLU":25.89,
        "TruthfulQA":51.24,
        "Winogrande":48.07,
        "GSM8K":0.0,
        "DROP":0.21,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"bigcode-openrail-m",
        "#Params (B)":1.31,
        "Hub \u2764\ufe0f":300.0,
        "Available on the hub":true,
        "Model sha":"132eb6b6cedaf579c2f333f1ecd78a16d7e45978",
        "model_name_for_query":"bigcode\/santacoder"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.33,
        "ARC":29.1,
        "HellaSwag":27.63,
        "MMLU":24.02,
        "TruthfulQA":48.23,
        "Winogrande":48.3,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"90ed388e5503c02f5e6ba8dbc7286687a85ce1c1",
        "model_name_for_query":"Abe13\/juniper-certificate-Llama-2-7b-chat-hf"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":25.28,
        "ARC":21.59,
        "HellaSwag":27.29,
        "MMLU":25.9,
        "TruthfulQA":47.06,
        "Winogrande":51.46,
        "GSM8K":0.3,
        "DROP":3.33,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.1,
        "Hub \u2764\ufe0f":14.0,
        "Available on the hub":true,
        "Model sha":"2ab25ed47af79376eed2baaf8bbb7a192a0c73ff",
        "model_name_for_query":"EleutherAI\/pythia-70m"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.27,
        "ARC":27.65,
        "HellaSwag":26.17,
        "MMLU":24.55,
        "TruthfulQA":48.33,
        "Winogrande":50.2,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"7444355ad764584ef05805f58ccf174bb03e0f46",
        "model_name_for_query":"marcchew\/test1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.26,
        "ARC":25.77,
        "HellaSwag":25.94,
        "MMLU":25.22,
        "TruthfulQA":49.33,
        "Winogrande":50.51,
        "GSM8K":0.0,
        "DROP":0.07,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.04,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"6c1725158a74a41a10f21696a48510d45b4b425b",
        "model_name_for_query":"BreadAi\/MusePy-1-2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.25,
        "ARC":29.95,
        "HellaSwag":26.65,
        "MMLU":22.74,
        "TruthfulQA":49.01,
        "Winogrande":48.38,
        "GSM8K":0.0,
        "DROP":0.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"e1b32a8fcfc0f37fd5f50cf765151897574c73c7",
        "model_name_for_query":"porkorbeef\/Llama-2-13b-public"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.25,
        "ARC":29.44,
        "HellaSwag":25.99,
        "MMLU":23.12,
        "TruthfulQA":48.61,
        "Winogrande":49.57,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":67.0,
        "Available on the hub":true,
        "Model sha":"2c732c2899fc329036d97e5c6f0a61eaff19d97d",
        "model_name_for_query":"openbmb\/UltraLM-13b"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":25.24,
        "ARC":20.73,
        "HellaSwag":29.56,
        "MMLU":25.23,
        "TruthfulQA":46.52,
        "Winogrande":51.14,
        "GSM8K":0.08,
        "DROP":3.42,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":0.76,
        "Hub \u2764\ufe0f":9.0,
        "Available on the hub":true,
        "Model sha":"f9b7a3222967b15169a09bcc86b118ac68a1ad62",
        "model_name_for_query":"cyberagent\/open-calm-large"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.24,
        "ARC":22.7,
        "HellaSwag":27.26,
        "MMLU":25.05,
        "TruthfulQA":51.23,
        "Winogrande":48.78,
        "GSM8K":0.0,
        "DROP":1.64,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":18.0,
        "Available on the hub":true,
        "Model sha":"e5f31df92bfb7b7a808ea8d1c7557488e1bdff7f",
        "model_name_for_query":"microsoft\/CodeGPT-small-py"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.22,
        "ARC":21.5,
        "HellaSwag":28.15,
        "MMLU":25.7,
        "TruthfulQA":44.5,
        "Winogrande":52.41,
        "GSM8K":0.0,
        "DROP":4.28,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.07,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":false,
        "Model sha":"9fce9b8252f7891dbd50299a8c3bd71cd25454db",
        "model_name_for_query":"nthngdy\/pythia-owt2-70m-50k"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.22,
        "ARC":23.89,
        "HellaSwag":25.76,
        "MMLU":24.09,
        "TruthfulQA":51.29,
        "Winogrande":50.83,
        "GSM8K":0.0,
        "DROP":0.68,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"6ff84442217565875450bd7a0457121dcedf6b0b",
        "model_name_for_query":"TaylorAI\/Flash-Llama-30M-20001"
    },
    {
        "T":"\ud83d\udfe6",
        "Average \u2b06\ufe0f":25.22,
        "ARC":23.89,
        "HellaSwag":24.76,
        "MMLU":23.13,
        "TruthfulQA":53.4,
        "Winogrande":51.07,
        "GSM8K":0.0,
        "DROP":0.29,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.19,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"3e1b0bf0a887feeb342982eee4f6d8041772a7dd",
        "model_name_for_query":"breadlicker45\/dough-instruct-base-001"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.22,
        "ARC":23.89,
        "HellaSwag":24.76,
        "MMLU":23.13,
        "TruthfulQA":53.4,
        "Winogrande":51.07,
        "GSM8K":0.0,
        "DROP":0.29,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.15,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"e42b65191f97d786eadaba450f1d34baea470734",
        "model_name_for_query":"breadlicker45\/dough-base-001"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.21,
        "ARC":22.95,
        "HellaSwag":27.29,
        "MMLU":26.25,
        "TruthfulQA":47.02,
        "Winogrande":50.67,
        "GSM8K":0.0,
        "DROP":2.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"b9b3577df726f7984721e4d73741296db50fa782",
        "model_name_for_query":"BreadAi\/gpt-YA-1-1_160M"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":25.21,
        "ARC":23.12,
        "HellaSwag":25.23,
        "MMLU":23.12,
        "TruthfulQA":51.67,
        "Winogrande":51.78,
        "GSM8K":0.0,
        "DROP":1.52,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.03,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"b29a3229f8d5317adeabafeb20677ec7bea9d703",
        "model_name_for_query":"pszemraj\/pythia-31m-KI_v1-2048-scratch"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.2,
        "ARC":26.71,
        "HellaSwag":25.6,
        "MMLU":23.0,
        "TruthfulQA":50.59,
        "Winogrande":49.72,
        "GSM8K":0.0,
        "DROP":0.76,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":0.33,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"7ddc381fa3968df22f72acb6cf03b75d3ac49661",
        "model_name_for_query":"MayaPH\/FinOPT-Lincoln"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.19,
        "ARC":22.1,
        "HellaSwag":28.21,
        "MMLU":26.03,
        "TruthfulQA":46.12,
        "Winogrande":51.54,
        "GSM8K":0.0,
        "DROP":2.37,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":0.04,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"aac86fff08965d84d8bfc3e7c14559d48b8c4c99",
        "model_name_for_query":"klosax\/pythia-70m-deduped-step44k-92bt"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":25.19,
        "ARC":23.21,
        "HellaSwag":26.97,
        "MMLU":24.86,
        "TruthfulQA":50.63,
        "Winogrande":50.28,
        "GSM8K":0.0,
        "DROP":0.39,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":1.1,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"a53eb20b72ae86441566f99acc204d9bb527bf32",
        "model_name_for_query":"nicholasKluge\/Aira-2-1B1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.19,
        "ARC":23.29,
        "HellaSwag":26.15,
        "MMLU":25.04,
        "TruthfulQA":48.16,
        "Winogrande":50.99,
        "GSM8K":0.0,
        "DROP":2.68,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.26,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"a5405585aec0b60c5de7d942ccd58421fe9239be",
        "model_name_for_query":"BreadAi\/DiscordPy"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.17,
        "ARC":23.38,
        "HellaSwag":25.77,
        "MMLU":23.81,
        "TruthfulQA":50.27,
        "Winogrande":52.41,
        "GSM8K":0.0,
        "DROP":0.58,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.77,
        "Hub \u2764\ufe0f":203.0,
        "Available on the hub":true,
        "Model sha":"04e3e47b52dadbcf7688aa61a7ed0438ecf9184c",
        "model_name_for_query":"microsoft\/DialoGPT-large"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":25.17,
        "ARC":27.39,
        "HellaSwag":25.89,
        "MMLU":25.37,
        "TruthfulQA":49.6,
        "Winogrande":47.91,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"1d636881854338e571825226c712180da06be72c",
        "model_name_for_query":"yhyhy3\/med-orca-instruct-33b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":25.16,
        "ARC":22.18,
        "HellaSwag":31.29,
        "MMLU":26.19,
        "TruthfulQA":40.35,
        "Winogrande":51.3,
        "GSM8K":0.15,
        "DROP":4.64,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"745c1864b752525789cad2b75166c519a327325e",
        "model_name_for_query":"lgaalves\/gpt2_open-platypus"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":25.15,
        "ARC":23.55,
        "HellaSwag":31.03,
        "MMLU":26.4,
        "TruthfulQA":40.02,
        "Winogrande":50.12,
        "GSM8K":0.0,
        "DROP":4.96,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"6bf0a8146cf255c829ec2ad83926c8b80945b431",
        "model_name_for_query":"lgaalves\/gpt2_guanaco-dolly-platypus"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":25.15,
        "ARC":23.21,
        "HellaSwag":31.04,
        "MMLU":26.16,
        "TruthfulQA":40.31,
        "Winogrande":50.36,
        "GSM8K":0.0,
        "DROP":4.98,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"bfa144d3eb087e54f1798fd2e2fb17e894cc39d3",
        "model_name_for_query":"lgaalves\/gpt2_platypus-dolly-guanaco"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.15,
        "ARC":20.99,
        "HellaSwag":27.28,
        "MMLU":24.78,
        "TruthfulQA":49.74,
        "Winogrande":52.41,
        "GSM8K":0.0,
        "DROP":0.84,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":0.1,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"692289413c47c219cf83b1596783a8e9223541eb",
        "model_name_for_query":"concedo\/Pythia-70M-ChatSalad"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":25.12,
        "ARC":22.78,
        "HellaSwag":31.58,
        "MMLU":25.66,
        "TruthfulQA":39.17,
        "Winogrande":50.51,
        "GSM8K":0.91,
        "DROP":5.21,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.94,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"f8f24b5480fa43f23d858f0eb8d1af1b7ad0af59",
        "model_name_for_query":"budecosystem\/boomer-1b"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":25.12,
        "ARC":25.77,
        "HellaSwag":25.67,
        "MMLU":27.0,
        "TruthfulQA":48.21,
        "Winogrande":49.17,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":1.32,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"9f1c45d5ce88a8eaf7ec03b760a4adfb5fda07eb",
        "model_name_for_query":"Harshvir\/LaMini-Neo-1.3B-Mental-Health_lora"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.11,
        "ARC":21.76,
        "HellaSwag":28.7,
        "MMLU":26.66,
        "TruthfulQA":41.81,
        "Winogrande":52.01,
        "GSM8K":0.0,
        "DROP":4.86,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.26,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"72df0b6d62d64002575687ea2edbb0df05712678",
        "model_name_for_query":"MBZUAI\/lamini-cerebras-256m"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.11,
        "ARC":24.32,
        "HellaSwag":30.82,
        "MMLU":24.99,
        "TruthfulQA":36.57,
        "Winogrande":51.38,
        "GSM8K":0.0,
        "DROP":7.7,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":14.0,
        "Available on the hub":true,
        "Model sha":"5c67c8c03c08e82d6138ce2a1eddf5317fac3a6b",
        "model_name_for_query":"MBZUAI\/LaMini-GPT-124M"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":25.1,
        "ARC":28.16,
        "HellaSwag":26.55,
        "MMLU":23.17,
        "TruthfulQA":48.79,
        "Winogrande":49.01,
        "GSM8K":0.0,
        "DROP":0.01,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"d9f3e490df2134784afc3a86f5c617a9bab8db4d",
        "model_name_for_query":"bsp-albz\/llama2-13b-platypus-ckpt-1000"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.1,
        "ARC":20.73,
        "HellaSwag":27.03,
        "MMLU":25.31,
        "TruthfulQA":49.19,
        "Winogrande":52.33,
        "GSM8K":0.0,
        "DROP":1.09,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc0-1.0",
        "#Params (B)":0.17,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"3e9187385d31234b04021ddc8b03cbd5cfef9fb4",
        "model_name_for_query":"euclaise\/gpt-neox-122m-minipile-digits"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":25.09,
        "ARC":28.67,
        "HellaSwag":26.41,
        "MMLU":23.12,
        "TruthfulQA":47.94,
        "Winogrande":49.49,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.18,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"e3d26f736b8b47d5275421be6133b81bef84db7d",
        "model_name_for_query":"voidful\/changpt-bart"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.09,
        "ARC":21.42,
        "HellaSwag":27.61,
        "MMLU":26.51,
        "TruthfulQA":47.31,
        "Winogrande":51.14,
        "GSM8K":0.08,
        "DROP":1.55,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"openrail",
        "#Params (B)":0.14,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"f881c740c82ee9bc3191b886ad53f18d741960ea",
        "model_name_for_query":"Locutusque\/gpt2-conversational-or-qa"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.09,
        "ARC":28.5,
        "HellaSwag":25.97,
        "MMLU":23.12,
        "TruthfulQA":48.61,
        "Winogrande":49.41,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":66.0,
        "Available on the hub":true,
        "Model sha":"2ea86d3c02ca0c2abb086a2145e1e85eaea4a23e",
        "model_name_for_query":"victor123\/WizardLM-13B-1.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.08,
        "ARC":22.78,
        "HellaSwag":30.6,
        "MMLU":23.84,
        "TruthfulQA":46.5,
        "Winogrande":50.43,
        "GSM8K":0.0,
        "DROP":1.42,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.14,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"f622239151c89c2db0f1cef495d1b42afd16ce64",
        "model_name_for_query":"alibidaran\/medical_transcription_generator"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":25.07,
        "ARC":23.63,
        "HellaSwag":31.74,
        "MMLU":23.18,
        "TruthfulQA":41.92,
        "Winogrande":50.91,
        "GSM8K":0.45,
        "DROP":3.65,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.13,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"46bdc280eb97b6141d5d51a935e0c4870ecaefcc",
        "model_name_for_query":"RWKV\/rwkv-4-169m-pile"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.06,
        "ARC":21.16,
        "HellaSwag":27.16,
        "MMLU":25.24,
        "TruthfulQA":48.57,
        "Winogrande":50.12,
        "GSM8K":0.0,
        "DROP":3.15,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.04,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"a97ff56bc68a81a9f6147f1590e53511246d1040",
        "model_name_for_query":"HWERI\/pythia-70m-deduped-cleansharegpt-en"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":25.04,
        "ARC":23.04,
        "HellaSwag":31.32,
        "MMLU":26.91,
        "TruthfulQA":39.56,
        "Winogrande":49.64,
        "GSM8K":0.0,
        "DROP":4.79,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"66165ff32ed8de6c39f3524a810f5e97ba6d3347",
        "model_name_for_query":"lgaalves\/gpt2_platypus-camel_physics"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":25.04,
        "ARC":23.04,
        "HellaSwag":31.32,
        "MMLU":26.91,
        "TruthfulQA":39.56,
        "Winogrande":49.64,
        "GSM8K":0.0,
        "DROP":4.79,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"66165ff32ed8de6c39f3524a810f5e97ba6d3347",
        "model_name_for_query":"lgaalves\/gpt2_camel_physics-platypus"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":25.03,
        "ARC":27.05,
        "HellaSwag":26.29,
        "MMLU":24.12,
        "TruthfulQA":48.46,
        "Winogrande":49.33,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"unknown",
        "#Params (B)":0.77,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"c2df1904aa18de22d03ba0fee925e831d8468898",
        "model_name_for_query":"FabbriSimo01\/GPT_Large_Quantized"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":25.03,
        "ARC":22.78,
        "HellaSwag":31.24,
        "MMLU":25.87,
        "TruthfulQA":38.95,
        "Winogrande":51.54,
        "GSM8K":0.0,
        "DROP":4.85,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"cd4d700d13b3bc9371bf45616ef74ac20d165c3d",
        "model_name_for_query":"behnamsh\/gpt2_platypus-camel_physics"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.03,
        "ARC":27.9,
        "HellaSwag":25.61,
        "MMLU":23.08,
        "TruthfulQA":49.57,
        "Winogrande":49.01,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"ab48674ffc55568ffe2a1207ef0e711c2febbaaf",
        "model_name_for_query":"Yukang\/Llama-2-7b-longlora-32k-ft"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.02,
        "ARC":21.84,
        "HellaSwag":31.6,
        "MMLU":25.86,
        "TruthfulQA":40.67,
        "Winogrande":50.12,
        "GSM8K":0.3,
        "DROP":4.78,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"379e02101b4dccba48e7ae792708d2fe7f0bbca2",
        "model_name_for_query":"dpv\/finetuned-gpt2-tiny"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":25.02,
        "ARC":21.84,
        "HellaSwag":31.6,
        "MMLU":25.86,
        "TruthfulQA":40.67,
        "Winogrande":50.12,
        "GSM8K":0.3,
        "DROP":4.78,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.14,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"ef61310a16ffda93bf8f6132e02658482ffc2bcc",
        "model_name_for_query":"SaylorTwift\/gpt2_test"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":25.02,
        "ARC":23.46,
        "HellaSwag":25.23,
        "MMLU":24.57,
        "TruthfulQA":49.4,
        "Winogrande":52.17,
        "GSM8K":0.0,
        "DROP":0.32,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":20.0,
        "Available on the hub":true,
        "Model sha":"8cd14d5339178f1b285f55baee14a0deff7103ac",
        "model_name_for_query":"roneneldan\/TinyStories-1M"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.02,
        "ARC":25.77,
        "HellaSwag":25.79,
        "MMLU":25.81,
        "TruthfulQA":47.49,
        "Winogrande":50.28,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.18,
        "Hub \u2764\ufe0f":55.0,
        "Available on the hub":true,
        "Model sha":"97d0fec744c2cb4d48f5db51d17e3258e185858e",
        "model_name_for_query":"microsoft\/DialoGPT-small"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.02,
        "ARC":24.4,
        "HellaSwag":31.61,
        "MMLU":25.36,
        "TruthfulQA":39.59,
        "Winogrande":50.2,
        "GSM8K":0.0,
        "DROP":3.96,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.67,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"ae0ac41e9be016f6dceac06821fbf6ebacc7edb9",
        "model_name_for_query":"Corianas\/Quokka_590m"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.01,
        "ARC":23.98,
        "HellaSwag":31.1,
        "MMLU":25.29,
        "TruthfulQA":38.98,
        "Winogrande":50.43,
        "GSM8K":0.0,
        "DROP":5.29,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":4.0,
        "Available on the hub":true,
        "Model sha":"bc719f990748ea72be4b6c270df34fc3d37291dc",
        "model_name_for_query":"aisquared\/dlite-v2-124m"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.0,
        "ARC":25.85,
        "HellaSwag":27.6,
        "MMLU":23.1,
        "TruthfulQA":48.89,
        "Winogrande":49.57,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"5f0cfdef590fc9bd7642042fb5f1ed9679260b93",
        "model_name_for_query":"Yukang\/Llama-2-13b-longlora-16k-ft"
    },
    {
        "T":null,
        "Average \u2b06\ufe0f":25.0,
        "ARC":25.0,
        "HellaSwag":25.0,
        "MMLU":25.0,
        "TruthfulQA":25.0,
        "Winogrande":50.0,
        "GSM8K":0.21,
        "DROP":0.47,
        "Type":"",
        "Precision":null,
        "Hub License":null,
        "#Params (B)":null,
        "Hub \u2764\ufe0f":null,
        "Available on the hub":null,
        "Model sha":"N\/A",
        "model_name_for_query":"baseline"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":25.0,
        "ARC":22.1,
        "HellaSwag":32.18,
        "MMLU":24.69,
        "TruthfulQA":39.05,
        "Winogrande":51.14,
        "GSM8K":0.0,
        "DROP":5.83,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":6.53,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"541600070459baf0f1be9560181d5ceb77794085",
        "model_name_for_query":"beomi\/KoRWKV-6B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":25.0,
        "ARC":23.98,
        "HellaSwag":32.25,
        "MMLU":23.37,
        "TruthfulQA":42.29,
        "Winogrande":49.17,
        "GSM8K":0.38,
        "DROP":3.53,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.13,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"1134d31db1aee9fc970d3e9dc4e7314fb8bba500",
        "model_name_for_query":"KnutJaegersberg\/RWKV-4-PilePlus-169M-20230520-done-ctx4096"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":24.98,
        "ARC":22.61,
        "HellaSwag":31.17,
        "MMLU":25.76,
        "TruthfulQA":38.04,
        "Winogrande":52.17,
        "GSM8K":0.3,
        "DROP":4.83,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.14,
        "Hub \u2764\ufe0f":14.0,
        "Available on the hub":true,
        "Model sha":"282e9bd56f0cab5d48e6954793647eecaa0871d9",
        "model_name_for_query":"vicgalle\/gpt2-alpaca-gpt4"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":24.98,
        "ARC":27.22,
        "HellaSwag":25.48,
        "MMLU":24.67,
        "TruthfulQA":49.95,
        "Winogrande":47.51,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":1.13,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"a065961fd627aa3b3e6dde21e77fd5e20f712189",
        "model_name_for_query":"TheBloke\/Llama-2-7b-Chat-AWQ"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":24.95,
        "ARC":21.16,
        "HellaSwag":30.84,
        "MMLU":24.97,
        "TruthfulQA":45.64,
        "Winogrande":47.83,
        "GSM8K":0.53,
        "DROP":3.72,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"openrail",
        "#Params (B)":1.12,
        "Hub \u2764\ufe0f":21.0,
        "Available on the hub":true,
        "Model sha":"291931872cae83498cf984b16319f47f5e9e7a07",
        "model_name_for_query":"bigcode\/gpt_bigcode-santacoder"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":24.95,
        "ARC":28.67,
        "HellaSwag":26.05,
        "MMLU":23.12,
        "TruthfulQA":48.61,
        "Winogrande":48.22,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"962d4e5d8da5a4ec0ec047b6f8f08f1bb9e509fe",
        "model_name_for_query":"Aspik101\/tulu-7b-instruct-pl-lora_unload"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":24.94,
        "ARC":23.46,
        "HellaSwag":31.65,
        "MMLU":24.89,
        "TruthfulQA":39.83,
        "Winogrande":51.62,
        "GSM8K":0.0,
        "DROP":3.16,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.53,
        "Hub \u2764\ufe0f":7.0,
        "Available on the hub":true,
        "Model sha":"427ee72c4350f26de1b287a0c07b842e7d168dbc",
        "model_name_for_query":"beomi\/KoAlpaca-KoRWKV-6B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":24.93,
        "ARC":28.16,
        "HellaSwag":25.43,
        "MMLU":23.48,
        "TruthfulQA":49.06,
        "Winogrande":48.38,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":37.0,
        "Available on the hub":true,
        "Model sha":"242c6469cab41b41d30826e850afa4687e422f24",
        "model_name_for_query":"Yukang\/Llama-2-7b-longlora-100k-ft"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":24.9,
        "ARC":25.77,
        "HellaSwag":26.08,
        "MMLU":24.5,
        "TruthfulQA":47.57,
        "Winogrande":50.36,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.0,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"fc98636655efb7c091bbe5d8014eb138ddfc5471",
        "model_name_for_query":"anas-awadalla\/mpt-1b-redpajama-200b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":24.89,
        "ARC":22.35,
        "HellaSwag":26.19,
        "MMLU":24.37,
        "TruthfulQA":49.1,
        "Winogrande":51.07,
        "GSM8K":0.0,
        "DROP":1.18,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.1,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"5c32081bd3bc1404c2f5b8dbb6f888048bcb7cd7",
        "model_name_for_query":"BreadAi\/StoryPy"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":24.88,
        "ARC":22.87,
        "HellaSwag":28.84,
        "MMLU":26.48,
        "TruthfulQA":39.47,
        "Winogrande":52.25,
        "GSM8K":0.0,
        "DROP":4.27,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.32,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"d4e69f714d360d39979eb7b8cbc9decdb7190c88",
        "model_name_for_query":"Corianas\/Quokka_256m"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":24.87,
        "ARC":25.17,
        "HellaSwag":26.25,
        "MMLU":24.83,
        "TruthfulQA":45.8,
        "Winogrande":51.07,
        "GSM8K":0.0,
        "DROP":1.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"cdd8a6cde7902de39757cf31d73af1f51df0d8e8",
        "model_name_for_query":"MayaPH\/FinOPT-Washington"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":24.86,
        "ARC":23.29,
        "HellaSwag":26.34,
        "MMLU":23.54,
        "TruthfulQA":48.63,
        "Winogrande":48.93,
        "GSM8K":0.0,
        "DROP":3.32,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"de88554a0212c16fdfeda030afb58f831ebcd895",
        "model_name_for_query":"BreadAi\/gpt-Youtube"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":24.86,
        "ARC":24.4,
        "HellaSwag":25.15,
        "MMLU":23.12,
        "TruthfulQA":51.36,
        "Winogrande":49.72,
        "GSM8K":0.0,
        "DROP":0.25,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":0.02,
        "Hub \u2764\ufe0f":15.0,
        "Available on the hub":true,
        "Model sha":"3930ca6bf3976e9b603815403cb373398ae509e5",
        "model_name_for_query":"concedo\/OPT-19M-ChatSalad"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":24.85,
        "ARC":23.12,
        "HellaSwag":25.66,
        "MMLU":23.11,
        "TruthfulQA":51.32,
        "Winogrande":49.88,
        "GSM8K":0.0,
        "DROP":0.86,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.03,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"01a3cd918dd7c233bc0c3c0c948a9a462a5359d1",
        "model_name_for_query":"pszemraj\/pythia-31m-goodwiki-deduped-2048-scratch"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":24.85,
        "ARC":21.93,
        "HellaSwag":31.11,
        "MMLU":25.05,
        "TruthfulQA":40.71,
        "Winogrande":50.12,
        "GSM8K":0.3,
        "DROP":4.73,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.14,
        "Hub \u2764\ufe0f":12.0,
        "Available on the hub":true,
        "Model sha":"e3620b53d164529575db66d9d4f4382311dd713c",
        "model_name_for_query":"crumb\/gpt2023"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":24.82,
        "ARC":27.39,
        "HellaSwag":25.94,
        "MMLU":23.12,
        "TruthfulQA":48.61,
        "Winogrande":48.7,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"instruction-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":70.0,
        "Available on the hub":true,
        "Model sha":"815e2dd7daabe446c429f3c9f70ef01582528f81",
        "model_name_for_query":"WizardLM\/WizardLM-30B-V1.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":24.81,
        "ARC":27.39,
        "HellaSwag":25.94,
        "MMLU":23.12,
        "TruthfulQA":48.61,
        "Winogrande":48.62,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"8bit",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":70.0,
        "Available on the hub":true,
        "Model sha":"815e2dd7daabe446c429f3c9f70ef01582528f81",
        "model_name_for_query":"WizardLM\/WizardLM-30B-V1.0"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":24.79,
        "ARC":26.37,
        "HellaSwag":25.76,
        "MMLU":24.46,
        "TruthfulQA":47.44,
        "Winogrande":49.49,
        "GSM8K":0.0,
        "DROP":0.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.05,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"feea91564dac0081f73aeb6744979c6cfe553fff",
        "model_name_for_query":"anton-l\/gpt-j-tiny-random"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":24.74,
        "ARC":20.48,
        "HellaSwag":30.65,
        "MMLU":25.22,
        "TruthfulQA":44.15,
        "Winogrande":48.54,
        "GSM8K":0.23,
        "DROP":3.93,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":6.66,
        "Hub \u2764\ufe0f":188.0,
        "Available on the hub":true,
        "Model sha":"276a5fb67510554e11ef191a2da44c919acccdf5",
        "model_name_for_query":"cyberagent\/open-calm-7b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":24.74,
        "ARC":24.49,
        "HellaSwag":26.21,
        "MMLU":25.84,
        "TruthfulQA":47.06,
        "Winogrande":49.57,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.36,
        "Hub \u2764\ufe0f":240.0,
        "Available on the hub":true,
        "Model sha":"9d5c5fadcc072b693fb5a5e29416bbf3f503c26c",
        "model_name_for_query":"microsoft\/DialoGPT-medium"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":24.73,
        "ARC":25.77,
        "HellaSwag":25.81,
        "MMLU":23.12,
        "TruthfulQA":47.69,
        "Winogrande":50.75,
        "GSM8K":0.0,
        "DROP":0.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.41,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"8df9f96cc14be8f681c40bd1672b3f3540b70e31",
        "model_name_for_query":"Quake24\/easyTermsSummerizer"
    },
    {
        "T":"\u2b55",
        "Average \u2b06\ufe0f":24.73,
        "ARC":21.76,
        "HellaSwag":30.77,
        "MMLU":24.66,
        "TruthfulQA":42.22,
        "Winogrande":49.57,
        "GSM8K":0.08,
        "DROP":4.08,
        "Type":"instruction-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"52fcf61a8eef255a981be6efde187481086e1a48",
        "model_name_for_query":"lgaalves\/gpt2-dolly"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":24.72,
        "ARC":23.46,
        "HellaSwag":31.12,
        "MMLU":26.27,
        "TruthfulQA":35.97,
        "Winogrande":50.43,
        "GSM8K":0.0,
        "DROP":5.74,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.14,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"ba6ae2b347bc613ae38980e059ec8c5ec8b26038",
        "model_name_for_query":"Mikivis\/xuanxuan"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":24.71,
        "ARC":21.08,
        "HellaSwag":27.17,
        "MMLU":25.26,
        "TruthfulQA":47.51,
        "Winogrande":49.64,
        "GSM8K":0.0,
        "DROP":2.3,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.1,
        "Hub \u2764\ufe0f":14.0,
        "Available on the hub":true,
        "Model sha":"e93a9faa9c77e5d09219f6c868bfc7a1bd65593c",
        "model_name_for_query":"EleutherAI\/pythia-70m-deduped"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":24.7,
        "ARC":21.59,
        "HellaSwag":25.79,
        "MMLU":24.99,
        "TruthfulQA":50.62,
        "Winogrande":48.62,
        "GSM8K":0.0,
        "DROP":1.32,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.03,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"91f011eb99502e667ebc2803f354ce5f5209ccf1",
        "model_name_for_query":"pszemraj\/pythia-31m-simplepile-lite-2048-scratch-2e"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":24.69,
        "ARC":26.37,
        "HellaSwag":26.37,
        "MMLU":23.75,
        "TruthfulQA":47.76,
        "Winogrande":48.62,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"c86de31b80866d047e680e08dbd3572e2965d4c5",
        "model_name_for_query":"Yukang\/Llama-2-7b-longlora-16k-ft"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":24.66,
        "ARC":22.87,
        "HellaSwag":31.14,
        "MMLU":26.26,
        "TruthfulQA":36.22,
        "Winogrande":50.67,
        "GSM8K":0.0,
        "DROP":5.46,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":0.14,
        "Hub \u2764\ufe0f":8.0,
        "Available on the hub":true,
        "Model sha":"e06875a588f7b3386c18a6efdc8cc7583d95b21b",
        "model_name_for_query":"vicgalle\/gpt2-alpaca"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":24.63,
        "ARC":22.78,
        "HellaSwag":25.61,
        "MMLU":23.12,
        "TruthfulQA":49.65,
        "Winogrande":50.51,
        "GSM8K":0.0,
        "DROP":0.72,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.03,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"4eaec0542e7609fd3f364cb34491f05d7c61a3d0",
        "model_name_for_query":"pszemraj\/pythia-31m-simplewiki-scratch-bf16"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":24.63,
        "ARC":23.29,
        "HellaSwag":25.93,
        "MMLU":23.76,
        "TruthfulQA":46.04,
        "Winogrande":53.35,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.14,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"292596e120591887383011c4520bc5b57e7e8993",
        "model_name_for_query":"abhiramtirumala\/DialoGPT-sarcastic-medium"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":24.62,
        "ARC":24.32,
        "HellaSwag":31.16,
        "MMLU":25.08,
        "TruthfulQA":36.38,
        "Winogrande":50.2,
        "GSM8K":0.0,
        "DROP":5.2,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"f6fd5f3960f31881e6cee23f5a872ecc80b40283",
        "model_name_for_query":"aisquared\/dlite-v1-124m"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":24.57,
        "ARC":22.1,
        "HellaSwag":27.12,
        "MMLU":25.51,
        "TruthfulQA":43.79,
        "Winogrande":51.22,
        "GSM8K":0.0,
        "DROP":2.22,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.11,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"e8e347b02f9305e4bc144eb9be2821c518d43183",
        "model_name_for_query":"MBZUAI\/lamini-cerebras-111m"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":24.51,
        "ARC":23.98,
        "HellaSwag":24.92,
        "MMLU":23.35,
        "TruthfulQA":46.68,
        "Winogrande":51.85,
        "GSM8K":0.0,
        "DROP":0.81,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.07,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"18e0bde7e72e477757832f0624a0410efc066216",
        "model_name_for_query":"blueapple8259\/TinyStories-Alpaca"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":24.48,
        "ARC":22.44,
        "HellaSwag":25.0,
        "MMLU":25.51,
        "TruthfulQA":48.7,
        "Winogrande":49.72,
        "GSM8K":0.0,
        "DROP":0.01,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"3f9b43b4db2da4fe3785071dd52c9fc92aa0801d",
        "model_name_for_query":"yeen214\/llama2_7b_small_tuning_v1"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":24.46,
        "ARC":24.4,
        "HellaSwag":29.71,
        "MMLU":23.18,
        "TruthfulQA":41.78,
        "Winogrande":50.67,
        "GSM8K":0.0,
        "DROP":1.49,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"02a1bbcee7b584ace743b2fe4885cc0eaf2179ac",
        "model_name_for_query":"huggingtweets\/gladosystem"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":24.39,
        "ARC":22.78,
        "HellaSwag":25.83,
        "MMLU":23.53,
        "TruthfulQA":48.08,
        "Winogrande":50.43,
        "GSM8K":0.0,
        "DROP":0.07,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.05,
        "Hub \u2764\ufe0f":5.0,
        "Available on the hub":true,
        "Model sha":"52dabea9997faf578489d619249616926e54ed18",
        "model_name_for_query":"roneneldan\/TinyStories-28M"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":24.38,
        "ARC":24.23,
        "HellaSwag":25.69,
        "MMLU":23.82,
        "TruthfulQA":47.64,
        "Winogrande":49.09,
        "GSM8K":0.0,
        "DROP":0.19,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.07,
        "Hub \u2764\ufe0f":61.0,
        "Available on the hub":true,
        "Model sha":"190d22e37cba4b12ddae57d6738a0c65f6ab1aa5",
        "model_name_for_query":"roneneldan\/TinyStories-33M"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":24.35,
        "ARC":19.97,
        "HellaSwag":26.34,
        "MMLU":24.27,
        "TruthfulQA":50.12,
        "Winogrande":49.09,
        "GSM8K":0.0,
        "DROP":0.66,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.03,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"8a3c2f1555de8a3c53d67d73b5d0d53a66a6c6c2",
        "model_name_for_query":"ethzanalytics\/pythia-31m"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":24.35,
        "ARC":22.18,
        "HellaSwag":25.55,
        "MMLU":23.12,
        "TruthfulQA":49.37,
        "Winogrande":49.41,
        "GSM8K":0.0,
        "DROP":0.81,
        "Type":"pretrained",
        "Precision":"torch.bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.03,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"95d47818055661250b55144c7d9beaf05dc126d8",
        "model_name_for_query":"pszemraj\/pythia-31m-simplewiki-2048"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":24.31,
        "ARC":24.57,
        "HellaSwag":24.25,
        "MMLU":25.23,
        "TruthfulQA":45.24,
        "Winogrande":50.91,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":1.27,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"df685c0bbf838f0627383c28f48e577ee901ba68",
        "model_name_for_query":"mncai\/SGPT-1.3B-insurance-epoch10"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":24.31,
        "ARC":24.66,
        "HellaSwag":25.03,
        "MMLU":23.33,
        "TruthfulQA":46.54,
        "Winogrande":50.28,
        "GSM8K":0.0,
        "DROP":0.31,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.02,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"8612e3b15c66ffa94eaa6ee0de5c96edd2d630af",
        "model_name_for_query":"roneneldan\/TinyStories-8M"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":24.27,
        "ARC":21.16,
        "HellaSwag":28.11,
        "MMLU":26.56,
        "TruthfulQA":42.06,
        "Winogrande":49.09,
        "GSM8K":0.0,
        "DROP":2.89,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.39,
        "Hub \u2764\ufe0f":3.0,
        "Available on the hub":true,
        "Model sha":"4c02d48f548103ba53a5e481b8aa81bf7a259287",
        "model_name_for_query":"psyche\/kogpt"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":24.25,
        "ARC":20.48,
        "HellaSwag":28.09,
        "MMLU":24.47,
        "TruthfulQA":46.47,
        "Winogrande":48.22,
        "GSM8K":0.0,
        "DROP":2.02,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.19,
        "Hub \u2764\ufe0f":9.0,
        "Available on the hub":true,
        "Model sha":"20a19af481bf59f38610a2977b2b513e9df51e3a",
        "model_name_for_query":"TurkuNLP\/gpt3-finnish-small"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":24.18,
        "ARC":22.01,
        "HellaSwag":25.58,
        "MMLU":24.99,
        "TruthfulQA":47.33,
        "Winogrande":49.25,
        "GSM8K":0.0,
        "DROP":0.1,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.01,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"cfaf26ec85ecdfc1bd7c2638104cce55cb67f894",
        "model_name_for_query":"roneneldan\/TinyStories-3M"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":24.1,
        "ARC":20.22,
        "HellaSwag":26.73,
        "MMLU":25.51,
        "TruthfulQA":46.31,
        "Winogrande":47.75,
        "GSM8K":0.0,
        "DROP":2.14,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.11,
        "Hub \u2764\ufe0f":62.0,
        "Available on the hub":true,
        "Model sha":"d2b54d7af419055f204690fe0385959616a1723e",
        "model_name_for_query":"cerebras\/Cerebras-GPT-111M"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":24.04,
        "ARC":19.71,
        "HellaSwag":26.68,
        "MMLU":25.28,
        "TruthfulQA":43.72,
        "Winogrande":50.2,
        "GSM8K":0.0,
        "DROP":2.69,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":0.15,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"ee58d79e27f8b9e3984aab29235c5851d2be01d4",
        "model_name_for_query":"Corianas\/111m"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":24.04,
        "ARC":19.71,
        "HellaSwag":26.68,
        "MMLU":25.28,
        "TruthfulQA":43.72,
        "Winogrande":50.2,
        "GSM8K":0.0,
        "DROP":2.69,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":0.15,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"1ac5d244402e2433b6abfcff1fe65e84af15766b",
        "model_name_for_query":"huashiyiqike\/testmodel"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":null,
        "ARC":25.94,
        "HellaSwag":25.76,
        "MMLU":24.65,
        "TruthfulQA":null,
        "Winogrande":50.83,
        "GSM8K":0.0,
        "DROP":0.04,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"07d9d32cd091148295d4e13802ba63486599aff4",
        "model_name_for_query":"Andron00e\/YetAnother_Open-Llama-3B-LoRA-OpenOrca"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":null,
        "ARC":25.94,
        "HellaSwag":25.76,
        "MMLU":24.65,
        "TruthfulQA":null,
        "Winogrande":51.38,
        "GSM8K":0.0,
        "DROP":0.05,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"52c5cb0178831908ed0571f1750fcb0f0fb125f9",
        "model_name_for_query":"Andron00e\/YetAnother_Open-Llama-3B-LoRA"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":null,
        "ARC":28.07,
        "HellaSwag":25.0,
        "MMLU":24.19,
        "TruthfulQA":null,
        "Winogrande":49.09,
        "GSM8K":0.0,
        "DROP":0.03,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":0.07,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"f441866d78feaead3dede6efd9e23990bb74c21e",
        "model_name_for_query":"BreadAi\/MuseCan"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":null,
        "ARC":25.09,
        "HellaSwag":26.05,
        "MMLU":24.51,
        "TruthfulQA":null,
        "Winogrande":51.07,
        "GSM8K":0.0,
        "DROP":0.17,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"fb2a8f95c0286f957c830af640fd5c989081e8e4",
        "model_name_for_query":"Dampish\/Dante-2.8B"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":null,
        "ARC":22.7,
        "HellaSwag":25.04,
        "MMLU":23.12,
        "TruthfulQA":null,
        "Winogrande":59.67,
        "GSM8K":0.15,
        "DROP":5.11,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":1.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"7ef72ccee9d91d06967809e4e63ffbef62a9ad4a",
        "model_name_for_query":"FabbriSimo01\/Facebook_opt_1.3b_Quantized"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":null,
        "ARC":22.7,
        "HellaSwag":25.04,
        "MMLU":23.12,
        "TruthfulQA":null,
        "Winogrande":49.57,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":false,
        "Model sha":"",
        "model_name_for_query":"Rardilit\/Panther_v1"
    },
    {
        "T":"\ud83d\udfe6",
        "Average \u2b06\ufe0f":null,
        "ARC":22.7,
        "HellaSwag":25.04,
        "MMLU":23.12,
        "TruthfulQA":null,
        "Winogrande":49.57,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"RL-tuned",
        "Precision":"torch.float16",
        "Hub License":"?",
        "#Params (B)":1.06,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"05f7f0fd82fb3a5798d4bb284b6c10dd9d380f22",
        "model_name_for_query":"TheTravellingEngineer\/bloom-1b1-RLHF-v2"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":null,
        "ARC":27.22,
        "HellaSwag":26.03,
        "MMLU":25.11,
        "TruthfulQA":null,
        "Winogrande":49.72,
        "GSM8K":0.0,
        "DROP":0.01,
        "Type":"fine-tuned",
        "Precision":"torch.bfloat16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"1357abceda30e8389007a023907824cc3a11e397",
        "model_name_for_query":"ahnyeonchan\/OpenOrca-AYT-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":null,
        "ARC":22.7,
        "HellaSwag":25.04,
        "MMLU":23.12,
        "TruthfulQA":null,
        "Winogrande":49.57,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the hub":true,
        "Model sha":"823a8320224cdac88e927aee00338ffa79395faa",
        "model_name_for_query":"aiplanet\/panda-coder-13B"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":null,
        "ARC":22.7,
        "HellaSwag":25.04,
        "MMLU":23.12,
        "TruthfulQA":null,
        "Winogrande":49.57,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":16.0,
        "Available on the hub":true,
        "Model sha":"42f07d6a86fac5574febb7b8fa13c3b1e14fcebd",
        "model_name_for_query":"clibrain\/Llama-2-ft-instruct-es"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":null,
        "ARC":25.26,
        "HellaSwag":26.55,
        "MMLU":24.7,
        "TruthfulQA":null,
        "Winogrande":59.43,
        "GSM8K":1.06,
        "DROP":5.17,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"mit",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":232.0,
        "Available on the hub":true,
        "Model sha":"f6c9be08f16fe4d3a719bee0a4a7c7415b5c65df",
        "model_name_for_query":"databricks\/dolly-v2-3b"
    },
    {
        "T":"\ud83d\udd36",
        "Average \u2b06\ufe0f":null,
        "ARC":22.7,
        "HellaSwag":25.04,
        "MMLU":23.12,
        "TruthfulQA":null,
        "Winogrande":49.57,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"fine-tuned",
        "Precision":"torch.float16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":10.0,
        "Available on the hub":false,
        "Model sha":"5d17f6b5f394f0745bd4377c8a1290c68051e351",
        "model_name_for_query":"dfurman\/llama-2-13b-dolphin-peft"
    },
    {
        "T":"?",
        "Average \u2b06\ufe0f":null,
        "ARC":26.96,
        "HellaSwag":28.87,
        "MMLU":24.03,
        "TruthfulQA":null,
        "Winogrande":48.38,
        "GSM8K":0.0,
        "DROP":0.33,
        "Type":"",
        "Precision":"torch.float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":0.56,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"5571f87f557b909e863005c6e3870bc2e77341a7",
        "model_name_for_query":"jslin09\/bloom-560m-finetuned-fraud"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":null,
        "ARC":22.7,
        "HellaSwag":25.48,
        "MMLU":27.11,
        "TruthfulQA":null,
        "Winogrande":49.72,
        "GSM8K":0.0,
        "DROP":0.17,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub \u2764\ufe0f":0.0,
        "Available on the hub":true,
        "Model sha":"aea467410ae0cead4fded6b98a3575e92b22862f",
        "model_name_for_query":"team-lucid\/mptk-1b"
    },
    {
        "T":"\ud83d\udfe2",
        "Average \u2b06\ufe0f":null,
        "ARC":22.7,
        "HellaSwag":25.04,
        "MMLU":23.12,
        "TruthfulQA":null,
        "Winogrande":49.57,
        "GSM8K":0.0,
        "DROP":0.0,
        "Type":"pretrained",
        "Precision":"torch.float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":1.0,
        "Available on the hub":true,
        "Model sha":"55f8f1874aa8bf4fc28c0abc92c7fbd1271ff7d7",
        "model_name_for_query":"wtang06\/mpt-125m-c4"
    }
]