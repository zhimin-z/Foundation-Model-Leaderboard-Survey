[
    {
        "table_id":2571,
        "row_id":14849,
        "rank":1,
        "method":"9xFGA (VGG)",
        "mlmodel":{

        },
        "method_short":"9xFGA ",
        "method_details":"VGG",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-11",
        "metrics":{
            "MRR":"68.92",
            "R@1":"55.16",
            "R@10":"92.95",
            "R@5":"86.26",
            "Mean Rank":"3.39"
        },
        "raw_metrics":{
            "MRR":68.92,
            "R@1":55.16,
            "R@10":92.95,
            "R@5":86.26,
            "Mean Rank":3.39
        },
        "uses_additional_data":false,
        "paper":{
            "id":111320,
            "title":"Factor Graph Attention",
            "url":"\/paper\/factor-graph-attention",
            "published":"2019-04-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/factor-graph-attention\/review\/?hl=14849"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2571,
        "row_id":14850,
        "rank":2,
        "method":"DAN",
        "mlmodel":{

        },
        "method_short":"DAN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-02-25",
        "metrics":{
            "MRR":"66.38",
            "R@1":"53.33",
            "R@10":"90.38",
            "R@5":"82.42",
            "Mean Rank":"4.04"
        },
        "raw_metrics":{
            "MRR":66.38,
            "R@1":53.33,
            "R@10":90.38,
            "R@5":82.42,
            "Mean Rank":4.04
        },
        "uses_additional_data":false,
        "paper":{
            "id":106715,
            "title":"Dual Attention Networks for Visual Reference Resolution in Visual Dialog",
            "url":"\/paper\/dual-attention-networks-for-visual-reference",
            "published":"2019-02-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dual-attention-networks-for-visual-reference\/review\/?hl=14850"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2571,
        "row_id":23556,
        "rank":3,
        "method":"CorefNMN (ResNet-152)",
        "mlmodel":{

        },
        "method_short":"CorefNMN ",
        "method_details":"ResNet-152",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-09-06",
        "metrics":{
            "MRR":"64.1",
            "R@1":"50.92",
            "R@10":"88.81",
            "R@5":"80.18",
            "Mean Rank":"4.45"
        },
        "raw_metrics":{
            "MRR":64.1,
            "R@1":50.92,
            "R@10":88.81,
            "R@5":80.18,
            "Mean Rank":4.45
        },
        "uses_additional_data":false,
        "paper":{
            "id":56473,
            "title":"Visual Coreference Resolution in Visual Dialog using Neural Module Networks",
            "url":"\/paper\/visual-coreference-resolution-in-visual",
            "published":"2018-09-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/visual-coreference-resolution-in-visual\/review\/?hl=23556"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":2571,
        "row_id":14851,
        "rank":4,
        "method":"CoAtt",
        "mlmodel":{

        },
        "method_short":"CoAtt",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-11-21",
        "metrics":{
            "MRR":"63.98",
            "R@1":"50.29",
            "R@10":"88.81",
            "R@5":"80.71",
            "Mean Rank":"4.47"
        },
        "raw_metrics":{
            "MRR":63.98,
            "R@1":50.29,
            "R@10":88.81,
            "R@5":80.71,
            "Mean Rank":4.47
        },
        "uses_additional_data":false,
        "paper":{
            "id":14397,
            "title":"Are You Talking to Me? Reasoned Visual Dialog Generation through Adversarial Learning",
            "url":"\/paper\/are-you-talking-to-me-reasoned-visual-dialog",
            "published":"2017-11-21T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/are-you-talking-to-me-reasoned-visual-dialog\/review\/?hl=14851"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2571,
        "row_id":23555,
        "rank":5,
        "method":"CorefNMN",
        "mlmodel":{

        },
        "method_short":"CorefNMN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-09-06",
        "metrics":{
            "MRR":"63.6",
            "R@1":"50.24",
            "R@10":"88.51",
            "R@5":"79.81",
            "Mean Rank":"4.53"
        },
        "raw_metrics":{
            "MRR":63.6,
            "R@1":50.24,
            "R@10":88.51,
            "R@5":79.81,
            "Mean Rank":4.53
        },
        "uses_additional_data":false,
        "paper":{
            "id":56473,
            "title":"Visual Coreference Resolution in Visual Dialog using Neural Module Networks",
            "url":"\/paper\/visual-coreference-resolution-in-visual",
            "published":"2018-09-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/visual-coreference-resolution-in-visual\/review\/?hl=23555"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2571,
        "row_id":14852,
        "rank":6,
        "method":"DualVD",
        "mlmodel":{

        },
        "method_short":"DualVD",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-17",
        "metrics":{
            "MRR":"62.94",
            "R@1":"48.64",
            "R@10":"89.94",
            "R@5":"80.89",
            "Mean Rank":"4.17"
        },
        "raw_metrics":{
            "MRR":62.94,
            "R@1":48.64,
            "R@10":89.94,
            "R@5":80.89,
            "Mean Rank":4.17
        },
        "uses_additional_data":false,
        "paper":{
            "id":173150,
            "title":"DualVD: An Adaptive Dual Encoding Model for Deep Visual Understanding in Visual Dialogue",
            "url":"\/paper\/dualvd-an-adaptive-dual-encoding-model-for",
            "published":"2019-11-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dualvd-an-adaptive-dual-encoding-model-for\/review\/?hl=14852"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2571,
        "row_id":23554,
        "rank":7,
        "method":"SF-QIH-se-2",
        "mlmodel":{

        },
        "method_short":"SF-QIH-se-2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-03-29",
        "metrics":{
            "MRR":"62.42",
            "R@1":"48.55",
            "R@10":"87.75",
            "R@5":"78.96",
            "Mean Rank":"4.70"
        },
        "raw_metrics":{
            "MRR":62.42,
            "R@1":48.55,
            "R@10":87.75,
            "R@5":78.96,
            "Mean Rank":4.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":7127,
            "title":"Two can play this Game: Visual Dialog with Discriminative Question Generation and Answering",
            "url":"\/paper\/two-can-play-this-game-visual-dialog-with",
            "published":"2018-03-29T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2571,
        "row_id":23553,
        "rank":8,
        "method":"HCIAE-NP-ATT",
        "mlmodel":{

        },
        "method_short":"HCIAE-NP-ATT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-06-05",
        "metrics":{
            "MRR":"62.22",
            "R@1":"48.48",
            "R@10":"87.59",
            "R@5":"78.75",
            "Mean Rank":"4.81"
        },
        "raw_metrics":{
            "MRR":62.22,
            "R@1":48.48,
            "R@10":87.59,
            "R@5":78.75,
            "Mean Rank":4.81
        },
        "uses_additional_data":false,
        "paper":{
            "id":15702,
            "title":"Best of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model",
            "url":"\/paper\/best-of-both-worlds-transferring-knowledge",
            "published":"2017-06-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/best-of-both-worlds-transferring-knowledge\/review\/?hl=23553"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2571,
        "row_id":23552,
        "rank":9,
        "method":"HieCoAtt-QI",
        "mlmodel":{

        },
        "method_short":"HieCoAtt-QI",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-05-31",
        "metrics":{
            "MRR":"57.88",
            "R@1":"43.51",
            "R@10":"83.96",
            "R@5":"74.49",
            "Mean Rank":"5.84"
        },
        "raw_metrics":{
            "MRR":57.88,
            "R@1":43.51,
            "R@10":83.96,
            "R@5":74.49,
            "Mean Rank":5.84
        },
        "uses_additional_data":false,
        "paper":{
            "id":26938,
            "title":"Hierarchical Question-Image Co-Attention for Visual Question Answering",
            "url":"\/paper\/hierarchical-question-image-co-attention-for",
            "published":"2016-05-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/hierarchical-question-image-co-attention-for\/review\/?hl=23552"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2571,
        "row_id":23565,
        "rank":10,
        "method":"HACAN",
        "mlmodel":{

        },
        "method_short":"HACAN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-02-25",
        "metrics":{
            "MRR":"0.6792",
            "R@1":"54.76",
            "R@10":"90.68",
            "R@5":"83.03",
            "Mean Rank":"3.97"
        },
        "raw_metrics":{
            "MRR":0.6792,
            "R@1":54.76,
            "R@10":90.68,
            "R@5":83.03,
            "Mean Rank":3.97
        },
        "uses_additional_data":false,
        "paper":{
            "id":106722,
            "title":"Making History Matter: History-Advantage Sequence Training for Visual Dialog",
            "url":"\/paper\/making-history-matter-gold-critic-sequence",
            "published":"2019-02-25T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/making-history-matter-gold-critic-sequence\/review\/?hl=23565"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2571,
        "row_id":23558,
        "rank":11,
        "method":"MVAN",
        "mlmodel":{

        },
        "method_short":"MVAN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-04-29",
        "metrics":{
            "MRR":"0.6765",
            "R@1":"54.65",
            "R@10":"91.47",
            "R@5":"83.85",
            "Mean Rank":"3.73"
        },
        "raw_metrics":{
            "MRR":0.6765,
            "R@1":54.65,
            "R@10":91.47,
            "R@5":83.85,
            "Mean Rank":3.73
        },
        "uses_additional_data":false,
        "paper":{
            "id":192828,
            "title":"Multi-View Attention Network for Visual Dialog",
            "url":"\/paper\/multi-view-attention-networks-for-visual",
            "published":"2020-04-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multi-view-attention-networks-for-visual\/review\/?hl=23558"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2571,
        "row_id":23567,
        "rank":12,
        "method":"CAG",
        "mlmodel":{

        },
        "method_short":"CAG",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-04-05",
        "metrics":{
            "MRR":"0.6756",
            "R@1":"54.64",
            "R@10":"91.48",
            "R@5":"83.72",
            "Mean Rank":"3.75"
        },
        "raw_metrics":{
            "MRR":0.6756,
            "R@1":54.64,
            "R@10":91.48,
            "R@5":83.72,
            "Mean Rank":3.75
        },
        "uses_additional_data":false,
        "paper":{
            "id":189676,
            "title":"Iterative Context-Aware Graph Inference for Visual Dialog",
            "url":"\/paper\/iterative-context-aware-graph-inference-for",
            "published":"2020-04-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/iterative-context-aware-graph-inference-for\/review\/?hl=23567"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2571,
        "row_id":23563,
        "rank":13,
        "method":"RVA",
        "mlmodel":{

        },
        "method_short":"RVA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-12-06",
        "metrics":{
            "MRR":"0.6634",
            "R@1":"52.71",
            "R@10":"90.73",
            "R@5":"82.97",
            "Mean Rank":"3.93"
        },
        "raw_metrics":{
            "MRR":0.6634,
            "R@1":52.71,
            "R@10":90.73,
            "R@5":82.97,
            "Mean Rank":3.93
        },
        "uses_additional_data":false,
        "paper":{
            "id":64543,
            "title":"Recursive Visual Attention in Visual Dialog",
            "url":"\/paper\/recursive-visual-attention-in-visual-dialog",
            "published":"2018-12-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/recursive-visual-attention-in-visual-dialog\/review\/?hl=23563"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2571,
        "row_id":23569,
        "rank":14,
        "method":"GNN",
        "mlmodel":{

        },
        "method_short":"GNN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-11",
        "metrics":{
            "MRR":"0.6285",
            "R@1":"48.95",
            "R@10":"88.36",
            "R@5":"79.65",
            "Mean Rank":"4.57"
        },
        "raw_metrics":{
            "MRR":0.6285,
            "R@1":48.95,
            "R@10":88.36,
            "R@5":79.65,
            "Mean Rank":4.57
        },
        "uses_additional_data":false,
        "paper":{
            "id":111376,
            "title":"Reasoning Visual Dialogs with Structural and Partial Observations",
            "url":"\/paper\/reasoning-visual-dialogs-with-structural-and",
            "published":"2019-04-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/reasoning-visual-dialogs-with-structural-and\/review\/?hl=23569"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2571,
        "row_id":23561,
        "rank":15,
        "method":"MN-QIH-D",
        "mlmodel":{

        },
        "method_short":"MN-QIH-D",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-11-26",
        "metrics":{
            "MRR":"0.5965",
            "R@1":"45.55",
            "R@10":"85.37",
            "R@5":"76.22",
            "Mean Rank":"5.46"
        },
        "raw_metrics":{
            "MRR":0.5965,
            "R@1":45.55,
            "R@10":85.37,
            "R@5":76.22,
            "Mean Rank":5.46
        },
        "uses_additional_data":false,
        "paper":{
            "id":19445,
            "title":"Visual Dialog",
            "url":"\/paper\/visual-dialog",
            "published":"2016-11-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/visual-dialog\/review\/?hl=23561"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2571,
        "row_id":23560,
        "rank":16,
        "method":"HRE-QIH-D",
        "mlmodel":{

        },
        "method_short":"HRE-QIH-D",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-11-26",
        "metrics":{
            "MRR":"0.5846",
            "R@1":"44.67",
            "R@10":"84.22",
            "R@5":"74.50",
            "Mean Rank":"5.72"
        },
        "raw_metrics":{
            "MRR":0.5846,
            "R@1":44.67,
            "R@10":84.22,
            "R@5":74.5,
            "Mean Rank":5.72
        },
        "uses_additional_data":false,
        "paper":{
            "id":19445,
            "title":"Visual Dialog",
            "url":"\/paper\/visual-dialog",
            "published":"2016-11-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/visual-dialog\/review\/?hl=23560"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2571,
        "row_id":14853,
        "rank":17,
        "method":"AMEM",
        "mlmodel":{

        },
        "method_short":"AMEM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-09-23",
        "metrics":{
            "MRR":null,
            "R@1":"48.53",
            "R@10":"87.43",
            "R@5":"78.66",
            "Mean Rank":"4.86"
        },
        "raw_metrics":{
            "MRR":null,
            "R@1":48.53,
            "R@10":87.43,
            "R@5":78.66,
            "Mean Rank":4.86
        },
        "uses_additional_data":false,
        "paper":{
            "id":14430,
            "title":"Visual Reference Resolution using Attention Memory for Visual Dialog",
            "url":"\/paper\/visual-reference-resolution-using-attention",
            "published":"2017-09-23T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/visual-reference-resolution-using-attention\/review\/?hl=14853"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]