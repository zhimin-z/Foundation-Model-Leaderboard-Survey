[
    {
        "T":"\ud83d\udd36",
        "Model":"TigerResearch\/tigerbot-70b-chat-v2",
        "Average":74.13,
        "ARC":87.03,
        "HellaSwag":82.83,
        "MMLU":66.0,
        "TruthfulQA":75.4,
        "Winogrande":79.16,
        "GSM8K":54.36,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":70.0,
        "Hub \u2764\ufe0f":15.0,
        "Available on the Hub":true,
        "Model Sha":"fef61765c33743586b659b3b379f6ae426ec4214"
    },
    {
        "T":"\u2b55",
        "Model":"SUSTech\/SUS-Chat-34B",
        "Average":73.22,
        "ARC":66.3,
        "HellaSwag":83.91,
        "MMLU":76.41,
        "TruthfulQA":57.04,
        "Winogrande":83.5,
        "GSM8K":72.18,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"other",
        "#Params (B)":34.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"01f1a7861667c4869bb03251dfd10526bf846e9c"
    },
    {
        "T":"\u2b55",
        "Model":"deepseek-ai\/deepseek-llm-67b-chat",
        "Average":71.79,
        "ARC":67.75,
        "HellaSwag":86.82,
        "MMLU":72.42,
        "TruthfulQA":55.85,
        "Winogrande":84.21,
        "GSM8K":63.68,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":67.0,
        "Hub \u2764\ufe0f":58.0,
        "Available on the Hub":true,
        "Model Sha":"79648bef7658bb824e4630740f6e1484c1b0620b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Tess-M-Creative-v1.0",
        "Average":71.73,
        "ARC":66.81,
        "HellaSwag":85.14,
        "MMLU":75.54,
        "TruthfulQA":57.68,
        "Winogrande":83.11,
        "GSM8K":62.09,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub \u2764\ufe0f":30.0,
        "Available on the Hub":true,
        "Model Sha":"26923a2648b9864e2ec6f0cc66b8b6fcfbbdd491"
    },
    {
        "T":"\u2b55",
        "Model":"bhenrym14\/platypus-yi-34b",
        "Average":71.69,
        "ARC":68.43,
        "HellaSwag":85.21,
        "MMLU":78.13,
        "TruthfulQA":54.48,
        "Winogrande":84.06,
        "GSM8K":59.82,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"66abec7cba89b35c7b6cab2140c3532049de0157"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"DiscoResearch\/DiscoLM-70b",
        "Average":71.37,
        "ARC":68.77,
        "HellaSwag":86.1,
        "MMLU":68.58,
        "TruthfulQA":57.64,
        "Winogrande":83.58,
        "GSM8K":63.53,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":68.98,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"5eab2c8ec1c079e53a60ebdb7811756c2faebd9b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"brucethemoose\/CapyTessBorosYi-34B-200K-DARE-Ties",
        "Average":71.31,
        "ARC":64.93,
        "HellaSwag":85.92,
        "MMLU":76.18,
        "TruthfulQA":55.84,
        "Winogrande":83.03,
        "GSM8K":61.94,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"0475128a0e57fc103e65c601be75013f28987e62"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"chargoddard\/Yi-34B-Llama",
        "Average":70.95,
        "ARC":64.59,
        "HellaSwag":85.63,
        "MMLU":76.31,
        "TruthfulQA":55.6,
        "Winogrande":82.79,
        "GSM8K":60.8,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub \u2764\ufe0f":36.0,
        "Available on the Hub":true,
        "Model Sha":"52feecf18e46dd8ed1db297345957007c3e45de1"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"01-ai\/Yi-34B-200K",
        "Average":70.81,
        "ARC":65.36,
        "HellaSwag":85.58,
        "MMLU":76.06,
        "TruthfulQA":53.64,
        "Winogrande":82.56,
        "GSM8K":61.64,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub \u2764\ufe0f":153.0,
        "Available on the Hub":true,
        "Model Sha":"bb196389dbbfdf271b5564ce840027f8cd3386ef"
    },
    {
        "T":"?",
        "Model":"brucethemoose\/Capybara-Tess-Yi-34B-200K",
        "Average":70.57,
        "ARC":66.13,
        "HellaSwag":86.24,
        "MMLU":74.89,
        "TruthfulQA":56.37,
        "Winogrande":82.4,
        "GSM8K":57.39,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"28a4464d357d9a4d91238d20ed30ecd2ee377be5"
    },
    {
        "T":"\u2b55",
        "Model":"upstage\/SOLAR-0-70b-16bit",
        "Average":70.11,
        "ARC":71.08,
        "HellaSwag":87.89,
        "MMLU":70.58,
        "TruthfulQA":62.25,
        "Winogrande":83.58,
        "GSM8K":45.26,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":214.0,
        "Available on the Hub":true,
        "Model Sha":"5f9c77b2c0397cf83d2f97740483f107c7109e8c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ICBU-NPU\/FashionGPT-70B-V1.1",
        "Average":70.05,
        "ARC":71.76,
        "HellaSwag":88.2,
        "MMLU":70.99,
        "TruthfulQA":65.26,
        "Winogrande":82.64,
        "GSM8K":41.47,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":38.0,
        "Available on the Hub":true,
        "Model Sha":"05941a3eaacff0dead79b09d2175b5d7b98c525b"
    },
    {
        "T":"\u2b55",
        "Model":"kyujinpy\/PlatYi-34B-Q",
        "Average":69.86,
        "ARC":66.89,
        "HellaSwag":85.14,
        "MMLU":77.66,
        "TruthfulQA":53.03,
        "Winogrande":82.48,
        "GSM8K":53.98,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":34.39,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"187442aa0d250dc3c44451d71bf8fcdd556bdb24"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"sequelbox\/StellarBright",
        "Average":69.86,
        "ARC":72.95,
        "HellaSwag":87.82,
        "MMLU":71.17,
        "TruthfulQA":64.46,
        "Winogrande":83.27,
        "GSM8K":39.5,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":69.24,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"43efad8bfdb47139934e810906c1e59c25b5e269"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Tess-M-v1.1",
        "Average":69.79,
        "ARC":67.15,
        "HellaSwag":84.76,
        "MMLU":74.5,
        "TruthfulQA":54.8,
        "Winogrande":82.87,
        "GSM8K":54.66,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"e5a016b08aa507fe9db45436074016928bf6f939"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Tess-M-v1.3",
        "Average":69.71,
        "ARC":62.54,
        "HellaSwag":83.95,
        "MMLU":75.36,
        "TruthfulQA":56.03,
        "Winogrande":81.14,
        "GSM8K":59.21,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":34.0,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"7d733ec8449ec0219a9f499084a94a4248846f7e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fblgit\/una-cybertron-7b-v2-bf16",
        "Average":69.67,
        "ARC":68.26,
        "HellaSwag":85.85,
        "MMLU":63.23,
        "TruthfulQA":64.63,
        "Winogrande":80.98,
        "GSM8K":55.04,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"82599694771bd375c91f36dfdf30c448e4e33b3c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fblgit\/una-cybertron-7b-v1-fp16",
        "Average":69.49,
        "ARC":68.43,
        "HellaSwag":85.42,
        "MMLU":63.34,
        "TruthfulQA":63.28,
        "Winogrande":81.37,
        "GSM8K":55.12,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"7bf918ddf0878a693f24f39e9f1a520464b44268"
    },
    {
        "T":"\u2b55",
        "Model":"MayaPH\/GodziLLa2-70B",
        "Average":69.46,
        "ARC":71.42,
        "HellaSwag":87.53,
        "MMLU":69.88,
        "TruthfulQA":61.54,
        "Winogrande":83.19,
        "GSM8K":43.21,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":68.98,
        "Hub \u2764\ufe0f":17.0,
        "Available on the Hub":true,
        "Model Sha":"7b78087db07eec97f7b461d10758ece76d685543"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"01-ai\/Yi-34B",
        "Average":69.42,
        "ARC":64.59,
        "HellaSwag":85.69,
        "MMLU":76.35,
        "TruthfulQA":56.23,
        "Winogrande":83.03,
        "GSM8K":50.64,
        "Type":"pretrained",
        "Architecture":"YiForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"custom",
        "#Params (B)":34.0,
        "Hub \u2764\ufe0f":15.0,
        "Available on the Hub":true,
        "Model Sha":"cd8d59de87ea11c6453ee287ac82e5523f08c8ec"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"garage-bAInd\/Platypus2-70B-instruct",
        "Average":69.3,
        "ARC":71.84,
        "HellaSwag":87.94,
        "MMLU":70.48,
        "TruthfulQA":62.26,
        "Winogrande":82.72,
        "GSM8K":40.56,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":148.0,
        "Available on the Hub":true,
        "Model Sha":"a66378c15f89756215ccc64572ba69b161173703"
    },
    {
        "T":"\u2b55",
        "Model":"jondurbin\/airoboros-l2-70b-2.2.1",
        "Average":69.13,
        "ARC":69.71,
        "HellaSwag":87.95,
        "MMLU":69.79,
        "TruthfulQA":59.49,
        "Winogrande":82.95,
        "GSM8K":44.88,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"eadc78a4a9e173bccdca7dc8d12a34e80317c66c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Q-bert\/Optimus-7B",
        "Average":69.09,
        "ARC":65.44,
        "HellaSwag":85.41,
        "MMLU":63.61,
        "TruthfulQA":55.79,
        "Winogrande":78.77,
        "GSM8K":65.5,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"d9dd63bc4437c2089f40ce37e689ad530060519c"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"chargoddard\/loyal-piano-m7-cdpo",
        "Average":69.08,
        "ARC":67.15,
        "HellaSwag":85.39,
        "MMLU":64.52,
        "TruthfulQA":61.53,
        "Winogrande":79.4,
        "GSM8K":56.48,
        "Type":"RL-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"5f5a78bedc2d3e5314589f685489bc981890cadf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/orca_mini_v3_70b",
        "Average":69.02,
        "ARC":71.25,
        "HellaSwag":87.85,
        "MMLU":70.18,
        "TruthfulQA":61.27,
        "Winogrande":82.72,
        "GSM8K":40.86,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":16.0,
        "Available on the Hub":true,
        "Model Sha":"c1d4f997f8ed685a6efc72229523b2e56fd0774b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"chargoddard\/loyal-piano-m7-cdpo",
        "Average":69.0,
        "ARC":67.06,
        "HellaSwag":85.42,
        "MMLU":64.54,
        "TruthfulQA":61.54,
        "Winogrande":79.08,
        "GSM8K":56.33,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"5f5a78bedc2d3e5314589f685489bc981890cadf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AIDC-ai-business\/Marcoroni-70B-v1",
        "Average":68.83,
        "ARC":73.55,
        "HellaSwag":87.62,
        "MMLU":70.67,
        "TruthfulQA":64.41,
        "Winogrande":83.43,
        "GSM8K":33.28,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":16.0,
        "Available on the Hub":true,
        "Model Sha":"55a30d29db194832c0b5de1392a6598a63582144"
    },
    {
        "T":"\u2b55",
        "Model":"adamo1139\/Yi-34B-AEZAKMI-v1",
        "Average":68.67,
        "ARC":64.33,
        "HellaSwag":84.31,
        "MMLU":73.91,
        "TruthfulQA":55.73,
        "Winogrande":80.82,
        "GSM8K":52.92,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"c56dc8471eba802f74fed756f555b718d975d00a"
    },
    {
        "T":"\u2b55",
        "Model":"chargoddard\/loyal-piano-m7",
        "Average":68.67,
        "ARC":66.72,
        "HellaSwag":85.03,
        "MMLU":64.43,
        "TruthfulQA":60.03,
        "Winogrande":79.08,
        "GSM8K":56.71,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"d74ae6cb13325e0f81797ee33c07f0e234a2caa4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/model_007",
        "Average":68.56,
        "ARC":71.08,
        "HellaSwag":87.65,
        "MMLU":69.04,
        "TruthfulQA":63.12,
        "Winogrande":83.35,
        "GSM8K":37.15,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":19.0,
        "Available on the Hub":true,
        "Model Sha":"0f5d81b13718a866cb078bd8762ab80a41972663"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/model_009",
        "Average":68.53,
        "ARC":71.59,
        "HellaSwag":87.7,
        "MMLU":69.43,
        "TruthfulQA":60.72,
        "Winogrande":82.32,
        "GSM8K":39.42,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"5020869e6394b1ac039bf80a0a1d2bed6be6707e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/model_101",
        "Average":68.46,
        "ARC":68.69,
        "HellaSwag":86.42,
        "MMLU":69.92,
        "TruthfulQA":58.85,
        "Winogrande":82.08,
        "GSM8K":44.81,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"884c53a64a3c5faf7b0706d36a587ca1532ed8f5"
    },
    {
        "T":"\u2b55",
        "Model":"budecosystem\/genz-70b",
        "Average":68.35,
        "ARC":71.42,
        "HellaSwag":87.99,
        "MMLU":70.78,
        "TruthfulQA":62.66,
        "Winogrande":83.5,
        "GSM8K":33.74,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":26.0,
        "Available on the Hub":true,
        "Model Sha":"32110b4f33e5e80073ca1f47638482fdc0e19297"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Intel\/neural-chat-7b-v3-2",
        "Average":68.29,
        "ARC":67.49,
        "HellaSwag":83.92,
        "MMLU":63.55,
        "TruthfulQA":59.68,
        "Winogrande":79.95,
        "GSM8K":55.12,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"2ecaf100bcf63da6cf87dd7bfbea5732fa74c413"
    },
    {
        "T":"\u2b55",
        "Model":"elinas\/chronos007-70b",
        "Average":68.25,
        "ARC":70.14,
        "HellaSwag":87.52,
        "MMLU":69.33,
        "TruthfulQA":57.65,
        "Winogrande":82.24,
        "GSM8K":42.61,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.98,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"c775f87a56f00725de4263f8d527995d40f611c4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mlabonne\/NeuralHermes-2.5-Mistral-7B",
        "Average":68.22,
        "ARC":66.55,
        "HellaSwag":84.9,
        "MMLU":63.32,
        "TruthfulQA":54.93,
        "Winogrande":78.3,
        "GSM8K":61.33,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"351028e0532a084c2c1370029fcf2ef805da3929"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TigerResearch\/tigerbot-70b-chat",
        "Average":68.2,
        "ARC":76.79,
        "HellaSwag":87.76,
        "MMLU":66.35,
        "TruthfulQA":55.09,
        "Winogrande":77.58,
        "GSM8K":45.64,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":68.95,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":false,
        "Model Sha":"7e506c4a056821e5d151a0e46572cd74d04194be"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OrionStarAI\/OrionStar-Yi-34B-Chat-Llama",
        "Average":68.17,
        "ARC":64.93,
        "HellaSwag":84.34,
        "MMLU":73.67,
        "TruthfulQA":53.35,
        "Winogrande":78.85,
        "GSM8K":53.9,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"333c788e0d026cdb76bb827b8dcbc14a859ae2cc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-llama2-70b-v10.1-bf16",
        "Average":68.16,
        "ARC":61.86,
        "HellaSwag":83.13,
        "MMLU":67.41,
        "TruthfulQA":56.18,
        "Winogrande":80.11,
        "GSM8K":60.27,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":68.76,
        "Hub \u2764\ufe0f":41.0,
        "Available on the Hub":true,
        "Model Sha":"a6ee90d262ac729f90ed8de97127766df070074c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TigerResearch\/tigerbot-70b-chat",
        "Average":68.11,
        "ARC":76.79,
        "HellaSwag":87.83,
        "MMLU":66.08,
        "TruthfulQA":55.1,
        "Winogrande":77.9,
        "GSM8K":44.96,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":68.95,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":false,
        "Model Sha":"7e506c4a056821e5d151a0e46572cd74d04194be"
    },
    {
        "T":"\u2b55",
        "Model":"kyujinpy\/PlatYi-34B-LoRA",
        "Average":68.1,
        "ARC":67.15,
        "HellaSwag":85.37,
        "MMLU":78.46,
        "TruthfulQA":53.32,
        "Winogrande":83.66,
        "GSM8K":40.64,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":34.39,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"5dcc36255b4632ba32a6b940fa43d53764a3fae3"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"meta-llama\/Llama-2-70b-hf",
        "Average":67.87,
        "ARC":67.32,
        "HellaSwag":87.33,
        "MMLU":69.83,
        "TruthfulQA":44.92,
        "Winogrande":83.74,
        "GSM8K":54.06,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":68.98,
        "Hub \u2764\ufe0f":623.0,
        "Available on the Hub":true,
        "Model Sha":"ed7b07231238f836b99bf45701b9a0063576b194"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"tiiuae\/falcon-180B",
        "Average":67.85,
        "ARC":69.45,
        "HellaSwag":88.86,
        "MMLU":70.5,
        "TruthfulQA":45.47,
        "Winogrande":86.9,
        "GSM8K":45.94,
        "Type":"pretrained",
        "Architecture":"?",
        "Precision":"8bit",
        "Hub License":"unknown",
        "#Params (B)":179.52,
        "Hub \u2764\ufe0f":830.0,
        "Available on the Hub":false,
        "Model Sha":"71a1a70b629e9963f7b4601e82f3f9079d48011e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/OpenHermes-2.5-neural-chat-7b-v3-1-7B",
        "Average":67.84,
        "ARC":66.55,
        "HellaSwag":84.47,
        "MMLU":63.34,
        "TruthfulQA":61.22,
        "Winogrande":78.37,
        "GSM8K":53.07,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"2e72eb3999108b7a9c7d0d0c6b8d81ad3470f1f5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"perlthoughts\/Chupacabra-7B",
        "Average":67.76,
        "ARC":66.81,
        "HellaSwag":83.52,
        "MMLU":62.68,
        "TruthfulQA":52.31,
        "Winogrande":79.08,
        "GSM8K":62.17,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"ae20703e16d89ba4a4301d12195cede64bd2ebdd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Q-bert\/Bumblebee-7B",
        "Average":67.73,
        "ARC":63.4,
        "HellaSwag":84.16,
        "MMLU":64.0,
        "TruthfulQA":50.96,
        "Winogrande":78.22,
        "GSM8K":65.66,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"0c95c597b9c6c5563273126d1306fdd56bd31618"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"beberik\/Nyxene-11B",
        "Average":67.72,
        "ARC":68.34,
        "HellaSwag":84.54,
        "MMLU":65.09,
        "TruthfulQA":57.5,
        "Winogrande":79.08,
        "GSM8K":51.78,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"55e115157836e1529dd28fc56e2900a5f0e79b89"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Euryale-1.3-L2-70B",
        "Average":67.66,
        "ARC":70.82,
        "HellaSwag":87.92,
        "MMLU":70.39,
        "TruthfulQA":59.85,
        "Winogrande":82.79,
        "GSM8K":34.19,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"6e3ce78eb5346bf3a5ee88cd60c25dc0d73de639"
    },
    {
        "T":"\u2b55",
        "Model":"mrfakename\/NeuralOrca-7B-v1",
        "Average":67.64,
        "ARC":65.27,
        "HellaSwag":85.07,
        "MMLU":63.68,
        "TruthfulQA":54.58,
        "Winogrande":78.77,
        "GSM8K":58.45,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"32fb215494467cc6fa2f283a4b02f23546a26807"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openaccess-ai-collective\/DPOpenHermes-7B",
        "Average":67.63,
        "ARC":65.96,
        "HellaSwag":85.9,
        "MMLU":63.98,
        "TruthfulQA":56.92,
        "Winogrande":78.22,
        "GSM8K":54.81,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f7742bd00c7d66791e94882b196b4d96fb88e63a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fangloveskari\/ORCA_LLaMA_70B_QLoRA",
        "Average":67.6,
        "ARC":72.27,
        "HellaSwag":87.74,
        "MMLU":70.23,
        "TruthfulQA":63.37,
        "Winogrande":83.66,
        "GSM8K":28.35,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":49.0,
        "Available on the Hub":true,
        "Model Sha":"ef9b04ef02ccc4d96f1181467da92bb6b5baf835"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"openaccess-ai-collective\/DPOpenHermes-7B",
        "Average":67.58,
        "ARC":65.7,
        "HellaSwag":85.96,
        "MMLU":63.89,
        "TruthfulQA":56.95,
        "Winogrande":78.61,
        "GSM8K":54.36,
        "Type":"RL-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f7742bd00c7d66791e94882b196b4d96fb88e63a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fangloveskari\/Platypus_QLoRA_LLaMA_70b",
        "Average":67.57,
        "ARC":72.1,
        "HellaSwag":87.46,
        "MMLU":71.02,
        "TruthfulQA":61.18,
        "Winogrande":82.87,
        "GSM8K":30.78,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"b9b8560832276f60ba6bf37ac913b230a85ac19b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ICBU-NPU\/FashionGPT-70B-V1",
        "Average":67.47,
        "ARC":71.08,
        "HellaSwag":87.32,
        "MMLU":70.7,
        "TruthfulQA":63.92,
        "Winogrande":83.66,
        "GSM8K":28.13,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"060c096af49700760f734c0102250a524d46b3eb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fblgit\/juanako-7b-UNA",
        "Average":67.46,
        "ARC":68.17,
        "HellaSwag":85.34,
        "MMLU":62.47,
        "TruthfulQA":65.13,
        "Winogrande":78.85,
        "GSM8K":44.81,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"3e12f691e1f442f69eaff408677a54ebc69d5dc8"
    },
    {
        "T":"\u2b55",
        "Model":"ehartford\/Samantha-1.1-70b",
        "Average":67.43,
        "ARC":68.77,
        "HellaSwag":87.46,
        "MMLU":68.6,
        "TruthfulQA":64.85,
        "Winogrande":83.27,
        "GSM8K":31.61,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"a3819d186f5b4d52ced7ddeb7fa16bf66e8a2ea7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"stabilityai\/StableBeluga2",
        "Average":67.42,
        "ARC":71.08,
        "HellaSwag":86.37,
        "MMLU":68.79,
        "TruthfulQA":59.44,
        "Winogrande":82.95,
        "GSM8K":35.86,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":836.0,
        "Available on the Hub":true,
        "Model Sha":"e4944caa6ece819413b140b8dcecea79fe7e22cf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/test_42_70b",
        "Average":67.38,
        "ARC":68.26,
        "HellaSwag":87.65,
        "MMLU":70.0,
        "TruthfulQA":48.76,
        "Winogrande":83.66,
        "GSM8K":45.94,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"ca3789cd6b683e97dcd6a5f0367f90a63d7a4e7b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"upstage\/Llama-2-70b-instruct",
        "Average":67.38,
        "ARC":70.9,
        "HellaSwag":87.48,
        "MMLU":69.8,
        "TruthfulQA":60.97,
        "Winogrande":82.87,
        "GSM8K":32.22,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":53.0,
        "Available on the Hub":true,
        "Model Sha":"8469429924dc2e1a9394b8095753985668a4052e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"sequelbox\/SharpBalance",
        "Average":67.36,
        "ARC":69.28,
        "HellaSwag":87.59,
        "MMLU":69.51,
        "TruthfulQA":59.05,
        "Winogrande":84.06,
        "GSM8K":34.65,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":69.24,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"a87cb1756d7b7389cc5a6d4647cf53377e962aea"
    },
    {
        "T":"\u2b55",
        "Model":"ehartford\/Samantha-1.11-70b",
        "Average":67.28,
        "ARC":70.05,
        "HellaSwag":87.55,
        "MMLU":67.82,
        "TruthfulQA":65.02,
        "Winogrande":83.27,
        "GSM8K":29.95,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":30.0,
        "Available on the Hub":true,
        "Model Sha":"49e5b5ee0bed2864f0b38ba8bf9e01ccc5e0ba5f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/neural-chat-7b-v3-1-OpenHermes-2.5-7B",
        "Average":67.19,
        "ARC":66.13,
        "HellaSwag":84.09,
        "MMLU":63.22,
        "TruthfulQA":61.23,
        "Winogrande":77.58,
        "GSM8K":50.87,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"b620ea7af98730695e051be48273cdded8923a2b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ICBU-NPU\/FashionGPT-70B-V1.2",
        "Average":67.17,
        "ARC":73.04,
        "HellaSwag":88.15,
        "MMLU":70.11,
        "TruthfulQA":65.15,
        "Winogrande":82.56,
        "GSM8K":24.03,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"990a1664fc058de6ee2406af62c0a817d7047304"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"berkeley-nest\/Starling-LM-7B-alpha",
        "Average":67.13,
        "ARC":63.82,
        "HellaSwag":84.9,
        "MMLU":64.67,
        "TruthfulQA":46.39,
        "Winogrande":80.58,
        "GSM8K":62.4,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":170.0,
        "Available on the Hub":true,
        "Model Sha":"f721e85293598f2ef774e483ae95343e39811577"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lizpreciatior\/lzlv_70b_fp16_hf",
        "Average":67.13,
        "ARC":70.14,
        "HellaSwag":87.54,
        "MMLU":70.23,
        "TruthfulQA":60.49,
        "Winogrande":83.43,
        "GSM8K":30.93,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-2.0",
        "#Params (B)":68.98,
        "Hub \u2764\ufe0f":23.0,
        "Available on the Hub":true,
        "Model Sha":"b366c0bb318ae592023cca894cc6b4421a607a0d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/model_007_v2",
        "Average":67.13,
        "ARC":71.42,
        "HellaSwag":87.31,
        "MMLU":68.58,
        "TruthfulQA":62.65,
        "Winogrande":84.14,
        "GSM8K":28.66,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"3d95e0f3598f7a76ab97cb2cc0e4aae957d77479"
    },
    {
        "T":"\u2b55",
        "Model":"chargoddard\/MelangeB-70b",
        "Average":67.12,
        "ARC":71.67,
        "HellaSwag":87.5,
        "MMLU":70.03,
        "TruthfulQA":59.36,
        "Winogrande":83.5,
        "GSM8K":30.63,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":68.98,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"08239fb1e30b1e42b14370f23e942bc51e76027c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rishiraj\/smol-7b",
        "Average":67.11,
        "ARC":63.74,
        "HellaSwag":84.77,
        "MMLU":65.0,
        "TruthfulQA":46.17,
        "Winogrande":80.66,
        "GSM8K":62.32,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"d3e24684f38e0332cf4a6c70a37ee894e7a27fdc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"berkeley-nest\/Starling-LM-7B-alpha",
        "Average":67.05,
        "ARC":63.65,
        "HellaSwag":84.87,
        "MMLU":64.7,
        "TruthfulQA":46.32,
        "Winogrande":80.43,
        "GSM8K":62.32,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":61.0,
        "Available on the Hub":true,
        "Model Sha":"76e60ca9807f55acd8eff3ec7ae022c5fbdf1e0e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"perlthoughts\/Chupacabra-7B-v2",
        "Average":67.04,
        "ARC":65.19,
        "HellaSwag":83.39,
        "MMLU":63.6,
        "TruthfulQA":57.17,
        "Winogrande":78.14,
        "GSM8K":54.74,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":25.0,
        "Available on the Hub":true,
        "Model Sha":"0c7f7c85359f15d3e6c361e8192738bdfb14ea6c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"meta-math\/MetaMath-70B-V1.0",
        "Average":67.02,
        "ARC":68.0,
        "HellaSwag":86.85,
        "MMLU":69.31,
        "TruthfulQA":50.98,
        "Winogrande":82.32,
        "GSM8K":44.66,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"783a3c7d5d0a75e6e11074f2577b90dd219ef7b1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Synthia-70B-v1.2b",
        "Average":67.0,
        "ARC":68.77,
        "HellaSwag":87.57,
        "MMLU":68.81,
        "TruthfulQA":57.69,
        "Winogrande":83.9,
        "GSM8K":35.25,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":17.0,
        "Available on the Hub":true,
        "Model Sha":"7b687d6e4101b8bb8cc4062f8a318d639098a55d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Walmart-the-bag\/Misted-7B",
        "Average":66.94,
        "ARC":63.65,
        "HellaSwag":84.14,
        "MMLU":63.94,
        "TruthfulQA":52.0,
        "Winogrande":78.3,
        "GSM8K":59.59,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"31245dbdcd0ace447a4434ac5e393a90ac862a87"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Synthia-70B-v1.2",
        "Average":66.9,
        "ARC":70.48,
        "HellaSwag":86.98,
        "MMLU":70.13,
        "TruthfulQA":58.64,
        "Winogrande":83.27,
        "GSM8K":31.92,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":16.0,
        "Available on the Hub":true,
        "Model Sha":"9b92ee1093b125035ba1649dca6f4ceb9d86a656"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Synthia-70B-v1.1",
        "Average":66.81,
        "ARC":70.05,
        "HellaSwag":87.12,
        "MMLU":70.34,
        "TruthfulQA":57.84,
        "Winogrande":83.66,
        "GSM8K":31.84,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"05a13f6adfe95a713dff04dc2eaa214c77c2512a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Synthia-70B",
        "Average":66.72,
        "ARC":69.45,
        "HellaSwag":87.11,
        "MMLU":68.91,
        "TruthfulQA":59.79,
        "Winogrande":83.66,
        "GSM8K":31.39,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"d63dfdd0baed756981f5f78f7419fd822c572362"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uni-tianyan\/Uni-TianYan",
        "Average":66.61,
        "ARC":72.1,
        "HellaSwag":87.4,
        "MMLU":69.91,
        "TruthfulQA":65.81,
        "Winogrande":82.32,
        "GSM8K":22.14,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":48.0,
        "Available on the Hub":true,
        "Model Sha":"46b78b9a10e78283e59c28b56cb59c2f33b0816a"
    },
    {
        "T":"\u2b55",
        "Model":"Ba2han\/HermesStar-OrcaWind-Synth-11B",
        "Average":66.59,
        "ARC":65.27,
        "HellaSwag":83.69,
        "MMLU":65.31,
        "TruthfulQA":48.55,
        "Winogrande":80.11,
        "GSM8K":56.63,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":10.73,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"61aefa2ac956ce0e8ce40aa2521bdb5634452766"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openaccess-ai-collective\/dpopenhermes-alpha-v0",
        "Average":66.52,
        "ARC":65.02,
        "HellaSwag":83.96,
        "MMLU":63.67,
        "TruthfulQA":51.75,
        "Winogrande":78.85,
        "GSM8K":55.88,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"81ce4a9354d3b73276a0fa96b95d384f66d2de3d"
    },
    {
        "T":"\u2b55",
        "Model":"v2ray\/LLaMA-2-Wizard-70B-QLoRA",
        "Average":66.47,
        "ARC":67.58,
        "HellaSwag":87.52,
        "MMLU":69.11,
        "TruthfulQA":61.79,
        "Winogrande":82.32,
        "GSM8K":30.48,
        "Type":"instruction-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":70.0,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":false,
        "Model Sha":"4bff676fe29f56d31961794c062aebc36312446e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Brillibits\/Instruct_Llama70B_Dolly15k",
        "Average":66.42,
        "ARC":68.34,
        "HellaSwag":87.21,
        "MMLU":69.52,
        "TruthfulQA":46.46,
        "Winogrande":84.29,
        "GSM8K":42.68,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"45444ac60488594e0700e6c7313ff444b4468240"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"openaccess-ai-collective\/openhermes-2_5-dpo-no-robots",
        "Average":66.4,
        "ARC":64.93,
        "HellaSwag":84.3,
        "MMLU":63.86,
        "TruthfulQA":52.12,
        "Winogrande":77.9,
        "GSM8K":55.27,
        "Type":"RL-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"bee345f7da9816e459846b6bc3dbea6c69850855"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"augtoma\/qCammel70",
        "Average":66.31,
        "ARC":68.34,
        "HellaSwag":87.87,
        "MMLU":70.18,
        "TruthfulQA":57.47,
        "Winogrande":84.29,
        "GSM8K":29.72,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":20.0,
        "Available on the Hub":true,
        "Model Sha":"cf1e917e42fd1e56ee1edef7ee1a98cbe705c18c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"augtoma\/qCammel-70v1",
        "Average":66.31,
        "ARC":68.34,
        "HellaSwag":87.87,
        "MMLU":70.18,
        "TruthfulQA":57.47,
        "Winogrande":84.29,
        "GSM8K":29.72,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":20.0,
        "Available on the Hub":true,
        "Model Sha":"cf1e917e42fd1e56ee1edef7ee1a98cbe705c18c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"augtoma\/qCammel-70x",
        "Average":66.31,
        "ARC":68.34,
        "HellaSwag":87.87,
        "MMLU":70.18,
        "TruthfulQA":57.47,
        "Winogrande":84.29,
        "GSM8K":29.72,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":20.0,
        "Available on the Hub":true,
        "Model Sha":"cf1e917e42fd1e56ee1edef7ee1a98cbe705c18c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"augtoma\/qCammel-70",
        "Average":66.31,
        "ARC":68.34,
        "HellaSwag":87.87,
        "MMLU":70.18,
        "TruthfulQA":57.47,
        "Winogrande":84.29,
        "GSM8K":29.72,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":20.0,
        "Available on the Hub":true,
        "Model Sha":"cf1e917e42fd1e56ee1edef7ee1a98cbe705c18c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"augtoma\/qCammel-70-x",
        "Average":66.31,
        "ARC":68.34,
        "HellaSwag":87.87,
        "MMLU":70.18,
        "TruthfulQA":57.47,
        "Winogrande":84.29,
        "GSM8K":29.72,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":20.0,
        "Available on the Hub":true,
        "Model Sha":"cf1e917e42fd1e56ee1edef7ee1a98cbe705c18c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"garage-bAInd\/Platypus2-70B",
        "Average":66.28,
        "ARC":70.65,
        "HellaSwag":87.15,
        "MMLU":70.08,
        "TruthfulQA":52.37,
        "Winogrande":84.37,
        "GSM8K":33.06,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":19.0,
        "Available on the Hub":true,
        "Model Sha":"16b6583ad58313331f86be18e531ab03f1857695"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Doctor-Shotgun\/mythospice-limarp-70b",
        "Average":66.27,
        "ARC":69.2,
        "HellaSwag":87.46,
        "MMLU":70.14,
        "TruthfulQA":55.86,
        "Winogrande":82.72,
        "GSM8K":32.22,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"agpl-3.0",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"ff29fed2a33fc050fd20d0e25b5b23c4a101b074"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Xwin-LM\/Xwin-LM-70B-V0.1",
        "Average":66.2,
        "ARC":70.22,
        "HellaSwag":87.25,
        "MMLU":69.77,
        "TruthfulQA":59.86,
        "Winogrande":82.87,
        "GSM8K":27.22,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":159.0,
        "Available on the Hub":true,
        "Model Sha":"d6c803a180e3d46c371f8d3cb3848b861596ccbc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Doctor-Shotgun\/mythospice-70b",
        "Average":66.17,
        "ARC":69.28,
        "HellaSwag":87.53,
        "MMLU":70.1,
        "TruthfulQA":56.76,
        "Winogrande":83.27,
        "GSM8K":30.1,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"b00992c26604c9cd496bc41472a05e4c01cd2008"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"quantumaikr\/llama-2-70b-fb16-orca-chat-10k",
        "Average":66.16,
        "ARC":68.09,
        "HellaSwag":87.07,
        "MMLU":69.21,
        "TruthfulQA":61.56,
        "Winogrande":84.14,
        "GSM8K":26.91,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.98,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"697aaeb8eb9905c9b25bebb736d1905444c774a6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/llama-2-70b-Guanaco-QLoRA-fp16",
        "Average":66.05,
        "ARC":68.26,
        "HellaSwag":88.32,
        "MMLU":70.23,
        "TruthfulQA":55.69,
        "Winogrande":83.98,
        "GSM8K":29.8,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":53.0,
        "Available on the Hub":true,
        "Model Sha":"54b0e39d5e9aee7b323f50b0a26db15295c3d5c9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"s1ghhh\/medllama-2-70b-qlora-1.1",
        "Average":65.99,
        "ARC":69.03,
        "HellaSwag":87.17,
        "MMLU":71.04,
        "TruthfulQA":52.41,
        "Winogrande":84.21,
        "GSM8K":32.07,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":70.0,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":false,
        "Model Sha":"d55e05e9d67418c639933c85a5b9d17c6f531a92"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/model_51",
        "Average":65.96,
        "ARC":68.43,
        "HellaSwag":86.71,
        "MMLU":69.31,
        "TruthfulQA":57.18,
        "Winogrande":81.77,
        "GSM8K":32.37,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"9542702011bf4d282f4b0f0bd79229f5822b6313"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CausalLM\/14B-DPO-alpha",
        "Average":65.91,
        "ARC":58.11,
        "HellaSwag":79.38,
        "MMLU":66.62,
        "TruthfulQA":54.15,
        "Winogrande":74.51,
        "GSM8K":62.7,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"wtfpl",
        "#Params (B)":14.0,
        "Hub \u2764\ufe0f":64.0,
        "Available on the Hub":true,
        "Model Sha":"34bc2dd73ae5f8738e5bcaaa5591427675f7801f"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Qwen\/Qwen-14B",
        "Average":65.86,
        "ARC":58.28,
        "HellaSwag":83.99,
        "MMLU":67.7,
        "TruthfulQA":49.43,
        "Winogrande":76.8,
        "GSM8K":58.98,
        "Type":"pretrained",
        "Architecture":"QWenLMHeadModel",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":14.17,
        "Hub \u2764\ufe0f":162.0,
        "Available on the Hub":true,
        "Model Sha":"5eda9482e32a8ea7ed2dc47178f3b491eb207939"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-falcon-180b-v13-preview0",
        "Average":65.85,
        "ARC":65.1,
        "HellaSwag":86.19,
        "MMLU":64.6,
        "TruthfulQA":54.97,
        "Winogrande":82.64,
        "GSM8K":41.62,
        "Type":"fine-tuned",
        "Architecture":"FalconForCausalLM",
        "Precision":"8bit",
        "Hub License":"?",
        "#Params (B)":178.64,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"7d7b93ffd67d1b0c39f3503050dbbcc951948120"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"meta-math\/MetaMath-Mistral-7B",
        "Average":65.78,
        "ARC":60.67,
        "HellaSwag":82.58,
        "MMLU":61.95,
        "TruthfulQA":44.89,
        "Winogrande":75.77,
        "GSM8K":68.84,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":32.0,
        "Available on the Hub":true,
        "Model Sha":"016a7bb03bfcd953860357e1a16d5b333b887d26"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/model_420",
        "Average":65.76,
        "ARC":70.14,
        "HellaSwag":87.73,
        "MMLU":70.35,
        "TruthfulQA":54.0,
        "Winogrande":83.74,
        "GSM8K":28.58,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"13c7b5f403c0f2af9bf7fce2d4a32deb9054c083"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dfurman\/llama-2-70b-dolphin-peft",
        "Average":65.72,
        "ARC":69.62,
        "HellaSwag":86.82,
        "MMLU":69.18,
        "TruthfulQA":57.43,
        "Winogrande":83.9,
        "GSM8K":27.37,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":70.0,
        "Hub \u2764\ufe0f":12.0,
        "Available on the Hub":false,
        "Model Sha":"a1190dee60b5854e80d340958dc3cc956bc56f68"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"v2ray\/LLaMA-2-Jannie-70B-QLoRA",
        "Average":65.6,
        "ARC":68.94,
        "HellaSwag":86.9,
        "MMLU":69.37,
        "TruthfulQA":53.67,
        "Winogrande":82.95,
        "GSM8K":31.77,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":70.0,
        "Hub \u2764\ufe0f":14.0,
        "Available on the Hub":false,
        "Model Sha":"e552ddca841a2b86e36bbe5f99840afedfdbcd14"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"garage-bAInd\/Camel-Platypus2-70B",
        "Average":65.59,
        "ARC":71.08,
        "HellaSwag":87.6,
        "MMLU":70.04,
        "TruthfulQA":58.09,
        "Winogrande":83.82,
        "GSM8K":22.9,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":12.0,
        "Available on the Hub":true,
        "Model Sha":"b9f8de09ab860ee8ba570db7227c5444020ea056"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JosephusCheung\/Yee-34B-200K-Chat",
        "Average":65.56,
        "ARC":65.61,
        "HellaSwag":84.33,
        "MMLU":74.91,
        "TruthfulQA":53.88,
        "Winogrande":79.79,
        "GSM8K":34.8,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"gpl-3.0",
        "#Params (B)":34.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"94bc30449e41628f59dd965cb7d9a8eb53ce9a45"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/model_42_70b",
        "Average":65.51,
        "ARC":68.26,
        "HellaSwag":87.65,
        "MMLU":70.0,
        "TruthfulQA":48.76,
        "Winogrande":83.66,
        "GSM8K":34.72,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"ca3789cd6b683e97dcd6a5f0367f90a63d7a4e7b"
    },
    {
        "T":"\u2b55",
        "Model":"pankajmathur\/Lima_Unchained_70b",
        "Average":65.51,
        "ARC":68.26,
        "HellaSwag":87.65,
        "MMLU":70.0,
        "TruthfulQA":48.76,
        "Winogrande":83.66,
        "GSM8K":34.72,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"7dadf059a03bdfec2eb4f4a47666545875c68e49"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jordiclive\/Llama-2-70b-oasst-1-200",
        "Average":65.5,
        "ARC":67.66,
        "HellaSwag":87.24,
        "MMLU":69.95,
        "TruthfulQA":51.28,
        "Winogrande":84.14,
        "GSM8K":32.75,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"153b209007e688d713cd670c9972f2827c597b45"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/fiction.live-Kimiko-V2-70B-fp16",
        "Average":65.48,
        "ARC":67.66,
        "HellaSwag":87.65,
        "MMLU":69.82,
        "TruthfulQA":49.28,
        "Winogrande":83.9,
        "GSM8K":34.57,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"6b0c2cb654133cad2d4920e7da2e3f6cb1c4f7fd"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"tiiuae\/falcon-180B",
        "Average":65.46,
        "ARC":69.2,
        "HellaSwag":88.89,
        "MMLU":69.59,
        "TruthfulQA":45.16,
        "Winogrande":86.74,
        "GSM8K":33.21,
        "Type":"pretrained",
        "Architecture":"?",
        "Precision":"4bit",
        "Hub License":"unknown",
        "#Params (B)":179.52,
        "Hub \u2764\ufe0f":830.0,
        "Available on the Hub":false,
        "Model Sha":"71a1a70b629e9963f7b4601e82f3f9079d48011e"
    },
    {
        "T":"\u2b55",
        "Model":"garage-bAInd\/Camel-Platypus2-70B",
        "Average":65.39,
        "ARC":70.14,
        "HellaSwag":87.71,
        "MMLU":69.83,
        "TruthfulQA":57.77,
        "Winogrande":82.95,
        "GSM8K":23.96,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"8bit",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":12.0,
        "Available on the Hub":true,
        "Model Sha":"6f958a1063fe1e6075f6e379fae621ff5a1d98c6"
    },
    {
        "T":"\u2b55",
        "Model":"OpenLemur\/lemur-70b-chat-v1",
        "Average":65.38,
        "ARC":66.98,
        "HellaSwag":85.73,
        "MMLU":65.99,
        "TruthfulQA":56.58,
        "Winogrande":81.69,
        "GSM8K":35.33,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":46.0,
        "Available on the Hub":true,
        "Model Sha":"33da87ba6d90662c6a00535bd628e5b39b3afd3b"
    },
    {
        "T":"?",
        "Model":"01-ai\/Yi-34B-Chat",
        "Average":65.32,
        "ARC":65.44,
        "HellaSwag":84.16,
        "MMLU":74.9,
        "TruthfulQA":55.37,
        "Winogrande":80.11,
        "GSM8K":31.92,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub \u2764\ufe0f":21.0,
        "Available on the Hub":true,
        "Model Sha":"a99ec35331cbfc9da596af7d4538fe2efecff03c"
    },
    {
        "T":"?",
        "Model":"liuxiang886\/llama2-70B-qlora-gpt4",
        "Average":65.29,
        "ARC":70.31,
        "HellaSwag":86.39,
        "MMLU":69.29,
        "TruthfulQA":54.02,
        "Winogrande":82.87,
        "GSM8K":28.89,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"08115ee077953e9c01c6a40f5086def3ecf9f5f0"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"simonveitner\/MathHermes-2.5-Mistral-7B",
        "Average":65.24,
        "ARC":64.76,
        "HellaSwag":84.19,
        "MMLU":63.59,
        "TruthfulQA":51.95,
        "Winogrande":77.66,
        "GSM8K":49.28,
        "Type":"RL-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"2a6ee2674304f91d1dcc772695deded76d4c32bd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"quantumaikr\/llama-2-70b-fb16-korean",
        "Average":65.23,
        "ARC":67.15,
        "HellaSwag":86.78,
        "MMLU":69.29,
        "TruthfulQA":56.5,
        "Winogrande":82.64,
        "GSM8K":29.04,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":68.98,
        "Hub \u2764\ufe0f":28.0,
        "Available on the Hub":true,
        "Model Sha":"fd57855006c15c4121feccab1cbeee8107de5b5a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/OpenOrca-Zephyr-7B",
        "Average":64.97,
        "ARC":64.08,
        "HellaSwag":83.82,
        "MMLU":62.46,
        "TruthfulQA":54.31,
        "Winogrande":78.93,
        "GSM8K":46.25,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"2a2c7d287a46243cccf3ff6628375d0d190394ac"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-l2-70b-gpt4-1.4.1",
        "Average":64.97,
        "ARC":70.39,
        "HellaSwag":87.82,
        "MMLU":70.31,
        "TruthfulQA":55.2,
        "Winogrande":83.58,
        "GSM8K":22.52,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":46.0,
        "Available on the Hub":true,
        "Model Sha":"ea98153fa721ed7110c77e73388e3b6f3996f2bb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/dolphin-2.2.1-mistral-7b",
        "Average":64.93,
        "ARC":63.31,
        "HellaSwag":83.76,
        "MMLU":63.17,
        "TruthfulQA":53.11,
        "Winogrande":78.14,
        "GSM8K":48.07,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":105.0,
        "Available on the Hub":true,
        "Model Sha":"001b48e9aebffb395c698af47b6b48364cc3cbe8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/Nous-Puffin-70B",
        "Average":64.91,
        "ARC":67.41,
        "HellaSwag":87.37,
        "MMLU":69.77,
        "TruthfulQA":46.77,
        "Winogrande":83.9,
        "GSM8K":34.27,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":16.0,
        "Available on the Hub":true,
        "Model Sha":"129e0af93d04b1b9cc85ea48bbb300f1ccb44210"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jarradh\/llama2_70b_chat_uncensored",
        "Average":64.88,
        "ARC":68.43,
        "HellaSwag":86.77,
        "MMLU":68.76,
        "TruthfulQA":52.5,
        "Winogrande":82.56,
        "GSM8K":30.25,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":37.0,
        "Available on the Hub":true,
        "Model Sha":"34b23982a9a996adc8f45c4c2eac7245c4e251b3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"oh-yeontaek\/llama-2-70B-LoRA-assemble-v2",
        "Average":64.73,
        "ARC":71.84,
        "HellaSwag":86.89,
        "MMLU":69.37,
        "TruthfulQA":64.79,
        "Winogrande":81.22,
        "GSM8K":14.25,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"7feeb5b665ab1ecdfd9cc4fe45fadb86b7b91b5b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-mistral-7b-dare-0.85",
        "Average":64.69,
        "ARC":63.57,
        "HellaSwag":84.82,
        "MMLU":64.29,
        "TruthfulQA":50.66,
        "Winogrande":79.24,
        "GSM8K":45.56,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"b19e60f64b3be7f41658958658658bc12038c68f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Euryale-L2-70B",
        "Average":64.66,
        "ARC":68.94,
        "HellaSwag":87.07,
        "MMLU":68.84,
        "TruthfulQA":54.49,
        "Winogrande":82.08,
        "GSM8K":26.54,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"6589310a57ce5d9d6877f353f3d00cda8fa9101c"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"maywell\/PiVoT-0.1-early",
        "Average":64.58,
        "ARC":62.46,
        "HellaSwag":82.97,
        "MMLU":61.02,
        "TruthfulQA":62.89,
        "Winogrande":73.72,
        "GSM8K":44.43,
        "Type":"RL-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"6eeae58a1a292a1d7f989952a07aead6d5da3c69"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-l2-70b-gpt4-m2.0",
        "Average":64.56,
        "ARC":70.05,
        "HellaSwag":87.83,
        "MMLU":70.67,
        "TruthfulQA":49.79,
        "Winogrande":83.58,
        "GSM8K":25.4,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"1cccd0b60a988bf6ddc4e2688895837845afa076"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Llama-2-70B-fp16",
        "Average":64.52,
        "ARC":67.32,
        "HellaSwag":87.33,
        "MMLU":69.83,
        "TruthfulQA":44.92,
        "Winogrande":83.74,
        "GSM8K":33.97,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":68.98,
        "Hub \u2764\ufe0f":40.0,
        "Available on the Hub":true,
        "Model Sha":"b25061ef1b440e970d15d4ac99bc42937cd442a2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"upstage\/llama-65b-instruct",
        "Average":64.51,
        "ARC":68.86,
        "HellaSwag":86.43,
        "MMLU":64.77,
        "TruthfulQA":59.7,
        "Winogrande":81.06,
        "GSM8K":26.23,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":9.0,
        "Available on the Hub":true,
        "Model Sha":"b95668861dfb7b0abca44ccdbef2db49b2dd8917"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"VAGOsolutions\/SauerkrautLM-7b-HerO",
        "Average":64.49,
        "ARC":63.23,
        "HellaSwag":83.52,
        "MMLU":63.3,
        "TruthfulQA":49.22,
        "Winogrande":78.37,
        "GSM8K":49.28,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":16.0,
        "Available on the Hub":true,
        "Model Sha":"0aeb810af28e2910a92b929c21b931a5c06073de"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-llama-65b-v8-bf16",
        "Average":64.47,
        "ARC":62.8,
        "HellaSwag":83.6,
        "MMLU":62.01,
        "TruthfulQA":55.09,
        "Winogrande":79.95,
        "GSM8K":43.37,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":65.07,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"445b77821fac8e6cfb77d0399fb827400b5bb71e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenAssistant\/llama2-70b-oasst-sft-v10",
        "Average":64.47,
        "ARC":67.06,
        "HellaSwag":86.38,
        "MMLU":67.7,
        "TruthfulQA":56.45,
        "Winogrande":82.0,
        "GSM8K":27.22,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":64.0,
        "Available on the Hub":true,
        "Model Sha":"e68a8a2888097def3c7f4fe5d443866a18d05c6c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NurtureAI\/Starling-LM-11B-alpha-v1",
        "Average":64.44,
        "ARC":62.2,
        "HellaSwag":83.24,
        "MMLU":64.03,
        "TruthfulQA":45.7,
        "Winogrande":80.51,
        "GSM8K":50.95,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"b2b3b9fc069a8b5d8be82f68f0f578a6f23e9e5f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Medilora\/medilora-mistral-7b",
        "Average":64.41,
        "ARC":61.69,
        "HellaSwag":83.13,
        "MMLU":62.22,
        "TruthfulQA":49.91,
        "Winogrande":77.66,
        "GSM8K":51.86,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"b6512d2a2202e685da461ff876a1ffb707034c97"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"elinas\/chronos-70b-v2",
        "Average":64.41,
        "ARC":68.09,
        "HellaSwag":86.5,
        "MMLU":68.28,
        "TruthfulQA":53.7,
        "Winogrande":81.22,
        "GSM8K":28.66,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":9.0,
        "Available on the Hub":true,
        "Model Sha":"373af41ca0b2855972b8d471fd63e72b63e4c9fc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/model_420_preview",
        "Average":64.22,
        "ARC":67.06,
        "HellaSwag":87.26,
        "MMLU":69.85,
        "TruthfulQA":44.57,
        "Winogrande":83.35,
        "GSM8K":33.21,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"5095384f1b7bb6e23a987f95589e66e21ae854ef"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-l2-70b-gpt4-2.0",
        "Average":64.14,
        "ARC":68.52,
        "HellaSwag":87.89,
        "MMLU":70.41,
        "TruthfulQA":49.79,
        "Winogrande":83.5,
        "GSM8K":24.72,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":12.0,
        "Available on the Hub":true,
        "Model Sha":"f16526d9bb814dc10adc911f94e8c7a520beb5b6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Enoch\/llama-65b-hf",
        "Average":63.99,
        "ARC":63.31,
        "HellaSwag":86.09,
        "MMLU":63.84,
        "TruthfulQA":43.43,
        "Winogrande":82.48,
        "GSM8K":44.81,
        "Type":"fine-tuned",
        "Architecture":"LLaMAForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":65.0,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"7a7b897ab10b3d82d1e7e6fbcd2159d70b4586cf"
    },
    {
        "T":"?",
        "Model":"openbmb\/UltraLM-65b",
        "Average":63.82,
        "ARC":67.06,
        "HellaSwag":84.98,
        "MMLU":63.48,
        "TruthfulQA":53.51,
        "Winogrande":81.14,
        "GSM8K":32.75,
        "Type":"",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":false,
        "Model Sha":""
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CausalLM\/14B",
        "Average":63.81,
        "ARC":56.66,
        "HellaSwag":79.08,
        "MMLU":65.86,
        "TruthfulQA":47.75,
        "Winogrande":74.9,
        "GSM8K":58.61,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"wtfpl",
        "#Params (B)":14.0,
        "Hub \u2764\ufe0f":220.0,
        "Available on the Hub":true,
        "Model Sha":"2576a37434e2e03804c841d36c669c8a34c729de"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Medilora\/medilora-qwen-14b",
        "Average":63.81,
        "ARC":56.66,
        "HellaSwag":79.08,
        "MMLU":65.86,
        "TruthfulQA":47.75,
        "Winogrande":74.9,
        "GSM8K":58.61,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"mit",
        "#Params (B)":14.17,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"0649cf49b7a879fe837567a346a3ebbbac77614a"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/CausalLM-Platypus-14B",
        "Average":63.8,
        "ARC":56.91,
        "HellaSwag":80.06,
        "MMLU":64.98,
        "TruthfulQA":47.57,
        "Winogrande":76.01,
        "GSM8K":57.24,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":14.17,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"1659d3cdbb8bb8dba902ab2874f4fa886980fc70"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"TigerResearch\/tigerbot-70b-base",
        "Average":63.71,
        "ARC":62.46,
        "HellaSwag":83.61,
        "MMLU":65.49,
        "TruthfulQA":52.76,
        "Winogrande":80.19,
        "GSM8K":37.76,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":68.95,
        "Hub \u2764\ufe0f":11.0,
        "Available on the Hub":true,
        "Model Sha":"8af85526293eb8625375f3f7a1bab69825176e48"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/gpt4-alpaca-lora_mlp-65B-HF",
        "Average":63.66,
        "ARC":65.02,
        "HellaSwag":86.13,
        "MMLU":62.73,
        "TruthfulQA":59.16,
        "Winogrande":80.66,
        "GSM8K":28.28,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"664ff8e3e1d446971a16a6c9018ab24de7664684"
    },
    {
        "T":"\u2b55",
        "Model":"monology\/openinstruct-mistral-7b",
        "Average":63.64,
        "ARC":59.73,
        "HellaSwag":82.77,
        "MMLU":60.55,
        "TruthfulQA":48.76,
        "Winogrande":79.56,
        "GSM8K":50.49,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"54f379bf7676ffd09b48b0ff607b7ae6c0a6f688"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"luffycodes\/higgs-llama-vicuna-ep25-70b",
        "Average":63.6,
        "ARC":62.29,
        "HellaSwag":86.07,
        "MMLU":64.25,
        "TruthfulQA":53.75,
        "Winogrande":80.66,
        "GSM8K":34.57,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"1da59e150f1d0bae67f66400738a01d408a8c45d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"argilla\/notus-7b-v1",
        "Average":63.49,
        "ARC":64.59,
        "HellaSwag":84.83,
        "MMLU":63.04,
        "TruthfulQA":54.35,
        "Winogrande":79.56,
        "GSM8K":34.57,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":11.0,
        "Available on the Hub":true,
        "Model Sha":"f23f4cf6cb76402c76e932ead01109191af72a60"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"xxyyy123\/Mistral7B_adaptor_v1",
        "Average":63.42,
        "ARC":62.97,
        "HellaSwag":83.81,
        "MMLU":63.56,
        "TruthfulQA":49.77,
        "Winogrande":79.16,
        "GSM8K":41.24,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"23e800094570c22fbaa4279ef7e7f27315ac61af"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"llm-agents\/tora-70b-v1.0",
        "Average":63.39,
        "ARC":67.75,
        "HellaSwag":85.83,
        "MMLU":69.22,
        "TruthfulQA":51.79,
        "Winogrande":81.93,
        "GSM8K":23.81,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"e95fd7daf017e7c414ec07ebef4ddf013c16f9a4"
    },
    {
        "T":"\u2b55",
        "Model":"maywell\/Mini_Synatra_SFT",
        "Average":63.39,
        "ARC":62.46,
        "HellaSwag":83.44,
        "MMLU":61.2,
        "TruthfulQA":53.67,
        "Winogrande":74.66,
        "GSM8K":44.88,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"fc042f671dc0c94b21a6107eda75a6f9c8d44f2d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"xxyyy123\/1701221123_Ads_Mistral7B-slimorca_all-Lqv-r4b128",
        "Average":63.37,
        "ARC":62.88,
        "HellaSwag":83.99,
        "MMLU":62.89,
        "TruthfulQA":50.55,
        "Winogrande":79.72,
        "GSM8K":40.18,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"2872cd97f88418d6b07082048b316ea5b996982d"
    },
    {
        "T":"\u2b55",
        "Model":"01-ai\/Yi-34B-Chat",
        "Average":63.17,
        "ARC":65.1,
        "HellaSwag":84.08,
        "MMLU":74.87,
        "TruthfulQA":55.41,
        "Winogrande":79.79,
        "GSM8K":19.79,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":34.39,
        "Hub \u2764\ufe0f":18.0,
        "Available on the Hub":true,
        "Model Sha":"a99ec35331cbfc9da596af7d4538fe2efecff03c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Kiddyz\/testllm-c2",
        "Average":63.13,
        "ARC":60.58,
        "HellaSwag":81.91,
        "MMLU":61.2,
        "TruthfulQA":49.87,
        "Winogrande":77.82,
        "GSM8K":47.38,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"b87c798bc27522824451dfccf5eae50edbd4263b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-falcon-180b-v12-preview0",
        "Average":63.06,
        "ARC":62.29,
        "HellaSwag":83.8,
        "MMLU":55.92,
        "TruthfulQA":53.05,
        "Winogrande":82.08,
        "GSM8K":41.24,
        "Type":"fine-tuned",
        "Architecture":"FalconForCausalLM",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":178.64,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"4f1aeb136860ee3216f23faec0c598014e5c40a6"
    },
    {
        "T":"?",
        "Model":"adonlee\/LLaMA_2_13B_SFT_v1",
        "Average":63.04,
        "ARC":64.51,
        "HellaSwag":83.38,
        "MMLU":58.6,
        "TruthfulQA":53.2,
        "Winogrande":78.53,
        "GSM8K":40.03,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"31421b19a3f5fe2eff4871c86d3a94d5723b6fd2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Airoboros-L2-70B-2.1-GPTQ",
        "Average":63.04,
        "ARC":70.39,
        "HellaSwag":86.54,
        "MMLU":68.89,
        "TruthfulQA":55.55,
        "Winogrande":81.61,
        "GSM8K":15.24,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"GPTQ",
        "Hub License":"llama2",
        "#Params (B)":72.82,
        "Hub \u2764\ufe0f":14.0,
        "Available on the Hub":true,
        "Model Sha":"23ed580cb77ebaee49ea11eb4538fd3ab3795b76"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Azure99\/blossom-v3-mistral-7b",
        "Average":62.95,
        "ARC":60.49,
        "HellaSwag":81.9,
        "MMLU":61.35,
        "TruthfulQA":50.31,
        "Winogrande":76.95,
        "GSM8K":46.7,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"ec6e84a662c801e248d3bb3a19529155de02bda0"
    },
    {
        "T":"\u2b55",
        "Model":"teknium\/CollectiveCognition-v1.1-Mistral-7B",
        "Average":62.92,
        "ARC":62.12,
        "HellaSwag":84.17,
        "MMLU":62.35,
        "TruthfulQA":57.62,
        "Winogrande":75.37,
        "GSM8K":35.86,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":45.0,
        "Available on the Hub":true,
        "Model Sha":"5f57f70ec99450c70da2540e94dd7fd67be4b23c"
    },
    {
        "T":"\u2b55",
        "Model":"chargoddard\/MelangeA-70b",
        "Average":62.82,
        "ARC":71.25,
        "HellaSwag":87.3,
        "MMLU":70.56,
        "TruthfulQA":60.61,
        "Winogrande":81.53,
        "GSM8K":5.69,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":68.98,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"d48cf79d1ead50154b1e70120779ae91bc5fafb4"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"huggyllama\/llama-65b",
        "Average":62.79,
        "ARC":63.48,
        "HellaSwag":86.09,
        "MMLU":63.93,
        "TruthfulQA":43.43,
        "Winogrande":82.56,
        "GSM8K":37.23,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":65.29,
        "Hub \u2764\ufe0f":64.0,
        "Available on the Hub":true,
        "Model Sha":"49707c5313d34d1c5a846e29cf2a2a650c22c8ee"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Faradaylab\/ARIA-70B-V3",
        "Average":62.73,
        "ARC":63.91,
        "HellaSwag":86.21,
        "MMLU":64.75,
        "TruthfulQA":51.32,
        "Winogrande":82.08,
        "GSM8K":28.13,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":68.98,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"6e7fdcd20626786dd744ea86c664a3c088ced39f"
    },
    {
        "T":"\u2b55",
        "Model":"mlinmg\/SG-Raccoon-Yi-200k-2.0",
        "Average":62.72,
        "ARC":62.54,
        "HellaSwag":80.26,
        "MMLU":73.29,
        "TruthfulQA":53.21,
        "Winogrande":76.32,
        "GSM8K":30.71,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":55.59,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"986706415fcb2118f35626dbc12e054457ec9ad3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/guanaco-65B-HF",
        "Average":62.67,
        "ARC":65.44,
        "HellaSwag":86.47,
        "MMLU":62.92,
        "TruthfulQA":52.81,
        "Winogrande":82.4,
        "GSM8K":26.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":26.0,
        "Available on the Hub":true,
        "Model Sha":"7f83ae526f8b83705ca8434535da8fd8c692f9d0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Tess-XS-v1-3-yarn-128K",
        "Average":62.66,
        "ARC":61.09,
        "HellaSwag":82.95,
        "MMLU":62.15,
        "TruthfulQA":50.13,
        "Winogrande":74.43,
        "GSM8K":45.19,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"0f5977a5d2fa791359dc92eb1574b6112e709cad"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/llama-2-70b-IA3-guanaco",
        "Average":62.61,
        "ARC":68.52,
        "HellaSwag":85.67,
        "MMLU":67.03,
        "TruthfulQA":43.47,
        "Winogrande":82.24,
        "GSM8K":28.73,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"e3230df22d065b6699096494d1151fa337dde9e8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-l2-70b-gpt4-2.0",
        "Average":62.6,
        "ARC":68.6,
        "HellaSwag":87.53,
        "MMLU":69.37,
        "TruthfulQA":48.52,
        "Winogrande":83.9,
        "GSM8K":17.66,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"4bit",
        "Hub License":"other",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":12.0,
        "Available on the Hub":true,
        "Model Sha":"f16526d9bb814dc10adc911f94e8c7a520beb5b6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/VicUnlocked-alpaca-65B-QLoRA-fp16",
        "Average":62.58,
        "ARC":65.61,
        "HellaSwag":85.15,
        "MMLU":63.13,
        "TruthfulQA":52.47,
        "Winogrande":81.29,
        "GSM8K":27.82,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":9.0,
        "Available on the Hub":true,
        "Model Sha":"6cdacfda96970aa144e316b108ab9bc17c99a573"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Azure99\/blossom-v3_1-mistral-7b",
        "Average":62.53,
        "ARC":60.49,
        "HellaSwag":81.71,
        "MMLU":61.0,
        "TruthfulQA":49.51,
        "Winogrande":75.53,
        "GSM8K":46.93,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"d84e28c169a93933829e10f314f1e3e674df9843"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Tess-XS-v1-3-yarn-128K",
        "Average":62.49,
        "ARC":61.6,
        "HellaSwag":82.96,
        "MMLU":62.1,
        "TruthfulQA":50.2,
        "Winogrande":74.74,
        "GSM8K":43.37,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"72d393d13f1bd26442e59993c57840b91ff6f6fc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ValiantLabs\/ShiningValiantXS",
        "Average":62.48,
        "ARC":64.42,
        "HellaSwag":83.58,
        "MMLU":60.37,
        "TruthfulQA":55.0,
        "Winogrande":76.8,
        "GSM8K":34.72,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"8c1f86bd2e646408eed2ed3a2634b38ea4e5c599"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"meta-llama\/Llama-2-70b-chat-hf",
        "Average":62.4,
        "ARC":64.59,
        "HellaSwag":85.88,
        "MMLU":63.91,
        "TruthfulQA":52.8,
        "Winogrande":80.51,
        "GSM8K":26.69,
        "Type":"RL-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":68.98,
        "Hub \u2764\ufe0f":1453.0,
        "Available on the Hub":false,
        "Model Sha":"7f54101c0fbb67a8143ca23eb8bd09b71f269c74"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"willyninja30\/ARIA-70B-French",
        "Average":62.37,
        "ARC":64.51,
        "HellaSwag":85.87,
        "MMLU":63.88,
        "TruthfulQA":52.8,
        "Winogrande":80.51,
        "GSM8K":26.69,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"d8580d360c51e71fddd27897445e2aa9d1888585"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-65b-gpt4-1.2",
        "Average":62.36,
        "ARC":65.87,
        "HellaSwag":86.08,
        "MMLU":63.37,
        "TruthfulQA":52.72,
        "Winogrande":79.56,
        "GSM8K":26.54,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":21.0,
        "Available on the Hub":true,
        "Model Sha":"50ab86e198e1c82ec81aefc628f23501c101d390"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/zephyr-7b-alpha-dare-0.85",
        "Average":62.35,
        "ARC":61.18,
        "HellaSwag":83.67,
        "MMLU":64.3,
        "TruthfulQA":44.41,
        "Winogrande":78.45,
        "GSM8K":42.08,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"afe35301593b4ce2e7b5d1696066724ef1f802eb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vmajor\/Orca2-13B-selfmerge-39B",
        "Average":62.24,
        "ARC":60.84,
        "HellaSwag":79.84,
        "MMLU":60.32,
        "TruthfulQA":56.38,
        "Winogrande":76.87,
        "GSM8K":39.2,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"ms-pl",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"7a9e6775716a3947d0e40842b5d61753bc0551ac"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vmajor\/Orca2-13B-selfmerge-26B",
        "Average":62.24,
        "ARC":60.84,
        "HellaSwag":79.84,
        "MMLU":60.32,
        "TruthfulQA":56.38,
        "Winogrande":76.87,
        "GSM8K":39.2,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"ms-pl",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"46cdde5be7e3c48ada1bd3143ad593eecfb641e7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/Orca-2-13b-f16",
        "Average":62.14,
        "ARC":60.67,
        "HellaSwag":79.81,
        "MMLU":60.37,
        "TruthfulQA":56.41,
        "Winogrande":76.64,
        "GSM8K":38.97,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"b29c52ea0757c460e83592e55ea89e016cef3549"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenLemur\/lemur-70b-v1",
        "Average":62.07,
        "ARC":64.33,
        "HellaSwag":85.72,
        "MMLU":65.85,
        "TruthfulQA":44.78,
        "Winogrande":83.03,
        "GSM8K":28.73,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":31.0,
        "Available on the Hub":true,
        "Model Sha":"74432ae16ef50207fe17fb88b2f1c1d32ef3b481"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-mistral-dolphin-orca-platypus-samantha-7b-dare-0.85",
        "Average":62.06,
        "ARC":61.69,
        "HellaSwag":83.85,
        "MMLU":64.43,
        "TruthfulQA":43.13,
        "Winogrande":78.93,
        "GSM8K":40.33,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"7a3def1c382793d2b12741896302c31a471b6d1d"
    },
    {
        "T":"?",
        "Model":"Aeala\/Alpaca-elina-65b",
        "Average":62.03,
        "ARC":65.27,
        "HellaSwag":85.75,
        "MMLU":63.42,
        "TruthfulQA":47.32,
        "Winogrande":81.37,
        "GSM8K":29.04,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"51ce30a69b3c3363c8cfcd6395bf1df974ba2977"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abhishek\/zephyr-beta-math",
        "Average":61.99,
        "ARC":56.66,
        "HellaSwag":81.26,
        "MMLU":57.24,
        "TruthfulQA":44.83,
        "Winogrande":75.53,
        "GSM8K":56.41,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"dd3d070a104d8b36ba98d14a485d88fa95aaab63"
    },
    {
        "T":"\u2b55",
        "Model":"chargoddard\/MelangeC-70b",
        "Average":61.96,
        "ARC":71.67,
        "HellaSwag":87.6,
        "MMLU":70.37,
        "TruthfulQA":58.13,
        "Winogrande":83.98,
        "GSM8K":0.0,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":68.98,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"e54a2b924dec135f3fa2373933ab8485178cde1b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Faradaylab\/ARIA-70B-V2",
        "Average":61.93,
        "ARC":62.12,
        "HellaSwag":85.68,
        "MMLU":63.49,
        "TruthfulQA":49.8,
        "Winogrande":81.69,
        "GSM8K":28.81,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"2bf026af438d522268533484a85a3e54178e7809"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Gryphe\/MythoMist-7b",
        "Average":61.67,
        "ARC":65.87,
        "HellaSwag":83.55,
        "MMLU":62.32,
        "TruthfulQA":59.98,
        "Winogrande":78.06,
        "GSM8K":20.24,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"3b6c71416d191ab161fd3043117304a10df99716"
    },
    {
        "T":"?",
        "Model":"lvkaokao\/mistral-7b-finetuned-orca-dpo-v2",
        "Average":61.59,
        "ARC":66.21,
        "HellaSwag":83.64,
        "MMLU":62.37,
        "TruthfulQA":59.65,
        "Winogrande":78.14,
        "GSM8K":19.56,
        "Type":"",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"a5c1daaec60a480e8c81b265135583034054be2b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Intel\/neural-chat-7b-v3-1",
        "Average":61.59,
        "ARC":66.21,
        "HellaSwag":83.64,
        "MMLU":62.37,
        "TruthfulQA":59.65,
        "Winogrande":78.14,
        "GSM8K":19.56,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":12.0,
        "Available on the Hub":true,
        "Model Sha":"3995e9a13d54ce95f0ad55de2eaa92e2dc580174"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Intel\/neural-chat-7b-v3-1",
        "Average":61.59,
        "ARC":65.7,
        "HellaSwag":83.54,
        "MMLU":62.12,
        "TruthfulQA":59.48,
        "Winogrande":78.61,
        "GSM8K":20.09,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"8bit",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":12.0,
        "Available on the Hub":true,
        "Model Sha":"af2489cde09e9d2c175622f651875e83824c4b10"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HuggingFaceH4\/zephyr-7b-beta",
        "Average":61.59,
        "ARC":62.46,
        "HellaSwag":84.35,
        "MMLU":60.7,
        "TruthfulQA":57.83,
        "Winogrande":77.11,
        "GSM8K":27.07,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"8bit",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":976.0,
        "Available on the Hub":true,
        "Model Sha":"0f17b36adfbe7d86ea1c591a9efeeae17b313f48"
    },
    {
        "T":"\u2b55",
        "Model":"tianlinliu0121\/zephyr-7b-dpo-full-beta-0.2",
        "Average":61.55,
        "ARC":61.77,
        "HellaSwag":84.04,
        "MMLU":61.79,
        "TruthfulQA":54.72,
        "Winogrande":76.95,
        "GSM8K":30.02,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"727b63fc1ca6a592072159a7185c22f74cd38480"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Intel\/neural-chat-7b-v3-1",
        "Average":61.54,
        "ARC":66.3,
        "HellaSwag":83.6,
        "MMLU":62.44,
        "TruthfulQA":59.54,
        "Winogrande":77.98,
        "GSM8K":19.41,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":12.0,
        "Available on the Hub":true,
        "Model Sha":"3995e9a13d54ce95f0ad55de2eaa92e2dc580174"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"teknium\/OpenHermes-2.5-Mistral-7B",
        "Average":61.52,
        "ARC":64.93,
        "HellaSwag":84.18,
        "MMLU":63.64,
        "TruthfulQA":52.24,
        "Winogrande":78.06,
        "GSM8K":26.08,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":125.0,
        "Available on the Hub":true,
        "Model Sha":"2a54cad766bc90828354db5c4199795aecfd0df1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/robin-65b-v2-fp16",
        "Average":61.48,
        "ARC":61.95,
        "HellaSwag":84.6,
        "MMLU":62.51,
        "TruthfulQA":52.31,
        "Winogrande":80.51,
        "GSM8K":26.99,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"40edb31ba93045d673735361bc98f56125bbc77b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"teknium\/OpenHermes-2.5-Mistral-7B",
        "Average":61.45,
        "ARC":64.93,
        "HellaSwag":84.3,
        "MMLU":63.82,
        "TruthfulQA":52.31,
        "Winogrande":77.9,
        "GSM8K":25.47,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":115.0,
        "Available on the Hub":true,
        "Model Sha":"2a54cad766bc90828354db5c4199795aecfd0df1"
    },
    {
        "T":"\u2b55",
        "Model":"tianlinliu0121\/zephyr-7b-dpo-full-beta-0.2",
        "Average":61.36,
        "ARC":61.86,
        "HellaSwag":83.98,
        "MMLU":61.85,
        "TruthfulQA":54.78,
        "Winogrande":76.95,
        "GSM8K":28.73,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"727b63fc1ca6a592072159a7185c22f74cd38480"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/alpaca-lora-65B-HF",
        "Average":61.33,
        "ARC":64.85,
        "HellaSwag":85.59,
        "MMLU":63.11,
        "TruthfulQA":45.15,
        "Winogrande":81.22,
        "GSM8K":28.05,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"113b61b37a2862b950ada68620e57acafbcefe13"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NickyNicky\/Mistral-7B-OpenOrca-oasst_top1_2023-08-25-v3",
        "Average":61.26,
        "ARC":60.58,
        "HellaSwag":83.34,
        "MMLU":61.53,
        "TruthfulQA":48.21,
        "Winogrande":77.74,
        "GSM8K":36.16,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"43abfcab8bf532a2601ed6e61e0c3614272b7df9"
    },
    {
        "T":"\u2b55",
        "Model":"WizardLM\/WizardLM-70B-V1.0",
        "Average":61.25,
        "ARC":65.44,
        "HellaSwag":84.41,
        "MMLU":64.05,
        "TruthfulQA":54.81,
        "Winogrande":80.82,
        "GSM8K":17.97,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":112.0,
        "Available on the Hub":true,
        "Model Sha":"6dae38060d70b82dcfe787a612d04aaf0adf0738"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openchat\/openchat_3.5",
        "Average":61.24,
        "ARC":63.91,
        "HellaSwag":84.79,
        "MMLU":64.94,
        "TruthfulQA":46.38,
        "Winogrande":80.58,
        "GSM8K":26.84,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":517.0,
        "Available on the Hub":true,
        "Model Sha":"5b874a33a91d63023055e6cb2d5d86afe883b4ec"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"Minirecord\/Mini_DPO_test02",
        "Average":61.23,
        "ARC":59.73,
        "HellaSwag":83.89,
        "MMLU":61.9,
        "TruthfulQA":48.47,
        "Winogrande":78.37,
        "GSM8K":35.03,
        "Type":"RL-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"cd417467644c4178100083e342bad88a3f968be6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openchat\/openchat_3.5",
        "Average":61.22,
        "ARC":63.82,
        "HellaSwag":84.8,
        "MMLU":64.98,
        "TruthfulQA":46.39,
        "Winogrande":80.74,
        "GSM8K":26.61,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":517.0,
        "Available on the Hub":true,
        "Model Sha":"5b874a33a91d63023055e6cb2d5d86afe883b4ec"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-65b-gpt4-2.0",
        "Average":61.2,
        "ARC":66.64,
        "HellaSwag":86.66,
        "MMLU":63.18,
        "TruthfulQA":49.11,
        "Winogrande":80.74,
        "GSM8K":20.85,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"ea4bdd0221f77de9b0343cd8291cbd0fd6033ca8"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"huggingface\/llama-65b",
        "Average":61.19,
        "ARC":63.48,
        "HellaSwag":86.09,
        "MMLU":63.93,
        "TruthfulQA":43.43,
        "Winogrande":82.56,
        "GSM8K":27.67,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":65.29,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"4ae2e56610e8b9b9a78472708390668e9096b4f9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openaccess-ai-collective\/mistral-7b-slimorcaboros",
        "Average":61.18,
        "ARC":63.65,
        "HellaSwag":83.7,
        "MMLU":63.46,
        "TruthfulQA":55.81,
        "Winogrande":77.03,
        "GSM8K":23.43,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"c06e1a6b6c0fe764117f9ec7611ce31e796e602a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openaccess-ai-collective\/jackalope-7b",
        "Average":61.16,
        "ARC":63.4,
        "HellaSwag":83.29,
        "MMLU":63.5,
        "TruthfulQA":50.06,
        "Winogrande":78.06,
        "GSM8K":28.66,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":16.0,
        "Available on the Hub":true,
        "Model Sha":"5ba23522319a51d0af23b336a6a83c72ae3780e7"
    },
    {
        "T":"\u2b55",
        "Model":"mrm8488\/mistral-7b-ft-h4-no_robots_instructions",
        "Average":61.16,
        "ARC":60.92,
        "HellaSwag":83.17,
        "MMLU":63.37,
        "TruthfulQA":43.63,
        "Winogrande":78.85,
        "GSM8K":37.0,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":9.0,
        "Available on the Hub":true,
        "Model Sha":"785446da9a53ceae48795069bf7ccaf46a91a5ba"
    },
    {
        "T":"\u2b55",
        "Model":"mrm8488\/mistral-7b-ft-h4-no_robots_instructions",
        "Average":61.16,
        "ARC":60.92,
        "HellaSwag":83.24,
        "MMLU":63.74,
        "TruthfulQA":43.64,
        "Winogrande":78.69,
        "GSM8K":36.69,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":9.0,
        "Available on the Hub":true,
        "Model Sha":"785446da9a53ceae48795069bf7ccaf46a91a5ba"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-65b-gpt4-2.0",
        "Average":61.14,
        "ARC":66.81,
        "HellaSwag":86.66,
        "MMLU":63.41,
        "TruthfulQA":49.17,
        "Winogrande":80.27,
        "GSM8K":20.55,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"ea4bdd0221f77de9b0343cd8291cbd0fd6033ca8"
    },
    {
        "T":"\u2b55",
        "Model":"ehartford\/dolphin-2.1-mistral-7b",
        "Average":61.12,
        "ARC":64.42,
        "HellaSwag":84.92,
        "MMLU":63.32,
        "TruthfulQA":55.56,
        "Winogrande":77.74,
        "GSM8K":20.77,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":54.0,
        "Available on the Hub":true,
        "Model Sha":"aa5bd48c8b3040d1155a8fd59328df160aa63680"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/dolphin-2.1-mistral-7b",
        "Average":61.0,
        "ARC":63.99,
        "HellaSwag":85.0,
        "MMLU":63.44,
        "TruthfulQA":55.57,
        "Winogrande":77.9,
        "GSM8K":20.09,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":54.0,
        "Available on the Hub":true,
        "Model Sha":"aa5bd48c8b3040d1155a8fd59328df160aa63680"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"mistralai\/Mistral-7B-v0.1",
        "Average":60.97,
        "ARC":59.98,
        "HellaSwag":83.31,
        "MMLU":64.16,
        "TruthfulQA":42.15,
        "Winogrande":78.37,
        "GSM8K":37.83,
        "Type":"pretrained",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":1173.0,
        "Available on the Hub":true,
        "Model Sha":"e836d8f71b5812f9fee65618453dc537c66bd82a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"upstage\/llama-30b-instruct-2048",
        "Average":60.91,
        "ARC":64.93,
        "HellaSwag":84.94,
        "MMLU":61.9,
        "TruthfulQA":56.3,
        "Winogrande":79.56,
        "GSM8K":17.82,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":102.0,
        "Available on the Hub":true,
        "Model Sha":"be44a37814a20e790063086703f570732597887a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HiTZ\/alpaca-lora-65b-en-pt-es-ca",
        "Average":60.89,
        "ARC":65.02,
        "HellaSwag":84.88,
        "MMLU":62.19,
        "TruthfulQA":46.06,
        "Winogrande":80.51,
        "GSM8K":26.69,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":65.0,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":false,
        "Model Sha":"aa5bd88bd132925cf2dd5c44eceafdb5ed5e5be4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/SlimOpenOrca-Mistral-7B",
        "Average":60.84,
        "ARC":62.97,
        "HellaSwag":83.49,
        "MMLU":62.3,
        "TruthfulQA":57.39,
        "Winogrande":77.43,
        "GSM8K":21.46,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"b0134a7512444dfbb60a2e2d81469a5bbbb18026"
    },
    {
        "T":"?",
        "Model":"CalderaAI\/30B-Epsilon",
        "Average":60.8,
        "ARC":63.05,
        "HellaSwag":83.59,
        "MMLU":56.89,
        "TruthfulQA":59.03,
        "Winogrande":77.66,
        "GSM8K":24.56,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":9.0,
        "Available on the Hub":true,
        "Model Sha":"6962638c2b0368ad496af6e20e46e3de97a7772b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-mistral-dolphin-orca-platypus-samantha-7b",
        "Average":60.79,
        "ARC":64.33,
        "HellaSwag":84.4,
        "MMLU":63.72,
        "TruthfulQA":52.52,
        "Winogrande":78.37,
        "GSM8K":21.38,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"d4039b40e842df7f6b8de50532444c8944ea5791"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-65b-gpt4-m2.0",
        "Average":60.79,
        "ARC":65.02,
        "HellaSwag":86.35,
        "MMLU":64.37,
        "TruthfulQA":46.66,
        "Winogrande":80.19,
        "GSM8K":22.14,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"fa081d52619b35d7016fb40ce855187d6a8e7e4c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddyEA\/openbuddy-llama-30b-v7.1-bf16",
        "Average":60.76,
        "ARC":62.37,
        "HellaSwag":82.29,
        "MMLU":58.18,
        "TruthfulQA":52.6,
        "Winogrande":77.51,
        "GSM8K":31.61,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":32.35,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"85f7ad9d6ff016312262a47d45ffd07dee54aab0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-mistral-six-in-one-7b",
        "Average":60.76,
        "ARC":62.97,
        "HellaSwag":84.6,
        "MMLU":63.29,
        "TruthfulQA":57.77,
        "Winogrande":77.51,
        "GSM8K":18.42,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"41e912e0f79094a80687f88ca5555f84aa9d307f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MetaIX\/GPT4-X-Alpasta-30b",
        "Average":60.76,
        "ARC":63.05,
        "HellaSwag":83.56,
        "MMLU":57.71,
        "TruthfulQA":51.52,
        "Winogrande":78.22,
        "GSM8K":30.48,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":64.0,
        "Available on the Hub":true,
        "Model Sha":"1a0d1d72a40946463fb4a9780207da19bfecc38b"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"Yhyu13\/oasst-rlhf-2-llama-30b-7k-steps-hf",
        "Average":60.74,
        "ARC":61.35,
        "HellaSwag":83.8,
        "MMLU":57.89,
        "TruthfulQA":51.18,
        "Winogrande":78.77,
        "GSM8K":31.46,
        "Type":"RL-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"e04207847429af03c4780f5ac85c726536217981"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddyEA\/openbuddy-llama-30b-v7.1-bf16",
        "Average":60.71,
        "ARC":62.46,
        "HellaSwag":82.3,
        "MMLU":58.15,
        "TruthfulQA":52.57,
        "Winogrande":77.82,
        "GSM8K":30.93,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":32.35,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"85f7ad9d6ff016312262a47d45ffd07dee54aab0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-65b-gpt4-m2.0",
        "Average":60.68,
        "ARC":65.1,
        "HellaSwag":86.34,
        "MMLU":64.32,
        "TruthfulQA":46.63,
        "Winogrande":80.11,
        "GSM8K":21.61,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"fa081d52619b35d7016fb40ce855187d6a8e7e4c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-65b-gpt4-1.4-peft",
        "Average":60.67,
        "ARC":65.78,
        "HellaSwag":85.83,
        "MMLU":62.27,
        "TruthfulQA":52.45,
        "Winogrande":79.64,
        "GSM8K":18.04,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":65.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"85ae3b595c6b8415df87000c22bc14ea18c174f5"
    },
    {
        "T":"?",
        "Model":"jondurbin\/airoboros-65b-gpt4-1.4",
        "Average":60.67,
        "ARC":65.78,
        "HellaSwag":85.83,
        "MMLU":62.27,
        "TruthfulQA":52.45,
        "Winogrande":79.64,
        "GSM8K":18.04,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":16.0,
        "Available on the Hub":true,
        "Model Sha":"ae256799615c16443f9c423c653ed9f60577e99e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Zephyrus-L1-33B",
        "Average":60.61,
        "ARC":64.51,
        "HellaSwag":84.15,
        "MMLU":57.37,
        "TruthfulQA":53.87,
        "Winogrande":80.19,
        "GSM8K":23.58,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":32.53,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"679aae34440d576456b283070371b2a15dbb948b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-65b-gpt4-1.4",
        "Average":60.59,
        "ARC":65.53,
        "HellaSwag":85.77,
        "MMLU":61.95,
        "TruthfulQA":52.43,
        "Winogrande":79.79,
        "GSM8K":18.04,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":16.0,
        "Available on the Hub":true,
        "Model Sha":"ae256799615c16443f9c423c653ed9f60577e99e"
    },
    {
        "T":"\u2b55",
        "Model":"maywell\/Synatra-7B-v0.3-dpo",
        "Average":60.55,
        "ARC":62.8,
        "HellaSwag":82.58,
        "MMLU":61.46,
        "TruthfulQA":56.46,
        "Winogrande":76.24,
        "GSM8K":23.73,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"405a4f1e6513cd1b8de5eb4e003bb49cc86d1f8a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/dolphin-2.2.1-mistral-7b",
        "Average":60.54,
        "ARC":63.48,
        "HellaSwag":83.86,
        "MMLU":63.28,
        "TruthfulQA":53.17,
        "Winogrande":78.37,
        "GSM8K":21.08,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":84.0,
        "Available on the Hub":true,
        "Model Sha":"001b48e9aebffb395c698af47b6b48364cc3cbe8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/Mistral-11B-TestBench9",
        "Average":60.52,
        "ARC":64.08,
        "HellaSwag":84.24,
        "MMLU":64.0,
        "TruthfulQA":56.19,
        "Winogrande":78.45,
        "GSM8K":16.15,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"4ff48527af8c3907129c06160c7f7b7b786a5a79"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/WizardLM-70B-V1.0-GPTQ",
        "Average":60.5,
        "ARC":63.82,
        "HellaSwag":83.85,
        "MMLU":63.68,
        "TruthfulQA":54.54,
        "Winogrande":78.61,
        "GSM8K":18.5,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"GPTQ",
        "Hub License":"llama2",
        "#Params (B)":72.82,
        "Hub \u2764\ufe0f":26.0,
        "Available on the Hub":true,
        "Model Sha":"c234d7c9c0fd26efb55757fdbfb604d549539fe0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/Dolphin2.1-OpenOrca-7B",
        "Average":60.47,
        "ARC":63.91,
        "HellaSwag":84.26,
        "MMLU":62.66,
        "TruthfulQA":53.84,
        "Winogrande":78.22,
        "GSM8K":19.94,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"076c0f7de93307e8fb3ad3bd820fb5f73325ca70"
    },
    {
        "T":"?",
        "Model":"TehVenom\/oasst-sft-6-llama-33b-xor-MERGED-16bit",
        "Average":60.45,
        "ARC":61.52,
        "HellaSwag":83.5,
        "MMLU":57.43,
        "TruthfulQA":50.7,
        "Winogrande":79.08,
        "GSM8K":30.48,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"62f92ddab8b37eaeda15cf5ecb5605141a0525eb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aeala\/GPT4-x-AlpacaDente-30b",
        "Average":60.43,
        "ARC":62.12,
        "HellaSwag":82.78,
        "MMLU":56.19,
        "TruthfulQA":52.68,
        "Winogrande":78.69,
        "GSM8K":30.1,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"ee76c821f861f0ab0276f9f429dd06565f1f2051"
    },
    {
        "T":"\u2b55",
        "Model":"WizardLM\/WizardMath-70B-V1.0",
        "Average":60.42,
        "ARC":68.17,
        "HellaSwag":86.49,
        "MMLU":68.89,
        "TruthfulQA":52.69,
        "Winogrande":82.32,
        "GSM8K":3.94,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":95.0,
        "Available on the Hub":true,
        "Model Sha":"e85b43e53c5379e35393b970c66d76c2d1060381"
    },
    {
        "T":"\u2b55",
        "Model":"WizardLM\/WizardMath-70B-V1.0",
        "Average":60.41,
        "ARC":67.92,
        "HellaSwag":86.46,
        "MMLU":68.92,
        "TruthfulQA":52.77,
        "Winogrande":82.32,
        "GSM8K":4.09,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":95.0,
        "Available on the Hub":true,
        "Model Sha":"e85b43e53c5379e35393b970c66d76c2d1060381"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ajibawa-2023\/SlimOrca-13B",
        "Average":60.39,
        "ARC":60.15,
        "HellaSwag":81.4,
        "MMLU":57.04,
        "TruthfulQA":49.37,
        "Winogrande":74.43,
        "GSM8K":39.95,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"75427e93dc99a5e1d8b9aefa106ad36fc750b744"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"speechlessai\/speechless-mistral-7b-dare-0.85",
        "Average":60.39,
        "ARC":63.31,
        "HellaSwag":84.93,
        "MMLU":64.22,
        "TruthfulQA":50.68,
        "Winogrande":79.32,
        "GSM8K":19.86,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"5eefd1b560cd65aec2f689880476f909b46d306c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/Dolphin2.1-OpenOrca-7B",
        "Average":60.38,
        "ARC":64.16,
        "HellaSwag":84.25,
        "MMLU":62.7,
        "TruthfulQA":53.83,
        "Winogrande":77.66,
        "GSM8K":19.71,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"076c0f7de93307e8fb3ad3bd820fb5f73325ca70"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Open-Orca\/Mistral-7B-SlimOrca",
        "Average":60.37,
        "ARC":62.54,
        "HellaSwag":83.86,
        "MMLU":62.77,
        "TruthfulQA":54.23,
        "Winogrande":77.43,
        "GSM8K":21.38,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"a9744d8cf9ce4230678a891bcf8bba7cbc0aaece"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openchat\/openchat_3.5",
        "Average":60.26,
        "ARC":62.46,
        "HellaSwag":83.96,
        "MMLU":62.89,
        "TruthfulQA":45.43,
        "Winogrande":81.06,
        "GSM8K":25.78,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":535.0,
        "Available on the Hub":true,
        "Model Sha":"5b874a33a91d63023055e6cb2d5d86afe883b4ec"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PulsarAI\/SlimOpenOrca-Mistral-7B-v2",
        "Average":60.25,
        "ARC":62.88,
        "HellaSwag":83.41,
        "MMLU":62.05,
        "TruthfulQA":56.65,
        "Winogrande":77.58,
        "GSM8K":18.95,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"7cd030ccdb169c2685fe028bb4380b91ad74920f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/Mistral-11B-TestBench11",
        "Average":60.25,
        "ARC":64.42,
        "HellaSwag":83.93,
        "MMLU":63.82,
        "TruthfulQA":56.68,
        "Winogrande":77.74,
        "GSM8K":14.94,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"9aae2b156b24557bb98e515f3a90c7865529d2e9"
    },
    {
        "T":"\u2b55",
        "Model":"vihangd\/smartyplats-7b-v2",
        "Average":60.24,
        "ARC":57.94,
        "HellaSwag":80.76,
        "MMLU":58.16,
        "TruthfulQA":50.26,
        "Winogrande":75.53,
        "GSM8K":38.82,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"99049eb184b9b3ef074043d6e626fe3db09f5a19"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lilloukas\/GPlatty-30B",
        "Average":60.23,
        "ARC":65.78,
        "HellaSwag":84.79,
        "MMLU":63.49,
        "TruthfulQA":52.45,
        "Winogrande":80.98,
        "GSM8K":13.87,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":18.0,
        "Available on the Hub":true,
        "Model Sha":"836cf4dcd60ebe2ff09415c72f809d94639e8d35"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"argilla\/notus-7b-v1",
        "Average":60.22,
        "ARC":64.59,
        "HellaSwag":84.78,
        "MMLU":63.03,
        "TruthfulQA":54.37,
        "Winogrande":79.4,
        "GSM8K":15.16,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"89f594b32aea9bf5de0abe3877f20ff302549934"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Open-Orca\/Mistral-7B-OpenOrca",
        "Average":60.17,
        "ARC":64.08,
        "HellaSwag":83.99,
        "MMLU":62.24,
        "TruthfulQA":53.05,
        "Winogrande":77.74,
        "GSM8K":19.94,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":314.0,
        "Available on the Hub":true,
        "Model Sha":"7233ac83317946d05c474b71cc1379f49eb74c14"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-65b-gpt4-1.3",
        "Average":60.15,
        "ARC":66.13,
        "HellaSwag":85.99,
        "MMLU":63.89,
        "TruthfulQA":51.32,
        "Winogrande":79.95,
        "GSM8K":13.65,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"4373e66135c6fb4a6063777c4270a34509e7e932"
    },
    {
        "T":"\u2b55",
        "Model":"teknium\/CollectiveCognition-v1-Mistral-7B",
        "Average":60.1,
        "ARC":62.37,
        "HellaSwag":85.5,
        "MMLU":62.76,
        "TruthfulQA":54.48,
        "Winogrande":77.58,
        "GSM8K":17.89,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"58777f0563610fa770c4fa252c0350de71d4ab9d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abhishek\/ccy0-2g7e-wqsa-0",
        "Average":60.07,
        "ARC":58.19,
        "HellaSwag":82.19,
        "MMLU":59.59,
        "TruthfulQA":49.99,
        "Winogrande":78.22,
        "GSM8K":32.22,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"1cd1158f3104fa8ed8469e2b09d674b997e229b4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aspik101\/llama-30b-2048-instruct-PL-lora_unload",
        "Average":60.03,
        "ARC":63.82,
        "HellaSwag":84.7,
        "MMLU":61.49,
        "TruthfulQA":52.49,
        "Winogrande":79.79,
        "GSM8K":17.89,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"b15f4310ea37fef99e4f16372a4b1f2342e27613"
    },
    {
        "T":"?",
        "Model":"ehartford\/WizardLM-33B-V1.0-Uncensored",
        "Average":59.99,
        "ARC":63.65,
        "HellaSwag":83.84,
        "MMLU":59.36,
        "TruthfulQA":56.8,
        "Winogrande":77.66,
        "GSM8K":18.65,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":44.0,
        "Available on the Hub":true,
        "Model Sha":"3eca9fdee0ce28d6a4a635a6f19d9a413caee3e7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jebcarter\/psyonic-cetacean-20B",
        "Average":59.97,
        "ARC":63.57,
        "HellaSwag":86.2,
        "MMLU":59.66,
        "TruthfulQA":57.55,
        "Winogrande":78.14,
        "GSM8K":14.71,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":19.99,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"298d2086a949d53af06096d229f64f4719261698"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Intel\/neural-chat-7b-v3-1",
        "Average":59.9,
        "ARC":64.25,
        "HellaSwag":82.49,
        "MMLU":60.79,
        "TruthfulQA":56.4,
        "Winogrande":77.35,
        "GSM8K":18.12,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":12.0,
        "Available on the Hub":true,
        "Model Sha":"3995e9a13d54ce95f0ad55de2eaa92e2dc580174"
    },
    {
        "T":"\u2b55",
        "Model":"ehartford\/samantha-1.2-mistral-7b",
        "Average":59.83,
        "ARC":64.08,
        "HellaSwag":85.08,
        "MMLU":63.91,
        "TruthfulQA":50.4,
        "Winogrande":78.53,
        "GSM8K":16.98,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"5574a021f55a446a756dcbc776f1765aefc280a1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aspik101\/llama-30b-instruct-2048-PL-lora",
        "Average":59.82,
        "ARC":63.31,
        "HellaSwag":84.66,
        "MMLU":61.66,
        "TruthfulQA":53.35,
        "Winogrande":79.08,
        "GSM8K":16.83,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"1a076bce564f03bd47951eecab628c541fb1a6ad"
    },
    {
        "T":"\u2b55",
        "Model":"WizardLM\/WizardMath-70B-V1.0",
        "Average":59.81,
        "ARC":67.49,
        "HellaSwag":86.03,
        "MMLU":68.44,
        "TruthfulQA":52.23,
        "Winogrande":81.77,
        "GSM8K":2.88,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"8bit",
        "Hub License":"llama2",
        "#Params (B)":68.72,
        "Hub \u2764\ufe0f":95.0,
        "Available on the Hub":true,
        "Model Sha":"97e5913edd2c593c3eef12070024674e7ee4e16c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HuggingFaceH4\/mistral-7b-sft-beta",
        "Average":59.78,
        "ARC":57.42,
        "HellaSwag":82.23,
        "MMLU":61.42,
        "TruthfulQA":43.58,
        "Winogrande":77.58,
        "GSM8K":36.47,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":14.0,
        "Available on the Hub":true,
        "Model Sha":"c985a04e76fb00d3c3f65214d0b02c5a751d2274"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Llama-2-70B-chat-GPTQ",
        "Average":59.75,
        "ARC":62.63,
        "HellaSwag":84.81,
        "MMLU":62.74,
        "TruthfulQA":50.98,
        "Winogrande":78.69,
        "GSM8K":18.65,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"GPTQ",
        "Hub License":"llama2",
        "#Params (B)":72.82,
        "Hub \u2764\ufe0f":204.0,
        "Available on the Hub":true,
        "Model Sha":"054fbf6f65e7ab7691ec07ec9ad366acf2dd90bf"
    },
    {
        "T":"\u2b55",
        "Model":"maywell\/Synatra-RP-Orca-2-7b-v0.1",
        "Average":59.65,
        "ARC":57.68,
        "HellaSwag":77.37,
        "MMLU":56.1,
        "TruthfulQA":52.52,
        "Winogrande":74.59,
        "GSM8K":39.65,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"da80bc823c407c28c464cc0547a8ed9e0ca82f79"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/Yarn-Mistral-7b-64k",
        "Average":59.63,
        "ARC":59.9,
        "HellaSwag":82.51,
        "MMLU":62.96,
        "TruthfulQA":41.86,
        "Winogrande":77.27,
        "GSM8K":33.28,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":32.0,
        "Available on the Hub":true,
        "Model Sha":"0273c624561fcecc8e8f4030492a9307aa60f945"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/SynthIA-7B-v1.5",
        "Average":59.59,
        "ARC":62.71,
        "HellaSwag":83.37,
        "MMLU":63.48,
        "TruthfulQA":51.32,
        "Winogrande":79.24,
        "GSM8K":17.44,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"5a9912ef90a0efc1aaea327e5cf3e9554c8bd897"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"internlm\/internlm-20b",
        "Average":59.55,
        "ARC":60.49,
        "HellaSwag":82.13,
        "MMLU":61.85,
        "TruthfulQA":52.61,
        "Winogrande":76.72,
        "GSM8K":23.5,
        "Type":"pretrained",
        "Architecture":"InternLMForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.0,
        "Hub \u2764\ufe0f":47.0,
        "Available on the Hub":true,
        "Model Sha":"b8825fe3394608fe84f0f5eb6471454384fb83aa"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"perlthoughts\/Chupacabra-v3",
        "Average":59.52,
        "ARC":66.21,
        "HellaSwag":81.29,
        "MMLU":59.36,
        "TruthfulQA":57.85,
        "Winogrande":77.43,
        "GSM8K":15.01,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"1dfa5e16d4be646b496d657d86554482ad48b3c9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/WizardLM-30B-fp16",
        "Average":59.51,
        "ARC":62.54,
        "HellaSwag":83.28,
        "MMLU":59.03,
        "TruthfulQA":52.49,
        "Winogrande":77.51,
        "GSM8K":22.21,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"465f87a243969963f25ae6cf8f8d2de6c0898bbe"
    },
    {
        "T":"?",
        "Model":"TheBloke\/gpt4-alpaca-lora-30b-HF",
        "Average":59.51,
        "ARC":64.85,
        "HellaSwag":85.72,
        "MMLU":58.51,
        "TruthfulQA":52.24,
        "Winogrande":80.19,
        "GSM8K":15.54,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":9.0,
        "Available on the Hub":true,
        "Model Sha":"3c8007467a081dc72ae09b9d358416b056b38920"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HuggingFaceH4\/zephyr-7b-alpha",
        "Average":59.5,
        "ARC":61.01,
        "HellaSwag":84.04,
        "MMLU":61.39,
        "TruthfulQA":57.9,
        "Winogrande":78.61,
        "GSM8K":14.03,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":377.0,
        "Available on the Hub":true,
        "Model Sha":"2cd2cd16a6ab22585d643cf264fac73b18e7852a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/HelpSteer-filtered-7B",
        "Average":59.49,
        "ARC":59.56,
        "HellaSwag":83.32,
        "MMLU":63.52,
        "TruthfulQA":41.11,
        "Winogrande":76.01,
        "GSM8K":33.43,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"0f14404caa1b4609bb2f50714df973223f443e40"
    },
    {
        "T":"?",
        "Model":"LLMs\/WizardLM-30B-V1.0",
        "Average":59.45,
        "ARC":62.54,
        "HellaSwag":83.27,
        "MMLU":59.05,
        "TruthfulQA":52.49,
        "Winogrande":77.51,
        "GSM8K":21.83,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"gpl-3.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"75318440dba949804d6263d368e1f29a94ea7c5f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/Yarn-Mistral-7b-128k",
        "Average":59.42,
        "ARC":59.64,
        "HellaSwag":82.5,
        "MMLU":63.02,
        "TruthfulQA":41.78,
        "Winogrande":76.95,
        "GSM8K":32.6,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":462.0,
        "Available on the Hub":true,
        "Model Sha":"d09f1f8ed437d61c1aff94c1beabee554843dcdd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Tess-XS-v1.1",
        "Average":59.39,
        "ARC":63.91,
        "HellaSwag":84.06,
        "MMLU":63.07,
        "TruthfulQA":49.92,
        "Winogrande":79.16,
        "GSM8K":16.22,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"e8850e534a3a9f602f72201b09c7ef8f879c1c0b"
    },
    {
        "T":"?",
        "Model":"openaccess-ai-collective\/grendel",
        "Average":59.36,
        "ARC":60.49,
        "HellaSwag":79.99,
        "MMLU":58.98,
        "TruthfulQA":52.68,
        "Winogrande":75.3,
        "GSM8K":28.73,
        "Type":"",
        "Architecture":"?",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"9444ef27ab9cc263745f9b24ffd7e2da60d2283c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/OpenAssistant-SFT-7-Llama-30B-HF",
        "Average":59.34,
        "ARC":60.58,
        "HellaSwag":82.17,
        "MMLU":57.93,
        "TruthfulQA":46.94,
        "Winogrande":78.61,
        "GSM8K":29.8,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":13.0,
        "Available on the Hub":true,
        "Model Sha":"a7a2306b9a63de2c545f35b24735f4540baf5903"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/SynthIA-7B-v1.3",
        "Average":59.34,
        "ARC":62.12,
        "HellaSwag":83.45,
        "MMLU":62.65,
        "TruthfulQA":51.37,
        "Winogrande":78.85,
        "GSM8K":17.59,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":115.0,
        "Available on the Hub":true,
        "Model Sha":"8e6d0b18be876e0ebfff47d6c4f33d776f189971"
    },
    {
        "T":"?",
        "Model":"bavest\/fin-llama-33b-merged",
        "Average":59.33,
        "ARC":65.02,
        "HellaSwag":86.2,
        "MMLU":58.73,
        "TruthfulQA":49.75,
        "Winogrande":80.03,
        "GSM8K":16.22,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"gpl",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":11.0,
        "Available on the Hub":true,
        "Model Sha":"17114520801da7b9599fe7a9fdf238915713a59b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Walmart-the-bag\/MysticFusion-13B",
        "Average":59.31,
        "ARC":61.35,
        "HellaSwag":84.43,
        "MMLU":57.29,
        "TruthfulQA":51.98,
        "Winogrande":76.01,
        "GSM8K":24.79,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"02255943c6eff59ef6bd17e1a43a37ce3751ff5e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ariellee\/SuperPlatty-30B",
        "Average":59.3,
        "ARC":65.78,
        "HellaSwag":83.95,
        "MMLU":62.57,
        "TruthfulQA":53.52,
        "Winogrande":80.35,
        "GSM8K":9.63,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":9.0,
        "Available on the Hub":true,
        "Model Sha":"017e1c32bca060107337dbf26db2044a7caa56f2"
    },
    {
        "T":"\u2b55",
        "Model":"Norquinal\/Mistral-7B-claude-instruct",
        "Average":59.27,
        "ARC":63.23,
        "HellaSwag":84.99,
        "MMLU":63.84,
        "TruthfulQA":47.47,
        "Winogrande":78.14,
        "GSM8K":17.97,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"faff0de73681ad1f0500169ae18d7a5ff424eb7f"
    },
    {
        "T":"\u2b55",
        "Model":"mergedlm\/zephyrnotus-11b-alpha",
        "Average":59.26,
        "ARC":61.35,
        "HellaSwag":82.8,
        "MMLU":60.67,
        "TruthfulQA":57.22,
        "Winogrande":76.4,
        "GSM8K":17.13,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"other",
        "#Params (B)":10.73,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"a6f74e800b6c77261a1d212bb3e6b2752cbedef9"
    },
    {
        "T":"\u2b55",
        "Model":"maywell\/Synatra-7B-v0.3-RP",
        "Average":59.26,
        "ARC":62.2,
        "HellaSwag":82.29,
        "MMLU":60.8,
        "TruthfulQA":52.64,
        "Winogrande":76.48,
        "GSM8K":21.15,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"372f6e0ab2c20b93e0c42218f76a71a4f9bb282e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HuggingFaceH4\/zephyr-7b-beta",
        "Average":59.23,
        "ARC":62.03,
        "HellaSwag":84.36,
        "MMLU":61.07,
        "TruthfulQA":57.45,
        "Winogrande":77.74,
        "GSM8K":12.74,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":784.0,
        "Available on the Hub":true,
        "Model Sha":"8af01af3d4f9dc9b962447180d6d0f8c5315da86"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gaodrew\/gaodrew-llama-30b-instruct-2048-Open-Platypus-100steps",
        "Average":59.22,
        "ARC":61.52,
        "HellaSwag":84.06,
        "MMLU":60.23,
        "TruthfulQA":51.05,
        "Winogrande":80.82,
        "GSM8K":17.66,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"1114ff08ed15ef417502da58f0237d2f6650c9ce"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Qwen\/Qwen-7B",
        "Average":59.19,
        "ARC":51.37,
        "HellaSwag":78.47,
        "MMLU":59.84,
        "TruthfulQA":47.79,
        "Winogrande":72.69,
        "GSM8K":44.96,
        "Type":"pretrained",
        "Architecture":"QWenLMHeadModel",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":7.72,
        "Hub \u2764\ufe0f":288.0,
        "Available on the Hub":true,
        "Model Sha":"c9bdb955021a80ae26fa6978891996dbe4951d8d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bofenghuang\/vigostral-7b-chat",
        "Average":59.18,
        "ARC":62.63,
        "HellaSwag":84.34,
        "MMLU":63.53,
        "TruthfulQA":49.24,
        "Winogrande":78.61,
        "GSM8K":16.76,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"969fbfc7a91f53c8562a2c48a3c24dd3745d5a97"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"maywell\/PiVoT-0.1-Evil-a",
        "Average":59.16,
        "ARC":59.64,
        "HellaSwag":81.48,
        "MMLU":58.94,
        "TruthfulQA":39.23,
        "Winogrande":75.3,
        "GSM8K":40.41,
        "Type":"RL-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"b6e20287ba4156f06b4288d4003acc677040527f"
    },
    {
        "T":"\u2b55",
        "Model":"FPHam\/Karen_TheEditor_V2_STRICT_Mistral_7B",
        "Average":59.13,
        "ARC":59.56,
        "HellaSwag":81.79,
        "MMLU":59.56,
        "TruthfulQA":49.36,
        "Winogrande":74.35,
        "GSM8K":30.17,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"0935960b2765aa23d7a63c49873361b09dd12f60"
    },
    {
        "T":"\u2b55",
        "Model":"PeanutJar\/Mistral-v0.1-PeanutButter-v0.0.0-7B",
        "Average":59.09,
        "ARC":62.2,
        "HellaSwag":84.1,
        "MMLU":64.14,
        "TruthfulQA":46.94,
        "Winogrande":78.69,
        "GSM8K":18.5,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"9609a969ba6429b84e538d96afac55eb133a9983"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HuggingFaceH4\/zephyr-7b-beta",
        "Average":59.08,
        "ARC":62.03,
        "HellaSwag":84.53,
        "MMLU":61.06,
        "TruthfulQA":57.44,
        "Winogrande":78.06,
        "GSM8K":11.37,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":784.0,
        "Available on the Hub":true,
        "Model Sha":"8af01af3d4f9dc9b962447180d6d0f8c5315da86"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NurtureAI\/openchat_3.5-16k",
        "Average":59.03,
        "ARC":63.31,
        "HellaSwag":83.58,
        "MMLU":61.9,
        "TruthfulQA":43.47,
        "Winogrande":80.11,
        "GSM8K":21.83,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":22.0,
        "Available on the Hub":true,
        "Model Sha":"e8d66e7fb2ebb918f468137ea5fa3dc13ddc69da"
    },
    {
        "T":"\u2b55",
        "Model":"garage-bAInd\/Platypus-30B",
        "Average":59.03,
        "ARC":64.59,
        "HellaSwag":84.26,
        "MMLU":64.23,
        "TruthfulQA":45.35,
        "Winogrande":81.37,
        "GSM8K":14.4,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":16.0,
        "Available on the Hub":true,
        "Model Sha":"c5d21054f8dd71099696bd7790df07ac54990f29"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lilloukas\/Platypus-30B",
        "Average":59.03,
        "ARC":64.59,
        "HellaSwag":84.24,
        "MMLU":64.19,
        "TruthfulQA":45.35,
        "Winogrande":81.37,
        "GSM8K":14.4,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":16.0,
        "Available on the Hub":true,
        "Model Sha":"979ad39b58a8e4a9419b7bc7a0dc8419f3912e71"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/orca_mini_v3_13B-GPTQ",
        "Average":59.01,
        "ARC":61.95,
        "HellaSwag":81.56,
        "MMLU":56.1,
        "TruthfulQA":49.22,
        "Winogrande":75.77,
        "GSM8K":29.49,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"GPTQ",
        "Hub License":"other",
        "#Params (B)":16.23,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"7b7a2dcd946f393e26215268c4c7e0699be2bbd8"
    },
    {
        "T":"\u2b55",
        "Model":"Weyaxi\/zephyr-alpha-Nebula-v2-7B",
        "Average":59.01,
        "ARC":58.62,
        "HellaSwag":83.05,
        "MMLU":56.68,
        "TruthfulQA":58.28,
        "Winogrande":73.56,
        "GSM8K":23.88,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"e8f1fd1acceda7fb662340f5afe312a7ef030374"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"unaidedelf87777\/wizard-mistral-v0.1",
        "Average":59.01,
        "ARC":61.77,
        "HellaSwag":83.51,
        "MMLU":63.99,
        "TruthfulQA":47.46,
        "Winogrande":78.3,
        "GSM8K":19.03,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"b66724f8195e7b76289f8f3f72a98392557c46ad"
    },
    {
        "T":"?",
        "Model":"ehartford\/samantha-1.1-llama-33b",
        "Average":58.98,
        "ARC":67.83,
        "HellaSwag":85.55,
        "MMLU":58.79,
        "TruthfulQA":61.19,
        "Winogrande":76.48,
        "GSM8K":4.02,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":13.0,
        "Available on the Hub":true,
        "Model Sha":"ad8892a17be1372f611203a4cf71560cc337e458"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Tess-XS-v1.0",
        "Average":58.95,
        "ARC":61.43,
        "HellaSwag":83.82,
        "MMLU":64.1,
        "TruthfulQA":47.12,
        "Winogrande":78.93,
        "GSM8K":18.27,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"a581ab1793366ff2d5f3c966ff0e7b8b1149d775"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Henk717\/chronoboros-33B",
        "Average":58.92,
        "ARC":63.91,
        "HellaSwag":85.0,
        "MMLU":59.44,
        "TruthfulQA":49.83,
        "Winogrande":80.35,
        "GSM8K":15.01,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":9.0,
        "Available on the Hub":true,
        "Model Sha":"a4deca117c5fa48f2cdc49ed2e2596046201d688"
    },
    {
        "T":"\u2b55",
        "Model":"akjindal53244\/Mistral-7B-v0.1-Open-Platypus",
        "Average":58.92,
        "ARC":62.37,
        "HellaSwag":85.08,
        "MMLU":63.79,
        "TruthfulQA":47.33,
        "Winogrande":77.66,
        "GSM8K":17.29,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"aa2c84e89c4c8a10e0569e45021b59e6d1c08bda"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"upstage\/llama-30b-instruct",
        "Average":58.91,
        "ARC":62.46,
        "HellaSwag":86.23,
        "MMLU":59.37,
        "TruthfulQA":52.78,
        "Winogrande":80.51,
        "GSM8K":12.13,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":21.0,
        "Available on the Hub":true,
        "Model Sha":"fea4312379557e8a1e8073965f560798de369edd"
    },
    {
        "T":"\u2b55",
        "Model":"mncai\/Mistral-7B-OpenOrca-1k",
        "Average":58.9,
        "ARC":62.97,
        "HellaSwag":84.66,
        "MMLU":62.2,
        "TruthfulQA":52.96,
        "Winogrande":78.61,
        "GSM8K":11.98,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"ae9e37811a54ffe45f41a572c7e68363aa11b062"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openaccess-ai-collective\/manticore-30b-chat-pyg-alpha",
        "Average":58.86,
        "ARC":64.16,
        "HellaSwag":84.38,
        "MMLU":57.49,
        "TruthfulQA":51.57,
        "Winogrande":79.48,
        "GSM8K":16.07,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":32.53,
        "Hub \u2764\ufe0f":13.0,
        "Available on the Hub":true,
        "Model Sha":"0cff8e9718e57202171003d556d2e6630061879d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-code-mistral-7b-v1.0",
        "Average":58.85,
        "ARC":60.58,
        "HellaSwag":83.75,
        "MMLU":62.98,
        "TruthfulQA":47.9,
        "Winogrande":78.69,
        "GSM8K":19.18,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"753852b8cb52dc5f0411568e98c0cb445a7835dc"
    },
    {
        "T":"\u2b55",
        "Model":"qblocks\/mistral_7b_norobots",
        "Average":58.85,
        "ARC":58.96,
        "HellaSwag":80.57,
        "MMLU":57.66,
        "TruthfulQA":41.91,
        "Winogrande":75.61,
        "GSM8K":38.36,
        "Type":"instruction-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"36dde2c5b08140d612042d1ae047dd7551b7e15b"
    },
    {
        "T":"?",
        "Model":"Henk717\/airochronos-33B",
        "Average":58.84,
        "ARC":64.42,
        "HellaSwag":85.21,
        "MMLU":59.79,
        "TruthfulQA":50.59,
        "Winogrande":79.32,
        "GSM8K":13.72,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":32.53,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"06843c6693cc265dabb464c818a3d3713239721a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NeverSleep\/Mistral-11B-SynthIAirOmniMix",
        "Average":58.84,
        "ARC":62.46,
        "HellaSwag":83.13,
        "MMLU":63.47,
        "TruthfulQA":55.69,
        "Winogrande":76.4,
        "GSM8K":11.9,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.73,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"19694dc88e74a018d54bac6070cf521dff6d4397"
    },
    {
        "T":"\u2b55",
        "Model":"PulsarAI\/Nebula-v2-7B",
        "Average":58.82,
        "ARC":58.7,
        "HellaSwag":83.06,
        "MMLU":57.61,
        "TruthfulQA":46.72,
        "Winogrande":75.14,
        "GSM8K":31.69,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"d2a5611f7d7c37bfa2270d1823bceef01c0be383"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ajibawa-2023\/scarlett-33b",
        "Average":58.81,
        "ARC":67.75,
        "HellaSwag":85.48,
        "MMLU":58.98,
        "TruthfulQA":61.05,
        "Winogrande":76.8,
        "GSM8K":2.81,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":19.0,
        "Available on the Hub":true,
        "Model Sha":"305eea72fb9fe2ac5929a62483ea51f152bcc060"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YeungNLP\/firefly-llama-30b",
        "Average":58.77,
        "ARC":64.25,
        "HellaSwag":83.64,
        "MMLU":58.23,
        "TruthfulQA":53.2,
        "Winogrande":77.43,
        "GSM8K":15.85,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"7f035eabd1d0e7b38ace395847a623f475d90da8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Henk717\/airochronos-33B",
        "Average":58.75,
        "ARC":64.25,
        "HellaSwag":85.2,
        "MMLU":59.83,
        "TruthfulQA":50.56,
        "Winogrande":79.08,
        "GSM8K":13.57,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"other",
        "#Params (B)":32.53,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"06843c6693cc265dabb464c818a3d3713239721a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-m-7b-3.1.2",
        "Average":58.75,
        "ARC":61.86,
        "HellaSwag":83.51,
        "MMLU":61.91,
        "TruthfulQA":53.75,
        "Winogrande":77.58,
        "GSM8K":13.87,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":20.0,
        "Available on the Hub":true,
        "Model Sha":"e9a7f0271fa442d65bf6be87feeb3f4de2f5760e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/dromedary-65b-lora-HF",
        "Average":58.73,
        "ARC":61.6,
        "HellaSwag":82.53,
        "MMLU":63.08,
        "TruthfulQA":38.82,
        "Winogrande":78.93,
        "GSM8K":27.45,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":65.02,
        "Hub \u2764\ufe0f":19.0,
        "Available on the Hub":true,
        "Model Sha":"3fa4546259d6bbd6b5d637484c325ab19181a73c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ausboss\/llama-30b-supercot",
        "Average":58.73,
        "ARC":64.85,
        "HellaSwag":85.08,
        "MMLU":56.56,
        "TruthfulQA":53.96,
        "Winogrande":80.03,
        "GSM8K":11.9,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":124.0,
        "Available on the Hub":true,
        "Model Sha":"dc9d81f454d286ea040c5cd45b058aecaa51c13e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openbmb\/UltraLM-13b-v2.0",
        "Average":58.72,
        "ARC":62.63,
        "HellaSwag":81.49,
        "MMLU":56.17,
        "TruthfulQA":49.48,
        "Winogrande":76.48,
        "GSM8K":26.08,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"a452045c96ae62379a98ef0d85666616a66e78a6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/CollectiveCognition-v1.1-Mistral-7B-dare-0.85",
        "Average":58.72,
        "ARC":61.01,
        "HellaSwag":84.31,
        "MMLU":64.34,
        "TruthfulQA":44.87,
        "Winogrande":78.85,
        "GSM8K":18.95,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"7ecfa4c5b100565bf8cfdfa7442e9772d28a9a23"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aeala\/GPT4-x-AlpacaDente2-30b",
        "Average":58.71,
        "ARC":60.58,
        "HellaSwag":81.81,
        "MMLU":56.63,
        "TruthfulQA":48.38,
        "Winogrande":78.14,
        "GSM8K":26.76,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":30.0,
        "Available on the Hub":true,
        "Model Sha":"9fe5a8dada738f44e7ee9293b2140ae0be021787"
    },
    {
        "T":"\u2b55",
        "Model":"bhenrym14\/mistral-7b-platypus-fp16",
        "Average":58.71,
        "ARC":63.05,
        "HellaSwag":84.15,
        "MMLU":64.11,
        "TruthfulQA":45.07,
        "Winogrande":78.53,
        "GSM8K":17.36,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"d836a261afa0871d3734a7dfd1a28dc23c173ea7"
    },
    {
        "T":"\u2b55",
        "Model":"Weyaxi\/Dolphin-Nebula-7B",
        "Average":58.69,
        "ARC":55.2,
        "HellaSwag":78.57,
        "MMLU":53.44,
        "TruthfulQA":57.97,
        "Winogrande":73.88,
        "GSM8K":33.06,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"c14b3545066e5ee5562c1724a037b41db95f1f0d"
    },
    {
        "T":"\u2b55",
        "Model":"PeanutJar\/Mistral-v0.1-PeanutButter-v0.0.2-7B",
        "Average":58.66,
        "ARC":61.77,
        "HellaSwag":84.11,
        "MMLU":64.38,
        "TruthfulQA":45.92,
        "Winogrande":78.37,
        "GSM8K":17.44,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f4d471d7a9447d0969a58d5b3146d50cfa3005b3"
    },
    {
        "T":"?",
        "Model":"PeanutJar\/Mistral-v0.1-PeanutButter-v0.0.5-DPO-7B-QLoRA",
        "Average":58.65,
        "ARC":61.26,
        "HellaSwag":84.52,
        "MMLU":63.63,
        "TruthfulQA":45.75,
        "Winogrande":78.61,
        "GSM8K":18.12,
        "Type":"",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"56e805fbebffaf25e61df5a3d68b75cb604a6e1c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"microsoft\/Orca-2-13b",
        "Average":58.64,
        "ARC":60.67,
        "HellaSwag":79.81,
        "MMLU":60.37,
        "TruthfulQA":56.41,
        "Winogrande":76.64,
        "GSM8K":17.97,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":138.0,
        "Available on the Hub":true,
        "Model Sha":"2539ff53e6baa4cc603774ad5a2d646f4041ea4e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dfurman\/falcon-40b-openassistant-peft",
        "Average":58.63,
        "ARC":62.63,
        "HellaSwag":85.59,
        "MMLU":57.77,
        "TruthfulQA":51.02,
        "Winogrande":81.45,
        "GSM8K":13.34,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":40.0,
        "Hub \u2764\ufe0f":38.0,
        "Available on the Hub":false,
        "Model Sha":"3d5084b6fbcb9f9f36493d9fd1e3795b0b9860f0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"quantumaikr\/QuantumLM-70B-hf",
        "Average":58.61,
        "ARC":59.47,
        "HellaSwag":83.02,
        "MMLU":62.25,
        "TruthfulQA":53.39,
        "Winogrande":78.77,
        "GSM8K":14.78,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":68.98,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"e13dd23ae5e611e959b6c8d5bc47bf4fd37cd9d7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CobraMamba\/mamba-gpt-7b-v1",
        "Average":58.61,
        "ARC":61.26,
        "HellaSwag":84.1,
        "MMLU":63.46,
        "TruthfulQA":46.34,
        "Winogrande":79.16,
        "GSM8K":17.36,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"e64d658b397748e409d9633fd24fc5a6df429600"
    },
    {
        "T":"\u2b55",
        "Model":"maywell\/koOpenChat-sft",
        "Average":58.61,
        "ARC":59.81,
        "HellaSwag":78.73,
        "MMLU":61.32,
        "TruthfulQA":51.24,
        "Winogrande":76.4,
        "GSM8K":24.18,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"47472b36e181694422564b130ee075ffa596537d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gradientputri\/MegaMix-T1-13B",
        "Average":58.61,
        "ARC":61.35,
        "HellaSwag":83.44,
        "MMLU":58.49,
        "TruthfulQA":48.19,
        "Winogrande":76.09,
        "GSM8K":24.11,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"55d31300f8972b56320855bb40efb5e3d1e1a6fc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/dolphin-2.0-mistral-7b",
        "Average":58.58,
        "ARC":59.22,
        "HellaSwag":80.26,
        "MMLU":56.9,
        "TruthfulQA":61.09,
        "Winogrande":75.37,
        "GSM8K":18.65,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":58.0,
        "Available on the Hub":true,
        "Model Sha":"c673387016c622fd0a707426953c03957398bc37"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"S4sch\/zephyr-neural-chat-frankenmerge11b",
        "Average":58.57,
        "ARC":61.52,
        "HellaSwag":84.09,
        "MMLU":61.51,
        "TruthfulQA":60.63,
        "Winogrande":76.24,
        "GSM8K":7.43,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.39,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"f915831e904e0dcda760873aa16a35daf5ac9e6d"
    },
    {
        "T":"?",
        "Model":"lmsys\/vicuna-33b-v1.3",
        "Average":58.54,
        "ARC":62.12,
        "HellaSwag":83.0,
        "MMLU":59.22,
        "TruthfulQA":56.16,
        "Winogrande":77.03,
        "GSM8K":13.72,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":230.0,
        "Available on the Hub":true,
        "Model Sha":"ef8d6becf883fb3ce52e3706885f761819477ab4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gradientputri\/MegaMix-A1-13B",
        "Average":58.52,
        "ARC":61.6,
        "HellaSwag":83.49,
        "MMLU":58.26,
        "TruthfulQA":47.48,
        "Winogrande":76.16,
        "GSM8K":24.11,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"14e0756c210bcf420fbf825e6b8087ee5c716e7f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/MLewd-ReMM-L2-Chat-20B",
        "Average":58.49,
        "ARC":62.46,
        "HellaSwag":85.62,
        "MMLU":59.13,
        "TruthfulQA":55.63,
        "Winogrande":77.19,
        "GSM8K":10.92,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":19.99,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"cda06630a1d8173541431e5ce8bc17dcfaa37e5e"
    },
    {
        "T":"?",
        "Model":"TheBloke\/Wizard-Vicuna-30B-Uncensored-GPTQ",
        "Average":58.47,
        "ARC":61.09,
        "HellaSwag":82.4,
        "MMLU":56.46,
        "TruthfulQA":49.9,
        "Winogrande":77.66,
        "GSM8K":23.28,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":35.58,
        "Hub \u2764\ufe0f":380.0,
        "Available on the Hub":true,
        "Model Sha":"56a82ece7a9309189561a590e8f4d2fe0d4be92b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Intel\/neural-chat-7b-v3",
        "Average":58.46,
        "ARC":67.15,
        "HellaSwag":83.29,
        "MMLU":62.26,
        "TruthfulQA":58.77,
        "Winogrande":78.06,
        "GSM8K":1.21,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"7a05c8a2151f7d32252d9ef5db10445c13ae1f20"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"posicube\/Llama2-chat-AYB-13B",
        "Average":58.45,
        "ARC":63.4,
        "HellaSwag":84.79,
        "MMLU":59.34,
        "TruthfulQA":55.62,
        "Winogrande":76.24,
        "GSM8K":11.3,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"cc7ca1b8f906b9f62ace094540f4ff4124dd581a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aspik101\/trurl-2-13b-pl-instruct_unload",
        "Average":58.44,
        "ARC":59.9,
        "HellaSwag":79.99,
        "MMLU":78.66,
        "TruthfulQA":45.56,
        "Winogrande":74.35,
        "GSM8K":12.21,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"17f57642165e30a4025d6817bd47dcd80d0c5c4d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CalderaAI\/30B-Lazarus",
        "Average":58.4,
        "ARC":64.93,
        "HellaSwag":84.27,
        "MMLU":56.47,
        "TruthfulQA":58.65,
        "Winogrande":78.37,
        "GSM8K":7.73,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":116.0,
        "Available on the Hub":true,
        "Model Sha":"24da9e88f2b2b7946bc6fe9412d6728b9adc2c3d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PocketDoc\/Dans-PersonalityEngine-30b",
        "Average":58.39,
        "ARC":63.48,
        "HellaSwag":84.37,
        "MMLU":58.99,
        "TruthfulQA":46.98,
        "Winogrande":80.98,
        "GSM8K":15.54,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"1990b46a2e2ac1f6282d961bce691ceceafed514"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/SynthIA-7B-v1.3-dare-0.85",
        "Average":58.38,
        "ARC":61.01,
        "HellaSwag":83.5,
        "MMLU":64.49,
        "TruthfulQA":43.77,
        "Winogrande":78.93,
        "GSM8K":18.57,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"91381d0ac625dcde542428ed6cb35177b4260923"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ajibawa-2023\/Uncensored-Frank-33B",
        "Average":58.38,
        "ARC":62.12,
        "HellaSwag":83.3,
        "MMLU":57.57,
        "TruthfulQA":54.03,
        "Winogrande":76.56,
        "GSM8K":16.68,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"1c1f4e9256ac2be145a9106863ee9f2e9d701e74"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"posicube\/Llama-chat-AY-13B",
        "Average":58.34,
        "ARC":62.8,
        "HellaSwag":83.23,
        "MMLU":60.01,
        "TruthfulQA":55.95,
        "Winogrande":75.93,
        "GSM8K":12.13,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"66037b5ee553f7b878d796d2b2d5ada5734cc164"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/SynthIA-v1.3-Nebula-v2-7B",
        "Average":58.33,
        "ARC":59.39,
        "HellaSwag":82.77,
        "MMLU":57.57,
        "TruthfulQA":50.62,
        "Winogrande":74.74,
        "GSM8K":24.87,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"c6030620e9d4390d54ec221a18ff3e530f4dcd84"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CobraMamba\/mamba-gpt-7b-v2",
        "Average":58.31,
        "ARC":61.95,
        "HellaSwag":83.83,
        "MMLU":61.74,
        "TruthfulQA":46.63,
        "Winogrande":78.45,
        "GSM8K":17.29,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"6439444e2c0b61253d3e61ae04fe0436717acc2f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aspik101\/30B-Lazarus-instruct-PL-lora_unload",
        "Average":58.29,
        "ARC":62.8,
        "HellaSwag":84.13,
        "MMLU":56.87,
        "TruthfulQA":55.49,
        "Winogrande":79.08,
        "GSM8K":11.37,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"eeb29b35ceb6dd5c532f1e4e1235f1cdd3f51f23"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"concedo\/Vicuzard-30B-Uncensored",
        "Average":58.26,
        "ARC":62.97,
        "HellaSwag":83.68,
        "MMLU":58.16,
        "TruthfulQA":52.27,
        "Winogrande":77.11,
        "GSM8K":15.39,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":11.0,
        "Available on the Hub":true,
        "Model Sha":"e2329c05a6e59660ba3cbcc01adf30a78f852594"
    },
    {
        "T":"\u2b55",
        "Model":"PeanutJar\/Mistral-v0.1-PeanutButter-v0.0.5-SFT-7B-QLoRA",
        "Average":58.24,
        "ARC":60.75,
        "HellaSwag":84.24,
        "MMLU":63.66,
        "TruthfulQA":44.94,
        "Winogrande":78.69,
        "GSM8K":17.13,
        "Type":"instruction-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"2609363766acf308877a71aba352e60d7c044b49"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-33b-gpt4-1.4",
        "Average":58.2,
        "ARC":64.42,
        "HellaSwag":85.13,
        "MMLU":59.53,
        "TruthfulQA":50.47,
        "Winogrande":77.9,
        "GSM8K":11.75,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":13.0,
        "Available on the Hub":true,
        "Model Sha":"04e1e194247a95cc60ba3cd70d026bc94c1f1764"
    },
    {
        "T":"\u2b55",
        "Model":"lgaalves\/mistral-7b-platypus1k",
        "Average":58.19,
        "ARC":61.6,
        "HellaSwag":82.93,
        "MMLU":63.16,
        "TruthfulQA":46.96,
        "Winogrande":78.14,
        "GSM8K":16.38,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"c34c4a249ecf0cc391beba142a1f9cb23154fcd1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Riiid\/sheep-duck-llama-2-13b",
        "Average":58.19,
        "ARC":63.14,
        "HellaSwag":84.52,
        "MMLU":59.89,
        "TruthfulQA":55.48,
        "Winogrande":76.95,
        "GSM8K":9.17,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"71edf22c49677d0239caf5f87d8139dd9cc79078"
    },
    {
        "T":"\u2b55",
        "Model":"Secbone\/llama-33B-instructed",
        "Average":58.18,
        "ARC":64.59,
        "HellaSwag":86.17,
        "MMLU":60.5,
        "TruthfulQA":44.12,
        "Winogrande":79.32,
        "GSM8K":14.4,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"gpl-3.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"7c40caaea4fe3264fd469dac428b0f9450e574a6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"42MARU\/sitebunny-13b",
        "Average":58.17,
        "ARC":63.14,
        "HellaSwag":83.64,
        "MMLU":59.91,
        "TruthfulQA":56.21,
        "Winogrande":76.72,
        "GSM8K":9.4,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"67107327d09c2f9bf3e4b316d97767c97f5a0804"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PocketDoc\/Dans-TotSirocco-7b",
        "Average":58.16,
        "ARC":62.2,
        "HellaSwag":84.28,
        "MMLU":63.8,
        "TruthfulQA":46.04,
        "Winogrande":79.48,
        "GSM8K":13.19,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"8bit",
        "Hub License":"?",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"824e3a4738818142374721306ce85b83770de24b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PocketDoc\/Dans-TotSirocco-7b",
        "Average":58.15,
        "ARC":62.03,
        "HellaSwag":84.23,
        "MMLU":64.19,
        "TruthfulQA":46.49,
        "Winogrande":78.69,
        "GSM8K":13.27,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"824e3a4738818142374721306ce85b83770de24b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/Mistral-7B-OpenOrca-lora",
        "Average":58.14,
        "ARC":61.95,
        "HellaSwag":83.62,
        "MMLU":64.16,
        "TruthfulQA":42.74,
        "Winogrande":79.08,
        "GSM8K":17.29,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"605dc043063cb9589c06883d839122920ed1eca5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"caisarl76\/Mistral-7B-guanaco1k-ep2",
        "Average":58.13,
        "ARC":60.07,
        "HellaSwag":82.76,
        "MMLU":61.5,
        "TruthfulQA":54.4,
        "Winogrande":78.06,
        "GSM8K":11.98,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"9c9f31f213b69da7797c2c0630c17cf8f785fc13"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"caisarl76\/mistral-guanaco1k-ep2",
        "Average":58.13,
        "ARC":60.07,
        "HellaSwag":82.76,
        "MMLU":61.5,
        "TruthfulQA":54.4,
        "Winogrande":78.06,
        "GSM8K":11.98,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"9c9f31f213b69da7797c2c0630c17cf8f785fc13"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Stheno-1.8-L2-13B",
        "Average":58.12,
        "ARC":63.48,
        "HellaSwag":84.12,
        "MMLU":58.57,
        "TruthfulQA":52.86,
        "Winogrande":76.4,
        "GSM8K":13.27,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"fe054ab749a69375285df40913a88bd40f1e2bf6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PulsarAI\/2x-LoRA-Assemble-13B",
        "Average":58.1,
        "ARC":63.65,
        "HellaSwag":83.47,
        "MMLU":59.82,
        "TruthfulQA":55.94,
        "Winogrande":76.48,
        "GSM8K":9.25,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"1aca45d37eade21eb381aaefc9245b58ec3b7b26"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-llama2-13b",
        "Average":58.1,
        "ARC":62.03,
        "HellaSwag":81.85,
        "MMLU":58.52,
        "TruthfulQA":55.7,
        "Winogrande":76.56,
        "GSM8K":13.95,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"c6362c4fc0dc03420e3c08454b2e7689e4e32d3a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aspik101\/Vicuzard-30B-Uncensored-instruct-PL-lora_unload",
        "Average":58.09,
        "ARC":62.46,
        "HellaSwag":83.66,
        "MMLU":57.82,
        "TruthfulQA":50.94,
        "Winogrande":78.37,
        "GSM8K":15.31,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"652f03ac67b4293198d98b618e64285fb32a28e9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bofenghuang\/vigogne-33b-instruct",
        "Average":58.08,
        "ARC":63.05,
        "HellaSwag":85.0,
        "MMLU":58.32,
        "TruthfulQA":52.1,
        "Winogrande":78.85,
        "GSM8K":11.14,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"openrail",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"9c2b558b888e0ef8b4a72e0771db72a06a5c8474"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"tiiuae\/falcon-40b",
        "Average":58.07,
        "ARC":61.86,
        "HellaSwag":85.28,
        "MMLU":56.89,
        "TruthfulQA":41.65,
        "Winogrande":81.29,
        "GSM8K":21.46,
        "Type":"pretrained",
        "Architecture":"RWForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":41.3,
        "Hub \u2764\ufe0f":1099.0,
        "Available on the Hub":true,
        "Model Sha":"3d7c5902f1dc9da830979a826cd96114b3ba4ec1"
    },
    {
        "T":"\u2b55",
        "Model":"mncai\/Mistral-7B-openplatypus-1k",
        "Average":58.07,
        "ARC":60.15,
        "HellaSwag":84.25,
        "MMLU":59.84,
        "TruthfulQA":49.86,
        "Winogrande":76.87,
        "GSM8K":17.44,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"dad401175da3782475a122008720ddc3338e2632"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"camel-ai\/CAMEL-33B-Combined-Data",
        "Average":58.06,
        "ARC":62.97,
        "HellaSwag":83.83,
        "MMLU":58.98,
        "TruthfulQA":50.21,
        "Winogrande":78.3,
        "GSM8K":14.1,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"62c74e7531625c1383bbbdc7c8346a996e9d1e21"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openaccess-ai-collective\/hippogriff-30b-chat",
        "Average":58.05,
        "ARC":64.51,
        "HellaSwag":85.2,
        "MMLU":59.09,
        "TruthfulQA":48.42,
        "Winogrande":80.82,
        "GSM8K":10.24,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":21.0,
        "Available on the Hub":true,
        "Model Sha":"64c10edf5312cd13704925b07413882d9e94c7a0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/airoboros-m-7b-3.1.2-dare-0.85",
        "Average":58.03,
        "ARC":61.09,
        "HellaSwag":83.57,
        "MMLU":64.05,
        "TruthfulQA":43.64,
        "Winogrande":78.37,
        "GSM8K":17.44,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"b5bc02f4e1008bd3a72046a93ac2f4dd4bef02da"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"sequelbox\/DaringFortitude",
        "Average":58.01,
        "ARC":63.48,
        "HellaSwag":83.56,
        "MMLU":59.81,
        "TruthfulQA":55.96,
        "Winogrande":76.48,
        "GSM8K":8.79,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"0c463888cd83b7acebd7b6fb961562e11402e47d"
    },
    {
        "T":"\u2b55",
        "Model":"Weyaxi\/Luban-Marcoroni-13B",
        "Average":57.98,
        "ARC":63.65,
        "HellaSwag":82.92,
        "MMLU":58.7,
        "TruthfulQA":55.55,
        "Winogrande":77.03,
        "GSM8K":10.01,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"bf152c36935acd67a9029c017f0c1ff2d7a92314"
    },
    {
        "T":"\u2b55",
        "Model":"ehartford\/samantha-mistral-7b",
        "Average":57.96,
        "ARC":63.4,
        "HellaSwag":84.1,
        "MMLU":61.36,
        "TruthfulQA":46.08,
        "Winogrande":76.8,
        "GSM8K":16.0,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":17.0,
        "Available on the Hub":true,
        "Model Sha":"7f9e40543fdff8c3e58eca0390c8a631829c1206"
    },
    {
        "T":"\u2b55",
        "Model":"Weyaxi\/Luban-Marcoroni-13B-v3",
        "Average":57.94,
        "ARC":63.74,
        "HellaSwag":82.88,
        "MMLU":58.64,
        "TruthfulQA":55.56,
        "Winogrande":76.87,
        "GSM8K":9.93,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"9b68680ed8351ef8ef6948169e69a888af40002e"
    },
    {
        "T":"\u2b55",
        "Model":"Weyaxi\/Luban-Marcoroni-13B-v2",
        "Average":57.92,
        "ARC":63.48,
        "HellaSwag":82.89,
        "MMLU":58.72,
        "TruthfulQA":55.56,
        "Winogrande":76.95,
        "GSM8K":9.93,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"d7c704a08218dcc03963bc08e9113e281c056f53"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"caisarl76\/Mistral-7B-OpenOrca-Guanaco-accu16",
        "Average":57.91,
        "ARC":59.73,
        "HellaSwag":83.08,
        "MMLU":61.29,
        "TruthfulQA":50.81,
        "Winogrande":76.56,
        "GSM8K":16.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"e83b8c1887c45473961a4ff36ae202ada1ca3d42"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"oh-yeontaek\/llama-2-13B-LoRA-assemble",
        "Average":57.91,
        "ARC":63.57,
        "HellaSwag":83.51,
        "MMLU":59.82,
        "TruthfulQA":55.96,
        "Winogrande":76.16,
        "GSM8K":8.42,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"85bb49d333dba4a08b051418663d16853ce30cee"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aeala\/Enterredaas-33b",
        "Average":57.9,
        "ARC":60.92,
        "HellaSwag":84.18,
        "MMLU":58.3,
        "TruthfulQA":49.02,
        "Winogrande":78.77,
        "GSM8K":16.22,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"d72dc1f05eaf1beb6373fd53fd22eb90f293a5c4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/Wizard-Vicuna-30B-Uncensored",
        "Average":57.89,
        "ARC":62.12,
        "HellaSwag":83.45,
        "MMLU":58.24,
        "TruthfulQA":50.81,
        "Winogrande":78.45,
        "GSM8K":14.25,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":72.0,
        "Available on the Hub":true,
        "Model Sha":"6374baef4cedd41f85c111b8eec3eb38ee24c4b9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Wizard-Vicuna-30B-Uncensored-fp16",
        "Average":57.89,
        "ARC":62.12,
        "HellaSwag":83.45,
        "MMLU":58.24,
        "TruthfulQA":50.81,
        "Winogrande":78.45,
        "GSM8K":14.25,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":16.0,
        "Available on the Hub":true,
        "Model Sha":"c7b7cecb5a314fc66deebabcb67c230a3fbe84f7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"posicube\/Llama2-chat-AYT-13B",
        "Average":57.88,
        "ARC":63.31,
        "HellaSwag":83.53,
        "MMLU":59.67,
        "TruthfulQA":55.8,
        "Winogrande":76.09,
        "GSM8K":8.87,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":12.0,
        "Available on the Hub":true,
        "Model Sha":"dd12dced8076a959c03b8b5c4a4266f234d6639a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aeala\/VicUnlocked-alpaca-30b",
        "Average":57.86,
        "ARC":61.86,
        "HellaSwag":83.79,
        "MMLU":57.64,
        "TruthfulQA":51.03,
        "Winogrande":78.22,
        "GSM8K":14.63,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"c63d117d1ec5794766dd6dc5e1469769df8aba1d"
    },
    {
        "T":"\u2b55",
        "Model":"PulsarAI\/Chat-AYB-Nova-13B",
        "Average":57.84,
        "ARC":62.97,
        "HellaSwag":84.28,
        "MMLU":58.58,
        "TruthfulQA":51.28,
        "Winogrande":77.58,
        "GSM8K":12.36,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"942af4d59533af09cf9ba13d1e369b8e871a0a4b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"souvik0306\/mistral_7b_2epoch_norobots",
        "Average":57.84,
        "ARC":61.01,
        "HellaSwag":83.37,
        "MMLU":63.96,
        "TruthfulQA":42.62,
        "Winogrande":79.08,
        "GSM8K":16.98,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"624be22cfde6797a100230ec9dc1421f52eb0aa2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Stheno-V2-Delta-fp16",
        "Average":57.81,
        "ARC":62.46,
        "HellaSwag":83.45,
        "MMLU":59.04,
        "TruthfulQA":55.25,
        "Winogrande":73.88,
        "GSM8K":12.81,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"3979769be8d92aa2dd0c7aebf385635863f16dd9"
    },
    {
        "T":"?",
        "Model":"Sao10K\/Stheno-v2-Delta-fp16",
        "Average":57.81,
        "ARC":62.46,
        "HellaSwag":83.45,
        "MMLU":59.04,
        "TruthfulQA":55.25,
        "Winogrande":73.88,
        "GSM8K":12.81,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"3979769be8d92aa2dd0c7aebf385635863f16dd9"
    },
    {
        "T":"\u2b55",
        "Model":"Weyaxi\/ChatAYT-Lora-Assamble-Marcoroni",
        "Average":57.76,
        "ARC":62.46,
        "HellaSwag":83.05,
        "MMLU":58.72,
        "TruthfulQA":56.12,
        "Winogrande":77.35,
        "GSM8K":8.87,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"51c9b600023cd26c4eb3754b9a89c60dde959ccc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-llama2-13b-v8.1-fp16",
        "Average":57.76,
        "ARC":55.97,
        "HellaSwag":79.79,
        "MMLU":54.95,
        "TruthfulQA":51.16,
        "Winogrande":74.35,
        "GSM8K":30.33,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.88,
        "Hub \u2764\ufe0f":62.0,
        "Available on the Hub":true,
        "Model Sha":"b51c6b29abdf7c420cb5e5f4f309ff83179c7bb8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Open-Orca\/OpenOrcaxOpenChat-Preview2-13B",
        "Average":57.76,
        "ARC":62.37,
        "HellaSwag":82.96,
        "MMLU":58.68,
        "TruthfulQA":51.23,
        "Winogrande":77.19,
        "GSM8K":14.1,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":94.0,
        "Available on the Hub":true,
        "Model Sha":"26d1bc5c54c1f60a5de0b1ed4d0b16f285aee230"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/MLewd-L2-Chat-13B",
        "Average":57.75,
        "ARC":62.03,
        "HellaSwag":84.19,
        "MMLU":58.75,
        "TruthfulQA":52.84,
        "Winogrande":77.43,
        "GSM8K":11.3,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":13.0,
        "Available on the Hub":true,
        "Model Sha":"6c66622a99c1bc73498aa6a15a59da825d875310"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JosephusCheung\/Pwen-14B-Chat-20_30",
        "Average":57.74,
        "ARC":56.14,
        "HellaSwag":79.78,
        "MMLU":60.01,
        "TruthfulQA":47.02,
        "Winogrande":76.48,
        "GSM8K":26.99,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":14.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"e878e1f1f7b533c32beb8e06ebcf0cfa23f3fe9b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ai-business\/Luban-13B",
        "Average":57.73,
        "ARC":63.05,
        "HellaSwag":82.8,
        "MMLU":58.73,
        "TruthfulQA":55.53,
        "Winogrande":76.56,
        "GSM8K":9.7,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":13.0,
        "Available on the Hub":true,
        "Model Sha":"01b0f2046083dd8d9d8f9e626d78d83eaa1d57dd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-33b-gpt4-1.2",
        "Average":57.69,
        "ARC":64.42,
        "HellaSwag":84.93,
        "MMLU":60.35,
        "TruthfulQA":49.18,
        "Winogrande":77.51,
        "GSM8K":9.78,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"b3254a827fb1dfe0d4e428bf5ab1c3a2bac82d68"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"digitous\/Alpacino30b",
        "Average":57.67,
        "ARC":62.71,
        "HellaSwag":85.04,
        "MMLU":58.48,
        "TruthfulQA":44.23,
        "Winogrande":79.79,
        "GSM8K":15.77,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":67.0,
        "Available on the Hub":true,
        "Model Sha":"300bc5f3dc129a3d17adf059394e381eff7fbd55"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"boomerchan\/magpie-13b",
        "Average":57.64,
        "ARC":63.31,
        "HellaSwag":84.25,
        "MMLU":58.15,
        "TruthfulQA":49.15,
        "Winogrande":76.48,
        "GSM8K":14.48,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"a58124cdc9f39ccd59d4290a8bdfda93ff3690dc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Orca-2-13B-GPTQ",
        "Average":57.6,
        "ARC":59.81,
        "HellaSwag":79.12,
        "MMLU":59.35,
        "TruthfulQA":55.14,
        "Winogrande":76.64,
        "GSM8K":15.54,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"GPTQ",
        "Hub License":"other",
        "#Params (B)":16.24,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"2fc627e11b197c7d563eeea9c4338c2adc8e2c93"
    },
    {
        "T":"?",
        "Model":"l3utterfly\/mistral-7b-v0.1-layla-v1",
        "Average":57.56,
        "ARC":60.15,
        "HellaSwag":83.25,
        "MMLU":60.31,
        "TruthfulQA":48.9,
        "Winogrande":75.93,
        "GSM8K":16.83,
        "Type":"",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"5f06add6aa1d51d78288dbdcbd1abfd5f0ed0c84"
    },
    {
        "T":"?",
        "Model":"TheBloke\/tulu-30B-fp16",
        "Average":57.53,
        "ARC":59.98,
        "HellaSwag":83.4,
        "MMLU":56.1,
        "TruthfulQA":45.14,
        "Winogrande":80.82,
        "GSM8K":19.71,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"37c3655676c37662f60c68dacfce3f0e861be846"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-llama2-hermes-orca-platypus-wizardlm-13b",
        "Average":57.52,
        "ARC":59.64,
        "HellaSwag":82.7,
        "MMLU":58.3,
        "TruthfulQA":56.0,
        "Winogrande":75.37,
        "GSM8K":13.12,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":24.0,
        "Available on the Hub":true,
        "Model Sha":"4410d8a20871927e9fe981c01bc8314b451b2fcd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-33b-gpt4-1.3",
        "Average":57.49,
        "ARC":63.82,
        "HellaSwag":85.09,
        "MMLU":58.94,
        "TruthfulQA":45.33,
        "Winogrande":79.01,
        "GSM8K":12.74,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"f94e5249d2b998933466d42e08fa9551e3238205"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/Unholy-v1-12L-13B",
        "Average":57.47,
        "ARC":63.57,
        "HellaSwag":83.75,
        "MMLU":58.08,
        "TruthfulQA":51.09,
        "Winogrande":77.27,
        "GSM8K":11.07,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":25.0,
        "Available on the Hub":true,
        "Model Sha":"ee25c078f08b0812d82597afa3f5e877c19a5c83"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PocketDoc\/Dans-AdventurousWinds-7b",
        "Average":57.46,
        "ARC":61.01,
        "HellaSwag":83.47,
        "MMLU":63.69,
        "TruthfulQA":42.65,
        "Winogrande":78.22,
        "GSM8K":15.69,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"ddc7e4fcbbb5c666a3fe1bbe4a47b4477151b699"
    },
    {
        "T":"\u2b55",
        "Model":"jondurbin\/airoboros-33b-gpt4-1.3",
        "Average":57.43,
        "ARC":63.91,
        "HellaSwag":85.04,
        "MMLU":58.53,
        "TruthfulQA":45.36,
        "Winogrande":78.69,
        "GSM8K":13.04,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"f94e5249d2b998933466d42e08fa9551e3238205"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/MXLewd-L2-20B",
        "Average":57.43,
        "ARC":63.23,
        "HellaSwag":85.33,
        "MMLU":57.36,
        "TruthfulQA":51.65,
        "Winogrande":76.09,
        "GSM8K":10.92,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":19.99,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"ac279478abd9ddb8d1f5adcc548be0287b963adf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-llama2-luban-orca-platypus-13b",
        "Average":57.42,
        "ARC":62.54,
        "HellaSwag":82.76,
        "MMLU":59.23,
        "TruthfulQA":54.66,
        "Winogrande":77.11,
        "GSM8K":8.19,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"908cfb670611875b52045c4bab81cff53f0279a7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SkunkworksAI\/Mistralic-7B-1",
        "Average":57.4,
        "ARC":60.84,
        "HellaSwag":82.29,
        "MMLU":60.8,
        "TruthfulQA":52.38,
        "Winogrande":77.03,
        "GSM8K":11.07,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":17.0,
        "Available on the Hub":true,
        "Model Sha":"ebf138de4fb7a57f0d187ad0ab43abd6b35bfb62"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"chargoddard\/llama-polyglot-13b",
        "Average":57.36,
        "ARC":59.81,
        "HellaSwag":81.27,
        "MMLU":55.04,
        "TruthfulQA":48.71,
        "Winogrande":76.72,
        "GSM8K":22.59,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"7a08a96118aa86e0405a5f980d7e40dadf86e1be"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"allenai\/digital-socrates-13b",
        "Average":57.34,
        "ARC":58.36,
        "HellaSwag":80.14,
        "MMLU":57.01,
        "TruthfulQA":44.47,
        "Winogrande":74.59,
        "GSM8K":29.49,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"c738ee4bb61e67eebb9d196c440dcb2d99e5f906"
    },
    {
        "T":"?",
        "Model":"TheBloke\/VicUnlocked-30B-LoRA-HF",
        "Average":57.33,
        "ARC":59.73,
        "HellaSwag":84.02,
        "MMLU":57.81,
        "TruthfulQA":48.54,
        "Winogrande":79.48,
        "GSM8K":14.4,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"3259cb3c2a10cfb429fb51c4a76fffa049f4c44d"
    },
    {
        "T":"?",
        "Model":"jondurbin\/airoboros-33b-gpt4",
        "Average":57.32,
        "ARC":63.74,
        "HellaSwag":84.87,
        "MMLU":58.54,
        "TruthfulQA":47.06,
        "Winogrande":77.03,
        "GSM8K":12.66,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"5b6bd680b1c008e52521dc8c663dbc87820da3d0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/BrainDerp2",
        "Average":57.32,
        "ARC":60.92,
        "HellaSwag":81.94,
        "MMLU":58.9,
        "TruthfulQA":57.19,
        "Winogrande":75.93,
        "GSM8K":9.02,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"948ee7af94a8b092807df4becfc0a8c1cd042878"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/OpenOrca-Platypus2-13B-QLoRA-0.80-epoch",
        "Average":57.31,
        "ARC":62.37,
        "HellaSwag":82.99,
        "MMLU":59.38,
        "TruthfulQA":52.2,
        "Winogrande":75.77,
        "GSM8K":11.14,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"39ae03b77b4f1d453b02468ce6bb4ddeb6526b77"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"adonlee\/LLaMA_2_13B_SFT_v0",
        "Average":57.31,
        "ARC":62.03,
        "HellaSwag":83.8,
        "MMLU":58.39,
        "TruthfulQA":49.92,
        "Winogrande":77.27,
        "GSM8K":12.43,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"a6790d83337578f38d2bcd51038a779eaa8d0fac"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Open-Orca\/OpenOrca-Platypus2-13B",
        "Average":57.28,
        "ARC":62.8,
        "HellaSwag":83.15,
        "MMLU":59.39,
        "TruthfulQA":53.08,
        "Winogrande":76.24,
        "GSM8K":9.02,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":204.0,
        "Available on the Hub":true,
        "Model Sha":"e7a40134f7eb687c6ab66d445dc7251257f8d391"
    },
    {
        "T":"\u2b55",
        "Model":"PulsarAI\/2x-LoRA-Assemble-Nova-13B",
        "Average":57.26,
        "ARC":62.63,
        "HellaSwag":83.24,
        "MMLU":58.64,
        "TruthfulQA":51.88,
        "Winogrande":76.95,
        "GSM8K":10.24,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"2a344b91b28ce4d0bd48b9b5a6cc87b71123eab5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/MLewd-ReMM-L2-Chat-20B-Inverted",
        "Average":57.25,
        "ARC":61.69,
        "HellaSwag":85.32,
        "MMLU":58.0,
        "TruthfulQA":53.77,
        "Winogrande":75.61,
        "GSM8K":9.1,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":19.99,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"b5b501b4d23ec7ab24b827f79e48b2c67e548ddb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/orca_mini_v3_13b",
        "Average":57.24,
        "ARC":63.14,
        "HellaSwag":82.35,
        "MMLU":56.52,
        "TruthfulQA":51.81,
        "Winogrande":76.48,
        "GSM8K":13.12,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":27.0,
        "Available on the Hub":true,
        "Model Sha":"99904e4119575f2c1606ca1e31d288f38a9f20b5"
    },
    {
        "T":"\u2b55",
        "Model":"pankajmathur\/orca_mini_v3_13b",
        "Average":57.24,
        "ARC":63.14,
        "HellaSwag":82.35,
        "MMLU":56.52,
        "TruthfulQA":51.81,
        "Winogrande":76.48,
        "GSM8K":13.12,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":27.0,
        "Available on the Hub":true,
        "Model Sha":"72eec98f68d240a71d3da8a266917b6e754ae831"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/MLewd-Chat-v2-13B",
        "Average":57.23,
        "ARC":61.86,
        "HellaSwag":83.81,
        "MMLU":57.0,
        "TruthfulQA":54.51,
        "Winogrande":75.77,
        "GSM8K":10.46,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"f6181961a6a2f9ca534e1a8907b4a4459be6b6bd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"IkariDev\/Athena-v4",
        "Average":57.23,
        "ARC":62.54,
        "HellaSwag":84.19,
        "MMLU":57.33,
        "TruthfulQA":50.87,
        "Winogrande":76.48,
        "GSM8K":11.98,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":11.0,
        "Available on the Hub":true,
        "Model Sha":"dde640538a44a08f6f456a2b7634e31a5d7a1245"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-llama2-hermes-orca-platypus-13b",
        "Average":57.17,
        "ARC":60.92,
        "HellaSwag":83.5,
        "MMLU":59.39,
        "TruthfulQA":54.29,
        "Winogrande":75.22,
        "GSM8K":9.7,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f227ad33b16726b099e35e5dc47f4db1f22665a7"
    },
    {
        "T":"\u2b55",
        "Model":"jondurbin\/airoboros-33b-2.1",
        "Average":57.16,
        "ARC":63.65,
        "HellaSwag":84.97,
        "MMLU":57.37,
        "TruthfulQA":52.17,
        "Winogrande":78.22,
        "GSM8K":6.6,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"12ccd0e6c9ef12c7d3c2eab8266cd32c0b2f7683"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-33b-gpt4-m2.0",
        "Average":57.16,
        "ARC":64.68,
        "HellaSwag":84.95,
        "MMLU":57.77,
        "TruthfulQA":47.44,
        "Winogrande":77.74,
        "GSM8K":10.39,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"8bit",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"57bd88e24d603dc4bbe4016ed0871db7c0e529d5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/BrainDerp3",
        "Average":57.13,
        "ARC":60.92,
        "HellaSwag":82.1,
        "MMLU":58.91,
        "TruthfulQA":57.18,
        "Winogrande":75.61,
        "GSM8K":8.04,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"0b575b9245406cca92942ce2ababb5b868109bed"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Doctor-Shotgun\/CalliopeDS-v2-L2-13B",
        "Average":57.12,
        "ARC":62.8,
        "HellaSwag":84.14,
        "MMLU":56.14,
        "TruthfulQA":51.06,
        "Winogrande":76.01,
        "GSM8K":12.59,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"e63d24870c840d47e82b029e7f405baa10ad9ea4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gradientputri\/MegaMix-S1-13B",
        "Average":57.12,
        "ARC":62.46,
        "HellaSwag":83.65,
        "MMLU":57.88,
        "TruthfulQA":44.52,
        "Winogrande":75.85,
        "GSM8K":18.35,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"afca2c9488cf8738faec4db6721f6a4c755a5d81"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/BrainDerp",
        "Average":57.11,
        "ARC":60.75,
        "HellaSwag":82.1,
        "MMLU":58.81,
        "TruthfulQA":56.9,
        "Winogrande":75.85,
        "GSM8K":8.26,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"ba21a7ed5458b3fa2b05ce6aab431acd1f857516"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/ReMM-v2.2-L2-13B",
        "Average":57.1,
        "ARC":61.26,
        "HellaSwag":84.16,
        "MMLU":56.22,
        "TruthfulQA":51.35,
        "Winogrande":75.61,
        "GSM8K":14.03,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"d55031fbcd41d749bc0c0ffbcd85636718d373b6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"IkariDev\/Athena-v3",
        "Average":57.09,
        "ARC":61.69,
        "HellaSwag":84.34,
        "MMLU":57.87,
        "TruthfulQA":51.26,
        "Winogrande":75.77,
        "GSM8K":11.6,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"5e4024b6694bb13f1a81ce4277ac9141f0b226df"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/vicuna-33b-coder",
        "Average":57.07,
        "ARC":60.41,
        "HellaSwag":83.27,
        "MMLU":57.17,
        "TruthfulQA":51.79,
        "Winogrande":76.87,
        "GSM8K":12.89,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"67f6e669d7a15c1104a1478057f3752a503e83c0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/Emerhyst-20B",
        "Average":57.07,
        "ARC":61.69,
        "HellaSwag":84.98,
        "MMLU":56.98,
        "TruthfulQA":54.16,
        "Winogrande":76.09,
        "GSM8K":8.49,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":19.99,
        "Hub \u2764\ufe0f":16.0,
        "Available on the Hub":true,
        "Model Sha":"e4c23af4f5dd88cb27d245e2bfc3b81db652632c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"circulus\/Llama-2-13b-orca-v1",
        "Average":57.05,
        "ARC":62.03,
        "HellaSwag":82.27,
        "MMLU":57.71,
        "TruthfulQA":49.61,
        "Winogrande":76.87,
        "GSM8K":13.8,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"e77ec90f432bdffa210a0e4310d117e5d1c662df"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"stabilityai\/StableBeluga-13B",
        "Average":57.05,
        "ARC":62.03,
        "HellaSwag":82.27,
        "MMLU":57.71,
        "TruthfulQA":49.61,
        "Winogrande":76.87,
        "GSM8K":13.8,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":107.0,
        "Available on the Hub":true,
        "Model Sha":"1d6eef4cc2b73f39600a568803ad8183f2da4514"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-33b-gpt4-m2.0",
        "Average":57.03,
        "ARC":63.4,
        "HellaSwag":85.19,
        "MMLU":57.46,
        "TruthfulQA":48.15,
        "Winogrande":78.37,
        "GSM8K":9.63,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"84a89dee5bf3447079f115a3ef4d58ef8f924798"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/zephyr-beta-Nebula-v2-7B",
        "Average":57.03,
        "ARC":56.57,
        "HellaSwag":82.53,
        "MMLU":56.4,
        "TruthfulQA":58.68,
        "Winogrande":70.48,
        "GSM8K":17.51,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"226caedb50a12730232c1f8fe9c96b6dcf818ba7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-33b-gpt4-2.0",
        "Average":57.02,
        "ARC":63.91,
        "HellaSwag":85.67,
        "MMLU":57.95,
        "TruthfulQA":45.54,
        "Winogrande":77.98,
        "GSM8K":11.07,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"a4e1b721add286900c5a6f529c3d7a3e0049b2e0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"WebraftAI\/synapsellm-7b-mistral-v0.3-preview",
        "Average":57.01,
        "ARC":53.84,
        "HellaSwag":74.86,
        "MMLU":54.81,
        "TruthfulQA":55.03,
        "Winogrande":74.59,
        "GSM8K":28.96,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"4e509275c5e51bee6e82c2c15082a6cc50d87b5b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-33b-gpt4-2.0",
        "Average":57.01,
        "ARC":63.82,
        "HellaSwag":85.65,
        "MMLU":58.44,
        "TruthfulQA":45.57,
        "Winogrande":77.9,
        "GSM8K":10.69,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"ddc598f492f5098a8e308f51a82834f98f29a4ce"
    },
    {
        "T":"\u2b55",
        "Model":"yulan-team\/YuLan-Chat-2-13b-fp16",
        "Average":57.01,
        "ARC":59.04,
        "HellaSwag":80.66,
        "MMLU":56.72,
        "TruthfulQA":52.18,
        "Winogrande":79.64,
        "GSM8K":13.8,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":12.95,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"2d439187efd6edd91a0c0146f08dff52d92aa7bc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/ReMM-v2-L2-13B",
        "Average":56.99,
        "ARC":61.95,
        "HellaSwag":84.0,
        "MMLU":56.14,
        "TruthfulQA":50.81,
        "Winogrande":75.85,
        "GSM8K":13.19,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"bc42c77f88482c37c72c85c66135e99972bbca1b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/OpenOrca-Platypus2-13B-GPTQ",
        "Average":56.98,
        "ARC":62.54,
        "HellaSwag":82.67,
        "MMLU":58.56,
        "TruthfulQA":51.93,
        "Winogrande":76.8,
        "GSM8K":9.4,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"GPTQ",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":16.24,
        "Hub \u2764\ufe0f":49.0,
        "Available on the Hub":true,
        "Model Sha":"0fa9a56066656fbc94e3ec088bc900fd1d4d38e8"
    },
    {
        "T":"\u2b55",
        "Model":"PulsarAI\/GenAI-Nova-13B",
        "Average":56.98,
        "ARC":62.29,
        "HellaSwag":83.27,
        "MMLU":59.47,
        "TruthfulQA":51.79,
        "Winogrande":77.35,
        "GSM8K":7.73,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"0ce62a64ca53cd5feb18f523a96dd3be86e6513d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-33b-gpt4-m2.0",
        "Average":56.97,
        "ARC":63.14,
        "HellaSwag":85.19,
        "MMLU":57.28,
        "TruthfulQA":48.07,
        "Winogrande":78.45,
        "GSM8K":9.7,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"96af3dc6c9f2248d964cf14cef6e5f2e5894583a"
    },
    {
        "T":"?",
        "Model":"huggyllama\/llama-30b",
        "Average":56.96,
        "ARC":61.43,
        "HellaSwag":84.73,
        "MMLU":58.45,
        "TruthfulQA":42.27,
        "Winogrande":80.03,
        "GSM8K":14.86,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":32.53,
        "Hub \u2764\ufe0f":37.0,
        "Available on the Hub":true,
        "Model Sha":"2b1edcdb3c7ced7bce6c1aa75c94545777c3118b"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"huggingface\/llama-30b",
        "Average":56.94,
        "ARC":61.26,
        "HellaSwag":84.73,
        "MMLU":58.47,
        "TruthfulQA":42.27,
        "Winogrande":80.03,
        "GSM8K":14.86,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":32.53,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"13c77caa472bfa79d4f3f0ec82cbdc9dd88e5d22"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Yhyu13\/llama-30B-hf-openassitant",
        "Average":56.94,
        "ARC":61.26,
        "HellaSwag":84.73,
        "MMLU":58.47,
        "TruthfulQA":42.27,
        "Winogrande":80.03,
        "GSM8K":14.86,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"fba493af11a73cf5a2ee7857dd7aecb98c659dc4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/UndiMix-v4-13B",
        "Average":56.93,
        "ARC":61.95,
        "HellaSwag":83.88,
        "MMLU":56.9,
        "TruthfulQA":48.96,
        "Winogrande":76.16,
        "GSM8K":13.72,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"6dd97c74cfe1d22432d5c993814e230f333ba401"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"andreaskoepf\/llama2-13b-megacode2_min100",
        "Average":56.92,
        "ARC":60.58,
        "HellaSwag":81.26,
        "MMLU":57.92,
        "TruthfulQA":48.89,
        "Winogrande":76.95,
        "GSM8K":15.92,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"b38d1b53c358a0313c69bcceebe97628327ada82"
    },
    {
        "T":"\u2b55",
        "Model":"rombodawg\/LosslessMegaCoder-llama2-13b-mini",
        "Average":56.92,
        "ARC":60.58,
        "HellaSwag":81.26,
        "MMLU":57.92,
        "TruthfulQA":48.89,
        "Winogrande":76.95,
        "GSM8K":15.92,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"1f5609ffd40bc3af2dcbc5c88e9312d47a73c4b4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"circulus\/Llama-2-13b-orca-v1",
        "Average":56.91,
        "ARC":62.2,
        "HellaSwag":82.32,
        "MMLU":57.67,
        "TruthfulQA":49.6,
        "Winogrande":76.8,
        "GSM8K":12.89,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"mit",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"e77ec90f432bdffa210a0e4310d117e5d1c662df"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/Emerald-13B",
        "Average":56.89,
        "ARC":62.29,
        "HellaSwag":83.69,
        "MMLU":55.7,
        "TruthfulQA":50.94,
        "Winogrande":75.93,
        "GSM8K":12.81,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f7696299463d8ec402a4e1eb001f3a447f1c5552"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/ReMM-Mistral-13B",
        "Average":56.89,
        "ARC":62.2,
        "HellaSwag":83.82,
        "MMLU":55.43,
        "TruthfulQA":53.32,
        "Winogrande":74.51,
        "GSM8K":12.05,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":false,
        "Model Sha":"a5ef9385d9430a81778183d71b58eb2b869d6a7e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/OpenOrcaxOpenChat-Preview2-13B-GPTQ",
        "Average":56.84,
        "ARC":61.26,
        "HellaSwag":82.14,
        "MMLU":57.85,
        "TruthfulQA":50.22,
        "Winogrande":77.11,
        "GSM8K":12.43,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"GPTQ",
        "Hub License":"llama2",
        "#Params (B)":16.24,
        "Hub \u2764\ufe0f":21.0,
        "Available on the Hub":true,
        "Model Sha":"ec9eb4f471b5bb6a7e5e505369628586c0c72252"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dsvv-cair\/alpaca-cleaned-llama-30b-bf16",
        "Average":56.82,
        "ARC":61.77,
        "HellaSwag":85.06,
        "MMLU":57.52,
        "TruthfulQA":51.49,
        "Winogrande":77.35,
        "GSM8K":7.73,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"2424b6346e9e8fd749b9a6734f5d7125b5926daf"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"01-ai\/Yi-6B-200K",
        "Average":56.76,
        "ARC":53.75,
        "HellaSwag":75.57,
        "MMLU":64.65,
        "TruthfulQA":41.56,
        "Winogrande":73.64,
        "GSM8K":31.39,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"other",
        "#Params (B)":6.06,
        "Hub \u2764\ufe0f":108.0,
        "Available on the Hub":true,
        "Model Sha":"6cb672ed8441c35d043dd3cda448466daa3b38b1"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/Platypus2xOpenOrca-13B-IA3-v3",
        "Average":56.74,
        "ARC":62.54,
        "HellaSwag":82.1,
        "MMLU":58.67,
        "TruthfulQA":46.96,
        "Winogrande":77.82,
        "GSM8K":12.36,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"17493c1f2e4620a44d7947edad0386d338e805ce"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/Orca-Nova-13B",
        "Average":56.72,
        "ARC":62.37,
        "HellaSwag":82.47,
        "MMLU":57.44,
        "TruthfulQA":45.97,
        "Winogrande":77.58,
        "GSM8K":14.48,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"5a6c3686749ecb76971a915403da8c07a98078a6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/ReMM-v2.1-L2-13B",
        "Average":56.71,
        "ARC":61.43,
        "HellaSwag":83.92,
        "MMLU":55.95,
        "TruthfulQA":50.3,
        "Winogrande":75.93,
        "GSM8K":12.74,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"e6b5ac97f74355cb281a621261debe5720fb4da2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Open-Orca\/OpenOrcaxOpenChat-Preview2-13B",
        "Average":56.7,
        "ARC":62.71,
        "HellaSwag":81.99,
        "MMLU":57.51,
        "TruthfulQA":47.45,
        "Winogrande":76.8,
        "GSM8K":13.72,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":94.0,
        "Available on the Hub":true,
        "Model Sha":"26d1bc5c54c1f60a5de0b1ed4d0b16f285aee230"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"01-ai\/Yi-6B-200K",
        "Average":56.69,
        "ARC":53.58,
        "HellaSwag":75.58,
        "MMLU":64.65,
        "TruthfulQA":41.74,
        "Winogrande":74.27,
        "GSM8K":30.33,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.06,
        "Hub \u2764\ufe0f":116.0,
        "Available on the Hub":true,
        "Model Sha":"6cb672ed8441c35d043dd3cda448466daa3b38b1"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/Platypus2xOpenOrca-13B-IA3",
        "Average":56.65,
        "ARC":62.12,
        "HellaSwag":82.1,
        "MMLU":58.84,
        "TruthfulQA":47.88,
        "Winogrande":77.11,
        "GSM8K":11.83,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"5ca46029dd22c007d4dc1706f6284a32be4546c2"
    },
    {
        "T":"\u2b55",
        "Model":"chargoddard\/storytime-13b",
        "Average":56.64,
        "ARC":62.03,
        "HellaSwag":83.96,
        "MMLU":57.48,
        "TruthfulQA":52.5,
        "Winogrande":75.53,
        "GSM8K":8.34,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":12.0,
        "Available on the Hub":true,
        "Model Sha":"233568319a636b6a7b02a4def2c51d08a3e0fbfc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ZoidBB\/unraveled-7b-a1",
        "Average":56.63,
        "ARC":59.81,
        "HellaSwag":82.8,
        "MMLU":63.39,
        "TruthfulQA":42.23,
        "Winogrande":77.19,
        "GSM8K":14.33,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"fac05775fa8121b58cda8031b7001323bd43983d"
    },
    {
        "T":"\u2b55",
        "Model":"chargoddard\/duplicitous-slurpbeast-13b",
        "Average":56.62,
        "ARC":62.12,
        "HellaSwag":83.92,
        "MMLU":57.53,
        "TruthfulQA":52.33,
        "Winogrande":75.06,
        "GSM8K":8.79,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"88dc61b7afebf2220ca42898e1286c59961ed440"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/Amethyst-13B",
        "Average":56.62,
        "ARC":62.63,
        "HellaSwag":83.17,
        "MMLU":55.91,
        "TruthfulQA":52.43,
        "Winogrande":74.74,
        "GSM8K":10.84,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"d4a85b1006f0b9439e64f0e7400533a7b867c24d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/Amethyst-13B-Mistral",
        "Average":56.62,
        "ARC":62.63,
        "HellaSwag":83.17,
        "MMLU":55.91,
        "TruthfulQA":52.43,
        "Winogrande":74.74,
        "GSM8K":10.84,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":false,
        "Model Sha":"4328809e568f01e3f0a05764e3bb58e901310415"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BELLE-2\/BELLE-Llama2-13B-chat-0.4M",
        "Average":56.62,
        "ARC":60.67,
        "HellaSwag":82.31,
        "MMLU":55.94,
        "TruthfulQA":50.85,
        "Winogrande":75.53,
        "GSM8K":14.4,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":27.0,
        "Available on the Hub":true,
        "Model Sha":"1776feacbf1052cff02eb3d7531a854555d3f6dc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"elinas\/chronos-33b",
        "Average":56.59,
        "ARC":62.2,
        "HellaSwag":83.48,
        "MMLU":55.87,
        "TruthfulQA":46.67,
        "Winogrande":78.3,
        "GSM8K":13.04,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":23.0,
        "Available on the Hub":true,
        "Model Sha":"3c11f81d9180618f13777276b1eb0eb70ab99cf0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Open-Orca\/LlongOrca-13B-16k",
        "Average":56.59,
        "ARC":62.46,
        "HellaSwag":82.75,
        "MMLU":55.54,
        "TruthfulQA":50.11,
        "Winogrande":76.4,
        "GSM8K":12.28,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":11.0,
        "Available on the Hub":true,
        "Model Sha":"8ea1fb205553cadbc90069d80a7e58281b6281c3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenAssistant\/llama2-13b-megacode2-oasst",
        "Average":56.59,
        "ARC":60.67,
        "HellaSwag":81.93,
        "MMLU":57.38,
        "TruthfulQA":47.85,
        "Winogrande":76.16,
        "GSM8K":15.54,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":9.0,
        "Available on the Hub":true,
        "Model Sha":"2c45ecf161da2ff2aa984900f2e4d2b7a7311ab8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Brouz\/Slerpeno",
        "Average":56.59,
        "ARC":61.69,
        "HellaSwag":84.1,
        "MMLU":56.77,
        "TruthfulQA":48.05,
        "Winogrande":76.4,
        "GSM8K":12.51,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"7ff32abd17851a769a031659e91e660f219be363"
    },
    {
        "T":"\u2b55",
        "Model":"chargoddard\/duplicitous-mammal-13b",
        "Average":56.57,
        "ARC":61.69,
        "HellaSwag":83.79,
        "MMLU":57.5,
        "TruthfulQA":52.27,
        "Winogrande":75.06,
        "GSM8K":9.1,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"a05d0562b8da2ac2e76aa65984e8063249bc85c8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/OpenRP-13B",
        "Average":56.57,
        "ARC":62.12,
        "HellaSwag":82.6,
        "MMLU":57.5,
        "TruthfulQA":48.29,
        "Winogrande":76.01,
        "GSM8K":12.89,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"d11815287c51ef51485fb003f8f72773cf6f19a4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/MM-ReMM-L2-20B",
        "Average":56.55,
        "ARC":60.84,
        "HellaSwag":85.18,
        "MMLU":56.45,
        "TruthfulQA":53.33,
        "Winogrande":75.77,
        "GSM8K":7.73,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":19.99,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"37869800c15fb37d017ea83bb50fec6d6141f6ba"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"sauce1337\/BerrySauce-L2-13b",
        "Average":56.55,
        "ARC":62.29,
        "HellaSwag":83.78,
        "MMLU":57.1,
        "TruthfulQA":48.3,
        "Winogrande":76.09,
        "GSM8K":11.75,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"c8788874b78c84bc5593586d16fbd8ae7b5b2991"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/MLewdBoros-L2-13B",
        "Average":56.51,
        "ARC":62.54,
        "HellaSwag":83.9,
        "MMLU":56.57,
        "TruthfulQA":48.14,
        "Winogrande":76.95,
        "GSM8K":10.99,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":11.0,
        "Available on the Hub":true,
        "Model Sha":"a3033ac5825662f1c66418d7543648dc76980185"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/Platypus2xOpenOrca-13B-IA3-v4",
        "Average":56.49,
        "ARC":61.43,
        "HellaSwag":81.84,
        "MMLU":59.02,
        "TruthfulQA":48.64,
        "Winogrande":77.19,
        "GSM8K":10.84,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"3aa9abe9cb2e5c699f80935e04fbb351cdfbf21b"
    },
    {
        "T":"\u2b55",
        "Model":"PulsarAI\/EnsembleV5-Nova-13B",
        "Average":56.49,
        "ARC":62.71,
        "HellaSwag":82.55,
        "MMLU":56.79,
        "TruthfulQA":49.86,
        "Winogrande":76.24,
        "GSM8K":10.77,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"3e25556187ba576082a85c270d2d4b4ea6ea9f6f"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/EnsembleV5-Nova-13B",
        "Average":56.49,
        "ARC":62.71,
        "HellaSwag":82.55,
        "MMLU":56.79,
        "TruthfulQA":49.86,
        "Winogrande":76.24,
        "GSM8K":10.77,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"7ba38d309709d35149b4a18f94096875885035ae"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PygmalionAI\/mythalion-13b",
        "Average":56.48,
        "ARC":61.26,
        "HellaSwag":83.81,
        "MMLU":56.53,
        "TruthfulQA":46.56,
        "Winogrande":77.43,
        "GSM8K":13.27,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":58.0,
        "Available on the Hub":true,
        "Model Sha":"24916f62b8243a7e4646ea53eeb45d890cbd308f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/WizardLM-30B-Uncensored",
        "Average":56.46,
        "ARC":60.24,
        "HellaSwag":82.93,
        "MMLU":56.8,
        "TruthfulQA":51.57,
        "Winogrande":74.35,
        "GSM8K":12.89,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":118.0,
        "Available on the Hub":true,
        "Model Sha":"761783745fcb97831ad8035d3cbd5de484aca3ce"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SciPhi\/SciPhi-Self-RAG-Mistral-7B-32k",
        "Average":56.46,
        "ARC":57.34,
        "HellaSwag":80.44,
        "MMLU":60.81,
        "TruthfulQA":45.63,
        "Winogrande":74.82,
        "GSM8K":19.71,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":38.0,
        "Available on the Hub":true,
        "Model Sha":"640192e2ba5898f87c407a9f771fc270f7628dee"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"royallab\/Pygmalion-2-13b-SuperCOT",
        "Average":56.46,
        "ARC":63.23,
        "HellaSwag":83.68,
        "MMLU":54.9,
        "TruthfulQA":53.14,
        "Winogrande":77.51,
        "GSM8K":6.29,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"763b3fd5afc3e7fb6c7c8768d40f06901c8d5913"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Stheno-Inverted-L2-13B",
        "Average":56.44,
        "ARC":59.3,
        "HellaSwag":82.9,
        "MMLU":56.45,
        "TruthfulQA":52.04,
        "Winogrande":74.74,
        "GSM8K":13.19,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"efaf592c95ae8e769e0d56d36ba4ed23e3bf4059"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/Nova-13B",
        "Average":56.44,
        "ARC":62.71,
        "HellaSwag":82.57,
        "MMLU":57.98,
        "TruthfulQA":51.34,
        "Winogrande":77.27,
        "GSM8K":6.75,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"ae1145f9fa846ab8d39d8b7da888287ef917efb5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Stheno-L2-13B",
        "Average":56.43,
        "ARC":61.01,
        "HellaSwag":83.95,
        "MMLU":56.33,
        "TruthfulQA":50.18,
        "Winogrande":75.14,
        "GSM8K":11.98,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"c4e7b771e30fdbfd6bd2e66a6928024bd5692bbd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Mythical-Destroyer-L2-13B",
        "Average":56.39,
        "ARC":58.7,
        "HellaSwag":82.0,
        "MMLU":57.66,
        "TruthfulQA":56.35,
        "Winogrande":74.66,
        "GSM8K":8.95,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"7c87376b201b1c30c4e12c0b7bc2f28f017ce7bc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yeen214\/llama2_7b_merge_orcafamily",
        "Average":56.38,
        "ARC":56.91,
        "HellaSwag":81.17,
        "MMLU":51.49,
        "TruthfulQA":49.68,
        "Winogrande":75.93,
        "GSM8K":23.12,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"fb65f697de632f2f3fef57fc3cd12fb5e4913a89"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PocketDoc\/Dans-AdventurousWinds-Mk2-7b",
        "Average":56.38,
        "ARC":58.19,
        "HellaSwag":83.48,
        "MMLU":61.8,
        "TruthfulQA":43.56,
        "Winogrande":76.32,
        "GSM8K":14.94,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"cfcc969a7e97275b2298253f1eabf4575e5a3768"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/MLewd-v2.4-13B",
        "Average":56.37,
        "ARC":61.69,
        "HellaSwag":83.83,
        "MMLU":55.1,
        "TruthfulQA":53.34,
        "Winogrande":74.51,
        "GSM8K":9.78,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"6f6ec6024ee054020e49fd96f149919692848f0b"
    },
    {
        "T":"\u2b55",
        "Model":"jondurbin\/airoboros-l2-13b-2.2.1",
        "Average":56.36,
        "ARC":60.92,
        "HellaSwag":83.77,
        "MMLU":56.47,
        "TruthfulQA":49.42,
        "Winogrande":76.01,
        "GSM8K":11.6,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"9b2dbc1f6f17a162228799df6e9449c903ddf04d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openchat\/openchat_v3.1",
        "Average":56.36,
        "ARC":59.81,
        "HellaSwag":82.8,
        "MMLU":56.76,
        "TruthfulQA":44.45,
        "Winogrande":76.24,
        "GSM8K":18.12,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"a95be7130d32da99bcd484f6f436b2dd49341110"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Doctor-Shotgun\/CalliopeDS-L2-13B",
        "Average":56.34,
        "ARC":60.49,
        "HellaSwag":83.38,
        "MMLU":55.8,
        "TruthfulQA":51.32,
        "Winogrande":77.03,
        "GSM8K":10.01,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"agpl-3.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"b373eda586a6527e62382eda5480204652a82499"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"iGenius-AI-Team\/LLAMA-13B-test-finetuning",
        "Average":56.34,
        "ARC":58.02,
        "HellaSwag":82.36,
        "MMLU":54.27,
        "TruthfulQA":44.14,
        "Winogrande":76.72,
        "GSM8K":22.52,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"5bd0eb026b12c59fd198f307c0c17188af69744c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Gryphe\/MythoMix-L2-13b",
        "Average":56.31,
        "ARC":61.09,
        "HellaSwag":83.86,
        "MMLU":55.42,
        "TruthfulQA":52.08,
        "Winogrande":75.45,
        "GSM8K":9.93,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":15.0,
        "Available on the Hub":true,
        "Model Sha":"eca790fb9394c9c61be27ef709080b3b92783a45"
    },
    {
        "T":"\u2b55",
        "Model":"lgaalves\/mistral-7b_open_platypus",
        "Average":56.29,
        "ARC":55.8,
        "HellaSwag":82.13,
        "MMLU":59.76,
        "TruthfulQA":48.87,
        "Winogrande":78.61,
        "GSM8K":12.59,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"b9a60b9ad0fe06bd314ffe99d543f1df6ecd10da"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/Platypus2xOpenOrca-13B-IA3-v2.1",
        "Average":56.29,
        "ARC":62.29,
        "HellaSwag":82.09,
        "MMLU":57.91,
        "TruthfulQA":47.03,
        "Winogrande":77.43,
        "GSM8K":10.99,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"31e1e3235515717a151915131bc970be188d964e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ajibawa-2023\/Uncensored-Jordan-13B",
        "Average":56.27,
        "ARC":57.42,
        "HellaSwag":82.7,
        "MMLU":55.75,
        "TruthfulQA":50.51,
        "Winogrande":76.16,
        "GSM8K":15.09,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"c56a396342133bbd75ab3f79622c85cb55be49a4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-code-mistral-orca-7b-v1.0",
        "Average":56.24,
        "ARC":59.64,
        "HellaSwag":82.25,
        "MMLU":61.33,
        "TruthfulQA":48.45,
        "Winogrande":77.51,
        "GSM8K":8.26,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"f7db67fe6c82657b35d0ffcf8b7ff1568d979482"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aspik101\/StableBeluga-13B-instruct-PL-lora_unload",
        "Average":56.24,
        "ARC":60.92,
        "HellaSwag":82.13,
        "MMLU":56.99,
        "TruthfulQA":48.64,
        "Winogrande":76.56,
        "GSM8K":12.21,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"6e1a6e1f91f6ac97b643be1bd24be6096e2e7dd3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Gryphe\/MythoLogic-L2-13b",
        "Average":56.19,
        "ARC":61.01,
        "HellaSwag":83.93,
        "MMLU":55.7,
        "TruthfulQA":48.64,
        "Winogrande":76.09,
        "GSM8K":11.75,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":15.0,
        "Available on the Hub":true,
        "Model Sha":"665948fc79acc2bcce3e9e7d2b0689ca43ae62d4"
    },
    {
        "T":"\u2b55",
        "Model":"maywell\/Synatra-11B-Testbench",
        "Average":56.17,
        "ARC":57.34,
        "HellaSwag":78.66,
        "MMLU":55.56,
        "TruthfulQA":51.97,
        "Winogrande":75.77,
        "GSM8K":17.74,
        "Type":"instruction-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":11.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"9399ea6c2a1d955e31d6b4d68b2b86115aea0e59"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Stheno-1.2-L2-13B",
        "Average":56.15,
        "ARC":60.75,
        "HellaSwag":83.67,
        "MMLU":56.27,
        "TruthfulQA":50.32,
        "Winogrande":74.98,
        "GSM8K":10.92,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"e76f35fe771ef142d6629092bd4a93301fd6cd4a"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/SpeechlessV1-Nova-13B",
        "Average":56.14,
        "ARC":61.77,
        "HellaSwag":82.68,
        "MMLU":57.75,
        "TruthfulQA":51.44,
        "Winogrande":77.43,
        "GSM8K":5.76,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"fbe6f0e32b5ecf9d75510d0b11a286466f46d79e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"WhoTookMyAmogusNickname\/NewHope_HF_not_official",
        "Average":56.11,
        "ARC":61.09,
        "HellaSwag":84.03,
        "MMLU":55.73,
        "TruthfulQA":44.96,
        "Winogrande":74.98,
        "GSM8K":15.85,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f587f4a31de6818f4200d9cdc7f116ca8ba1cdc2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Austism\/chronos-hermes-13b-v2",
        "Average":56.1,
        "ARC":60.32,
        "HellaSwag":83.21,
        "MMLU":55.05,
        "TruthfulQA":50.91,
        "Winogrande":75.37,
        "GSM8K":11.75,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":11.0,
        "Available on the Hub":true,
        "Model Sha":"2f0e2cb734685a6ce0736a9f3e909a795d7592cc"
    },
    {
        "T":"\u2b55",
        "Model":"PulsarAI\/Nebula-7B",
        "Average":56.1,
        "ARC":59.3,
        "HellaSwag":83.46,
        "MMLU":57.0,
        "TruthfulQA":45.56,
        "Winogrande":76.4,
        "GSM8K":14.86,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"569f848698a468fb03d37033c67f3734bbaec127"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kaist-ai\/prometheus-13b-v1.0",
        "Average":56.09,
        "ARC":53.24,
        "HellaSwag":80.75,
        "MMLU":51.49,
        "TruthfulQA":45.66,
        "Winogrande":73.72,
        "GSM8K":31.69,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":29.0,
        "Available on the Hub":true,
        "Model Sha":"9088377314f91af4b48940e09a0c76d0878f5020"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"augtoma\/qCammel-13",
        "Average":56.05,
        "ARC":60.84,
        "HellaSwag":83.66,
        "MMLU":56.73,
        "TruthfulQA":47.54,
        "Winogrande":76.16,
        "GSM8K":11.37,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"af473e64f6a4fa02a7e24ee7679eea9505eb179d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"The-Face-Of-Goonery\/Huginn-13b-v1.2",
        "Average":56.03,
        "ARC":60.92,
        "HellaSwag":83.56,
        "MMLU":55.33,
        "TruthfulQA":51.97,
        "Winogrande":75.22,
        "GSM8K":9.17,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"cb3562e7aae05a95fe61610b7b8f4957d3529ce7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/ReMM-SLERP-L2-13B",
        "Average":56.03,
        "ARC":60.92,
        "HellaSwag":83.56,
        "MMLU":55.33,
        "TruthfulQA":51.97,
        "Winogrande":75.22,
        "GSM8K":9.17,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"27baccf242bc1dc34fc39661a40bbf867cbea8b5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ajibawa-2023\/carl-33b",
        "Average":56.03,
        "ARC":64.59,
        "HellaSwag":85.27,
        "MMLU":58.38,
        "TruthfulQA":45.32,
        "Winogrande":76.24,
        "GSM8K":6.37,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"5f80b372b493d901cab4490b4f23c71499023615"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/neural-chat-7b-v3-1-Nebula-v2-7B",
        "Average":56.01,
        "ARC":61.77,
        "HellaSwag":80.21,
        "MMLU":59.07,
        "TruthfulQA":58.56,
        "Winogrande":71.82,
        "GSM8K":4.62,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"0b98e4ca35764da09cabcaaebbdac1f827629219"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Gryphe\/MythoMax-L2-13b",
        "Average":56.0,
        "ARC":60.92,
        "HellaSwag":83.56,
        "MMLU":55.33,
        "TruthfulQA":51.97,
        "Winogrande":75.22,
        "GSM8K":9.02,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":119.0,
        "Available on the Hub":true,
        "Model Sha":"faa4ef8c87dbb00d447904ceb048d49b6a463d07"
    },
    {
        "T":"\u2b55",
        "Model":"The-Face-Of-Goonery\/huginnv1.2",
        "Average":55.98,
        "ARC":62.37,
        "HellaSwag":84.28,
        "MMLU":57.02,
        "TruthfulQA":47.81,
        "Winogrande":75.22,
        "GSM8K":9.17,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"aed4ddc951c657993939fa5b87a4088550569a3b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/Nous-Hermes-Llama2-13b",
        "Average":55.97,
        "ARC":61.52,
        "HellaSwag":83.29,
        "MMLU":55.11,
        "TruthfulQA":50.38,
        "Winogrande":75.45,
        "GSM8K":10.08,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":221.0,
        "Available on the Hub":true,
        "Model Sha":"8f95aa9cd207db7b24179fc779c2b8973e71bee2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/Samantha-1.11-13b",
        "Average":55.97,
        "ARC":60.84,
        "HellaSwag":82.99,
        "MMLU":55.96,
        "TruthfulQA":47.72,
        "Winogrande":76.01,
        "GSM8K":12.28,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"e355ead3a939f471fe2586201156fb972fad0f4b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/Nous-Hermes-13B-Code",
        "Average":55.93,
        "ARC":61.18,
        "HellaSwag":83.21,
        "MMLU":55.13,
        "TruthfulQA":50.56,
        "Winogrande":75.14,
        "GSM8K":10.39,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"5a45cb2a6442581ce32cc19c561c49cec1db4ebb"
    },
    {
        "T":"\u2b55",
        "Model":"PulsarAI\/Chat-AYB-Platypus2-13B",
        "Average":55.93,
        "ARC":60.49,
        "HellaSwag":84.03,
        "MMLU":57.83,
        "TruthfulQA":54.52,
        "Winogrande":75.77,
        "GSM8K":2.96,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"5a54eb9d5a66df4720ec52422f5627ccd94d5fd6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"sauce1337\/AppleSauce-L2-13b",
        "Average":55.91,
        "ARC":61.01,
        "HellaSwag":83.61,
        "MMLU":57.07,
        "TruthfulQA":47.81,
        "Winogrande":75.93,
        "GSM8K":10.01,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"ba253c52eb85e24987c81e5d36b5a9a00e276ce7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Synthia-13B-v1.2",
        "Average":55.9,
        "ARC":61.26,
        "HellaSwag":82.93,
        "MMLU":56.47,
        "TruthfulQA":47.27,
        "Winogrande":76.48,
        "GSM8K":10.99,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"60d4937ac3c4dcb84c40bbf7265c5cc7f5f3d4f9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openBuddy\/openbuddy-llama2-34b-v11.1-bf16",
        "Average":55.88,
        "ARC":50.0,
        "HellaSwag":71.19,
        "MMLU":55.71,
        "TruthfulQA":53.01,
        "Winogrande":70.8,
        "GSM8K":34.57,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":33.53,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"21ac0d26c0097e5ac5b4a757493574b156da7731"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-codellama2-34b-v11.1-bf16",
        "Average":55.88,
        "ARC":50.0,
        "HellaSwag":71.19,
        "MMLU":55.71,
        "TruthfulQA":53.01,
        "Winogrande":70.8,
        "GSM8K":34.57,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":33.53,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"1b361b3634bf59913b47c9dad1b138e99833472b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YeungNLP\/firefly-llama2-13b-v1.2",
        "Average":55.87,
        "ARC":60.67,
        "HellaSwag":80.46,
        "MMLU":56.51,
        "TruthfulQA":51.03,
        "Winogrande":74.82,
        "GSM8K":11.75,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"97279d20a8c7e2d0576c9ff4b2e15a421c40d58a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"maywell\/Synatra-V0.1-7B",
        "Average":55.86,
        "ARC":55.29,
        "HellaSwag":76.63,
        "MMLU":55.29,
        "TruthfulQA":55.76,
        "Winogrande":72.77,
        "GSM8K":19.41,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":12.0,
        "Available on the Hub":true,
        "Model Sha":"7ee3416f31a3c7e8d5ab4295ac1b641075f36345"
    },
    {
        "T":"\u2b55",
        "Model":"maywell\/Synatra-V0.1-7B-Instruct",
        "Average":55.86,
        "ARC":55.29,
        "HellaSwag":76.63,
        "MMLU":55.29,
        "TruthfulQA":55.76,
        "Winogrande":72.77,
        "GSM8K":19.41,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":12.0,
        "Available on the Hub":true,
        "Model Sha":"7ee3416f31a3c7e8d5ab4295ac1b641075f36345"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CausalLM\/7B-DPO-alpha",
        "Average":55.84,
        "ARC":50.85,
        "HellaSwag":73.0,
        "MMLU":63.39,
        "TruthfulQA":57.58,
        "Winogrande":67.56,
        "GSM8K":22.67,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"wtfpl",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":46.0,
        "Available on the Hub":true,
        "Model Sha":"36501a519950fb80c2e7df77e12c9110dca580f4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Danielbrdz\/Barcenas-13b",
        "Average":55.83,
        "ARC":61.26,
        "HellaSwag":82.13,
        "MMLU":56.25,
        "TruthfulQA":46.67,
        "Winogrande":76.32,
        "GSM8K":12.36,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"fa988ba73f67ad0c8e7fa8f408106ea040070258"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"feidfoe\/Metamath-reproduce-7b",
        "Average":55.81,
        "ARC":47.18,
        "HellaSwag":73.65,
        "MMLU":42.94,
        "TruthfulQA":41.58,
        "Winogrande":71.35,
        "GSM8K":58.15,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"9955b88b535863a36ee9d9a255260bbc2cdab47b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-OpenOrca_5w",
        "Average":55.8,
        "ARC":61.01,
        "HellaSwag":82.82,
        "MMLU":56.09,
        "TruthfulQA":44.87,
        "Winogrande":77.74,
        "GSM8K":12.28,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"0ddd810c9150492d7318656acac44849651edbf2"
    },
    {
        "T":"?",
        "Model":"CoruNethron\/neu-sai-it1",
        "Average":55.78,
        "ARC":61.26,
        "HellaSwag":81.39,
        "MMLU":60.17,
        "TruthfulQA":51.49,
        "Winogrande":77.51,
        "GSM8K":2.88,
        "Type":"",
        "Architecture":"?",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"c78cd605142d20c62c78b2c7456fe61d49d990a6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/Nous-Hermes-Llama2-13b",
        "Average":55.75,
        "ARC":61.26,
        "HellaSwag":83.26,
        "MMLU":55.04,
        "TruthfulQA":50.41,
        "Winogrande":75.37,
        "GSM8K":9.17,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":221.0,
        "Available on the Hub":true,
        "Model Sha":"8f95aa9cd207db7b24179fc779c2b8973e71bee2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"garage-bAInd\/Stable-Platypus2-13B",
        "Average":55.75,
        "ARC":62.71,
        "HellaSwag":82.29,
        "MMLU":58.3,
        "TruthfulQA":52.52,
        "Winogrande":76.87,
        "GSM8K":1.82,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":19.0,
        "Available on the Hub":true,
        "Model Sha":"0e54aa49c24617e30a23a20c0c5da61419b9fe68"
    },
    {
        "T":"\u2b55",
        "Model":"lu-vae\/llama2-13B-sharegpt4-orca-openplatypus-8w",
        "Average":55.75,
        "ARC":62.8,
        "HellaSwag":84.04,
        "MMLU":55.13,
        "TruthfulQA":45.66,
        "Winogrande":75.14,
        "GSM8K":11.75,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"ad086aacf0176911133b6cccfb34364afce9de5a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PulsarAI\/CollectiveCognition-v1.1-Nebula-7B",
        "Average":55.72,
        "ARC":58.11,
        "HellaSwag":82.39,
        "MMLU":57.03,
        "TruthfulQA":53.53,
        "Winogrande":73.72,
        "GSM8K":9.55,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"c41d373a2d49b79236d6c4d0dfc4086e709c07eb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openchat\/openchat_v3.1",
        "Average":55.71,
        "ARC":60.15,
        "HellaSwag":82.84,
        "MMLU":56.84,
        "TruthfulQA":44.38,
        "Winogrande":76.24,
        "GSM8K":13.8,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"cc708183e430234b8718c08d9f90474569eabeac"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Stheno-1.1-L2-13B",
        "Average":55.71,
        "ARC":60.75,
        "HellaSwag":83.64,
        "MMLU":56.39,
        "TruthfulQA":50.3,
        "Winogrande":75.22,
        "GSM8K":7.96,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"0f45a9f834dd216ce25ffa606b3b1ef2c99e7acd"
    },
    {
        "T":"\u2b55",
        "Model":"lu-vae\/llama2-13b-sharegpt4-test",
        "Average":55.69,
        "ARC":58.02,
        "HellaSwag":82.65,
        "MMLU":55.99,
        "TruthfulQA":48.27,
        "Winogrande":76.09,
        "GSM8K":13.12,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"2be36a2dab4ed0f97727a1508367f53d59950818"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"meta-llama\/Llama-2-13b-hf",
        "Average":55.69,
        "ARC":59.39,
        "HellaSwag":82.13,
        "MMLU":55.77,
        "TruthfulQA":37.38,
        "Winogrande":76.64,
        "GSM8K":22.82,
        "Type":"pretrained",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":389.0,
        "Available on the Hub":false,
        "Model Sha":"7da18fb10421c3ae2a1eb92815bad75e84816e35"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openchat\/openchat_v3.2",
        "Average":55.68,
        "ARC":59.64,
        "HellaSwag":82.68,
        "MMLU":56.68,
        "TruthfulQA":44.49,
        "Winogrande":76.95,
        "GSM8K":13.65,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":36.0,
        "Available on the Hub":true,
        "Model Sha":"65320bf6dbe0cb4682d45a9e55dbc876502f8b66"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YeungNLP\/firefly-llama2-13b",
        "Average":55.68,
        "ARC":59.13,
        "HellaSwag":81.99,
        "MMLU":55.49,
        "TruthfulQA":51.57,
        "Winogrande":74.66,
        "GSM8K":11.22,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":20.0,
        "Available on the Hub":true,
        "Model Sha":"6e918dc8beb1e764def5938fdb8e3f64ba40a456"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-hermes-coig-lite-13b",
        "Average":55.65,
        "ARC":59.47,
        "HellaSwag":82.28,
        "MMLU":55.18,
        "TruthfulQA":47.6,
        "Winogrande":78.61,
        "GSM8K":10.77,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"2ee11d9c7acaefb723796227e2ad099b165f0dd9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/U-Amethyst-20B",
        "Average":55.65,
        "ARC":62.2,
        "HellaSwag":83.11,
        "MMLU":55.88,
        "TruthfulQA":53.2,
        "Winogrande":74.19,
        "GSM8K":5.31,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":19.99,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"c0cbe0b3c88041bb6beef27dbe85146af8dddec9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Enno-Ai\/ennodata-13b-8bit-raw-15epoch",
        "Average":55.65,
        "ARC":61.6,
        "HellaSwag":82.2,
        "MMLU":57.55,
        "TruthfulQA":53.58,
        "Winogrande":77.51,
        "GSM8K":1.44,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"ee2ceaae9cb806bc30df84ba4d598fdf32e53b17"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ajibawa-2023\/Uncensored-Frank-13B",
        "Average":55.64,
        "ARC":61.6,
        "HellaSwag":82.62,
        "MMLU":54.55,
        "TruthfulQA":48.34,
        "Winogrande":74.74,
        "GSM8K":11.98,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"73a27445e5e5a72857626e551c70542ec607f60c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TFLai\/Nova-13B-50-step",
        "Average":55.61,
        "ARC":61.6,
        "HellaSwag":82.31,
        "MMLU":57.27,
        "TruthfulQA":51.53,
        "Winogrande":76.56,
        "GSM8K":4.4,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"1a827ccb7f00157b3cc9ce538d61a6ba8d5a65db"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Severian\/ANIMA-Phi-Neptune-Mistral-7B-v4",
        "Average":55.61,
        "ARC":55.46,
        "HellaSwag":77.63,
        "MMLU":53.12,
        "TruthfulQA":59.01,
        "Winogrande":73.48,
        "GSM8K":14.94,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"a8e18f970f7ca994740177d6c228adee9e17aba9"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/Stable-Platypus2-13B-QLoRA-0.80-epoch",
        "Average":55.56,
        "ARC":62.29,
        "HellaSwag":82.46,
        "MMLU":57.09,
        "TruthfulQA":51.41,
        "Winogrande":76.56,
        "GSM8K":3.56,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"0c15b8540335b3e21a976a5fc5c33b47927fea6c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Severian\/ANIMA-Phi-Neptune-Mistral-7B",
        "Average":55.54,
        "ARC":55.97,
        "HellaSwag":76.22,
        "MMLU":52.89,
        "TruthfulQA":59.76,
        "Winogrande":73.48,
        "GSM8K":14.94,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":false,
        "Model Sha":"e8e9a4804c842b84def9e9aaae38236d4754f277"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"internlm\/internlm-20b-chat",
        "Average":55.53,
        "ARC":55.38,
        "HellaSwag":78.58,
        "MMLU":58.53,
        "TruthfulQA":43.22,
        "Winogrande":78.77,
        "GSM8K":18.73,
        "Type":"fine-tuned",
        "Architecture":"InternLMForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.0,
        "Hub \u2764\ufe0f":114.0,
        "Available on the Hub":true,
        "Model Sha":"79946225fa7a215e0ebcf4440a9cce88e475deaa"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-dolphin_5w",
        "Average":55.53,
        "ARC":60.67,
        "HellaSwag":82.69,
        "MMLU":56.23,
        "TruthfulQA":44.41,
        "Winogrande":77.35,
        "GSM8K":11.83,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"0ec406128968b41a9b7a5f18c358f7638d696b56"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-hermes-coig-lite-13b",
        "Average":55.51,
        "ARC":59.56,
        "HellaSwag":82.26,
        "MMLU":55.3,
        "TruthfulQA":47.56,
        "Winogrande":78.53,
        "GSM8K":9.86,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"2ee11d9c7acaefb723796227e2ad099b165f0dd9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Stheno-Inverted-1.2-L2-13B",
        "Average":55.5,
        "ARC":59.39,
        "HellaSwag":83.01,
        "MMLU":55.77,
        "TruthfulQA":51.22,
        "Winogrande":74.66,
        "GSM8K":8.95,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"8d2e9087093eef1c9173e167beb40b9d034a4655"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kingbri\/airolima-chronos-grad-l2-13B",
        "Average":55.5,
        "ARC":59.56,
        "HellaSwag":83.5,
        "MMLU":55.78,
        "TruthfulQA":44.67,
        "Winogrande":75.85,
        "GSM8K":13.65,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"d2ad57b2b50361485b2b04e59a989161599cb08b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/UndiMix-v1-13b",
        "Average":55.5,
        "ARC":59.47,
        "HellaSwag":82.45,
        "MMLU":55.83,
        "TruthfulQA":49.78,
        "Winogrande":75.45,
        "GSM8K":10.01,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"fd311f52648825d6988d2f945918468ceb32289f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kingbri\/chronolima-airo-grad-l2-13B",
        "Average":55.5,
        "ARC":59.56,
        "HellaSwag":83.47,
        "MMLU":55.8,
        "TruthfulQA":44.58,
        "Winogrande":75.61,
        "GSM8K":13.95,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"agpl-3.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"9195bd6ea775daf347a275e190665e10bf1fb54b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openchat\/openchat_v3.2",
        "Average":55.49,
        "ARC":59.47,
        "HellaSwag":82.6,
        "MMLU":56.82,
        "TruthfulQA":44.51,
        "Winogrande":76.09,
        "GSM8K":13.42,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":36.0,
        "Available on the Hub":true,
        "Model Sha":"bc771c901529dedbf04864d0b81452f62301f882"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lmsys\/vicuna-13b-v1.5",
        "Average":55.41,
        "ARC":57.08,
        "HellaSwag":81.24,
        "MMLU":56.67,
        "TruthfulQA":51.51,
        "Winogrande":74.66,
        "GSM8K":11.3,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":100.0,
        "Available on the Hub":true,
        "Model Sha":"3deb0106f72a3a433f0c6ea0cb978bdf14bcd3a6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/model_007_13b_v2",
        "Average":55.41,
        "ARC":61.95,
        "HellaSwag":82.48,
        "MMLU":57.32,
        "TruthfulQA":53.5,
        "Winogrande":75.85,
        "GSM8K":1.36,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"1c959d4b5d5b8683b051f07475bb5c1ab24c8bb0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Expert68\/llama2_13b_instructed_version2",
        "Average":55.41,
        "ARC":60.07,
        "HellaSwag":84.05,
        "MMLU":55.61,
        "TruthfulQA":46.12,
        "Winogrande":75.61,
        "GSM8K":10.99,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"ea321257d81e0f41c985f5155297b7fbd6ac375a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Synthia-13B",
        "Average":55.41,
        "ARC":59.98,
        "HellaSwag":81.86,
        "MMLU":56.11,
        "TruthfulQA":47.41,
        "Winogrande":76.09,
        "GSM8K":10.99,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"fbb23bc41438b016f1df1e9180c6c350a03557ea"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Enno-Ai\/ennodata-raw-pankajmathur-13b-peft",
        "Average":55.4,
        "ARC":61.95,
        "HellaSwag":82.21,
        "MMLU":57.44,
        "TruthfulQA":53.57,
        "Winogrande":75.93,
        "GSM8K":1.29,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"8bit",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"206553873db96a6730d36477837335dbbcc906fc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"luffycodes\/nash-vicuna-13b-v1dot5-ep2-w-rag-w-simple",
        "Average":55.4,
        "ARC":59.13,
        "HellaSwag":80.64,
        "MMLU":56.12,
        "TruthfulQA":51.29,
        "Winogrande":74.66,
        "GSM8K":10.54,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"848ef91ab46a72260542283918a971347c6bfa93"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-orca-platypus-coig-lite-2k-0.6e-13b",
        "Average":55.4,
        "ARC":59.9,
        "HellaSwag":80.76,
        "MMLU":58.34,
        "TruthfulQA":47.97,
        "Winogrande":77.9,
        "GSM8K":7.51,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"65214c9923d55795ecd6e7f9e0fcee5ba5f26929"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mosaicml\/mpt-30b-chat",
        "Average":55.38,
        "ARC":58.7,
        "HellaSwag":82.54,
        "MMLU":51.16,
        "TruthfulQA":52.42,
        "Winogrande":75.3,
        "GSM8K":12.13,
        "Type":"fine-tuned",
        "Architecture":"MPTForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":29.96,
        "Hub \u2764\ufe0f":183.0,
        "Available on the Hub":true,
        "Model Sha":"54f33278a04aa4e612bca482b82f801ab658e890"
    },
    {
        "T":"\u2b55",
        "Model":"ehartford\/minotaur-llama2-13b-qlora",
        "Average":55.37,
        "ARC":60.07,
        "HellaSwag":82.42,
        "MMLU":55.87,
        "TruthfulQA":45.57,
        "Winogrande":76.24,
        "GSM8K":12.05,
        "Type":"instruction-tuned",
        "Architecture":"?",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":false,
        "Model Sha":"22c83f7d68e547fb0b59acfa01c60b108c59fe55"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gaodrew\/gaodrew-gorgonzola-13b",
        "Average":55.35,
        "ARC":53.84,
        "HellaSwag":78.86,
        "MMLU":71.54,
        "TruthfulQA":42.58,
        "Winogrande":75.3,
        "GSM8K":10.01,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"a53fbe358d4cb546916847d861ccfaf7c724a103"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/Luban-Platypus2-13B-QLora-0.80-epoch",
        "Average":55.34,
        "ARC":60.24,
        "HellaSwag":82.22,
        "MMLU":58.03,
        "TruthfulQA":55.26,
        "Winogrande":75.37,
        "GSM8K":0.91,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"15a99bc147cf9b744cbab7a7c8c5f232cd0c8d10"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/SthenoWriter-L2-13B",
        "Average":55.33,
        "ARC":62.29,
        "HellaSwag":83.28,
        "MMLU":56.14,
        "TruthfulQA":44.72,
        "Winogrande":74.35,
        "GSM8K":11.22,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"a6d9e26ab765eb170cc0aa428ee5e25b08524657"
    },
    {
        "T":"?",
        "Model":"minlik\/chinese-alpaca-33b-merged",
        "Average":55.33,
        "ARC":59.3,
        "HellaSwag":78.43,
        "MMLU":57.69,
        "TruthfulQA":52.45,
        "Winogrande":76.09,
        "GSM8K":8.04,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":32.44,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"fc2535104c0b48afc42575f9fe10bbcbb7612ec3"
    },
    {
        "T":"\u2b55",
        "Model":"PulsarAI\/2x-LoRA-Assemble-Platypus2-13B",
        "Average":55.33,
        "ARC":60.58,
        "HellaSwag":82.56,
        "MMLU":58.25,
        "TruthfulQA":54.77,
        "Winogrande":74.9,
        "GSM8K":0.91,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f147bf8428c174d1dc0332da626d4b039690ceab"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/llama-2-13b-Guanaco-QLoRA",
        "Average":55.31,
        "ARC":61.09,
        "HellaSwag":82.99,
        "MMLU":55.47,
        "TruthfulQA":44.12,
        "Winogrande":77.19,
        "GSM8K":10.99,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"67e68284234538d3851d5c0c334383daffec57a2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Xwin-LM\/Xwin-LM-13B-V0.1",
        "Average":55.29,
        "ARC":62.54,
        "HellaSwag":82.8,
        "MMLU":56.53,
        "TruthfulQA":45.96,
        "Winogrande":74.27,
        "GSM8K":9.63,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":57.0,
        "Available on the Hub":true,
        "Model Sha":"32938856dc3d713dcba706aded7c82791b6ff647"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-OpenOrca_20w",
        "Average":55.28,
        "ARC":59.9,
        "HellaSwag":82.51,
        "MMLU":56.3,
        "TruthfulQA":43.14,
        "Winogrande":77.19,
        "GSM8K":12.66,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f01882672e89b164f76093cf3bd26cfc6ecf72ed"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-llama2-13b-v11.1-bf16",
        "Average":55.28,
        "ARC":51.79,
        "HellaSwag":76.23,
        "MMLU":56.13,
        "TruthfulQA":49.7,
        "Winogrande":73.48,
        "GSM8K":24.34,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":12.88,
        "Hub \u2764\ufe0f":16.0,
        "Available on the Hub":true,
        "Model Sha":"76fb7d00836eb2f1d9c9605d8881d73b782cf324"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"elinas\/chronos-13b-v2",
        "Average":55.25,
        "ARC":58.7,
        "HellaSwag":82.52,
        "MMLU":53.39,
        "TruthfulQA":50.55,
        "Winogrande":75.06,
        "GSM8K":11.3,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":17.0,
        "Available on the Hub":true,
        "Model Sha":"e5d411138e72370c5613dfea0f66ded99f6e62f9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/CreativityEngine",
        "Average":55.25,
        "ARC":59.3,
        "HellaSwag":82.42,
        "MMLU":53.55,
        "TruthfulQA":52.46,
        "Winogrande":74.19,
        "GSM8K":9.55,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"7870cc50b82b5cbebfa9935b6d73a9d20170299a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"beaugogh\/Llama2-13b-sharegpt4",
        "Average":55.25,
        "ARC":61.77,
        "HellaSwag":84.53,
        "MMLU":55.21,
        "TruthfulQA":45.94,
        "Winogrande":75.22,
        "GSM8K":8.79,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"294c40349bf0c5377f71d92e7539bf5de3176a74"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"teknium\/OpenHermes-13B",
        "Average":55.24,
        "ARC":59.81,
        "HellaSwag":82.24,
        "MMLU":56.35,
        "TruthfulQA":46.01,
        "Winogrande":75.45,
        "GSM8K":11.6,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":28.0,
        "Available on the Hub":true,
        "Model Sha":"f09d0fe655ad57cce9179b7b40ea6f81e07db18c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aspik101\/vicuna-13b-v1.5-PL-lora_unload",
        "Average":55.24,
        "ARC":56.91,
        "HellaSwag":81.22,
        "MMLU":56.06,
        "TruthfulQA":49.76,
        "Winogrande":75.22,
        "GSM8K":12.28,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"5c8aeb722e11d1c7258abd45f9f2840f57976c28"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"shareAI\/llama2-13b-Chinese-chat",
        "Average":55.22,
        "ARC":60.58,
        "HellaSwag":82.19,
        "MMLU":55.45,
        "TruthfulQA":45.11,
        "Winogrande":76.64,
        "GSM8K":11.37,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":37.0,
        "Available on the Hub":false,
        "Model Sha":"31103acf93479d5c3865fb9b51dcb38e10d8b801"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/OrcaMini-Platypus2-13B-QLoRA-0.80-epoch",
        "Average":55.22,
        "ARC":60.84,
        "HellaSwag":82.56,
        "MMLU":56.42,
        "TruthfulQA":53.32,
        "Winogrande":75.93,
        "GSM8K":2.27,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"1f81c0439f60d848e3cbc7f06fcd58b5161a8557"
    },
    {
        "T":"\u2b55",
        "Model":"chargoddard\/Chronorctypus-Limarobormes-13b",
        "Average":55.22,
        "ARC":59.9,
        "HellaSwag":82.75,
        "MMLU":58.45,
        "TruthfulQA":51.9,
        "Winogrande":74.43,
        "GSM8K":3.87,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"75c1bf5f4b40cf61873ff6487ccd3efc4f684330"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-l2-13b-3.0",
        "Average":55.21,
        "ARC":59.81,
        "HellaSwag":83.71,
        "MMLU":54.86,
        "TruthfulQA":47.79,
        "Winogrande":76.16,
        "GSM8K":8.95,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"2fcef275782b2c1061cf671d889aea652d13236c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Mythical-Destroyer-V2-L2-13B",
        "Average":55.2,
        "ARC":59.3,
        "HellaSwag":82.66,
        "MMLU":57.39,
        "TruthfulQA":57.09,
        "Winogrande":74.74,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"cbc8b2e4a3beafc311b9e61f8fa9f7526a77c360"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openaccess-ai-collective\/minotaur-13b-fixed",
        "Average":55.19,
        "ARC":59.04,
        "HellaSwag":81.66,
        "MMLU":50.1,
        "TruthfulQA":50.36,
        "Winogrande":76.87,
        "GSM8K":13.12,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"5dac6f7559dba1c6fb59fee18c3e713cc3c83db7"
    },
    {
        "T":"\u2b55",
        "Model":"qblocks\/zephyr_7b_norobots",
        "Average":55.16,
        "ARC":56.48,
        "HellaSwag":79.64,
        "MMLU":55.52,
        "TruthfulQA":44.6,
        "Winogrande":74.11,
        "GSM8K":20.62,
        "Type":"instruction-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"312485e3c11a5cace45ad04dcf87a89df6e69571"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/Platypus2xOpenOrca-13B-LoRa",
        "Average":55.15,
        "ARC":60.75,
        "HellaSwag":82.09,
        "MMLU":58.77,
        "TruthfulQA":45.15,
        "Winogrande":77.03,
        "GSM8K":7.13,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"8b2f5d65c03d415b7c43530def622e133e1ef014"
    },
    {
        "T":"\u2b55",
        "Model":"jondurbin\/airoboros-c34b-2.2.1",
        "Average":55.15,
        "ARC":54.69,
        "HellaSwag":76.84,
        "MMLU":55.43,
        "TruthfulQA":51.36,
        "Winogrande":72.53,
        "GSM8K":20.02,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"79d9761af231fecbfaf6066d6d405a0f8c04f4ba"
    },
    {
        "T":"\u2b55",
        "Model":"dfurman\/Llama-2-13B-Instruct-v0.2",
        "Average":55.14,
        "ARC":60.58,
        "HellaSwag":81.96,
        "MMLU":55.46,
        "TruthfulQA":45.71,
        "Winogrande":77.82,
        "GSM8K":9.33,
        "Type":"instruction-tuned",
        "Architecture":"?",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":false,
        "Model Sha":"ac4b0962df8430f0b31c76a3d97a61134114c87e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/WizardLM-1.0-Uncensored-Llama2-13b",
        "Average":55.14,
        "ARC":55.72,
        "HellaSwag":80.34,
        "MMLU":55.4,
        "TruthfulQA":51.44,
        "Winogrande":74.66,
        "GSM8K":13.27,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":36.0,
        "Available on the Hub":true,
        "Model Sha":"134cea14627fd875f6f277cad92f988024855478"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bofenghuang\/vigogne-2-13b-instruct",
        "Average":55.14,
        "ARC":61.18,
        "HellaSwag":83.25,
        "MMLU":55.92,
        "TruthfulQA":51.08,
        "Winogrande":77.35,
        "GSM8K":2.05,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":11.0,
        "Available on the Hub":true,
        "Model Sha":"ac1f326ea75a28197c4b8e7c015071e8eef64485"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CalderaAI\/13B-Legerdemain-L2",
        "Average":55.13,
        "ARC":61.26,
        "HellaSwag":83.26,
        "MMLU":56.0,
        "TruthfulQA":41.99,
        "Winogrande":75.22,
        "GSM8K":13.04,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"d6624ce1bcc6b50c86b86e879a8c9822218b84d2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PygmalionAI\/pygmalion-2-13b",
        "Average":55.12,
        "ARC":60.32,
        "HellaSwag":82.37,
        "MMLU":56.02,
        "TruthfulQA":42.22,
        "Winogrande":78.06,
        "GSM8K":11.75,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":29.0,
        "Available on the Hub":true,
        "Model Sha":"3cdc103995ccd5fc7fd2cb5f51f71b510466f5fc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"totally-not-an-llm\/PuddleJumper-13b",
        "Average":55.11,
        "ARC":58.7,
        "HellaSwag":81.18,
        "MMLU":58.25,
        "TruthfulQA":56.44,
        "Winogrande":72.77,
        "GSM8K":3.34,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"f3a8a475ff0c6ae37ac8ae0690980be11cac731a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/WizardLM-1.0-Uncensored-Llama2-13b",
        "Average":55.1,
        "ARC":55.8,
        "HellaSwag":80.41,
        "MMLU":55.59,
        "TruthfulQA":51.42,
        "Winogrande":74.11,
        "GSM8K":13.27,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":36.0,
        "Available on the Hub":true,
        "Model Sha":"134cea14627fd875f6f277cad92f988024855478"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wei123602\/Llama-2-13b-FINETUNE4",
        "Average":55.09,
        "ARC":58.7,
        "HellaSwag":81.93,
        "MMLU":57.21,
        "TruthfulQA":43.26,
        "Winogrande":76.95,
        "GSM8K":12.51,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"939d06081210fa943c60210a47583f43b60901ad"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenAssistant\/llama2-13b-orca-8k-3319",
        "Average":55.09,
        "ARC":60.75,
        "HellaSwag":81.91,
        "MMLU":57.06,
        "TruthfulQA":42.64,
        "Winogrande":77.19,
        "GSM8K":10.99,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":115.0,
        "Available on the Hub":true,
        "Model Sha":"160f58ec85ef25ad935eb583f14c7e8c7f7e7839"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"speechlessai\/speechless-llama2-dolphin-orca-platypus-13b",
        "Average":55.09,
        "ARC":59.64,
        "HellaSwag":82.65,
        "MMLU":57.9,
        "TruthfulQA":43.44,
        "Winogrande":77.19,
        "GSM8K":9.7,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"fd23b7d052eb7c18ecd2acc1be77c66b7b8d6dad"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FlagAlpha\/Llama2-Chinese-13b-Chat",
        "Average":55.07,
        "ARC":55.97,
        "HellaSwag":82.05,
        "MMLU":54.74,
        "TruthfulQA":48.9,
        "Winogrande":76.16,
        "GSM8K":12.59,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":206.0,
        "Available on the Hub":true,
        "Model Sha":"cb69cda10a72bc9736b1c10181ac41f28b69ff9b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jphme\/Llama-2-13b-chat-german",
        "Average":55.07,
        "ARC":57.85,
        "HellaSwag":81.66,
        "MMLU":54.45,
        "TruthfulQA":46.32,
        "Winogrande":76.48,
        "GSM8K":13.65,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":46.0,
        "Available on the Hub":true,
        "Model Sha":"d72667bd92fd6f76835466d302563d213e0b1ee1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-dolphin_20w",
        "Average":55.06,
        "ARC":59.56,
        "HellaSwag":82.55,
        "MMLU":55.89,
        "TruthfulQA":42.67,
        "Winogrande":77.27,
        "GSM8K":12.43,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"c75073d7545a4d222f40dc519021c55a81850d75"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ajibawa-2023\/Python-Code-33B",
        "Average":55.06,
        "ARC":56.31,
        "HellaSwag":81.01,
        "MMLU":54.22,
        "TruthfulQA":44.39,
        "Winogrande":75.22,
        "GSM8K":19.18,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":33.0,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"cf9a561b57145748455fd3e193d2b0e4ae0a0fce"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MayaPH\/GodziLLa-30B",
        "Average":55.05,
        "ARC":61.52,
        "HellaSwag":82.13,
        "MMLU":54.21,
        "TruthfulQA":55.91,
        "Winogrande":76.16,
        "GSM8K":0.38,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"aa9912a2ac60abeac28b4566731cd903dcc582ac"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"WizardLM\/WizardLM-13B-V1.1",
        "Average":55.05,
        "ARC":60.24,
        "HellaSwag":81.39,
        "MMLU":50.92,
        "TruthfulQA":54.56,
        "Winogrande":75.06,
        "GSM8K":8.11,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":69.0,
        "Available on the Hub":true,
        "Model Sha":"badd80f8a6f46fb15310fedf6d4db54959854897"
    },
    {
        "T":"\u2b55",
        "Model":"chargoddard\/llama-2-16b-nastychat",
        "Average":55.04,
        "ARC":57.42,
        "HellaSwag":80.59,
        "MMLU":55.99,
        "TruthfulQA":53.45,
        "Winogrande":74.66,
        "GSM8K":8.11,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":16.19,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"6fb7f82d486b3eee53d750f83cc7eae434349809"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"duliadotio\/dulia-13b-8k-alpha",
        "Average":55.0,
        "ARC":60.67,
        "HellaSwag":82.0,
        "MMLU":56.87,
        "TruthfulQA":42.59,
        "Winogrande":77.19,
        "GSM8K":10.69,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"c3bcafd7f6133a7e7c069f8765a99fe84989d926"
    },
    {
        "T":"\u2b55",
        "Model":"Aspik101\/Redmond-Puffin-13B-instruct-PL-lora_unload",
        "Average":55.0,
        "ARC":60.92,
        "HellaSwag":82.43,
        "MMLU":55.61,
        "TruthfulQA":44.26,
        "Winogrande":75.69,
        "GSM8K":11.07,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"b933009635299bca32c694336aa2007d756a2dda"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openchat\/openchat_v3.2_super",
        "Average":54.99,
        "ARC":59.81,
        "HellaSwag":82.5,
        "MMLU":55.9,
        "TruthfulQA":42.3,
        "Winogrande":75.93,
        "GSM8K":13.5,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":24.0,
        "Available on the Hub":true,
        "Model Sha":"aab7ce4d48b31a295a0116b61569d8e87a09bb7a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ziqingyang\/chinese-alpaca-2-13b",
        "Average":54.99,
        "ARC":58.7,
        "HellaSwag":79.74,
        "MMLU":55.1,
        "TruthfulQA":50.22,
        "Winogrande":75.69,
        "GSM8K":10.46,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.97,
        "Hub \u2764\ufe0f":64.0,
        "Available on the Hub":true,
        "Model Sha":"576094cbf4988baf88b3bb66678be1db70bd720a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"prithivida\/Asimov-7B-v1",
        "Average":54.98,
        "ARC":59.04,
        "HellaSwag":80.04,
        "MMLU":56.35,
        "TruthfulQA":51.15,
        "Winogrande":73.95,
        "GSM8K":9.33,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"4bit",
        "Hub License":"mit",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"0b33ad0a6dde60156ee6008ff47f7cfa6cd27937"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lmsys\/vicuna-13b-v1.5-16k",
        "Average":54.97,
        "ARC":56.74,
        "HellaSwag":80.37,
        "MMLU":55.28,
        "TruthfulQA":51.96,
        "Winogrande":72.38,
        "GSM8K":13.12,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":167.0,
        "Available on the Hub":true,
        "Model Sha":"277697af19d4b267626ebc9f4e078d19a9a0fddf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LLMs\/WizardLM-13B-V1.0",
        "Average":54.97,
        "ARC":57.25,
        "HellaSwag":80.88,
        "MMLU":52.92,
        "TruthfulQA":50.55,
        "Winogrande":74.11,
        "GSM8K":14.1,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"gpl-3.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"f802ea7c01e2da27b0f7091c70d3ecfd8fc042b9"
    },
    {
        "T":"\u2b55",
        "Model":"mistralai\/Mistral-7B-Instruct-v0.1",
        "Average":54.96,
        "ARC":54.52,
        "HellaSwag":75.63,
        "MMLU":55.38,
        "TruthfulQA":56.28,
        "Winogrande":73.72,
        "GSM8K":14.25,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":698.0,
        "Available on the Hub":true,
        "Model Sha":"7961f5aa9b736bf8e364b2e6f201190f97a27931"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/wizardLM-13B-1.0-fp16",
        "Average":54.93,
        "ARC":57.25,
        "HellaSwag":80.88,
        "MMLU":52.9,
        "TruthfulQA":50.55,
        "Winogrande":74.11,
        "GSM8K":13.87,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"b79733805e98e668ff9a459975c259881b1b8014"
    },
    {
        "T":"?",
        "Model":"digitous\/13B-Chimera",
        "Average":54.92,
        "ARC":57.59,
        "HellaSwag":81.5,
        "MMLU":49.86,
        "TruthfulQA":52.59,
        "Winogrande":77.27,
        "GSM8K":10.69,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"85cfe8e6db2bee804873cfdb48955696cc5b0689"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"meta-llama\/Llama-2-13b-chat-hf",
        "Average":54.91,
        "ARC":59.04,
        "HellaSwag":81.94,
        "MMLU":54.64,
        "TruthfulQA":44.12,
        "Winogrande":74.51,
        "GSM8K":15.24,
        "Type":"RL-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":617.0,
        "Available on the Hub":true,
        "Model Sha":"f848cf15ab9a51ae5735ab28120a9a0773eeb541"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NewstaR\/Morningstar-13b-hf",
        "Average":54.91,
        "ARC":59.04,
        "HellaSwag":81.93,
        "MMLU":54.63,
        "TruthfulQA":44.12,
        "Winogrande":74.51,
        "GSM8K":15.24,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"2605b5b3b0ecba906ac26d39aab40f33c2ec81c9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"deepse\/CodeUp-Llama-2-13b-chat-hf",
        "Average":54.91,
        "ARC":59.04,
        "HellaSwag":81.93,
        "MMLU":54.63,
        "TruthfulQA":44.12,
        "Winogrande":74.51,
        "GSM8K":15.24,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"openrail++",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":25.0,
        "Available on the Hub":true,
        "Model Sha":"d4af0b233a5b6a214e96582e103396e99dcf5f95"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Kimiko-v2-13B-fp16",
        "Average":54.91,
        "ARC":61.01,
        "HellaSwag":83.32,
        "MMLU":55.17,
        "TruthfulQA":40.65,
        "Winogrande":76.8,
        "GSM8K":12.51,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"0fed305667508e50330e71a2d43e9cee5ea73783"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"The-Face-Of-Goonery\/Huginn-13b-FP16",
        "Average":54.89,
        "ARC":60.58,
        "HellaSwag":82.53,
        "MMLU":53.71,
        "TruthfulQA":54.46,
        "Winogrande":73.72,
        "GSM8K":4.32,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":11.0,
        "Available on the Hub":true,
        "Model Sha":"69615d9a8e1547f2407afd3380868a99f780e008"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"garage-bAInd\/Platypus2-13B",
        "Average":54.89,
        "ARC":61.26,
        "HellaSwag":82.56,
        "MMLU":56.7,
        "TruthfulQA":44.86,
        "Winogrande":76.87,
        "GSM8K":7.05,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":15.0,
        "Available on the Hub":true,
        "Model Sha":"b5e926e3d6c03e83c7983e87eb71098b5e80a62e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE4_addto15k_4.5w-r16-gate_up_down",
        "Average":54.88,
        "ARC":58.53,
        "HellaSwag":82.27,
        "MMLU":55.9,
        "TruthfulQA":40.26,
        "Winogrande":76.95,
        "GSM8K":15.39,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"fdc145fe1b47cdda483535c018e35a5ab249a552"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/LewdEngine",
        "Average":54.88,
        "ARC":60.49,
        "HellaSwag":83.08,
        "MMLU":54.84,
        "TruthfulQA":43.63,
        "Winogrande":74.9,
        "GSM8K":12.36,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"6e918ff9f563552af4ad66f4308f6d040e24af4b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mikael110\/llama-2-13b-guanaco-fp16",
        "Average":54.86,
        "ARC":60.92,
        "HellaSwag":83.18,
        "MMLU":54.58,
        "TruthfulQA":44.0,
        "Winogrande":74.9,
        "GSM8K":11.6,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":11.0,
        "Available on the Hub":true,
        "Model Sha":"feb7ef47ceca6aec9548264a39622b63fdcb853c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openaccess-ai-collective\/manticore-13b",
        "Average":54.86,
        "ARC":58.7,
        "HellaSwag":81.63,
        "MMLU":50.84,
        "TruthfulQA":49.17,
        "Winogrande":76.64,
        "GSM8K":12.21,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":108.0,
        "Available on the Hub":true,
        "Model Sha":"aed786b0200251c9962ac200c50f7e367f264b46"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/Redmond-Puffin-13B",
        "Average":54.86,
        "ARC":60.41,
        "HellaSwag":83.2,
        "MMLU":55.36,
        "TruthfulQA":42.12,
        "Winogrande":76.64,
        "GSM8K":11.45,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":99.0,
        "Available on the Hub":true,
        "Model Sha":"12af25fa7ea02c4fc636952ea8b9dc9cf48e35be"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/OpenOrcaPlatypus2-Platypus2-13B-QLora-0.80-epoch",
        "Average":54.86,
        "ARC":59.81,
        "HellaSwag":82.69,
        "MMLU":56.96,
        "TruthfulQA":52.92,
        "Winogrande":74.43,
        "GSM8K":2.35,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"5427ceec420f943a0b011a4d96f3efc292306933"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CausalLM\/7B",
        "Average":54.86,
        "ARC":50.0,
        "HellaSwag":74.58,
        "MMLU":61.79,
        "TruthfulQA":50.13,
        "Winogrande":69.69,
        "GSM8K":22.97,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"wtfpl",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":103.0,
        "Available on the Hub":true,
        "Model Sha":"3f4f76e2d94308ea6b0edc3de83f18c213a8fde5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/Samantha-1.11-CodeLlama-34b",
        "Average":54.8,
        "ARC":56.57,
        "HellaSwag":75.47,
        "MMLU":53.51,
        "TruthfulQA":50.46,
        "Winogrande":73.48,
        "GSM8K":19.33,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":41.0,
        "Available on the Hub":true,
        "Model Sha":"3fd110de9282e52f56f999bf1da1a76425f00e29"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wei123602\/llama-13b-FINETUNE3",
        "Average":54.79,
        "ARC":59.3,
        "HellaSwag":81.53,
        "MMLU":57.46,
        "TruthfulQA":41.63,
        "Winogrande":76.72,
        "GSM8K":12.13,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"bacd035db122dafaf86bf52bb9ca8c613070cc58"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/Ensemble5-Platypus2-13B-QLora-0.80-epoch",
        "Average":54.76,
        "ARC":59.73,
        "HellaSwag":82.66,
        "MMLU":56.94,
        "TruthfulQA":52.92,
        "Winogrande":74.43,
        "GSM8K":1.9,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"2af03c3287c60c4ba2fb6afa86c26cf722ab001d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"WizardLM\/WizardLM-13B-V1.2",
        "Average":54.76,
        "ARC":59.04,
        "HellaSwag":82.21,
        "MMLU":54.64,
        "TruthfulQA":47.27,
        "Winogrande":71.9,
        "GSM8K":13.5,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":157.0,
        "Available on the Hub":true,
        "Model Sha":"6760d0c07ffdc2405295ed7a29437cf4dc414bac"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/Redmond-Puffin-13B",
        "Average":54.74,
        "ARC":60.49,
        "HellaSwag":83.21,
        "MMLU":54.95,
        "TruthfulQA":42.08,
        "Winogrande":76.48,
        "GSM8K":11.22,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":99.0,
        "Available on the Hub":true,
        "Model Sha":"12af25fa7ea02c4fc636952ea8b9dc9cf48e35be"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"The-Face-Of-Goonery\/Chronos-Beluga-v2-13bfp16",
        "Average":54.74,
        "ARC":60.75,
        "HellaSwag":81.94,
        "MMLU":54.08,
        "TruthfulQA":53.23,
        "Winogrande":73.8,
        "GSM8K":4.62,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"6d50e6681bc26c9bc0c8377c26c438e295ee0c2f"
    },
    {
        "T":"\u2b55",
        "Model":"Weyaxi\/TekniumAiroboros-Nebula-7B",
        "Average":54.74,
        "ARC":57.17,
        "HellaSwag":81.72,
        "MMLU":55.25,
        "TruthfulQA":51.64,
        "Winogrande":73.24,
        "GSM8K":9.4,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"ef964d514cc25a600b0de78fc469d1acbec34591"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/MythoMix-Platypus2-13B-QLoRA-0.80-epoch",
        "Average":54.74,
        "ARC":60.32,
        "HellaSwag":83.72,
        "MMLU":55.74,
        "TruthfulQA":52.18,
        "Winogrande":75.53,
        "GSM8K":0.91,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"3d91f63d82abd598d5b80d24d74feb6b00b7d80f"
    },
    {
        "T":"\u2b55",
        "Model":"CalderaAI\/13B-Thorns-l2",
        "Average":54.72,
        "ARC":62.88,
        "HellaSwag":83.57,
        "MMLU":56.95,
        "TruthfulQA":49.52,
        "Winogrande":74.51,
        "GSM8K":0.91,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"adc5e7befcc3d0a26f46198fdda4a098a2742fe6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Medusa-13b",
        "Average":54.72,
        "ARC":58.19,
        "HellaSwag":81.35,
        "MMLU":57.39,
        "TruthfulQA":51.24,
        "Winogrande":73.32,
        "GSM8K":6.82,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"be755c9eef8233ca59e0178db75de878f5859222"
    },
    {
        "T":"\u2b55",
        "Model":"abacusai\/Giraffe-beta-13b-32k",
        "Average":54.69,
        "ARC":55.63,
        "HellaSwag":80.42,
        "MMLU":53.61,
        "TruthfulQA":42.58,
        "Winogrande":74.59,
        "GSM8K":21.3,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"259f3fe9ebbff7532498f44286f253d56699da6f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/LLaMA2-13B-Psyfighter2",
        "Average":54.66,
        "ARC":60.07,
        "HellaSwag":84.02,
        "MMLU":55.07,
        "TruthfulQA":53.0,
        "Winogrande":74.35,
        "GSM8K":1.44,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"cc51a4e64b0821feda101dc04737486b4ff60735"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-codellama-34b-v1.9",
        "Average":54.64,
        "ARC":54.27,
        "HellaSwag":75.2,
        "MMLU":56.12,
        "TruthfulQA":43.92,
        "Winogrande":73.56,
        "GSM8K":24.79,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"68aad9f8452b2abf7d5415d48c09bd55d5b7ca05"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE5_4w-r8-q_k_v_o",
        "Average":54.64,
        "ARC":57.25,
        "HellaSwag":81.73,
        "MMLU":55.72,
        "TruthfulQA":41.53,
        "Winogrande":77.58,
        "GSM8K":14.03,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"209da26cff560ab34064f277190ab63f8c970b93"
    },
    {
        "T":"\u2b55",
        "Model":"Secbone\/llama-2-13B-instructed",
        "Average":54.63,
        "ARC":59.39,
        "HellaSwag":83.88,
        "MMLU":55.57,
        "TruthfulQA":46.89,
        "Winogrande":74.03,
        "GSM8K":8.04,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"e676fbd9015beacfba5d71426beace7605200477"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE5_4w-r16-q_k_v_o",
        "Average":54.63,
        "ARC":58.7,
        "HellaSwag":81.66,
        "MMLU":53.87,
        "TruthfulQA":43.02,
        "Winogrande":76.72,
        "GSM8K":13.8,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"33fd8a46a711ab8c45698dae9601678dfd7b3d33"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/UltraLM-13B-fp16",
        "Average":54.62,
        "ARC":57.59,
        "HellaSwag":80.2,
        "MMLU":51.85,
        "TruthfulQA":51.56,
        "Winogrande":75.85,
        "GSM8K":10.69,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"734f5641f6c548474517d1536c46024517f120e0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Chat-Stheno-L2-13B",
        "Average":54.61,
        "ARC":58.45,
        "HellaSwag":80.96,
        "MMLU":54.8,
        "TruthfulQA":43.31,
        "Winogrande":75.37,
        "GSM8K":14.78,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"20419fdd5b4bdcbbf075223c33b396958c48a6cf"
    },
    {
        "T":"\u2b55",
        "Model":"dhmeltzer\/Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16",
        "Average":54.61,
        "ARC":60.41,
        "HellaSwag":82.58,
        "MMLU":55.86,
        "TruthfulQA":43.61,
        "Winogrande":76.72,
        "GSM8K":8.49,
        "Type":"instruction-tuned",
        "Architecture":"?",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"891be2d8f205baa04c8a92f6ab1225f0d0c3e5bd"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/Nous-Hermes-Platypus2-13B-QLoRA-0.80-epoch",
        "Average":54.6,
        "ARC":59.9,
        "HellaSwag":83.29,
        "MMLU":56.69,
        "TruthfulQA":51.08,
        "Winogrande":75.22,
        "GSM8K":1.44,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"6e49d3d205e7f2e15c01ace0901da8931bbaab3b"
    },
    {
        "T":"\u2b55",
        "Model":"Weyaxi\/Samantha-Nebula-7B",
        "Average":54.58,
        "ARC":57.0,
        "HellaSwag":82.25,
        "MMLU":54.21,
        "TruthfulQA":49.58,
        "Winogrande":73.09,
        "GSM8K":11.37,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"a7d4b8a1683e33dd3c60064d7dd9d5c35691323f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gaodrew\/OpenOrca-Platypus2-13B-thera-1250",
        "Average":54.56,
        "ARC":59.22,
        "HellaSwag":81.02,
        "MMLU":57.04,
        "TruthfulQA":48.43,
        "Winogrande":73.09,
        "GSM8K":8.57,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"b1c2ebcda387211732e87911e39edca503502a33"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"microsoft\/Orca-2-7b",
        "Average":54.55,
        "ARC":54.1,
        "HellaSwag":76.19,
        "MMLU":56.37,
        "TruthfulQA":52.45,
        "Winogrande":73.48,
        "GSM8K":14.71,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":49.0,
        "Available on the Hub":true,
        "Model Sha":"60e31e6bdcf582ad103b807cb74b73ee1d2c4b17"
    },
    {
        "T":"\u2b55",
        "Model":"KoboldAI\/LLaMA2-13B-Holomax",
        "Average":54.52,
        "ARC":60.49,
        "HellaSwag":82.86,
        "MMLU":54.67,
        "TruthfulQA":42.97,
        "Winogrande":74.66,
        "GSM8K":11.45,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":12.0,
        "Available on the Hub":true,
        "Model Sha":"2c4fddeb097636d6462b7628a8e053ad3ff4678c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/LLaMA2-13B-Tiefighter",
        "Average":54.51,
        "ARC":59.9,
        "HellaSwag":84.0,
        "MMLU":54.98,
        "TruthfulQA":53.02,
        "Winogrande":74.51,
        "GSM8K":0.68,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":22.0,
        "Available on the Hub":true,
        "Model Sha":"0d193a4562d6836724485cb7df6e58ca846bbfeb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"chansung\/gpt4-alpaca-lora-13b-decapoda-1024",
        "Average":54.51,
        "ARC":59.39,
        "HellaSwag":81.87,
        "MMLU":47.75,
        "TruthfulQA":52.59,
        "Winogrande":77.35,
        "GSM8K":8.11,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":false,
        "Model Sha":"7aedafea409de07a997d70a84e30242c7b86877c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r16-q_k_v_o_gate_up_down",
        "Average":54.5,
        "ARC":59.22,
        "HellaSwag":81.52,
        "MMLU":54.94,
        "TruthfulQA":42.83,
        "Winogrande":76.87,
        "GSM8K":11.6,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"a759c4fae8dc5fcd264bf58b89b9fd13d06784ae"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/Platypus2-13B-LoRa",
        "Average":54.48,
        "ARC":60.67,
        "HellaSwag":82.5,
        "MMLU":56.34,
        "TruthfulQA":43.91,
        "Winogrande":75.93,
        "GSM8K":7.51,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"1450c541cf9e378e81862fabeb234b8e0a2bdf5a"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/Limarp-Platypus2-13B-QLoRA-0.80-epoch",
        "Average":54.46,
        "ARC":60.49,
        "HellaSwag":82.76,
        "MMLU":56.52,
        "TruthfulQA":44.14,
        "Winogrande":76.8,
        "GSM8K":6.07,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"0a8560232ff73ca3c3f8e217b4517fa6c4f55558"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wei123602\/Llama-2-13b-FINETUNE4_TEST2",
        "Average":54.46,
        "ARC":58.45,
        "HellaSwag":81.7,
        "MMLU":56.61,
        "TruthfulQA":40.19,
        "Winogrande":76.64,
        "GSM8K":13.19,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"e312c4c59cab9d130c33288c92aad7c0cb5331d5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bhenrym14\/airophin-13b-pntk-16k-fp16",
        "Average":54.44,
        "ARC":61.18,
        "HellaSwag":82.86,
        "MMLU":55.19,
        "TruthfulQA":43.2,
        "Winogrande":76.16,
        "GSM8K":8.04,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"6b5418b69e8270df659eacb192f469e7c3af70b3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NekoPunchBBB\/Llama-2-13b-hf_Open-Platypus-QLoRA-multigpu",
        "Average":54.4,
        "ARC":57.51,
        "HellaSwag":82.49,
        "MMLU":54.83,
        "TruthfulQA":43.81,
        "Winogrande":77.27,
        "GSM8K":10.46,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f65029ea8f030731ace568e40bab33a7097a13de"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE1_17w-r16",
        "Average":54.37,
        "ARC":57.25,
        "HellaSwag":82.27,
        "MMLU":56.16,
        "TruthfulQA":39.75,
        "Winogrande":77.43,
        "GSM8K":13.34,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"5da5c92f3cf85a62c1be90a0bb2ae8dffce64a7d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-huangyt_Fintune_1_17w-q_k_v_o_proj",
        "Average":54.35,
        "ARC":59.73,
        "HellaSwag":81.06,
        "MMLU":54.53,
        "TruthfulQA":38.64,
        "Winogrande":78.14,
        "GSM8K":14.03,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"aeeded8db9eea97e2e6a2e19a006ce1acd110a82"
    },
    {
        "T":"?",
        "Model":"TheBloke\/orca_mini_v3_7B-GPTQ",
        "Average":54.35,
        "ARC":54.52,
        "HellaSwag":78.53,
        "MMLU":51.85,
        "TruthfulQA":51.2,
        "Winogrande":74.66,
        "GSM8K":15.31,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"GPTQ",
        "Hub License":"other",
        "#Params (B)":9.05,
        "Hub \u2764\ufe0f":9.0,
        "Available on the Hub":true,
        "Model Sha":"4f06a6151128861d5bb256275620f7eadcab3238"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Lajonbot\/Llama-2-13b-hf-instruct-pl-lora_unload",
        "Average":54.34,
        "ARC":59.47,
        "HellaSwag":82.16,
        "MMLU":54.83,
        "TruthfulQA":41.45,
        "Winogrande":76.24,
        "GSM8K":11.9,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"4ef2c736641c2983996c4662bf481782a9de5055"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bofenghuang\/vigogne-13b-instruct",
        "Average":54.34,
        "ARC":57.94,
        "HellaSwag":81.32,
        "MMLU":47.62,
        "TruthfulQA":50.23,
        "Winogrande":77.11,
        "GSM8K":11.83,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"openrail",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":13.0,
        "Available on the Hub":true,
        "Model Sha":"a13e08a36c355d64fae59f28162e5fa542a8d235"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"anhnv125\/llama-op-v4",
        "Average":54.34,
        "ARC":61.52,
        "HellaSwag":79.21,
        "MMLU":57.01,
        "TruthfulQA":42.72,
        "Winogrande":75.93,
        "GSM8K":9.63,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"6cd644049de2b944beaefcc6aa34965c00e08529"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r16-gate_up_down",
        "Average":54.32,
        "ARC":58.7,
        "HellaSwag":81.89,
        "MMLU":56.08,
        "TruthfulQA":38.95,
        "Winogrande":77.35,
        "GSM8K":12.96,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"4c3a4cb54c0487666bd58589b50f90c22de80969"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"garage-bAInd\/Camel-Platypus2-13B",
        "Average":54.32,
        "ARC":60.75,
        "HellaSwag":83.61,
        "MMLU":56.51,
        "TruthfulQA":49.6,
        "Winogrande":75.37,
        "GSM8K":0.08,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"0480a52799cb8e8de73bb41994df8b6b793937c7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-Open-Platypus_2.5w",
        "Average":54.32,
        "ARC":59.56,
        "HellaSwag":82.46,
        "MMLU":56.06,
        "TruthfulQA":42.45,
        "Winogrande":76.8,
        "GSM8K":8.57,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"bc55678af8226e1323305f743a4882da31994e0c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"StudentLLM\/Alpagasus-2-13b-QLoRA-merged",
        "Average":54.31,
        "ARC":61.09,
        "HellaSwag":82.46,
        "MMLU":55.27,
        "TruthfulQA":38.53,
        "Winogrande":77.35,
        "GSM8K":11.14,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"dacbafa40716a2d87e593240cc5c1dc883b5066a"
    },
    {
        "T":"?",
        "Model":"TheBloke\/WizardLM-13B-V1.1-GPTQ",
        "Average":54.28,
        "ARC":58.53,
        "HellaSwag":80.66,
        "MMLU":49.59,
        "TruthfulQA":54.35,
        "Winogrande":74.43,
        "GSM8K":8.11,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":16.22,
        "Hub \u2764\ufe0f":26.0,
        "Available on the Hub":true,
        "Model Sha":"9df807ac64034bc6e7387326689d6e39656ce5e0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openaccess-ai-collective\/wizard-mega-13b",
        "Average":54.27,
        "ARC":57.34,
        "HellaSwag":81.09,
        "MMLU":50.59,
        "TruthfulQA":50.22,
        "Winogrande":76.32,
        "GSM8K":10.08,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":103.0,
        "Available on the Hub":true,
        "Model Sha":"76e90314541be6cfa2b55208831c99f1351c1a33"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lmsys\/vicuna-13b-v1.3",
        "Average":54.27,
        "ARC":54.61,
        "HellaSwag":80.41,
        "MMLU":52.88,
        "TruthfulQA":52.14,
        "Winogrande":74.82,
        "GSM8K":10.77,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":160.0,
        "Available on the Hub":true,
        "Model Sha":"7900eeb715a49affee9e6390f824e62eea3f3fb1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"layoric\/llama-2-13b-code-alpaca",
        "Average":54.25,
        "ARC":60.84,
        "HellaSwag":82.14,
        "MMLU":55.93,
        "TruthfulQA":38.27,
        "Winogrande":76.4,
        "GSM8K":11.9,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"aa1d543fe3391fe9f0e6143ef785fffe9c871225"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"digitous\/13B-HyperMantis",
        "Average":54.25,
        "ARC":58.53,
        "HellaSwag":82.2,
        "MMLU":50.61,
        "TruthfulQA":47.5,
        "Winogrande":76.24,
        "GSM8K":10.39,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":26.0,
        "Available on the Hub":true,
        "Model Sha":"aa828ef92c363a5577ffd7d29e678277b9d2eb3c"
    },
    {
        "T":"\u2b55",
        "Model":"totally-not-an-llm\/EverythingLM-13b-V3-peft",
        "Average":54.24,
        "ARC":58.36,
        "HellaSwag":81.03,
        "MMLU":54.7,
        "TruthfulQA":52.98,
        "Winogrande":72.85,
        "GSM8K":5.53,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"7a2eed5038addcf4fa3b8dd358b45eb96134e749"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/Platypus2-13B-IA3",
        "Average":54.23,
        "ARC":61.09,
        "HellaSwag":82.65,
        "MMLU":56.32,
        "TruthfulQA":38.35,
        "Winogrande":75.69,
        "GSM8K":11.3,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"b738c64d536df02f5c137a94bc7a32a4c486012b"
    },
    {
        "T":"\u2b55",
        "Model":"lgaalves\/llama-2-13b-hf-platypus",
        "Average":54.22,
        "ARC":58.87,
        "HellaSwag":82.14,
        "MMLU":54.98,
        "TruthfulQA":42.84,
        "Winogrande":77.11,
        "GSM8K":9.4,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"39e07f6213a64d79cf31e9c0773dea6224f7f021"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NekoPunchBBB\/Llama-2-13b-hf_Open-Platypus",
        "Average":54.22,
        "ARC":58.87,
        "HellaSwag":82.14,
        "MMLU":54.98,
        "TruthfulQA":42.84,
        "Winogrande":77.11,
        "GSM8K":9.4,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"c318a24121bd69509f395e17a9636093213ece21"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FPHam\/Free_Sydney_13b_HF",
        "Average":54.22,
        "ARC":59.39,
        "HellaSwag":81.4,
        "MMLU":53.73,
        "TruthfulQA":45.63,
        "Winogrande":76.01,
        "GSM8K":9.17,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"5474ecbccd1f2a2cda9f77a157993f55c97377ed"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"budecosystem\/genz-13b-v2",
        "Average":54.2,
        "ARC":55.97,
        "HellaSwag":79.98,
        "MMLU":54.3,
        "TruthfulQA":48.09,
        "Winogrande":74.59,
        "GSM8K":12.28,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"98e0e2086df11b9f80e1571110540a657e52c2e8"
    },
    {
        "T":"\u2b55",
        "Model":"StudentLLM\/Alpagasus-2-13b-QLoRA-merged",
        "Average":54.2,
        "ARC":60.84,
        "HellaSwag":82.43,
        "MMLU":55.55,
        "TruthfulQA":38.65,
        "Winogrande":76.87,
        "GSM8K":10.84,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"e324e828c8d68aa8510f50dfab133388a44fd821"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/llama2-13b-math1.2",
        "Average":54.19,
        "ARC":56.91,
        "HellaSwag":80.71,
        "MMLU":53.21,
        "TruthfulQA":48.25,
        "Winogrande":74.74,
        "GSM8K":11.3,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"b05b4c22893e950e8e33acb67087a9acc8f0ab97"
    },
    {
        "T":"\u2b55",
        "Model":"totally-not-an-llm\/PuddleJumper-13b-V2",
        "Average":54.19,
        "ARC":57.0,
        "HellaSwag":81.06,
        "MMLU":58.3,
        "TruthfulQA":52.66,
        "Winogrande":72.45,
        "GSM8K":3.64,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"1fe9494e334a32ba73dc2926f58246450850c534"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/llama2-13b-math1.1",
        "Average":54.18,
        "ARC":57.25,
        "HellaSwag":80.74,
        "MMLU":53.56,
        "TruthfulQA":48.43,
        "Winogrande":74.43,
        "GSM8K":10.69,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"3c4d83d3525e54a493ff510443fdcca44bf63b59"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE1_17w-r4",
        "Average":54.18,
        "ARC":56.74,
        "HellaSwag":82.27,
        "MMLU":56.18,
        "TruthfulQA":39.65,
        "Winogrande":77.03,
        "GSM8K":13.19,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"7e0046627fabb0f23ace4b71f279d459ec4a0ff1"
    },
    {
        "T":"\u2b55",
        "Model":"dhmeltzer\/Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16_merged",
        "Average":54.16,
        "ARC":59.13,
        "HellaSwag":82.13,
        "MMLU":54.98,
        "TruthfulQA":44.23,
        "Winogrande":76.4,
        "GSM8K":8.11,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"aad13bce3b243721e52e9cda479f1102dda99f12"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TehVenom\/Metharme-13b-Merged",
        "Average":54.15,
        "ARC":59.9,
        "HellaSwag":81.12,
        "MMLU":47.18,
        "TruthfulQA":51.18,
        "Winogrande":76.8,
        "GSM8K":8.72,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"90c02cc338afcdd890a948af06432674743363ad"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/llama2-13b-math1.1",
        "Average":54.14,
        "ARC":56.83,
        "HellaSwag":80.69,
        "MMLU":53.43,
        "TruthfulQA":48.48,
        "Winogrande":74.74,
        "GSM8K":10.69,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"3c4d83d3525e54a493ff510443fdcca44bf63b59"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/Wizard-Vicuna-13B-Uncensored",
        "Average":54.14,
        "ARC":58.96,
        "HellaSwag":81.95,
        "MMLU":47.92,
        "TruthfulQA":51.69,
        "Winogrande":75.69,
        "GSM8K":8.64,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":218.0,
        "Available on the Hub":true,
        "Model Sha":"95bfd1640a54e76b3e857c2462fd3a77eca0b275"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Wizard-Vicuna-13B-Uncensored-HF",
        "Average":54.14,
        "ARC":58.96,
        "HellaSwag":81.95,
        "MMLU":47.92,
        "TruthfulQA":51.69,
        "Winogrande":75.69,
        "GSM8K":8.64,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":197.0,
        "Available on the Hub":true,
        "Model Sha":"fff9ac7f0e2e7b340f2301f5f089d989fc03be67"
    },
    {
        "T":"\u2b55",
        "Model":"dhmeltzer\/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16",
        "Average":54.14,
        "ARC":59.98,
        "HellaSwag":82.43,
        "MMLU":55.41,
        "TruthfulQA":39.9,
        "Winogrande":76.56,
        "GSM8K":10.54,
        "Type":"instruction-tuned",
        "Architecture":"?",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"6a0a2b6672c7b36c714a66c4a836e0b50c6cb5e6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-orca-platypus-coig-lite-4k-0.5e-13b",
        "Average":54.13,
        "ARC":58.02,
        "HellaSwag":80.15,
        "MMLU":57.26,
        "TruthfulQA":48.04,
        "Winogrande":75.45,
        "GSM8K":5.84,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"081d1da5cfa2f6ad43abdf4fb5e41f8ec5846224"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openaccess-ai-collective\/manticore-13b-chat-pyg",
        "Average":54.13,
        "ARC":58.53,
        "HellaSwag":81.96,
        "MMLU":48.76,
        "TruthfulQA":48.76,
        "Winogrande":77.19,
        "GSM8K":9.55,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":24.0,
        "Available on the Hub":true,
        "Model Sha":"f9ef65a3cf50e3c09ccb443f99225148e08517aa"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CalderaAI\/13B-BlueMethod",
        "Average":54.12,
        "ARC":59.64,
        "HellaSwag":82.07,
        "MMLU":50.34,
        "TruthfulQA":47.74,
        "Winogrande":77.11,
        "GSM8K":7.81,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"315aa0924dd42840b8cced581c9db1240f9bae1d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-huangyt_Fintune_1_17w-gate_up_down_proj",
        "Average":54.12,
        "ARC":57.17,
        "HellaSwag":82.26,
        "MMLU":55.89,
        "TruthfulQA":39.93,
        "Winogrande":76.56,
        "GSM8K":12.89,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"c1a5ad1b5e490ed860eeb1b449a02e14da10717f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"IkariDev\/Athena-v1",
        "Average":54.11,
        "ARC":60.07,
        "HellaSwag":82.64,
        "MMLU":55.61,
        "TruthfulQA":46.58,
        "Winogrande":74.82,
        "GSM8K":4.93,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":11.0,
        "Available on the Hub":true,
        "Model Sha":"8f96e561c8c795e383ca0faeb1696fa1e33e87de"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"chickencaesar\/llama2-platypus-llama2-chat-13B-hf",
        "Average":54.11,
        "ARC":62.97,
        "HellaSwag":82.75,
        "MMLU":56.86,
        "TruthfulQA":42.93,
        "Winogrande":76.32,
        "GSM8K":2.81,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"e35bb473156d74c8b5ad23a5e9df815891e8139a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Fredithefish\/Guanaco-13B-Uncensored",
        "Average":54.1,
        "ARC":59.56,
        "HellaSwag":82.7,
        "MMLU":53.65,
        "TruthfulQA":43.26,
        "Winogrande":76.32,
        "GSM8K":9.1,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":9.0,
        "Available on the Hub":true,
        "Model Sha":"cf315234979f5924ad73399bcdcdf51b05a1fc98"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/llama-2-13b-Beluga-QLoRA",
        "Average":54.09,
        "ARC":59.22,
        "HellaSwag":81.92,
        "MMLU":56.67,
        "TruthfulQA":48.23,
        "Winogrande":77.19,
        "GSM8K":1.29,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"c0d3c0a5d4e9001ea933c6b71ca3adc99d1f71a2"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"01-ai\/Yi-6B",
        "Average":54.08,
        "ARC":55.55,
        "HellaSwag":76.57,
        "MMLU":64.11,
        "TruthfulQA":41.96,
        "Winogrande":74.19,
        "GSM8K":12.13,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.06,
        "Hub \u2764\ufe0f":287.0,
        "Available on the Hub":true,
        "Model Sha":"e00f7cbde45745a22625ac85c6ad5d5b9f27098d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r16-q_k_v_o",
        "Average":54.08,
        "ARC":59.3,
        "HellaSwag":81.2,
        "MMLU":55.58,
        "TruthfulQA":38.13,
        "Winogrande":76.8,
        "GSM8K":13.5,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"71224344025dbfada6821c6a89cade1d8358dad1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"stabilityai\/StableBeluga1-Delta",
        "Average":54.08,
        "ARC":68.17,
        "HellaSwag":85.88,
        "MMLU":64.83,
        "TruthfulQA":55.81,
        "Winogrande":49.8,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":65.29,
        "Hub \u2764\ufe0f":54.0,
        "Available on the Hub":false,
        "Model Sha":"40a78d91d43ad9aef6663ff15ddc15be9922bce5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bhenrym14\/airophin-v2-13b-PI-8k-fp16",
        "Average":54.07,
        "ARC":60.58,
        "HellaSwag":82.96,
        "MMLU":56.75,
        "TruthfulQA":40.14,
        "Winogrande":76.64,
        "GSM8K":7.35,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"26b7edfd282af223d86d5e539451357bb114247b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r8-q_k_v_o",
        "Average":54.06,
        "ARC":57.68,
        "HellaSwag":81.91,
        "MMLU":54.95,
        "TruthfulQA":41.31,
        "Winogrande":76.48,
        "GSM8K":12.05,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f76f93dad8408523e69c59abbb96ce6b1b9b9f69"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/ReMM-L2-13B-PIPPA",
        "Average":54.06,
        "ARC":59.73,
        "HellaSwag":83.12,
        "MMLU":54.1,
        "TruthfulQA":49.94,
        "Winogrande":74.51,
        "GSM8K":2.96,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"79e711178c6881496ae1f5635b08bc193f370709"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/ReMM-L2-13B",
        "Average":54.06,
        "ARC":59.73,
        "HellaSwag":83.1,
        "MMLU":54.11,
        "TruthfulQA":49.94,
        "Winogrande":74.51,
        "GSM8K":2.96,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"c4710577003a23ca8e9040d16dfb8f3e9bc5d636"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/llama2-13b-math1.2",
        "Average":54.05,
        "ARC":57.08,
        "HellaSwag":80.61,
        "MMLU":53.05,
        "TruthfulQA":48.3,
        "Winogrande":74.27,
        "GSM8K":10.99,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"b05b4c22893e950e8e33acb67087a9acc8f0ab97"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wei123602\/Llama-2-13b-FINETUNE4_compare8k2",
        "Average":54.05,
        "ARC":58.28,
        "HellaSwag":81.39,
        "MMLU":56.87,
        "TruthfulQA":39.86,
        "Winogrande":76.01,
        "GSM8K":11.9,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"fe1b604097aad9408ce63fa7ffc9c320cdd06e4f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/airoboros-13B-HF",
        "Average":54.05,
        "ARC":58.28,
        "HellaSwag":81.05,
        "MMLU":50.03,
        "TruthfulQA":51.57,
        "Winogrande":76.24,
        "GSM8K":7.13,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":12.0,
        "Available on the Hub":true,
        "Model Sha":"9219b61a0e8bc880e4cd0f8bebc48a97ee0950c7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/Nous-Hermes-13b",
        "Average":54.04,
        "ARC":56.57,
        "HellaSwag":82.11,
        "MMLU":50.44,
        "TruthfulQA":51.5,
        "Winogrande":75.3,
        "GSM8K":8.34,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"gpl",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":388.0,
        "Available on the Hub":true,
        "Model Sha":"24e8c03148ffd1f3e469744dfc24ad2ad82848f8"
    },
    {
        "T":"\u2b55",
        "Model":"The-Face-Of-Goonery\/Huginn-13b-V4",
        "Average":54.04,
        "ARC":60.67,
        "HellaSwag":82.34,
        "MMLU":52.32,
        "TruthfulQA":50.62,
        "Winogrande":73.64,
        "GSM8K":4.62,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"6186feee849e0c2b7e62d4cbdc4cdc48260ac684"
    },
    {
        "T":"\u2b55",
        "Model":"The-Face-Of-Goonery\/Huginn-13b-v4.5",
        "Average":54.04,
        "ARC":60.67,
        "HellaSwag":82.34,
        "MMLU":52.32,
        "TruthfulQA":50.62,
        "Winogrande":73.64,
        "GSM8K":4.62,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"f3be56d8bf71a8d3905974b1e5fcba7336b02159"
    },
    {
        "T":"\u2b55",
        "Model":"The-Face-Of-Goonery\/Huginn-v3-13b",
        "Average":54.04,
        "ARC":60.67,
        "HellaSwag":82.34,
        "MMLU":52.32,
        "TruthfulQA":50.62,
        "Winogrande":73.64,
        "GSM8K":4.62,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":11.0,
        "Available on the Hub":true,
        "Model Sha":"6c2faf828c5380d28c51fcb4d3d0f1a420fb9a9a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-13b",
        "Average":54.02,
        "ARC":58.28,
        "HellaSwag":81.05,
        "MMLU":50.03,
        "TruthfulQA":51.57,
        "Winogrande":76.24,
        "GSM8K":6.97,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":93.0,
        "Available on the Hub":true,
        "Model Sha":"44830f9e1559f318f5dad875bab40d1d1beddbfc"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"01-ai\/Yi-6B",
        "Average":54.02,
        "ARC":55.55,
        "HellaSwag":76.42,
        "MMLU":63.85,
        "TruthfulQA":41.86,
        "Winogrande":73.8,
        "GSM8K":12.66,
        "Type":"pretrained",
        "Architecture":"YiForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"custom",
        "#Params (B)":6.0,
        "Hub \u2764\ufe0f":15.0,
        "Available on the Hub":true,
        "Model Sha":"d8029c814d8faa68e1aef2e488f668a3af5d1a8a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dvruette\/llama-13b-pretrained",
        "Average":54.02,
        "ARC":56.31,
        "HellaSwag":79.32,
        "MMLU":47.03,
        "TruthfulQA":48.42,
        "Winogrande":76.95,
        "GSM8K":16.07,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"c28cc0cf5a1a1bf4de96b23d06b02129dca85eb9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE5_4w-r8-gate_up_down",
        "Average":54.02,
        "ARC":57.17,
        "HellaSwag":82.15,
        "MMLU":54.88,
        "TruthfulQA":40.23,
        "Winogrande":76.32,
        "GSM8K":13.34,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"86adab5c098c9338e098a8e5b0188b0aa39b2478"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/MythicalDestroyerV2-Platypus2-13B-QLora-0.80-epoch",
        "Average":54.01,
        "ARC":57.34,
        "HellaSwag":81.24,
        "MMLU":55.64,
        "TruthfulQA":55.98,
        "Winogrande":73.88,
        "GSM8K":0.0,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"ada55b32fe8ed55b7691d997ad2e86f232c91aad"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/based-30b",
        "Average":54.0,
        "ARC":63.91,
        "HellaSwag":85.67,
        "MMLU":58.28,
        "TruthfulQA":35.7,
        "Winogrande":80.11,
        "GSM8K":0.3,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":39.0,
        "Available on the Hub":true,
        "Model Sha":"5818a6344f48dc5a324589b57cb288a9d54c0b79"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-orca-platypus-coig-lite-4k-0.6e-13b",
        "Average":53.99,
        "ARC":58.79,
        "HellaSwag":79.93,
        "MMLU":56.77,
        "TruthfulQA":48.29,
        "Winogrande":75.93,
        "GSM8K":4.25,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"6bf4cf6211489bdbea70585a4a5c0f39deefb4e5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r8-q_k_v_o",
        "Average":53.99,
        "ARC":56.06,
        "HellaSwag":81.89,
        "MMLU":55.04,
        "TruthfulQA":40.12,
        "Winogrande":76.56,
        "GSM8K":14.25,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f907fffbb08698040325b3f2e47200a1b48b3ed9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/gpt4-alpaca-lora-13B-HF",
        "Average":53.98,
        "ARC":59.56,
        "HellaSwag":82.09,
        "MMLU":47.48,
        "TruthfulQA":48.96,
        "Winogrande":76.72,
        "GSM8K":9.1,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"49678a2dd15fb4e1f1b99616ccc1ffd269912833"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KnutJaegersberg\/webMistral-7B",
        "Average":53.97,
        "ARC":59.04,
        "HellaSwag":80.89,
        "MMLU":59.0,
        "TruthfulQA":39.71,
        "Winogrande":76.32,
        "GSM8K":8.87,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"0b221c617df3d2f883cfd925f646ebd93de23037"
    },
    {
        "T":"\u2b55",
        "Model":"WizardLM\/WizardMath-13B-V1.0",
        "Average":53.97,
        "ARC":60.07,
        "HellaSwag":82.01,
        "MMLU":54.8,
        "TruthfulQA":42.7,
        "Winogrande":71.9,
        "GSM8K":12.36,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":16.0,
        "Available on the Hub":true,
        "Model Sha":"209316bea6eab73d8b18fca2a730b1dff3dcf999"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openaccess-ai-collective\/minotaur-13b",
        "Average":53.97,
        "ARC":56.4,
        "HellaSwag":79.13,
        "MMLU":49.61,
        "TruthfulQA":49.62,
        "Winogrande":76.56,
        "GSM8K":12.51,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":9.0,
        "Available on the Hub":true,
        "Model Sha":"b5ae4519d4c8f4559a0aa80b6efe2008413ece01"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openchat\/openchat_v2_w",
        "Average":53.96,
        "ARC":57.34,
        "HellaSwag":81.23,
        "MMLU":50.17,
        "TruthfulQA":50.7,
        "Winogrande":75.93,
        "GSM8K":8.42,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":29.0,
        "Available on the Hub":true,
        "Model Sha":"0eb53946b8fac30606dc72541f2fc073cb6a0e12"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openchat\/openchat_v2",
        "Average":53.96,
        "ARC":57.17,
        "HellaSwag":81.14,
        "MMLU":50.58,
        "TruthfulQA":49.54,
        "Winogrande":76.24,
        "GSM8K":9.1,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":12.0,
        "Available on the Hub":true,
        "Model Sha":"bd2a0968964c0f2dfae8f5a8950b43e35142f830"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wei123602\/Llama-2-13b-FINETUNE4_TEST3",
        "Average":53.95,
        "ARC":59.04,
        "HellaSwag":81.65,
        "MMLU":56.37,
        "TruthfulQA":39.98,
        "Winogrande":75.45,
        "GSM8K":11.22,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"e81b5d4550224711929fdea4effdd990cc0c7404"
    },
    {
        "T":"\u2b55",
        "Model":"Weyaxi\/Platypus-Nebula-v2-7B",
        "Average":53.95,
        "ARC":55.38,
        "HellaSwag":83.02,
        "MMLU":56.07,
        "TruthfulQA":46.94,
        "Winogrande":72.22,
        "GSM8K":10.08,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"2d95180bae03c0b268dff44a1f9806fc295adc09"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-huangyt_FINETUNE2_3w-gate_up_down_proj",
        "Average":53.95,
        "ARC":57.42,
        "HellaSwag":82.42,
        "MMLU":55.57,
        "TruthfulQA":39.19,
        "Winogrande":77.03,
        "GSM8K":12.05,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"469c6674ad2190b639d6f5ce6bfecc1463825dfb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE4_compare15k_4.5w-r16-gate_up_down",
        "Average":53.94,
        "ARC":58.36,
        "HellaSwag":82.33,
        "MMLU":56.14,
        "TruthfulQA":39.51,
        "Winogrande":76.4,
        "GSM8K":10.92,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"d824054153586d58139b7c3527ba211f33a81382"
    },
    {
        "T":"\u2b55",
        "Model":"Voicelab\/trurl-2-13b-academic",
        "Average":53.94,
        "ARC":57.94,
        "HellaSwag":79.55,
        "MMLU":55.2,
        "TruthfulQA":43.46,
        "Winogrande":76.56,
        "GSM8K":10.92,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"2e95049edf02368bbd4b4f6ffb50bc8821e919bb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JosephusCheung\/Pwen-7B-Chat-20_30",
        "Average":53.93,
        "ARC":51.45,
        "HellaSwag":73.99,
        "MMLU":62.08,
        "TruthfulQA":47.01,
        "Winogrande":68.43,
        "GSM8K":20.62,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"e6c38a7d2f4ba7b867fff421c08c02ba1908224e"
    },
    {
        "T":"\u2b55",
        "Model":"euclaise\/Ferret-7B",
        "Average":53.93,
        "ARC":62.29,
        "HellaSwag":81.31,
        "MMLU":60.27,
        "TruthfulQA":40.01,
        "Winogrande":77.66,
        "GSM8K":2.05,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"b1ef5adff5ceb06d2d9808bccf5e06705f9e19dc"
    },
    {
        "T":"\u2b55",
        "Model":"lgaalves\/llama-2-13b-chat-platypus",
        "Average":53.92,
        "ARC":53.84,
        "HellaSwag":80.67,
        "MMLU":54.44,
        "TruthfulQA":46.23,
        "Winogrande":76.01,
        "GSM8K":12.36,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"828aa1020fc7d394fe8ee2c596e3211df7656eac"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bhenrym14\/airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-fp16",
        "Average":53.92,
        "ARC":60.58,
        "HellaSwag":82.97,
        "MMLU":52.1,
        "TruthfulQA":46.1,
        "Winogrande":73.64,
        "GSM8K":8.11,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":32.53,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"24ebae726954e4c1f24a8b2cbe0ca863012a7338"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-huangyt_Fintune_1_17w",
        "Average":53.91,
        "ARC":59.47,
        "HellaSwag":81.0,
        "MMLU":54.31,
        "TruthfulQA":38.17,
        "Winogrande":77.27,
        "GSM8K":13.27,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"aa5b161b39900c5e80d5bb39d098f6333ad964f7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Gryphe\/MythoBoros-13b",
        "Average":53.9,
        "ARC":58.19,
        "HellaSwag":81.75,
        "MMLU":50.13,
        "TruthfulQA":48.93,
        "Winogrande":75.77,
        "GSM8K":8.64,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":12.0,
        "Available on the Hub":true,
        "Model Sha":"67695d15e6610bc8055fbcde82f298e48ad2d374"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/llama-2-13b-QLoRA",
        "Average":53.87,
        "ARC":58.02,
        "HellaSwag":82.33,
        "MMLU":55.8,
        "TruthfulQA":46.23,
        "Winogrande":77.58,
        "GSM8K":3.26,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"d1a41d83c6bcc14378ee4859d65ef77a261d39d7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-13b-gpt4-1.4-fp16",
        "Average":53.87,
        "ARC":59.64,
        "HellaSwag":83.22,
        "MMLU":47.56,
        "TruthfulQA":48.82,
        "Winogrande":76.24,
        "GSM8K":7.73,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"037e369be06a8a0eef87f2cddfd3469670483f29"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-13b-gpt4-1.4",
        "Average":53.87,
        "ARC":59.64,
        "HellaSwag":83.22,
        "MMLU":47.56,
        "TruthfulQA":48.82,
        "Winogrande":76.24,
        "GSM8K":7.73,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":18.0,
        "Available on the Hub":true,
        "Model Sha":"d0d2687ed2b4a63a644ed6c5b3f6401844718659"
    },
    {
        "T":"\u2b55",
        "Model":"euclaise\/Ferret-7B",
        "Average":53.87,
        "ARC":62.29,
        "HellaSwag":81.33,
        "MMLU":60.09,
        "TruthfulQA":39.94,
        "Winogrande":77.51,
        "GSM8K":2.05,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"other",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"e96b5245ef97999f143a2c9f9739e5cf52ec0d64"
    },
    {
        "T":"\u2b55",
        "Model":"euclaise\/Ferret_7B",
        "Average":53.87,
        "ARC":62.29,
        "HellaSwag":81.33,
        "MMLU":60.09,
        "TruthfulQA":39.94,
        "Winogrande":77.51,
        "GSM8K":2.05,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"other",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"c1e1e2743ffa7b9369aebac751b04f7e8740f80d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE5_4w-r4-gate_up_down",
        "Average":53.86,
        "ARC":55.38,
        "HellaSwag":81.92,
        "MMLU":55.28,
        "TruthfulQA":40.76,
        "Winogrande":76.09,
        "GSM8K":13.72,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"2ca747d779feaa99c475b8015c9b4a50aea41cd2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Lajonbot\/WizardLM-13B-V1.2-PL-lora_unload",
        "Average":53.86,
        "ARC":58.53,
        "HellaSwag":81.1,
        "MMLU":55.15,
        "TruthfulQA":46.18,
        "Winogrande":71.03,
        "GSM8K":11.14,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"5f14e6f5ea67fd2840791c46b3e00846cbdb32cf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Gryphe\/MythoLogic-13b",
        "Average":53.85,
        "ARC":58.45,
        "HellaSwag":81.56,
        "MMLU":49.36,
        "TruthfulQA":49.47,
        "Winogrande":75.61,
        "GSM8K":8.64,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":17.0,
        "Available on the Hub":true,
        "Model Sha":"d89d925ad1eeaee465c4de3e5c74240a5a40b585"
    },
    {
        "T":"\u2b55",
        "Model":"chargoddard\/platypus-2-22b-relora",
        "Average":53.83,
        "ARC":57.68,
        "HellaSwag":82.44,
        "MMLU":55.33,
        "TruthfulQA":43.61,
        "Winogrande":77.35,
        "GSM8K":6.6,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":21.83,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"15bca3e9b25cc2f280fec21686ef3bc445217503"
    },
    {
        "T":"\u2b55",
        "Model":"Envoid\/Libra-19B",
        "Average":53.83,
        "ARC":60.58,
        "HellaSwag":82.04,
        "MMLU":55.57,
        "TruthfulQA":48.41,
        "Winogrande":76.32,
        "GSM8K":0.08,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":19.2,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"a4e1f8f62740d676c25eedb4f29f4e776dcc0c22"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-Open_Platypus_and_ccp_2.6w-3_epoch",
        "Average":53.8,
        "ARC":58.62,
        "HellaSwag":82.56,
        "MMLU":55.84,
        "TruthfulQA":42.09,
        "Winogrande":76.64,
        "GSM8K":7.05,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"001a5f96daea57b5f256c2df270b35653b439f6f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Weyaxi\/test-help-steer-filtered-orig",
        "Average":53.77,
        "ARC":57.59,
        "HellaSwag":80.42,
        "MMLU":57.24,
        "TruthfulQA":41.1,
        "Winogrande":76.64,
        "GSM8K":9.63,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"bda6d45ddb3ef73df4d198d95416c66872429927"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NekoPunchBBB\/Llama-2-13b-hf_Open-Platypus-8bit-att",
        "Average":53.75,
        "ARC":57.51,
        "HellaSwag":82.14,
        "MMLU":54.56,
        "TruthfulQA":42.21,
        "Winogrande":76.56,
        "GSM8K":9.55,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"83a8e51d0a72dcfbe5de13dc7ee10dc20e91602e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Kimiko-13B-fp16",
        "Average":53.75,
        "ARC":59.22,
        "HellaSwag":82.35,
        "MMLU":55.85,
        "TruthfulQA":39.55,
        "Winogrande":76.72,
        "GSM8K":8.79,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"27868769e2d6b1af46337f0997c71b0577952a3d"
    },
    {
        "T":"\u2b55",
        "Model":"NobodyExistsOnTheInternet\/GiftedConvo13bLoraNoEconsE4",
        "Average":53.74,
        "ARC":59.9,
        "HellaSwag":84.11,
        "MMLU":54.67,
        "TruthfulQA":41.94,
        "Winogrande":74.03,
        "GSM8K":7.81,
        "Type":"instruction-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"f3d421aadb29830345bf392f793ce3c33e7d68c5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-huangyt_FINETUNE2_3w-q_k_v_o_proj",
        "Average":53.74,
        "ARC":58.53,
        "HellaSwag":82.47,
        "MMLU":53.9,
        "TruthfulQA":37.92,
        "Winogrande":76.8,
        "GSM8K":12.81,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"d74752b931bfddaa063a292e7ea85dfb1d7a4998"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/Platypus2-13B-QLoRa",
        "Average":53.74,
        "ARC":57.51,
        "HellaSwag":82.55,
        "MMLU":57.34,
        "TruthfulQA":43.38,
        "Winogrande":76.64,
        "GSM8K":5.0,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"e62a8fafce0d64ac03d465a4e915bc1f50776a08"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zarakiquemparte\/zarafusionex-1.2-l2-7b",
        "Average":53.73,
        "ARC":56.66,
        "HellaSwag":79.16,
        "MMLU":51.94,
        "TruthfulQA":51.29,
        "Winogrande":74.74,
        "GSM8K":8.57,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"68ca01427848528ab21263fd06720a081b09d063"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r16-q_k_v_o_gate_up_down",
        "Average":53.71,
        "ARC":57.25,
        "HellaSwag":81.49,
        "MMLU":55.9,
        "TruthfulQA":39.79,
        "Winogrande":75.77,
        "GSM8K":12.05,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"a12fb5937e6904977e8123b0d5ef21283b6895d4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r8-gate_up_down",
        "Average":53.71,
        "ARC":57.25,
        "HellaSwag":81.79,
        "MMLU":53.96,
        "TruthfulQA":39.66,
        "Winogrande":77.82,
        "GSM8K":11.75,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"8a75b17d4b60f820159bb0100f26f438727bb199"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Lajonbot\/vicuna-13b-v1.3-PL-lora_unload",
        "Average":53.7,
        "ARC":54.86,
        "HellaSwag":80.41,
        "MMLU":52.2,
        "TruthfulQA":49.62,
        "Winogrande":76.09,
        "GSM8K":9.02,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"5582369752583b02df3cba4bd2a733d12265cddb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"gaodrew\/gaodrew-gorgonzola-13b",
        "Average":53.7,
        "ARC":50.94,
        "HellaSwag":77.65,
        "MMLU":68.93,
        "TruthfulQA":40.63,
        "Winogrande":75.45,
        "GSM8K":8.57,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"a53fbe358d4cb546916847d861ccfaf7c724a103"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE5_4w-r8-q_k_v_o_gate_up_down",
        "Average":53.69,
        "ARC":55.72,
        "HellaSwag":81.55,
        "MMLU":53.9,
        "TruthfulQA":41.89,
        "Winogrande":77.19,
        "GSM8K":11.9,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"eb934db4644738a74143b381445213979c8858ed"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-huangyt_FINETUNE2_3w",
        "Average":53.69,
        "ARC":58.62,
        "HellaSwag":82.32,
        "MMLU":54.25,
        "TruthfulQA":38.17,
        "Winogrande":76.8,
        "GSM8K":11.98,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"08bc7112a775dd4223d441355f3d619694013789"
    },
    {
        "T":"\u2b55",
        "Model":"BramVanroy\/Llama-2-13b-chat-dutch",
        "Average":53.69,
        "ARC":59.3,
        "HellaSwag":81.45,
        "MMLU":55.82,
        "TruthfulQA":38.23,
        "Winogrande":76.64,
        "GSM8K":10.69,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"428508a0cf288c0f5b7891c9b2f758ddf4d62c26"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-13b-gpt4-1.1",
        "Average":53.68,
        "ARC":59.04,
        "HellaSwag":83.05,
        "MMLU":49.41,
        "TruthfulQA":46.62,
        "Winogrande":75.77,
        "GSM8K":8.19,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"19c7060adcb34d42e742fe51dd36b8657ac069b7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r16-q_k_v_o",
        "Average":53.68,
        "ARC":56.23,
        "HellaSwag":81.98,
        "MMLU":55.87,
        "TruthfulQA":39.76,
        "Winogrande":76.72,
        "GSM8K":11.52,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"cc3c5e5a874cf4ff4f94ea919e819f8a914c8acb"
    },
    {
        "T":"?",
        "Model":"shareAI\/bimoGPT-llama2-13b",
        "Average":53.68,
        "ARC":58.79,
        "HellaSwag":82.08,
        "MMLU":55.6,
        "TruthfulQA":37.82,
        "Winogrande":76.48,
        "GSM8K":11.3,
        "Type":"",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"openrail",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":false,
        "Model Sha":"c29b67965ea55da3e2ac678eef7ffdf36f8ef5ab"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TaylorAI\/Flash-Llama-13B",
        "Average":53.67,
        "ARC":59.3,
        "HellaSwag":82.15,
        "MMLU":55.67,
        "TruthfulQA":37.39,
        "Winogrande":76.64,
        "GSM8K":10.84,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"81b40096471a8980e3e1a8998f358bd363033783"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NewstaR\/Starlight-13B",
        "Average":53.67,
        "ARC":59.3,
        "HellaSwag":82.15,
        "MMLU":55.67,
        "TruthfulQA":37.39,
        "Winogrande":76.64,
        "GSM8K":10.84,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"cb9fced568b1abd881133c642c427aaa488f00cc"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"TheBloke\/Llama-2-13B-fp16",
        "Average":53.67,
        "ARC":59.3,
        "HellaSwag":82.15,
        "MMLU":55.67,
        "TruthfulQA":37.39,
        "Winogrande":76.64,
        "GSM8K":10.84,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":51.0,
        "Available on the Hub":true,
        "Model Sha":"b2e65e8ad4bb35e5abaee0170ebd5fc2134a50bb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r16-gate_up_down-test1",
        "Average":53.66,
        "ARC":55.8,
        "HellaSwag":82.27,
        "MMLU":55.63,
        "TruthfulQA":38.15,
        "Winogrande":77.43,
        "GSM8K":12.66,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"48b8ceeb62e5ca897f284bbc0923201689af7c89"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"chargoddard\/llama2-22b-blocktriangular",
        "Average":53.65,
        "ARC":58.28,
        "HellaSwag":82.69,
        "MMLU":54.53,
        "TruthfulQA":39.23,
        "Winogrande":75.93,
        "GSM8K":11.22,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":21.62,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"7adbaa5b8e122bb93bf510d8655ec4132d7b4a8a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-13b-gpt4",
        "Average":53.64,
        "ARC":59.39,
        "HellaSwag":83.29,
        "MMLU":47.89,
        "TruthfulQA":47.65,
        "Winogrande":75.77,
        "GSM8K":7.88,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":17.0,
        "Available on the Hub":true,
        "Model Sha":"c0eef6e6f63d4b11953539308717cea0079b44f9"
    },
    {
        "T":"?",
        "Model":"chargoddard\/llama2-22b",
        "Average":53.64,
        "ARC":58.53,
        "HellaSwag":82.55,
        "MMLU":54.68,
        "TruthfulQA":39.84,
        "Winogrande":76.32,
        "GSM8K":9.93,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":21.62,
        "Hub \u2764\ufe0f":35.0,
        "Available on the Hub":true,
        "Model Sha":"2bece0787009b4b584f49d0e0d1b49ecf4a52da9"
    },
    {
        "T":"\u2b55",
        "Model":"chargoddard\/platypus2-22b-relora",
        "Average":53.64,
        "ARC":57.51,
        "HellaSwag":82.36,
        "MMLU":54.94,
        "TruthfulQA":43.62,
        "Winogrande":77.11,
        "GSM8K":6.29,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":21.83,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"15bca3e9b25cc2f280fec21686ef3bc445217503"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NobodyExistsOnTheInternet\/PuffedLIMA13bQLORA",
        "Average":53.63,
        "ARC":59.9,
        "HellaSwag":84.39,
        "MMLU":53.68,
        "TruthfulQA":39.9,
        "Winogrande":75.22,
        "GSM8K":8.72,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"7da6d235d625e16c850ccd0b947dee40071b1f89"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/deacon-13b",
        "Average":53.63,
        "ARC":57.85,
        "HellaSwag":82.63,
        "MMLU":55.25,
        "TruthfulQA":39.33,
        "Winogrande":76.32,
        "GSM8K":10.39,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"6c3a002f6c9e8a481a7375d91856d603bf6dd040"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r4-q_k_v_o",
        "Average":53.62,
        "ARC":59.04,
        "HellaSwag":81.15,
        "MMLU":53.0,
        "TruthfulQA":40.16,
        "Winogrande":76.48,
        "GSM8K":11.9,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"ac40ecf48cf5f7168e8c3929632c654bc834c3d7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wei123602\/llama2-13b-FINETUNE3_TEST2",
        "Average":53.62,
        "ARC":54.69,
        "HellaSwag":81.48,
        "MMLU":56.8,
        "TruthfulQA":39.93,
        "Winogrande":76.24,
        "GSM8K":12.59,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"9e6431061bd13852a7435f5fe7a6eb0bbd148e14"
    },
    {
        "T":"\u2b55",
        "Model":"llm-agents\/tora-13b-v1.0",
        "Average":53.62,
        "ARC":58.96,
        "HellaSwag":82.31,
        "MMLU":54.73,
        "TruthfulQA":40.25,
        "Winogrande":75.61,
        "GSM8K":9.86,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"0636c1f582c979a5a292cc5f3dc293800b1494e2"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/MistralInstructLongish",
        "Average":53.62,
        "ARC":60.75,
        "HellaSwag":81.86,
        "MMLU":60.49,
        "TruthfulQA":40.55,
        "Winogrande":76.56,
        "GSM8K":1.52,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"813c4707970cb5bf3e2a49f7f350af59e7032c24"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NobodyExistsOnTheInternet\/PuffedConvo13bLoraE4",
        "Average":53.62,
        "ARC":59.81,
        "HellaSwag":84.39,
        "MMLU":53.62,
        "TruthfulQA":39.87,
        "Winogrande":75.22,
        "GSM8K":8.79,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"40e4fce0c25bd23f6011b424748ee2b5374b98d5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wei123602\/Llama-2-13b-FINETUNE4_TEST",
        "Average":53.62,
        "ARC":54.78,
        "HellaSwag":81.52,
        "MMLU":56.03,
        "TruthfulQA":39.14,
        "Winogrande":77.03,
        "GSM8K":13.19,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"0ed198a814192b06e60715112d2a4b6bfd630806"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aspik101\/Nous-Hermes-13b-pl-lora_unload",
        "Average":53.61,
        "ARC":57.08,
        "HellaSwag":81.49,
        "MMLU":49.17,
        "TruthfulQA":48.3,
        "Winogrande":76.4,
        "GSM8K":9.25,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"d0ef3991a11c4dc2ea2f832d4082c89c3c5e810c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ajibawa-2023\/Python-Code-13B",
        "Average":53.61,
        "ARC":58.79,
        "HellaSwag":81.66,
        "MMLU":54.78,
        "TruthfulQA":42.83,
        "Winogrande":74.03,
        "GSM8K":9.55,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"981454b6a2275f787592589609df7f2bf558706d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BramVanroy\/llama2-13b-ft-mc4_nl_cleaned_tiny",
        "Average":53.6,
        "ARC":59.3,
        "HellaSwag":82.04,
        "MMLU":54.67,
        "TruthfulQA":38.03,
        "Winogrande":77.27,
        "GSM8K":10.31,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"b23fe7d174653b87dc08507d9b83504a8dddbc45"
    },
    {
        "T":"\u2b55",
        "Model":"ehartford\/WizardLM-1.0-Uncensored-CodeLlama-34b",
        "Average":53.59,
        "ARC":56.4,
        "HellaSwag":75.45,
        "MMLU":54.51,
        "TruthfulQA":43.06,
        "Winogrande":72.45,
        "GSM8K":19.64,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":20.0,
        "Available on the Hub":true,
        "Model Sha":"3e8df2cf4a4ee1c0b2d079cb7be70024d425ea8c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r8-gate_up_down",
        "Average":53.58,
        "ARC":54.35,
        "HellaSwag":82.13,
        "MMLU":55.33,
        "TruthfulQA":39.6,
        "Winogrande":77.19,
        "GSM8K":12.89,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"1646a2b77ddeaf0f848c96ed68726556c7539729"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/tulu-13B-fp16",
        "Average":53.58,
        "ARC":53.92,
        "HellaSwag":80.66,
        "MMLU":53.19,
        "TruthfulQA":43.84,
        "Winogrande":75.61,
        "GSM8K":14.25,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"532aeb363b0ceee155b3cf9479ef635b797cee7c"
    },
    {
        "T":"\u2b55",
        "Model":"dhmeltzer\/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16_merged",
        "Average":53.57,
        "ARC":58.96,
        "HellaSwag":81.94,
        "MMLU":55.0,
        "TruthfulQA":40.26,
        "Winogrande":76.56,
        "GSM8K":8.72,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"30edbe648df2661dd779cd19ef613e6914dcc8e0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"stabilityai\/StableBeluga-7B",
        "Average":53.56,
        "ARC":56.31,
        "HellaSwag":79.14,
        "MMLU":52.71,
        "TruthfulQA":50.19,
        "Winogrande":75.22,
        "GSM8K":7.81,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":112.0,
        "Available on the Hub":true,
        "Model Sha":"329adcfc39f48dce183eb0b155b732dbe03c6304"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"circulus\/Llama-2-7b-orca-v1",
        "Average":53.56,
        "ARC":56.31,
        "HellaSwag":79.14,
        "MMLU":52.71,
        "TruthfulQA":50.19,
        "Winogrande":75.22,
        "GSM8K":7.81,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"e501f231277671710384ba0397da2c4486865958"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/dolphin-llama-13b",
        "Average":53.56,
        "ARC":55.55,
        "HellaSwag":77.11,
        "MMLU":52.16,
        "TruthfulQA":52.23,
        "Winogrande":69.93,
        "GSM8K":14.4,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":56.0,
        "Available on the Hub":true,
        "Model Sha":"b6d16c3e1cffef5e914863f41fd96152dafddd6f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dvruette\/oasst-llama-13b-2-epochs",
        "Average":53.55,
        "ARC":57.94,
        "HellaSwag":82.4,
        "MMLU":48.56,
        "TruthfulQA":47.27,
        "Winogrande":76.87,
        "GSM8K":8.26,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"0e3796192f7edf43968541b9454ea35da4a2b1c5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/guanaco-13B-HF",
        "Average":53.54,
        "ARC":57.85,
        "HellaSwag":83.84,
        "MMLU":48.28,
        "TruthfulQA":46.73,
        "Winogrande":75.85,
        "GSM8K":8.72,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"bd59c700815124df616a17f5b49a0bc51590b231"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Lajonbot\/tableBeluga-7B-instruct-pl-lora_unload",
        "Average":53.54,
        "ARC":56.23,
        "HellaSwag":79.12,
        "MMLU":52.7,
        "TruthfulQA":50.19,
        "Winogrande":75.22,
        "GSM8K":7.81,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"eeb22ca9481a5ed7e131a329324494f234300a45"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"chargoddard\/llama2-22b-blocktriangular",
        "Average":53.53,
        "ARC":58.53,
        "HellaSwag":82.59,
        "MMLU":54.64,
        "TruthfulQA":39.3,
        "Winogrande":76.32,
        "GSM8K":9.78,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":21.62,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"40a51343ae776b5cb39f2b4343ae8f9b676ffd58"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YeungNLP\/firefly-llama-13b",
        "Average":53.53,
        "ARC":58.96,
        "HellaSwag":79.71,
        "MMLU":49.1,
        "TruthfulQA":49.59,
        "Winogrande":75.61,
        "GSM8K":8.19,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"dd326f89ce885844d714d9ab33603e0d17f56cc5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-Open_Platypus_and_ccp_2.6w",
        "Average":53.52,
        "ARC":58.96,
        "HellaSwag":82.51,
        "MMLU":56.12,
        "TruthfulQA":40.07,
        "Winogrande":76.64,
        "GSM8K":6.82,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"2929bfa1049db46df94f5710755178d18a981665"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r16-gate_up_down",
        "Average":53.52,
        "ARC":55.03,
        "HellaSwag":81.97,
        "MMLU":56.64,
        "TruthfulQA":38.07,
        "Winogrande":77.19,
        "GSM8K":12.21,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"555486843f613276b6edb480f6d37b9203daa226"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"IGeniusDev\/llama13B-quant8-testv1-openorca-customdataset",
        "Average":53.5,
        "ARC":60.49,
        "HellaSwag":82.97,
        "MMLU":54.44,
        "TruthfulQA":37.34,
        "Winogrande":75.69,
        "GSM8K":10.08,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f364d000bedac80e72aa103c08b77aee1b61b7da"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bofenghuang\/vigogne-13b-chat",
        "Average":53.5,
        "ARC":58.62,
        "HellaSwag":80.85,
        "MMLU":47.76,
        "TruthfulQA":48.73,
        "Winogrande":76.72,
        "GSM8K":8.34,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"openrail",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"27002e974774c3599e6a4d731dd44e68b9e41f92"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-mistral-7b-v13",
        "Average":53.5,
        "ARC":52.3,
        "HellaSwag":75.09,
        "MMLU":56.34,
        "TruthfulQA":50.81,
        "Winogrande":71.74,
        "GSM8K":14.71,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.13,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"e6c4cc00e1bb2aa2082c2b8fd93c949aa36ce300"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YeungNLP\/firefly-llama-13b-v1.2",
        "Average":53.49,
        "ARC":56.74,
        "HellaSwag":80.34,
        "MMLU":48.9,
        "TruthfulQA":51.0,
        "Winogrande":75.93,
        "GSM8K":8.04,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"c0a56d9f5a15bea07493191b5a6295f6797a9b2c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r4-gate_up_down",
        "Average":53.48,
        "ARC":55.8,
        "HellaSwag":81.74,
        "MMLU":55.09,
        "TruthfulQA":39.12,
        "Winogrande":76.32,
        "GSM8K":12.81,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"aefc3a122cb054b070a212d1127600775aded4be"
    },
    {
        "T":"\u2b55",
        "Model":"pankajmathur\/orca_mini_v3_7b",
        "Average":53.47,
        "ARC":56.91,
        "HellaSwag":79.64,
        "MMLU":52.37,
        "TruthfulQA":50.51,
        "Winogrande":74.27,
        "GSM8K":7.13,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":37.0,
        "Available on the Hub":true,
        "Model Sha":"f9849ea6bf0f6ebb78dca1cea1c7a3ef8f7d715c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/orca_mini_v3_7b",
        "Average":53.47,
        "ARC":56.91,
        "HellaSwag":79.64,
        "MMLU":52.37,
        "TruthfulQA":50.51,
        "Winogrande":74.27,
        "GSM8K":7.13,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":37.0,
        "Available on the Hub":true,
        "Model Sha":"a1583d2f02041fb37df28eeae4da644d8dff33eb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YeungNLP\/firefly-llama2-13b-chat",
        "Average":53.46,
        "ARC":57.51,
        "HellaSwag":77.94,
        "MMLU":52.56,
        "TruthfulQA":48.18,
        "Winogrande":74.74,
        "GSM8K":9.86,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.97,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"9497e3bd12e19e1300bc7b1980fbe232420134b9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE5_4w-r16-gate_up_down",
        "Average":53.44,
        "ARC":55.8,
        "HellaSwag":82.1,
        "MMLU":55.33,
        "TruthfulQA":39.82,
        "Winogrande":76.24,
        "GSM8K":11.37,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"86f255afabc8986c73376cafd98628a068649022"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r8-q_k_v_o_gate_up_down",
        "Average":53.43,
        "ARC":57.94,
        "HellaSwag":81.19,
        "MMLU":53.43,
        "TruthfulQA":40.48,
        "Winogrande":76.72,
        "GSM8K":10.84,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"15f1b122d60631091419cb8e668a28737b92a0e0"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"TigerResearch\/tigerbot-13b-base",
        "Average":53.42,
        "ARC":53.84,
        "HellaSwag":77.05,
        "MMLU":53.57,
        "TruthfulQA":44.06,
        "Winogrande":74.98,
        "GSM8K":17.06,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"2df5ed76be7eff0962f2d816a64eca1e78e1cbf3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zarakiquemparte\/zarafusionex-1.1-l2-7b",
        "Average":53.41,
        "ARC":56.14,
        "HellaSwag":79.34,
        "MMLU":52.1,
        "TruthfulQA":50.66,
        "Winogrande":74.43,
        "GSM8K":7.81,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"3268ff5291934a14f3f5e7013bbb408f33adb542"
    },
    {
        "T":"?",
        "Model":"quantumaikr\/QuantumLM",
        "Average":53.41,
        "ARC":55.8,
        "HellaSwag":79.74,
        "MMLU":54.17,
        "TruthfulQA":46.71,
        "Winogrande":74.19,
        "GSM8K":9.86,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"9058130b416355b37f5f78777748aa56d98a4da0"
    },
    {
        "T":"\u2b55",
        "Model":"ehartford\/samantha-mistral-instruct-7b",
        "Average":53.4,
        "ARC":53.5,
        "HellaSwag":75.14,
        "MMLU":51.72,
        "TruthfulQA":58.81,
        "Winogrande":70.4,
        "GSM8K":10.84,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":14.0,
        "Available on the Hub":true,
        "Model Sha":"3a33eea0858d411617c472c3c0ae39f17d2b3f5d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mosaicml\/mpt-30b-instruct",
        "Average":53.4,
        "ARC":58.45,
        "HellaSwag":84.31,
        "MMLU":49.15,
        "TruthfulQA":38.05,
        "Winogrande":75.14,
        "GSM8K":15.31,
        "Type":"fine-tuned",
        "Architecture":"MPTForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-sa-3.0",
        "#Params (B)":29.96,
        "Hub \u2764\ufe0f":93.0,
        "Available on the Hub":true,
        "Model Sha":"2abf1163dd8c9b11f07d805c06e6ec90a1f2037e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aeala\/GPT4-x-Alpasta-13b",
        "Average":53.38,
        "ARC":58.53,
        "HellaSwag":79.92,
        "MMLU":46.03,
        "TruthfulQA":53.06,
        "Winogrande":73.95,
        "GSM8K":8.79,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"50af05b015446110a2dc52a1b4b341142c98e62b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE5_4w-r4-q_k_v_o_gate_up_down",
        "Average":53.38,
        "ARC":55.89,
        "HellaSwag":81.38,
        "MMLU":53.77,
        "TruthfulQA":40.25,
        "Winogrande":76.72,
        "GSM8K":12.28,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"a8b15badead658df6ec5b884b813962b9fd29cfb"
    },
    {
        "T":"\u2b55",
        "Model":"NobodyExistsOnTheInternet\/GiftedConvo13bLoraNoEcons",
        "Average":53.35,
        "ARC":59.39,
        "HellaSwag":83.19,
        "MMLU":55.15,
        "TruthfulQA":40.56,
        "Winogrande":74.03,
        "GSM8K":7.81,
        "Type":"instruction-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"9d7031e7d956dd2d25c61d85f594d115ce65b172"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r4-gate_up_down",
        "Average":53.35,
        "ARC":56.4,
        "HellaSwag":81.93,
        "MMLU":53.63,
        "TruthfulQA":39.23,
        "Winogrande":76.95,
        "GSM8K":11.98,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"dd61a482fa2f71efe6f22aae6949355ca4b06ccc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-l2-13b-2.1",
        "Average":53.34,
        "ARC":59.47,
        "HellaSwag":82.47,
        "MMLU":54.83,
        "TruthfulQA":44.65,
        "Winogrande":75.06,
        "GSM8K":3.56,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"172e30e56e939f73d7d00a165c2d49cbd284481f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE5_4w-r4-q_k_v_o",
        "Average":53.32,
        "ARC":58.36,
        "HellaSwag":81.1,
        "MMLU":54.53,
        "TruthfulQA":37.02,
        "Winogrande":76.64,
        "GSM8K":12.28,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"5cbcd9c0a6b9a19f0d099e653cde18e11bf95303"
    },
    {
        "T":"?",
        "Model":"TheBloke\/vicuna-13b-v1.3.0-GPTQ",
        "Average":53.29,
        "ARC":54.35,
        "HellaSwag":79.47,
        "MMLU":51.97,
        "TruthfulQA":50.88,
        "Winogrande":74.66,
        "GSM8K":8.42,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":16.22,
        "Hub \u2764\ufe0f":19.0,
        "Available on the Hub":true,
        "Model Sha":"6ef1f8d8638ea2d6681a8e3da73be57c501d847b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kevinpro\/Vicuna-13B-CoT",
        "Average":53.29,
        "ARC":52.73,
        "HellaSwag":80.13,
        "MMLU":51.94,
        "TruthfulQA":52.08,
        "Winogrande":74.19,
        "GSM8K":8.64,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"346e3c46959cf9f1e03feffa761afe020c0fb6a8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/vicuna-13B-1.1-HF",
        "Average":53.29,
        "ARC":52.73,
        "HellaSwag":80.13,
        "MMLU":51.94,
        "TruthfulQA":52.08,
        "Winogrande":74.19,
        "GSM8K":8.64,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":96.0,
        "Available on the Hub":true,
        "Model Sha":"8c71dbe9221e83d2ec72e4dc08beccfc78b563c0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"pillowtalks-ai\/delta13b",
        "Average":53.29,
        "ARC":52.73,
        "HellaSwag":80.13,
        "MMLU":51.94,
        "TruthfulQA":52.08,
        "Winogrande":74.19,
        "GSM8K":8.64,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"83fa0860990df1db35550f973ba4306449e35412"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"eachadea\/vicuna-13b-1.1",
        "Average":53.29,
        "ARC":52.73,
        "HellaSwag":80.13,
        "MMLU":51.94,
        "TruthfulQA":52.08,
        "Winogrande":74.19,
        "GSM8K":8.64,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":134.0,
        "Available on the Hub":true,
        "Model Sha":"bfcc6ca66694310be6c85ba0638597f4256c4143"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Vicuna-13B-CoT-fp16",
        "Average":53.28,
        "ARC":52.73,
        "HellaSwag":80.14,
        "MMLU":51.9,
        "TruthfulQA":52.08,
        "Winogrande":74.19,
        "GSM8K":8.64,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"fe74a0ece9089828b301bd0f067ae5f257516179"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lmsys\/vicuna-13b-delta-v1.1",
        "Average":53.28,
        "ARC":52.73,
        "HellaSwag":80.14,
        "MMLU":51.9,
        "TruthfulQA":52.08,
        "Winogrande":74.19,
        "GSM8K":8.64,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":402.0,
        "Available on the Hub":true,
        "Model Sha":"ffed4c7cf1b9814812078efbe29ec3f610ea39e7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lmsys\/vicuna-13b-v1.1",
        "Average":53.28,
        "ARC":52.73,
        "HellaSwag":80.14,
        "MMLU":51.9,
        "TruthfulQA":52.08,
        "Winogrande":74.19,
        "GSM8K":8.64,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":96.0,
        "Available on the Hub":true,
        "Model Sha":"8c71dbe9221e83d2ec72e4dc08beccfc78b563c0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Llama-2-13B-GPTQ",
        "Average":53.26,
        "ARC":59.13,
        "HellaSwag":81.48,
        "MMLU":54.45,
        "TruthfulQA":37.07,
        "Winogrande":76.16,
        "GSM8K":11.3,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"GPTQ",
        "Hub License":"llama2",
        "#Params (B)":16.23,
        "Hub \u2764\ufe0f":99.0,
        "Available on the Hub":true,
        "Model Sha":"b7db471d1789802a3a8e3b93cdd66a9f046f17c3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r4-q_k_v_o_gate_up_down",
        "Average":53.23,
        "ARC":56.31,
        "HellaSwag":81.43,
        "MMLU":55.3,
        "TruthfulQA":39.11,
        "Winogrande":76.8,
        "GSM8K":10.46,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"0d8d502e4e5ef89592dd0d3bc7223eaf7f77f78b"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/airoboros-2.1-llama-2-13B-QLoRa",
        "Average":53.23,
        "ARC":59.73,
        "HellaSwag":82.91,
        "MMLU":54.77,
        "TruthfulQA":45.14,
        "Winogrande":74.03,
        "GSM8K":2.81,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"ebf991c8d34314caab6ccc6b078c681d20bac39a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE2_TEST_2.2w",
        "Average":53.2,
        "ARC":56.23,
        "HellaSwag":82.7,
        "MMLU":55.35,
        "TruthfulQA":39.55,
        "Winogrande":76.72,
        "GSM8K":8.64,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"3be177b35f1b44d147751ab38ca6d8a008eb6b7f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r4-q_k_v_o",
        "Average":53.18,
        "ARC":54.78,
        "HellaSwag":81.4,
        "MMLU":54.73,
        "TruthfulQA":41.02,
        "Winogrande":76.64,
        "GSM8K":10.54,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"8702b433008a62e9f8bf15e70ba15fa7100e991c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zarakiquemparte\/zarafusionix-l2-7b",
        "Average":53.18,
        "ARC":55.55,
        "HellaSwag":79.4,
        "MMLU":51.21,
        "TruthfulQA":51.05,
        "Winogrande":74.66,
        "GSM8K":7.2,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"13d0e2498a4b5f53f6dc2464f20e093b07a4bd4b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/WizardLM-13B-V1-1-SuperHOT-8K-fp16",
        "Average":53.16,
        "ARC":58.62,
        "HellaSwag":81.07,
        "MMLU":48.32,
        "TruthfulQA":54.19,
        "Winogrande":76.01,
        "GSM8K":0.76,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"83905656ca3e63877b8d9f3a74118da0c9bc6939"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/Athena-Platypus2-13B-QLora-0.80-epoch",
        "Average":53.16,
        "ARC":56.66,
        "HellaSwag":80.56,
        "MMLU":55.43,
        "TruthfulQA":53.62,
        "Winogrande":72.61,
        "GSM8K":0.08,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f7b6c11b4df16079dfdd1e8dd8c489a8835c7cc4"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/Airboros2.1-Platypus2-13B-QLora-0.80-epoch",
        "Average":53.15,
        "ARC":58.96,
        "HellaSwag":82.46,
        "MMLU":54.62,
        "TruthfulQA":47.71,
        "Winogrande":75.14,
        "GSM8K":0.0,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"45bd1e47218ba2e075e03f6407980eb839e67eb3"
    },
    {
        "T":"\u2b55",
        "Model":"Enno-Ai\/vigogne2-enno-13b-sft-lora-4bit",
        "Average":53.15,
        "ARC":62.03,
        "HellaSwag":82.65,
        "MMLU":54.11,
        "TruthfulQA":42.98,
        "Winogrande":76.95,
        "GSM8K":0.15,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"2a1b03977395eee44742abda63a4787ea5371d06"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Airoboros-L2-13B-2.1-GPTQ",
        "Average":53.14,
        "ARC":58.96,
        "HellaSwag":81.72,
        "MMLU":53.16,
        "TruthfulQA":44.68,
        "Winogrande":74.35,
        "GSM8K":5.99,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"GPTQ",
        "Hub License":"llama2",
        "#Params (B)":16.23,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"d90d96e40b9359cb5c35e6b6c8f0eb24896e827b"
    },
    {
        "T":"\u2b55",
        "Model":"dhmeltzer\/Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16",
        "Average":53.14,
        "ARC":59.04,
        "HellaSwag":82.33,
        "MMLU":55.36,
        "TruthfulQA":35.75,
        "Winogrande":76.32,
        "GSM8K":10.01,
        "Type":"instruction-tuned",
        "Architecture":"?",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"a3ed7416156963f49bf4dc056188e006c0c214d2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dvruette\/llama-13b-pretrained-sft-do2",
        "Average":53.12,
        "ARC":58.96,
        "HellaSwag":80.32,
        "MMLU":47.25,
        "TruthfulQA":47.41,
        "Winogrande":75.53,
        "GSM8K":9.25,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"6cb016f5bfcbc24ee08312b52f08ef5e8f860871"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/MLewd-L2-13B",
        "Average":53.12,
        "ARC":58.28,
        "HellaSwag":82.32,
        "MMLU":54.67,
        "TruthfulQA":48.66,
        "Winogrande":73.48,
        "GSM8K":1.29,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"feb1fa71e0b24261d3ca428b4aed881dd31f166e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jjaaaww\/posi_13b",
        "Average":53.12,
        "ARC":59.64,
        "HellaSwag":82.52,
        "MMLU":56.56,
        "TruthfulQA":42.14,
        "Winogrande":76.24,
        "GSM8K":1.59,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"ff4eeb0f876c41553c302020041a0e78a15f9aa7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dvruette\/llama-13b-pretrained-sft-epoch-1",
        "Average":53.11,
        "ARC":57.25,
        "HellaSwag":79.99,
        "MMLU":45.52,
        "TruthfulQA":44.45,
        "Winogrande":77.58,
        "GSM8K":13.87,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"1f839c019153789c15bbc45ecbb512d0f5015881"
    },
    {
        "T":"?",
        "Model":"TheBloke\/manticore-13b-chat-pyg-GPTQ",
        "Average":53.11,
        "ARC":57.85,
        "HellaSwag":81.07,
        "MMLU":47.56,
        "TruthfulQA":47.77,
        "Winogrande":75.93,
        "GSM8K":8.49,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":16.22,
        "Hub \u2764\ufe0f":33.0,
        "Available on the Hub":true,
        "Model Sha":"923f27245d13058c9c1b3ab0eab6c6c93ffc162e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"itsliupeng\/llama2_7b_mmlu",
        "Average":53.1,
        "ARC":56.14,
        "HellaSwag":79.13,
        "MMLU":60.04,
        "TruthfulQA":40.95,
        "Winogrande":74.43,
        "GSM8K":7.88,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"553178f8d5d69eb1dfa5b9503d2ce0c1e481e5b1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wei123602\/llama2-13b-FINETUNE3_TEST",
        "Average":53.09,
        "ARC":53.67,
        "HellaSwag":79.66,
        "MMLU":54.48,
        "TruthfulQA":40.22,
        "Winogrande":75.93,
        "GSM8K":14.56,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"22cea7bf138eb0d6c962812df2b2235290acbee2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE3_3.3w-r4-q_k_v_o_gate_up_down",
        "Average":53.06,
        "ARC":57.76,
        "HellaSwag":80.78,
        "MMLU":54.32,
        "TruthfulQA":40.8,
        "Winogrande":76.72,
        "GSM8K":7.96,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"ebe1b75fa315a9b55f686368070a0bcd0245ee39"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wei123602\/FINETUNE3_TEST4",
        "Average":53.02,
        "ARC":55.63,
        "HellaSwag":81.31,
        "MMLU":52.13,
        "TruthfulQA":41.14,
        "Winogrande":76.72,
        "GSM8K":11.22,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"5195e87bb34317c5aaf201faa476aae78ecc9f1b"
    },
    {
        "T":"\u2b55",
        "Model":"Open-Orca\/LlongOrca-7B-16k",
        "Average":53.02,
        "ARC":57.51,
        "HellaSwag":79.44,
        "MMLU":49.35,
        "TruthfulQA":49.84,
        "Winogrande":74.51,
        "GSM8K":7.51,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":39.0,
        "Available on the Hub":true,
        "Model Sha":"1370c7c595e6c8394e6332bc535ae25e21def85b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-l2-13b-gpt4-1.4.1",
        "Average":53.02,
        "ARC":59.13,
        "HellaSwag":82.78,
        "MMLU":55.62,
        "TruthfulQA":40.27,
        "Winogrande":73.32,
        "GSM8K":6.97,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":12.0,
        "Available on the Hub":true,
        "Model Sha":"35ff51ebe5668269dfd33a9ed94412d88f1f4b65"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dvruette\/llama-13b-pretrained-dropout",
        "Average":52.99,
        "ARC":56.4,
        "HellaSwag":79.34,
        "MMLU":46.59,
        "TruthfulQA":48.6,
        "Winogrande":75.22,
        "GSM8K":11.83,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"045c84727d495bfb4b612a2482ce0d807c067b46"
    },
    {
        "T":"\u2b55",
        "Model":"The-Face-Of-Goonery\/Huginn-19b-prototype",
        "Average":52.99,
        "ARC":59.22,
        "HellaSwag":81.03,
        "MMLU":55.73,
        "TruthfulQA":41.15,
        "Winogrande":76.4,
        "GSM8K":4.4,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":19.36,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"d2c8cc15c57da217ff29ebaaae4bc4f57d6b21b0"
    },
    {
        "T":"\u2b55",
        "Model":"heegyu\/LIMA2-13b-hf",
        "Average":52.98,
        "ARC":60.24,
        "HellaSwag":83.69,
        "MMLU":53.17,
        "TruthfulQA":41.81,
        "Winogrande":73.24,
        "GSM8K":5.76,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"ed3535921eb24e0737f9a6cda70b1a3fd71532cd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"xzuyn\/Alpacino-SuperCOT-13B",
        "Average":52.97,
        "ARC":58.36,
        "HellaSwag":81.69,
        "MMLU":47.89,
        "TruthfulQA":45.42,
        "Winogrande":76.95,
        "GSM8K":7.51,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"3a82b04684fe99d59556421c3f96a187049a3cec"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"allenai\/digital-socrates-7b",
        "Average":52.95,
        "ARC":54.44,
        "HellaSwag":75.99,
        "MMLU":51.41,
        "TruthfulQA":44.88,
        "Winogrande":73.09,
        "GSM8K":17.89,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"5d26db18b95778c31dc8425871052f495b267563"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zarakiquemparte\/zaraxe-l2-7b",
        "Average":52.95,
        "ARC":57.17,
        "HellaSwag":79.34,
        "MMLU":51.0,
        "TruthfulQA":49.11,
        "Winogrande":73.48,
        "GSM8K":7.58,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"0875bf202aedeef7a58d7382fd6f55f5bca12968"
    },
    {
        "T":"\u2b55",
        "Model":"dhmeltzer\/Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16_merged",
        "Average":52.94,
        "ARC":58.45,
        "HellaSwag":81.97,
        "MMLU":55.02,
        "TruthfulQA":35.85,
        "Winogrande":75.69,
        "GSM8K":10.69,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"5a89844b1aea3f0573e696143ec66727df4b5d79"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"project-baize\/baize-v2-13b",
        "Average":52.94,
        "ARC":56.91,
        "HellaSwag":79.29,
        "MMLU":49.72,
        "TruthfulQA":47.88,
        "Winogrande":74.9,
        "GSM8K":8.95,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":22.0,
        "Available on the Hub":true,
        "Model Sha":"a3c4bbccca8b650700a49a225582c17bb49b446b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-llama2-13b-v11-bf16",
        "Average":52.93,
        "ARC":52.99,
        "HellaSwag":75.38,
        "MMLU":51.36,
        "TruthfulQA":47.94,
        "Winogrande":71.03,
        "GSM8K":18.88,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":12.88,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"4d4e72c553e9d60fdc208663b0a1c0364caa2f30"
    },
    {
        "T":"\u2b55",
        "Model":"Korabbit\/Llama-2-7b-chat-hf-afr-100step-flan-v2",
        "Average":52.92,
        "ARC":53.24,
        "HellaSwag":78.43,
        "MMLU":48.43,
        "TruthfulQA":45.66,
        "Winogrande":72.3,
        "GSM8K":19.48,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"0f1873b505a5f32ca429c164a229bab663eaf617"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nkpz\/llama2-22b-chat-wizard-uncensored",
        "Average":52.9,
        "ARC":56.23,
        "HellaSwag":80.39,
        "MMLU":53.62,
        "TruthfulQA":45.76,
        "Winogrande":70.24,
        "GSM8K":11.14,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":21.83,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"90cffebc8f530161505b84740ff6c8f646299d6c"
    },
    {
        "T":"\u2b55",
        "Model":"pe-nlp\/llama-2-13b-platypus-vicuna-wizard",
        "Average":52.9,
        "ARC":61.26,
        "HellaSwag":82.31,
        "MMLU":55.21,
        "TruthfulQA":41.91,
        "Winogrande":75.77,
        "GSM8K":0.91,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"71aa919fc15fa9d9def9185791b15a3f76e7bd8d"
    },
    {
        "T":"\u2b55",
        "Model":"clibrain\/Llama-2-13b-ft-instruct-es",
        "Average":52.89,
        "ARC":59.39,
        "HellaSwag":81.51,
        "MMLU":54.31,
        "TruthfulQA":37.81,
        "Winogrande":75.77,
        "GSM8K":8.57,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"772b53f64f484fa0d651d453bcefc35a0f52f251"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wei123602\/llama2-13b-fintune2-4E",
        "Average":52.88,
        "ARC":55.89,
        "HellaSwag":80.95,
        "MMLU":53.73,
        "TruthfulQA":42.72,
        "Winogrande":73.09,
        "GSM8K":10.92,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"645ede9d6ec60d8fa051bc7ad32ab5f7bfdc066d"
    },
    {
        "T":"\u2b55",
        "Model":"Korabbit\/Llama-2-7b-chat-hf-afr-100step-flan",
        "Average":52.88,
        "ARC":52.9,
        "HellaSwag":78.44,
        "MMLU":48.4,
        "TruthfulQA":45.67,
        "Winogrande":72.38,
        "GSM8K":19.48,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"1d502ae9a15c38118baa5ae55e048a080cb05c89"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CHIH-HUNG\/llama-2-13b-FINETUNE4_3.8w-r8-q_k_v_o_gate_up_down",
        "Average":52.88,
        "ARC":55.97,
        "HellaSwag":81.53,
        "MMLU":54.42,
        "TruthfulQA":40.72,
        "Winogrande":75.06,
        "GSM8K":9.55,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"905fc0b26dcb9e1fc5be99e73596e0884f9b71df"
    },
    {
        "T":"?",
        "Model":"Yhyu13\/chimera-inst-chat-13b-hf",
        "Average":52.86,
        "ARC":55.38,
        "HellaSwag":78.93,
        "MMLU":50.6,
        "TruthfulQA":50.12,
        "Winogrande":73.95,
        "GSM8K":8.19,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"a6943d2d30d0af904b3321559157d589e60f9e0f"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"mosaicml\/mpt-30b",
        "Average":52.77,
        "ARC":55.97,
        "HellaSwag":82.42,
        "MMLU":48.0,
        "TruthfulQA":38.42,
        "Winogrande":74.9,
        "GSM8K":16.91,
        "Type":"pretrained",
        "Architecture":"MPTForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":29.96,
        "Hub \u2764\ufe0f":183.0,
        "Available on the Hub":true,
        "Model Sha":"0261af71d7177453889f868d26607dec8d5aaa2e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/Llama2-13B-no_robots-alpaca-lora",
        "Average":52.77,
        "ARC":58.87,
        "HellaSwag":82.43,
        "MMLU":53.11,
        "TruthfulQA":40.46,
        "Winogrande":75.3,
        "GSM8K":6.44,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"581aba329e607533c299746bb9eb4154a7aab139"
    },
    {
        "T":"\u2b55",
        "Model":"chargoddard\/ypotryll-22b-epoch2-qlora",
        "Average":52.75,
        "ARC":59.22,
        "HellaSwag":80.66,
        "MMLU":54.52,
        "TruthfulQA":40.42,
        "Winogrande":76.32,
        "GSM8K":5.38,
        "Type":"instruction-tuned",
        "Architecture":"?",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":22.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"26fdd8fa420d72ed835c7d17086f0441db0985d4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/wizard-vicuna-13B-HF",
        "Average":52.75,
        "ARC":54.69,
        "HellaSwag":79.18,
        "MMLU":48.88,
        "TruthfulQA":49.62,
        "Winogrande":74.82,
        "GSM8K":9.33,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":48.0,
        "Available on the Hub":true,
        "Model Sha":"12dc8aacb474522ae2a83c18cb0fdf0907987f8f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/orca_mini_v2_13b",
        "Average":52.75,
        "ARC":55.12,
        "HellaSwag":79.69,
        "MMLU":50.07,
        "TruthfulQA":52.56,
        "Winogrande":72.69,
        "GSM8K":6.37,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":30.0,
        "Available on the Hub":true,
        "Model Sha":"1058709314f7ca090937d0a2b7b37b0b3a8f12a3"
    },
    {
        "T":"\u2b55",
        "Model":"Korabbit\/Llama-2-7b-chat-hf-afr-200step-flan-v2",
        "Average":52.75,
        "ARC":52.65,
        "HellaSwag":78.04,
        "MMLU":48.51,
        "TruthfulQA":45.42,
        "Winogrande":72.93,
        "GSM8K":18.95,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"35e4747656b719af659625092174f188584934c1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"totally-not-an-llm\/EverythingLM-13b-V2-16k",
        "Average":52.75,
        "ARC":58.7,
        "HellaSwag":80.88,
        "MMLU":49.69,
        "TruthfulQA":47.37,
        "Winogrande":73.01,
        "GSM8K":6.82,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":31.0,
        "Available on the Hub":true,
        "Model Sha":"943f932ae1ae462389e6d2db5273158530749fff"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"junelee\/wizard-vicuna-13b",
        "Average":52.73,
        "ARC":54.69,
        "HellaSwag":79.18,
        "MMLU":48.88,
        "TruthfulQA":49.62,
        "Winogrande":74.82,
        "GSM8K":9.17,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":70.0,
        "Available on the Hub":true,
        "Model Sha":"419dc5acc391de54a60d0b041e94e767d1ef2032"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openchat\/openchat_8192",
        "Average":52.72,
        "ARC":59.56,
        "HellaSwag":81.44,
        "MMLU":46.26,
        "TruthfulQA":46.7,
        "Winogrande":74.98,
        "GSM8K":7.35,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":220.0,
        "Available on the Hub":true,
        "Model Sha":"f661da5af278fbda8a43b19ff0250e4efc103e3e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Synthia-7B-v1.2",
        "Average":52.71,
        "ARC":54.35,
        "HellaSwag":79.29,
        "MMLU":49.33,
        "TruthfulQA":48.92,
        "Winogrande":73.56,
        "GSM8K":10.84,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"85ea4f4818478084eedd01e958ac5cc7cf64b3bb"
    },
    {
        "T":"?",
        "Model":"PocketDoc\/Dans-PersonalityEngine-13b",
        "Average":52.71,
        "ARC":58.45,
        "HellaSwag":82.3,
        "MMLU":47.58,
        "TruthfulQA":41.12,
        "Winogrande":77.51,
        "GSM8K":9.33,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"3b37c31e04419adcc91eddb57f24fd6f9ac91938"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"meta-math\/MetaMath-13B-V1.0",
        "Average":52.71,
        "ARC":49.49,
        "HellaSwag":76.48,
        "MMLU":47.74,
        "TruthfulQA":41.58,
        "Winogrande":72.45,
        "GSM8K":28.51,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"0b448f6f64808f8bca94dc871e96a3eae7e95621"
    },
    {
        "T":"\u2b55",
        "Model":"Yehoon\/yehoon_llama2",
        "Average":52.71,
        "ARC":54.78,
        "HellaSwag":78.98,
        "MMLU":51.29,
        "TruthfulQA":49.17,
        "Winogrande":74.74,
        "GSM8K":7.28,
        "Type":"instruction-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"443cb81ce988ea6c0b1e20132c170463d559367e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"luffycodes\/mcq-hal-vicuna-13b-v1.5",
        "Average":52.7,
        "ARC":55.97,
        "HellaSwag":80.72,
        "MMLU":52.85,
        "TruthfulQA":45.03,
        "Winogrande":72.77,
        "GSM8K":8.87,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"bb3029bce8347b09c2fd6908475b195bcabe53e3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/Nous-Capybara-7B",
        "Average":52.7,
        "ARC":55.29,
        "HellaSwag":80.73,
        "MMLU":48.72,
        "TruthfulQA":51.13,
        "Winogrande":73.32,
        "GSM8K":6.97,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":15.0,
        "Available on the Hub":true,
        "Model Sha":"42dfc6f7d735670e2f3e30b0919708a81f9a0df9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HyperbeeAI\/Tulpar-7b-v0",
        "Average":52.69,
        "ARC":56.31,
        "HellaSwag":79.01,
        "MMLU":52.55,
        "TruthfulQA":51.68,
        "Winogrande":73.88,
        "GSM8K":2.73,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":22.0,
        "Available on the Hub":true,
        "Model Sha":"d7c2bc52a3ae13571357f51273ae948caf84400e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/Capybara-7B",
        "Average":52.69,
        "ARC":55.2,
        "HellaSwag":80.76,
        "MMLU":48.8,
        "TruthfulQA":51.07,
        "Winogrande":73.4,
        "GSM8K":6.9,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":15.0,
        "Available on the Hub":true,
        "Model Sha":"42dfc6f7d735670e2f3e30b0919708a81f9a0df9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Undi95\/CodeEngine",
        "Average":52.68,
        "ARC":58.36,
        "HellaSwag":82.27,
        "MMLU":54.18,
        "TruthfulQA":45.18,
        "Winogrande":74.59,
        "GSM8K":1.52,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f57879831c39f2dcb656cb2c9e9ce5878e92bb44"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"luffycodes\/mcq-vicuna-13b-v1.5",
        "Average":52.68,
        "ARC":56.66,
        "HellaSwag":81.09,
        "MMLU":53.3,
        "TruthfulQA":43.99,
        "Winogrande":73.01,
        "GSM8K":8.04,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f769a92cfeffe8ee07beee8814ce7eca7cd62805"
    },
    {
        "T":"\u2b55",
        "Model":"teknium\/Mistral-Trismegistus-7B",
        "Average":52.66,
        "ARC":54.1,
        "HellaSwag":77.91,
        "MMLU":54.49,
        "TruthfulQA":49.36,
        "Winogrande":70.17,
        "GSM8K":9.93,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":59.0,
        "Available on the Hub":true,
        "Model Sha":"0a5752d096ebab21759dbe203f6b7c7f6092faf2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-l2-13b-gpt4-m2.0",
        "Average":52.66,
        "ARC":59.22,
        "HellaSwag":81.02,
        "MMLU":53.73,
        "TruthfulQA":39.7,
        "Winogrande":73.64,
        "GSM8K":8.64,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":26.0,
        "Available on the Hub":true,
        "Model Sha":"a852b77f7d0777092c76898bc83f8e657ca2af3e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LeoLM\/leo-hessianai-13b",
        "Average":52.65,
        "ARC":57.25,
        "HellaSwag":81.94,
        "MMLU":53.65,
        "TruthfulQA":38.03,
        "Winogrande":76.09,
        "GSM8K":8.95,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":16.0,
        "Available on the Hub":true,
        "Model Sha":"a947965cb07ca12a38ff981fe65b618d7dea28d3"
    },
    {
        "T":"\u2b55",
        "Model":"Korabbit\/Llama-2-7b-chat-hf-afr-200step-flan",
        "Average":52.62,
        "ARC":52.47,
        "HellaSwag":78.02,
        "MMLU":48.42,
        "TruthfulQA":45.47,
        "Winogrande":72.69,
        "GSM8K":18.65,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"03550d05aac147dde6d70b7b63f4a1661ecf5cb3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-mistral-7b-v13.1",
        "Average":52.62,
        "ARC":52.56,
        "HellaSwag":75.73,
        "MMLU":56.68,
        "TruthfulQA":50.44,
        "Winogrande":71.59,
        "GSM8K":8.72,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.13,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"b64386bde3d7850a01df763f5c777c74888d34fc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"heegyu\/LIMA-13b-hf",
        "Average":52.61,
        "ARC":57.42,
        "HellaSwag":81.68,
        "MMLU":48.72,
        "TruthfulQA":41.76,
        "Winogrande":77.19,
        "GSM8K":8.87,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"98faa74a9b41cbd9033904cd58420705936849eb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LinkSoul\/Chinese-Llama-2-7b",
        "Average":52.59,
        "ARC":52.99,
        "HellaSwag":75.64,
        "MMLU":50.74,
        "TruthfulQA":48.94,
        "Winogrande":72.77,
        "GSM8K":14.48,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"openrail",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":261.0,
        "Available on the Hub":true,
        "Model Sha":"72efd71d7f89d9c46008b7a574faf90300ed9ba8"
    },
    {
        "T":"?",
        "Model":"frank098\/Wizard-Vicuna-13B-juniper",
        "Average":52.55,
        "ARC":55.89,
        "HellaSwag":79.75,
        "MMLU":44.99,
        "TruthfulQA":54.72,
        "Winogrande":72.69,
        "GSM8K":7.28,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"24f58beb9ed4cf635fc962853ed71d0f4b1909ba"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wahaha1987\/llama_13b_sharegpt94k_fastchat",
        "Average":52.55,
        "ARC":53.75,
        "HellaSwag":79.47,
        "MMLU":51.5,
        "TruthfulQA":49.54,
        "Winogrande":72.61,
        "GSM8K":8.42,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"388bc2f82a1ee8b963c7f94f9c7b6743f7214306"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"luffycodes\/mcq-vicuna-13b-v1.5",
        "Average":52.55,
        "ARC":56.23,
        "HellaSwag":81.15,
        "MMLU":53.38,
        "TruthfulQA":44.08,
        "Winogrande":72.93,
        "GSM8K":7.51,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f769a92cfeffe8ee07beee8814ce7eca7cd62805"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-codellama-dolphin-orca-platypus-34b",
        "Average":52.53,
        "ARC":52.47,
        "HellaSwag":74.13,
        "MMLU":53.47,
        "TruthfulQA":47.14,
        "Winogrande":73.24,
        "GSM8K":14.71,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"57e18e617b4fd7ab61bd7da8ee9516513ad76842"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"speechlessai\/speechless-codellama-34b-v1.0",
        "Average":52.53,
        "ARC":52.47,
        "HellaSwag":74.13,
        "MMLU":53.47,
        "TruthfulQA":47.14,
        "Winogrande":73.24,
        "GSM8K":14.71,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"1d64d871cd56da3031e19bc267ef8bd0b85b9936"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-codellama-34b-v2.0",
        "Average":52.51,
        "ARC":54.35,
        "HellaSwag":75.65,
        "MMLU":54.67,
        "TruthfulQA":45.21,
        "Winogrande":73.56,
        "GSM8K":11.6,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"cb81174d72dbe06f8db1c406ef97981532de6f09"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lvkaokao\/llama2-7b-hf-chat-lora-v2",
        "Average":52.5,
        "ARC":55.03,
        "HellaSwag":78.81,
        "MMLU":51.35,
        "TruthfulQA":44.05,
        "Winogrande":74.9,
        "GSM8K":10.84,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"0b8e61d3325cddbad207cbf885c2b5db6a83a059"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-l2-13b-gpt4-2.0",
        "Average":52.49,
        "ARC":59.04,
        "HellaSwag":82.82,
        "MMLU":54.71,
        "TruthfulQA":36.47,
        "Winogrande":74.19,
        "GSM8K":7.73,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":14.0,
        "Available on the Hub":true,
        "Model Sha":"ec556571acc6783fea4414e4ca72d291c563b6dc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Yukang\/Llama-2-13b-longlora-32k-ft",
        "Average":52.49,
        "ARC":59.47,
        "HellaSwag":82.61,
        "MMLU":52.13,
        "TruthfulQA":37.44,
        "Winogrande":75.53,
        "GSM8K":7.73,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"6d17c854025b0bd54ce572ac803f1bb052875dbf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lvkaokao\/llama2-7b-hf-chat-lora-v3",
        "Average":52.48,
        "ARC":57.25,
        "HellaSwag":78.62,
        "MMLU":50.57,
        "TruthfulQA":50.62,
        "Winogrande":76.32,
        "GSM8K":1.52,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"79047f667253c878ad3143b016e3dcb3df707572"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"beaugogh\/Llama2-7b-openorca-mc-v2",
        "Average":52.47,
        "ARC":55.55,
        "HellaSwag":81.26,
        "MMLU":48.3,
        "TruthfulQA":51.49,
        "Winogrande":72.85,
        "GSM8K":5.38,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"1e74a9cca843cdeb8591d4e4f4320dc1870adf1b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lizhuang144\/llama_mirror_13b_v1.0",
        "Average":52.46,
        "ARC":57.59,
        "HellaSwag":80.53,
        "MMLU":48.0,
        "TruthfulQA":44.54,
        "Winogrande":76.64,
        "GSM8K":7.43,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"379cb8f080110f3418155029f534f67a79e25db4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bofenghuang\/vigogne-2-7b-chat",
        "Average":52.45,
        "ARC":55.63,
        "HellaSwag":78.71,
        "MMLU":50.98,
        "TruthfulQA":47.21,
        "Winogrande":74.43,
        "GSM8K":7.73,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":14.0,
        "Available on the Hub":true,
        "Model Sha":"7a1b76feabe3e0ed007ea83ee93f7644156d3b23"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ausboss\/llama-13b-supercot",
        "Average":52.44,
        "ARC":56.06,
        "HellaSwag":81.71,
        "MMLU":45.36,
        "TruthfulQA":48.55,
        "Winogrande":75.77,
        "GSM8K":7.2,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"f6953fa162b487a3d4c6bdc7b7951e09576c2ae5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"camel-ai\/CAMEL-13B-Combined-Data",
        "Average":52.44,
        "ARC":55.63,
        "HellaSwag":79.25,
        "MMLU":49.74,
        "TruthfulQA":47.42,
        "Winogrande":75.45,
        "GSM8K":7.13,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":11.0,
        "Available on the Hub":true,
        "Model Sha":"6d98f2801f13d89de7978ee9f348a52ea46a24ec"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PocketDoc\/Dans-PileOfSets-Mk1-llama-13b-merged",
        "Average":52.43,
        "ARC":58.79,
        "HellaSwag":81.79,
        "MMLU":48.12,
        "TruthfulQA":41.24,
        "Winogrande":76.16,
        "GSM8K":8.49,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"a7e5484df8aceae7800ae9301a3954cf74b527e9"
    },
    {
        "T":"\u2b55",
        "Model":"Korabbit\/Llama-2-7b-chat-hf-afr-300step-flan-v2",
        "Average":52.41,
        "ARC":52.56,
        "HellaSwag":77.76,
        "MMLU":48.51,
        "TruthfulQA":45.14,
        "Winogrande":72.53,
        "GSM8K":17.97,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"a2191bd90b04396016b7420dd14675916056f44a"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/PuddleJumper-Platypus2-13B-QLoRA-0.80-epoch",
        "Average":52.41,
        "ARC":54.52,
        "HellaSwag":79.36,
        "MMLU":55.15,
        "TruthfulQA":54.32,
        "Winogrande":71.11,
        "GSM8K":0.0,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"4b5aabc51907e4cba49f373c6dc09a2634f2fb8a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zarakiquemparte\/zararp-l2-7b",
        "Average":52.39,
        "ARC":56.31,
        "HellaSwag":79.19,
        "MMLU":51.36,
        "TruthfulQA":51.26,
        "Winogrande":74.51,
        "GSM8K":1.74,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"6032c5106970f98d59925959fbd330ae4b1d1a7e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"digitous\/Alpacino13b",
        "Average":52.39,
        "ARC":58.53,
        "HellaSwag":81.31,
        "MMLU":47.92,
        "TruthfulQA":41.66,
        "Winogrande":76.95,
        "GSM8K":7.96,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":29.0,
        "Available on the Hub":true,
        "Model Sha":"7092a5c8dec649694dd66ff8cfe5452ce52e6a40"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mncai\/Llama2-7B-guanaco-dolphin-500",
        "Average":52.38,
        "ARC":56.74,
        "HellaSwag":81.63,
        "MMLU":48.69,
        "TruthfulQA":46.94,
        "Winogrande":74.27,
        "GSM8K":5.99,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"afe00170f084f773e401ba7d738d692533cca6b4"
    },
    {
        "T":"\u2b55",
        "Model":"The-Face-Of-Goonery\/Huginn-22b-Prototype",
        "Average":52.36,
        "ARC":57.68,
        "HellaSwag":80.69,
        "MMLU":49.81,
        "TruthfulQA":52.11,
        "Winogrande":71.59,
        "GSM8K":2.27,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":21.83,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"29222b05794abb862ad0aaaf3020696c9f599810"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"totally-not-an-llm\/EverythingLM-13b-16k",
        "Average":52.33,
        "ARC":56.57,
        "HellaSwag":80.58,
        "MMLU":50.18,
        "TruthfulQA":47.46,
        "Winogrande":72.77,
        "GSM8K":6.44,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":29.0,
        "Available on the Hub":true,
        "Model Sha":"8456a856a8b115b05e76a7d0d945853b10ac71e2"
    },
    {
        "T":"\u2b55",
        "Model":"beaugogh\/Llama2-7b-openorca-mc-v2-dpo",
        "Average":52.32,
        "ARC":54.78,
        "HellaSwag":81.48,
        "MMLU":47.2,
        "TruthfulQA":53.13,
        "Winogrande":72.85,
        "GSM8K":4.47,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"734a6f0c69e1e53b988c107926bc17cb0536f851"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-atom-13b-v9-bf16",
        "Average":52.31,
        "ARC":51.19,
        "HellaSwag":75.99,
        "MMLU":49.33,
        "TruthfulQA":48.66,
        "Winogrande":73.32,
        "GSM8K":15.39,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.94,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"35bb2c73953f6ea40be6f0c8c6b2dfa7ecbaa0df"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-13b-gpt4-1.2",
        "Average":52.31,
        "ARC":58.36,
        "HellaSwag":81.61,
        "MMLU":48.84,
        "TruthfulQA":47.54,
        "Winogrande":73.64,
        "GSM8K":3.87,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"482bd38b65e73fde13f5d03fed2bee7acda8fadd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"eachadea\/vicuna-13b",
        "Average":52.3,
        "ARC":51.71,
        "HellaSwag":79.94,
        "MMLU":50.84,
        "TruthfulQA":52.68,
        "Winogrande":71.03,
        "GSM8K":7.58,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":95.0,
        "Available on the Hub":true,
        "Model Sha":"ac4218770a58baaaaf25201076fe082abb6ffd13"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"prithivida\/Asimov-7B-v2",
        "Average":52.29,
        "ARC":54.27,
        "HellaSwag":78.72,
        "MMLU":52.59,
        "TruthfulQA":45.44,
        "Winogrande":71.82,
        "GSM8K":10.92,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"4bit",
        "Hub License":"mit",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"0aeea2284ac78cac081bee88e5a98a19bb987227"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mncai\/Llama2-7B-guanaco-1k",
        "Average":52.28,
        "ARC":55.12,
        "HellaSwag":80.53,
        "MMLU":47.93,
        "TruthfulQA":47.69,
        "Winogrande":74.82,
        "GSM8K":7.58,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"5f3194b779897bbc4c4218a9dddc44a9b5faea15"
    },
    {
        "T":"\u2b55",
        "Model":"TFLai\/Platypus2-13B-QLoRA-0.80-epoch",
        "Average":52.27,
        "ARC":57.76,
        "HellaSwag":81.63,
        "MMLU":55.63,
        "TruthfulQA":39.7,
        "Winogrande":75.93,
        "GSM8K":2.96,
        "Type":"instruction-tuned",
        "Architecture":"?",
        "Precision":"4bit",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"114eb8efd2de1c9eae85d92de490b95c854dfae9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"oh-yeontaek\/llama-2-7B-LoRA-assemble",
        "Average":52.26,
        "ARC":57.34,
        "HellaSwag":78.81,
        "MMLU":50.75,
        "TruthfulQA":53.18,
        "Winogrande":73.48,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"72e866a96a2e9afc6527c8d757c69088c3a069c8"
    },
    {
        "T":"\u2b55",
        "Model":"Korabbit\/Llama-2-7b-chat-hf-afr-200step-merged",
        "Average":52.26,
        "ARC":52.05,
        "HellaSwag":77.38,
        "MMLU":48.65,
        "TruthfulQA":44.6,
        "Winogrande":71.9,
        "GSM8K":18.95,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"858de1c14854e55d5141b8d1b3954b335044669e"
    },
    {
        "T":"\u2b55",
        "Model":"beaugogh\/Llama2-7b-openorca-mc-v1",
        "Average":52.24,
        "ARC":55.63,
        "HellaSwag":80.17,
        "MMLU":48.44,
        "TruthfulQA":51.62,
        "Winogrande":73.48,
        "GSM8K":4.09,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"2c4096fa2129665fb127f1c2a1302f30565a5265"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zarakiquemparte\/zararp-1.1-l2-7b",
        "Average":52.22,
        "ARC":56.48,
        "HellaSwag":78.85,
        "MMLU":51.49,
        "TruthfulQA":51.99,
        "Winogrande":73.4,
        "GSM8K":1.14,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"31fa6527a3285d5fd320219d7c2dadde07b83718"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Nous-Hermes-13B-SuperHOT-8K-fp16",
        "Average":52.18,
        "ARC":55.29,
        "HellaSwag":81.87,
        "MMLU":48.23,
        "TruthfulQA":51.19,
        "Winogrande":75.3,
        "GSM8K":1.21,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"b407c1ece029ad5693d38e6e0931e9482962ed15"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dvruette\/oasst-llama-13b-1000-steps",
        "Average":52.18,
        "ARC":58.11,
        "HellaSwag":81.52,
        "MMLU":48.65,
        "TruthfulQA":35.99,
        "Winogrande":77.51,
        "GSM8K":11.3,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"d2cd599cc40db3370009f45d6caa7e486cb6d31f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HyperbeeAI\/Tulpar-7b-v1",
        "Average":52.16,
        "ARC":57.0,
        "HellaSwag":79.69,
        "MMLU":51.33,
        "TruthfulQA":51.83,
        "Winogrande":72.45,
        "GSM8K":0.68,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"719d8e1eb4a820f01e0a92ef6220d041964bb472"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/WizardLM-13B-V1-1-SuperHOT-8K-GPTQ",
        "Average":52.15,
        "ARC":57.0,
        "HellaSwag":80.32,
        "MMLU":47.08,
        "TruthfulQA":53.46,
        "Winogrande":74.35,
        "GSM8K":0.68,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"GPTQ",
        "Hub License":"other",
        "#Params (B)":16.22,
        "Hub \u2764\ufe0f":44.0,
        "Available on the Hub":true,
        "Model Sha":"085eb5cd394f30d72bf5efcf83a580e87264b3e8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Stheno-1.3-L2-13B",
        "Average":52.15,
        "ARC":56.83,
        "HellaSwag":81.7,
        "MMLU":52.79,
        "TruthfulQA":50.23,
        "Winogrande":71.11,
        "GSM8K":0.23,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"45ba2f603769aa6b97639962f522b8d7398c2393"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"xxyyy123\/mc_data_30k_from_platpus_orca_7b_10k_v1_lora_qkvo_rank14_v2",
        "Average":52.13,
        "ARC":57.17,
        "HellaSwag":79.57,
        "MMLU":50.24,
        "TruthfulQA":52.51,
        "Winogrande":72.93,
        "GSM8K":0.38,
        "Type":"RL-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"9c4a7444d6fb12931e50f111053e016531fe60b7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"StudentLLM\/Alpagasus-2-13B-QLoRA-pipeline",
        "Average":52.13,
        "ARC":58.28,
        "HellaSwag":80.98,
        "MMLU":54.14,
        "TruthfulQA":34.21,
        "Winogrande":75.93,
        "GSM8K":9.25,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"4bit",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":false,
        "Model Sha":"86329885e029c1f4fb6ff6b6f3409007158499e7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Xwin-LM\/Xwin-LM-7B-V0.1",
        "Average":52.08,
        "ARC":56.57,
        "HellaSwag":79.4,
        "MMLU":49.98,
        "TruthfulQA":47.89,
        "Winogrande":73.32,
        "GSM8K":5.31,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":73.0,
        "Available on the Hub":true,
        "Model Sha":"470e680120a7249d6e8a875345015ddba1711100"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lmsys\/vicuna-7b-v1.5",
        "Average":52.06,
        "ARC":53.24,
        "HellaSwag":77.39,
        "MMLU":51.04,
        "TruthfulQA":50.34,
        "Winogrande":72.14,
        "GSM8K":8.19,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":70.0,
        "Available on the Hub":true,
        "Model Sha":"de56c35b1763eaae20f4d60efd64af0a9091ebe5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"l3utterfly\/llama2-7b-layla",
        "Average":52.05,
        "ARC":54.18,
        "HellaSwag":79.34,
        "MMLU":49.7,
        "TruthfulQA":46.5,
        "Winogrande":74.11,
        "GSM8K":8.49,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"733016abcd2abee63eb45ed63d2bba14b91da217"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LTC-AI-Labs\/L2-7b-Beluga-WVG-Test",
        "Average":52.04,
        "ARC":53.75,
        "HellaSwag":78.38,
        "MMLU":51.57,
        "TruthfulQA":45.76,
        "Winogrande":74.9,
        "GSM8K":7.88,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"b90c207e248c0ad541274c2eb5ef76da1181802f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lvkaokao\/llama2-7b-hf-chat-lora",
        "Average":52.03,
        "ARC":55.72,
        "HellaSwag":78.75,
        "MMLU":47.99,
        "TruthfulQA":43.11,
        "Winogrande":75.85,
        "GSM8K":10.77,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"e92a1439ac8d2edb5e311b8a42e13ed7c5e70db5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bofenghuang\/vigogne-2-7b-instruct",
        "Average":52.02,
        "ARC":56.23,
        "HellaSwag":79.97,
        "MMLU":47.17,
        "TruthfulQA":49.51,
        "Winogrande":75.45,
        "GSM8K":3.79,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":17.0,
        "Available on the Hub":true,
        "Model Sha":"8f4dd9c870f748322989168af5c109e16b01c63d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"haonan-li\/bactrian-x-llama-13b-merged",
        "Average":52.0,
        "ARC":56.4,
        "HellaSwag":79.33,
        "MMLU":48.4,
        "TruthfulQA":48.38,
        "Winogrande":73.95,
        "GSM8K":5.53,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"cc5ee2231066c147423f89e9df40f7364c3275a5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lmsys\/vicuna-7b-v1.5",
        "Average":51.99,
        "ARC":53.24,
        "HellaSwag":77.39,
        "MMLU":50.82,
        "TruthfulQA":50.33,
        "Winogrande":72.06,
        "GSM8K":8.11,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":70.0,
        "Available on the Hub":true,
        "Model Sha":"de56c35b1763eaae20f4d60efd64af0a9091ebe5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JosephusCheung\/Qwen-LLaMAfied-7B-Chat",
        "Average":51.99,
        "ARC":50.94,
        "HellaSwag":83.47,
        "MMLU":53.52,
        "TruthfulQA":46.09,
        "Winogrande":73.16,
        "GSM8K":4.78,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"4d70cf0047a7a5cd2c864bc2606e81f0830e4405"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-mistral-7b-v13-base",
        "Average":51.99,
        "ARC":52.9,
        "HellaSwag":76.12,
        "MMLU":57.54,
        "TruthfulQA":52.82,
        "Winogrande":71.35,
        "GSM8K":1.21,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.13,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"8ff18d61b1c8295ecd73153b8e0b63934187a50e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/spicyboros-7b-2.2",
        "Average":51.95,
        "ARC":56.57,
        "HellaSwag":80.09,
        "MMLU":48.47,
        "TruthfulQA":47.22,
        "Winogrande":74.51,
        "GSM8K":4.85,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":20.0,
        "Available on the Hub":true,
        "Model Sha":"fdf075081555f3ed84c037e8dd3fe85c3b3609d7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"xxyyy123\/10k_v1_lora_qkvo_rank28_v2",
        "Average":51.95,
        "ARC":55.38,
        "HellaSwag":79.21,
        "MMLU":50.5,
        "TruthfulQA":52.75,
        "Winogrande":73.24,
        "GSM8K":0.61,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"70e38a7424544193f0ad6a93ae26a5bfd15e4e90"
    },
    {
        "T":"\u2b55",
        "Model":"pe-nlp\/llama-2-13b-vicuna-wizard",
        "Average":51.94,
        "ARC":57.76,
        "HellaSwag":82.16,
        "MMLU":54.68,
        "TruthfulQA":41.11,
        "Winogrande":74.98,
        "GSM8K":0.91,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"b51bf8c4e132308751cc8b9d9c1131539f79f07f"
    },
    {
        "T":"\u2b55",
        "Model":"chinoll\/Yi-7b-dpo",
        "Average":51.93,
        "ARC":43.09,
        "HellaSwag":74.53,
        "MMLU":64.0,
        "TruthfulQA":45.51,
        "Winogrande":73.09,
        "GSM8K":11.37,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.06,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"925c5fbaeccb321ba8edbde79c3d994adc460a41"
    },
    {
        "T":"\u2b55",
        "Model":"chinoll\/Yi-6b-200k-dpo",
        "Average":51.93,
        "ARC":43.09,
        "HellaSwag":74.53,
        "MMLU":64.0,
        "TruthfulQA":45.51,
        "Winogrande":73.09,
        "GSM8K":11.37,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.06,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"925c5fbaeccb321ba8edbde79c3d994adc460a41"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/Nous-Hermes-llama-2-7b",
        "Average":51.87,
        "ARC":55.12,
        "HellaSwag":78.94,
        "MMLU":48.34,
        "TruthfulQA":49.01,
        "Winogrande":74.03,
        "GSM8K":5.76,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":[
            "mit"
        ],
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":48.0,
        "Available on the Hub":true,
        "Model Sha":"60e58acecdc1552e1b1752a38d1d91d942d1c3f0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ashercn97\/manatee-7b",
        "Average":51.84,
        "ARC":54.52,
        "HellaSwag":78.95,
        "MMLU":49.26,
        "TruthfulQA":46.77,
        "Winogrande":74.51,
        "GSM8K":7.05,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"e66094c43ffe6c5b3f4164cd4ba048d3bc422fd0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"migtissera\/Synthia-7B",
        "Average":51.83,
        "ARC":56.14,
        "HellaSwag":78.6,
        "MMLU":50.35,
        "TruthfulQA":45.03,
        "Winogrande":74.27,
        "GSM8K":6.6,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"4f9e95665d95b4c692910190ff77257216e476f1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Medusa-1.1-L2-7B",
        "Average":51.8,
        "ARC":56.48,
        "HellaSwag":78.57,
        "MMLU":51.56,
        "TruthfulQA":47.7,
        "Winogrande":75.06,
        "GSM8K":1.44,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"df23c3d22bc546dbce0267415e94bdb482446c06"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"elliotthwang\/Elliott-Chinese-LLaMa-GPTQ",
        "Average":51.79,
        "ARC":51.02,
        "HellaSwag":75.23,
        "MMLU":49.58,
        "TruthfulQA":45.09,
        "Winogrande":72.61,
        "GSM8K":17.21,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":53.9,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"bbbca62bb340b4ae0a19ba93dae38fc9f9787c16"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Sao10K\/Stheno-Mix-L2-20B",
        "Average":51.79,
        "ARC":57.76,
        "HellaSwag":79.63,
        "MMLU":52.51,
        "TruthfulQA":51.8,
        "Winogrande":68.98,
        "GSM8K":0.08,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":20.63,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"6f9dcdaae6ef9071effe63d2107abe8b9712345b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-13b-gpt4-1.3",
        "Average":51.76,
        "ARC":58.53,
        "HellaSwag":81.6,
        "MMLU":46.96,
        "TruthfulQA":45.29,
        "Winogrande":75.85,
        "GSM8K":2.35,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"32a474742c2a235ca12c96afaea57dcb6b46ef56"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Lazycuber\/L2-7b-Orca-WVG-Test",
        "Average":51.72,
        "ARC":54.86,
        "HellaSwag":78.25,
        "MMLU":51.13,
        "TruthfulQA":43.68,
        "Winogrande":74.35,
        "GSM8K":8.04,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"6073a87872eb36149404bfb7d60e0108074ee1c3"
    },
    {
        "T":"\u2b55",
        "Model":"Azure99\/blossom-v2-llama2-7b",
        "Average":51.71,
        "ARC":54.1,
        "HellaSwag":78.57,
        "MMLU":51.66,
        "TruthfulQA":46.84,
        "Winogrande":74.35,
        "GSM8K":4.78,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"8c71cdb481ce6bbda3b2042e5526a232ab23825c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jphme\/em_german_leo_mistral",
        "Average":51.69,
        "ARC":52.82,
        "HellaSwag":78.03,
        "MMLU":50.03,
        "TruthfulQA":50.19,
        "Winogrande":73.48,
        "GSM8K":5.61,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"aa63a32154923034fb89b1408d3d7ffa994d3327"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"haoranxu\/ALMA-13B-Pretrain",
        "Average":51.68,
        "ARC":56.91,
        "HellaSwag":80.15,
        "MMLU":50.31,
        "TruthfulQA":37.44,
        "Winogrande":76.4,
        "GSM8K":8.87,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"69e9e12d8bab66dffdcb15fa534fc3f0dc34acec"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YeungNLP\/firefly-ziya-13b",
        "Average":51.67,
        "ARC":55.38,
        "HellaSwag":78.47,
        "MMLU":45.18,
        "TruthfulQA":49.29,
        "Winogrande":74.82,
        "GSM8K":6.9,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.89,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"9a21051ae490d2f8ab8b1181c1b45e0412d71a90"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LTC-AI-Labs\/L2-7b-Base-test-WVG",
        "Average":51.66,
        "ARC":54.27,
        "HellaSwag":77.81,
        "MMLU":51.07,
        "TruthfulQA":46.28,
        "Winogrande":73.56,
        "GSM8K":6.97,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"2491546f1219c3e9bb1a8cf37fbecf0b299c2177"
    },
    {
        "T":"\u2b55",
        "Model":"rombodawg\/LosslessMegaCoder-llama2-7b-mini",
        "Average":51.66,
        "ARC":53.5,
        "HellaSwag":77.38,
        "MMLU":49.72,
        "TruthfulQA":45.77,
        "Winogrande":74.03,
        "GSM8K":9.55,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"186b105d61054611d0b921a55c220d41c6aefe43"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"elliotthwang\/Elliott-Chinese-LLaMa-GPTQ-V1.0",
        "Average":51.64,
        "ARC":50.68,
        "HellaSwag":75.36,
        "MMLU":49.33,
        "TruthfulQA":44.7,
        "Winogrande":72.38,
        "GSM8K":17.36,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":53.9,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"01305dc473ba231519fe71e7f4b2d1e3f6aa9bc8"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"TheBloke\/stable-vicuna-13B-HF",
        "Average":51.64,
        "ARC":53.33,
        "HellaSwag":78.5,
        "MMLU":50.29,
        "TruthfulQA":48.38,
        "Winogrande":75.22,
        "GSM8K":4.09,
        "Type":"RL-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":95.0,
        "Available on the Hub":true,
        "Model Sha":"2b099b2be0dafb2606ae9808c0f6183fe4bff7bc"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/Platypus2xOpenOrca-13B-LoRa-v2",
        "Average":51.61,
        "ARC":58.62,
        "HellaSwag":81.17,
        "MMLU":50.23,
        "TruthfulQA":43.43,
        "Winogrande":76.16,
        "GSM8K":0.08,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"568ac6a5f1a9f5eb6bc09efb2188740d771ed0e9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rufjdk5480\/llama-7b-ludwig-alpaca",
        "Average":51.6,
        "ARC":54.01,
        "HellaSwag":78.73,
        "MMLU":45.8,
        "TruthfulQA":41.91,
        "Winogrande":74.27,
        "GSM8K":14.86,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":false,
        "Model Sha":"7928584c0329c3ed88915a823033908be90ba657"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lmsys\/vicuna-7b-v1.5-16k",
        "Average":51.58,
        "ARC":54.69,
        "HellaSwag":77.32,
        "MMLU":49.51,
        "TruthfulQA":50.41,
        "Winogrande":71.11,
        "GSM8K":6.44,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":55.0,
        "Available on the Hub":true,
        "Model Sha":"9a93d7d11fac7f3f9074510b80092b53bc1a5bec"
    },
    {
        "T":"\u2b55",
        "Model":"Envoid\/Yousei-22B",
        "Average":51.56,
        "ARC":55.89,
        "HellaSwag":78.55,
        "MMLU":52.31,
        "TruthfulQA":50.68,
        "Winogrande":71.51,
        "GSM8K":0.45,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":21.83,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"ae8f93963266d31000433f1a52d43435e1473e2b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PocketDoc\/Dans-MysteryModel-13b",
        "Average":51.54,
        "ARC":57.0,
        "HellaSwag":80.35,
        "MMLU":52.06,
        "TruthfulQA":45.0,
        "Winogrande":74.82,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"c38a9df20162455b53eb35d38a9b67fb824559e8"
    },
    {
        "T":"\u2b55",
        "Model":"lvkaokao\/llama2-7b-hf-instruction-lora",
        "Average":51.54,
        "ARC":55.38,
        "HellaSwag":78.57,
        "MMLU":49.39,
        "TruthfulQA":41.83,
        "Winogrande":74.19,
        "GSM8K":9.86,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f660a40323b29040e78097acca320517ed242512"
    },
    {
        "T":"\u2b55",
        "Model":"jondurbin\/airoboros-c34b-2.1",
        "Average":51.52,
        "ARC":54.69,
        "HellaSwag":76.45,
        "MMLU":55.08,
        "TruthfulQA":46.15,
        "Winogrande":68.43,
        "GSM8K":8.34,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":13.0,
        "Available on the Hub":true,
        "Model Sha":"2caa8ce3aab012bf34c7c531827f6befc7cc1c98"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"elliotthwang\/Elliott-Chinese-LLaMa-GPTQ-V2.0",
        "Average":51.47,
        "ARC":50.77,
        "HellaSwag":75.36,
        "MMLU":49.41,
        "TruthfulQA":44.7,
        "Winogrande":72.61,
        "GSM8K":16.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":52.86,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"ebffe57ba6cc70b60ff5295889abc62d91eeb4dd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Lajonbot\/vicuna-7b-v1.5-PL-lora_unload",
        "Average":51.46,
        "ARC":53.5,
        "HellaSwag":76.74,
        "MMLU":49.69,
        "TruthfulQA":49.68,
        "Winogrande":71.98,
        "GSM8K":7.2,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"92bf763ce7ae0bfe155bfd60190eed64582e5080"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JosephusCheung\/Pwen-VL-Chat-20_30",
        "Average":51.45,
        "ARC":50.17,
        "HellaSwag":72.21,
        "MMLU":56.34,
        "TruthfulQA":42.52,
        "Winogrande":68.35,
        "GSM8K":19.11,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"64a9b89fb18140fc1af1f11471dc9fe34ebc7446"
    },
    {
        "T":"?",
        "Model":"amazon\/MistralLite",
        "Average":51.45,
        "ARC":59.56,
        "HellaSwag":81.84,
        "MMLU":50.93,
        "TruthfulQA":37.87,
        "Winogrande":77.43,
        "GSM8K":1.06,
        "Type":"",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":334.0,
        "Available on the Hub":true,
        "Model Sha":"23486089ab7ba741b34adc69ab7555885f8abe71"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"frank098\/WizardLM_13B_juniper",
        "Average":51.45,
        "ARC":55.38,
        "HellaSwag":77.2,
        "MMLU":45.46,
        "TruthfulQA":51.5,
        "Winogrande":71.11,
        "GSM8K":8.04,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"2204970fc0d96b071e2b1b003fbc5c87cfc46840"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"itsliupeng\/llama2_7b_zh",
        "Average":51.44,
        "ARC":52.05,
        "HellaSwag":74.88,
        "MMLU":60.69,
        "TruthfulQA":42.86,
        "Winogrande":71.74,
        "GSM8K":6.44,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"mit",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"410711781d2e24226c0d62959e4990d1de851c3c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"tlphams\/zoyllm-7b-slimorca",
        "Average":51.44,
        "ARC":50.6,
        "HellaSwag":72.12,
        "MMLU":48.78,
        "TruthfulQA":49.13,
        "Winogrande":67.32,
        "GSM8K":20.7,
        "Type":"fine-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":7.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"4b49caa2c42b3e8757f986624b047dab485ee26f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"camel-ai\/CAMEL-13B-Role-Playing-Data",
        "Average":51.42,
        "ARC":54.95,
        "HellaSwag":79.25,
        "MMLU":46.61,
        "TruthfulQA":46.35,
        "Winogrande":74.03,
        "GSM8K":7.35,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":12.0,
        "Available on the Hub":true,
        "Model Sha":"762ecb0d85572c8f8bcbca06d27f7f64a4d74615"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lmsys\/vicuna-7b-v1.5-16k",
        "Average":51.42,
        "ARC":54.18,
        "HellaSwag":77.31,
        "MMLU":49.3,
        "TruthfulQA":50.35,
        "Winogrande":71.03,
        "GSM8K":6.37,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":55.0,
        "Available on the Hub":true,
        "Model Sha":"9a93d7d11fac7f3f9074510b80092b53bc1a5bec"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hiyouga\/Baichuan2-7B-Chat-LLaMAfied",
        "Average":51.42,
        "ARC":52.47,
        "HellaSwag":74.04,
        "MMLU":53.88,
        "TruthfulQA":48.04,
        "Winogrande":69.14,
        "GSM8K":10.92,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.99,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"da2cd76e2d61bf0247bd67a4f2835319c54a7d62"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nkpz\/llama2-22b-daydreamer-v3",
        "Average":51.39,
        "ARC":56.06,
        "HellaSwag":80.07,
        "MMLU":52.49,
        "TruthfulQA":42.43,
        "Winogrande":73.48,
        "GSM8K":3.79,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":21.62,
        "Hub \u2764\ufe0f":11.0,
        "Available on the Hub":true,
        "Model Sha":"e6c74222958328e50712aa00294dc818c24075b2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hpcai-tech\/Colossal-LLaMA-2-7b-base",
        "Average":51.39,
        "ARC":53.5,
        "HellaSwag":70.5,
        "MMLU":54.4,
        "TruthfulQA":50.19,
        "Winogrande":70.01,
        "GSM8K":9.7,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.76,
        "Hub \u2764\ufe0f":54.0,
        "Available on the Hub":true,
        "Model Sha":"1f30e4f2037e1e30122667639b8ef37138e85057"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Open-Orca\/OpenOrca-Preview1-13B",
        "Average":51.38,
        "ARC":54.95,
        "HellaSwag":78.19,
        "MMLU":50.12,
        "TruthfulQA":49.05,
        "Winogrande":71.03,
        "GSM8K":4.93,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":143.0,
        "Available on the Hub":true,
        "Model Sha":"d120381b03051b60a7c77ec3fb1be6c3c1546466"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zarakiquemparte\/kuchiki-1.1-l2-7b",
        "Average":51.36,
        "ARC":54.18,
        "HellaSwag":78.0,
        "MMLU":48.14,
        "TruthfulQA":49.96,
        "Winogrande":73.16,
        "GSM8K":4.7,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"10fe70fec0df5c4dcbdfd2e9ec74830c41b3cfd2"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"huggingface\/llama-13b",
        "Average":51.36,
        "ARC":56.23,
        "HellaSwag":80.93,
        "MMLU":47.67,
        "TruthfulQA":39.48,
        "Winogrande":76.24,
        "GSM8K":7.58,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"4022c52fcc7473ce7364bb5ac166195903ea1efb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LTC-AI-Labs\/L2-7b-Hermes-WVG-Test",
        "Average":51.35,
        "ARC":54.95,
        "HellaSwag":78.48,
        "MMLU":48.36,
        "TruthfulQA":45.72,
        "Winogrande":74.74,
        "GSM8K":5.84,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"eb5b1d65fdf916ca71f89a46eb91175c1c630a57"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"huggyllama\/llama-13b",
        "Average":51.33,
        "ARC":56.14,
        "HellaSwag":80.92,
        "MMLU":47.61,
        "TruthfulQA":39.48,
        "Winogrande":76.24,
        "GSM8K":7.58,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":119.0,
        "Available on the Hub":true,
        "Model Sha":"bf57045473f207bb1de1ed035ace226f4d9f9bba"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zarakiquemparte\/kuchiki-l2-7b",
        "Average":51.33,
        "ARC":54.35,
        "HellaSwag":78.44,
        "MMLU":47.74,
        "TruthfulQA":49.88,
        "Winogrande":73.09,
        "GSM8K":4.47,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"745c34e70aa92056e8cd79c1d16e8fcfe1797645"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jordiclive\/gpt4all-alpaca-oa-codealpaca-lora-13b",
        "Average":51.33,
        "ARC":56.14,
        "HellaSwag":80.93,
        "MMLU":47.66,
        "TruthfulQA":39.48,
        "Winogrande":76.16,
        "GSM8K":7.58,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":11.0,
        "Available on the Hub":false,
        "Model Sha":"13443d633eaa5b7e1a90ac9cdb4a4d51b1c8d0d1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Tap-M\/Luna-AI-Llama2-Uncensored",
        "Average":51.29,
        "ARC":54.35,
        "HellaSwag":78.6,
        "MMLU":46.7,
        "TruthfulQA":45.5,
        "Winogrande":72.77,
        "GSM8K":9.86,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":101.0,
        "Available on the Hub":true,
        "Model Sha":"6b5e1067e412cc5750aec7415a065671df3618be"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zarakiquemparte\/zarablend-l2-7b",
        "Average":51.29,
        "ARC":54.44,
        "HellaSwag":78.62,
        "MMLU":47.61,
        "TruthfulQA":49.38,
        "Winogrande":73.32,
        "GSM8K":4.4,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"8b14e71ae3f52c409a25e1ac98dd05e0bb91eaff"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hyunseoki\/ko-en-llama2-13b",
        "Average":51.27,
        "ARC":58.19,
        "HellaSwag":81.89,
        "MMLU":52.02,
        "TruthfulQA":39.96,
        "Winogrande":74.82,
        "GSM8K":0.76,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":13.0,
        "Available on the Hub":true,
        "Model Sha":"2768cf6f955b65868ccbb20658e2cc444b2f3be9"
    },
    {
        "T":"\u2b55",
        "Model":"teknium\/OpenHermes-7B",
        "Average":51.26,
        "ARC":56.14,
        "HellaSwag":78.32,
        "MMLU":48.62,
        "TruthfulQA":45.0,
        "Winogrande":74.51,
        "GSM8K":5.0,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"mit",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"74edb1ad58d3d517ef46c4e2a31081084ecbc473"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Norquinal\/llama-2-7b-claude-chat-rp",
        "Average":51.25,
        "ARC":54.95,
        "HellaSwag":80.05,
        "MMLU":47.03,
        "TruthfulQA":43.47,
        "Winogrande":74.74,
        "GSM8K":7.28,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"4309eedebe8ba5709e0cc7cf186cb783f3bc8060"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zarakiquemparte\/zarablend-1.1-l2-7b",
        "Average":51.25,
        "ARC":54.86,
        "HellaSwag":78.58,
        "MMLU":47.89,
        "TruthfulQA":49.0,
        "Winogrande":72.61,
        "GSM8K":4.55,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"e46bfa43829cbea7608192a6d07bcc147387fdb7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LTC-AI-Labs\/L2-7b-Synthia-WVG-Test",
        "Average":51.25,
        "ARC":55.97,
        "HellaSwag":77.89,
        "MMLU":49.48,
        "TruthfulQA":44.11,
        "Winogrande":74.11,
        "GSM8K":5.91,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"23ae02efba01c37abe3cff0fedc7d2d9644fe98e"
    },
    {
        "T":"\u2b55",
        "Model":"jondurbin\/airoboros-l2-7b-2.2.1",
        "Average":51.22,
        "ARC":55.03,
        "HellaSwag":80.06,
        "MMLU":47.64,
        "TruthfulQA":44.65,
        "Winogrande":73.8,
        "GSM8K":6.14,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"eafbba6fec094a17ca7bce6d9605cac97b90a483"
    },
    {
        "T":"\u2b55",
        "Model":"Harshvir\/Llama-2-7B-physics",
        "Average":51.22,
        "ARC":52.9,
        "HellaSwag":77.71,
        "MMLU":48.83,
        "TruthfulQA":48.93,
        "Winogrande":71.9,
        "GSM8K":7.05,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"5e66b59c145586266b2351a63f0cf1b4f62f5454"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"klyang\/MentaLLaMA-chat-7B",
        "Average":51.17,
        "ARC":52.82,
        "HellaSwag":76.1,
        "MMLU":47.51,
        "TruthfulQA":44.02,
        "Winogrande":70.4,
        "GSM8K":16.15,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"eb0b119279aada6404042c69763aaadb5be5000d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/koala-13B-HF",
        "Average":51.16,
        "ARC":52.99,
        "HellaSwag":77.59,
        "MMLU":45.32,
        "TruthfulQA":50.23,
        "Winogrande":74.03,
        "GSM8K":6.82,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":40.0,
        "Available on the Hub":true,
        "Model Sha":"b20f96a0171ce4c0fa27d6048215ebe710521587"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"64bits\/LexPodLM-13B",
        "Average":51.14,
        "ARC":57.76,
        "HellaSwag":81.04,
        "MMLU":48.38,
        "TruthfulQA":43.48,
        "Winogrande":76.16,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"3553d84037addc97678f99a3464be4c866a0c268"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FlagAlpha\/Llama2-Chinese-7b-Chat",
        "Average":51.13,
        "ARC":52.39,
        "HellaSwag":77.52,
        "MMLU":47.72,
        "TruthfulQA":46.87,
        "Winogrande":74.27,
        "GSM8K":8.04,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":104.0,
        "Available on the Hub":true,
        "Model Sha":"4c3bc725f71898c6a1acd4ea98a2f8d74d1b1b6b"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"chargoddard\/llama-2-26b-trenchcoat-stack",
        "Average":51.13,
        "ARC":55.03,
        "HellaSwag":79.9,
        "MMLU":53.73,
        "TruthfulQA":40.48,
        "Winogrande":74.74,
        "GSM8K":2.88,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":25.7,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"075d67c3223f4b379ab7f997c3787cd0630d80f7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"totally-not-an-llm\/EverythingLM-13b-V3-16k",
        "Average":51.11,
        "ARC":58.19,
        "HellaSwag":80.12,
        "MMLU":50.48,
        "TruthfulQA":45.18,
        "Winogrande":70.72,
        "GSM8K":1.97,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"1de9244bfadb947f80872727f76790cbc76e7142"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PygmalionAI\/pygmalion-2-7b",
        "Average":51.11,
        "ARC":54.01,
        "HellaSwag":78.23,
        "MMLU":49.11,
        "TruthfulQA":43.78,
        "Winogrande":75.14,
        "GSM8K":6.37,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":25.0,
        "Available on the Hub":true,
        "Model Sha":"983f8ad5c156f4a0e4d2b7b5f1146981ad2e8a8b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abhishek\/llama2guanacotest",
        "Average":51.08,
        "ARC":51.62,
        "HellaSwag":77.55,
        "MMLU":48.49,
        "TruthfulQA":43.88,
        "Winogrande":73.16,
        "GSM8K":11.75,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"679d17809939a0bf9b79bbb027898cbea64045b2"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"ehartford\/Samantha-1.11-7b",
        "Average":51.07,
        "ARC":55.03,
        "HellaSwag":79.12,
        "MMLU":40.51,
        "TruthfulQA":50.37,
        "Winogrande":74.19,
        "GSM8K":7.2,
        "Type":"RL-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"730cbd8f3077f3d24001aab714def991f1e4e7e8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HWERI\/Llama2-7b-sharegpt4",
        "Average":51.05,
        "ARC":55.72,
        "HellaSwag":80.94,
        "MMLU":47.47,
        "TruthfulQA":48.34,
        "Winogrande":71.19,
        "GSM8K":2.65,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"8ecaba5dd0e9929f5858cfe9f5f8cd8ba285c9e5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"beaugogh\/Llama2-7b-sharegpt4",
        "Average":51.05,
        "ARC":55.72,
        "HellaSwag":80.94,
        "MMLU":47.47,
        "TruthfulQA":48.34,
        "Winogrande":71.19,
        "GSM8K":2.65,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":false,
        "Model Sha":"922d1d963ad1b042c30b774a818d9f6180c28075"
    },
    {
        "T":"\u2b55",
        "Model":"heegyu\/WizardVicuna2-13b-hf",
        "Average":51.05,
        "ARC":55.38,
        "HellaSwag":79.14,
        "MMLU":48.46,
        "TruthfulQA":42.43,
        "Winogrande":73.48,
        "GSM8K":7.43,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"6cfd95e2dcdb6996afa9eb5c63273a1a3524c6c6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mikael110\/llama-2-7b-guanaco-fp16",
        "Average":51.04,
        "ARC":54.86,
        "HellaSwag":79.65,
        "MMLU":46.38,
        "TruthfulQA":43.83,
        "Winogrande":75.22,
        "GSM8K":6.29,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":9.0,
        "Available on the Hub":true,
        "Model Sha":"f769fed10874af73ad12115efd044cb4a64506b0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ziqingyang\/chinese-llama-2-13b",
        "Average":51.04,
        "ARC":55.8,
        "HellaSwag":79.53,
        "MMLU":53.01,
        "TruthfulQA":38.24,
        "Winogrande":75.69,
        "GSM8K":3.94,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.97,
        "Hub \u2764\ufe0f":22.0,
        "Available on the Hub":true,
        "Model Sha":"484c8a18b02f95eb2b6f6302105cf9a329e76ec8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Delcos\/Mistral-Pygmalion-7b",
        "Average":51.02,
        "ARC":54.44,
        "HellaSwag":78.48,
        "MMLU":49.23,
        "TruthfulQA":41.82,
        "Winogrande":75.3,
        "GSM8K":6.82,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"4e5fa9ae7f572b4841b02c3f96d8a3c7a7e59521"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Norquinal\/llama-2-7b-claude-chat",
        "Average":50.98,
        "ARC":54.44,
        "HellaSwag":80.66,
        "MMLU":46.74,
        "TruthfulQA":41.39,
        "Winogrande":74.9,
        "GSM8K":7.73,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"e65d34ed31cdcd2637f6284aa0605f30ef5a9381"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AlekseyKorshuk\/vic15-exp-syn-fight-cp3838",
        "Average":50.97,
        "ARC":51.79,
        "HellaSwag":75.79,
        "MMLU":50.23,
        "TruthfulQA":49.61,
        "Winogrande":71.82,
        "GSM8K":6.6,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"91ce25dbdb67793ad1fcfdfd59f7603c2be65aea"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"meta-llama\/Llama-2-7b-hf",
        "Average":50.97,
        "ARC":53.07,
        "HellaSwag":78.59,
        "MMLU":46.87,
        "TruthfulQA":38.76,
        "Winogrande":74.03,
        "GSM8K":14.48,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":679.0,
        "Available on the Hub":true,
        "Model Sha":"e8f058fa738b6b308540024e9aa12e274e291f75"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"formulae\/Dorflan",
        "Average":50.96,
        "ARC":54.44,
        "HellaSwag":75.78,
        "MMLU":51.36,
        "TruthfulQA":51.17,
        "Winogrande":72.61,
        "GSM8K":0.38,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"5d8e7e5764ace89e6ccd1deece33b0e8a4b4587b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dfurman\/llama-2-7b-instruct-peft",
        "Average":50.94,
        "ARC":51.19,
        "HellaSwag":78.92,
        "MMLU":46.63,
        "TruthfulQA":48.5,
        "Winogrande":74.43,
        "GSM8K":5.99,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"unknown",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":false,
        "Model Sha":"0fc43413117187e0723cdac133068ab527c80fe2"
    },
    {
        "T":"\u2b55",
        "Model":"PeanutJar\/LLaMa-2-PeanutButter_v18_B-7B",
        "Average":50.94,
        "ARC":54.61,
        "HellaSwag":81.0,
        "MMLU":47.07,
        "TruthfulQA":41.93,
        "Winogrande":74.51,
        "GSM8K":6.52,
        "Type":"instruction-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"bc8c239cacf1e3211f05e27be67a74d84c12aea9"
    },
    {
        "T":"\u2b55",
        "Model":"davzoku\/cria-llama2-7b-v1.3",
        "Average":50.93,
        "ARC":52.73,
        "HellaSwag":78.58,
        "MMLU":48.3,
        "TruthfulQA":45.58,
        "Winogrande":71.9,
        "GSM8K":8.49,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"163a5bec7b6f5aaa4667aa6a95746deff50ceab1"
    },
    {
        "T":"\u2b55",
        "Model":"Korabbit\/Llama-2-7b-chat-hf-afr-100step-v2",
        "Average":50.89,
        "ARC":52.65,
        "HellaSwag":78.25,
        "MMLU":48.47,
        "TruthfulQA":45.18,
        "Winogrande":72.3,
        "GSM8K":8.49,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"4ee3182f614473f9ea3b6e429b01872bc90e89f1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Danielbrdz\/Barcenas-7b",
        "Average":50.87,
        "ARC":55.12,
        "HellaSwag":77.4,
        "MMLU":49.27,
        "TruthfulQA":43.64,
        "Winogrande":73.64,
        "GSM8K":6.14,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"770fa73981a599e935c21a95b1817a553c726694"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-l2-13b-2.1",
        "Average":50.84,
        "ARC":55.12,
        "HellaSwag":80.24,
        "MMLU":50.89,
        "TruthfulQA":44.62,
        "Winogrande":71.9,
        "GSM8K":2.27,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"172e30e56e939f73d7d00a165c2d49cbd284481f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LTC-AI-Labs\/Guanaco-Vicuna-7B-L2",
        "Average":50.83,
        "ARC":53.24,
        "HellaSwag":78.89,
        "MMLU":46.77,
        "TruthfulQA":42.75,
        "Winogrande":75.37,
        "GSM8K":7.96,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"ba8e755feab0bbf90675dcb9f8875a42f92112a5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YeungNLP\/firefly-llama2-13b-pretrain",
        "Average":50.77,
        "ARC":53.92,
        "HellaSwag":79.1,
        "MMLU":51.25,
        "TruthfulQA":36.24,
        "Winogrande":75.53,
        "GSM8K":8.57,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.97,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f87d66f9c4541c575a6fad3c19a31b11568e0dfb"
    },
    {
        "T":"\u2b55",
        "Model":"PeanutJar\/LLaMa-2-PeanutButter_v10-7B",
        "Average":50.75,
        "ARC":55.29,
        "HellaSwag":81.69,
        "MMLU":46.97,
        "TruthfulQA":43.78,
        "Winogrande":70.88,
        "GSM8K":5.91,
        "Type":"instruction-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"f98bb987216448aa3aa89e575a7494fae8b68066"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"meta-llama\/Llama-2-7b-chat-hf",
        "Average":50.74,
        "ARC":52.9,
        "HellaSwag":78.55,
        "MMLU":48.32,
        "TruthfulQA":45.57,
        "Winogrande":71.74,
        "GSM8K":7.35,
        "Type":"RL-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":1415.0,
        "Available on the Hub":true,
        "Model Sha":"b7701a9e825e79a5ab18b5801be113c2160cc627"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Charlie911\/vicuna-7b-v1.5-lora-timedial-unit-080082",
        "Average":50.74,
        "ARC":52.82,
        "HellaSwag":76.07,
        "MMLU":50.47,
        "TruthfulQA":43.54,
        "Winogrande":73.72,
        "GSM8K":7.81,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"372c90543ebb2a317fb9b51ff3890cc270e5ce3a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Charlie911\/vicuna-7b-v1.5-lora-timedial-unit-080091",
        "Average":50.71,
        "ARC":52.82,
        "HellaSwag":76.1,
        "MMLU":50.58,
        "TruthfulQA":43.4,
        "Winogrande":73.72,
        "GSM8K":7.66,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"ae7e0fb58f4201bb14fd4e641d0d6dcc22674e0e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"revolutionarybukhari\/Llama-2-7b-chat-finetune-AUTOMATE",
        "Average":50.68,
        "ARC":53.07,
        "HellaSwag":75.59,
        "MMLU":48.8,
        "TruthfulQA":44.73,
        "Winogrande":73.24,
        "GSM8K":8.64,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"55862462a23ab43fb73d4c784f1518ab4645764c"
    },
    {
        "T":"\u2b55",
        "Model":"psyche\/kollama2-7b-v2",
        "Average":50.66,
        "ARC":53.33,
        "HellaSwag":78.5,
        "MMLU":43.61,
        "TruthfulQA":46.37,
        "Winogrande":75.61,
        "GSM8K":6.52,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"d5b6e9d5b882d4f6ba322396e027925ed915f848"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vonjack\/Qwen-LLaMAfied-HFTok-7B-Chat",
        "Average":50.64,
        "ARC":50.51,
        "HellaSwag":83.65,
        "MMLU":51.53,
        "TruthfulQA":44.23,
        "Winogrande":71.43,
        "GSM8K":2.5,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":7.1,
        "Hub \u2764\ufe0f":19.0,
        "Available on the Hub":true,
        "Model Sha":"b8d5c09c83b1ef23668cb9209dbc43c0df2de8ae"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LTC-AI-Labs\/L2-7b-Base-WVG-Uncensored",
        "Average":50.63,
        "ARC":53.24,
        "HellaSwag":79.13,
        "MMLU":46.65,
        "TruthfulQA":42.59,
        "Winogrande":75.14,
        "GSM8K":7.05,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"67ede9be6ceffdf574294351cca937d88d7d448d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"DopeorNope\/LaOT",
        "Average":50.62,
        "ARC":55.63,
        "HellaSwag":78.96,
        "MMLU":50.3,
        "TruthfulQA":44.72,
        "Winogrande":74.11,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"df3a2c77a63a370405c7711b323e7ffa550cdd9e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"zarakiquemparte\/zaraxls-l2-7b",
        "Average":50.61,
        "ARC":54.44,
        "HellaSwag":78.94,
        "MMLU":50.39,
        "TruthfulQA":46.51,
        "Winogrande":73.16,
        "GSM8K":0.23,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"cc1dad50689b3ebcc1c9c67f275da6b4bb63e2ce"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Voicelab\/trurl-2-7b",
        "Average":50.58,
        "ARC":53.41,
        "HellaSwag":75.29,
        "MMLU":50.0,
        "TruthfulQA":45.42,
        "Winogrande":72.22,
        "GSM8K":7.13,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"e26ca5f157c60fc527170cc04db7fc0ea04ad26f"
    },
    {
        "T":"\u2b55",
        "Model":"guardrail\/llama-2-7b-guanaco-instruct-sharded",
        "Average":50.58,
        "ARC":53.75,
        "HellaSwag":78.69,
        "MMLU":46.65,
        "TruthfulQA":43.93,
        "Winogrande":72.61,
        "GSM8K":7.81,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"fc7a3abbc3b9a9b3e163ef3c4844307ac270fca7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"maximuslee07\/llama-2-7b-rockwell-final",
        "Average":50.55,
        "ARC":52.73,
        "HellaSwag":79.1,
        "MMLU":47.88,
        "TruthfulQA":47.21,
        "Winogrande":68.43,
        "GSM8K":7.96,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"de4cfe99e9e3db62733b40f48b2b11faf9abe4bf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Lazycuber\/L2-7b-Guanaco-Uncensored",
        "Average":50.55,
        "ARC":50.6,
        "HellaSwag":76.99,
        "MMLU":48.93,
        "TruthfulQA":43.42,
        "Winogrande":75.37,
        "GSM8K":7.96,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"9d49378c69c00113cf7f6e66d1ddb9d9b003dddc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aspik101\/trurl-2-7b-pl-instruct_unload",
        "Average":50.52,
        "ARC":53.16,
        "HellaSwag":74.64,
        "MMLU":49.89,
        "TruthfulQA":45.74,
        "Winogrande":72.3,
        "GSM8K":7.43,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"768d800e4dbe3fc95334f30ca7cd02113d3e3fd3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"922-CA\/monika-ddlc-7b-v1",
        "Average":50.49,
        "ARC":54.95,
        "HellaSwag":76.78,
        "MMLU":45.61,
        "TruthfulQA":43.94,
        "Winogrande":72.85,
        "GSM8K":8.79,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"4157d696bb0015da3ba26a58c1d24925515e4125"
    },
    {
        "T":"\u2b55",
        "Model":"WizardLM\/WizardCoder-Python-34B-V1.0",
        "Average":50.46,
        "ARC":52.13,
        "HellaSwag":74.78,
        "MMLU":49.15,
        "TruthfulQA":48.85,
        "Winogrande":68.35,
        "GSM8K":9.48,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":616.0,
        "Available on the Hub":true,
        "Model Sha":"5cdc34e4a81d202f1d4a3b5d60e028aab895dfeb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Lazycuber\/L2-7b-Base-Guanaco-Uncensored",
        "Average":50.45,
        "ARC":52.22,
        "HellaSwag":79.08,
        "MMLU":46.63,
        "TruthfulQA":42.97,
        "Winogrande":74.51,
        "GSM8K":7.28,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"dd51a3b26ad378e2953c947a1e4c2f8febe0cb52"
    },
    {
        "T":"?",
        "Model":"chavinlo\/gpt4-x-alpaca",
        "Average":50.41,
        "ARC":52.82,
        "HellaSwag":79.59,
        "MMLU":48.19,
        "TruthfulQA":48.88,
        "Winogrande":70.17,
        "GSM8K":2.81,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":456.0,
        "Available on the Hub":true,
        "Model Sha":"6a571f458cab9a23d14324ec63e0abd1744c8353"
    },
    {
        "T":"\u2b55",
        "Model":"dhmeltzer\/Llama-2-7b-hf-eli5-cleaned-1024_qlora_merged",
        "Average":50.4,
        "ARC":53.67,
        "HellaSwag":78.21,
        "MMLU":45.9,
        "TruthfulQA":46.13,
        "Winogrande":73.8,
        "GSM8K":4.7,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"1295069e9fef63aed87d36fe108d6c934cb34ded"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheTravellingEngineer\/llama2-7b-chat-hf-dpo",
        "Average":50.38,
        "ARC":53.67,
        "HellaSwag":78.79,
        "MMLU":46.78,
        "TruthfulQA":43.97,
        "Winogrande":71.74,
        "GSM8K":7.35,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"ec98429034fc84a4555dd4e3db4d6af534a03832"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NewstaR\/Koss-7B-chat",
        "Average":50.37,
        "ARC":53.67,
        "HellaSwag":78.79,
        "MMLU":46.72,
        "TruthfulQA":43.97,
        "Winogrande":71.74,
        "GSM8K":7.35,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"b1ab836d9ebf7029fafa07949b51d3838501d537"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lmsys\/vicuna-7b-delta-v1.1",
        "Average":50.37,
        "ARC":53.67,
        "HellaSwag":77.5,
        "MMLU":45.61,
        "TruthfulQA":48.95,
        "Winogrande":70.96,
        "GSM8K":5.53,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":197.0,
        "Available on the Hub":true,
        "Model Sha":"24fb8e1e9cc78e0aa7ef154b026c4a83296e3fc4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"eachadea\/vicuna-7b-1.1",
        "Average":50.37,
        "ARC":53.67,
        "HellaSwag":77.46,
        "MMLU":45.63,
        "TruthfulQA":48.94,
        "Winogrande":70.96,
        "GSM8K":5.53,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":109.0,
        "Available on the Hub":true,
        "Model Sha":"9d8eea215e00b388a22e8f050768ea8911d41f1d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Ejafa\/vicuna_7B_vanilla_1.1",
        "Average":50.37,
        "ARC":53.67,
        "HellaSwag":77.46,
        "MMLU":45.63,
        "TruthfulQA":48.94,
        "Winogrande":70.96,
        "GSM8K":5.53,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"d971d788db19648ad16bf77ec3f1de35ebf9a8e0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"joehuangx\/spatial-vicuna-7b-v1.5-LoRA",
        "Average":50.36,
        "ARC":50.77,
        "HellaSwag":74.63,
        "MMLU":48.13,
        "TruthfulQA":49.36,
        "Winogrande":72.38,
        "GSM8K":6.9,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"dc71924cfb214b91461d35178e6ea6fef7946f13"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Charlie911\/vicuna-7b-v1.5-lora-timedial",
        "Average":50.35,
        "ARC":52.9,
        "HellaSwag":76.29,
        "MMLU":50.47,
        "TruthfulQA":41.6,
        "Winogrande":73.56,
        "GSM8K":7.28,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"1e1709818cca48af4cd31c07c493f996854aa10f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"willnguyen\/lacda-2-7B-chat-v0.1",
        "Average":50.29,
        "ARC":53.07,
        "HellaSwag":77.57,
        "MMLU":46.03,
        "TruthfulQA":44.57,
        "Winogrande":74.19,
        "GSM8K":6.29,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"afca346816726b83e331bb4d93246ed5146e1675"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"beomi\/Yi-Ko-6B",
        "Average":50.27,
        "ARC":48.89,
        "HellaSwag":74.48,
        "MMLU":55.72,
        "TruthfulQA":37.09,
        "Winogrande":72.93,
        "GSM8K":12.51,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"other",
        "#Params (B)":6.18,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"8f2f500574cd3c2972f05b7ae6e2807819cce051"
    },
    {
        "T":"\u2b55",
        "Model":"starmpcc\/Asclepius-Llama2-13B",
        "Average":50.25,
        "ARC":55.89,
        "HellaSwag":79.66,
        "MMLU":52.38,
        "TruthfulQA":40.76,
        "Winogrande":72.69,
        "GSM8K":0.15,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"579271bebb894d89369205060d151120a217ce81"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/tulu-7B-fp16",
        "Average":50.24,
        "ARC":50.17,
        "HellaSwag":77.04,
        "MMLU":47.63,
        "TruthfulQA":41.61,
        "Winogrande":73.8,
        "GSM8K":11.22,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"8a026683f79119643f4007da4e9155c7849792cc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aspik101\/Llama-2-7b-hf-instruct-pl-lora_unload",
        "Average":50.23,
        "ARC":53.75,
        "HellaSwag":78.34,
        "MMLU":46.8,
        "TruthfulQA":42.34,
        "Winogrande":73.95,
        "GSM8K":6.22,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"3dfef350be9c8ce92c2d314dbe96a002bd6ca97d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"GeneZC\/MiniChat-1.5-3B",
        "Average":50.23,
        "ARC":46.5,
        "HellaSwag":68.28,
        "MMLU":46.67,
        "TruthfulQA":50.71,
        "Winogrande":65.04,
        "GSM8K":24.18,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"886af9601d57d8675c09bab02144b68366cd4437"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"conceptofmind\/LLongMA-2-13b-16k",
        "Average":50.22,
        "ARC":54.27,
        "HellaSwag":79.63,
        "MMLU":50.97,
        "TruthfulQA":37.71,
        "Winogrande":72.77,
        "GSM8K":5.99,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"c2defe28e2f3f10460baf8f778b00986a53aa7a2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kashif\/stack-llama-2",
        "Average":50.21,
        "ARC":53.07,
        "HellaSwag":78.57,
        "MMLU":46.8,
        "TruthfulQA":38.75,
        "Winogrande":74.03,
        "GSM8K":10.01,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"bigscience-openrail-m",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"28a206689c0097738177840a40e455a308db2d7d"
    },
    {
        "T":"\u2b55",
        "Model":"Korabbit\/Llama-2-7b-chat-hf-afr-200step-v2",
        "Average":50.21,
        "ARC":51.79,
        "HellaSwag":77.41,
        "MMLU":48.55,
        "TruthfulQA":43.69,
        "Winogrande":71.9,
        "GSM8K":7.88,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"a3575a542e1dc3db4a7794b8f36b104c93b39875"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"elliotthwang\/elliott_Llama-2-7b-hf",
        "Average":50.2,
        "ARC":53.16,
        "HellaSwag":78.33,
        "MMLU":47.09,
        "TruthfulQA":42.11,
        "Winogrande":73.64,
        "GSM8K":6.9,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"ac5d22e14c2c7a400519da5d12d88e4fe683ccfa"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"edor\/Platypus2-mini-7B",
        "Average":50.18,
        "ARC":53.33,
        "HellaSwag":78.81,
        "MMLU":45.58,
        "TruthfulQA":42.0,
        "Winogrande":75.14,
        "GSM8K":6.22,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"4ede4a6f8a8d6cc3bfff8b98837116c74c280f63"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"quantumaikr\/llama-2-7b-hf-guanaco-1k",
        "Average":50.13,
        "ARC":51.62,
        "HellaSwag":76.73,
        "MMLU":47.45,
        "TruthfulQA":44.79,
        "Winogrande":72.77,
        "GSM8K":7.43,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"bdb57c5c992872ced47f48cb2177a5fa159f926a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheTravellingEngineer\/llama2-7b-hf-guanaco",
        "Average":50.12,
        "ARC":52.47,
        "HellaSwag":78.75,
        "MMLU":45.33,
        "TruthfulQA":43.9,
        "Winogrande":74.19,
        "GSM8K":6.07,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"6c1fc95e67b11f1011a3b2fc1aa05c7b83251e40"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TinyPixel\/elm-test",
        "Average":50.09,
        "ARC":53.16,
        "HellaSwag":78.98,
        "MMLU":47.04,
        "TruthfulQA":39.51,
        "Winogrande":74.35,
        "GSM8K":7.51,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"aa8f81624d897aa493474bcd96dc3feae9f7a535"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"conceptofmind\/LLongMA-2-13b-16k",
        "Average":50.09,
        "ARC":54.27,
        "HellaSwag":79.66,
        "MMLU":50.86,
        "TruthfulQA":37.68,
        "Winogrande":72.61,
        "GSM8K":5.46,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"c2defe28e2f3f10460baf8f778b00986a53aa7a2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"togethercomputer\/Llama-2-7B-32K-Instruct",
        "Average":50.02,
        "ARC":51.11,
        "HellaSwag":78.51,
        "MMLU":46.11,
        "TruthfulQA":44.86,
        "Winogrande":73.88,
        "GSM8K":5.69,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":111.0,
        "Available on the Hub":true,
        "Model Sha":"35696b9a7ab330dcbe240ff76fb44ab1eccf45bf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheTravellingEngineer\/llama2-7b-chat-hf-guanaco",
        "Average":50.02,
        "ARC":50.51,
        "HellaSwag":76.72,
        "MMLU":48.03,
        "TruthfulQA":43.36,
        "Winogrande":72.93,
        "GSM8K":8.57,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"5d33696ee324899d52fc43794b46009fea08a9af"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dhmeltzer\/llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged",
        "Average":50.0,
        "ARC":53.75,
        "HellaSwag":78.76,
        "MMLU":46.02,
        "TruthfulQA":43.31,
        "Winogrande":73.48,
        "GSM8K":4.7,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"6ba5416f618ed3e11b409326e84c36fa542f0951"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"dhmeltzer\/llama-7b-SFT-qlora-eli5-wiki_DPO_ds_RM_top_2_1024_r_64_alpha_16",
        "Average":49.98,
        "ARC":54.1,
        "HellaSwag":78.74,
        "MMLU":45.44,
        "TruthfulQA":43.4,
        "Winogrande":73.64,
        "GSM8K":4.55,
        "Type":"RL-tuned",
        "Architecture":"?",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"f1f3b9fdb1e2d8d8fa913d57a8fe15d7bdf72c20"
    },
    {
        "T":"\u2b55",
        "Model":"garage-bAInd\/Platypus2-7B",
        "Average":49.97,
        "ARC":55.2,
        "HellaSwag":78.84,
        "MMLU":49.83,
        "TruthfulQA":40.64,
        "Winogrande":73.48,
        "GSM8K":1.82,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"f784afa7887b0738d92ea470797582756f02e630"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"RoversX\/llama-2-7b-hf-small-shards-Samantha-V1-SFT",
        "Average":49.96,
        "ARC":53.16,
        "HellaSwag":77.71,
        "MMLU":43.47,
        "TruthfulQA":45.28,
        "Winogrande":73.8,
        "GSM8K":6.37,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"c39cee3821269e7fdffa690c2d0836c74dfebd25"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CobraMamba\/mamba-gpt-7b",
        "Average":49.96,
        "ARC":51.19,
        "HellaSwag":75.4,
        "MMLU":47.47,
        "TruthfulQA":42.06,
        "Winogrande":71.67,
        "GSM8K":11.98,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"cb0b04b1bff7921614efbd87d5b87bac04c58d13"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TinyPixel\/lima-test",
        "Average":49.96,
        "ARC":53.07,
        "HellaSwag":78.88,
        "MMLU":46.42,
        "TruthfulQA":39.4,
        "Winogrande":74.03,
        "GSM8K":7.96,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"4d6a006c6341f29b11c02f19bf9535f51b4da1b5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ajibawa-2023\/Uncensored-Jordan-7B",
        "Average":49.95,
        "ARC":51.28,
        "HellaSwag":77.37,
        "MMLU":45.69,
        "TruthfulQA":47.5,
        "Winogrande":71.11,
        "GSM8K":6.75,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"96a9fbe5aaef8410a8d0dad25f3cc97b408c4efb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mrm8488\/llama-2-coder-7b",
        "Average":49.95,
        "ARC":54.01,
        "HellaSwag":78.35,
        "MMLU":46.25,
        "TruthfulQA":38.49,
        "Winogrande":75.45,
        "GSM8K":7.13,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":36.0,
        "Available on the Hub":true,
        "Model Sha":"f21c0d5e3f9f8c5addf093358e6885afa9602296"
    },
    {
        "T":"\u2b55",
        "Model":"PeanutJar\/LLaMa-2-PeanutButter_v18_A-7B",
        "Average":49.88,
        "ARC":53.16,
        "HellaSwag":78.11,
        "MMLU":45.54,
        "TruthfulQA":40.37,
        "Winogrande":74.9,
        "GSM8K":7.2,
        "Type":"instruction-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"15b2fa81418792841014f589e61d1d9e30457040"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wenge-research\/yayi-7b-llama2",
        "Average":49.88,
        "ARC":54.78,
        "HellaSwag":77.94,
        "MMLU":41.35,
        "TruthfulQA":44.02,
        "Winogrande":74.51,
        "GSM8K":6.67,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"18a4ed38285c732efc583a4bd883b3a681f8d005"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TinyPixel\/testmodel2",
        "Average":49.88,
        "ARC":53.24,
        "HellaSwag":78.78,
        "MMLU":46.61,
        "TruthfulQA":39.17,
        "Winogrande":73.8,
        "GSM8K":7.66,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"cb1111653997cee2818ffcf13a1c37237ea2934d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Lajonbot\/Llama-2-7b-chat-hf-instruct-pl-lora_unload",
        "Average":49.86,
        "ARC":52.99,
        "HellaSwag":77.49,
        "MMLU":47.12,
        "TruthfulQA":42.61,
        "Winogrande":72.06,
        "GSM8K":6.9,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f838fda8d2b97effae1e8af4dbb6217eab14fb7e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psyche\/kollama2-7b",
        "Average":49.81,
        "ARC":53.24,
        "HellaSwag":78.78,
        "MMLU":42.31,
        "TruthfulQA":44.56,
        "Winogrande":73.95,
        "GSM8K":5.99,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"48fca4ba1e2d31ff4fbe6856b9b93ad2d97da8b7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TinyPixel\/testmodel-3",
        "Average":49.79,
        "ARC":53.24,
        "HellaSwag":78.72,
        "MMLU":46.57,
        "TruthfulQA":38.75,
        "Winogrande":73.88,
        "GSM8K":7.58,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"a1fbc4d8a2c1a3d211325bdff9e7f0539fa7a2b1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"WizardLM\/WizardMath-7B-V1.0",
        "Average":49.78,
        "ARC":54.1,
        "HellaSwag":79.55,
        "MMLU":45.97,
        "TruthfulQA":43.65,
        "Winogrande":72.69,
        "GSM8K":2.73,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":32.0,
        "Available on the Hub":true,
        "Model Sha":"06dbd3e0da08255c575e585cb82e0554c1d2707a"
    },
    {
        "T":"\u2b55",
        "Model":"elyza\/ELYZA-japanese-Llama-2-7b-instruct",
        "Average":49.78,
        "ARC":53.16,
        "HellaSwag":78.25,
        "MMLU":47.07,
        "TruthfulQA":39.08,
        "Winogrande":73.24,
        "GSM8K":7.88,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":32.0,
        "Available on the Hub":true,
        "Model Sha":"48fa08b3098a23d3671e09565499a4cfbaff1923"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lmsys\/vicuna-7b-v1.3",
        "Average":49.78,
        "ARC":50.43,
        "HellaSwag":76.92,
        "MMLU":48.14,
        "TruthfulQA":47.01,
        "Winogrande":70.48,
        "GSM8K":5.69,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":112.0,
        "Available on the Hub":true,
        "Model Sha":"ac066c83424c4a7221aa10c0ebe074b24d3bcdb6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheTravellingEngineer\/llama2-7b-chat-hf-v4",
        "Average":49.78,
        "ARC":53.41,
        "HellaSwag":78.56,
        "MMLU":46.43,
        "TruthfulQA":38.71,
        "Winogrande":74.03,
        "GSM8K":7.51,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"405c54ec7aea0735996ef5ff6ede6c35ab930381"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"undi95\/llama2-to-mistral-diff",
        "Average":49.78,
        "ARC":53.41,
        "HellaSwag":78.56,
        "MMLU":46.43,
        "TruthfulQA":38.71,
        "Winogrande":74.03,
        "GSM8K":7.51,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":false,
        "Model Sha":"16c279c5e7d12b8a6ff7771881808ef253a406b9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Charlie911\/vicuna-7b-v1.5-lora-mixed-datasets-time-unit",
        "Average":49.77,
        "ARC":51.79,
        "HellaSwag":76.41,
        "MMLU":49.58,
        "TruthfulQA":40.33,
        "Winogrande":73.4,
        "GSM8K":7.13,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"26626ea669172be6bc8e6b2b0bc5f14aef8061aa"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wenge-research\/yayi-7b-llama2",
        "Average":49.75,
        "ARC":55.03,
        "HellaSwag":77.84,
        "MMLU":40.92,
        "TruthfulQA":44.02,
        "Winogrande":73.72,
        "GSM8K":6.97,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"f1a9e8d91e5b636cde3ea7fcf752a9f0234bd92a"
    },
    {
        "T":"\u2b55",
        "Model":"lgaalves\/llama-2-7b-hf_open-platypus",
        "Average":49.73,
        "ARC":51.45,
        "HellaSwag":78.63,
        "MMLU":43.6,
        "TruthfulQA":43.71,
        "Winogrande":74.43,
        "GSM8K":6.6,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"c7e776f3f3afc0fa22cb7aff0d00522e571e9b29"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yeen214\/test_llama2_7b",
        "Average":49.73,
        "ARC":53.07,
        "HellaSwag":78.57,
        "MMLU":46.86,
        "TruthfulQA":38.75,
        "Winogrande":74.03,
        "GSM8K":7.13,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"69a4886f51ed752216cdd7f41a584d14240126f9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bongchoi\/test-llama2-7b",
        "Average":49.73,
        "ARC":53.07,
        "HellaSwag":78.57,
        "MMLU":46.86,
        "TruthfulQA":38.75,
        "Winogrande":74.03,
        "GSM8K":7.13,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"ebe2e68699cb7ab6bb22688f265c89be2ac0fa6d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NewstaR\/Starlight-7B",
        "Average":49.73,
        "ARC":53.07,
        "HellaSwag":78.57,
        "MMLU":46.8,
        "TruthfulQA":38.75,
        "Winogrande":74.03,
        "GSM8K":7.13,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"1f7436c458ebc3d8d31b91091c1a7a48e942cd3b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheTravellingEngineer\/llama2-7b-chat-hf-v2",
        "Average":49.73,
        "ARC":53.07,
        "HellaSwag":78.57,
        "MMLU":46.8,
        "TruthfulQA":38.75,
        "Winogrande":74.03,
        "GSM8K":7.13,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"1c97650d4b919e2c6a2829778caa3a109935a58c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheTravellingEngineer\/llama2-7b-chat-hf-v4",
        "Average":49.73,
        "ARC":53.07,
        "HellaSwag":78.57,
        "MMLU":46.8,
        "TruthfulQA":38.75,
        "Winogrande":74.03,
        "GSM8K":7.13,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"405c54ec7aea0735996ef5ff6ede6c35ab930381"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TaylorAI\/Flash-Llama-7B",
        "Average":49.73,
        "ARC":53.07,
        "HellaSwag":78.57,
        "MMLU":46.8,
        "TruthfulQA":38.75,
        "Winogrande":74.03,
        "GSM8K":7.13,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"27c84ef23d850582453e1cc2dcea13de48da090f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ibranze\/araproje-llama2-7b-hf",
        "Average":49.73,
        "ARC":53.07,
        "HellaSwag":78.57,
        "MMLU":46.8,
        "TruthfulQA":38.75,
        "Winogrande":74.03,
        "GSM8K":7.13,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"7fe54f507e762b0f62265813aef908765b1298c0"
    },
    {
        "T":"\u2b55",
        "Model":"davzoku\/cria-llama2-7b-v1.3_peft",
        "Average":49.72,
        "ARC":51.45,
        "HellaSwag":77.35,
        "MMLU":46.47,
        "TruthfulQA":45.52,
        "Winogrande":70.8,
        "GSM8K":6.75,
        "Type":"instruction-tuned",
        "Architecture":"?",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"6864fa8ee43fa4d6b4f3ae055bbf464a5dcca570"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ToolBench\/ToolLLaMA-7b-LoRA",
        "Average":49.72,
        "ARC":52.99,
        "HellaSwag":78.62,
        "MMLU":46.87,
        "TruthfulQA":38.67,
        "Winogrande":74.35,
        "GSM8K":6.82,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":false,
        "Model Sha":"67f2e8af850049a86fb9ee8ef581deb0f51e58e6"
    },
    {
        "T":"\u2b55",
        "Model":"dhmeltzer\/Llama-2-7b-hf-eli5-cleaned-wiki65k-1024_qlora_merged",
        "Average":49.71,
        "ARC":53.67,
        "HellaSwag":78.09,
        "MMLU":45.63,
        "TruthfulQA":41.72,
        "Winogrande":73.56,
        "GSM8K":5.61,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"2af3d3acb0466fef466512bc17b9bf57024629e8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Charlie911\/vicuna-7b-v1.5-lora-mixed-datasets",
        "Average":49.7,
        "ARC":51.71,
        "HellaSwag":76.44,
        "MMLU":50.13,
        "TruthfulQA":39.57,
        "Winogrande":73.24,
        "GSM8K":7.13,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"9c74b9396ff6b33e7a7622e59aa1f46103d993fe"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"georgesung\/llama2_7b_chat_uncensored",
        "Average":49.67,
        "ARC":53.58,
        "HellaSwag":78.66,
        "MMLU":44.49,
        "TruthfulQA":41.34,
        "Winogrande":74.11,
        "GSM8K":5.84,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":152.0,
        "Available on the Hub":true,
        "Model Sha":"e9a972b12c6b59bfbcf30fe3779c2c933ce755bd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"shibing624\/chinese-alpaca-plus-13b-hf",
        "Average":49.66,
        "ARC":53.16,
        "HellaSwag":73.51,
        "MMLU":48.81,
        "TruthfulQA":45.32,
        "Winogrande":75.06,
        "GSM8K":2.12,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.94,
        "Hub \u2764\ufe0f":32.0,
        "Available on the Hub":true,
        "Model Sha":"a118d2c35573b9a70c6f5b56fba4b657f74ce00c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HuggingFaceH4\/starchat-beta",
        "Average":49.66,
        "ARC":52.47,
        "HellaSwag":80.59,
        "MMLU":42.85,
        "TruthfulQA":47.22,
        "Winogrande":69.69,
        "GSM8K":5.16,
        "Type":"fine-tuned",
        "Architecture":"GPTBigCodeForCausalLM",
        "Precision":"float16",
        "Hub License":"bigcode-openrail-m",
        "#Params (B)":15.52,
        "Hub \u2764\ufe0f":222.0,
        "Available on the Hub":true,
        "Model Sha":"b1bcda690655777373f57ea6614eb095ec2c886f"
    },
    {
        "T":"\u2b55",
        "Model":"togethercomputer\/Llama-2-7B-32K-Instruct",
        "Average":49.65,
        "ARC":51.37,
        "HellaSwag":78.47,
        "MMLU":45.53,
        "TruthfulQA":45.01,
        "Winogrande":72.85,
        "GSM8K":4.7,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"8bit",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":111.0,
        "Available on the Hub":true,
        "Model Sha":"b050a6f17d46e32c4b90a30492f14746589f74b7"
    },
    {
        "T":"\u2b55",
        "Model":"TaylorAI\/FLAN-Llama-7B-2_Llama2-7B-Flash_868_full_model",
        "Average":49.64,
        "ARC":52.47,
        "HellaSwag":79.08,
        "MMLU":47.58,
        "TruthfulQA":37.14,
        "Winogrande":74.74,
        "GSM8K":6.82,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"819f3f384e37f8906a62a8048556c9e58e495c02"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-l2-7b-2.1",
        "Average":49.64,
        "ARC":54.44,
        "HellaSwag":78.68,
        "MMLU":44.45,
        "TruthfulQA":43.95,
        "Winogrande":74.11,
        "GSM8K":2.2,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"699491e2e73cc2936205db143f59c1a686b88f14"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lmsys\/longchat-13b-16k",
        "Average":49.64,
        "ARC":53.58,
        "HellaSwag":77.67,
        "MMLU":45.24,
        "TruthfulQA":47.07,
        "Winogrande":70.09,
        "GSM8K":4.17,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":123.0,
        "Available on the Hub":true,
        "Model Sha":"70e2e38b82f1e25d8b90b50fbfc2361123bef45f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"clibrain\/Llama-2-7b-ft-instruct-es",
        "Average":49.63,
        "ARC":53.67,
        "HellaSwag":77.83,
        "MMLU":46.58,
        "TruthfulQA":38.82,
        "Winogrande":75.22,
        "GSM8K":5.69,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":13.0,
        "Available on the Hub":true,
        "Model Sha":"b62f431c88b232204ea7046f9d906ae1daa68437"
    },
    {
        "T":"\u2b55",
        "Model":"jondurbin\/airocoder-34b-2.1",
        "Average":49.61,
        "ARC":54.18,
        "HellaSwag":73.84,
        "MMLU":50.67,
        "TruthfulQA":40.7,
        "Winogrande":69.93,
        "GSM8K":8.34,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"f66e783ac783837b3f59f274ecf55f18a9221cd0"
    },
    {
        "T":"\u2b55",
        "Model":"gywy\/llama2-13b-chinese-v2",
        "Average":49.58,
        "ARC":53.92,
        "HellaSwag":74.64,
        "MMLU":49.74,
        "TruthfulQA":45.43,
        "Winogrande":71.59,
        "GSM8K":2.2,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.94,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"8f6b11ca4344ac230d6b55defa4e04e60a39f9b5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"sia-ai\/llama-2-7b-1-percent-open-orca-1000-steps-v0",
        "Average":49.56,
        "ARC":51.28,
        "HellaSwag":78.75,
        "MMLU":44.68,
        "TruthfulQA":45.83,
        "Winogrande":74.11,
        "GSM8K":2.73,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"openrail",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"a893ebef4b818de1968dd9e932da2f513d16386a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dotvignesh\/perry-7b",
        "Average":49.55,
        "ARC":51.79,
        "HellaSwag":76.43,
        "MMLU":46.18,
        "TruthfulQA":40.08,
        "Winogrande":72.53,
        "GSM8K":10.31,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f35ae37b436637cd3e14d086324ccdaccfd69045"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CalderaAI\/13B-Ouroboros",
        "Average":49.54,
        "ARC":57.42,
        "HellaSwag":82.11,
        "MMLU":51.43,
        "TruthfulQA":47.99,
        "Winogrande":57.85,
        "GSM8K":0.45,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"97981254d4b0ac0d1472376f602c004670070fdd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-l2-7b-gpt4-1.4.1",
        "Average":49.54,
        "ARC":55.12,
        "HellaSwag":79.6,
        "MMLU":45.17,
        "TruthfulQA":40.29,
        "Winogrande":74.27,
        "GSM8K":2.81,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"77bdd1f049f27876c38b68782fc240518208f391"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jb723\/llama2-ko-7B-model",
        "Average":49.52,
        "ARC":56.31,
        "HellaSwag":79.51,
        "MMLU":45.71,
        "TruthfulQA":40.98,
        "Winogrande":72.06,
        "GSM8K":2.58,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":6.67,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"03d23910fa0f9b0542ce7634cbcd36983321f55a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"llama-anon\/instruct-13b",
        "Average":49.52,
        "ARC":56.14,
        "HellaSwag":80.27,
        "MMLU":47.89,
        "TruthfulQA":36.97,
        "Winogrande":73.56,
        "GSM8K":2.27,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"agpl-3.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"142e198df473fd0cd4370b0d50be5f57e1da399b"
    },
    {
        "T":"?",
        "Model":"quantumaikr\/QuantumLM-7B",
        "Average":49.51,
        "ARC":50.26,
        "HellaSwag":76.1,
        "MMLU":45.27,
        "TruthfulQA":46.25,
        "Winogrande":71.51,
        "GSM8K":7.66,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"f44998432fb90d88094ddf42e57ec458877a197f"
    },
    {
        "T":"?",
        "Model":"TheBloke\/Manticore-13B-Chat-Pyg-Guanaco-SuperHOT-8K-GPTQ",
        "Average":49.47,
        "ARC":52.82,
        "HellaSwag":79.63,
        "MMLU":39.83,
        "TruthfulQA":52.55,
        "Winogrande":71.82,
        "GSM8K":0.15,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":16.22,
        "Hub \u2764\ufe0f":15.0,
        "Available on the Hub":true,
        "Model Sha":"bd3c66e626c81de4977f197e1534bd3dfa2f569d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Fredithefish\/Guanaco-7B-Uncensored",
        "Average":49.35,
        "ARC":52.13,
        "HellaSwag":78.77,
        "MMLU":43.42,
        "TruthfulQA":44.45,
        "Winogrande":73.09,
        "GSM8K":4.25,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"db068e363e66e5d4b131e1d7a42a3a849e406a9b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rameshm\/llama-2-13b-mathgpt-v4",
        "Average":49.35,
        "ARC":50.94,
        "HellaSwag":75.56,
        "MMLU":43.78,
        "TruthfulQA":41.96,
        "Winogrande":69.14,
        "GSM8K":14.71,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"c5072a762070c6b3756385c63805348c155004b5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-openllama-13b-v7-fp16",
        "Average":49.31,
        "ARC":47.61,
        "HellaSwag":72.24,
        "MMLU":47.74,
        "TruthfulQA":48.73,
        "Winogrande":69.69,
        "GSM8K":9.86,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.89,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"8690c065bccd3e897ccbf3d8aa24b0216a6f5dba"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"qualis2006\/llama-2-7b-int4-python-code-18k",
        "Average":49.3,
        "ARC":52.13,
        "HellaSwag":78.55,
        "MMLU":46.25,
        "TruthfulQA":37.69,
        "Winogrande":74.98,
        "GSM8K":6.22,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"aed968a4b3f3b716064eb8b50c5ae24b38007627"
    },
    {
        "T":"\u2b55",
        "Model":"LeoLM\/leo-hessianai-7b-chat",
        "Average":49.29,
        "ARC":52.56,
        "HellaSwag":77.61,
        "MMLU":45.58,
        "TruthfulQA":44.89,
        "Winogrande":69.93,
        "GSM8K":5.16,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"7c343a501f5cd3b768d2f78d9941b760fd66815d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bofenghuang\/vigogne-7b-chat",
        "Average":49.27,
        "ARC":52.47,
        "HellaSwag":78.35,
        "MMLU":39.51,
        "TruthfulQA":44.52,
        "Winogrande":73.16,
        "GSM8K":7.58,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"openrail",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"9af636df9c8693ea857b62442bd1c6c73d657dc6"
    },
    {
        "T":"\u2b55",
        "Model":"heegyu\/LIMA2-7b-hf",
        "Average":49.27,
        "ARC":53.24,
        "HellaSwag":80.6,
        "MMLU":43.22,
        "TruthfulQA":44.74,
        "Winogrande":69.93,
        "GSM8K":3.87,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"6a1aa59cb7624f059728840ce68b20b1070ebdcb"
    },
    {
        "T":"?",
        "Model":"TehVenom\/Pygmalion-Vicuna-1.1-7b",
        "Average":49.25,
        "ARC":52.82,
        "HellaSwag":78.66,
        "MMLU":43.61,
        "TruthfulQA":42.21,
        "Winogrande":71.98,
        "GSM8K":6.22,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":25.0,
        "Available on the Hub":true,
        "Model Sha":"bdac596568769d1ba4af8df9a611eee9723adf29"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dhmeltzer\/llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged",
        "Average":49.22,
        "ARC":53.41,
        "HellaSwag":77.9,
        "MMLU":43.56,
        "TruthfulQA":40.81,
        "Winogrande":74.59,
        "GSM8K":5.08,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"6ca41503b383c654aee8d5496e70fbdfaa33db10"
    },
    {
        "T":"\u2b55",
        "Model":"venkycs\/llama-v2-7b-32kC-Security",
        "Average":49.19,
        "ARC":49.83,
        "HellaSwag":77.33,
        "MMLU":44.41,
        "TruthfulQA":47.96,
        "Winogrande":71.74,
        "GSM8K":3.87,
        "Type":"instruction-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":false,
        "Model Sha":"0ae2abdc539a79ad84b141f894d614adf3754882"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wahaha1987\/llama_7b_sharegpt94k_fastchat",
        "Average":49.19,
        "ARC":53.24,
        "HellaSwag":76.94,
        "MMLU":44.64,
        "TruthfulQA":45.34,
        "Winogrande":70.64,
        "GSM8K":4.32,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"2d82abff150b7a5ae484f9cd7c64c72fd4eaf7f5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PocketDoc\/Dans-RetroRodeo-13b",
        "Average":49.15,
        "ARC":53.84,
        "HellaSwag":79.63,
        "MMLU":48.93,
        "TruthfulQA":38.73,
        "Winogrande":73.8,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"102f9fdad903f5eaffe1ed8173ae56081072e429"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/vicuna-7B-physics",
        "Average":49.15,
        "ARC":49.49,
        "HellaSwag":75.88,
        "MMLU":46.58,
        "TruthfulQA":49.31,
        "Winogrande":69.38,
        "GSM8K":4.25,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"2147983e9493347c3424c07403f65e7a81c0b19f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"elyza\/ELYZA-japanese-Llama-2-7b-fast-instruct",
        "Average":49.15,
        "ARC":53.75,
        "HellaSwag":77.55,
        "MMLU":46.85,
        "TruthfulQA":38.84,
        "Winogrande":71.59,
        "GSM8K":6.29,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.66,
        "Hub \u2764\ufe0f":42.0,
        "Available on the Hub":true,
        "Model Sha":"89de33d1ad568855853196802aeaecd799c6586f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ajibawa-2023\/scarlett-7b",
        "Average":49.09,
        "ARC":57.17,
        "HellaSwag":80.27,
        "MMLU":36.11,
        "TruthfulQA":48.52,
        "Winogrande":72.14,
        "GSM8K":0.3,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"0715b738e750830ba7213f26fe32fa1cc1bb15b3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"itsliupeng\/llama2_7b_code",
        "Average":49.05,
        "ARC":52.13,
        "HellaSwag":75.71,
        "MMLU":48.05,
        "TruthfulQA":38.76,
        "Winogrande":71.51,
        "GSM8K":8.11,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"0e6d1edd87c8753b55d280179c8fb0e65ebf5fa2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hiyouga\/Baichuan2-7B-Base-LLaMAfied",
        "Average":48.99,
        "ARC":49.57,
        "HellaSwag":73.45,
        "MMLU":54.86,
        "TruthfulQA":37.54,
        "Winogrande":70.72,
        "GSM8K":7.81,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.99,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"dc5bda435771212fc73a8c6556fbdf4fcd87f96d"
    },
    {
        "T":"\u2b55",
        "Model":"llm-agents\/tora-code-34b-v1.0",
        "Average":48.95,
        "ARC":50.43,
        "HellaSwag":75.54,
        "MMLU":46.78,
        "TruthfulQA":39.66,
        "Winogrande":68.19,
        "GSM8K":13.12,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"cbb33eea774cc03d4363c424d81e8c9d58332274"
    },
    {
        "T":"?",
        "Model":"meta-llama\/Llama-2-7b-hf",
        "Average":48.93,
        "ARC":53.07,
        "HellaSwag":77.74,
        "MMLU":43.8,
        "TruthfulQA":38.98,
        "Winogrande":74.59,
        "GSM8K":5.38,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"xzuyn\/MedicWizard-7B",
        "Average":48.88,
        "ARC":53.5,
        "HellaSwag":78.39,
        "MMLU":44.61,
        "TruthfulQA":41.32,
        "Winogrande":70.56,
        "GSM8K":4.93,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":13.0,
        "Available on the Hub":true,
        "Model Sha":"0b3ef975fb5e8ac1eae775160ab54c98221889df"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ziqingyang\/chinese-alpaca-2-7b",
        "Average":48.85,
        "ARC":49.57,
        "HellaSwag":72.62,
        "MMLU":46.5,
        "TruthfulQA":48.63,
        "Winogrande":70.01,
        "GSM8K":5.76,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.7,
        "Hub \u2764\ufe0f":93.0,
        "Available on the Hub":true,
        "Model Sha":"ab2476bffedeed752daedd77e71900578e136e7c"
    },
    {
        "T":"?",
        "Model":"openchat\/opencoderplus",
        "Average":48.84,
        "ARC":50.6,
        "HellaSwag":78.22,
        "MMLU":42.73,
        "TruthfulQA":50.72,
        "Winogrande":66.14,
        "GSM8K":4.62,
        "Type":"",
        "Architecture":"GPTBigCodeForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":15.52,
        "Hub \u2764\ufe0f":100.0,
        "Available on the Hub":true,
        "Model Sha":"845e9e4452dd4440760b3d5f680400fc014e91b5"
    },
    {
        "T":"\u2b55",
        "Model":"dhmeltzer\/llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged",
        "Average":48.82,
        "ARC":54.35,
        "HellaSwag":78.06,
        "MMLU":45.35,
        "TruthfulQA":37.11,
        "Winogrande":73.4,
        "GSM8K":4.62,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"684c4f4612fadae47c2c7db9fe9e9be4aaafc7e2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheTravellingEngineer\/llama2-7b-chat-hf-v3",
        "Average":48.81,
        "ARC":52.22,
        "HellaSwag":76.78,
        "MMLU":45.89,
        "TruthfulQA":38.38,
        "Winogrande":73.4,
        "GSM8K":6.22,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"a5269bc93a7f98e192e34553cec1302877ca4327"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aspik101\/vicuna-7b-v1.3-instruct-pl-lora_unload",
        "Average":48.74,
        "ARC":48.04,
        "HellaSwag":76.28,
        "MMLU":47.42,
        "TruthfulQA":44.4,
        "Winogrande":70.09,
        "GSM8K":6.22,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"e4b19d9d6168b32402da4ab2b5ec7ff27cf40d9b"
    },
    {
        "T":"\u2b55",
        "Model":"LeoLM\/leo-hessianai-7b-chat-bilingual",
        "Average":48.72,
        "ARC":51.02,
        "HellaSwag":76.03,
        "MMLU":44.68,
        "TruthfulQA":47.16,
        "Winogrande":70.72,
        "GSM8K":2.73,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"5ee98fd03b310e3081f0c9986c5153b27ec5dce6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"GOAT-AI\/GOAT-7B-Community",
        "Average":48.71,
        "ARC":48.81,
        "HellaSwag":74.63,
        "MMLU":49.58,
        "TruthfulQA":42.48,
        "Winogrande":72.3,
        "GSM8K":4.47,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":34.0,
        "Available on the Hub":true,
        "Model Sha":"a7073a0f5142ce04aaa1603b0812b358f62a8de8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"elyza\/ELYZA-japanese-Llama-2-7b",
        "Average":48.7,
        "ARC":52.22,
        "HellaSwag":76.42,
        "MMLU":44.6,
        "TruthfulQA":37.92,
        "Winogrande":72.69,
        "GSM8K":8.34,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":32.0,
        "Available on the Hub":true,
        "Model Sha":"976887c5891284db204320860bb84b71d598063e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheTravellingEngineer\/llama2-7b-chat-hf-v3",
        "Average":48.65,
        "ARC":51.96,
        "HellaSwag":76.7,
        "MMLU":45.36,
        "TruthfulQA":38.31,
        "Winogrande":73.56,
        "GSM8K":5.99,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"a5269bc93a7f98e192e34553cec1302877ca4327"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PocketDoc\/Dans-CreepingSenseOfDoom",
        "Average":48.58,
        "ARC":53.33,
        "HellaSwag":78.9,
        "MMLU":48.09,
        "TruthfulQA":37.84,
        "Winogrande":73.32,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"efc7cbc5d0461c137e8ea0c83e54bc5357188783"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-7b-gpt4-1.1",
        "Average":48.57,
        "ARC":54.61,
        "HellaSwag":80.15,
        "MMLU":39.25,
        "TruthfulQA":41.22,
        "Winogrande":73.09,
        "GSM8K":3.11,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"5a45a16bac51ed9529a6dc2eab7355cc61eefb5b"
    },
    {
        "T":"\u2b55",
        "Model":"rinna\/youri-7b-chat",
        "Average":48.51,
        "ARC":51.19,
        "HellaSwag":76.09,
        "MMLU":46.06,
        "TruthfulQA":41.17,
        "Winogrande":75.06,
        "GSM8K":1.52,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":12.0,
        "Available on the Hub":true,
        "Model Sha":"96d1690c4a1fa192ab26c4be8f9c79e1faed8346"
    },
    {
        "T":"\u2b55",
        "Model":"llm-agents\/tora-7b-v1.0",
        "Average":48.5,
        "ARC":52.47,
        "HellaSwag":78.68,
        "MMLU":45.9,
        "TruthfulQA":37.9,
        "Winogrande":73.56,
        "GSM8K":2.5,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"717edbee98945192b1a396fc9c337c5b32d6c79c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TehVenom\/Pygmalion-13b-Merged",
        "Average":48.49,
        "ARC":56.48,
        "HellaSwag":80.02,
        "MMLU":42.93,
        "TruthfulQA":35.86,
        "Winogrande":75.53,
        "GSM8K":0.08,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":26.0,
        "Available on the Hub":true,
        "Model Sha":"f96308083033c84db47b6c093da3817c085c87c7"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"TheBloke\/Llama-2-7B-GPTQ",
        "Average":48.48,
        "ARC":52.05,
        "HellaSwag":77.59,
        "MMLU":43.99,
        "TruthfulQA":39.32,
        "Winogrande":72.93,
        "GSM8K":5.0,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"GPTQ",
        "Hub License":"llama2",
        "#Params (B)":9.05,
        "Hub \u2764\ufe0f":58.0,
        "Available on the Hub":true,
        "Model Sha":"ecd7ab9f6adc36ecbe0d751eeea0d90ae1863c3b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/WizardLM-13B-Uncensored",
        "Average":48.48,
        "ARC":50.94,
        "HellaSwag":76.64,
        "MMLU":43.96,
        "TruthfulQA":46.73,
        "Winogrande":70.56,
        "GSM8K":2.05,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":433.0,
        "Available on the Hub":true,
        "Model Sha":"9025c5f96fef9525da9238369ad082961b0e9494"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"medalpaca\/medalpaca-7b",
        "Average":48.45,
        "ARC":54.1,
        "HellaSwag":80.42,
        "MMLU":41.47,
        "TruthfulQA":40.46,
        "Winogrande":71.19,
        "GSM8K":3.03,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":30.0,
        "Available on the Hub":true,
        "Model Sha":"b57b9f5ff34059e485b769973d023021fc66a8f7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/vicuna-7B-chemical",
        "Average":48.42,
        "ARC":49.83,
        "HellaSwag":74.42,
        "MMLU":44.1,
        "TruthfulQA":51.7,
        "Winogrande":67.17,
        "GSM8K":3.34,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"fbf6476ebfa856ffe743e41f8d4413c15b2127c9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-7b-gpt4-1.4",
        "Average":48.4,
        "ARC":53.92,
        "HellaSwag":80.33,
        "MMLU":38.61,
        "TruthfulQA":41.05,
        "Winogrande":72.77,
        "GSM8K":3.71,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"cae1ab8991f66bbe66ae95ed23a87846e7343047"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-l2-7b-gpt4-2.0",
        "Average":48.38,
        "ARC":52.9,
        "HellaSwag":78.53,
        "MMLU":45.09,
        "TruthfulQA":39.45,
        "Winogrande":71.11,
        "GSM8K":3.18,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"8432fe95c426ca7709cf2d31a64eee612c4dea42"
    },
    {
        "T":"?",
        "Model":"AlpinDale\/pygmalion-instruct",
        "Average":48.37,
        "ARC":52.56,
        "HellaSwag":77.65,
        "MMLU":35.94,
        "TruthfulQA":42.13,
        "Winogrande":72.06,
        "GSM8K":9.86,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"1665b271316dfee05b2a8daf8b9d6c22ed0aef60"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LLMs\/AlpacaGPT4-7B-elina",
        "Average":48.35,
        "ARC":55.03,
        "HellaSwag":78.79,
        "MMLU":37.5,
        "TruthfulQA":41.53,
        "Winogrande":72.69,
        "GSM8K":4.55,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"bbece5e3f8ee9be09c8defc536a95c6ef780c681"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Monero\/WizardLM-13b-OpenAssistant-Uncensored",
        "Average":48.32,
        "ARC":48.55,
        "HellaSwag":76.03,
        "MMLU":43.15,
        "TruthfulQA":49.4,
        "Winogrande":69.77,
        "GSM8K":3.03,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"ff8e15fd68119d36ae1f0cebaa87f16e2ad3c732"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Juniplayground\/Mist_LLaMA-2-7B-1024_V3",
        "Average":48.31,
        "ARC":51.37,
        "HellaSwag":77.74,
        "MMLU":41.34,
        "TruthfulQA":41.21,
        "Winogrande":73.32,
        "GSM8K":4.85,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"05ec8f4a568777e1e543acdf8a587e080fb18fba"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Wizard-Vicuna-7B-Uncensored-HF",
        "Average":48.27,
        "ARC":53.41,
        "HellaSwag":78.85,
        "MMLU":37.09,
        "TruthfulQA":43.48,
        "Winogrande":72.22,
        "GSM8K":4.55,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":19.0,
        "Available on the Hub":true,
        "Model Sha":"b802f1b4401d0b2242137160c20cc11b9ffd3a4c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/Wizard-Vicuna-7B-Uncensored",
        "Average":48.27,
        "ARC":53.41,
        "HellaSwag":78.85,
        "MMLU":37.09,
        "TruthfulQA":43.48,
        "Winogrande":72.22,
        "GSM8K":4.55,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":63.0,
        "Available on the Hub":true,
        "Model Sha":"1097285acd9c48a1d09bc0a9844d365384732111"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Charlie911\/vicuna-7b-v1.5-lora-mctaco",
        "Average":48.02,
        "ARC":45.65,
        "HellaSwag":75.65,
        "MMLU":49.27,
        "TruthfulQA":43.12,
        "Winogrande":69.93,
        "GSM8K":4.47,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"883b0fa4158de8207d0a94f4b8cb188e6250aa9d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"titan087\/OpenLlama13B-Guanaco",
        "Average":47.99,
        "ARC":51.19,
        "HellaSwag":75.24,
        "MMLU":43.76,
        "TruthfulQA":38.4,
        "Winogrande":71.74,
        "GSM8K":7.58,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"42ed3023ae1afe861f533570be881a03b10fc860"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lmsys\/longchat-7b-v1.5-32k",
        "Average":47.95,
        "ARC":51.71,
        "HellaSwag":74.97,
        "MMLU":43.16,
        "TruthfulQA":44.42,
        "Winogrande":68.67,
        "GSM8K":4.78,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":44.0,
        "Available on the Hub":true,
        "Model Sha":"16deb633ef4d6a18d5750239edc5a85ffeaf3918"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-l2-7b-gpt4-m2.0",
        "Average":47.95,
        "ARC":50.51,
        "HellaSwag":76.87,
        "MMLU":45.35,
        "TruthfulQA":41.34,
        "Winogrande":69.53,
        "GSM8K":4.09,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":9.0,
        "Available on the Hub":true,
        "Model Sha":"67729407add902e3d4d36bb105d7c011fb368ea5"
    },
    {
        "T":"\u2b55",
        "Model":"LLMs\/Stable-Vicuna-13B",
        "Average":47.95,
        "ARC":53.41,
        "HellaSwag":78.57,
        "MMLU":50.37,
        "TruthfulQA":48.36,
        "Winogrande":56.99,
        "GSM8K":0.0,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"gpl-3.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"51f3d9eaa71de287c96195abd0ff954839857b19"
    },
    {
        "T":"?",
        "Model":"TigerResearch\/tigerbot-7b-base",
        "Average":47.93,
        "ARC":47.7,
        "HellaSwag":72.08,
        "MMLU":45.11,
        "TruthfulQA":42.27,
        "Winogrande":69.61,
        "GSM8K":10.84,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.73,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"300831494aa1eb16e59799310a09531f60dcc904"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vibhorag101\/llama-2-7b-chat-hf-phr_mental_health-2048",
        "Average":47.92,
        "ARC":52.39,
        "HellaSwag":75.39,
        "MMLU":39.77,
        "TruthfulQA":42.89,
        "Winogrande":71.19,
        "GSM8K":5.91,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"81d424a431ab7fa4ff725925b6d0e4269d4563e4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"DevaMalla\/llama_7b_qlora_pds-eval",
        "Average":47.9,
        "ARC":53.92,
        "HellaSwag":78.13,
        "MMLU":32.98,
        "TruthfulQA":45.6,
        "Winogrande":72.61,
        "GSM8K":4.17,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"d20419e1d9e9a6a59ced3edf5169e8e7b3e8394c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ajibawa-2023\/Uncensored-Frank-7B",
        "Average":47.9,
        "ARC":54.27,
        "HellaSwag":76.52,
        "MMLU":37.5,
        "TruthfulQA":43.86,
        "Winogrande":70.24,
        "GSM8K":5.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"65bbcb80158a6d2e133bba99a90142caf4e2e242"
    },
    {
        "T":"\u2b55",
        "Model":"synapsoft\/Llama-2-7b-chat-hf-flan2022-1.2M",
        "Average":47.89,
        "ARC":49.57,
        "HellaSwag":76.25,
        "MMLU":45.99,
        "TruthfulQA":42.17,
        "Winogrande":71.82,
        "GSM8K":1.52,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"825506858e4603745a479215b8dea1524bfab6a0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"golaxy\/goims",
        "Average":47.8,
        "ARC":49.49,
        "HellaSwag":72.67,
        "MMLU":43.85,
        "TruthfulQA":44.8,
        "Winogrande":69.69,
        "GSM8K":6.29,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.76,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"9ef1045ca31f670d9cbf820af904b33a097cd787"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mosaicml\/mpt-7b-8k-chat",
        "Average":47.78,
        "ARC":48.04,
        "HellaSwag":77.62,
        "MMLU":41.88,
        "TruthfulQA":43.68,
        "Winogrande":71.03,
        "GSM8K":4.4,
        "Type":"fine-tuned",
        "Architecture":"MPTForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":26.0,
        "Available on the Hub":true,
        "Model Sha":"ef97b878a279cd1765fbed7b8321fb3cff1aa5b5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bofenghuang\/vigogne-7b-instruct",
        "Average":47.76,
        "ARC":51.96,
        "HellaSwag":78.11,
        "MMLU":38.43,
        "TruthfulQA":42.47,
        "Winogrande":72.85,
        "GSM8K":2.73,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"openrail",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":21.0,
        "Available on the Hub":true,
        "Model Sha":"c6e2f515a0b289478118b5b75ff74107002ad962"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"quantumaikr\/KoreanLM-hf",
        "Average":47.73,
        "ARC":51.45,
        "HellaSwag":76.77,
        "MMLU":40.61,
        "TruthfulQA":44.34,
        "Winogrande":69.77,
        "GSM8K":3.41,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"a7261e7ae6ee76c78e1ba1ac8c59bcc3e0868bf9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LeoLM\/leo-hessianai-7b",
        "Average":47.72,
        "ARC":51.96,
        "HellaSwag":75.84,
        "MMLU":42.85,
        "TruthfulQA":37.94,
        "Winogrande":72.14,
        "GSM8K":5.61,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":22.0,
        "Available on the Hub":true,
        "Model Sha":"88c5ac07006ea8f1b5d10aa4f03f0d624dd27e56"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-7b-gpt4",
        "Average":47.7,
        "ARC":53.07,
        "HellaSwag":78.69,
        "MMLU":38.9,
        "TruthfulQA":40.72,
        "Winogrande":73.09,
        "GSM8K":1.74,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"d9bcb0ad365bfacdf95128bc1272b4106aff7be9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/airoboros-7b-gpt4-fp16",
        "Average":47.7,
        "ARC":53.07,
        "HellaSwag":78.67,
        "MMLU":38.88,
        "TruthfulQA":40.73,
        "Winogrande":73.09,
        "GSM8K":1.74,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"14aa50fba9f6418c0d5e2d24087eb802931040ef"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"microsoft\/phi-1_5",
        "Average":47.69,
        "ARC":52.9,
        "HellaSwag":63.79,
        "MMLU":43.89,
        "TruthfulQA":40.89,
        "Winogrande":72.22,
        "GSM8K":12.43,
        "Type":"pretrained",
        "Architecture":"MixFormerSequentialForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"other",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":916.0,
        "Available on the Hub":true,
        "Model Sha":"ea95720a352172db6fcbcd89032bfb1cb8481797"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"elyza\/ELYZA-japanese-Llama-2-7b-fast",
        "Average":47.67,
        "ARC":51.88,
        "HellaSwag":75.46,
        "MMLU":44.34,
        "TruthfulQA":36.45,
        "Winogrande":71.59,
        "GSM8K":6.29,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.66,
        "Hub \u2764\ufe0f":15.0,
        "Available on the Hub":true,
        "Model Sha":"e326078aa122fb1c4973997952d7b8630071776a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jphme\/orca_mini_v2_ger_7b",
        "Average":47.65,
        "ARC":49.83,
        "HellaSwag":75.5,
        "MMLU":39.1,
        "TruthfulQA":45.74,
        "Winogrande":71.59,
        "GSM8K":4.17,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"175965f50907c6a8cd40f1a4b10d28342969c066"
    },
    {
        "T":"\u2b55",
        "Model":"openthaigpt\/openthaigpt-1.0.0-alpha-7b-chat-ckpt-hf",
        "Average":47.65,
        "ARC":50.85,
        "HellaSwag":74.89,
        "MMLU":40.02,
        "TruthfulQA":47.23,
        "Winogrande":69.06,
        "GSM8K":3.87,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"cdffb3488c5cb1a9aa5039a6b3bc72af24827db0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"keyfan\/vicuna-chinese-replication-v1.1",
        "Average":47.65,
        "ARC":42.83,
        "HellaSwag":71.47,
        "MMLU":47.47,
        "TruthfulQA":47.24,
        "Winogrande":67.4,
        "GSM8K":9.48,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.94,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"259ab0967975012a546f2362d6cd03ab10768157"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"golaxy\/gowizardlm",
        "Average":47.64,
        "ARC":49.74,
        "HellaSwag":71.9,
        "MMLU":42.96,
        "TruthfulQA":47.66,
        "Winogrande":69.61,
        "GSM8K":3.94,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.76,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"385f2d164e7fe780e053276d95d36240f2368c21"
    },
    {
        "T":"\u2b55",
        "Model":"project-baize\/baize-healthcare-lora-7B",
        "Average":47.62,
        "ARC":54.1,
        "HellaSwag":77.32,
        "MMLU":37.09,
        "TruthfulQA":39.96,
        "Winogrande":72.85,
        "GSM8K":4.4,
        "Type":"instruction-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":16.0,
        "Available on the Hub":false,
        "Model Sha":"e3eb8bb0d8840431afe24760d964f8ba57edd83e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bigcode\/starcoderplus",
        "Average":47.61,
        "ARC":48.72,
        "HellaSwag":77.3,
        "MMLU":43.72,
        "TruthfulQA":37.85,
        "Winogrande":70.01,
        "GSM8K":8.04,
        "Type":"fine-tuned",
        "Architecture":"GPTBigCodeForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":15.52,
        "Hub \u2764\ufe0f":181.0,
        "Available on the Hub":true,
        "Model Sha":"95be82087c33f14ee9941c812a154a9dd66efe72"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Neko-Institute-of-Science\/metharme-7b",
        "Average":47.48,
        "ARC":53.67,
        "HellaSwag":78.62,
        "MMLU":35.91,
        "TruthfulQA":39.16,
        "Winogrande":72.53,
        "GSM8K":5.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":12.0,
        "Available on the Hub":true,
        "Model Sha":"62ca156891feead8db117be8f5f35687b6274e6e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"DevaMalla\/llama_7b_qlora_cds",
        "Average":47.43,
        "ARC":52.47,
        "HellaSwag":77.76,
        "MMLU":32.38,
        "TruthfulQA":46.14,
        "Winogrande":71.74,
        "GSM8K":4.09,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"b6b5c65c5c1cce34d24c8f790bb0cc011e0f0808"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aiplanet\/effi-7b",
        "Average":47.42,
        "ARC":55.12,
        "HellaSwag":78.07,
        "MMLU":35.91,
        "TruthfulQA":39.71,
        "Winogrande":72.53,
        "GSM8K":3.18,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":false,
        "Model Sha":"d58c62ee27cae60392bd0bd53e1fd05ea82e273b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-7b-gpt4-1.2",
        "Average":47.42,
        "ARC":52.13,
        "HellaSwag":78.14,
        "MMLU":38.64,
        "TruthfulQA":41.79,
        "Winogrande":71.67,
        "GSM8K":2.12,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":28.0,
        "Available on the Hub":true,
        "Model Sha":"431fda60009d9b37a73211123ffb9c797764e182"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/orca_mini_v2_7b",
        "Average":47.41,
        "ARC":50.77,
        "HellaSwag":76.02,
        "MMLU":39.5,
        "TruthfulQA":43.86,
        "Winogrande":71.43,
        "GSM8K":2.88,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":34.0,
        "Available on the Hub":true,
        "Model Sha":"165850882991d7fa4eabab577a03ed84e0713bfa"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-7b",
        "Average":47.4,
        "ARC":53.07,
        "HellaSwag":77.65,
        "MMLU":37.23,
        "TruthfulQA":43.39,
        "Winogrande":70.96,
        "GSM8K":2.12,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":14.0,
        "Available on the Hub":true,
        "Model Sha":"7ea67f85ff3a7a8ec77f1819dec3e56779b764b1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"luffycodes\/vicuna-shishya-7b-ep3-v1",
        "Average":47.4,
        "ARC":45.9,
        "HellaSwag":76.36,
        "MMLU":50.04,
        "TruthfulQA":40.32,
        "Winogrande":71.74,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"082cf758aa3f6d8f956056003b5b3b6cde447d88"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jxhong\/CAlign-alpaca-7b",
        "Average":47.39,
        "ARC":50.94,
        "HellaSwag":74.55,
        "MMLU":38.56,
        "TruthfulQA":46.89,
        "Winogrande":72.06,
        "GSM8K":1.36,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f5cc642a10160a014e2afeefcd57d4781994c51e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mosaicml\/mpt-7b-8k-instruct",
        "Average":47.37,
        "ARC":45.9,
        "HellaSwag":74.47,
        "MMLU":41.97,
        "TruthfulQA":35.21,
        "Winogrande":65.98,
        "GSM8K":20.7,
        "Type":"fine-tuned",
        "Architecture":"MPTForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-sa-3.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":23.0,
        "Available on the Hub":true,
        "Model Sha":"736f68aceeb61298a5de3cf5ae81d0bc2697edf4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/guanaco-7B-HF",
        "Average":47.34,
        "ARC":52.99,
        "HellaSwag":80.05,
        "MMLU":35.32,
        "TruthfulQA":39.2,
        "Winogrande":71.43,
        "GSM8K":5.08,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"293c24105fa15afa127a2ec3905fdc2a0a3a6dac"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"openlm-research\/open_llama_13b",
        "Average":47.26,
        "ARC":51.19,
        "HellaSwag":75.23,
        "MMLU":43.75,
        "TruthfulQA":38.08,
        "Winogrande":72.06,
        "GSM8K":3.26,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":434.0,
        "Available on the Hub":true,
        "Model Sha":"b6d7fde8392250730d24cc2fcfa3b7e5f9a03ce8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FelixChao\/CodeLlama13B-Finetune-v1",
        "Average":47.19,
        "ARC":45.82,
        "HellaSwag":69.36,
        "MMLU":45.05,
        "TruthfulQA":44.97,
        "Winogrande":66.93,
        "GSM8K":10.99,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"40ff78ce37efcaf83718534c494829a573b9d719"
    },
    {
        "T":"\u2b55",
        "Model":"mosaicml\/mpt-7b-8k-instruct",
        "Average":47.18,
        "ARC":45.48,
        "HellaSwag":74.41,
        "MMLU":42.11,
        "TruthfulQA":35.06,
        "Winogrande":65.51,
        "GSM8K":20.55,
        "Type":"instruction-tuned",
        "Architecture":"MPTForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-sa-3.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":23.0,
        "Available on the Hub":true,
        "Model Sha":"736f68aceeb61298a5de3cf5ae81d0bc2697edf4"
    },
    {
        "T":"\u2b55",
        "Model":"starmpcc\/Asclepius-Llama2-7B",
        "Average":47.15,
        "ARC":50.85,
        "HellaSwag":76.53,
        "MMLU":43.61,
        "TruthfulQA":43.31,
        "Winogrande":68.27,
        "GSM8K":0.3,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"2f15bd8250d7825307e59cc2c785074ebbec3395"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"rinna\/youri-7b",
        "Average":47.11,
        "ARC":49.06,
        "HellaSwag":74.89,
        "MMLU":42.22,
        "TruthfulQA":36.03,
        "Winogrande":71.82,
        "GSM8K":8.64,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":15.0,
        "Available on the Hub":true,
        "Model Sha":"2be40b8a7b669c4520bc04ce954bdbd7d4b0da7e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"togethercomputer\/LLaMA-2-7B-32K",
        "Average":47.07,
        "ARC":47.53,
        "HellaSwag":76.14,
        "MMLU":43.33,
        "TruthfulQA":39.23,
        "Winogrande":71.9,
        "GSM8K":4.32,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":465.0,
        "Available on the Hub":true,
        "Model Sha":"aef6d8946ae1015bdb65c478a2dd73b58daaef47"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-gpt-3.5-turbo-100k-7b",
        "Average":47.05,
        "ARC":53.07,
        "HellaSwag":76.16,
        "MMLU":33.63,
        "TruthfulQA":45.07,
        "Winogrande":70.8,
        "GSM8K":3.56,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":11.0,
        "Available on the Hub":true,
        "Model Sha":"53887996c0f17f7711d182537505a895fb404542"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"notstoic\/PygmalionCoT-7b",
        "Average":47.0,
        "ARC":51.45,
        "HellaSwag":76.92,
        "MMLU":33.35,
        "TruthfulQA":48.13,
        "Winogrande":68.9,
        "GSM8K":3.26,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":15.0,
        "Available on the Hub":true,
        "Model Sha":"c03ac527360663d17bb142405251028eec843ed9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mncai\/chatdoctor",
        "Average":46.95,
        "ARC":53.75,
        "HellaSwag":78.54,
        "MMLU":35.95,
        "TruthfulQA":43.55,
        "Winogrande":69.93,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LLaMAForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"8fdcfdda6877d7f21173dfac48b2c14499ba8264"
    },
    {
        "T":"?",
        "Model":"ausboss\/llama7b-wizardlm-unfiltered",
        "Average":46.94,
        "ARC":52.99,
        "HellaSwag":77.89,
        "MMLU":36.41,
        "TruthfulQA":37.75,
        "Winogrande":72.3,
        "GSM8K":4.32,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"2123beec77083c414b2ae51dd25b7a870b0b936c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/dolphin-llama2-7b",
        "Average":46.94,
        "ARC":46.59,
        "HellaSwag":67.52,
        "MMLU":48.37,
        "TruthfulQA":49.72,
        "Winogrande":63.77,
        "GSM8K":5.69,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":64.0,
        "Available on the Hub":true,
        "Model Sha":"85aa4f67191fd016ab7ea8c389fddb5d9e5a9a52"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-7b-gpt4-1.3",
        "Average":46.91,
        "ARC":52.47,
        "HellaSwag":77.98,
        "MMLU":41.97,
        "TruthfulQA":35.73,
        "Winogrande":72.3,
        "GSM8K":0.99,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"7b5f77827636bbf3174c48ca16e774c89d71d7bd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"DevaMalla\/llama_7b_lora",
        "Average":46.77,
        "ARC":54.86,
        "HellaSwag":79.1,
        "MMLU":33.63,
        "TruthfulQA":34.74,
        "Winogrande":72.77,
        "GSM8K":5.53,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"7f4cbd810b4bef0d75c1fd3f551146b4ea97d9fd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"project-baize\/baize-v2-7b",
        "Average":46.72,
        "ARC":48.98,
        "HellaSwag":75.06,
        "MMLU":39.6,
        "TruthfulQA":41.39,
        "Winogrande":71.11,
        "GSM8K":4.17,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":23.0,
        "Available on the Hub":true,
        "Model Sha":"e4731c2c2671e2d0b47b5eba08c753ca21671fab"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-codellama-dolphin-orca-platypus-13b",
        "Average":46.7,
        "ARC":44.8,
        "HellaSwag":68.6,
        "MMLU":44.03,
        "TruthfulQA":46.28,
        "Winogrande":66.93,
        "GSM8K":9.55,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"0c41023f8f665946a2c46c3823afee431408bcbd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-codellama-platypus-13b",
        "Average":46.68,
        "ARC":46.16,
        "HellaSwag":68.88,
        "MMLU":44.55,
        "TruthfulQA":44.98,
        "Winogrande":66.14,
        "GSM8K":9.4,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"7a771bd8899b9ef4ba9680e96f84dc85810a67d6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"golaxy\/gogpt2-7b",
        "Average":46.65,
        "ARC":46.76,
        "HellaSwag":71.53,
        "MMLU":42.85,
        "TruthfulQA":47.85,
        "Winogrande":68.67,
        "GSM8K":2.27,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.76,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"ee60ed402dedf24b6154aef05df54512e02fc9e2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"DevaMalla\/llama_7b_qlora",
        "Average":46.61,
        "ARC":55.12,
        "HellaSwag":78.26,
        "MMLU":35.71,
        "TruthfulQA":33.98,
        "Winogrande":72.06,
        "GSM8K":4.55,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"7f94b0be78193abc54722cf723541c3800426f7b"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"stabilityai\/stablelm-3b-4e1t",
        "Average":46.58,
        "ARC":46.59,
        "HellaSwag":75.94,
        "MMLU":45.23,
        "TruthfulQA":37.2,
        "Winogrande":71.19,
        "GSM8K":3.34,
        "Type":"pretrained",
        "Architecture":"StableLMEpochForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":2.8,
        "Hub \u2764\ufe0f":192.0,
        "Available on the Hub":true,
        "Model Sha":"a4750ace0db6f08d7bbba0aa52a585f231ea3cde"
    },
    {
        "T":"?",
        "Model":"chavinlo\/alpaca-native",
        "Average":46.58,
        "ARC":52.3,
        "HellaSwag":77.09,
        "MMLU":41.6,
        "TruthfulQA":37.58,
        "Winogrande":69.46,
        "GSM8K":1.44,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":249.0,
        "Available on the Hub":true,
        "Model Sha":"cc7773cac2478231807c56ef2f02292d98f85cf5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"golaxy\/gogpt2-13b",
        "Average":46.55,
        "ARC":48.38,
        "HellaSwag":71.78,
        "MMLU":44.5,
        "TruthfulQA":44.73,
        "Winogrande":67.88,
        "GSM8K":2.05,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.04,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"16d4c4214fa8d5a962b9064a8b958076b7c79a17"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"DevaMalla\/llama7b_alpaca_1gpu_bf16",
        "Average":46.49,
        "ARC":52.73,
        "HellaSwag":78.78,
        "MMLU":36.26,
        "TruthfulQA":33.71,
        "Winogrande":72.93,
        "GSM8K":4.55,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"305683c1b95f6888b8668dbc6b56d9efa5d07fef"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TehVenom\/Pygmalion_AlpacaLora-7b",
        "Average":46.49,
        "ARC":53.24,
        "HellaSwag":76.92,
        "MMLU":35.92,
        "TruthfulQA":39.44,
        "Winogrande":72.22,
        "GSM8K":1.21,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"1f61442e1238062095b31b4909c5e9ab26105794"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jerryjalapeno\/nart-100k-7b",
        "Average":46.39,
        "ARC":54.1,
        "HellaSwag":78.47,
        "MMLU":34.98,
        "TruthfulQA":36.74,
        "Winogrande":70.48,
        "GSM8K":3.56,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":13.0,
        "Available on the Hub":true,
        "Model Sha":"50e61b8e6cc17cb3fbcb490fe3dc7e2c8b248378"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"golaxy\/gogpt-7b",
        "Average":46.38,
        "ARC":48.81,
        "HellaSwag":73.79,
        "MMLU":43.03,
        "TruthfulQA":41.0,
        "Winogrande":69.77,
        "GSM8K":1.9,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.76,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"7eb70c0e330b7d3ff490047ddbb153bb96294882"
    },
    {
        "T":"?",
        "Model":"huggyllama\/llama-7b",
        "Average":46.37,
        "ARC":50.94,
        "HellaSwag":77.81,
        "MMLU":35.69,
        "TruthfulQA":34.33,
        "Winogrande":71.43,
        "GSM8K":8.04,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":226.0,
        "Available on the Hub":true,
        "Model Sha":"8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jondurbin\/airoboros-7b-gpt4-1.4.1-qlora",
        "Average":46.34,
        "ARC":52.73,
        "HellaSwag":77.89,
        "MMLU":38.77,
        "TruthfulQA":36.07,
        "Winogrande":70.32,
        "GSM8K":2.27,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"91ffa900ed637cf5fd904d96e6985b6f7857ad64"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wenge-research\/yayi-13b-llama2",
        "Average":46.32,
        "ARC":48.55,
        "HellaSwag":74.82,
        "MMLU":38.68,
        "TruthfulQA":42.19,
        "Winogrande":69.69,
        "GSM8K":4.02,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"9fc1bc4409b9e71f54213245a91c2742fbf7b3d0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"speechlessai\/speechless-codellama-dolphin-orca-platypus-13b",
        "Average":46.32,
        "ARC":45.82,
        "HellaSwag":67.71,
        "MMLU":45.88,
        "TruthfulQA":44.67,
        "Winogrande":65.35,
        "GSM8K":8.49,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"25e1c346c2a01588a728307d5c35fbeecd58b51b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-codellama-orca-13b",
        "Average":46.28,
        "ARC":46.33,
        "HellaSwag":67.71,
        "MMLU":47.19,
        "TruthfulQA":46.66,
        "Winogrande":63.77,
        "GSM8K":5.99,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"a82467de3cb9438aa8f9e0ea8ea692f16a5724b2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yhyhy3\/open_llama_7b_v2_med_instruct",
        "Average":46.24,
        "ARC":46.5,
        "HellaSwag":76.91,
        "MMLU":42.32,
        "TruthfulQA":40.33,
        "Winogrande":69.3,
        "GSM8K":2.05,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"cabb47abd422a2d67161e2d038265ee23be45fb8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YeungNLP\/firefly-llama2-7b-pretrain",
        "Average":46.18,
        "ARC":48.63,
        "HellaSwag":74.83,
        "MMLU":41.04,
        "TruthfulQA":39.08,
        "Winogrande":70.24,
        "GSM8K":3.26,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.7,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"444c85ef809f8793d84b0813ab78bec50700cfcf"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"stabilityai\/stablelm-base-alpha-7b-v2",
        "Average":46.18,
        "ARC":47.35,
        "HellaSwag":77.08,
        "MMLU":45.1,
        "TruthfulQA":36.46,
        "Winogrande":68.51,
        "GSM8K":2.58,
        "Type":"pretrained",
        "Architecture":"StableLMAlphaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":6.89,
        "Hub \u2764\ufe0f":36.0,
        "Available on the Hub":true,
        "Model Sha":"eb3b56fee1ad4b1efe6625bbbc7a277df8ab5b96"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ajibawa-2023\/carl-7b",
        "Average":46.16,
        "ARC":53.5,
        "HellaSwag":78.29,
        "MMLU":33.96,
        "TruthfulQA":40.29,
        "Winogrande":68.59,
        "GSM8K":2.35,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"de4c7af9598bebc47dd43253c972be719f3195d6"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"bigscience\/bloom",
        "Average":46.07,
        "ARC":50.43,
        "HellaSwag":76.41,
        "MMLU":30.85,
        "TruthfulQA":39.76,
        "Winogrande":72.06,
        "GSM8K":6.9,
        "Type":"pretrained",
        "Architecture":"BloomForCausalLM",
        "Precision":"float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":176.25,
        "Hub \u2764\ufe0f":461.0,
        "Available on the Hub":true,
        "Model Sha":"053d9cd9fbe814e091294f67fcfedb3397b954bb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"fireballoon\/baichuan-vicuna-chinese-7b",
        "Average":46.06,
        "ARC":43.52,
        "HellaSwag":71.12,
        "MMLU":46.87,
        "TruthfulQA":42.45,
        "Winogrande":66.85,
        "GSM8K":5.53,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":61.0,
        "Available on the Hub":true,
        "Model Sha":"6cdb9e75cd473e31e87067c2a0b646083247d9ab"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"illuin\/test-custom-llama",
        "Average":46.05,
        "ARC":52.3,
        "HellaSwag":77.49,
        "MMLU":36.61,
        "TruthfulQA":33.81,
        "Winogrande":72.06,
        "GSM8K":4.02,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"d985610bef080473e40f01c53266083c5f0c3169"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Neko-Institute-of-Science\/pygmalion-7b",
        "Average":46.04,
        "ARC":51.37,
        "HellaSwag":77.81,
        "MMLU":35.68,
        "TruthfulQA":34.54,
        "Winogrande":72.22,
        "GSM8K":4.62,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":38.0,
        "Available on the Hub":true,
        "Model Sha":"6473f9996d758fde48a181f37cc5de575aff1606"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Charlie911\/vicuna-7b-v1.5-lora-mctaco-modified2",
        "Average":46.03,
        "ARC":42.92,
        "HellaSwag":73.97,
        "MMLU":48.49,
        "TruthfulQA":40.43,
        "Winogrande":69.69,
        "GSM8K":0.68,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"8e1930bbbbdeb4f6f4639e837f09d9878bbf7831"
    },
    {
        "T":"\u2b55",
        "Model":"codellama\/CodeLlama-13b-Instruct-hf",
        "Average":45.82,
        "ARC":44.54,
        "HellaSwag":64.93,
        "MMLU":38.89,
        "TruthfulQA":45.88,
        "Winogrande":68.03,
        "GSM8K":12.66,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":66.0,
        "Available on the Hub":true,
        "Model Sha":"b9f91b7351ecd589118d883afa23d5c93a38c612"
    },
    {
        "T":"\u2b55",
        "Model":"TheBloke\/CodeLlama-13B-Instruct-fp16",
        "Average":45.82,
        "ARC":44.62,
        "HellaSwag":64.94,
        "MMLU":38.77,
        "TruthfulQA":45.88,
        "Winogrande":68.03,
        "GSM8K":12.66,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":28.0,
        "Available on the Hub":true,
        "Model Sha":"521c208c7251ccd3e44ccd9500b6bed419bca565"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"huggingface\/llama-7b",
        "Average":45.65,
        "ARC":51.02,
        "HellaSwag":77.82,
        "MMLU":35.71,
        "TruthfulQA":34.33,
        "Winogrande":71.43,
        "GSM8K":3.56,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f356572651e58fb337d610470d4b36976e7fb802"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Planner-7B-fp16",
        "Average":45.65,
        "ARC":51.02,
        "HellaSwag":77.82,
        "MMLU":35.71,
        "TruthfulQA":34.33,
        "Winogrande":71.43,
        "GSM8K":3.56,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"afb4604a06c8541960fb51240259777764c4ce7e"
    },
    {
        "T":"\u2b55",
        "Model":"uukuguy\/speechless-codellama-platypus-13b",
        "Average":45.64,
        "ARC":45.31,
        "HellaSwag":68.63,
        "MMLU":42.82,
        "TruthfulQA":42.38,
        "Winogrande":65.59,
        "GSM8K":9.1,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"81cb1bca46ce646b8339501537837e02116de1b8"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"DevaMalla\/llama-base-7b",
        "Average":45.62,
        "ARC":50.94,
        "HellaSwag":77.8,
        "MMLU":35.67,
        "TruthfulQA":34.34,
        "Winogrande":71.43,
        "GSM8K":3.56,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"e01d89d8e444f7d751ea58feaf22ff8c9af69d2a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"WeOpenML\/PandaLM-Alpaca-7B-v1",
        "Average":45.59,
        "ARC":50.85,
        "HellaSwag":77.36,
        "MMLU":35.91,
        "TruthfulQA":36.63,
        "Winogrande":71.9,
        "GSM8K":0.91,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"7fe5cb1a7009fdade8dfcfec335527997a730fcf"
    },
    {
        "T":"\u2b55",
        "Model":"yeontaek\/WizardCoder-Python-13B-LoRa",
        "Average":45.56,
        "ARC":47.78,
        "HellaSwag":69.6,
        "MMLU":38.76,
        "TruthfulQA":43.97,
        "Winogrande":65.43,
        "GSM8K":7.81,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"32ffc44ffdf1adfe2d8ef219327fbd534f3d5955"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Linly-AI\/Chinese-LLaMA-2-7B-hf",
        "Average":45.44,
        "ARC":48.04,
        "HellaSwag":73.25,
        "MMLU":35.04,
        "TruthfulQA":39.92,
        "Winogrande":70.17,
        "GSM8K":6.22,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.64,
        "Hub \u2764\ufe0f":15.0,
        "Available on the Hub":true,
        "Model Sha":"a2d55220b3d0693825fe69e1174653dc6cc4a920"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"shibing624\/chinese-llama-plus-13b-hf",
        "Average":45.39,
        "ARC":46.25,
        "HellaSwag":71.88,
        "MMLU":40.74,
        "TruthfulQA":39.89,
        "Winogrande":73.09,
        "GSM8K":0.53,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.94,
        "Hub \u2764\ufe0f":17.0,
        "Available on the Hub":true,
        "Model Sha":"f17a52b8067d551a814069d2c710e1f5c487a3ce"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mosaicml\/mpt-7b-chat",
        "Average":45.39,
        "ARC":46.5,
        "HellaSwag":75.51,
        "MMLU":37.62,
        "TruthfulQA":40.16,
        "Winogrande":68.43,
        "GSM8K":4.09,
        "Type":"fine-tuned",
        "Architecture":"MPTForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":485.0,
        "Available on the Hub":true,
        "Model Sha":"64e5c9c9fb53a8e89690c2dee75a5add37f7113e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Charlie911\/vicuna-7b-v1.5-lora-mctaco-modified1",
        "Average":45.38,
        "ARC":40.87,
        "HellaSwag":73.4,
        "MMLU":47.42,
        "TruthfulQA":39.87,
        "Winogrande":69.46,
        "GSM8K":1.29,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"a7749ff092ef03900de34b69d41c767a6a48ea9e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"beomi\/llama-2-ko-7b",
        "Average":45.32,
        "ARC":48.46,
        "HellaSwag":75.28,
        "MMLU":39.56,
        "TruthfulQA":34.49,
        "Winogrande":72.14,
        "GSM8K":1.97,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.86,
        "Hub \u2764\ufe0f":85.0,
        "Available on the Hub":true,
        "Model Sha":"d5c58cc2cae21b4fb96aaad2658acc898ab22d99"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"GeneZC\/MiniChat-3B",
        "Average":45.31,
        "ARC":44.03,
        "HellaSwag":67.19,
        "MMLU":39.17,
        "TruthfulQA":45.67,
        "Winogrande":65.27,
        "GSM8K":10.54,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.0,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"123d23bd291bb2d5fdb3b91dc1570d0b11654a78"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ashercn97\/giraffe-7b",
        "Average":45.29,
        "ARC":47.18,
        "HellaSwag":75.53,
        "MMLU":38.89,
        "TruthfulQA":38.48,
        "Winogrande":68.98,
        "GSM8K":2.65,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"9af88449bed5be4709befcfbbba123ee75805479"
    },
    {
        "T":"?",
        "Model":"facebook\/opt-iml-max-30b",
        "Average":45.28,
        "ARC":43.86,
        "HellaSwag":72.39,
        "MMLU":41.09,
        "TruthfulQA":38.16,
        "Winogrande":73.72,
        "GSM8K":2.5,
        "Type":"",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":29.98,
        "Hub \u2764\ufe0f":34.0,
        "Available on the Hub":true,
        "Model Sha":"291753b04817a31a742631053ee361874d6db8a4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-openllama-7b-v12-bf16",
        "Average":45.28,
        "ARC":42.06,
        "HellaSwag":62.01,
        "MMLU":46.53,
        "TruthfulQA":45.18,
        "Winogrande":65.04,
        "GSM8K":10.84,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.63,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"bb94ff691996484b1a9d899a6c0956ef6750d86a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NurtureAI\/Orca-2-13B-16k",
        "Average":45.22,
        "ARC":53.67,
        "HellaSwag":69.48,
        "MMLU":41.02,
        "TruthfulQA":45.3,
        "Winogrande":60.06,
        "GSM8K":1.82,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"0daee08a5e065d02726e9ae0f05cdfd78992cfba"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"luffycodes\/llama-shishya-7b-ep3-v1",
        "Average":45.19,
        "ARC":48.04,
        "HellaSwag":76.63,
        "MMLU":46.12,
        "TruthfulQA":30.9,
        "Winogrande":69.46,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"8dc109f45ef36cc7bbd0f5d83fb65ac8e768d1bd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Enno-Ai\/ennodata-7b",
        "Average":45.13,
        "ARC":51.02,
        "HellaSwag":77.62,
        "MMLU":33.95,
        "TruthfulQA":33.53,
        "Winogrande":70.96,
        "GSM8K":3.71,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"8bit",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"7872a492ebbb3c6a899f9acbd34dfd5f7e674fdd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jlevin\/guanaco-unchained-llama-2-7b",
        "Average":45.11,
        "ARC":47.35,
        "HellaSwag":72.16,
        "MMLU":41.76,
        "TruthfulQA":41.49,
        "Winogrande":64.48,
        "GSM8K":3.41,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"43f3de8bcef63eec03a1b00079c08b5932c1a429"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Charlie911\/vicuna-7b-v1.5-lora-mctaco-modified4",
        "Average":45.1,
        "ARC":40.7,
        "HellaSwag":73.08,
        "MMLU":47.26,
        "TruthfulQA":41.59,
        "Winogrande":67.88,
        "GSM8K":0.08,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"715b03c8573df06f3825d1c08b307e2a83fa8bf9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"speechlessai\/speechless-coding-7b-16k-tora",
        "Average":45.05,
        "ARC":41.13,
        "HellaSwag":64.48,
        "MMLU":38.86,
        "TruthfulQA":44.95,
        "Winogrande":63.85,
        "GSM8K":17.06,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"37281f20d54d895f8e3bc660e68564244c775ac2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JosephusCheung\/Qwen-VL-LLaMAfied-7B-Chat",
        "Average":45.0,
        "ARC":47.35,
        "HellaSwag":69.97,
        "MMLU":44.12,
        "TruthfulQA":42.87,
        "Winogrande":65.67,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"ccbd599ac46bcfbf7020be393afeecef404bce2b"
    },
    {
        "T":"?",
        "Model":"csitfun\/llama-7b-logicot",
        "Average":44.95,
        "ARC":47.01,
        "HellaSwag":72.56,
        "MMLU":38.93,
        "TruthfulQA":43.63,
        "Winogrande":67.56,
        "GSM8K":0.0,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"8e9c93c09e6a6c7d504c88d6ca598144829bced8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/WizardLM-7B-Uncensored",
        "Average":44.92,
        "ARC":47.87,
        "HellaSwag":73.08,
        "MMLU":35.42,
        "TruthfulQA":41.49,
        "Winogrande":68.43,
        "GSM8K":3.26,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":339.0,
        "Available on the Hub":true,
        "Model Sha":"14c23f9fa775ab5ce49010418f00df06d92b0b13"
    },
    {
        "T":"\u2b55",
        "Model":"OpenAssistant\/codellama-13b-oasst-sft-v10",
        "Average":44.85,
        "ARC":45.39,
        "HellaSwag":62.36,
        "MMLU":35.36,
        "TruthfulQA":45.02,
        "Winogrande":67.8,
        "GSM8K":13.19,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":42.0,
        "Available on the Hub":true,
        "Model Sha":"612dab2a8b2d77edb4fd36cfc28b3ffbbb20ffc1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"shareAI\/CodeLLaMA-chat-13b-Chinese",
        "Average":44.84,
        "ARC":43.26,
        "HellaSwag":63.87,
        "MMLU":34.29,
        "TruthfulQA":48.97,
        "Winogrande":67.88,
        "GSM8K":10.77,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"openrail",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":13.0,
        "Available on the Hub":true,
        "Model Sha":"675b3e35a9601683c2cb4ec7f1b11d2869842f36"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mosaicml\/mpt-7b-instruct",
        "Average":44.83,
        "ARC":50.34,
        "HellaSwag":77.91,
        "MMLU":32.35,
        "TruthfulQA":35.08,
        "Winogrande":70.48,
        "GSM8K":2.81,
        "Type":"fine-tuned",
        "Architecture":"MPTForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-sa-3.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":437.0,
        "Available on the Hub":true,
        "Model Sha":"925e0d80e50e77aaddaf9c3ced41ca4ea23a1025"
    },
    {
        "T":"\u2b55",
        "Model":"uukuguy\/speechless-codellama-orca-13b",
        "Average":44.83,
        "ARC":44.37,
        "HellaSwag":65.2,
        "MMLU":43.46,
        "TruthfulQA":45.94,
        "Winogrande":64.01,
        "GSM8K":5.99,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"6fdfeabe817235df3d560a6e6465c3722bc3a4ba"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"shibing624\/chinese-alpaca-plus-7b-hf",
        "Average":44.77,
        "ARC":49.23,
        "HellaSwag":70.48,
        "MMLU":38.39,
        "TruthfulQA":39.72,
        "Winogrande":70.09,
        "GSM8K":0.68,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.68,
        "Hub \u2764\ufe0f":46.0,
        "Available on the Hub":true,
        "Model Sha":"0deb5a13732f1e3e3240ea83f403c57283fe2dc8"
    },
    {
        "T":"\u2b55",
        "Model":"Writer\/palmyra-med-20b",
        "Average":44.71,
        "ARC":46.93,
        "HellaSwag":73.51,
        "MMLU":44.34,
        "TruthfulQA":35.47,
        "Winogrande":65.35,
        "GSM8K":2.65,
        "Type":"instruction-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.26,
        "Hub \u2764\ufe0f":14.0,
        "Available on the Hub":true,
        "Model Sha":"407810f75698c95000dc0ae1a9a0457be625e972"
    },
    {
        "T":"?",
        "Model":"TheBloke\/Project-Baize-v2-7B-GPTQ",
        "Average":44.5,
        "ARC":45.99,
        "HellaSwag":73.44,
        "MMLU":35.46,
        "TruthfulQA":39.92,
        "Winogrande":69.69,
        "GSM8K":2.5,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":9.04,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"5dc039834e1ea42ac334458b2e3090fe3705cc59"
    },
    {
        "T":"\u2b55",
        "Model":"qblocks\/falcon_7b_norobots",
        "Average":44.46,
        "ARC":47.87,
        "HellaSwag":77.92,
        "MMLU":27.94,
        "TruthfulQA":36.81,
        "Winogrande":71.74,
        "GSM8K":4.47,
        "Type":"instruction-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"bbe8e4a0c19ec5a94f6eff680b5a55bd08e11e31"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"WeOpenML\/Alpaca-7B-v1",
        "Average":44.41,
        "ARC":49.06,
        "HellaSwag":75.71,
        "MMLU":33.76,
        "TruthfulQA":36.28,
        "Winogrande":71.51,
        "GSM8K":0.15,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"be5cb84a84a859dd6e5e3efc4648d6d5d1a5d188"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"qblocks\/falcon_7b_norobots",
        "Average":44.4,
        "ARC":48.12,
        "HellaSwag":77.9,
        "MMLU":28.11,
        "TruthfulQA":36.76,
        "Winogrande":71.59,
        "GSM8K":3.94,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"bbe8e4a0c19ec5a94f6eff680b5a55bd08e11e31"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"luffycodes\/llama-shishya-7b-ep3-v2",
        "Average":44.33,
        "ARC":47.35,
        "HellaSwag":75.88,
        "MMLU":43.84,
        "TruthfulQA":30.16,
        "Winogrande":68.75,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"679c6cb9e869df686b1ae415ed440e6cfc05f80b"
    },
    {
        "T":"\u2b55",
        "Model":"codellama\/CodeLlama-34b-Instruct-hf",
        "Average":44.33,
        "ARC":40.78,
        "HellaSwag":35.66,
        "MMLU":39.72,
        "TruthfulQA":44.29,
        "Winogrande":74.51,
        "GSM8K":31.01,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":33.74,
        "Hub \u2764\ufe0f":154.0,
        "Available on the Hub":true,
        "Model Sha":"c109b9dde086b31725fa09ff7effdc04c03c033d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/koala-7B-HF",
        "Average":44.29,
        "ARC":47.1,
        "HellaSwag":73.58,
        "MMLU":25.53,
        "TruthfulQA":45.96,
        "Winogrande":69.93,
        "GSM8K":3.64,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":18.0,
        "Available on the Hub":true,
        "Model Sha":"d102fe3b68f1a5a50d547e4fd1c8b33b783c993b"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"mosaicml\/mpt-7b",
        "Average":44.28,
        "ARC":47.7,
        "HellaSwag":77.57,
        "MMLU":30.8,
        "TruthfulQA":33.44,
        "Winogrande":72.14,
        "GSM8K":4.02,
        "Type":"pretrained",
        "Architecture":"MPTForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":1083.0,
        "Available on the Hub":true,
        "Model Sha":"72e5f594ce36f9cabfa2a9fd8f58b491eb467ee7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"anas-awadalla\/mpt-7b",
        "Average":44.28,
        "ARC":47.7,
        "HellaSwag":77.57,
        "MMLU":30.8,
        "TruthfulQA":33.44,
        "Winogrande":72.14,
        "GSM8K":4.02,
        "Type":"fine-tuned",
        "Architecture":"MPTForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"b772e556c8e8a17d087db6935e7cd019e5eefb0f"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"openlm-research\/open_llama_7b_v2",
        "Average":44.26,
        "ARC":43.69,
        "HellaSwag":72.2,
        "MMLU":41.29,
        "TruthfulQA":35.54,
        "Winogrande":69.38,
        "GSM8K":3.49,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":86.0,
        "Available on the Hub":true,
        "Model Sha":"e5961def23172a2384543940e773ab676033c963"
    },
    {
        "T":"\u2b55",
        "Model":"Writer\/palmyra-20b-chat",
        "Average":44.18,
        "ARC":43.52,
        "HellaSwag":72.83,
        "MMLU":35.18,
        "TruthfulQA":43.17,
        "Winogrande":66.46,
        "GSM8K":3.94,
        "Type":"instruction-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":20.26,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"3b7442b7e2240846bc9cfac545bd8861c1660aa2"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"tiiuae\/falcon-7b",
        "Average":44.17,
        "ARC":47.87,
        "HellaSwag":78.13,
        "MMLU":27.79,
        "TruthfulQA":34.26,
        "Winogrande":72.38,
        "GSM8K":4.62,
        "Type":"pretrained",
        "Architecture":"RWForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.92,
        "Hub \u2764\ufe0f":885.0,
        "Available on the Hub":true,
        "Model Sha":"378337427557d1df3e742264a2901a49f25d4eb1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"speechlessai\/speechless-codellama-airoboros-orca-platypus-13b",
        "Average":44.1,
        "ARC":44.88,
        "HellaSwag":67.7,
        "MMLU":43.16,
        "TruthfulQA":40.88,
        "Winogrande":66.14,
        "GSM8K":1.82,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f01d3ab70cc23e31dcf5d6418406b08dc2003153"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"togethercomputer\/GPT-JT-6B-v0",
        "Average":44.05,
        "ARC":42.06,
        "HellaSwag":67.96,
        "MMLU":49.34,
        "TruthfulQA":38.89,
        "Winogrande":64.8,
        "GSM8K":1.21,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"41bd1937dbc51f9e589d310bddab5b4c1409e783"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/BigTranslate-13B-GPTQ",
        "Average":43.86,
        "ARC":45.31,
        "HellaSwag":75.1,
        "MMLU":31.18,
        "TruthfulQA":40.6,
        "Winogrande":70.96,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":17.99,
        "Hub \u2764\ufe0f":15.0,
        "Available on the Hub":true,
        "Model Sha":"f2968552d2f522023f3289747234aea5508980e2"
    },
    {
        "T":"\u2b55",
        "Model":"AI-Sweden-Models\/gpt-sw3-20b-instruct",
        "Average":43.7,
        "ARC":43.17,
        "HellaSwag":71.09,
        "MMLU":31.32,
        "TruthfulQA":41.02,
        "Winogrande":66.77,
        "GSM8K":8.79,
        "Type":"instruction-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"bfloat16",
        "Hub License":"other",
        "#Params (B)":20.92,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"006477ad4c4875611f20cd927f1fd76bbf5ba5ba"
    },
    {
        "T":"\u2b55",
        "Model":"synapsoft\/Llama-2-7b-hf-flan2022-1.2M",
        "Average":43.68,
        "ARC":23.29,
        "HellaSwag":78.46,
        "MMLU":42.33,
        "TruthfulQA":37.97,
        "Winogrande":75.53,
        "GSM8K":4.47,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"792f946a1413a7c58378d7a350b7d75b9df80561"
    },
    {
        "T":"\u2b55",
        "Model":"souvik0306\/falcon_7b_3epoch_norobots",
        "Average":43.65,
        "ARC":47.61,
        "HellaSwag":77.24,
        "MMLU":29.73,
        "TruthfulQA":36.27,
        "Winogrande":69.53,
        "GSM8K":1.52,
        "Type":"instruction-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"55b11c279d1a5b83f59cec0381fb41c31fd02d8d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hyunseoki\/ko-ref-llama2-13b",
        "Average":43.62,
        "ARC":48.38,
        "HellaSwag":73.56,
        "MMLU":34.83,
        "TruthfulQA":35.82,
        "Winogrande":69.14,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"c5d09631c88ab5012b48187ecd90ae773cd4bbd9"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"AI-Sweden-Models\/gpt-sw3-40b",
        "Average":43.42,
        "ARC":43.0,
        "HellaSwag":72.37,
        "MMLU":34.97,
        "TruthfulQA":37.52,
        "Winogrande":67.96,
        "GSM8K":4.7,
        "Type":"pretrained",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":39.93,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"ed18193e7292b5a821e5271d5dac95fffdf9617c"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"codellama\/CodeLlama-13b-hf",
        "Average":43.35,
        "ARC":40.87,
        "HellaSwag":63.35,
        "MMLU":32.81,
        "TruthfulQA":43.79,
        "Winogrande":67.17,
        "GSM8K":12.13,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":48.0,
        "Available on the Hub":true,
        "Model Sha":"55876f398020b287ac845b34ca08089acf4f4bc3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/CodeLlama-13b-hf",
        "Average":43.35,
        "ARC":40.87,
        "HellaSwag":63.35,
        "MMLU":32.81,
        "TruthfulQA":43.79,
        "Winogrande":67.17,
        "GSM8K":12.13,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"b7cfbbce945b966607d15ae275704922a6d04afc"
    },
    {
        "T":"?",
        "Model":"TigerResearch\/tigerbot-7b-sft",
        "Average":43.35,
        "ARC":41.64,
        "HellaSwag":60.56,
        "MMLU":29.89,
        "TruthfulQA":58.18,
        "Winogrande":63.54,
        "GSM8K":6.29,
        "Type":"",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.07,
        "Hub \u2764\ufe0f":13.0,
        "Available on the Hub":false,
        "Model Sha":"98b847905d63f74624e834db1ff95ee2814cbbd3"
    },
    {
        "T":"\u2b55",
        "Model":"tiiuae\/falcon-7b-instruct",
        "Average":43.26,
        "ARC":46.16,
        "HellaSwag":70.85,
        "MMLU":25.84,
        "TruthfulQA":44.08,
        "Winogrande":67.96,
        "GSM8K":4.7,
        "Type":"instruction-tuned",
        "Architecture":"FalconForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.92,
        "Hub \u2764\ufe0f":676.0,
        "Available on the Hub":true,
        "Model Sha":"cf4b3c42ce2fdfe24f753f0f0d179202fea59c99"
    },
    {
        "T":"?",
        "Model":"JosephusCheung\/Guanaco",
        "Average":43.25,
        "ARC":50.17,
        "HellaSwag":72.69,
        "MMLU":30.3,
        "TruthfulQA":37.64,
        "Winogrande":68.67,
        "GSM8K":0.0,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"gpl-3.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":213.0,
        "Available on the Hub":true,
        "Model Sha":"bed6f3bd18f07a4a379525645cbd86d622b12836"
    },
    {
        "T":"\u2b55",
        "Model":"tiiuae\/falcon-7b-instruct",
        "Average":43.16,
        "ARC":45.82,
        "HellaSwag":70.78,
        "MMLU":25.66,
        "TruthfulQA":44.07,
        "Winogrande":68.03,
        "GSM8K":4.62,
        "Type":"instruction-tuned",
        "Architecture":"RWForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.92,
        "Hub \u2764\ufe0f":676.0,
        "Available on the Hub":true,
        "Model Sha":"eb410fb6ffa9028e97adb801f0d6ec46d02f8b07"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ziqingyang\/chinese-llama-2-7b",
        "Average":43.14,
        "ARC":44.45,
        "HellaSwag":69.5,
        "MMLU":37.47,
        "TruthfulQA":37.0,
        "Winogrande":68.98,
        "GSM8K":1.44,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.7,
        "Hub \u2764\ufe0f":64.0,
        "Available on the Hub":true,
        "Model Sha":"557b5cbd48a4a4eb5a08e975c4b6e11ac1ed4cbc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"togethercomputer\/GPT-JT-6B-v1",
        "Average":43.13,
        "ARC":40.87,
        "HellaSwag":67.15,
        "MMLU":47.19,
        "TruthfulQA":37.07,
        "Winogrande":65.27,
        "GSM8K":1.21,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":302.0,
        "Available on the Hub":true,
        "Model Sha":"f34aa35f906895602c1f86f5685e598afdea8051"
    },
    {
        "T":"?",
        "Model":"FreedomIntelligence\/phoenix-inst-chat-7b",
        "Average":43.03,
        "ARC":44.71,
        "HellaSwag":63.23,
        "MMLU":39.06,
        "TruthfulQA":47.08,
        "Winogrande":62.83,
        "GSM8K":1.29,
        "Type":"",
        "Architecture":"BloomForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.07,
        "Hub \u2764\ufe0f":44.0,
        "Available on the Hub":true,
        "Model Sha":"5ed4d9570e0f76e1becb05bf467a7b4ff7b66055"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"togethercomputer\/GPT-NeoXT-Chat-Base-20B",
        "Average":43.02,
        "ARC":45.65,
        "HellaSwag":74.03,
        "MMLU":29.92,
        "TruthfulQA":34.51,
        "Winogrande":67.09,
        "GSM8K":6.9,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.24,
        "Hub \u2764\ufe0f":691.0,
        "Available on the Hub":true,
        "Model Sha":"d386708e84d862a65f7d2b4989f64750cb657227"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"GeorgiaTechResearchInstitute\/galpaca-30b",
        "Average":43.0,
        "ARC":49.57,
        "HellaSwag":58.2,
        "MMLU":43.78,
        "TruthfulQA":41.16,
        "Winogrande":62.51,
        "GSM8K":2.81,
        "Type":"fine-tuned",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":29.97,
        "Hub \u2764\ufe0f":55.0,
        "Available on the Hub":true,
        "Model Sha":"a1f0c4bedd65b485a0d4d3a3bd60d7a4599f1eaf"
    },
    {
        "T":"\u2b55",
        "Model":"TheBloke\/CodeLlama-34B-Instruct-fp16",
        "Average":43.0,
        "ARC":40.78,
        "HellaSwag":35.66,
        "MMLU":39.72,
        "TruthfulQA":44.29,
        "Winogrande":74.51,
        "GSM8K":23.05,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":33.74,
        "Hub \u2764\ufe0f":9.0,
        "Available on the Hub":true,
        "Model Sha":"a4d0ce949de4d5b5f74691641efb5b70736a32a8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lyogavin\/Anima-7B-100K",
        "Average":42.98,
        "ARC":46.59,
        "HellaSwag":72.28,
        "MMLU":33.4,
        "TruthfulQA":37.84,
        "Winogrande":67.09,
        "GSM8K":0.68,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":14.0,
        "Available on the Hub":true,
        "Model Sha":"e303cf09e553c38ca5e0c0816d83631801ca5776"
    },
    {
        "T":"\u2b55",
        "Model":"Writer\/InstructPalmyra-20b",
        "Average":42.91,
        "ARC":47.1,
        "HellaSwag":73.0,
        "MMLU":28.26,
        "TruthfulQA":41.81,
        "Winogrande":64.72,
        "GSM8K":2.58,
        "Type":"instruction-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.26,
        "Hub \u2764\ufe0f":36.0,
        "Available on the Hub":true,
        "Model Sha":"c78df447c70d4677b128b1df864b9fff8338d900"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dvruette\/gpt-neox-20b-full-precision",
        "Average":42.87,
        "ARC":48.81,
        "HellaSwag":74.44,
        "MMLU":26.16,
        "TruthfulQA":36.89,
        "Winogrande":68.27,
        "GSM8K":2.65,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":20.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"20b347273d90da7c2c9eb4c32d4173dba862a0d2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/landmark-attention-llama7b-fp16",
        "Average":42.84,
        "ARC":47.35,
        "HellaSwag":65.81,
        "MMLU":31.59,
        "TruthfulQA":42.63,
        "Winogrande":68.03,
        "GSM8K":1.59,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":false,
        "Model Sha":"bf8bdcb0c30cceb0ceda33cf5fde683807e39a58"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"facebook\/opt-66b",
        "Average":42.78,
        "ARC":46.33,
        "HellaSwag":76.25,
        "MMLU":26.99,
        "TruthfulQA":35.43,
        "Winogrande":70.01,
        "GSM8K":1.67,
        "Type":"pretrained",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":65.72,
        "Hub \u2764\ufe0f":171.0,
        "Available on the Hub":true,
        "Model Sha":"7259969061237fe940036d22bea0fd349e4485e9"
    },
    {
        "T":"?",
        "Model":"Vmware\/open-llama-7b-v2-open-instruct",
        "Average":42.75,
        "ARC":39.76,
        "HellaSwag":70.31,
        "MMLU":35.16,
        "TruthfulQA":39.53,
        "Winogrande":64.33,
        "GSM8K":7.43,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"b8fbe09571a71603ab517fe897a1281005060b62"
    },
    {
        "T":"\u2b55",
        "Model":"llm-agents\/tora-code-13b-v1.0",
        "Average":42.7,
        "ARC":44.45,
        "HellaSwag":69.29,
        "MMLU":36.67,
        "TruthfulQA":34.98,
        "Winogrande":62.59,
        "GSM8K":8.19,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"4bf5b528d95a507b435c24a8986afe80d5951782"
    },
    {
        "T":"?",
        "Model":"VMware\/open-llama-7b-open-instruct",
        "Average":42.59,
        "ARC":49.74,
        "HellaSwag":73.67,
        "MMLU":31.52,
        "TruthfulQA":34.65,
        "Winogrande":65.43,
        "GSM8K":0.53,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-sa-3.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":25.0,
        "Available on the Hub":true,
        "Model Sha":"fdf9f034163cce67e04d55172155f0e07b1b19a0"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Salesforce\/codegen-16B-nl",
        "Average":42.59,
        "ARC":46.76,
        "HellaSwag":71.87,
        "MMLU":32.35,
        "TruthfulQA":33.95,
        "Winogrande":67.96,
        "GSM8K":2.65,
        "Type":"pretrained",
        "Architecture":"CodeGenForCausalLM",
        "Precision":"float16",
        "Hub License":"bsd-3-clause",
        "#Params (B)":15.72,
        "Hub \u2764\ufe0f":17.0,
        "Available on the Hub":true,
        "Model Sha":"b65951b0cf7c5639f73caea801a892788608ed69"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"h2oai\/h2ogpt-gm-oasst1-en-1024-20b",
        "Average":42.58,
        "ARC":48.04,
        "HellaSwag":72.76,
        "MMLU":25.96,
        "TruthfulQA":39.92,
        "Winogrande":66.3,
        "GSM8K":2.5,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.24,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"1a5b8d25587eab67d837621a6c9423e7ef6df289"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dvruette\/oasst-gpt-neox-20b-1000-steps",
        "Average":42.51,
        "ARC":48.55,
        "HellaSwag":74.61,
        "MMLU":26.39,
        "TruthfulQA":35.63,
        "Winogrande":66.77,
        "GSM8K":3.11,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":20.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"4aec11ef19103796fb21387ce925b63c9d61dae1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vibhorag101\/llama-2-13b-chat-hf-phr_mental_therapy",
        "Average":42.5,
        "ARC":38.82,
        "HellaSwag":72.76,
        "MMLU":23.12,
        "TruthfulQA":46.92,
        "Winogrande":65.59,
        "GSM8K":7.81,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"4bit",
        "Hub License":"mit",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"0fe5a48f3d99492cb180fc6efda5b138677ca1de"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"h2oai\/h2ogpt-oasst1-512-20b",
        "Average":42.44,
        "ARC":46.93,
        "HellaSwag":72.77,
        "MMLU":26.25,
        "TruthfulQA":37.5,
        "Winogrande":68.03,
        "GSM8K":3.18,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.24,
        "Hub \u2764\ufe0f":38.0,
        "Available on the Hub":true,
        "Model Sha":"3bdf6f870ca14bcc5587b666fbe57488f7854d30"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"JosephusCheung\/LL7M",
        "Average":42.38,
        "ARC":44.97,
        "HellaSwag":68.81,
        "MMLU":34.44,
        "TruthfulQA":41.39,
        "Winogrande":64.09,
        "GSM8K":0.61,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":6.64,
        "Hub \u2764\ufe0f":34.0,
        "Available on the Hub":true,
        "Model Sha":"9b31bbf38a43d41eaf166fb3573f706b23cb1c13"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"togethercomputer\/RedPajama-INCITE-7B-Instruct",
        "Average":42.38,
        "ARC":44.11,
        "HellaSwag":72.02,
        "MMLU":37.62,
        "TruthfulQA":33.96,
        "Winogrande":64.96,
        "GSM8K":1.59,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":103.0,
        "Available on the Hub":true,
        "Model Sha":"95667a602ff2646bf67fe3a57c4eb9a1edec87fe"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"togethercomputer\/RedPajama-INCITE-Instruct-7B-v0.1",
        "Average":42.38,
        "ARC":44.11,
        "HellaSwag":72.02,
        "MMLU":37.62,
        "TruthfulQA":33.96,
        "Winogrande":64.96,
        "GSM8K":1.59,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":103.0,
        "Available on the Hub":true,
        "Model Sha":"95667a602ff2646bf67fe3a57c4eb9a1edec87fe"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"openlm-research\/open_llama_7b",
        "Average":42.31,
        "ARC":47.01,
        "HellaSwag":71.98,
        "MMLU":30.49,
        "TruthfulQA":34.85,
        "Winogrande":67.96,
        "GSM8K":1.59,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":99.0,
        "Available on the Hub":true,
        "Model Sha":"6fb184ff23774c25bf84b3628e49c8b78372c7be"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cmarkea\/bloomz-7b1-mt-sft-chat",
        "Average":42.24,
        "ARC":44.03,
        "HellaSwag":62.6,
        "MMLU":38.64,
        "TruthfulQA":44.34,
        "Winogrande":63.3,
        "GSM8K":0.53,
        "Type":"fine-tuned",
        "Architecture":"BloomForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":7.07,
        "Hub \u2764\ufe0f":9.0,
        "Available on the Hub":true,
        "Model Sha":"8c2dc302780fe320ee3428f3db2ee7ff3684dcef"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/Galpaca-30b-MiniOrca",
        "Average":42.23,
        "ARC":48.89,
        "HellaSwag":57.8,
        "MMLU":43.72,
        "TruthfulQA":41.1,
        "Winogrande":60.06,
        "GSM8K":1.82,
        "Type":"instruction-tuned",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":29.97,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"681d92f8f71ca3e8425da19afee89ed84baedf1d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenAssistant\/pythia-12b-sft-v8-7k-steps",
        "Average":42.21,
        "ARC":44.03,
        "HellaSwag":70.28,
        "MMLU":26.55,
        "TruthfulQA":36.53,
        "Winogrande":65.27,
        "GSM8K":10.61,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.58,
        "Hub \u2764\ufe0f":21.0,
        "Available on the Hub":true,
        "Model Sha":"275c9b71bfab4e271d1ed85515c61e317b6ef65e"
    },
    {
        "T":"?",
        "Model":"bigscience\/bloomz-7b1",
        "Average":42.21,
        "ARC":42.49,
        "HellaSwag":63.01,
        "MMLU":37.85,
        "TruthfulQA":45.2,
        "Winogrande":64.64,
        "GSM8K":0.08,
        "Type":"",
        "Architecture":"BloomForCausalLM",
        "Precision":"float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":7.07,
        "Hub \u2764\ufe0f":110.0,
        "Available on the Hub":true,
        "Model Sha":"2f4c4f3ebcf171dbbe2bae989ea2d2f3d3486a97"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"klosax\/open_llama_13b_600bt_preview",
        "Average":42.21,
        "ARC":44.28,
        "HellaSwag":72.43,
        "MMLU":31.47,
        "TruthfulQA":34.66,
        "Winogrande":68.43,
        "GSM8K":1.97,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"3465eaca4d293ccc6ce66888e6c8bd9032ae7071"
    },
    {
        "T":"?",
        "Model":"TehVenom\/Moderator-Chan_GPT-JT-6b",
        "Average":42.17,
        "ARC":43.69,
        "HellaSwag":70.77,
        "MMLU":35.61,
        "TruthfulQA":36.05,
        "Winogrande":65.59,
        "GSM8K":1.29,
        "Type":"",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f2b7cda25f6965c1551fa78e9e38676994bc6638"
    },
    {
        "T":"?",
        "Model":"bigscience\/bloomz-7b1-mt",
        "Average":42.14,
        "ARC":43.86,
        "HellaSwag":62.91,
        "MMLU":37.35,
        "TruthfulQA":45.65,
        "Winogrande":63.06,
        "GSM8K":0.0,
        "Type":"",
        "Architecture":"BloomForCausalLM",
        "Precision":"float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":7.07,
        "Hub \u2764\ufe0f":120.0,
        "Available on the Hub":true,
        "Model Sha":"76875e6ea8df98157fb032c48ad6e354fd6a077b"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Writer\/palmyra-large",
        "Average":42.09,
        "ARC":44.97,
        "HellaSwag":71.85,
        "MMLU":28.54,
        "TruthfulQA":35.93,
        "Winogrande":67.88,
        "GSM8K":3.41,
        "Type":"pretrained",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.26,
        "Hub \u2764\ufe0f":17.0,
        "Available on the Hub":true,
        "Model Sha":"40086d791942cb28f55e679cd3fb6f6b5ba4effd"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"RWKV\/rwkv-raven-14b",
        "Average":42.09,
        "ARC":44.62,
        "HellaSwag":71.25,
        "MMLU":25.92,
        "TruthfulQA":41.93,
        "Winogrande":66.69,
        "GSM8K":2.12,
        "Type":"pretrained",
        "Architecture":"RwkvForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.89,
        "Hub \u2764\ufe0f":47.0,
        "Available on the Hub":true,
        "Model Sha":"359c0649b4f1d10a26ebea32908035bc00d152ee"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AlekseyKorshuk\/pygmalion-6b-vicuna-chatml",
        "Average":42.08,
        "ARC":40.61,
        "HellaSwag":67.73,
        "MMLU":33.92,
        "TruthfulQA":42.76,
        "Winogrande":63.06,
        "GSM8K":4.4,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"ee3ada91a69a194cedfabbfeab98f1499b75cb44"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"acrastt\/Marx-3B-V2",
        "Average":42.08,
        "ARC":44.03,
        "HellaSwag":72.92,
        "MMLU":27.84,
        "TruthfulQA":39.92,
        "Winogrande":66.54,
        "GSM8K":1.21,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":21.0,
        "Available on the Hub":true,
        "Model Sha":"5fba568304f6f876f5b9e42026f986ea245b836b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NurtureAI\/Orca-2-7B-16k",
        "Average":42.05,
        "ARC":50.6,
        "HellaSwag":63.89,
        "MMLU":36.68,
        "TruthfulQA":45.37,
        "Winogrande":54.22,
        "GSM8K":1.52,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"ab373033e98dcdbcc3aadb51374ae392656c6603"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-tora-code-7b-v1.0",
        "Average":42.04,
        "ARC":42.66,
        "HellaSwag":65.16,
        "MMLU":38.56,
        "TruthfulQA":42.06,
        "Winogrande":62.9,
        "GSM8K":0.91,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f7b1f87a096045f1bba8f68c62e062102218717b"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"facebook\/opt-30b",
        "Average":42.0,
        "ARC":43.26,
        "HellaSwag":74.07,
        "MMLU":26.66,
        "TruthfulQA":35.16,
        "Winogrande":70.64,
        "GSM8K":2.2,
        "Type":"pretrained",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":29.98,
        "Hub \u2764\ufe0f":133.0,
        "Available on the Hub":true,
        "Model Sha":"ceea0a90ac0f6fae7c2c34bcb40477438c152546"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dvruette\/oasst-gpt-neox-20b-3000-steps",
        "Average":41.97,
        "ARC":46.42,
        "HellaSwag":72.08,
        "MMLU":26.16,
        "TruthfulQA":35.53,
        "Winogrande":68.75,
        "GSM8K":2.88,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":20.24,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f0462a8b7908f61202d86e6a9a2996d8339363b5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenAssistant\/pythia-12b-sft-v8-2.5k-steps",
        "Average":41.97,
        "ARC":42.32,
        "HellaSwag":70.15,
        "MMLU":27.36,
        "TruthfulQA":36.75,
        "Winogrande":65.67,
        "GSM8K":9.55,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.58,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"142e306db8e279a07c557ea5a919ab7e7a4af17c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"h2oai\/h2ogpt-gm-oasst1-multilang-1024-20b",
        "Average":41.9,
        "ARC":47.44,
        "HellaSwag":72.58,
        "MMLU":26.37,
        "TruthfulQA":34.39,
        "Winogrande":68.43,
        "GSM8K":2.2,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.24,
        "Hub \u2764\ufe0f":9.0,
        "Available on the Hub":true,
        "Model Sha":"b3a6bf4250a037c09e451344e2a4e987011b79de"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"wenge-research\/yayi-7b",
        "Average":41.88,
        "ARC":46.33,
        "HellaSwag":61.72,
        "MMLU":36.34,
        "TruthfulQA":43.7,
        "Winogrande":62.27,
        "GSM8K":0.91,
        "Type":"fine-tuned",
        "Architecture":"BloomForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":7.07,
        "Hub \u2764\ufe0f":28.0,
        "Available on the Hub":true,
        "Model Sha":"00be6c9e41a8367a855c6f18ebfa08f5ecdb2cc4"
    },
    {
        "T":"?",
        "Model":"togethercomputer\/GPT-JT-Moderation-6B",
        "Average":41.8,
        "ARC":40.53,
        "HellaSwag":67.66,
        "MMLU":41.63,
        "TruthfulQA":37.33,
        "Winogrande":62.67,
        "GSM8K":0.99,
        "Type":"",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":30.0,
        "Available on the Hub":true,
        "Model Sha":"1297870783f6091294769014afddf94499966a78"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Yukang\/LongAlpaca-13B",
        "Average":41.74,
        "ARC":42.58,
        "HellaSwag":72.03,
        "MMLU":34.91,
        "TruthfulQA":36.85,
        "Winogrande":64.09,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"e80966ae720de9a844441a4a2bbc661106969915"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Danielbrdz\/Barcenas-3b",
        "Average":41.74,
        "ARC":43.17,
        "HellaSwag":67.82,
        "MMLU":29.16,
        "TruthfulQA":41.56,
        "Winogrande":66.22,
        "GSM8K":2.5,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":3.0,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"2b6b8bfd3946c02fa4a5182ed008df8ad324a406"
    },
    {
        "T":"\u2b55",
        "Model":"AI-Sweden-Models\/gpt-sw3-6.7b-v2-instruct",
        "Average":41.72,
        "ARC":40.78,
        "HellaSwag":67.77,
        "MMLU":31.57,
        "TruthfulQA":40.32,
        "Winogrande":63.54,
        "GSM8K":6.37,
        "Type":"instruction-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"bfloat16",
        "Hub License":"other",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"81ca95a4e93746240994d1e6797ffa64dc796bd9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"acrastt\/Marx-3B",
        "Average":41.71,
        "ARC":43.17,
        "HellaSwag":72.68,
        "MMLU":28.46,
        "TruthfulQA":39.09,
        "Winogrande":65.59,
        "GSM8K":1.29,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":11.0,
        "Available on the Hub":true,
        "Model Sha":"c0dcc44989cf4e006efae31abbcef7e8be8547c0"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/gpt-neox-20b",
        "Average":41.69,
        "ARC":45.73,
        "HellaSwag":73.45,
        "MMLU":25.0,
        "TruthfulQA":31.61,
        "Winogrande":68.9,
        "GSM8K":5.46,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.74,
        "Hub \u2764\ufe0f":433.0,
        "Available on the Hub":true,
        "Model Sha":"9369f145ca7b66ef62760f9351af951b2d53b77f"
    },
    {
        "T":"?",
        "Model":"OpenAssistant\/pythia-12b-sft-v8-rlhf-2k-steps",
        "Average":41.65,
        "ARC":43.43,
        "HellaSwag":70.08,
        "MMLU":26.12,
        "TruthfulQA":36.06,
        "Winogrande":64.64,
        "GSM8K":9.55,
        "Type":"",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.58,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"a0debfed4a020d449e3d00f4e75f2c2aefb68db3"
    },
    {
        "T":"\u2b55",
        "Model":"vihangd\/shearedplats-2.7b-v2",
        "Average":41.61,
        "ARC":42.41,
        "HellaSwag":72.58,
        "MMLU":27.52,
        "TruthfulQA":39.76,
        "Winogrande":65.9,
        "GSM8K":1.52,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":2.7,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"2837296f28d6aa0fb6c1fe382f553e65c8e1e5f3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"glaiveai\/glaive-coder-7b",
        "Average":41.56,
        "ARC":42.66,
        "HellaSwag":64.69,
        "MMLU":37.15,
        "TruthfulQA":39.88,
        "Winogrande":59.75,
        "GSM8K":5.23,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":43.0,
        "Available on the Hub":true,
        "Model Sha":"72a255a58480ef0713eed988312fe82f77f94f37"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"togethercomputer\/RedPajama-INCITE-7B-Base",
        "Average":41.49,
        "ARC":46.25,
        "HellaSwag":71.63,
        "MMLU":27.68,
        "TruthfulQA":33.03,
        "Winogrande":67.32,
        "GSM8K":3.03,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":89.0,
        "Available on the Hub":true,
        "Model Sha":"78f7e482443971f4873ba3239f0ac810a367833b"
    },
    {
        "T":"?",
        "Model":"nomic-ai\/gpt4all-j",
        "Average":41.49,
        "ARC":41.98,
        "HellaSwag":64.06,
        "MMLU":28.2,
        "TruthfulQA":42.78,
        "Winogrande":64.72,
        "GSM8K":7.2,
        "Type":"",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":250.0,
        "Available on the Hub":true,
        "Model Sha":"73c15208cb608be2949b7c6e4ba6d88f0176c267"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dvruette\/oasst-pythia-12b-pretrained-sft",
        "Average":41.48,
        "ARC":45.31,
        "HellaSwag":67.67,
        "MMLU":27.81,
        "TruthfulQA":38.16,
        "Winogrande":65.9,
        "GSM8K":4.02,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":11.58,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"c21fbece4253841f2d6e15f04f60fe1ba6f990dd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"harborwater\/open-llama-3b-v2-wizard-evol-instuct-v2-196k",
        "Average":41.46,
        "ARC":41.81,
        "HellaSwag":73.01,
        "MMLU":26.36,
        "TruthfulQA":38.99,
        "Winogrande":66.69,
        "GSM8K":1.9,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"4da0c661e6df1235c9997b996c8e395b87248406"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"GeneZC\/MiniMA-3B",
        "Average":41.44,
        "ARC":43.43,
        "HellaSwag":68.06,
        "MMLU":28.69,
        "TruthfulQA":39.76,
        "Winogrande":65.98,
        "GSM8K":2.73,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.0,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"0a2f9d6bbb3959d68fe52e07ee6f54e8242f91ec"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"harborwater\/open-llama-3b-everything-v2",
        "Average":41.41,
        "ARC":42.83,
        "HellaSwag":73.28,
        "MMLU":26.87,
        "TruthfulQA":37.26,
        "Winogrande":66.61,
        "GSM8K":1.59,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"31ce2c1611d9f7d56184ceb5bff6a7e95a180c03"
    },
    {
        "T":"?",
        "Model":"Fredithefish\/ReasonixPajama-3B-HF",
        "Average":41.41,
        "ARC":39.25,
        "HellaSwag":63.47,
        "MMLU":26.09,
        "TruthfulQA":55.42,
        "Winogrande":63.69,
        "GSM8K":0.53,
        "Type":"",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.91,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"fa87c904b5921231b9f6f94b9c537cdda8783b96"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hakurei\/mommygpt-3B",
        "Average":41.36,
        "ARC":41.89,
        "HellaSwag":71.69,
        "MMLU":28.74,
        "TruthfulQA":37.9,
        "Winogrande":65.82,
        "GSM8K":2.12,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"0369335d693b753774050ae44dbaf73bac39e9eb"
    },
    {
        "T":"?",
        "Model":"psmathur\/orca_mini_13b",
        "Average":41.36,
        "ARC":42.06,
        "HellaSwag":63.4,
        "MMLU":35.43,
        "TruthfulQA":43.1,
        "Winogrande":64.17,
        "GSM8K":0.0,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":95.0,
        "Available on the Hub":true,
        "Model Sha":"ca900c8f3145de40cd188c559b2901a2e4711546"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"NucleusAI\/nucleus-22B-token-500B",
        "Average":41.33,
        "ARC":40.7,
        "HellaSwag":69.39,
        "MMLU":30.11,
        "TruthfulQA":39.16,
        "Winogrande":67.64,
        "GSM8K":0.99,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":21.83,
        "Hub \u2764\ufe0f":19.0,
        "Available on the Hub":true,
        "Model Sha":"49bb1a47c0d32b4bfa6630a4eff04a857adcd4ca"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"chargoddard\/llama-2-34b-uncode",
        "Average":41.33,
        "ARC":39.51,
        "HellaSwag":33.9,
        "MMLU":38.49,
        "TruthfulQA":40.94,
        "Winogrande":74.35,
        "GSM8K":20.77,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":33.74,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"d434d06249feb6ca511b0a09162130bcc59d84e3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenAssistant\/oasst-sft-4-pythia-12b-epoch-3.5",
        "Average":41.31,
        "ARC":45.73,
        "HellaSwag":68.59,
        "MMLU":26.82,
        "TruthfulQA":37.81,
        "Winogrande":65.9,
        "GSM8K":3.03,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.58,
        "Hub \u2764\ufe0f":335.0,
        "Available on the Hub":true,
        "Model Sha":"626b8c140cfdedb119dfb78c626cd772283dee33"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/orca_mini_7b",
        "Average":41.27,
        "ARC":43.94,
        "HellaSwag":65.22,
        "MMLU":29.97,
        "TruthfulQA":42.03,
        "Winogrande":66.06,
        "GSM8K":0.38,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":17.0,
        "Available on the Hub":true,
        "Model Sha":"6ed0dca683685cb5b9e7df599f87d311f00ba6db"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/GPT-NeoX-20B-Erebus",
        "Average":41.26,
        "ARC":45.48,
        "HellaSwag":72.79,
        "MMLU":26.77,
        "TruthfulQA":32.15,
        "Winogrande":68.11,
        "GSM8K":2.27,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.24,
        "Hub \u2764\ufe0f":70.0,
        "Available on the Hub":true,
        "Model Sha":"1a80940a290452af71caf17a8e520955eb338e0f"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"togethercomputer\/RedPajama-INCITE-Base-7B-v0.1",
        "Average":41.25,
        "ARC":46.25,
        "HellaSwag":71.63,
        "MMLU":27.68,
        "TruthfulQA":33.03,
        "Winogrande":67.32,
        "GSM8K":1.59,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":89.0,
        "Available on the Hub":true,
        "Model Sha":"78f7e482443971f4873ba3239f0ac810a367833b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CobraMamba\/mamba-gpt-3b-v4",
        "Average":41.24,
        "ARC":42.58,
        "HellaSwag":71.04,
        "MMLU":30.04,
        "TruthfulQA":37.26,
        "Winogrande":65.82,
        "GSM8K":0.68,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"49cdf710c1a9178ddf616da79211fdcdb2170c3f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"acrastt\/Griffin-3B",
        "Average":41.13,
        "ARC":41.81,
        "HellaSwag":72.3,
        "MMLU":26.36,
        "TruthfulQA":38.33,
        "Winogrande":67.01,
        "GSM8K":0.99,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"edbea6fe86d0bc2673c10269828008a1cb451919"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"VMware\/open-llama-0.7T-7B-open-instruct-v1.1",
        "Average":41.11,
        "ARC":46.67,
        "HellaSwag":67.67,
        "MMLU":28.55,
        "TruthfulQA":37.6,
        "Winogrande":65.43,
        "GSM8K":0.76,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"75741b55ad462330e3498d1506f438f835152177"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CobraMamba\/mamba-gpt-3b-v3",
        "Average":41.11,
        "ARC":41.72,
        "HellaSwag":71.05,
        "MMLU":27.31,
        "TruthfulQA":37.86,
        "Winogrande":67.48,
        "GSM8K":1.21,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":12.0,
        "Available on the Hub":true,
        "Model Sha":"d860a90ef6b30c695b985dd2ff382d4bbb80e857"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenAssistant\/pythia-12b-pre-v8-12.5k-steps",
        "Average":41.1,
        "ARC":41.47,
        "HellaSwag":68.8,
        "MMLU":26.58,
        "TruthfulQA":36.82,
        "Winogrande":65.27,
        "GSM8K":7.66,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.58,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"37ca702e957a4b740689d67c58c284224e2fbae2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/GPT-NeoX-20B-Skein",
        "Average":41.1,
        "ARC":44.97,
        "HellaSwag":72.68,
        "MMLU":25.99,
        "TruthfulQA":31.64,
        "Winogrande":68.43,
        "GSM8K":2.88,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":20.24,
        "Hub \u2764\ufe0f":9.0,
        "Available on the Hub":true,
        "Model Sha":"dd98d514b5aff4e820922c88a73d6d5bf17f332e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"harborwater\/open-llama-3b-v2-wizard-evol-instuct-v2-196k",
        "Average":41.09,
        "ARC":41.21,
        "HellaSwag":72.88,
        "MMLU":25.39,
        "TruthfulQA":38.87,
        "Winogrande":66.61,
        "GSM8K":1.59,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"8bit",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"4da0c661e6df1235c9997b996c8e395b87248406"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"RobbeD\/OpenLlama-Platypus-3B",
        "Average":41.05,
        "ARC":41.21,
        "HellaSwag":71.67,
        "MMLU":29.86,
        "TruthfulQA":36.45,
        "Winogrande":65.98,
        "GSM8K":1.14,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"d3a0bf8e1181be02cc9c4c4cdfedaedacaefbfac"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"acrastt\/Puma-3B",
        "Average":41.02,
        "ARC":41.3,
        "HellaSwag":71.85,
        "MMLU":27.51,
        "TruthfulQA":38.34,
        "Winogrande":66.38,
        "GSM8K":0.76,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"1159e9cdd05c03d31331f329ba58e4e3444943be"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"harborwater\/wizard-orca-3b",
        "Average":41.0,
        "ARC":41.72,
        "HellaSwag":71.78,
        "MMLU":24.49,
        "TruthfulQA":40.04,
        "Winogrande":66.93,
        "GSM8K":1.06,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"ffc81b58375342f12e38a67272d95458a72e8d09"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"harborwater\/open-llama-3b-claude-30k",
        "Average":40.93,
        "ARC":41.72,
        "HellaSwag":72.64,
        "MMLU":24.03,
        "TruthfulQA":38.46,
        "Winogrande":66.54,
        "GSM8K":2.2,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"049db7fda44e5ce1e8febf5c3f45e3a93aaaa859"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"princeton-nlp\/Sheared-LLaMA-2.7B",
        "Average":40.84,
        "ARC":41.72,
        "HellaSwag":71.01,
        "MMLU":26.92,
        "TruthfulQA":37.32,
        "Winogrande":67.01,
        "GSM8K":1.06,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.62,
        "Hub \u2764\ufe0f":24.0,
        "Available on the Hub":true,
        "Model Sha":"16347024c4df6cd114720958964a850fc287cac0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"digitous\/GPT-R",
        "Average":40.8,
        "ARC":41.21,
        "HellaSwag":66.89,
        "MMLU":36.5,
        "TruthfulQA":34.22,
        "Winogrande":64.4,
        "GSM8K":1.59,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"bigscience-openrail-m",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":9.0,
        "Available on the Hub":true,
        "Model Sha":"92b955a3ff74aa577fa0d8517dfc314847ef60af"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AtAndDev\/ShortKing-3b-v0.3",
        "Average":40.8,
        "ARC":40.96,
        "HellaSwag":70.72,
        "MMLU":26.21,
        "TruthfulQA":38.78,
        "Winogrande":66.93,
        "GSM8K":1.21,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"4bcf1610eb1f3959568d5acee74833c41502bf04"
    },
    {
        "T":"?",
        "Model":"dvruette\/oasst-pythia-12b-6000-steps",
        "Average":40.77,
        "ARC":45.39,
        "HellaSwag":69.68,
        "MMLU":25.97,
        "TruthfulQA":39.85,
        "Winogrande":63.22,
        "GSM8K":0.53,
        "Type":"",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":11.58,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"e2ccc0ef8d1cc5ffc8b0e2e885f03ef50597ea8a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenAssistant\/oasst-sft-1-pythia-12b",
        "Average":40.77,
        "ARC":46.42,
        "HellaSwag":70.0,
        "MMLU":26.19,
        "TruthfulQA":39.19,
        "Winogrande":62.19,
        "GSM8K":0.61,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.58,
        "Hub \u2764\ufe0f":277.0,
        "Available on the Hub":true,
        "Model Sha":"293df535fe7711a5726987fc2f17dfc87de452a1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"golaxy\/gogpt-7b-bloom",
        "Average":40.75,
        "ARC":44.62,
        "HellaSwag":62.56,
        "MMLU":33.81,
        "TruthfulQA":40.61,
        "Winogrande":62.9,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"BloomForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.07,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"8f9996f852db583b982efbd671465d18ad13ffae"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hyunseoki\/ko-ref-llama2-7b",
        "Average":40.75,
        "ARC":42.66,
        "HellaSwag":66.58,
        "MMLU":30.41,
        "TruthfulQA":38.62,
        "Winogrande":66.22,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"1ee08c79ae7393473754b77e82b1472ef63d5dd2"
    },
    {
        "T":"?",
        "Model":"dvruette\/oasst-pythia-12b-flash-attn-5000-steps",
        "Average":40.73,
        "ARC":44.97,
        "HellaSwag":69.75,
        "MMLU":26.64,
        "TruthfulQA":38.89,
        "Winogrande":63.14,
        "GSM8K":0.99,
        "Type":"",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":11.58,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"5227ec9c9def4b0bdf6c7ad95d9f77cbf458283d"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"AI-Sweden-Models\/gpt-sw3-20b",
        "Average":40.71,
        "ARC":41.81,
        "HellaSwag":68.75,
        "MMLU":28.47,
        "TruthfulQA":37.1,
        "Winogrande":67.17,
        "GSM8K":0.99,
        "Type":"pretrained",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"bfloat16",
        "Hub License":"other",
        "#Params (B)":20.92,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"36797b7835a9e656af456e0006465a3af48735fc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AlekseyKorshuk\/chatml-pyg-v1",
        "Average":40.7,
        "ARC":37.88,
        "HellaSwag":63.29,
        "MMLU":32.77,
        "TruthfulQA":42.61,
        "Winogrande":62.51,
        "GSM8K":5.16,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"79d5a4d53953ca1c26bc2155f168b7e2108f377f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"h2oai\/h2ogpt-gm-oasst1-en-1024-12b",
        "Average":40.65,
        "ARC":43.09,
        "HellaSwag":69.75,
        "MMLU":25.87,
        "TruthfulQA":38.0,
        "Winogrande":66.14,
        "GSM8K":1.06,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.59,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"e547fffafb382fd39ef5de35ba3b5afc1b43e74d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/fairseq-dense-13B",
        "Average":40.62,
        "ARC":40.36,
        "HellaSwag":75.51,
        "MMLU":27.07,
        "TruthfulQA":32.83,
        "Winogrande":67.96,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"XGLMForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.84,
        "Hub \u2764\ufe0f":13.0,
        "Available on the Hub":true,
        "Model Sha":"785793f6b216afd9fc664fc63e8e6c776a016825"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"harborwater\/open-llama-3b-everythingLM-2048",
        "Average":40.62,
        "ARC":42.75,
        "HellaSwag":71.72,
        "MMLU":27.16,
        "TruthfulQA":34.26,
        "Winogrande":66.3,
        "GSM8K":1.52,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"1f9e8d48163feb63ed190eaa982f393542a75d30"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Rallio67\/7B-redpajama-conditional-alpha",
        "Average":40.56,
        "ARC":42.58,
        "HellaSwag":69.91,
        "MMLU":26.53,
        "TruthfulQA":36.42,
        "Winogrande":67.17,
        "GSM8K":0.76,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"9a3f69a1eba3618930f222d4e013d534102a2af5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"digitous\/Javalion-R",
        "Average":40.51,
        "ARC":41.72,
        "HellaSwag":68.02,
        "MMLU":30.81,
        "TruthfulQA":34.44,
        "Winogrande":65.43,
        "GSM8K":2.65,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"b881231ab6ea85da2a9a139f282df85d1d18b002"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"h2oai\/h2ogpt-oasst1-512-12b",
        "Average":40.48,
        "ARC":42.32,
        "HellaSwag":70.24,
        "MMLU":26.01,
        "TruthfulQA":36.41,
        "Winogrande":66.22,
        "GSM8K":1.67,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.59,
        "Hub \u2764\ufe0f":26.0,
        "Available on the Hub":true,
        "Model Sha":"c6bb0fe363e0105839d34ca757793b61c9606f95"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"digitous\/Javelin-R",
        "Average":40.39,
        "ARC":41.64,
        "HellaSwag":69.01,
        "MMLU":30.7,
        "TruthfulQA":34.5,
        "Winogrande":64.8,
        "GSM8K":1.67,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"4c4a5caf5d9049a47f5565b72e5a53dede08ac8b"
    },
    {
        "T":"?",
        "Model":"dvruette\/oasst-pythia-12b-reference",
        "Average":40.33,
        "ARC":43.0,
        "HellaSwag":67.91,
        "MMLU":28.33,
        "TruthfulQA":36.57,
        "Winogrande":64.96,
        "GSM8K":1.21,
        "Type":"",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":11.58,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"c5a9b7fad884e6c45ce5d2ca551aa1c03db6865f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"WizardLM\/WizardCoder-Python-7B-V1.0",
        "Average":40.32,
        "ARC":41.81,
        "HellaSwag":65.06,
        "MMLU":32.29,
        "TruthfulQA":36.32,
        "Winogrande":61.72,
        "GSM8K":4.7,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":44.0,
        "Available on the Hub":true,
        "Model Sha":"e40673a27a4aefcff2c6d2b3b1e0681a38703e4e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Pirr\/pythia-13b-deduped-green_devil",
        "Average":40.31,
        "ARC":42.32,
        "HellaSwag":68.89,
        "MMLU":26.01,
        "TruthfulQA":35.56,
        "Winogrande":66.93,
        "GSM8K":2.12,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":11.58,
        "Hub \u2764\ufe0f":9.0,
        "Available on the Hub":true,
        "Model Sha":"7faeb395c26189eeab9bf3a98994696687ad31a3"
    },
    {
        "T":"\u2b55",
        "Model":"vihangd\/smartyplats-3b-v2",
        "Average":40.29,
        "ARC":41.04,
        "HellaSwag":71.19,
        "MMLU":24.32,
        "TruthfulQA":36.66,
        "Winogrande":66.93,
        "GSM8K":1.59,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"920609897049f674bc4a9678579f6869f6cbed13"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/openllama_3b_EvolInstruct_lora_merged",
        "Average":40.28,
        "ARC":40.27,
        "HellaSwag":71.6,
        "MMLU":27.12,
        "TruthfulQA":34.78,
        "Winogrande":67.01,
        "GSM8K":0.91,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-4.0",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"c55e3e114951346f273c519d266170e4d52781e9"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"openlm-research\/open_llama_3b_v2",
        "Average":40.28,
        "ARC":40.27,
        "HellaSwag":71.6,
        "MMLU":27.12,
        "TruthfulQA":34.78,
        "Winogrande":67.01,
        "GSM8K":0.91,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":61.0,
        "Available on the Hub":true,
        "Model Sha":"bce5d60d3b0c68318862270ec4e794d83308d80a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kfkas\/Llama-2-ko-7b-Chat",
        "Average":40.27,
        "ARC":40.44,
        "HellaSwag":67.16,
        "MMLU":30.4,
        "TruthfulQA":35.48,
        "Winogrande":66.85,
        "GSM8K":1.29,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.67,
        "Hub \u2764\ufe0f":51.0,
        "Available on the Hub":true,
        "Model Sha":"3293b98cd8204371988f898dafa9b5a297555cbe"
    },
    {
        "T":"\u2b55",
        "Model":"TheBloke\/CodeLlama-34B-Python-fp16",
        "Average":40.27,
        "ARC":38.14,
        "HellaSwag":34.8,
        "MMLU":32.95,
        "TruthfulQA":43.57,
        "Winogrande":72.14,
        "GSM8K":20.02,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":33.74,
        "Hub \u2764\ufe0f":11.0,
        "Available on the Hub":true,
        "Model Sha":"875f9d97fb6c9619d8867887dd1d80918ff0f593"
    },
    {
        "T":"\u2b55",
        "Model":"codellama\/CodeLlama-34b-Python-hf",
        "Average":40.27,
        "ARC":40.19,
        "HellaSwag":36.82,
        "MMLU":34.79,
        "TruthfulQA":44.28,
        "Winogrande":71.19,
        "GSM8K":14.33,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":33.74,
        "Hub \u2764\ufe0f":61.0,
        "Available on the Hub":true,
        "Model Sha":"3dd8ab05bbd273b9f77088b1d4015b7f1848793d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"l3utterfly\/open-llama-3b-v2-layla",
        "Average":40.25,
        "ARC":38.23,
        "HellaSwag":66.43,
        "MMLU":28.56,
        "TruthfulQA":44.4,
        "Winogrande":62.83,
        "GSM8K":1.06,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"465669ddafad25393ac3cfe94d3726cced112b30"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"kfkas\/Llama-2-ko-7b-Chat",
        "Average":40.25,
        "ARC":40.44,
        "HellaSwag":67.12,
        "MMLU":30.19,
        "TruthfulQA":35.45,
        "Winogrande":66.61,
        "GSM8K":1.67,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":6.67,
        "Hub \u2764\ufe0f":51.0,
        "Available on the Hub":true,
        "Model Sha":"3293b98cd8204371988f898dafa9b5a297555cbe"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"digitous\/Javelin-GPTJ",
        "Average":40.23,
        "ARC":42.66,
        "HellaSwag":70.45,
        "MMLU":26.2,
        "TruthfulQA":36.08,
        "Winogrande":64.17,
        "GSM8K":1.82,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"bee7068ab002784420a1a30170db3906185359f2"
    },
    {
        "T":"\u2b55",
        "Model":"llm-agents\/tora-code-7b-v1.0",
        "Average":40.21,
        "ARC":40.7,
        "HellaSwag":65.86,
        "MMLU":33.34,
        "TruthfulQA":34.84,
        "Winogrande":61.56,
        "GSM8K":4.93,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"777501b69bb0ba2675abdcaf7b1309ab05320c2e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"digitous\/Janin-R",
        "Average":40.19,
        "ARC":40.44,
        "HellaSwag":67.36,
        "MMLU":31.24,
        "TruthfulQA":34.49,
        "Winogrande":65.35,
        "GSM8K":2.27,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"f6963f77098d8421ff4a1cf4d36f1e94c6c8f44b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"acrastt\/Bean-3B",
        "Average":40.18,
        "ARC":40.36,
        "HellaSwag":72.0,
        "MMLU":26.43,
        "TruthfulQA":36.11,
        "Winogrande":65.67,
        "GSM8K":0.53,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"4a1ce189a3fb1d58b3fa47ebe30b3c037592670c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TaylorAI\/Flash-Llama-3B",
        "Average":40.13,
        "ARC":40.1,
        "HellaSwag":71.56,
        "MMLU":26.88,
        "TruthfulQA":34.74,
        "Winogrande":66.61,
        "GSM8K":0.91,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"b4c7bb49171ff6955cfc1f7e33143383c57f7606"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TehVenom\/Dolly_Shygmalion-6b-Dev_V8P2",
        "Average":40.11,
        "ARC":41.38,
        "HellaSwag":67.67,
        "MMLU":28.48,
        "TruthfulQA":36.86,
        "Winogrande":64.33,
        "GSM8K":1.97,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"6413b1d9e8b58df9d3aac91a862e8d505d8c6716"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/gpt-j-6b",
        "Average":40.1,
        "ARC":41.38,
        "HellaSwag":67.54,
        "MMLU":26.78,
        "TruthfulQA":35.96,
        "Winogrande":65.98,
        "GSM8K":2.96,
        "Type":"pretrained",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":1290.0,
        "Available on the Hub":true,
        "Model Sha":"47e169305d2e8376be1d31e765533382721b2cc1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Xilabs\/calypso-3b-alpha-v2",
        "Average":40.09,
        "ARC":41.55,
        "HellaSwag":71.48,
        "MMLU":25.82,
        "TruthfulQA":35.73,
        "Winogrande":65.27,
        "GSM8K":0.68,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"933fb9db10f131f7ea54f4e6024ed2acf41c711a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Danielbrdz\/CodeBarcenas-7b",
        "Average":40.09,
        "ARC":42.32,
        "HellaSwag":63.43,
        "MMLU":33.39,
        "TruthfulQA":38.51,
        "Winogrande":60.38,
        "GSM8K":2.5,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"fe7a232baac5394e821f349cb7ef31dbd4ca2078"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/CodeLlama-34b-hf",
        "Average":40.08,
        "ARC":37.54,
        "HellaSwag":31.84,
        "MMLU":37.2,
        "TruthfulQA":38.89,
        "Winogrande":73.4,
        "GSM8K":21.61,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"4e61ec70eb258047f5bc689fa6a66f7753da52b8"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"facebook\/opt-13b",
        "Average":40.06,
        "ARC":39.93,
        "HellaSwag":71.2,
        "MMLU":24.9,
        "TruthfulQA":34.1,
        "Winogrande":68.51,
        "GSM8K":1.74,
        "Type":"pretrained",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":59.0,
        "Available on the Hub":true,
        "Model Sha":"e515202d1e7750da62d245fbccb2723b9c1790f5"
    },
    {
        "T":"\u2b55",
        "Model":"codellama\/CodeLlama-7b-Instruct-hf",
        "Average":40.05,
        "ARC":36.52,
        "HellaSwag":55.44,
        "MMLU":34.54,
        "TruthfulQA":41.25,
        "Winogrande":64.56,
        "GSM8K":7.96,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":76.0,
        "Available on the Hub":true,
        "Model Sha":"7affc442e639b8aa1c4b3e98a10a2f45a21b8b4f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/GPT-J-6B-Skein",
        "Average":40.02,
        "ARC":42.58,
        "HellaSwag":68.69,
        "MMLU":24.88,
        "TruthfulQA":38.7,
        "Winogrande":63.85,
        "GSM8K":1.44,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":12.0,
        "Available on the Hub":true,
        "Model Sha":"acfe27303f74129930fef5e6fadbc5f58c6b8590"
    },
    {
        "T":"\u2b55",
        "Model":"vihangd\/smartyplats-3b-v1",
        "Average":40.0,
        "ARC":40.53,
        "HellaSwag":70.85,
        "MMLU":25.31,
        "TruthfulQA":36.53,
        "Winogrande":65.75,
        "GSM8K":1.06,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"89272b9edb323f5ace09e097a6449554c0dcd4e7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-tools-7b",
        "Average":40.0,
        "ARC":38.91,
        "HellaSwag":57.69,
        "MMLU":33.24,
        "TruthfulQA":44.08,
        "Winogrande":58.56,
        "GSM8K":7.51,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"81aefc8983d1192378c2c803f0e0d14d48561117"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Salesforce\/codegen-6B-nl",
        "Average":40.0,
        "ARC":42.32,
        "HellaSwag":68.59,
        "MMLU":25.93,
        "TruthfulQA":34.47,
        "Winogrande":66.46,
        "GSM8K":2.2,
        "Type":"pretrained",
        "Architecture":"CodeGenForCausalLM",
        "Precision":"float16",
        "Hub License":"bsd-3-clause",
        "#Params (B)":6.85,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"dff91c0aea702edbea3528344d01d8b9aaee6e39"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"digitous\/Javalion-GPTJ",
        "Average":39.97,
        "ARC":41.89,
        "HellaSwag":68.69,
        "MMLU":26.85,
        "TruthfulQA":35.44,
        "Winogrande":65.27,
        "GSM8K":1.67,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"3ce176bc0f91cae416c78e99f964f54b12472de0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aspik101\/WizardVicuna-Uncensored-3B-instruct-PL-lora_unload",
        "Average":39.95,
        "ARC":41.98,
        "HellaSwag":66.82,
        "MMLU":25.69,
        "TruthfulQA":39.67,
        "Winogrande":64.88,
        "GSM8K":0.68,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"e471ec778771f29992293d1660cc108f29c9c69e"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"RWKV\/rwkv-4-14b-pile",
        "Average":39.92,
        "ARC":44.45,
        "HellaSwag":71.07,
        "MMLU":26.12,
        "TruthfulQA":32.04,
        "Winogrande":65.43,
        "GSM8K":0.38,
        "Type":"pretrained",
        "Architecture":"RwkvForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.89,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"4effb0fa9d15c2f383a1d159f4a40df0e09eb6d5"
    },
    {
        "T":"?",
        "Model":"TheBloke\/WizardLM-30B-GPTQ",
        "Average":39.9,
        "ARC":28.84,
        "HellaSwag":26.08,
        "MMLU":24.62,
        "TruthfulQA":49.14,
        "Winogrande":76.32,
        "GSM8K":34.42,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":35.58,
        "Hub \u2764\ufe0f":19.0,
        "Available on the Hub":true,
        "Model Sha":"e2e97475a9775d2fe7afba098aee37e694b9220f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"h2oai\/h2ogpt-gm-oasst1-en-1024-open-llama-7b-preview-400bt",
        "Average":39.89,
        "ARC":41.3,
        "HellaSwag":62.44,
        "MMLU":27.55,
        "TruthfulQA":42.0,
        "Winogrande":64.56,
        "GSM8K":1.52,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"29604e6e19822531b0d49d3f19abef603a97d0ec"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TehVenom\/Dolly_Shygmalion-6b",
        "Average":39.89,
        "ARC":41.89,
        "HellaSwag":68.48,
        "MMLU":27.58,
        "TruthfulQA":33.91,
        "Winogrande":65.35,
        "GSM8K":2.12,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":14.0,
        "Available on the Hub":true,
        "Model Sha":"108fabf8a916900525492c294c50998d7c09f10b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"digitous\/Skegma-GPTJ",
        "Average":39.87,
        "ARC":43.77,
        "HellaSwag":69.22,
        "MMLU":25.37,
        "TruthfulQA":34.67,
        "Winogrande":64.64,
        "GSM8K":1.52,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"4dff006b2ea7e8d9b067dfe8af8ca1a16bc44dce"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TehVenom\/PPO_Shygmalion-V8p4_Dev-6b",
        "Average":39.85,
        "ARC":40.7,
        "HellaSwag":67.04,
        "MMLU":29.31,
        "TruthfulQA":35.57,
        "Winogrande":63.93,
        "GSM8K":2.58,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"fa3d503bca50c947e7a5bbde4bdd82f699f65c02"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TehVenom\/PPO_Pygway-V8p4_Dev-6b",
        "Average":39.85,
        "ARC":40.36,
        "HellaSwag":67.15,
        "MMLU":29.3,
        "TruthfulQA":35.26,
        "Winogrande":64.4,
        "GSM8K":2.65,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"f30709dba36c665869f9ac8cd0cef5a8a2e7c8df"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"togethercomputer\/Pythia-Chat-Base-7B",
        "Average":39.81,
        "ARC":40.02,
        "HellaSwag":68.67,
        "MMLU":27.44,
        "TruthfulQA":34.63,
        "Winogrande":64.01,
        "GSM8K":4.09,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":61.0,
        "Available on the Hub":true,
        "Model Sha":"97aa918c383820e1a69f042801091d7deb996c20"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NousResearch\/CodeLlama-7b-hf",
        "Average":39.81,
        "ARC":39.85,
        "HellaSwag":59.58,
        "MMLU":30.47,
        "TruthfulQA":38.62,
        "Winogrande":64.88,
        "GSM8K":5.46,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"855c92912ea4a8eb5f0be1db4bf776ffd0815dac"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"codellama\/CodeLlama-7b-hf",
        "Average":39.81,
        "ARC":39.93,
        "HellaSwag":60.8,
        "MMLU":31.12,
        "TruthfulQA":37.82,
        "Winogrande":64.01,
        "GSM8K":5.16,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":145.0,
        "Available on the Hub":true,
        "Model Sha":"be52f4ad322f5a47da121c761aeb5ba20ed77b17"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TehVenom\/Dolly_Malion-6b",
        "Average":39.77,
        "ARC":42.83,
        "HellaSwag":68.43,
        "MMLU":27.13,
        "TruthfulQA":33.03,
        "Winogrande":65.43,
        "GSM8K":1.74,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"f239eb8d24fe26db3b0a9a69115dc305fc9351af"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"heegyu\/WizardVicuna-Uncensored-3B-0719",
        "Average":39.73,
        "ARC":41.38,
        "HellaSwag":66.19,
        "MMLU":26.53,
        "TruthfulQA":39.35,
        "Winogrande":63.77,
        "GSM8K":1.14,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"36841c80535bc3e8403e3cc084e8e65884c75076"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TehVenom\/ChanMalion",
        "Average":39.73,
        "ARC":41.89,
        "HellaSwag":68.25,
        "MMLU":27.29,
        "TruthfulQA":33.89,
        "Winogrande":65.35,
        "GSM8K":1.67,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":9.0,
        "Available on the Hub":true,
        "Model Sha":"2667b0e0b705ed23f81f3e2b69673d722e8f4964"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/pythia-12b-deduped",
        "Average":39.7,
        "ARC":41.38,
        "HellaSwag":70.26,
        "MMLU":25.63,
        "TruthfulQA":33.0,
        "Winogrande":66.46,
        "GSM8K":1.44,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.59,
        "Hub \u2764\ufe0f":48.0,
        "Available on the Hub":true,
        "Model Sha":"39c1bd94f9dbe4ebd1d191f364cb33a2e5c47707"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"digitous\/Janin-GPTJ",
        "Average":39.67,
        "ARC":40.87,
        "HellaSwag":67.29,
        "MMLU":27.4,
        "TruthfulQA":36.25,
        "Winogrande":64.25,
        "GSM8K":1.97,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"a6773861798f2abea3849514aa6f60961518af9c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TehVenom\/GPT-J-Pyg_PPO-6B-Dev-V8p4",
        "Average":39.61,
        "ARC":40.19,
        "HellaSwag":66.43,
        "MMLU":30.39,
        "TruthfulQA":34.76,
        "Winogrande":64.01,
        "GSM8K":1.9,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"bigscience-openrail-m",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"930dc82245c607ce43558a0e6c0225e77b341ea6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/OPT-13B-Erebus",
        "Average":39.61,
        "ARC":40.02,
        "HellaSwag":70.07,
        "MMLU":25.32,
        "TruthfulQA":34.93,
        "Winogrande":66.54,
        "GSM8K":0.76,
        "Type":"fine-tuned",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":149.0,
        "Available on the Hub":true,
        "Model Sha":"8a949353677d2b971910a6c4afcc70e95d838c2a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/OPT-13B-Nerybus-Mix",
        "Average":39.61,
        "ARC":39.85,
        "HellaSwag":70.6,
        "MMLU":24.9,
        "TruthfulQA":34.02,
        "Winogrande":67.88,
        "GSM8K":0.38,
        "Type":"fine-tuned",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":25.0,
        "Available on the Hub":true,
        "Model Sha":"c27a7e2360dd313406719980851e89abf46ebb13"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/GPT-J-6B-Shinen",
        "Average":39.6,
        "ARC":39.85,
        "HellaSwag":67.06,
        "MMLU":27.72,
        "TruthfulQA":36.94,
        "Winogrande":64.09,
        "GSM8K":1.97,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":15.0,
        "Available on the Hub":true,
        "Model Sha":"afa5a11b24cb23eee708e17c83b920a788e9e07b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Corianas\/gpt-j-6B-Dolly",
        "Average":39.6,
        "ARC":41.3,
        "HellaSwag":65.97,
        "MMLU":26.78,
        "TruthfulQA":37.91,
        "Winogrande":64.72,
        "GSM8K":0.91,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"83d8c754aac12f838d7c847d4352a09396c383d0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TehVenom\/GPT-J-Pyg_PPO-6B",
        "Average":39.6,
        "ARC":42.06,
        "HellaSwag":67.51,
        "MMLU":28.52,
        "TruthfulQA":31.95,
        "Winogrande":64.72,
        "GSM8K":2.81,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"bigscience-openrail-m",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"cde5bab3ae16e1704c5fec54a6a7ff1169c935e6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/GPT-J-6B-Janeway",
        "Average":39.54,
        "ARC":40.87,
        "HellaSwag":67.11,
        "MMLU":27.45,
        "TruthfulQA":35.74,
        "Winogrande":64.72,
        "GSM8K":1.36,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":11.0,
        "Available on the Hub":true,
        "Model Sha":"036bb03496d648ddc8cf932ad91df8ef1287116c"
    },
    {
        "T":"?",
        "Model":"amazon\/LightGPT",
        "Average":39.54,
        "ARC":39.93,
        "HellaSwag":63.82,
        "MMLU":28.45,
        "TruthfulQA":36.69,
        "Winogrande":64.48,
        "GSM8K":3.87,
        "Type":"",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":64.0,
        "Available on the Hub":true,
        "Model Sha":"1f6ffd8f162030396a3bc1ca2e3504896dbe6434"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/OPT-13B-Nerys-v2",
        "Average":39.53,
        "ARC":39.68,
        "HellaSwag":70.53,
        "MMLU":25.36,
        "TruthfulQA":33.5,
        "Winogrande":67.88,
        "GSM8K":0.23,
        "Type":"fine-tuned",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":9.0,
        "Available on the Hub":true,
        "Model Sha":"b0aa4f3630356f7801ca083c00b03d03da13b8bb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"togethercomputer\/RedPajama-INCITE-Chat-3B-v1",
        "Average":39.53,
        "ARC":42.83,
        "HellaSwag":67.62,
        "MMLU":26.23,
        "TruthfulQA":34.44,
        "Winogrande":65.51,
        "GSM8K":0.53,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":115.0,
        "Available on the Hub":true,
        "Model Sha":"f0e0995eba801096ed04cb87931d96a8316871af"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"AI-Sweden-Models\/gpt-sw3-6.7b-v2",
        "Average":39.49,
        "ARC":39.42,
        "HellaSwag":66.39,
        "MMLU":30.09,
        "TruthfulQA":35.6,
        "Winogrande":64.25,
        "GSM8K":1.21,
        "Type":"pretrained",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"bfloat16",
        "Hub License":"other",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"7a7f93d4318658b354c5411cde64e9f0121f6b1f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"heegyu\/WizardVicuna-3B-0719",
        "Average":39.48,
        "ARC":40.7,
        "HellaSwag":65.45,
        "MMLU":25.44,
        "TruthfulQA":40.71,
        "Winogrande":63.85,
        "GSM8K":0.76,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"62d3d450b8ab2bd2fb9f82383b55d1ecae33a401"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"databricks\/dolly-v2-12b",
        "Average":39.46,
        "ARC":42.41,
        "HellaSwag":72.53,
        "MMLU":25.92,
        "TruthfulQA":33.83,
        "Winogrande":60.85,
        "GSM8K":1.21,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":11.58,
        "Hub \u2764\ufe0f":1872.0,
        "Available on the Hub":true,
        "Model Sha":"19308160448536e378e3db21a73a751579ee7fdd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/PPO_Pygway-6b-Mix",
        "Average":39.43,
        "ARC":41.81,
        "HellaSwag":67.77,
        "MMLU":28.42,
        "TruthfulQA":32.5,
        "Winogrande":64.4,
        "GSM8K":1.67,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":19.0,
        "Available on the Hub":true,
        "Model Sha":"b31d25819e00d5031ccdb22a9584f0850dcfe39c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Fredithefish\/RedPajama-INCITE-Chat-3B-Instruction-Tuning-with-GPT-4",
        "Average":39.38,
        "ARC":41.64,
        "HellaSwag":66.23,
        "MMLU":27.26,
        "TruthfulQA":36.1,
        "Winogrande":64.4,
        "GSM8K":0.68,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"cc",
        "#Params (B)":2.91,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"c588a5924749b86a6cb36a687dafa544c189bb6f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"togethercomputer\/RedPajama-INCITE-7B-Chat",
        "Average":39.37,
        "ARC":42.06,
        "HellaSwag":70.82,
        "MMLU":26.94,
        "TruthfulQA":36.09,
        "Winogrande":59.83,
        "GSM8K":0.45,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":87.0,
        "Available on the Hub":true,
        "Model Sha":"47b94a739e2f3164b438501c8684acc5d5acc146"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"togethercomputer\/RedPajama-INCITE-Chat-7B-v0.1",
        "Average":39.37,
        "ARC":42.06,
        "HellaSwag":70.82,
        "MMLU":26.94,
        "TruthfulQA":36.09,
        "Winogrande":59.83,
        "GSM8K":0.45,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":87.0,
        "Available on the Hub":true,
        "Model Sha":"47b94a739e2f3164b438501c8684acc5d5acc146"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Yukang\/LongAlpaca-7B",
        "Average":39.36,
        "ARC":42.66,
        "HellaSwag":65.89,
        "MMLU":27.28,
        "TruthfulQA":40.16,
        "Winogrande":60.14,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"bebfcb894b3f5170ce54e3bb98b6e565fae7b6c0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TehVenom\/PPO_Shygmalion-6b",
        "Average":39.35,
        "ARC":40.27,
        "HellaSwag":66.88,
        "MMLU":27.53,
        "TruthfulQA":34.24,
        "Winogrande":65.35,
        "GSM8K":1.82,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"573e4546fdccc5c8a52b9d7cb23a2e10f0f2ef51"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"digitous\/Adventien-GPTJ",
        "Average":39.31,
        "ARC":42.49,
        "HellaSwag":69.21,
        "MMLU":25.4,
        "TruthfulQA":36.95,
        "Winogrande":60.22,
        "GSM8K":1.59,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"4fbfe9eae03a1d6ecf60fda8cf39c4123f0438bd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"mosaicml\/mpt-7b-storywriter",
        "Average":39.31,
        "ARC":45.65,
        "HellaSwag":74.14,
        "MMLU":28.8,
        "TruthfulQA":36.12,
        "Winogrande":51.14,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"MPTForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":690.0,
        "Available on the Hub":true,
        "Model Sha":"a5e85ae1941e31bb705adbcafce9b0dfd6f3a48b"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/pythia-6.9b-deduped",
        "Average":39.3,
        "ARC":41.3,
        "HellaSwag":67.05,
        "MMLU":26.48,
        "TruthfulQA":35.19,
        "Winogrande":64.09,
        "GSM8K":1.67,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"372b1c08d9b5b0fc18ce86bbf294930e26e66ed5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/fairseq-dense-6.7B",
        "Average":39.26,
        "ARC":39.42,
        "HellaSwag":71.26,
        "MMLU":26.91,
        "TruthfulQA":32.73,
        "Winogrande":65.27,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"XGLMForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"d62d83b8eb7a6ba012a762752a5b5679add3b40c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"databricks\/dolly-v2-7b",
        "Average":39.24,
        "ARC":44.54,
        "HellaSwag":69.64,
        "MMLU":25.18,
        "TruthfulQA":34.88,
        "Winogrande":60.06,
        "GSM8K":1.14,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":132.0,
        "Available on the Hub":true,
        "Model Sha":"d632f0c8b75b1ae5b26b250d25bfba4e99cb7c6f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"acrastt\/RedPajama-INCITE-Chat-Instruct-3B-V1",
        "Average":39.23,
        "ARC":42.58,
        "HellaSwag":67.48,
        "MMLU":25.99,
        "TruthfulQA":33.62,
        "Winogrande":64.8,
        "GSM8K":0.91,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.78,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"e19eef572d57fc734bf3ea07c7d0098b3901ec9b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"heegyu\/RedTulu-Uncensored-3B-0719",
        "Average":39.19,
        "ARC":40.02,
        "HellaSwag":62.55,
        "MMLU":30.37,
        "TruthfulQA":37.59,
        "Winogrande":62.35,
        "GSM8K":2.27,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"c92bf022cddc3f57b4552ec3391df487295a2f87"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"bigscience\/bloom-7b1",
        "Average":39.18,
        "ARC":41.13,
        "HellaSwag":62.0,
        "MMLU":26.25,
        "TruthfulQA":38.9,
        "Winogrande":65.43,
        "GSM8K":1.36,
        "Type":"pretrained",
        "Architecture":"BloomForCausalLM",
        "Precision":"float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":7.07,
        "Hub \u2764\ufe0f":137.0,
        "Available on the Hub":true,
        "Model Sha":"e83e90ba86f87f74aa2731cdab25ccf33976bd66"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"DanielSc4\/RedPajama-INCITE-Chat-3B-v1-RL-LoRA-8bit-test1",
        "Average":39.16,
        "ARC":41.3,
        "HellaSwag":66.82,
        "MMLU":26.1,
        "TruthfulQA":35.04,
        "Winogrande":65.43,
        "GSM8K":0.3,
        "Type":"RL-tuned",
        "Architecture":"?",
        "Precision":"8bit",
        "Hub License":"apache-2.0",
        "#Params (B)":3.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"a2ee88a9fa1c9ad41e0a8c15217a4b1230ec33c8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dvruette\/oasst-pythia-6.9b-4000-steps",
        "Average":39.15,
        "ARC":41.64,
        "HellaSwag":64.24,
        "MMLU":26.26,
        "TruthfulQA":40.43,
        "Winogrande":61.8,
        "GSM8K":0.53,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"0e201b6f344ac6382dda40d389e1c9144a87d027"
    },
    {
        "T":"\u2b55",
        "Model":"matsuo-lab\/weblab-10b-instruction-sft",
        "Average":39.13,
        "ARC":40.1,
        "HellaSwag":65.3,
        "MMLU":26.66,
        "TruthfulQA":36.79,
        "Winogrande":64.09,
        "GSM8K":1.82,
        "Type":"instruction-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.47,
        "Hub \u2764\ufe0f":68.0,
        "Available on the Hub":true,
        "Model Sha":"112a5ad9f556078ab14a5cd93511b9db4a0d4413"
    },
    {
        "T":"?",
        "Model":"TheBloke\/robin-33B-v2-GPTQ",
        "Average":39.1,
        "ARC":27.73,
        "HellaSwag":26.29,
        "MMLU":23.53,
        "TruthfulQA":49.54,
        "Winogrande":79.79,
        "GSM8K":27.75,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":35.58,
        "Hub \u2764\ufe0f":13.0,
        "Available on the Hub":true,
        "Model Sha":"4c2588d65302e9ca634548ed81e8650fb2975686"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/OPT-6.7B-Erebus",
        "Average":39.09,
        "ARC":39.16,
        "HellaSwag":68.66,
        "MMLU":24.58,
        "TruthfulQA":35.12,
        "Winogrande":65.98,
        "GSM8K":1.06,
        "Type":"fine-tuned",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.66,
        "Hub \u2764\ufe0f":84.0,
        "Available on the Hub":true,
        "Model Sha":"9c4d1af96f93224e01d2f69c303fc6d6f686bdcc"
    },
    {
        "T":"?",
        "Model":"YeungNLP\/firefly-bloom-7b1",
        "Average":39.09,
        "ARC":40.44,
        "HellaSwag":61.2,
        "MMLU":26.83,
        "TruthfulQA":40.83,
        "Winogrande":64.56,
        "GSM8K":0.68,
        "Type":"",
        "Architecture":"BloomForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":7.07,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"6b4385dc45c47d509b6400c41a2ff3665ad1d189"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"facebook\/opt-6.7b",
        "Average":39.08,
        "ARC":39.16,
        "HellaSwag":68.66,
        "MMLU":24.57,
        "TruthfulQA":35.12,
        "Winogrande":65.98,
        "GSM8K":0.99,
        "Type":"pretrained",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.66,
        "Hub \u2764\ufe0f":77.0,
        "Available on the Hub":true,
        "Model Sha":"a45aa65bbeb77c1558bc99bedc6779195462dab0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"togethercomputer\/RedPajama-INCITE-Instruct-3B-v1",
        "Average":39.06,
        "ARC":41.55,
        "HellaSwag":65.48,
        "MMLU":25.03,
        "TruthfulQA":36.41,
        "Winogrande":64.48,
        "GSM8K":1.36,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":84.0,
        "Available on the Hub":true,
        "Model Sha":"0c66778ee09a036886741707733620b91057909a"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/deacon-3b",
        "Average":39.05,
        "ARC":39.68,
        "HellaSwag":66.42,
        "MMLU":27.13,
        "TruthfulQA":36.07,
        "Winogrande":64.64,
        "GSM8K":0.38,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"c96b846ce7bacf5ad231957630dc94d59f329339"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Fredithefish\/ScarletPajama-3B-HF",
        "Average":39.04,
        "ARC":39.76,
        "HellaSwag":64.89,
        "MMLU":27.28,
        "TruthfulQA":37.6,
        "Winogrande":64.48,
        "GSM8K":0.23,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":9.0,
        "Available on the Hub":true,
        "Model Sha":"9dd07308b6eb3f270c5762250b6d46abd6f87b6f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psmathur\/orca_mini_3b",
        "Average":39.03,
        "ARC":41.55,
        "HellaSwag":61.52,
        "MMLU":26.79,
        "TruthfulQA":42.42,
        "Winogrande":61.8,
        "GSM8K":0.08,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":129.0,
        "Available on the Hub":true,
        "Model Sha":"fd2754e80ce80757a3a68a840d7d287dd7def676"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/black_goo_recipe_c",
        "Average":39.01,
        "ARC":38.74,
        "HellaSwag":66.83,
        "MMLU":26.57,
        "TruthfulQA":36.54,
        "Winogrande":64.72,
        "GSM8K":0.68,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"78c0a6432ac0a6c2e54a2c3aac4cb70f446eb18b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Fredithefish\/Guanaco-3B-Uncensored-v2",
        "Average":38.98,
        "ARC":42.15,
        "HellaSwag":66.72,
        "MMLU":26.18,
        "TruthfulQA":35.21,
        "Winogrande":63.3,
        "GSM8K":0.3,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.78,
        "Hub \u2764\ufe0f":9.0,
        "Available on the Hub":true,
        "Model Sha":"e07122091fd4b318dcea105b16c73144d95bc2f6"
    },
    {
        "T":"\u2b55",
        "Model":"jb723\/cross_lingual_epoch2",
        "Average":38.97,
        "ARC":39.25,
        "HellaSwag":47.92,
        "MMLU":36.66,
        "TruthfulQA":47.9,
        "Winogrande":62.12,
        "GSM8K":0.0,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"86e59e85b234e6c882758724849d7a1e4fe0b30a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Guanaco-3B-Uncensored-v2-GPTQ",
        "Average":38.95,
        "ARC":41.64,
        "HellaSwag":64.76,
        "MMLU":26.25,
        "TruthfulQA":36.58,
        "Winogrande":64.33,
        "GSM8K":0.15,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"GPTQ",
        "Hub License":"apache-2.0",
        "#Params (B)":4.78,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"c80e2f01377d551ad17c8c9bac3f52578c38d653"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Fredithefish\/Guanaco-3B-Uncensored",
        "Average":38.94,
        "ARC":42.49,
        "HellaSwag":66.99,
        "MMLU":25.55,
        "TruthfulQA":34.71,
        "Winogrande":63.38,
        "GSM8K":0.53,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.78,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"084a12f767b31c1fde681bebb14e9a291e506ea8"
    },
    {
        "T":"\u2b55",
        "Model":"health360\/Healix-3B",
        "Average":38.93,
        "ARC":37.71,
        "HellaSwag":65.94,
        "MMLU":26.02,
        "TruthfulQA":37.4,
        "Winogrande":65.75,
        "GSM8K":0.76,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"52297e0b6845b3c1b26f336fd2a2c9b2f56ce6ba"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"CobraMamba\/mamba-gpt-3b",
        "Average":38.87,
        "ARC":40.53,
        "HellaSwag":64.94,
        "MMLU":25.35,
        "TruthfulQA":37.14,
        "Winogrande":65.04,
        "GSM8K":0.23,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"21a8212e3641dd14924d6bdead0774b64dda8ce0"
    },
    {
        "T":"?",
        "Model":"OpenAssistant\/galactica-6.7b-finetuned",
        "Average":38.84,
        "ARC":41.55,
        "HellaSwag":51.01,
        "MMLU":38.03,
        "TruthfulQA":41.65,
        "Winogrande":57.7,
        "GSM8K":3.11,
        "Type":"",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.66,
        "Hub \u2764\ufe0f":34.0,
        "Available on the Hub":true,
        "Model Sha":"d86db70e16111175ff7900f71d40806ccf4b8491"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"frank098\/orca_mini_3b_juniper",
        "Average":38.83,
        "ARC":40.87,
        "HellaSwag":61.73,
        "MMLU":26.37,
        "TruthfulQA":43.19,
        "Winogrande":60.3,
        "GSM8K":0.53,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"c08749034baa053834f1b709b6e7b88b914cd1fb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/OPT-6.7B-Nerybus-Mix",
        "Average":38.83,
        "ARC":39.16,
        "HellaSwag":68.63,
        "MMLU":24.47,
        "TruthfulQA":34.84,
        "Winogrande":65.11,
        "GSM8K":0.76,
        "Type":"fine-tuned",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.66,
        "Hub \u2764\ufe0f":18.0,
        "Available on the Hub":true,
        "Model Sha":"9afe4dca5a9dbd71cb90d1050d142837f4c739f6"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/pythia-12b",
        "Average":38.82,
        "ARC":39.59,
        "HellaSwag":68.82,
        "MMLU":26.76,
        "TruthfulQA":31.85,
        "Winogrande":64.17,
        "GSM8K":1.74,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.59,
        "Hub \u2764\ufe0f":111.0,
        "Available on the Hub":true,
        "Model Sha":"35c9d7f32fbb108fb8b5bdd574eb03369d1eed49"
    },
    {
        "T":"\u2b55",
        "Model":"heegyu\/WizardVicuna-open-llama-3b-v2",
        "Average":38.77,
        "ARC":37.71,
        "HellaSwag":66.6,
        "MMLU":27.23,
        "TruthfulQA":36.8,
        "Winogrande":63.3,
        "GSM8K":0.99,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"1c69905286171d7d3ef3f95f8e1bbc9150bad3cd"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/black_goo_recipe_a",
        "Average":38.73,
        "ARC":38.14,
        "HellaSwag":66.56,
        "MMLU":25.75,
        "TruthfulQA":37.46,
        "Winogrande":63.93,
        "GSM8K":0.53,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"7067f68d4d9e7b10a1aa2c9fa97456bc04678867"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/OPT-6B-nerys-v2",
        "Average":38.72,
        "ARC":38.4,
        "HellaSwag":68.57,
        "MMLU":24.34,
        "TruthfulQA":34.73,
        "Winogrande":65.59,
        "GSM8K":0.68,
        "Type":"fine-tuned",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.66,
        "Hub \u2764\ufe0f":21.0,
        "Available on the Hub":true,
        "Model Sha":"9e1f1498391df2c28ce35a9290a5a24b8022a43b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hakurei\/instruct-12b",
        "Average":38.63,
        "ARC":42.58,
        "HellaSwag":66.76,
        "MMLU":26.79,
        "TruthfulQA":31.96,
        "Winogrande":63.46,
        "GSM8K":0.23,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":11.58,
        "Hub \u2764\ufe0f":17.0,
        "Available on the Hub":true,
        "Model Sha":"ff4699b502b79c716330b6f761002588a65dcba6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"h2oai\/h2ogpt-oig-oasst1-256-6_9b",
        "Average":38.62,
        "ARC":39.93,
        "HellaSwag":65.42,
        "MMLU":26.39,
        "TruthfulQA":35.0,
        "Winogrande":63.38,
        "GSM8K":1.59,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"f1c9bac89b74d3487cb092788ce828fb9520c1a7"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"matsuo-lab\/weblab-10b",
        "Average":38.59,
        "ARC":39.51,
        "HellaSwag":65.76,
        "MMLU":26.29,
        "TruthfulQA":36.02,
        "Winogrande":62.51,
        "GSM8K":1.44,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":10.47,
        "Hub \u2764\ufe0f":55.0,
        "Available on the Hub":true,
        "Model Sha":"d6fc432983b1633a4c1568d121c60de6b8c3e511"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/black_goo_recipe_d",
        "Average":38.57,
        "ARC":37.8,
        "HellaSwag":66.5,
        "MMLU":26.64,
        "TruthfulQA":36.46,
        "Winogrande":63.61,
        "GSM8K":0.38,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"fdf7f93837808958f9463d3c683314e7f649a088"
    },
    {
        "T":"\u2b55",
        "Model":"RWKV\/rwkv-raven-7b",
        "Average":38.55,
        "ARC":39.42,
        "HellaSwag":66.48,
        "MMLU":23.64,
        "TruthfulQA":38.56,
        "Winogrande":62.9,
        "GSM8K":0.3,
        "Type":"instruction-tuned",
        "Architecture":"RwkvForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":7.19,
        "Hub \u2764\ufe0f":16.0,
        "Available on the Hub":true,
        "Model Sha":"a2dfc9f659be13556a25d9e38da642c6f67aeee3"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"togethercomputer\/RedPajama-INCITE-Base-3B-v1",
        "Average":38.54,
        "ARC":40.19,
        "HellaSwag":64.77,
        "MMLU":27.03,
        "TruthfulQA":33.23,
        "Winogrande":64.72,
        "GSM8K":1.29,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":80.0,
        "Available on the Hub":true,
        "Model Sha":"094fbdd0c911feb485ce55de1952ab2e75277e1e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Lazycuber\/pyg-instruct-wizardlm",
        "Average":38.54,
        "ARC":40.96,
        "HellaSwag":66.71,
        "MMLU":26.33,
        "TruthfulQA":31.93,
        "Winogrande":63.69,
        "GSM8K":1.59,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"f00ef7a7b0cc6f02af2a11ac764270dfd61b9e2f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/OPT-30B-Erebus",
        "Average":38.53,
        "ARC":36.69,
        "HellaSwag":65.6,
        "MMLU":24.8,
        "TruthfulQA":38.76,
        "Winogrande":65.11,
        "GSM8K":0.23,
        "Type":"fine-tuned",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":29.97,
        "Hub \u2764\ufe0f":40.0,
        "Available on the Hub":true,
        "Model Sha":"a1041efcf9599c962822274e92040710579a5bf2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Fredithefish\/CrimsonPajama",
        "Average":38.52,
        "ARC":40.19,
        "HellaSwag":65.47,
        "MMLU":25.95,
        "TruthfulQA":33.78,
        "Winogrande":65.19,
        "GSM8K":0.53,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"ff054eeff9e3541464383d40b36d182057d01113"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"h2oai\/h2ogpt-oig-oasst1-512-6_9b",
        "Average":38.52,
        "ARC":40.44,
        "HellaSwag":65.58,
        "MMLU":24.9,
        "TruthfulQA":36.68,
        "Winogrande":62.51,
        "GSM8K":0.99,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":16.0,
        "Available on the Hub":true,
        "Model Sha":"029a787e0d98fcd3fecffbfbeb4a75a425474937"
    },
    {
        "T":"?",
        "Model":"TheBloke\/guanaco-33B-GPTQ",
        "Average":38.51,
        "ARC":28.16,
        "HellaSwag":26.34,
        "MMLU":24.94,
        "TruthfulQA":48.98,
        "Winogrande":78.85,
        "GSM8K":23.81,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":35.58,
        "Hub \u2764\ufe0f":71.0,
        "Available on the Hub":true,
        "Model Sha":"8e42e031bfc8be3bbf31dc546d7c51fb991ff6e0"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/LLongMA-3b-LIMA",
        "Average":38.51,
        "ARC":39.08,
        "HellaSwag":67.15,
        "MMLU":26.43,
        "TruthfulQA":34.71,
        "Winogrande":63.38,
        "GSM8K":0.3,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"333b8c41e42a46a6f3aecaf8f3fa8a17c6d83990"
    },
    {
        "T":"?",
        "Model":"pszemraj\/pythia-6.9b-HC3",
        "Average":38.51,
        "ARC":36.52,
        "HellaSwag":61.76,
        "MMLU":26.94,
        "TruthfulQA":45.05,
        "Winogrande":60.77,
        "GSM8K":0.0,
        "Type":"",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"c5c60ea656e921e6c5415f6feaebac4dd9b2aa2a"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/black_goo_recipe_b",
        "Average":38.49,
        "ARC":37.63,
        "HellaSwag":66.72,
        "MMLU":25.68,
        "TruthfulQA":37.09,
        "Winogrande":63.77,
        "GSM8K":0.08,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"42faec8429cee8c9f4f5db58ffa193f6f8e0d498"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Fredithefish\/RedPajama-INCITE-Chat-3B-ShareGPT-11K",
        "Average":38.47,
        "ARC":40.61,
        "HellaSwag":64.84,
        "MMLU":26.13,
        "TruthfulQA":35.41,
        "Winogrande":63.54,
        "GSM8K":0.3,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"ec33d12d08d61ed821e67b1a55ad404dc3457ebf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PygmalionAI\/pygmalion-6b",
        "Average":38.47,
        "ARC":40.53,
        "HellaSwag":67.47,
        "MMLU":25.73,
        "TruthfulQA":32.53,
        "Winogrande":62.51,
        "GSM8K":2.05,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":690.0,
        "Available on the Hub":true,
        "Model Sha":"30e2405100eac6bd53f75964cc7345eeafd19f7d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/WizardLM-33B-V1.0-Uncensored-GPTQ",
        "Average":38.43,
        "ARC":27.39,
        "HellaSwag":26.03,
        "MMLU":25.81,
        "TruthfulQA":48.9,
        "Winogrande":77.9,
        "GSM8K":24.56,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":35.58,
        "Hub \u2764\ufe0f":37.0,
        "Available on the Hub":true,
        "Model Sha":"1c65902c620fcdf6b9c8e36ce17f21360e186a1e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"anhnv125\/pygmalion-6b-roleplay",
        "Average":38.34,
        "ARC":40.53,
        "HellaSwag":67.47,
        "MMLU":25.73,
        "TruthfulQA":32.53,
        "Winogrande":62.67,
        "GSM8K":1.14,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"e49ed0bde45de0a436bff678ec4872069e8f230c"
    },
    {
        "T":"?",
        "Model":"TehVenom\/DiffMerge_Pygmalion_Main-onto-V8P4",
        "Average":38.31,
        "ARC":40.53,
        "HellaSwag":67.48,
        "MMLU":25.68,
        "TruthfulQA":32.55,
        "Winogrande":62.51,
        "GSM8K":1.14,
        "Type":"",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"f855780745aa34c3bdbe020e4c51253d538cb21e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"acrastt\/OmegLLaMA-3B",
        "Average":38.28,
        "ARC":40.36,
        "HellaSwag":66.13,
        "MMLU":28.0,
        "TruthfulQA":33.31,
        "Winogrande":61.64,
        "GSM8K":0.23,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"520c5f1ceb5c90d4011887e2a8d3becf15e7e66e"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"openlm-research\/open_llama_3b",
        "Average":38.26,
        "ARC":39.85,
        "HellaSwag":62.65,
        "MMLU":26.94,
        "TruthfulQA":34.97,
        "Winogrande":64.72,
        "GSM8K":0.45,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.32,
        "Hub \u2764\ufe0f":112.0,
        "Available on the Hub":true,
        "Model Sha":"141067009124b9c0aea62c76b3eb952174864057"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ewof\/koishi-instruct-3b",
        "Average":38.16,
        "ARC":40.96,
        "HellaSwag":64.54,
        "MMLU":26.58,
        "TruthfulQA":31.65,
        "Winogrande":64.09,
        "GSM8K":1.14,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":2.91,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"2bb7f3842398b048efa4ae2d1aafb9e2f18a8586"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/pythia-6.7b",
        "Average":38.06,
        "ARC":40.1,
        "HellaSwag":65.0,
        "MMLU":24.64,
        "TruthfulQA":32.85,
        "Winogrande":64.72,
        "GSM8K":1.06,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"b666a6e46eeade607c73ed1334ecda3b9345e4bf"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"RWKV\/rwkv-4-7b-pile",
        "Average":37.95,
        "ARC":39.68,
        "HellaSwag":66.31,
        "MMLU":24.96,
        "TruthfulQA":33.65,
        "Winogrande":62.35,
        "GSM8K":0.76,
        "Type":"pretrained",
        "Architecture":"RwkvForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":7.19,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"922e22a761427e50d7be457b31a76b1126021b8b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KnutJaegersberg\/Galactica-6.7B-EssayWriter",
        "Average":37.75,
        "ARC":40.1,
        "HellaSwag":50.29,
        "MMLU":33.88,
        "TruthfulQA":40.27,
        "Winogrande":58.48,
        "GSM8K":3.49,
        "Type":"fine-tuned",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.66,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"ac74fdd938de1ffd34832d66a25db20b0230983e"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Devio\/test-22B",
        "Average":37.71,
        "ARC":39.42,
        "HellaSwag":64.51,
        "MMLU":27.13,
        "TruthfulQA":37.13,
        "Winogrande":57.7,
        "GSM8K":0.38,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":21.83,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"cd72f5954ab5801dd2c1b499e59265f7504f9ee6"
    },
    {
        "T":"\u2b55",
        "Model":"ericzzz\/falcon-rw-1b-instruct-openorca",
        "Average":37.63,
        "ARC":34.56,
        "HellaSwag":60.93,
        "MMLU":28.77,
        "TruthfulQA":37.42,
        "Winogrande":60.69,
        "GSM8K":3.41,
        "Type":"instruction-tuned",
        "Architecture":"FalconForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"bb5f86170d8d01aa850bb216bb2797899570c13e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"euclaise\/falcon_1b_stage2",
        "Average":37.59,
        "ARC":35.49,
        "HellaSwag":65.56,
        "MMLU":23.83,
        "TruthfulQA":38.32,
        "Winogrande":62.35,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"FalconForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"c3ef73a8c9dc06fae4bfe4460d2f293147aecbb0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ikala\/bloom-zh-3b-chat",
        "Average":37.58,
        "ARC":38.82,
        "HellaSwag":54.71,
        "MMLU":31.62,
        "TruthfulQA":41.25,
        "Winogrande":58.64,
        "GSM8K":0.45,
        "Type":"fine-tuned",
        "Architecture":"BloomForCausalLM",
        "Precision":"float16",
        "Hub License":"bigscience-openrail-m",
        "#Params (B)":3.0,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"4ea0ad223a2623fc15e8824c1c4f8e6539bc40b0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"h2oai\/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt-v2",
        "Average":37.55,
        "ARC":36.43,
        "HellaSwag":61.41,
        "MMLU":25.01,
        "TruthfulQA":37.59,
        "Winogrande":64.64,
        "GSM8K":0.23,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"fdc6ff469295d0aaabec8948525b70d6688728ac"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/CodeLlama-13B-Python-fp16",
        "Average":37.52,
        "ARC":33.19,
        "HellaSwag":44.5,
        "MMLU":25.94,
        "TruthfulQA":43.99,
        "Winogrande":67.4,
        "GSM8K":10.08,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":26.0,
        "Available on the Hub":true,
        "Model Sha":"442282f4207442b828953a72c51a919c332cba5c"
    },
    {
        "T":"\u2b55",
        "Model":"HiTZ\/GoLLIE-7B",
        "Average":37.48,
        "ARC":36.09,
        "HellaSwag":57.93,
        "MMLU":29.38,
        "TruthfulQA":39.27,
        "Winogrande":58.96,
        "GSM8K":3.26,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":11.0,
        "Available on the Hub":true,
        "Model Sha":"d3e41fef45f6a7d438c46ba7d9fce5d0d486c7a9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/fairseq-dense-2.7B",
        "Average":37.41,
        "ARC":33.79,
        "HellaSwag":65.74,
        "MMLU":26.44,
        "TruthfulQA":34.57,
        "Winogrande":63.93,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"XGLMForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":2.78,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"4201f4b101bad2992efc8452009317a354ec52d2"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"cerebras\/Cerebras-GPT-13B",
        "Average":37.4,
        "ARC":38.14,
        "HellaSwag":60.01,
        "MMLU":25.92,
        "TruthfulQA":39.19,
        "Winogrande":59.83,
        "GSM8K":1.29,
        "Type":"pretrained",
        "Architecture":"GPT2Model",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":632.0,
        "Available on the Hub":true,
        "Model Sha":"7e97fa4b15edd955094c4395d62e6f4290e365b5"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Dampish\/StellarX-4B-V0",
        "Average":37.31,
        "ARC":36.95,
        "HellaSwag":61.9,
        "MMLU":26.85,
        "TruthfulQA":34.3,
        "Winogrande":63.85,
        "GSM8K":0.0,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":3.83,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"0a79832bd57a8cdadc61626fb77bdc26c85b9fa4"
    },
    {
        "T":"?",
        "Model":"TheBloke\/WizardLM-30B-Uncensored-GPTQ",
        "Average":37.27,
        "ARC":29.44,
        "HellaSwag":26.47,
        "MMLU":24.35,
        "TruthfulQA":49.15,
        "Winogrande":73.16,
        "GSM8K":21.08,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":35.58,
        "Hub \u2764\ufe0f":107.0,
        "Available on the Hub":true,
        "Model Sha":"43c701ddbe0bceac26c860307e06763cc5203500"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"DanielSc4\/RedPajama-INCITE-Chat-3B-v1-FT-LoRA-8bit-test1",
        "Average":37.27,
        "ARC":38.65,
        "HellaSwag":63.53,
        "MMLU":25.16,
        "TruthfulQA":36.07,
        "Winogrande":60.14,
        "GSM8K":0.08,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"8bit",
        "Hub License":"apache-2.0",
        "#Params (B)":3.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"f477d24b00e05fe4c5f8d5f933080994cfd90e4e"
    },
    {
        "T":"?",
        "Model":"GeorgiaTechResearchInstitute\/galactica-6.7b-evol-instruct-70k",
        "Average":37.27,
        "ARC":42.58,
        "HellaSwag":49.3,
        "MMLU":32.96,
        "TruthfulQA":42.1,
        "Winogrande":56.27,
        "GSM8K":0.38,
        "Type":"",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":6.66,
        "Hub \u2764\ufe0f":16.0,
        "Available on the Hub":true,
        "Model Sha":"14fa470051d0bc38fd871643186a9edfd3a8a9aa"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"euclaise\/falcon_1b_stage1",
        "Average":37.25,
        "ARC":35.15,
        "HellaSwag":62.4,
        "MMLU":24.47,
        "TruthfulQA":40.0,
        "Winogrande":61.48,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"FalconForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f85d91ff3f6cadc93f7222a19b9c4930c8842366"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"AI-Sweden-Models\/gpt-sw3-6.7b",
        "Average":37.23,
        "ARC":36.35,
        "HellaSwag":60.75,
        "MMLU":26.0,
        "TruthfulQA":39.04,
        "Winogrande":60.69,
        "GSM8K":0.53,
        "Type":"pretrained",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"bfloat16",
        "Hub License":"other",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"7b20cb87e793e1b73b6a73da5261c6010f2b5410"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/pythia-2.7b",
        "Average":37.09,
        "ARC":37.37,
        "HellaSwag":60.74,
        "MMLU":25.86,
        "TruthfulQA":35.4,
        "Winogrande":62.12,
        "GSM8K":1.06,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.91,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"b9d8cace80b1a97f5ed380711aea31f2d1b24310"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"tiiuae\/falcon-rw-1b",
        "Average":37.07,
        "ARC":35.07,
        "HellaSwag":63.56,
        "MMLU":25.28,
        "TruthfulQA":35.96,
        "Winogrande":62.04,
        "GSM8K":0.53,
        "Type":"pretrained",
        "Architecture":"FalconForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub \u2764\ufe0f":58.0,
        "Available on the Hub":true,
        "Model Sha":"e4b9872bb803165eb22f0a867d4e6a64d34fce19"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Phind\/Phind-CodeLlama-34B-v1",
        "Average":37.06,
        "ARC":27.13,
        "HellaSwag":28.28,
        "MMLU":28.94,
        "TruthfulQA":44.94,
        "Winogrande":72.61,
        "GSM8K":20.47,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":317.0,
        "Available on the Hub":true,
        "Model Sha":"b073c9bb418ae52ca76b4ab48ac2dfbc8622f434"
    },
    {
        "T":"?",
        "Model":"bigscience\/bloomz-3b",
        "Average":37.03,
        "ARC":36.86,
        "HellaSwag":54.95,
        "MMLU":32.91,
        "TruthfulQA":40.34,
        "Winogrande":57.14,
        "GSM8K":0.0,
        "Type":"",
        "Architecture":"BloomForCausalLM",
        "Precision":"float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":3.0,
        "Hub \u2764\ufe0f":70.0,
        "Available on the Hub":true,
        "Model Sha":"31eefcb2bcd69632925adf07e090debafe95436d"
    },
    {
        "T":"\u2b55",
        "Model":"codellama\/CodeLlama-13b-Python-hf",
        "Average":37.0,
        "ARC":32.59,
        "HellaSwag":43.94,
        "MMLU":27.23,
        "TruthfulQA":44.59,
        "Winogrande":65.04,
        "GSM8K":8.64,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":22.0,
        "Available on the Hub":true,
        "Model Sha":"ea1b775799b477fe22e64f8ac9107f28950b5c87"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/OPT-2.7B-Erebus",
        "Average":36.96,
        "ARC":34.39,
        "HellaSwag":60.91,
        "MMLU":26.7,
        "TruthfulQA":37.82,
        "Winogrande":61.64,
        "GSM8K":0.3,
        "Type":"fine-tuned",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":32.0,
        "Available on the Hub":true,
        "Model Sha":"39ca914ceb82f7f14a38484023bc04f0cd5d0a8d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cmarkea\/bloomz-3b-sft-chat",
        "Average":36.94,
        "ARC":36.86,
        "HellaSwag":54.34,
        "MMLU":31.49,
        "TruthfulQA":39.69,
        "Winogrande":58.88,
        "GSM8K":0.38,
        "Type":"fine-tuned",
        "Architecture":"BloomForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":3.0,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"a35b6ae6809891e253b45fb5795979c33992e548"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Azure99\/blossom-v1-3b",
        "Average":36.9,
        "ARC":36.86,
        "HellaSwag":55.1,
        "MMLU":26.7,
        "TruthfulQA":43.45,
        "Winogrande":58.88,
        "GSM8K":0.38,
        "Type":"fine-tuned",
        "Architecture":"BloomForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"3235ee41e3793c98749b7bbd2bb80882a12ac889"
    },
    {
        "T":"\u2b55",
        "Model":"Phind\/Phind-CodeLlama-34B-v2",
        "Average":36.89,
        "ARC":24.57,
        "HellaSwag":27.6,
        "MMLU":25.76,
        "TruthfulQA":48.37,
        "Winogrande":71.82,
        "GSM8K":23.2,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":365.0,
        "Available on the Hub":true,
        "Model Sha":"949f61e203f91b412efe8f679c798f09f0ff4b0c"
    },
    {
        "T":"\u2b55",
        "Model":"codellama\/CodeLlama-7b-Python-hf",
        "Average":36.89,
        "ARC":31.31,
        "HellaSwag":52.86,
        "MMLU":27.32,
        "TruthfulQA":42.21,
        "Winogrande":63.06,
        "GSM8K":4.55,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"4bit",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":54.0,
        "Available on the Hub":true,
        "Model Sha":"ec4dd26f30674fdee00ef161b55f464ce28f9c20"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Rallio67\/3B-redpajama-conditional-alpha",
        "Average":36.88,
        "ARC":36.26,
        "HellaSwag":61.9,
        "MMLU":25.42,
        "TruthfulQA":36.31,
        "Winogrande":60.77,
        "GSM8K":0.61,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"7e2156c14b4b7981a4cd6db7b878888a98144df0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"euclaise\/falcon_1b_stage2",
        "Average":36.88,
        "ARC":33.11,
        "HellaSwag":63.19,
        "MMLU":24.22,
        "TruthfulQA":38.4,
        "Winogrande":62.35,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"FalconForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"025c77e9ee457c6771c5a36dbacd064c269642a5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/OPT-2.7B-Nerybus-Mix",
        "Average":36.88,
        "ARC":33.7,
        "HellaSwag":61.21,
        "MMLU":26.6,
        "TruthfulQA":37.57,
        "Winogrande":62.04,
        "GSM8K":0.15,
        "Type":"fine-tuned",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":9.0,
        "Available on the Hub":true,
        "Model Sha":"b4131723cfff1fa42f6cbab546c5b4bb0d19fd83"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenBuddy\/openbuddy-openllama-3b-v10-bf16",
        "Average":36.87,
        "ARC":36.26,
        "HellaSwag":58.38,
        "MMLU":23.89,
        "TruthfulQA":42.04,
        "Winogrande":59.67,
        "GSM8K":0.99,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.34,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"7f24d32de53aa4bc150f04ca2418604475173921"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Writer\/camel-5b-hf",
        "Average":36.81,
        "ARC":35.15,
        "HellaSwag":57.62,
        "MMLU":26.07,
        "TruthfulQA":40.65,
        "Winogrande":61.01,
        "GSM8K":0.38,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.05,
        "Hub \u2764\ufe0f":103.0,
        "Available on the Hub":true,
        "Model Sha":"d1438e22a33b9115af0e47ab3a0fe844cbf588a6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TFLai\/pythia-2.8b-4bit-alpaca",
        "Average":36.77,
        "ARC":34.73,
        "HellaSwag":58.96,
        "MMLU":25.53,
        "TruthfulQA":39.14,
        "Winogrande":61.64,
        "GSM8K":0.61,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":2.8,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":false,
        "Model Sha":"40e84b6d38aac92a0302c2a682498794ef0fd901"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/OPT-2.7B-Nerys-v2",
        "Average":36.75,
        "ARC":33.28,
        "HellaSwag":61.23,
        "MMLU":26.44,
        "TruthfulQA":37.23,
        "Winogrande":62.04,
        "GSM8K":0.3,
        "Type":"fine-tuned",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"91d7afd6dbf3bbd1e4ccc6b9a2618d632a8cbb92"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"facebook\/opt-2.7b",
        "Average":36.74,
        "ARC":33.96,
        "HellaSwag":61.43,
        "MMLU":25.43,
        "TruthfulQA":37.43,
        "Winogrande":61.96,
        "GSM8K":0.23,
        "Type":"pretrained",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":46.0,
        "Available on the Hub":true,
        "Model Sha":"397f71a473a150c00f0fe3fc4a2f78ff3ccaf82d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"L-R\/LLmRa-2.7B",
        "Average":36.72,
        "ARC":37.03,
        "HellaSwag":60.65,
        "MMLU":25.58,
        "TruthfulQA":35.23,
        "Winogrande":61.56,
        "GSM8K":0.3,
        "Type":"fine-tuned",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":2.7,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"93201b7d778272fb3252481c1cbd56f726d43e6b"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/pythia-2.8b-deduped",
        "Average":36.72,
        "ARC":36.26,
        "HellaSwag":60.66,
        "MMLU":26.78,
        "TruthfulQA":35.56,
        "Winogrande":60.22,
        "GSM8K":0.83,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.91,
        "Hub \u2764\ufe0f":12.0,
        "Available on the Hub":true,
        "Model Sha":"7d977fed8c4ce9649816af8cd5fe36a639cbe5b2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aisquared\/chopt-2_7b",
        "Average":36.72,
        "ARC":36.01,
        "HellaSwag":63.38,
        "MMLU":25.44,
        "TruthfulQA":37.71,
        "Winogrande":57.77,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"45f57352c10a1fb1ec13c4bf387a15552ca1fe65"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"danielhanchen\/open_llama_3b_600bt_preview",
        "Average":36.65,
        "ARC":36.86,
        "HellaSwag":59.96,
        "MMLU":25.97,
        "TruthfulQA":32.81,
        "Winogrande":63.69,
        "GSM8K":0.61,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"d8fddf7651dfcae5aefda59d9e868c9111d8bdb3"
    },
    {
        "T":"\u2b55",
        "Model":"42dot\/42dot_LLM-SFT-1.3B",
        "Average":36.61,
        "ARC":36.09,
        "HellaSwag":58.96,
        "MMLU":25.51,
        "TruthfulQA":39.98,
        "Winogrande":58.41,
        "GSM8K":0.68,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.44,
        "Hub \u2764\ufe0f":12.0,
        "Available on the Hub":true,
        "Model Sha":"7474cafe5dc60549c19f89f7c49392a8a32b9199"
    },
    {
        "T":"\u2b55",
        "Model":"PSanni\/Deer-3b",
        "Average":36.55,
        "ARC":38.48,
        "HellaSwag":57.41,
        "MMLU":25.64,
        "TruthfulQA":39.98,
        "Winogrande":57.46,
        "GSM8K":0.3,
        "Type":"instruction-tuned",
        "Architecture":"BloomForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.0,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"53ea8f8862fc1820f0cd31f62953b7290fd79867"
    },
    {
        "T":"\u2b55",
        "Model":"codellama\/CodeLlama-7b-Python-hf",
        "Average":36.42,
        "ARC":29.27,
        "HellaSwag":50.12,
        "MMLU":28.37,
        "TruthfulQA":41.61,
        "Winogrande":64.01,
        "GSM8K":5.16,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":6.74,
        "Hub \u2764\ufe0f":54.0,
        "Available on the Hub":true,
        "Model Sha":"ec4dd26f30674fdee00ef161b55f464ce28f9c20"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"facebook\/xglm-7.5B",
        "Average":36.38,
        "ARC":34.13,
        "HellaSwag":60.77,
        "MMLU":27.79,
        "TruthfulQA":36.66,
        "Winogrande":58.72,
        "GSM8K":0.23,
        "Type":"pretrained",
        "Architecture":"XGLMForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":7.49,
        "Hub \u2764\ufe0f":43.0,
        "Available on the Hub":true,
        "Model Sha":"732d59308a844004bd9a4def972cc7c3896a38e0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Phind\/Phind-CodeLlama-34B-Python-v1",
        "Average":36.33,
        "ARC":24.66,
        "HellaSwag":29.77,
        "MMLU":27.95,
        "TruthfulQA":45.27,
        "Winogrande":68.82,
        "GSM8K":21.53,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":231.0,
        "Available on the Hub":true,
        "Model Sha":"3aabef8c9bc1b3ec2fffed053645bc1e2d829b6c"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"cerebras\/Cerebras-GPT-6.7B",
        "Average":36.27,
        "ARC":35.07,
        "HellaSwag":59.36,
        "MMLU":25.93,
        "TruthfulQA":38.02,
        "Winogrande":58.72,
        "GSM8K":0.53,
        "Type":"pretrained",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.66,
        "Hub \u2764\ufe0f":61.0,
        "Available on the Hub":true,
        "Model Sha":"4f56c6e28f9a2a1c470626f1a064238806f19f09"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/gpt-neo-2.7B",
        "Average":36.2,
        "ARC":33.36,
        "HellaSwag":56.24,
        "MMLU":26.45,
        "TruthfulQA":39.78,
        "Winogrande":60.06,
        "GSM8K":1.29,
        "Type":"pretrained",
        "Architecture":"GPTNeoForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":2.72,
        "Hub \u2764\ufe0f":361.0,
        "Available on the Hub":true,
        "Model Sha":"e24fa291132763e59f4a5422741b424fb5d59056"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bertin-project\/bertin-gpt-j-6B-alpaca",
        "Average":36.19,
        "ARC":36.01,
        "HellaSwag":54.3,
        "MMLU":27.66,
        "TruthfulQA":43.38,
        "Winogrande":55.8,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"openrail",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"636b17d6044189343475d1889f076aba73036905"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"euclaise\/falcon_1b_stage3_2",
        "Average":36.19,
        "ARC":34.56,
        "HellaSwag":58.37,
        "MMLU":23.87,
        "TruthfulQA":39.89,
        "Winogrande":60.46,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"FalconForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"aec2f59879ea6dfa5233611c4cf83cf3cb974d40"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Dampish\/StellarX-4B-V0.2",
        "Average":36.15,
        "ARC":34.64,
        "HellaSwag":56.74,
        "MMLU":25.55,
        "TruthfulQA":38.55,
        "Winogrande":61.4,
        "GSM8K":0.0,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"605b6812956400dbde24ad7b8649a744a2ddfc8e"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"bigscience\/bloom-3b",
        "Average":36.07,
        "ARC":35.75,
        "HellaSwag":54.37,
        "MMLU":26.59,
        "TruthfulQA":40.57,
        "Winogrande":57.62,
        "GSM8K":1.52,
        "Type":"pretrained",
        "Architecture":"BloomForCausalLM",
        "Precision":"float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":3.0,
        "Hub \u2764\ufe0f":70.0,
        "Available on the Hub":true,
        "Model Sha":"52bc5b43010b4844513826b8be3f78c7344c37d7"
    },
    {
        "T":"?",
        "Model":"TheBloke\/Wizard-Vicuna-13B-Uncensored-GPTQ",
        "Average":36.06,
        "ARC":29.61,
        "HellaSwag":25.47,
        "MMLU":25.34,
        "TruthfulQA":50.25,
        "Winogrande":75.77,
        "GSM8K":9.93,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":16.22,
        "Hub \u2764\ufe0f":242.0,
        "Available on the Hub":true,
        "Model Sha":"d9b00ec47ae3546398432f0693fe2d5d92bf143b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Azure99\/blossom-v2-3b",
        "Average":35.98,
        "ARC":35.32,
        "HellaSwag":54.1,
        "MMLU":23.99,
        "TruthfulQA":43.11,
        "Winogrande":58.8,
        "GSM8K":0.53,
        "Type":"fine-tuned",
        "Architecture":"BloomForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"1a403344de52ddb7f18548a526a927714adfe4d4"
    },
    {
        "T":"\u2b55",
        "Model":"vihangd\/shearedplats-1.3b-v1",
        "Average":35.97,
        "ARC":35.41,
        "HellaSwag":62.75,
        "MMLU":24.75,
        "TruthfulQA":33.93,
        "Winogrande":58.48,
        "GSM8K":0.53,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":1.3,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"7ac93152e1807ec1d732500255a747e27922fb1a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"princeton-nlp\/Sheared-LLaMA-1.3B",
        "Average":35.95,
        "ARC":32.85,
        "HellaSwag":60.91,
        "MMLU":25.71,
        "TruthfulQA":37.14,
        "Winogrande":58.64,
        "GSM8K":0.45,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.28,
        "Hub \u2764\ufe0f":28.0,
        "Available on the Hub":true,
        "Model Sha":"b1c3f74c8495e27b3963d64af0781d4a611794f3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/GPT-J-6B-Adventure",
        "Average":35.95,
        "ARC":37.12,
        "HellaSwag":61.26,
        "MMLU":25.94,
        "TruthfulQA":34.56,
        "Winogrande":55.96,
        "GSM8K":0.83,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":15.0,
        "Available on the Hub":true,
        "Model Sha":"e2c00dc99f986f2430f5d34c0214969cee786755"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ehartford\/CodeLlama-34b-Python-hf",
        "Average":35.92,
        "ARC":38.05,
        "HellaSwag":34.79,
        "MMLU":32.96,
        "TruthfulQA":43.57,
        "Winogrande":66.14,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":33.48,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"45f38e53a579a2b39298cc57ab04078722bebec0"
    },
    {
        "T":"\u2b55",
        "Model":"MayaPH\/opt-flan-iml-6.7b",
        "Average":35.84,
        "ARC":30.12,
        "HellaSwag":58.82,
        "MMLU":25.12,
        "TruthfulQA":36.74,
        "Winogrande":64.25,
        "GSM8K":0.0,
        "Type":"instruction-tuned",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":6.66,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"cbe8d60db6f3c52e653ca73e23a1c34c08127d02"
    },
    {
        "T":"\u2b55",
        "Model":"RWKV\/rwkv-raven-3b",
        "Average":35.81,
        "ARC":36.69,
        "HellaSwag":59.78,
        "MMLU":24.87,
        "TruthfulQA":35.6,
        "Winogrande":57.46,
        "GSM8K":0.45,
        "Type":"instruction-tuned",
        "Architecture":"RwkvForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":2.86,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"1ddeea6a7313c8ba8824645d7aa88d5449458f67"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"42dot\/42dot_LLM-PLM-1.3B",
        "Average":35.7,
        "ARC":32.42,
        "HellaSwag":56.39,
        "MMLU":27.09,
        "TruthfulQA":38.68,
        "Winogrande":58.88,
        "GSM8K":0.76,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.44,
        "Hub \u2764\ufe0f":12.0,
        "Available on the Hub":true,
        "Model Sha":"a72bf57eb02cd4ea4388a344b4a5893aa95698da"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"sartmis1\/starcoder-finetune-selfinstruct",
        "Average":35.65,
        "ARC":31.23,
        "HellaSwag":47.66,
        "MMLU":29.52,
        "TruthfulQA":41.63,
        "Winogrande":57.77,
        "GSM8K":6.07,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"b21bd307ea7417185e7dc59557c399a3e4e0092b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"habanoz\/tinyllama-oasst1-top1-instruct-full-lr1-5-v0.1",
        "Average":35.58,
        "ARC":32.85,
        "HellaSwag":58.16,
        "MMLU":25.96,
        "TruthfulQA":38.35,
        "Winogrande":57.7,
        "GSM8K":0.45,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"e55b262cbd0ee52f7a4cbda136dbf1a027987c47"
    },
    {
        "T":"?",
        "Model":"TheBloke\/wizard-vicuna-13B-GPTQ",
        "Average":35.56,
        "ARC":28.67,
        "HellaSwag":25.94,
        "MMLU":25.84,
        "TruthfulQA":48.53,
        "Winogrande":74.74,
        "GSM8K":9.63,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":16.22,
        "Hub \u2764\ufe0f":99.0,
        "Available on the Hub":true,
        "Model Sha":"936a51c0219744d7a9598d0c65a7d18e01660601"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PY007\/TinyLlama-1.1B-Chat-v0.3",
        "Average":35.56,
        "ARC":35.07,
        "HellaSwag":57.7,
        "MMLU":25.53,
        "TruthfulQA":36.67,
        "Winogrande":57.7,
        "GSM8K":0.68,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.03,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":false,
        "Model Sha":"20dd44d78aa09480bf15ca0ecc0c0780951d49a9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"pythainlp\/wangchanglm-7.5B-sft-en-sharded",
        "Average":35.55,
        "ARC":34.47,
        "HellaSwag":59.81,
        "MMLU":26.37,
        "TruthfulQA":34.15,
        "Winogrande":58.25,
        "GSM8K":0.23,
        "Type":"fine-tuned",
        "Architecture":"XGLMForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":7.49,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"dd22eaea8be3fcb8c28f61b513a89d1adac00ffd"
    },
    {
        "T":"?",
        "Model":"HuggingFaceH4\/starchat-alpha",
        "Average":35.49,
        "ARC":31.57,
        "HellaSwag":49.43,
        "MMLU":30.76,
        "TruthfulQA":43.66,
        "Winogrande":55.09,
        "GSM8K":2.43,
        "Type":"",
        "Architecture":"GPTBigCodeForCausalLM",
        "Precision":"float16",
        "Hub License":"bigcode-openrail-m",
        "#Params (B)":15.52,
        "Hub \u2764\ufe0f":220.0,
        "Available on the Hub":true,
        "Model Sha":"b693a7a7d52bed1cd7cc0fe00399db838b09c74f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bhenrym14\/airoboros-33b-gpt4-1.4.1-PI-8192-fp16",
        "Average":35.46,
        "ARC":32.0,
        "HellaSwag":53.88,
        "MMLU":31.43,
        "TruthfulQA":38.59,
        "Winogrande":56.83,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":32.53,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"1dd7804dbbb547c1be852652ce74568ba41d4e73"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"AtAndDev\/ShortKingv0.1",
        "Average":35.45,
        "ARC":34.22,
        "HellaSwag":54.59,
        "MMLU":25.78,
        "TruthfulQA":41.64,
        "Winogrande":56.04,
        "GSM8K":0.45,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.42,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"6cd9b5bc13ee15b5e7e7cfb46477bc6a7c0b5d47"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"habanoz\/TinyLlama-1.1B-intermediate-step-715k-1.5T-lr-5-2.2epochs-oasst1-top1-instruct-V1",
        "Average":35.45,
        "ARC":31.48,
        "HellaSwag":54.4,
        "MMLU":25.47,
        "TruthfulQA":42.34,
        "Winogrande":57.54,
        "GSM8K":1.44,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"74cd9eba94e77832b3081689fc5c99c37c063790"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nnpy\/Nape-0",
        "Average":35.43,
        "ARC":32.68,
        "HellaSwag":58.68,
        "MMLU":24.88,
        "TruthfulQA":38.99,
        "Winogrande":57.3,
        "GSM8K":0.08,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":1.1,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"47e07bd518b989890a7f694d39e2772e703384c9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lizhuang144\/starcoder_mirror",
        "Average":35.43,
        "ARC":31.31,
        "HellaSwag":45.82,
        "MMLU":29.29,
        "TruthfulQA":43.38,
        "Winogrande":57.22,
        "GSM8K":5.53,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"eb5f39bac15ccab9463001aa203e33d49f4ff7cb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"habanoz\/TinyLlama-1.1B-intermediate-step-715k-1.5T-lr-5-3epochs-oasst1-top1-instruct-V1",
        "Average":35.42,
        "ARC":31.4,
        "HellaSwag":54.24,
        "MMLU":25.36,
        "TruthfulQA":42.47,
        "Winogrande":57.7,
        "GSM8K":1.36,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"b1ec2a1e08eb790b9a32a43053316650921af943"
    },
    {
        "T":"?",
        "Model":"TheBloke\/openchat_v2_openorca_preview-GPTQ",
        "Average":35.38,
        "ARC":27.99,
        "HellaSwag":26.06,
        "MMLU":24.24,
        "TruthfulQA":50.08,
        "Winogrande":70.64,
        "GSM8K":13.27,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":16.22,
        "Hub \u2764\ufe0f":14.0,
        "Available on the Hub":true,
        "Model Sha":"5a4c2ea612b71d7c00118f796db7189bc1a0c930"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aisquared\/chopt-1_3b",
        "Average":35.32,
        "ARC":31.48,
        "HellaSwag":56.63,
        "MMLU":25.35,
        "TruthfulQA":40.19,
        "Winogrande":58.25,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":1.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"fdd3691978f557baf9d1c20d4ede900c47f7e135"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"vihangd\/dopeyplats-1.1b-2T-v1",
        "Average":35.28,
        "ARC":33.11,
        "HellaSwag":54.31,
        "MMLU":24.55,
        "TruthfulQA":39.26,
        "Winogrande":58.8,
        "GSM8K":1.67,
        "Type":"RL-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"4ca47b470296de0e7bf3261e377aabaff9ad5c06"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"habanoz\/TinyLlama-1.1B-intermediate-step-715k-1.5T-lr-5-4epochs-oasst1-top1-instruct-V1",
        "Average":35.28,
        "ARC":31.14,
        "HellaSwag":54.31,
        "MMLU":25.42,
        "TruthfulQA":41.72,
        "Winogrande":57.77,
        "GSM8K":1.29,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"7cd6d5ad10180127771e4326772eae3d40fa8445"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"RWKV\/rwkv-4-3b-pile",
        "Average":35.25,
        "ARC":36.01,
        "HellaSwag":59.66,
        "MMLU":24.67,
        "TruthfulQA":32.14,
        "Winogrande":58.33,
        "GSM8K":0.68,
        "Type":"pretrained",
        "Architecture":"RwkvForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":2.86,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"7fdda3c5570d4a9711f8f02cc3a20941a5623cd3"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/Deacon-1b",
        "Average":35.21,
        "ARC":32.42,
        "HellaSwag":58.62,
        "MMLU":24.89,
        "TruthfulQA":35.05,
        "Winogrande":59.59,
        "GSM8K":0.68,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.1,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"77f16fd4c605fe043033d4335024fb887cedef69"
    },
    {
        "T":"?",
        "Model":"facebook\/opt-iml-max-1.3b",
        "Average":35.21,
        "ARC":30.72,
        "HellaSwag":53.81,
        "MMLU":27.61,
        "TruthfulQA":38.34,
        "Winogrande":60.22,
        "GSM8K":0.53,
        "Type":"",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":1.32,
        "Hub \u2764\ufe0f":34.0,
        "Available on the Hub":true,
        "Model Sha":"d60fa58f50def19751da2075791da359ca19d273"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Writer\/palmyra-base",
        "Average":35.18,
        "ARC":31.91,
        "HellaSwag":55.39,
        "MMLU":27.15,
        "TruthfulQA":37.57,
        "Winogrande":58.09,
        "GSM8K":0.99,
        "Type":"pretrained",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":5.05,
        "Hub \u2764\ufe0f":33.0,
        "Available on the Hub":true,
        "Model Sha":"df2f3bdb7cbe4295d69cf0cbc35f3ceaf451de82"
    },
    {
        "T":"?",
        "Model":"TheBloke\/wizard-mega-13B-GPTQ",
        "Average":35.18,
        "ARC":27.73,
        "HellaSwag":26.01,
        "MMLU":24.97,
        "TruthfulQA":48.69,
        "Winogrande":74.74,
        "GSM8K":8.95,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":16.22,
        "Hub \u2764\ufe0f":102.0,
        "Available on the Hub":true,
        "Model Sha":"848bf2514f804799dd28c188e5428d497dc983fb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/fairseq-dense-1.3B",
        "Average":35.16,
        "ARC":31.14,
        "HellaSwag":58.39,
        "MMLU":24.98,
        "TruthfulQA":37.43,
        "Winogrande":59.04,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"XGLMForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":1.41,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"20bf1732212ea81adb45b782a25ce69e65a01ad2"
    },
    {
        "T":"?",
        "Model":"TheBloke\/chronos-wizardlm-uc-scot-st-13B-GPTQ",
        "Average":35.15,
        "ARC":27.99,
        "HellaSwag":26.1,
        "MMLU":25.72,
        "TruthfulQA":49.68,
        "Winogrande":74.51,
        "GSM8K":6.9,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":16.22,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"c4246e4b8d3fc77b9fe4ebb1ead61cda4b83575b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"beaugogh\/pythia-1.4b-deduped-sharegpt",
        "Average":35.11,
        "ARC":34.3,
        "HellaSwag":54.49,
        "MMLU":24.0,
        "TruthfulQA":41.81,
        "Winogrande":55.25,
        "GSM8K":0.83,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.42,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"03dfdc25c111a6a4a16d3da12190697611936426"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HWERI\/pythia-1.4b-deduped-sharegpt",
        "Average":35.11,
        "ARC":34.3,
        "HellaSwag":54.49,
        "MMLU":24.0,
        "TruthfulQA":41.81,
        "Winogrande":55.25,
        "GSM8K":0.83,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.42,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"5b50336208840f557ef3301d841e7994caaa63bb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"pythainlp\/wangchanglm-7.5B-sft-enth",
        "Average":35.11,
        "ARC":33.79,
        "HellaSwag":58.99,
        "MMLU":24.52,
        "TruthfulQA":34.9,
        "Winogrande":57.93,
        "GSM8K":0.53,
        "Type":"fine-tuned",
        "Architecture":"XGLMForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":7.49,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"eeee33ea6778a5e66184eeb4bf4294d4316b1933"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PygmalionAI\/metharme-1.3b",
        "Average":35.04,
        "ARC":34.39,
        "HellaSwag":55.94,
        "MMLU":25.07,
        "TruthfulQA":37.68,
        "Winogrande":56.43,
        "GSM8K":0.76,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.52,
        "Hub \u2764\ufe0f":18.0,
        "Available on the Hub":true,
        "Model Sha":"62ec4ff53042f692ef0661e54f371747214707a4"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/falcon-1b-t-sft",
        "Average":35.02,
        "ARC":32.94,
        "HellaSwag":57.24,
        "MMLU":25.26,
        "TruthfulQA":38.49,
        "Winogrande":55.88,
        "GSM8K":0.3,
        "Type":"instruction-tuned",
        "Architecture":"FalconForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.31,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"3b891a0c37f8fa98301c85fcf34baae876e4cac1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"L-R\/LLmRa-1.3B",
        "Average":35.0,
        "ARC":32.68,
        "HellaSwag":58.77,
        "MMLU":23.23,
        "TruthfulQA":36.21,
        "Winogrande":59.04,
        "GSM8K":0.08,
        "Type":"fine-tuned",
        "Architecture":"XGLMForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"8d5e8bb336cb886e20a7570bc00c2381792338a5"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/pythia-1.4b-deduped",
        "Average":35.0,
        "ARC":32.68,
        "HellaSwag":54.96,
        "MMLU":25.56,
        "TruthfulQA":38.66,
        "Winogrande":57.3,
        "GSM8K":0.83,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub \u2764\ufe0f":18.0,
        "Available on the Hub":true,
        "Model Sha":"77f320b24ccae4aa85a5890dbb9514bd11267bb3"
    },
    {
        "T":"?",
        "Model":"habanoz\/TinyLlama-1.1B-intermediate-step-715k-1.5T-lr-5-1epch-airoboros3.1-1k-instruct-V1",
        "Average":34.98,
        "ARC":30.72,
        "HellaSwag":54.32,
        "MMLU":24.78,
        "TruthfulQA":41.67,
        "Winogrande":57.62,
        "GSM8K":0.76,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"2b961bacab9fcd4bf9a0d6979b024fe23f61555e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"euclaise\/falcon_1b_stage3",
        "Average":34.95,
        "ARC":33.11,
        "HellaSwag":54.08,
        "MMLU":25.11,
        "TruthfulQA":37.92,
        "Winogrande":59.51,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"FalconForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"593e48197e91537b203ba288260f6580b9cbcbe6"
    },
    {
        "T":"?",
        "Model":"TinyLlama\/TinyLlama-1.1B-Chat-v0.6",
        "Average":34.94,
        "ARC":31.66,
        "HellaSwag":55.79,
        "MMLU":25.98,
        "TruthfulQA":34.72,
        "Winogrande":59.35,
        "GSM8K":2.12,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"bf9ae1c8bf026667e6f810768de259bb4a7f4777"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"OpenAssistant\/stablelm-7b-sft-v7-epoch-3",
        "Average":34.85,
        "ARC":36.01,
        "HellaSwag":55.81,
        "MMLU":25.01,
        "TruthfulQA":37.02,
        "Winogrande":54.85,
        "GSM8K":0.38,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":7.56,
        "Hub \u2764\ufe0f":65.0,
        "Available on the Hub":true,
        "Model Sha":"4c454bfc0e3618b3d574e28ba71369607e637e91"
    },
    {
        "T":"\u2b55",
        "Model":"Jiayi-Pan\/Tiny-Vicuna-1B",
        "Average":34.76,
        "ARC":33.45,
        "HellaSwag":55.92,
        "MMLU":25.45,
        "TruthfulQA":33.82,
        "Winogrande":58.41,
        "GSM8K":1.52,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"175336a0000f36b508575ef1a2da05755faf48c3"
    },
    {
        "T":"\u2b55",
        "Model":"w95\/megachat",
        "Average":34.75,
        "ARC":30.8,
        "HellaSwag":54.35,
        "MMLU":25.55,
        "TruthfulQA":39.85,
        "Winogrande":56.99,
        "GSM8K":0.99,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"789b259a18ca7b168ced4995138ad6195cd2e8e8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MBZUAI\/lamini-neo-1.3b",
        "Average":34.73,
        "ARC":32.76,
        "HellaSwag":49.13,
        "MMLU":28.79,
        "TruthfulQA":41.05,
        "Winogrande":56.51,
        "GSM8K":0.15,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.32,
        "Hub \u2764\ufe0f":11.0,
        "Available on the Hub":true,
        "Model Sha":"a5c7ecc4d908e7a9469d080308af64ae775c733d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MBZUAI\/LaMini-GPT-1.5B",
        "Average":34.67,
        "ARC":31.4,
        "HellaSwag":48.38,
        "MMLU":29.92,
        "TruthfulQA":42.47,
        "Winogrande":55.88,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.56,
        "Hub \u2764\ufe0f":32.0,
        "Available on the Hub":true,
        "Model Sha":"88ca6f5abe2335bac317e82684e574afdd6046b5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"WizardLM\/WizardCoder-15B-V1.0",
        "Average":34.64,
        "ARC":32.34,
        "HellaSwag":47.2,
        "MMLU":29.43,
        "TruthfulQA":41.56,
        "Winogrande":55.17,
        "GSM8K":2.12,
        "Type":"fine-tuned",
        "Architecture":"GPTBigCodeForCausalLM",
        "Precision":"float16",
        "Hub License":"bigscience-openrail-m",
        "#Params (B)":15.52,
        "Hub \u2764\ufe0f":621.0,
        "Available on the Hub":true,
        "Model Sha":"926ca1b215c4631bc5f8c3e47173381452c23e5c"
    },
    {
        "T":"?",
        "Model":"facebook\/opt-1.3b",
        "Average":34.6,
        "ARC":29.52,
        "HellaSwag":54.53,
        "MMLU":24.96,
        "TruthfulQA":38.71,
        "Winogrande":59.75,
        "GSM8K":0.15,
        "Type":"",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":1.32,
        "Hub \u2764\ufe0f":117.0,
        "Available on the Hub":true,
        "Model Sha":"8c7b10754972749675d22364c25c428b29face51"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PY007\/TinyLlama-1.1B-Chat-v0.1",
        "Average":34.57,
        "ARC":32.0,
        "HellaSwag":54.21,
        "MMLU":26.71,
        "TruthfulQA":39.03,
        "Winogrande":54.93,
        "GSM8K":0.53,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub \u2764\ufe0f":22.0,
        "Available on the Hub":true,
        "Model Sha":"7abc14e7779eabc3a028bc695342869d0410dea2"
    },
    {
        "T":"?",
        "Model":"TinyLlama\/TinyLlama-1.1B-intermediate-step-955k-token-2T",
        "Average":34.56,
        "ARC":30.29,
        "HellaSwag":54.84,
        "MMLU":26.47,
        "TruthfulQA":36.07,
        "Winogrande":58.33,
        "GSM8K":1.36,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub \u2764\ufe0f":12.0,
        "Available on the Hub":true,
        "Model Sha":"f62ecb34ea0d4acea9d896040a4616a9538e2f36"
    },
    {
        "T":"\u2b55",
        "Model":"AI-Sweden-Models\/gpt-sw3-1.3b-instruct",
        "Average":34.54,
        "ARC":30.97,
        "HellaSwag":51.42,
        "MMLU":26.17,
        "TruthfulQA":40.31,
        "Winogrande":56.75,
        "GSM8K":1.59,
        "Type":"instruction-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"bfloat16",
        "Hub License":"other",
        "#Params (B)":1.44,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"5f2f03167dedc59192ee02694e07424a890d9206"
    },
    {
        "T":"?",
        "Model":"habanoz\/TinyLlama-1.1B-step-2T-lr-5-5ep-oasst1-top1-instruct-V1",
        "Average":34.53,
        "ARC":31.06,
        "HellaSwag":55.02,
        "MMLU":26.41,
        "TruthfulQA":35.08,
        "Winogrande":58.01,
        "GSM8K":1.59,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"586c223b539e05fd8a63733c6a540f292460e639"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bhenrym14\/airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-fp16",
        "Average":34.53,
        "ARC":25.34,
        "HellaSwag":26.66,
        "MMLU":23.36,
        "TruthfulQA":49.51,
        "Winogrande":73.72,
        "GSM8K":8.57,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":32.53,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"468225a547a8cb0a62758d813cf9606b58506ab4"
    },
    {
        "T":"\u2b55",
        "Model":"lgaalves\/tinyllama-1.1b-chat-v0.3_platypus",
        "Average":34.5,
        "ARC":30.29,
        "HellaSwag":55.12,
        "MMLU":26.13,
        "TruthfulQA":39.15,
        "Winogrande":55.8,
        "GSM8K":0.53,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":1.03,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"0bb6ebe1d41d394bae0ed9107ec8d776d9d76a68"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/pythia-1.3b",
        "Average":34.46,
        "ARC":31.14,
        "HellaSwag":51.43,
        "MMLU":26.55,
        "TruthfulQA":39.24,
        "Winogrande":57.38,
        "GSM8K":0.99,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"34b668ff0acfe56f2d541aa46b385557ee39eb3f"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"NYTK\/PULI-GPTrio",
        "Average":34.42,
        "ARC":30.72,
        "HellaSwag":53.49,
        "MMLU":24.73,
        "TruthfulQA":39.03,
        "Winogrande":57.77,
        "GSM8K":0.76,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":7.06,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"c85efce322a0f6d93d64f7b9096525753da6913e"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"PY007\/TinyLlama-1.1B-intermediate-step-480k-1T",
        "Average":34.37,
        "ARC":30.89,
        "HellaSwag":52.97,
        "MMLU":25.0,
        "TruthfulQA":39.55,
        "Winogrande":57.3,
        "GSM8K":0.53,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.03,
        "Hub \u2764\ufe0f":15.0,
        "Available on the Hub":true,
        "Model Sha":"098830e58452a0a08f90eb0189ec5925803fd48b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/EverythingLM-13B-16K-GPTQ",
        "Average":34.37,
        "ARC":29.27,
        "HellaSwag":26.24,
        "MMLU":25.4,
        "TruthfulQA":48.58,
        "Winogrande":71.35,
        "GSM8K":5.38,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":16.23,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"f14d3df05577f3e1ac35e2c4ec32ce0d39b97508"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"stabilityai\/stablelm-base-alpha-7b",
        "Average":34.37,
        "ARC":32.0,
        "HellaSwag":51.78,
        "MMLU":26.21,
        "TruthfulQA":40.19,
        "Winogrande":55.41,
        "GSM8K":0.61,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":7.56,
        "Hub \u2764\ufe0f":208.0,
        "Available on the Hub":true,
        "Model Sha":"38366357b5a45e002af2d254ff3d559444ec2147"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"h2oai\/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt",
        "Average":34.32,
        "ARC":34.04,
        "HellaSwag":50.51,
        "MMLU":24.66,
        "TruthfulQA":41.8,
        "Winogrande":54.93,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":12.0,
        "Available on the Hub":true,
        "Model Sha":"754e0c90ed5d9241fdfd5a188572b3ea2152eaa7"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"facebook\/xglm-4.5B",
        "Average":34.31,
        "ARC":31.48,
        "HellaSwag":57.95,
        "MMLU":25.43,
        "TruthfulQA":35.84,
        "Winogrande":54.93,
        "GSM8K":0.23,
        "Type":"pretrained",
        "Architecture":"XGLMForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":5.08,
        "Hub \u2764\ufe0f":11.0,
        "Available on the Hub":true,
        "Model Sha":"dc6a67fac06c8bca7860b84656a0cb736293a7a8"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"AI-Sweden-Models\/gpt-sw3-1.3b",
        "Average":34.31,
        "ARC":30.38,
        "HellaSwag":50.4,
        "MMLU":26.14,
        "TruthfulQA":39.97,
        "Winogrande":58.88,
        "GSM8K":0.08,
        "Type":"pretrained",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.44,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"b0d9545a27cfaf9a937adac72ed6953f2dc597de"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"winglian\/llama-2-4b",
        "Average":34.23,
        "ARC":31.23,
        "HellaSwag":53.29,
        "MMLU":24.22,
        "TruthfulQA":38.72,
        "Winogrande":57.46,
        "GSM8K":0.45,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":3.37,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"fbba77f9894cf738ad8d7d08fc6874856fb42507"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"L-R\/LLmRa-1.3B_V2",
        "Average":34.21,
        "ARC":30.46,
        "HellaSwag":53.03,
        "MMLU":26.06,
        "TruthfulQA":36.46,
        "Winogrande":59.27,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":1.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"a760ebda8f736988eafea879173c5be468ea68d0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aisquared\/dlite-v2-1_5b",
        "Average":34.2,
        "ARC":32.59,
        "HellaSwag":53.98,
        "MMLU":24.93,
        "TruthfulQA":38.77,
        "Winogrande":54.7,
        "GSM8K":0.23,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.56,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"97440ff1b6ef749423758e3495cdce1b5e68ee92"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LoupGarou\/WizardCoder-Guanaco-15B-V1.1",
        "Average":34.19,
        "ARC":32.59,
        "HellaSwag":45.42,
        "MMLU":25.88,
        "TruthfulQA":42.33,
        "Winogrande":56.04,
        "GSM8K":2.88,
        "Type":"fine-tuned",
        "Architecture":"GPTBigCodeForCausalLM",
        "Precision":"float16",
        "Hub License":[
            "apache-2.0"
        ],
        "#Params (B)":15.52,
        "Hub \u2764\ufe0f":11.0,
        "Available on the Hub":true,
        "Model Sha":"979531c84ec0b4e1712d6a5cec6907126a21e605"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"GeorgiaTechResearchInstitute\/starcoder-gpteacher-code-instruct",
        "Average":34.15,
        "ARC":32.68,
        "HellaSwag":47.6,
        "MMLU":28.63,
        "TruthfulQA":40.41,
        "Winogrande":55.56,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPTBigCodeForCausalLM",
        "Precision":"float16",
        "Hub License":"bigcode-openrail-m",
        "#Params (B)":15.52,
        "Hub \u2764\ufe0f":74.0,
        "Available on the Hub":true,
        "Model Sha":"d866b68daa719239dc44979dbf39a608ed6f7bce"
    },
    {
        "T":"\u2b55",
        "Model":"lgaalves\/gpt2-xl_lima",
        "Average":34.12,
        "ARC":31.14,
        "HellaSwag":51.28,
        "MMLU":25.43,
        "TruthfulQA":38.74,
        "Winogrande":57.22,
        "GSM8K":0.91,
        "Type":"instruction-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f7db5b1db521abd7578b95138e737637e0037ca5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"habanoz\/TinyLlama-1.1B-2T-lr-2e-4-3ep-dolly-15k-instruct-v1",
        "Average":34.04,
        "ARC":30.55,
        "HellaSwag":53.7,
        "MMLU":26.07,
        "TruthfulQA":35.85,
        "Winogrande":58.09,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"152436a0dd6ca1603b3993bbf08a227ea131f85d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"stabilityai\/stablelm-tuned-alpha-7b",
        "Average":34.04,
        "ARC":31.91,
        "HellaSwag":53.59,
        "MMLU":24.41,
        "TruthfulQA":40.37,
        "Winogrande":53.12,
        "GSM8K":0.83,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":7.56,
        "Hub \u2764\ufe0f":350.0,
        "Available on the Hub":true,
        "Model Sha":"25071b093c15c0d1cb2b2876c6deb621b764fcf5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"jzjiao\/opt-1.3b-rlhf",
        "Average":33.99,
        "ARC":28.92,
        "HellaSwag":52.77,
        "MMLU":25.39,
        "TruthfulQA":37.44,
        "Winogrande":58.96,
        "GSM8K":0.45,
        "Type":"fine-tuned",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":1.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"5b12df71b21b6b7d76ca9d56de6751f25022e854"
    },
    {
        "T":"?",
        "Model":"bigscience\/bloom-1b7",
        "Average":33.98,
        "ARC":30.63,
        "HellaSwag":47.6,
        "MMLU":27.48,
        "TruthfulQA":41.31,
        "Winogrande":56.04,
        "GSM8K":0.83,
        "Type":"",
        "Architecture":"BloomForCausalLM",
        "Precision":"float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":1.72,
        "Hub \u2764\ufe0f":100.0,
        "Available on the Hub":true,
        "Model Sha":"cc72a88036c2fb937d65efeacc57a0c2ef5d6fe5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PygmalionAI\/pygmalion-2.7b",
        "Average":33.98,
        "ARC":32.76,
        "HellaSwag":54.13,
        "MMLU":23.28,
        "TruthfulQA":37.17,
        "Winogrande":56.51,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoForCausalLM",
        "Precision":"float16",
        "Hub License":"creativeml-openrail-m",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":48.0,
        "Available on the Hub":true,
        "Model Sha":"9533805293bc48e8ddfe9dc1940d8cbc5662113e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"LoupGarou\/WizardCoder-Guanaco-15B-V1.0",
        "Average":33.96,
        "ARC":30.46,
        "HellaSwag":45.59,
        "MMLU":26.79,
        "TruthfulQA":46.39,
        "Winogrande":53.12,
        "GSM8K":1.44,
        "Type":"fine-tuned",
        "Architecture":"GPTBigCodeForCausalLM",
        "Precision":"float16",
        "Hub License":[
            "apache-2.0"
        ],
        "#Params (B)":15.52,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"ab5ea678d63eb2324658dcc8cfae267eabc366ef"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"golaxy\/gogpt-3b-bloom",
        "Average":33.96,
        "ARC":31.91,
        "HellaSwag":50.32,
        "MMLU":25.2,
        "TruthfulQA":41.79,
        "Winogrande":54.38,
        "GSM8K":0.15,
        "Type":"fine-tuned",
        "Architecture":"BloomForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.0,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"fe942d5d0faca8156eaf456ecdf569993eab8062"
    },
    {
        "T":"\u2b55",
        "Model":"lgaalves\/gpt-2-xl_camel-ai-physics",
        "Average":33.96,
        "ARC":29.52,
        "HellaSwag":50.62,
        "MMLU":26.79,
        "TruthfulQA":39.12,
        "Winogrande":57.54,
        "GSM8K":0.15,
        "Type":"instruction-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":1.56,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"e20cf5a8c89441f4dc15fd2af12dbe72b7df8e60"
    },
    {
        "T":"?",
        "Model":"TheBloke\/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GPTQ",
        "Average":33.78,
        "ARC":28.41,
        "HellaSwag":26.05,
        "MMLU":24.71,
        "TruthfulQA":49.54,
        "Winogrande":68.67,
        "GSM8K":5.31,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":35.58,
        "Hub \u2764\ufe0f":69.0,
        "Available on the Hub":true,
        "Model Sha":"cd07cc7c55b46524f61214012653c25226d24c0d"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"PY007\/TinyLlama-1.1B-intermediate-step-240k-503b",
        "Average":33.72,
        "ARC":29.27,
        "HellaSwag":49.71,
        "MMLU":26.26,
        "TruthfulQA":40.17,
        "Winogrande":56.59,
        "GSM8K":0.3,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub \u2764\ufe0f":14.0,
        "Available on the Hub":true,
        "Model Sha":"213ebf60d7fdd3258fa5574840b06c97a7e8cf5d"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/gpt-neo-1.3B",
        "Average":33.58,
        "ARC":31.23,
        "HellaSwag":48.47,
        "MMLU":24.82,
        "TruthfulQA":39.63,
        "Winogrande":56.91,
        "GSM8K":0.45,
        "Type":"pretrained",
        "Architecture":"GPTNeoForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":1.37,
        "Hub \u2764\ufe0f":206.0,
        "Available on the Hub":true,
        "Model Sha":"8282180b53cba30a1575e49de1530019e5931739"
    },
    {
        "T":"\u2b55",
        "Model":"RWKV\/rwkv-raven-1b5",
        "Average":33.56,
        "ARC":31.83,
        "HellaSwag":52.6,
        "MMLU":25.96,
        "TruthfulQA":37.09,
        "Winogrande":53.91,
        "GSM8K":0.0,
        "Type":"instruction-tuned",
        "Architecture":"RwkvForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":1.41,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"571a3bd891ce33f2ee3fc6de09218178edb0dae2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"lxe\/Cerebras-GPT-2.7B-Alpaca-SP",
        "Average":33.5,
        "ARC":30.8,
        "HellaSwag":48.88,
        "MMLU":25.12,
        "TruthfulQA":40.24,
        "Winogrande":55.41,
        "GSM8K":0.53,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"ae7f22e90cb968b0a73355aa2001d6bc7df28477"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BEE-spoke-data\/TinyLlama-1.1bee",
        "Average":33.38,
        "ARC":30.55,
        "HellaSwag":51.8,
        "MMLU":24.25,
        "TruthfulQA":39.01,
        "Winogrande":54.46,
        "GSM8K":0.23,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"5889ec467cf80a83c4092b55686f8121e81bf001"
    },
    {
        "T":"?",
        "Model":"l3utterfly\/llama2-3b-distilled-layla-v1",
        "Average":33.36,
        "ARC":30.46,
        "HellaSwag":46.05,
        "MMLU":23.91,
        "TruthfulQA":42.14,
        "Winogrande":57.38,
        "GSM8K":0.23,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":3.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"a1ba0a65e5262bc134dbc562a9faf80865b0a72f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aisquared\/dlite-v1-1_5b",
        "Average":33.35,
        "ARC":31.66,
        "HellaSwag":49.69,
        "MMLU":25.62,
        "TruthfulQA":37.08,
        "Winogrande":55.96,
        "GSM8K":0.08,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.56,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"4ac21faec255e3544e96aeb3591c27bdee5ebf45"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/polyglot-ko-12.8b",
        "Average":33.33,
        "ARC":27.05,
        "HellaSwag":51.68,
        "MMLU":26.64,
        "TruthfulQA":34.69,
        "Winogrande":59.75,
        "GSM8K":0.15,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.06,
        "Hub \u2764\ufe0f":59.0,
        "Available on the Hub":true,
        "Model Sha":"09dfc839067bf44e7f52976eca8adbc17f04e1b0"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MrNJK\/gpt2-xl-sft",
        "Average":33.31,
        "ARC":30.03,
        "HellaSwag":49.17,
        "MMLU":25.56,
        "TruthfulQA":38.78,
        "Winogrande":55.56,
        "GSM8K":0.76,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.56,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"53250831436460254b7ee9afc4014d4d3156b372"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Corianas\/Quokka_2.7b",
        "Average":33.26,
        "ARC":31.06,
        "HellaSwag":47.72,
        "MMLU":24.8,
        "TruthfulQA":40.14,
        "Winogrande":55.49,
        "GSM8K":0.38,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.79,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"abe5e0f574d32f3234035b6e8c5d68bbb201e03c"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"cerebras\/Cerebras-GPT-2.7B",
        "Average":33.25,
        "ARC":29.1,
        "HellaSwag":49.29,
        "MMLU":25.17,
        "TruthfulQA":41.37,
        "Winogrande":54.14,
        "GSM8K":0.45,
        "Type":"pretrained",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":41.0,
        "Available on the Hub":true,
        "Model Sha":"4383dfd80aafdbcfd0876419d246de51e6cbf7c1"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"RWKV\/rwkv-4-1b5-pile",
        "Average":33.25,
        "ARC":31.83,
        "HellaSwag":52.25,
        "MMLU":25.77,
        "TruthfulQA":35.8,
        "Winogrande":53.83,
        "GSM8K":0.0,
        "Type":"pretrained",
        "Architecture":"RwkvForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":1.41,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"643585471eaf5821d94dfcb498ab5b94a36b42cf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"shaohang\/Sparse0.5_OPT-1.3",
        "Average":33.19,
        "ARC":27.13,
        "HellaSwag":48.69,
        "MMLU":25.6,
        "TruthfulQA":39.11,
        "Winogrande":58.56,
        "GSM8K":0.08,
        "Type":"fine-tuned",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"06249d582b0cfefac537dd6bee2e578002ffff00"
    },
    {
        "T":"?",
        "Model":"shaohang\/SparseOPT-1.3B",
        "Average":33.19,
        "ARC":27.13,
        "HellaSwag":48.69,
        "MMLU":25.6,
        "TruthfulQA":39.11,
        "Winogrande":58.56,
        "GSM8K":0.08,
        "Type":"",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"06249d582b0cfefac537dd6bee2e578002ffff00"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"TurkuNLP\/gpt3-finnish-13B",
        "Average":32.95,
        "ARC":24.66,
        "HellaSwag":46.76,
        "MMLU":23.49,
        "TruthfulQA":44.47,
        "Winogrande":58.01,
        "GSM8K":0.3,
        "Type":"pretrained",
        "Architecture":"BloomModel",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.26,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":true,
        "Model Sha":"ade35fd78ac2c29f7a56ffd3087321d297bb97a9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aisquared\/dlite-v2-774m",
        "Average":32.86,
        "ARC":30.12,
        "HellaSwag":47.68,
        "MMLU":25.37,
        "TruthfulQA":40.0,
        "Winogrande":53.99,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.77,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"0ea894a33e491912cd1a65dde47b4af03f03c4f2"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/pythia-1b-deduped",
        "Average":32.78,
        "ARC":29.1,
        "HellaSwag":49.65,
        "MMLU":24.27,
        "TruthfulQA":38.94,
        "Winogrande":53.59,
        "GSM8K":1.14,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.08,
        "Hub \u2764\ufe0f":13.0,
        "Available on the Hub":true,
        "Model Sha":"7199d8fc61a6d565cd1f3c62bf11525b563e13b2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KnutJaegersberg\/RWKV-4-PilePlus-1B5-20230520-2942-486Gtokens-ctx4096",
        "Average":32.68,
        "ARC":30.63,
        "HellaSwag":52.63,
        "MMLU":25.04,
        "TruthfulQA":34.96,
        "Winogrande":52.8,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.41,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"657e40fe890c2baa1705b45084a93a70b98842eb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"w601sxs\/b1ade-1b",
        "Average":32.59,
        "ARC":28.58,
        "HellaSwag":46.08,
        "MMLU":25.11,
        "TruthfulQA":41.34,
        "Winogrande":53.83,
        "GSM8K":0.61,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":0.91,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"b4b0fd71589e6590089e1ec14a840ecab10894ae"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TFLai\/gpt-neo-1.3B-4bit-alpaca",
        "Average":32.58,
        "ARC":28.24,
        "HellaSwag":46.35,
        "MMLU":25.19,
        "TruthfulQA":39.26,
        "Winogrande":56.2,
        "GSM8K":0.23,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"4bit",
        "Hub License":"?",
        "#Params (B)":1.3,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":false,
        "Model Sha":"137d483d1dc757c81c59bd190016f7c5df01f978"
    },
    {
        "T":"?",
        "Model":"bigscience\/bloom-1b1",
        "Average":32.47,
        "ARC":28.33,
        "HellaSwag":42.78,
        "MMLU":26.7,
        "TruthfulQA":41.8,
        "Winogrande":55.01,
        "GSM8K":0.23,
        "Type":"",
        "Architecture":"BloomForCausalLM",
        "Precision":"float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":1.06,
        "Hub \u2764\ufe0f":41.0,
        "Available on the Hub":true,
        "Model Sha":"6f4195539db0eef1c9d010289f32e0645d9a2354"
    },
    {
        "T":"\u2b55",
        "Model":"rinna\/bilingual-gpt-neox-4b-instruction-sft",
        "Average":32.46,
        "ARC":28.07,
        "HellaSwag":47.5,
        "MMLU":23.12,
        "TruthfulQA":43.76,
        "Winogrande":52.33,
        "GSM8K":0.0,
        "Type":"instruction-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":3.8,
        "Hub \u2764\ufe0f":14.0,
        "Available on the Hub":true,
        "Model Sha":"c20e42bd49a3b1b0d0a07151899a322c4760e871"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Kunhao\/pile-7b-250b-tokens",
        "Average":32.44,
        "ARC":29.27,
        "HellaSwag":46.29,
        "MMLU":25.25,
        "TruthfulQA":40.49,
        "Winogrande":52.8,
        "GSM8K":0.53,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":5.87,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"caefdf7a7c177905b0b16fbe9d4c7ba08def97c2"
    },
    {
        "T":"?",
        "Model":"MBZUAI\/LaMini-GPT-774M",
        "Average":32.43,
        "ARC":27.65,
        "HellaSwag":43.81,
        "MMLU":26.3,
        "TruthfulQA":40.26,
        "Winogrande":56.59,
        "GSM8K":0.0,
        "Type":"",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.77,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"4f3bd4b37d249e6aa335be677afd39f417e05b5d"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Salesforce\/codegen-6B-multi",
        "Average":32.43,
        "ARC":27.22,
        "HellaSwag":41.11,
        "MMLU":25.71,
        "TruthfulQA":45.65,
        "Winogrande":53.91,
        "GSM8K":0.99,
        "Type":"pretrained",
        "Architecture":"CodeGenForCausalLM",
        "Precision":"float16",
        "Hub License":"bsd-3-clause",
        "#Params (B)":6.85,
        "Hub \u2764\ufe0f":18.0,
        "Available on the Hub":true,
        "Model Sha":"2d58b1e73791e8f0be7ea59c2720dccb6f4d0f06"
    },
    {
        "T":"?",
        "Model":"FabbriSimo01\/Bloom_1b_Quantized",
        "Average":32.41,
        "ARC":27.73,
        "HellaSwag":42.83,
        "MMLU":26.28,
        "TruthfulQA":41.82,
        "Winogrande":55.64,
        "GSM8K":0.15,
        "Type":"",
        "Architecture":"BloomForCausalLM",
        "Precision":"float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":1.06,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f31188966c6735bd894edacfee8371a6eaf7dbc7"
    },
    {
        "T":"\u2b55",
        "Model":"deepseek-ai\/deepseek-coder-1.3b-instruct",
        "Average":32.4,
        "ARC":28.58,
        "HellaSwag":39.87,
        "MMLU":28.47,
        "TruthfulQA":44.02,
        "Winogrande":52.41,
        "GSM8K":1.06,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":1.3,
        "Hub \u2764\ufe0f":28.0,
        "Available on the Hub":true,
        "Model Sha":"e04e04028d6345ab3225644cd615e2573ffb9b8c"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"Locutusque\/gpt2-large-conversational",
        "Average":32.33,
        "ARC":26.96,
        "HellaSwag":44.98,
        "MMLU":26.33,
        "TruthfulQA":39.6,
        "Winogrande":56.04,
        "GSM8K":0.08,
        "Type":"RL-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"bfloat16",
        "Hub License":"openrail",
        "#Params (B)":0.77,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"6674ad1ed9f518054561b866172eb88b7a769413"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"rinna\/bilingual-gpt-neox-4b-8k",
        "Average":32.23,
        "ARC":28.58,
        "HellaSwag":43.94,
        "MMLU":25.38,
        "TruthfulQA":47.48,
        "Winogrande":47.99,
        "GSM8K":0.0,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":3.95,
        "Hub \u2764\ufe0f":22.0,
        "Available on the Hub":true,
        "Model Sha":"ad56d7fc86db4ad5a7036bc9f80e11cd6f435a60"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Rachneet\/gpt2-xl-alpaca",
        "Average":32.21,
        "ARC":26.79,
        "HellaSwag":43.85,
        "MMLU":26.31,
        "TruthfulQA":39.4,
        "Winogrande":56.91,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":1.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"a1a19acc0ef161bfa35f460c15ed3015595714d8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Devio\/test-3b",
        "Average":32.2,
        "ARC":27.65,
        "HellaSwag":44.79,
        "MMLU":23.53,
        "TruthfulQA":41.42,
        "Winogrande":55.49,
        "GSM8K":0.3,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":3.5,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"b81c038ee2fa2addd285acde08b1a7ca3cb2854d"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"rinna\/bilingual-gpt-neox-4b",
        "Average":32.14,
        "ARC":29.18,
        "HellaSwag":43.73,
        "MMLU":23.1,
        "TruthfulQA":45.0,
        "Winogrande":51.85,
        "GSM8K":0.0,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":3.95,
        "Hub \u2764\ufe0f":21.0,
        "Available on the Hub":true,
        "Model Sha":"f02f6f3c8da0093f3c1ce59220409bc2fa9fbb17"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"stabilityai\/stablelm-tuned-alpha-3b",
        "Average":32.14,
        "ARC":27.82,
        "HellaSwag":44.06,
        "MMLU":23.08,
        "TruthfulQA":42.33,
        "Winogrande":55.01,
        "GSM8K":0.53,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":[
            "cc-by-nc-sa-4.0"
        ],
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":106.0,
        "Available on the Hub":true,
        "Model Sha":"d1c03d2114451d562416b9efe4281d319ceff99e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mohammed-Altaf\/Medical-ChatBot",
        "Average":32.13,
        "ARC":30.55,
        "HellaSwag":38.63,
        "MMLU":25.98,
        "TruthfulQA":41.25,
        "Winogrande":55.41,
        "GSM8K":0.99,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"8bit",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"9e2d5d7a6189762164690a2fe714b00ce497b253"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mohammed-Altaf\/Medical-ChatBot",
        "Average":31.98,
        "ARC":30.46,
        "HellaSwag":38.6,
        "MMLU":25.96,
        "TruthfulQA":41.04,
        "Winogrande":54.85,
        "GSM8K":0.99,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"9e2d5d7a6189762164690a2fe714b00ce497b253"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mohammed-Altaf\/Medical-ChatBot",
        "Average":31.87,
        "ARC":30.46,
        "HellaSwag":38.55,
        "MMLU":25.91,
        "TruthfulQA":41.02,
        "Winogrande":54.22,
        "GSM8K":1.06,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"bfloat16",
        "Hub License":"mit",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"9e2d5d7a6189762164690a2fe714b00ce497b253"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"PY007\/TinyLlama-1.1B-step-50K-105b",
        "Average":31.86,
        "ARC":25.85,
        "HellaSwag":44.1,
        "MMLU":26.78,
        "TruthfulQA":39.51,
        "Winogrande":54.38,
        "GSM8K":0.53,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.1,
        "Hub \u2764\ufe0f":102.0,
        "Available on the Hub":true,
        "Model Sha":"c1f1ef67c12e4bb85fe0bdf1747c645a202cc118"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mikivis\/gpt2-large-lora-sft",
        "Average":31.82,
        "ARC":26.79,
        "HellaSwag":44.15,
        "MMLU":25.82,
        "TruthfulQA":39.06,
        "Winogrande":55.09,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":0.77,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"1c0c5a686f3c83692e033416197155557e4d3a0d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"YeungNLP\/firefly-bloom-2b6-v2",
        "Average":31.82,
        "ARC":27.65,
        "HellaSwag":39.23,
        "MMLU":25.24,
        "TruthfulQA":42.27,
        "Winogrande":54.78,
        "GSM8K":1.74,
        "Type":"fine-tuned",
        "Architecture":"BloomForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":2.48,
        "Hub \u2764\ufe0f":9.0,
        "Available on the Hub":true,
        "Model Sha":"8334b22c39937c0404e09dd22a867e2e2a6fc9e0"
    },
    {
        "T":"\u2b55",
        "Model":"llm-jp\/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0",
        "Average":31.77,
        "ARC":26.88,
        "HellaSwag":44.78,
        "MMLU":23.12,
        "TruthfulQA":45.19,
        "Winogrande":50.67,
        "GSM8K":0.0,
        "Type":"instruction-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"68282fe744c69ea2e4420a4a6833c0b9168215eb"
    },
    {
        "T":"?",
        "Model":"TheBloke\/orca_mini_13B-GPTQ",
        "Average":31.73,
        "ARC":27.3,
        "HellaSwag":25.85,
        "MMLU":25.31,
        "TruthfulQA":48.06,
        "Winogrande":63.77,
        "GSM8K":0.08,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":16.22,
        "Hub \u2764\ufe0f":43.0,
        "Available on the Hub":true,
        "Model Sha":"8ec18e5c597da86fa123c08b6e6bef7da6ec7440"
    },
    {
        "T":"\u2b55",
        "Model":"llm-jp\/llm-jp-13b-instruct-full-jaster-v1.0",
        "Average":31.63,
        "ARC":27.22,
        "HellaSwag":44.7,
        "MMLU":23.12,
        "TruthfulQA":44.69,
        "Winogrande":50.04,
        "GSM8K":0.0,
        "Type":"instruction-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":13.0,
        "Available on the Hub":true,
        "Model Sha":"b44eac954eac7ddbceba4f510325fd710c977eab"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/fairseq-dense-355M",
        "Average":31.58,
        "ARC":25.43,
        "HellaSwag":46.67,
        "MMLU":25.3,
        "TruthfulQA":39.19,
        "Winogrande":52.88,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"XGLMForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.4,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"24da1ea670f0638c2df911596e95c764bcd5fb44"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/pythia-410m",
        "Average":31.55,
        "ARC":26.19,
        "HellaSwag":40.85,
        "MMLU":27.25,
        "TruthfulQA":41.22,
        "Winogrande":53.12,
        "GSM8K":0.68,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.51,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"9879c9b5f8bea9051dcb0e68dff21493d67e9d4f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Yukang\/Llama-2-13b-chat-longlora-32k-sft",
        "Average":31.54,
        "ARC":26.54,
        "HellaSwag":26.1,
        "MMLU":23.12,
        "TruthfulQA":49.16,
        "Winogrande":64.33,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":19.0,
        "Available on the Hub":true,
        "Model Sha":"6f2924e354c3ab035aa2ff7c7e28d0e5327e2667"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aisquared\/dlite-v1-774m",
        "Average":31.51,
        "ARC":28.07,
        "HellaSwag":44.35,
        "MMLU":25.91,
        "TruthfulQA":36.11,
        "Winogrande":54.62,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.77,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"d3f5401d07965fb13c2cb8b458ffaed9a5a79c2d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mikivis\/gpt2-large-lora-stf4",
        "Average":31.5,
        "ARC":26.88,
        "HellaSwag":42.17,
        "MMLU":25.53,
        "TruthfulQA":40.84,
        "Winogrande":53.59,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.77,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"82eff3a62116fd589ad7319c9d75ff6b12f42f72"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"stabilityai\/stablelm-base-alpha-3b",
        "Average":31.5,
        "ARC":26.45,
        "HellaSwag":42.24,
        "MMLU":25.43,
        "TruthfulQA":40.5,
        "Winogrande":53.91,
        "GSM8K":0.45,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":[
            "cc-by-sa-4.0"
        ],
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":82.0,
        "Available on the Hub":true,
        "Model Sha":"99567ccfe45fabe467c71393aa6716106edb83c2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Yukang\/Llama-2-13b-chat-longlora-32k-sft",
        "Average":31.43,
        "ARC":26.11,
        "HellaSwag":26.17,
        "MMLU":23.12,
        "TruthfulQA":49.07,
        "Winogrande":64.09,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":19.0,
        "Available on the Hub":true,
        "Model Sha":"6f2924e354c3ab035aa2ff7c7e28d0e5327e2667"
    },
    {
        "T":"?",
        "Model":"facebook\/xglm-1.7B",
        "Average":31.42,
        "ARC":25.85,
        "HellaSwag":45.68,
        "MMLU":25.1,
        "TruthfulQA":37.21,
        "Winogrande":53.91,
        "GSM8K":0.76,
        "Type":"",
        "Architecture":"XGLMForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":1.73,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"d23a5e8e2164af31a84a26756b9b17f925143050"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mikivis\/gpt2-large-lora-sft2",
        "Average":31.33,
        "ARC":26.62,
        "HellaSwag":42.68,
        "MMLU":24.72,
        "TruthfulQA":40.31,
        "Winogrande":53.67,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.77,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"1244efb5d20765beb54f6b4a4e1426cf6d5daf44"
    },
    {
        "T":"\u2b55",
        "Model":"nicholasKluge\/Aira-2-774M",
        "Average":31.33,
        "ARC":28.75,
        "HellaSwag":40.8,
        "MMLU":25.1,
        "TruthfulQA":41.33,
        "Winogrande":52.01,
        "GSM8K":0.0,
        "Type":"instruction-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.77,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":false,
        "Model Sha":"f43044cfe7bf0827a176f0d319c63251c2b29373"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KnutJaegersberg\/gpt-2-xl-EvolInstruct",
        "Average":31.32,
        "ARC":27.39,
        "HellaSwag":38.46,
        "MMLU":25.67,
        "TruthfulQA":42.76,
        "Winogrande":53.51,
        "GSM8K":0.15,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"3e68735b9bfbca5c2e6a8e4367f003ab3d3c1512"
    },
    {
        "T":"?",
        "Model":"FabbriSimo01\/Cerebras_1.3b_Quantized",
        "Average":31.31,
        "ARC":25.94,
        "HellaSwag":38.56,
        "MMLU":26.79,
        "TruthfulQA":42.67,
        "Winogrande":53.51,
        "GSM8K":0.38,
        "Type":"",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":1.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"e2126a42a1c8a938553dd513e4adafec41cb793e"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"cerebras\/Cerebras-GPT-1.3B",
        "Average":31.3,
        "ARC":26.28,
        "HellaSwag":38.54,
        "MMLU":26.59,
        "TruthfulQA":42.7,
        "Winogrande":53.43,
        "GSM8K":0.23,
        "Type":"pretrained",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.32,
        "Hub \u2764\ufe0f":43.0,
        "Available on the Hub":true,
        "Model Sha":"5b95400ee8d1e3cc9f79f0dec7182ed9c1009c34"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/pythia-410m-deduped",
        "Average":31.29,
        "ARC":24.83,
        "HellaSwag":41.29,
        "MMLU":25.99,
        "TruthfulQA":40.95,
        "Winogrande":54.38,
        "GSM8K":0.3,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.51,
        "Hub \u2764\ufe0f":13.0,
        "Available on the Hub":true,
        "Model Sha":"c4fc8d586d62df497f1f9b69d66d3ca419992d3e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aisquared\/dlite-v2-355m",
        "Average":31.2,
        "ARC":28.33,
        "HellaSwag":40.54,
        "MMLU":26.77,
        "TruthfulQA":38.76,
        "Winogrande":52.8,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.36,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"f51d310aebc16a9fe0d999d2a437b5faff635716"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"winglian\/basilisk-4b",
        "Average":31.15,
        "ARC":25.85,
        "HellaSwag":39.6,
        "MMLU":24.61,
        "TruthfulQA":43.74,
        "Winogrande":53.12,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":3.37,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"b91c2e5389f4f0ce2d6042fdce5927343d8dcb06"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PygmalionAI\/pygmalion-1.3b",
        "Average":31.14,
        "ARC":28.07,
        "HellaSwag":46.96,
        "MMLU":24.12,
        "TruthfulQA":37.64,
        "Winogrande":50.04,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"agpl-3.0",
        "#Params (B)":1.52,
        "Hub \u2764\ufe0f":51.0,
        "Available on the Hub":true,
        "Model Sha":"bef2c90128c00ff6f16c0f397463423b7d988e17"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mikivis\/gpt2-large-lora-sft1",
        "Average":31.01,
        "ARC":24.66,
        "HellaSwag":42.67,
        "MMLU":24.89,
        "TruthfulQA":39.37,
        "Winogrande":54.46,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.77,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"8e26a8d2dc1661d87a8652c75f00b805d63e7330"
    },
    {
        "T":"\u2b55",
        "Model":"nicholasKluge\/Aira-2-355M",
        "Average":31.0,
        "ARC":27.56,
        "HellaSwag":38.92,
        "MMLU":27.26,
        "TruthfulQA":38.53,
        "Winogrande":53.75,
        "GSM8K":0.0,
        "Type":"instruction-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.36,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"2479f5b1bb62251ec88e60182ba81390a4c19cf9"
    },
    {
        "T":null,
        "Model":"baseline",
        "Average":31.0,
        "ARC":25.0,
        "HellaSwag":25.0,
        "MMLU":25.0,
        "TruthfulQA":25.0,
        "Winogrande":50.0,
        "GSM8K":0.21,
        "Type":"",
        "Architecture":null,
        "Precision":null,
        "Hub License":null,
        "#Params (B)":null,
        "Hub \u2764\ufe0f":null,
        "Available on the Hub":null,
        "Model Sha":"N\/A"
    },
    {
        "T":"\u2b55",
        "Model":"SummerSigh\/GPTNeo350M-Instruct-SFT",
        "Average":31.0,
        "ARC":25.94,
        "HellaSwag":38.55,
        "MMLU":25.76,
        "TruthfulQA":45.25,
        "Winogrande":50.2,
        "GSM8K":0.3,
        "Type":"instruction-tuned",
        "Architecture":"GPTNeoForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.46,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"5e41660ced3edf13c47e933112efd280b710b977"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"postbot\/emailgen-pythia-410m-deduped",
        "Average":30.93,
        "ARC":27.9,
        "HellaSwag":40.04,
        "MMLU":27.35,
        "TruthfulQA":38.2,
        "Winogrande":52.09,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.51,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"e0208b02990c49138350da791f0b6fcb8a65e738"
    },
    {
        "T":"\u2b55",
        "Model":"AI-Sweden-Models\/gpt-sw3-356m-instruct",
        "Average":30.93,
        "ARC":26.96,
        "HellaSwag":38.01,
        "MMLU":25.53,
        "TruthfulQA":40.74,
        "Winogrande":52.57,
        "GSM8K":1.74,
        "Type":"instruction-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"bfloat16",
        "Hub License":"other",
        "#Params (B)":0.47,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"85615b7c700ca7f38c32db8c7efabfa97668f1c2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Corianas\/Quokka_1.3b",
        "Average":30.86,
        "ARC":27.73,
        "HellaSwag":37.91,
        "MMLU":26.66,
        "TruthfulQA":40.14,
        "Winogrande":52.72,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.42,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"8a8d738e841a524d658897d89b9e39e7b9272ed8"
    },
    {
        "T":"?",
        "Model":"Corianas\/1.3b",
        "Average":30.76,
        "ARC":27.3,
        "HellaSwag":38.3,
        "MMLU":26.77,
        "TruthfulQA":39.02,
        "Winogrande":53.04,
        "GSM8K":0.15,
        "Type":"",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.42,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"9831f95df82155ef95ff46a505506bf6194b131a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cmarkea\/bloomz-560m-sft-chat",
        "Average":30.72,
        "ARC":27.47,
        "HellaSwag":37.05,
        "MMLU":23.93,
        "TruthfulQA":42.35,
        "Winogrande":53.51,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"BloomForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":0.56,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"e2bbcbdd534c7d75b7d2f9408e74f6682cf3a05e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"player1537\/dolphinette",
        "Average":30.65,
        "ARC":24.91,
        "HellaSwag":37.33,
        "MMLU":25.37,
        "TruthfulQA":42.08,
        "Winogrande":54.22,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"BloomForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.56,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"20529d47b0a82343014727edd1639a9a6a6b09e6"
    },
    {
        "T":"?",
        "Model":"bigscience\/bloomz-560m",
        "Average":30.63,
        "ARC":23.55,
        "HellaSwag":36.31,
        "MMLU":25.1,
        "TruthfulQA":45.69,
        "Winogrande":53.12,
        "GSM8K":0.0,
        "Type":"",
        "Architecture":"BloomForCausalLM",
        "Precision":"float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":0.56,
        "Hub \u2764\ufe0f":81.0,
        "Available on the Hub":true,
        "Model Sha":"a2845d7e13dd12efae154a9f1c63fcc2e0cc4b05"
    },
    {
        "T":"?",
        "Model":"TheBloke\/medalpaca-13B-GPTQ-4bit",
        "Average":30.62,
        "ARC":29.35,
        "HellaSwag":26.32,
        "MMLU":25.44,
        "TruthfulQA":49.51,
        "Winogrande":53.12,
        "GSM8K":0.0,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":16.22,
        "Hub \u2764\ufe0f":28.0,
        "Available on the Hub":true,
        "Model Sha":"12190f743a19e91dfe1f5c77abc0c1bf486073dd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aisquared\/dlite-v1-355m",
        "Average":30.54,
        "ARC":27.13,
        "HellaSwag":39.07,
        "MMLU":27.12,
        "TruthfulQA":37.13,
        "Winogrande":52.8,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.36,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"c5f4b5a61e6a66a5c7613164d99a70db5bf7e9a2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"xhyi\/PT_GPTNEO350_ATG",
        "Average":30.46,
        "ARC":25.43,
        "HellaSwag":37.59,
        "MMLU":24.79,
        "TruthfulQA":43.05,
        "Winogrande":51.46,
        "GSM8K":0.45,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.36,
        "Hub \u2764\ufe0f":18.0,
        "Available on the Hub":true,
        "Model Sha":"56ab08aaa6802d0f830d42c352d5d536be72811d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TehVenom\/DiffMerge-DollyGPT-Pygmalion",
        "Average":30.45,
        "ARC":23.63,
        "HellaSwag":34.38,
        "MMLU":24.41,
        "TruthfulQA":46.48,
        "Winogrande":53.83,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":5.84,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"6a00b371146d4bd2903890814485ee1b775162e7"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"RWKV\/rwkv-4-430m-pile",
        "Average":30.45,
        "ARC":26.71,
        "HellaSwag":40.01,
        "MMLU":24.85,
        "TruthfulQA":39.58,
        "Winogrande":51.14,
        "GSM8K":0.38,
        "Type":"pretrained",
        "Architecture":"RwkvForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.38,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"a4f6ec80438d4262d1bbc8f385feb2ef1a4a9d6b"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"TheTravellingEngineer\/bloom-560m-RLHF-v2",
        "Average":30.43,
        "ARC":26.45,
        "HellaSwag":37.67,
        "MMLU":23.95,
        "TruthfulQA":43.51,
        "Winogrande":50.91,
        "GSM8K":0.08,
        "Type":"RL-tuned",
        "Architecture":"BloomForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.56,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"7128cbfcdaf67f1eff27e45d875c35e7b47618db"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"AI-Sweden-Models\/gpt-sw3-356m",
        "Average":30.41,
        "ARC":23.63,
        "HellaSwag":37.05,
        "MMLU":25.93,
        "TruthfulQA":42.55,
        "Winogrande":53.04,
        "GSM8K":0.23,
        "Type":"pretrained",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"bfloat16",
        "Hub License":"other",
        "#Params (B)":0.47,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"15ba8a812d3eb265342f62cb0ee9ab6a45fdbd89"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"robowaifudev\/megatron-gpt2-345m",
        "Average":30.4,
        "ARC":24.23,
        "HellaSwag":39.18,
        "MMLU":24.32,
        "TruthfulQA":41.51,
        "Winogrande":52.96,
        "GSM8K":0.23,
        "Type":"pretrained",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.38,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"b39f8d00fb9f33da4271be2035da848da896a23b"
    },
    {
        "T":"\u2b55",
        "Model":"uukuguy\/speechless-codellama-orca-airoboros-13b-0.10e",
        "Average":30.36,
        "ARC":29.44,
        "HellaSwag":25.71,
        "MMLU":25.43,
        "TruthfulQA":49.64,
        "Winogrande":51.93,
        "GSM8K":0.0,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"dbd1d1f7ad7b6b359f8246141650b25ca0bb8cbb"
    },
    {
        "T":"\u2b55",
        "Model":"KnutJaegersberg\/megatron-gpt2-345m-evol_instruct_v2",
        "Average":30.31,
        "ARC":26.37,
        "HellaSwag":38.39,
        "MMLU":23.6,
        "TruthfulQA":41.19,
        "Winogrande":52.33,
        "GSM8K":0.0,
        "Type":"instruction-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.36,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"2866eeaaf62014a7a6e939d18b6e27f44df48428"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"porkorbeef\/Llama-2-13b-sf",
        "Average":30.22,
        "ARC":29.52,
        "HellaSwag":26.49,
        "MMLU":25.98,
        "TruthfulQA":48.97,
        "Winogrande":50.36,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaModel",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"06253ee259e6b205c4734ab6ec3fa850737b2110"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-codellama-orca-airoboros-13b-0.10e",
        "Average":30.22,
        "ARC":29.27,
        "HellaSwag":25.74,
        "MMLU":25.69,
        "TruthfulQA":49.61,
        "Winogrande":50.99,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"dbd1d1f7ad7b6b359f8246141650b25ca0bb8cbb"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"yec019\/fbopt-350m-8bit",
        "Average":30.21,
        "ARC":23.55,
        "HellaSwag":36.6,
        "MMLU":26.22,
        "TruthfulQA":40.97,
        "Winogrande":52.64,
        "GSM8K":1.29,
        "Type":"pretrained",
        "Architecture":"OPTForCausalLM",
        "Precision":"8bit",
        "Hub License":"unknown",
        "#Params (B)":0.33,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"305f804054d75a406a85a568ea99dca17cfc998d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KnutJaegersberg\/RWKV-4-PilePlus-430M-20230520-6162-1018Gtokens-ctx4098",
        "Average":30.18,
        "ARC":26.02,
        "HellaSwag":40.39,
        "MMLU":24.45,
        "TruthfulQA":37.57,
        "Winogrande":52.41,
        "GSM8K":0.23,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.38,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"e31777c9d3b8c5c9f803b23f49550c009cbdcf6d"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"ahxt\/llama2_xs_460M_experimental",
        "Average":30.17,
        "ARC":24.91,
        "HellaSwag":38.47,
        "MMLU":26.17,
        "TruthfulQA":41.59,
        "Winogrande":49.88,
        "GSM8K":0.0,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.41,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"c8db281477559f5c969a9be794ce236f8a99e1a0"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"uukuguy\/Orca-2-7b-f16",
        "Average":30.15,
        "ARC":29.61,
        "HellaSwag":25.62,
        "MMLU":26.7,
        "TruthfulQA":48.36,
        "Winogrande":50.59,
        "GSM8K":0.0,
        "Type":"pretrained",
        "Architecture":"LlamaModel",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f6b2f717467dc12b2b19cad90ed4362153863ad9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/OPT-350M-Erebus",
        "Average":30.14,
        "ARC":23.81,
        "HellaSwag":34.35,
        "MMLU":26.23,
        "TruthfulQA":43.58,
        "Winogrande":52.57,
        "GSM8K":0.3,
        "Type":"fine-tuned",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":0.33,
        "Hub \u2764\ufe0f":11.0,
        "Available on the Hub":true,
        "Model Sha":"83ce2f4e78d308968cf7ecd03d86a1f64aea8336"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"TheTravellingEngineer\/bloom-1b1-RLHF",
        "Average":30.14,
        "ARC":27.99,
        "HellaSwag":26.19,
        "MMLU":26.86,
        "TruthfulQA":48.88,
        "Winogrande":50.91,
        "GSM8K":0.0,
        "Type":"RL-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"65bd72580520a1d4a0c19fcb23f68c1f28464e1b"
    },
    {
        "T":"?",
        "Model":"bigscience\/bloom-560m",
        "Average":30.13,
        "ARC":24.74,
        "HellaSwag":37.15,
        "MMLU":24.22,
        "TruthfulQA":42.44,
        "Winogrande":51.93,
        "GSM8K":0.3,
        "Type":"",
        "Architecture":"BloomForCausalLM",
        "Precision":"float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":0.56,
        "Hub \u2764\ufe0f":263.0,
        "Available on the Hub":true,
        "Model Sha":"4f42c91d806a19ae1a46af6c3fb5f4990d884cd6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yhyhy3\/med-orca-instruct-33b",
        "Average":30.12,
        "ARC":28.84,
        "HellaSwag":25.63,
        "MMLU":26.5,
        "TruthfulQA":49.26,
        "Winogrande":50.51,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaModel",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"1d636881854338e571825226c712180da06be72c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"porkorbeef\/Llama-2-13b",
        "Average":30.11,
        "ARC":29.35,
        "HellaSwag":26.35,
        "MMLU":24.94,
        "TruthfulQA":48.32,
        "Winogrande":51.7,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaModel",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"06253ee259e6b205c4734ab6ec3fa850737b2110"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"health360\/Healix-410M",
        "Average":30.1,
        "ARC":25.09,
        "HellaSwag":32.02,
        "MMLU":24.94,
        "TruthfulQA":44.42,
        "Winogrande":54.14,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":0.35,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"df5a3cec54a0bdd22e1644bfe576c7b58eca6bfd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"marcchew\/Marcoroni-7B-LaMini-80K",
        "Average":30.09,
        "ARC":28.75,
        "HellaSwag":26.13,
        "MMLU":24.46,
        "TruthfulQA":49.71,
        "Winogrande":51.46,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"8bit",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"ea7a283403ec1a40570bfc25f2c4b8fcb089b6bb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"doas\/test5",
        "Average":30.06,
        "ARC":28.41,
        "HellaSwag":26.63,
        "MMLU":25.36,
        "TruthfulQA":47.34,
        "Winogrande":52.64,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaModel",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"b0dae937b7137790d8946794375e1affd51c760a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MBZUAI\/lamini-cerebras-1.3b",
        "Average":30.05,
        "ARC":26.88,
        "HellaSwag":37.96,
        "MMLU":28.43,
        "TruthfulQA":36.45,
        "Winogrande":50.59,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":1.32,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"502e70081df53edc8a9156acf5a26a11a9dad8fb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KnutJaegersberg\/megatron-GPT-2-345m-EvolInstruct",
        "Average":30.01,
        "ARC":24.06,
        "HellaSwag":35.12,
        "MMLU":24.48,
        "TruthfulQA":41.25,
        "Winogrande":54.78,
        "GSM8K":0.38,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.38,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"dc95fda9f1e51d94870e28751e35410c66563d18"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"facebook\/opt-350m",
        "Average":30.01,
        "ARC":23.55,
        "HellaSwag":36.73,
        "MMLU":26.02,
        "TruthfulQA":40.83,
        "Winogrande":52.64,
        "GSM8K":0.3,
        "Type":"pretrained",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":0.33,
        "Hub \u2764\ufe0f":75.0,
        "Available on the Hub":true,
        "Model Sha":"cb32f77e905cccbca1d970436fb0f5e6b58ee3c5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yeen214\/test_llama2_ko_7b",
        "Average":29.99,
        "ARC":29.95,
        "HellaSwag":26.94,
        "MMLU":25.62,
        "TruthfulQA":49.03,
        "Winogrande":48.38,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.67,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"45901e1d6ccb22f5ed8aec3f9dd366823fdd1c33"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vikp\/phi2",
        "Average":29.98,
        "ARC":22.87,
        "HellaSwag":30.7,
        "MMLU":27.55,
        "TruthfulQA":46.1,
        "Winogrande":52.01,
        "GSM8K":0.68,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":1.31,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"9fd01ce09da870fc66af88616d43e53db642ef46"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"uukuguy\/speechless-codellama-orca-platypus-13b-0.10e",
        "Average":29.96,
        "ARC":28.92,
        "HellaSwag":25.76,
        "MMLU":25.28,
        "TruthfulQA":49.22,
        "Winogrande":50.59,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"119abfc73f9ce541a40779f167fe21e95faed4e8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"IDEA-CCNL\/Ziya-LLaMA-13B-Pretrain-v1",
        "Average":29.96,
        "ARC":27.99,
        "HellaSwag":26.0,
        "MMLU":27.04,
        "TruthfulQA":48.59,
        "Winogrande":50.12,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"gpl-3.0",
        "#Params (B)":12.89,
        "Hub \u2764\ufe0f":20.0,
        "Available on the Hub":true,
        "Model Sha":"826e83e411df32f358893ab21f5eae680499ae9a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"PygmalionAI\/pygmalion-350m",
        "Average":29.95,
        "ARC":25.0,
        "HellaSwag":37.8,
        "MMLU":25.68,
        "TruthfulQA":40.41,
        "Winogrande":50.28,
        "GSM8K":0.53,
        "Type":"fine-tuned",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.33,
        "Hub \u2764\ufe0f":48.0,
        "Available on the Hub":true,
        "Model Sha":"d65832d913f6b396e2ffb64c373d9383c9da9303"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hoskinson-center\/proofGPT-v0.1",
        "Average":29.94,
        "ARC":22.87,
        "HellaSwag":28.66,
        "MMLU":25.96,
        "TruthfulQA":51.64,
        "Winogrande":50.43,
        "GSM8K":0.08,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":1.31,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"1e4dd330ca90c0ef6d77ca71bd49cbe3d71f26b8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"marcchew\/LaMini-40k-Platypus2-7B",
        "Average":29.91,
        "ARC":28.5,
        "HellaSwag":26.32,
        "MMLU":27.04,
        "TruthfulQA":47.39,
        "Winogrande":50.2,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"8bit",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"e8c03e43eab479a216b5f4f182a711c3624f38bd"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/OPT-350M-Nerys-v2",
        "Average":29.9,
        "ARC":23.63,
        "HellaSwag":35.49,
        "MMLU":25.91,
        "TruthfulQA":42.08,
        "Winogrande":51.62,
        "GSM8K":0.68,
        "Type":"fine-tuned",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":0.33,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"59b1019c35ab17a7d77ea1ad32b45a8375ba6e89"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"postbot\/gpt2-medium-emailgen",
        "Average":29.87,
        "ARC":26.45,
        "HellaSwag":34.31,
        "MMLU":24.1,
        "TruthfulQA":43.96,
        "Winogrande":50.43,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"bfloat16",
        "Hub License":[
            "apache-2.0"
        ],
        "#Params (B)":0.38,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"1b9b03d00b2b300d3c04c37fe3782c180ef51a27"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"doas\/test2",
        "Average":29.87,
        "ARC":29.61,
        "HellaSwag":26.65,
        "MMLU":24.34,
        "TruthfulQA":48.49,
        "Winogrande":50.12,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaModel",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f08d224deae510ebf1408ce38bc2610b1e4c77eb"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"TheTravellingEngineer\/bloom-560m-RLHF",
        "Average":29.86,
        "ARC":24.4,
        "HellaSwag":36.96,
        "MMLU":23.63,
        "TruthfulQA":40.76,
        "Winogrande":53.12,
        "GSM8K":0.3,
        "Type":"RL-tuned",
        "Architecture":"BloomForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.56,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"b1769e92f325d8a28e7db1c21f133e6c85b84e78"
    },
    {
        "T":"?",
        "Model":"TheBloke\/WizardLM-7B-uncensored-GPTQ",
        "Average":29.86,
        "ARC":28.5,
        "HellaSwag":25.37,
        "MMLU":24.85,
        "TruthfulQA":50.86,
        "Winogrande":49.57,
        "GSM8K":0.0,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":9.04,
        "Hub \u2764\ufe0f":150.0,
        "Available on the Hub":true,
        "Model Sha":"cc30c031fd795ee3d3a50312ab4549415bfbdb46"
    },
    {
        "T":"\u2b55",
        "Model":"uukuguy\/speechless-codellama-orca-platypus-13b-0.10e",
        "Average":29.83,
        "ARC":28.75,
        "HellaSwag":25.88,
        "MMLU":25.36,
        "TruthfulQA":49.27,
        "Winogrande":49.72,
        "GSM8K":0.0,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"119abfc73f9ce541a40779f167fe21e95faed4e8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"IDEA-CCNL\/Ziya-LLaMA-13B-v1",
        "Average":29.82,
        "ARC":27.73,
        "HellaSwag":25.96,
        "MMLU":27.04,
        "TruthfulQA":48.65,
        "Winogrande":49.57,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"gpl-3.0",
        "#Params (B)":12.89,
        "Hub \u2764\ufe0f":251.0,
        "Available on the Hub":true,
        "Model Sha":"fccf34387d2c9f2f95ff59ae380e6de3718e41ff"
    },
    {
        "T":"?",
        "Model":"Panchovix\/WizardLM-33B-V1.0-Uncensored-SuperHOT-8k",
        "Average":29.81,
        "ARC":25.43,
        "HellaSwag":31.97,
        "MMLU":23.43,
        "TruthfulQA":47.0,
        "Winogrande":51.07,
        "GSM8K":0.0,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"b6d0002b10d43ab48aa14e365d9e7b40655ec160"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"marcchew\/Marcoroni-7B-LaMini-40K",
        "Average":29.78,
        "ARC":27.65,
        "HellaSwag":26.23,
        "MMLU":26.92,
        "TruthfulQA":47.4,
        "Winogrande":50.51,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"8bit",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"27868e4faed5d68d059c8c57dbd3e24e4933ca28"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MayaPH\/FinOPT-Franklin",
        "Average":29.78,
        "ARC":27.73,
        "HellaSwag":24.91,
        "MMLU":23.12,
        "TruthfulQA":52.4,
        "Winogrande":50.51,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":1.32,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"1b13331834190bfe49a176f1661ba4d8309a5051"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"NEU-HAI\/mental-alpaca",
        "Average":29.77,
        "ARC":28.58,
        "HellaSwag":26.02,
        "MMLU":27.04,
        "TruthfulQA":48.61,
        "Winogrande":48.38,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f5f24d4a11ed52b4a224f365b6a694cf4e27c1bc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BreadAi\/PM_modelV2",
        "Average":29.77,
        "ARC":25.09,
        "HellaSwag":26.45,
        "MMLU":26.14,
        "TruthfulQA":51.36,
        "Winogrande":49.57,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"4edde209eea33af491206f8651c0c47e70e08289"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"hoskinson-center\/proofGPT-v0.1-6.7B",
        "Average":29.72,
        "ARC":23.29,
        "HellaSwag":28.45,
        "MMLU":24.57,
        "TruthfulQA":50.87,
        "Winogrande":51.14,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":6.65,
        "Hub \u2764\ufe0f":9.0,
        "Available on the Hub":true,
        "Model Sha":"02f405f08ca0e5b1aaa90a7c3b11303b5f245102"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"danielpark\/gorani-100k-llama2-13b-instruct",
        "Average":29.69,
        "ARC":28.07,
        "HellaSwag":26.3,
        "MMLU":25.17,
        "TruthfulQA":48.96,
        "Winogrande":49.64,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f7d38ee654e505ad7a454f192d5e3d85cb60b3b8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TFLai\/gpt2-turkish-uncased",
        "Average":29.68,
        "ARC":24.49,
        "HellaSwag":25.08,
        "MMLU":26.59,
        "TruthfulQA":52.3,
        "Winogrande":49.64,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":0.14,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"4807e7df1dfb9d60c6d98e3cfeff62cb6b9a1579"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"porkorbeef\/Llama-2-13b-12_153950",
        "Average":29.68,
        "ARC":28.58,
        "HellaSwag":26.58,
        "MMLU":20.79,
        "TruthfulQA":49.03,
        "Winogrande":53.12,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaModel",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"ee9b0cf26f521b5cb2322d743880e8b6bfadb0b7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"marcchew\/Platypus-2-7B-LaMini-14K",
        "Average":29.64,
        "ARC":29.52,
        "HellaSwag":26.15,
        "MMLU":23.13,
        "TruthfulQA":48.29,
        "Winogrande":50.75,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"8bit",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"50199ba51c4d002cc86cf3fb2ac921ec52bf4828"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openbmb\/UltraRM-13b",
        "Average":29.58,
        "ARC":28.16,
        "HellaSwag":26.13,
        "MMLU":25.96,
        "TruthfulQA":47.91,
        "Winogrande":49.33,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaRewardModel",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"4b231ae58c15244e6e15f0d2f4e26ec37b846229"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vicgalle\/alpaca-7b",
        "Average":29.57,
        "ARC":28.07,
        "HellaSwag":25.83,
        "MMLU":25.31,
        "TruthfulQA":48.49,
        "Winogrande":49.72,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"7f22882125208d1f54765c21abf84fd162aa454a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"SebastianSchramm\/Cerebras-GPT-111M-instruction",
        "Average":29.57,
        "ARC":24.4,
        "HellaSwag":26.05,
        "MMLU":25.87,
        "TruthfulQA":49.46,
        "Winogrande":51.62,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.11,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"09f1ec782ae2243fc605b24eb13ec8d5e4fd2734"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"golaxy\/gogpt-560m",
        "Average":29.56,
        "ARC":26.37,
        "HellaSwag":31.86,
        "MMLU":25.29,
        "TruthfulQA":43.12,
        "Winogrande":50.75,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"BloomForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.56,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"82bd8b88b95068eee614a35b790388c5d2415705"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HWERI\/pythia-70m-deduped-cleansharegpt",
        "Average":29.56,
        "ARC":25.68,
        "HellaSwag":25.4,
        "MMLU":23.12,
        "TruthfulQA":51.15,
        "Winogrande":52.01,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.07,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"6ea42abd94cb0017918f6fe5e71d78bcb7c75548"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"facebook\/xglm-564M",
        "Average":29.55,
        "ARC":24.57,
        "HellaSwag":34.64,
        "MMLU":25.18,
        "TruthfulQA":40.43,
        "Winogrande":52.25,
        "GSM8K":0.23,
        "Type":"pretrained",
        "Architecture":"XGLMForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":0.56,
        "Hub \u2764\ufe0f":28.0,
        "Available on the Hub":true,
        "Model Sha":"f3059f01b98ccc877c673149e0178c0e957660f9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Abe13\/juniper-certificate-Llama-2-7b-chat-hf",
        "Average":29.55,
        "ARC":29.1,
        "HellaSwag":27.63,
        "MMLU":24.02,
        "TruthfulQA":48.23,
        "Winogrande":48.3,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"90ed388e5503c02f5e6ba8dbc7286687a85ce1c1"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"winglian\/Llama-2-3b-hf",
        "Average":29.53,
        "ARC":26.96,
        "HellaSwag":26.52,
        "MMLU":23.33,
        "TruthfulQA":50.71,
        "Winogrande":49.64,
        "GSM8K":0.0,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":3.37,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"293f071b223efd7959f9e1fac66285369aaa959d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ai-forever\/rugpt3large_based_on_gpt2",
        "Average":29.53,
        "ARC":22.61,
        "HellaSwag":32.84,
        "MMLU":24.9,
        "TruthfulQA":43.39,
        "Winogrande":53.12,
        "GSM8K":0.3,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.76,
        "Hub \u2764\ufe0f":60.0,
        "Available on the Hub":true,
        "Model Sha":"8201db0de8deb68f25e7309db04d163b71970494"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"bigcode\/santacoder",
        "Average":29.51,
        "ARC":26.28,
        "HellaSwag":25.6,
        "MMLU":25.89,
        "TruthfulQA":51.24,
        "Winogrande":48.07,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadCustomModel",
        "Precision":"float16",
        "Hub License":"bigcode-openrail-m",
        "#Params (B)":1.31,
        "Hub \u2764\ufe0f":300.0,
        "Available on the Hub":true,
        "Model Sha":"132eb6b6cedaf579c2f333f1ecd78a16d7e45978"
    },
    {
        "T":"?",
        "Model":"WangZeJun\/bloom-820m-chat",
        "Average":29.5,
        "ARC":23.38,
        "HellaSwag":34.16,
        "MMLU":25.98,
        "TruthfulQA":40.32,
        "Winogrande":53.2,
        "GSM8K":0.0,
        "Type":"",
        "Architecture":"BloomForCausalLM",
        "Precision":"float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":0.75,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"f98b1f9c1bd358dd837d05d443d992c495497606"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"huggingtweets\/bladeecity-jerma985",
        "Average":29.49,
        "ARC":22.87,
        "HellaSwag":30.53,
        "MMLU":26.56,
        "TruthfulQA":44.99,
        "Winogrande":52.01,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"9bf3a0db7f6bc960c51f2c0dc6fb66ed982b0180"
    },
    {
        "T":"?",
        "Model":"Panchovix\/airoboros-33b-gpt4-1.2-SuperHOT-8k",
        "Average":29.48,
        "ARC":24.66,
        "HellaSwag":31.23,
        "MMLU":23.13,
        "TruthfulQA":47.44,
        "Winogrande":50.43,
        "GSM8K":0.0,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"47c14f699cbbc9bd24458edd86eb70d87552b623"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"marcchew\/test1",
        "Average":29.48,
        "ARC":27.65,
        "HellaSwag":26.17,
        "MMLU":24.55,
        "TruthfulQA":48.33,
        "Winogrande":50.2,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"8bit",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"7444355ad764584ef05805f58ccf174bb03e0f46"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"codeparrot\/codeparrot",
        "Average":29.48,
        "ARC":21.67,
        "HellaSwag":28.34,
        "MMLU":25.55,
        "TruthfulQA":50.87,
        "Winogrande":50.2,
        "GSM8K":0.23,
        "Type":"pretrained",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":1.53,
        "Hub \u2764\ufe0f":87.0,
        "Available on the Hub":true,
        "Model Sha":"065248a99f051da363b1c2cbf05da943c8b6211b"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/gpt-neo-125m",
        "Average":29.47,
        "ARC":22.95,
        "HellaSwag":30.26,
        "MMLU":25.97,
        "TruthfulQA":45.58,
        "Winogrande":51.78,
        "GSM8K":0.3,
        "Type":"pretrained",
        "Architecture":"GPTNeoForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":0.15,
        "Hub \u2764\ufe0f":132.0,
        "Available on the Hub":true,
        "Model Sha":"6cb0d322a3a484e99667e7cb240e22f1ac036b99"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"beomi\/KoAlpaca-Polyglot-5.8B",
        "Average":29.46,
        "ARC":27.65,
        "HellaSwag":35.58,
        "MMLU":24.72,
        "TruthfulQA":39.74,
        "Winogrande":49.01,
        "GSM8K":0.08,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.0,
        "Hub \u2764\ufe0f":48.0,
        "Available on the Hub":true,
        "Model Sha":"1051dacf82ca9fba0ba4a4ff67f1d98a81ef7a2e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BreadAi\/MusePy-1-2",
        "Average":29.46,
        "ARC":25.77,
        "HellaSwag":25.94,
        "MMLU":25.22,
        "TruthfulQA":49.33,
        "Winogrande":50.51,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.04,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"6c1725158a74a41a10f21696a48510d45b4b425b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"porkorbeef\/Llama-2-13b-public",
        "Average":29.45,
        "ARC":29.95,
        "HellaSwag":26.65,
        "MMLU":22.74,
        "TruthfulQA":49.01,
        "Winogrande":48.38,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaModel",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"e1b32a8fcfc0f37fd5f50cf765151897574c73c7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"openbmb\/UltraLM-13b",
        "Average":29.45,
        "ARC":29.44,
        "HellaSwag":25.99,
        "MMLU":23.12,
        "TruthfulQA":48.61,
        "Winogrande":49.57,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":67.0,
        "Available on the Hub":true,
        "Model Sha":"2c732c2899fc329036d97e5c6f0a61eaff19d97d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MBZUAI\/lamini-neo-125m",
        "Average":29.44,
        "ARC":24.57,
        "HellaSwag":30.22,
        "MMLU":26.74,
        "TruthfulQA":42.85,
        "Winogrande":52.25,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":13.0,
        "Available on the Hub":true,
        "Model Sha":"f01e73ba67da96f6645be3067158cc493b0cbbcb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KoboldAI\/fairseq-dense-125M",
        "Average":29.41,
        "ARC":24.06,
        "HellaSwag":34.14,
        "MMLU":23.98,
        "TruthfulQA":43.72,
        "Winogrande":50.59,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"XGLMForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"c8fb975220512b34e7b4a9fc570ca333ddcaf9b5"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"bigcode\/tiny_starcoder_py",
        "Average":29.41,
        "ARC":20.99,
        "HellaSwag":28.77,
        "MMLU":26.79,
        "TruthfulQA":47.68,
        "Winogrande":51.22,
        "GSM8K":0.99,
        "Type":"pretrained",
        "Architecture":"GPTBigCodeForCausalLM",
        "Precision":"float16",
        "Hub License":"bigcode-openrail-m",
        "#Params (B)":0.16,
        "Hub \u2764\ufe0f":57.0,
        "Available on the Hub":true,
        "Model Sha":"8547527bef0bc927268c1653cce6948c5c242dd1"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"cerebras\/Cerebras-GPT-256M",
        "Average":29.38,
        "ARC":22.01,
        "HellaSwag":28.99,
        "MMLU":26.83,
        "TruthfulQA":45.98,
        "Winogrande":52.49,
        "GSM8K":0.0,
        "Type":"pretrained",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.26,
        "Hub \u2764\ufe0f":20.0,
        "Available on the Hub":true,
        "Model Sha":"d77812ac95aece1f1edef6745ae2a1b325ad01a4"
    },
    {
        "T":"?",
        "Model":"l3utterfly\/mistral-7b-v0.1-layla-v2",
        "Average":29.38,
        "ARC":27.05,
        "HellaSwag":25.87,
        "MMLU":25.38,
        "TruthfulQA":49.04,
        "Winogrande":48.93,
        "GSM8K":0.0,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"0f444610d26724271cd6dfb168667a405a2a3e6c"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/pythia-160m-deduped",
        "Average":29.38,
        "ARC":24.06,
        "HellaSwag":31.39,
        "MMLU":24.86,
        "TruthfulQA":44.34,
        "Winogrande":51.38,
        "GSM8K":0.23,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.21,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"582159a2dfe3e712a8d47ae83dec95ae3bde8e7e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"breadlicker45\/dough-base-001",
        "Average":29.37,
        "ARC":23.89,
        "HellaSwag":24.76,
        "MMLU":23.13,
        "TruthfulQA":53.4,
        "Winogrande":51.07,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.15,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"e42b65191f97d786eadaba450f1d34baea470734"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"breadlicker45\/dough-instruct-base-001",
        "Average":29.37,
        "ARC":23.89,
        "HellaSwag":24.76,
        "MMLU":23.13,
        "TruthfulQA":53.4,
        "Winogrande":51.07,
        "GSM8K":0.0,
        "Type":"RL-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.19,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"3e1b0bf0a887feeb342982eee4f6d8041772a7dd"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Deci\/DeciCoder-1b",
        "Average":29.37,
        "ARC":21.16,
        "HellaSwag":31.09,
        "MMLU":24.34,
        "TruthfulQA":47.05,
        "Winogrande":50.83,
        "GSM8K":1.74,
        "Type":"pretrained",
        "Architecture":"DeciLlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.11,
        "Hub \u2764\ufe0f":221.0,
        "Available on the Hub":true,
        "Model Sha":"af2ef45ef8cbe82eb7eb4074f260412bc14c7b11"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"amazingvince\/zephyr-smol_llama-100m-dpo-full",
        "Average":29.37,
        "ARC":25.0,
        "HellaSwag":28.54,
        "MMLU":25.18,
        "TruthfulQA":45.75,
        "Winogrande":51.07,
        "GSM8K":0.68,
        "Type":"RL-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.1,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"be3400c89d66ed66f0aa96f1b8131604c118b67b"
    },
    {
        "T":"?",
        "Model":"yhyhy3\/med-orca-instruct-33b",
        "Average":29.36,
        "ARC":27.39,
        "HellaSwag":25.89,
        "MMLU":25.37,
        "TruthfulQA":49.6,
        "Winogrande":47.91,
        "GSM8K":0.0,
        "Type":"",
        "Architecture":"LlamaModel",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"1d636881854338e571825226c712180da06be72c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"FINDA-FIT\/llama-r",
        "Average":29.34,
        "ARC":21.59,
        "HellaSwag":30.18,
        "MMLU":26.13,
        "TruthfulQA":45.38,
        "Winogrande":52.17,
        "GSM8K":0.61,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.69,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"6bdde9a227da60c2db803024d5b2e3a53a41cf0b"
    },
    {
        "T":"\u2b55",
        "Model":"nicholasKluge\/Aira-2-1B1",
        "Average":29.32,
        "ARC":23.21,
        "HellaSwag":26.97,
        "MMLU":24.86,
        "TruthfulQA":50.63,
        "Winogrande":50.28,
        "GSM8K":0.0,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":1.1,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"a53eb20b72ae86441566f99acc204d9bb527bf32"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TaylorAI\/Flash-Llama-30M-20001",
        "Average":29.31,
        "ARC":23.89,
        "HellaSwag":25.76,
        "MMLU":24.09,
        "TruthfulQA":51.29,
        "Winogrande":50.83,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"6ff84442217565875450bd7a0457121dcedf6b0b"
    },
    {
        "T":"\u2b55",
        "Model":"Harshvir\/LaMini-Neo-1.3B-Mental-Health_lora",
        "Average":29.3,
        "ARC":25.77,
        "HellaSwag":25.67,
        "MMLU":27.0,
        "TruthfulQA":48.21,
        "Winogrande":49.17,
        "GSM8K":0.0,
        "Type":"instruction-tuned",
        "Architecture":"GPTNeoForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":1.32,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"9f1c45d5ce88a8eaf7ec03b760a4adfb5fda07eb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"klosax\/pythia-160m-deduped-step92k-193bt",
        "Average":29.3,
        "ARC":24.23,
        "HellaSwag":32.33,
        "MMLU":24.54,
        "TruthfulQA":43.49,
        "Winogrande":50.83,
        "GSM8K":0.38,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"9eac24dad1bd7194e38ce8083a0197cee456456c"
    },
    {
        "T":"\u2b55",
        "Model":"bsp-albz\/llama2-13b-platypus-ckpt-1000",
        "Average":29.28,
        "ARC":28.16,
        "HellaSwag":26.55,
        "MMLU":23.17,
        "TruthfulQA":48.79,
        "Winogrande":49.01,
        "GSM8K":0.0,
        "Type":"instruction-tuned",
        "Architecture":"LlamaModel",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"d9f3e490df2134784afc3a86f5c617a9bab8db4d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"microsoft\/DialoGPT-large",
        "Average":29.27,
        "ARC":23.38,
        "HellaSwag":25.77,
        "MMLU":23.81,
        "TruthfulQA":50.27,
        "Winogrande":52.41,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":0.77,
        "Hub \u2764\ufe0f":203.0,
        "Available on the Hub":true,
        "Model Sha":"04e3e47b52dadbcf7688aa61a7ed0438ecf9184c"
    },
    {
        "T":"\u2b55",
        "Model":"voidful\/changpt-bart",
        "Average":29.27,
        "ARC":28.67,
        "HellaSwag":26.41,
        "MMLU":23.12,
        "TruthfulQA":47.94,
        "Winogrande":49.49,
        "GSM8K":0.0,
        "Type":"instruction-tuned",
        "Architecture":"BartForConditionalGeneration",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.18,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"e3d26f736b8b47d5275421be6133b81bef84db7d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MayaPH\/FinOPT-Lincoln",
        "Average":29.27,
        "ARC":26.71,
        "HellaSwag":25.6,
        "MMLU":23.0,
        "TruthfulQA":50.59,
        "Winogrande":49.72,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":0.33,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"7ddc381fa3968df22f72acb6cf03b75d3ac49661"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"victor123\/WizardLM-13B-1.0",
        "Average":29.27,
        "ARC":28.5,
        "HellaSwag":25.97,
        "MMLU":23.12,
        "TruthfulQA":48.61,
        "Winogrande":49.41,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":66.0,
        "Available on the Hub":true,
        "Model Sha":"2ea86d3c02ca0c2abb086a2145e1e85eaea4a23e"
    },
    {
        "T":"?",
        "Model":"postbot\/pythia-160m-hq-emails",
        "Average":29.26,
        "ARC":23.12,
        "HellaSwag":30.05,
        "MMLU":26.58,
        "TruthfulQA":45.51,
        "Winogrande":50.28,
        "GSM8K":0.0,
        "Type":"",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"6eeded627780b47b5221ed72ebea436514621964"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"BEE-spoke-data\/NanoLlama-GQA-L10-A32_KV8-v13-KI",
        "Average":29.23,
        "ARC":23.81,
        "HellaSwag":29.39,
        "MMLU":25.37,
        "TruthfulQA":44.77,
        "Winogrande":51.14,
        "GSM8K":0.91,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.22,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"29fc3a802ee639be914d2a54fa6d9f595036ecf2"
    },
    {
        "T":"?",
        "Model":"FabbriSimo01\/GPT_Large_Quantized",
        "Average":29.21,
        "ARC":27.05,
        "HellaSwag":26.29,
        "MMLU":24.12,
        "TruthfulQA":48.46,
        "Winogrande":49.33,
        "GSM8K":0.0,
        "Type":"",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"unknown",
        "#Params (B)":0.77,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"c2df1904aa18de22d03ba0fee925e831d8468898"
    },
    {
        "T":"\u2b55",
        "Model":"lgaalves\/gpt2-dolly",
        "Average":29.21,
        "ARC":22.7,
        "HellaSwag":30.15,
        "MMLU":25.81,
        "TruthfulQA":44.97,
        "Winogrande":51.46,
        "GSM8K":0.15,
        "Type":"instruction-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"4bit",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"7e75e6f4626437305e4d3e7b2aa36f617c517247"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"concedo\/Pythia-70M-ChatSalad",
        "Average":29.2,
        "ARC":20.99,
        "HellaSwag":27.28,
        "MMLU":24.78,
        "TruthfulQA":49.74,
        "Winogrande":52.41,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":0.1,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"692289413c47c219cf83b1596783a8e9223541eb"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Yukang\/Llama-2-7b-longlora-32k-ft",
        "Average":29.2,
        "ARC":27.9,
        "HellaSwag":25.61,
        "MMLU":23.08,
        "TruthfulQA":49.57,
        "Winogrande":49.01,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"ab48674ffc55568ffe2a1207ef0e711c2febbaaf"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"microsoft\/DialoGPT-small",
        "Average":29.19,
        "ARC":25.77,
        "HellaSwag":25.79,
        "MMLU":25.81,
        "TruthfulQA":47.49,
        "Winogrande":50.28,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":0.18,
        "Hub \u2764\ufe0f":55.0,
        "Available on the Hub":true,
        "Model Sha":"97d0fec744c2cb4d48f5db51d17e3258e185858e"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Yukang\/Llama-2-13b-longlora-16k-ft",
        "Average":29.17,
        "ARC":25.85,
        "HellaSwag":27.6,
        "MMLU":23.1,
        "TruthfulQA":48.89,
        "Winogrande":49.57,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"5f0cfdef590fc9bd7642042fb5f1ed9679260b93"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"microsoft\/CodeGPT-small-py",
        "Average":29.17,
        "ARC":22.7,
        "HellaSwag":27.26,
        "MMLU":25.05,
        "TruthfulQA":51.23,
        "Winogrande":48.78,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":18.0,
        "Available on the Hub":true,
        "Model Sha":"e5f31df92bfb7b7a808ea8d1c7557488e1bdff7f"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"pszemraj\/pythia-31m-KI_v1-2048-scratch",
        "Average":29.15,
        "ARC":23.12,
        "HellaSwag":25.23,
        "MMLU":23.12,
        "TruthfulQA":51.67,
        "Winogrande":51.78,
        "GSM8K":0.0,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.03,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"b29a3229f8d5317adeabafeb20677ec7bea9d703"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"facebook\/opt-125m",
        "Average":29.15,
        "ARC":22.87,
        "HellaSwag":31.47,
        "MMLU":26.02,
        "TruthfulQA":42.87,
        "Winogrande":51.62,
        "GSM8K":0.08,
        "Type":"pretrained",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":81.0,
        "Available on the Hub":true,
        "Model Sha":"3d2b5f275bdf882b8775f902e1bfdb790e2cfc32"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ogimgio\/gpt-neo-125m-neurallinguisticpioneers",
        "Average":29.15,
        "ARC":22.44,
        "HellaSwag":30.36,
        "MMLU":25.14,
        "TruthfulQA":45.64,
        "Winogrande":51.22,
        "GSM8K":0.08,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"515fd7753c5fecbf4a2951f7cebb2846d91324b3"
    },
    {
        "T":"?",
        "Model":"cerebras\/Cerebras-GPT-590M",
        "Average":29.14,
        "ARC":23.72,
        "HellaSwag":32.4,
        "MMLU":25.97,
        "TruthfulQA":44.15,
        "Winogrande":48.15,
        "GSM8K":0.45,
        "Type":"",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.59,
        "Hub \u2764\ufe0f":17.0,
        "Available on the Hub":true,
        "Model Sha":"67a653304fd782a34906d59f3795a37f9e053397"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"roneneldan\/TinyStories-1M",
        "Average":29.14,
        "ARC":23.46,
        "HellaSwag":25.23,
        "MMLU":24.57,
        "TruthfulQA":49.4,
        "Winogrande":52.17,
        "GSM8K":0.0,
        "Type":"pretrained",
        "Architecture":"GPTNeoForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.0,
        "Hub \u2764\ufe0f":20.0,
        "Available on the Hub":true,
        "Model Sha":"8cd14d5339178f1b285f55baee14a0deff7103ac"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"TheBloke\/Llama-2-7b-Chat-AWQ",
        "Average":29.14,
        "ARC":27.22,
        "HellaSwag":25.48,
        "MMLU":24.67,
        "TruthfulQA":49.95,
        "Winogrande":47.51,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":1.13,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"a065961fd627aa3b3e6dde21e77fd5e20f712189"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Aspik101\/tulu-7b-instruct-pl-lora_unload",
        "Average":29.11,
        "ARC":28.67,
        "HellaSwag":26.05,
        "MMLU":23.12,
        "TruthfulQA":48.61,
        "Winogrande":48.22,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"962d4e5d8da5a4ec0ec047b6f8f08f1bb9e509fe"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"TurkuNLP\/gpt3-finnish-large",
        "Average":29.11,
        "ARC":21.76,
        "HellaSwag":32.88,
        "MMLU":24.11,
        "TruthfulQA":44.35,
        "Winogrande":51.54,
        "GSM8K":0.0,
        "Type":"pretrained",
        "Architecture":"BloomModel",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.88,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"b9a3dd97387fc70d07010d469888a918842d3449"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"euclaise\/gpt-neox-122m-minipile-digits",
        "Average":29.1,
        "ARC":20.73,
        "HellaSwag":27.03,
        "MMLU":25.31,
        "TruthfulQA":49.19,
        "Winogrande":52.33,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"cc0-1.0",
        "#Params (B)":0.17,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"3e9187385d31234b04021ddc8b03cbd5cfef9fb4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Yukang\/Llama-2-7b-longlora-100k-ft",
        "Average":29.08,
        "ARC":28.16,
        "HellaSwag":25.43,
        "MMLU":23.48,
        "TruthfulQA":49.06,
        "Winogrande":48.38,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":37.0,
        "Available on the Hub":true,
        "Model Sha":"242c6469cab41b41d30826e850afa4687e422f24"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"anas-awadalla\/mpt-1b-redpajama-200b",
        "Average":29.05,
        "ARC":25.77,
        "HellaSwag":26.08,
        "MMLU":24.5,
        "TruthfulQA":47.57,
        "Winogrande":50.36,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"MosaicGPT",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.0,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"fc98636655efb7c091bbe5d8014eb138ddfc5471"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BreadAi\/gpt-YA-1-1_160M",
        "Average":29.03,
        "ARC":22.95,
        "HellaSwag":27.29,
        "MMLU":26.25,
        "TruthfulQA":47.02,
        "Winogrande":50.67,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"b9b3577df726f7984721e4d73741296db50fa782"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"alibidaran\/medical_transcription_generator",
        "Average":29.03,
        "ARC":22.78,
        "HellaSwag":30.6,
        "MMLU":23.84,
        "TruthfulQA":46.5,
        "Winogrande":50.43,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.14,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"f622239151c89c2db0f1cef495d1b42afd16ce64"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/pythia-160m",
        "Average":29.02,
        "ARC":22.78,
        "HellaSwag":30.34,
        "MMLU":24.95,
        "TruthfulQA":44.26,
        "Winogrande":51.54,
        "GSM8K":0.23,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.21,
        "Hub \u2764\ufe0f":12.0,
        "Available on the Hub":true,
        "Model Sha":"50f5173d932e8e61f858120bcb800b97af589f46"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Locutusque\/gpt2-conversational-or-qa",
        "Average":29.01,
        "ARC":21.42,
        "HellaSwag":27.61,
        "MMLU":26.51,
        "TruthfulQA":47.31,
        "Winogrande":51.14,
        "GSM8K":0.08,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"openrail",
        "#Params (B)":0.14,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"f881c740c82ee9bc3191b886ad53f18d741960ea"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"klosax\/pythia-70m-deduped-step44k-92bt",
        "Average":29.0,
        "ARC":22.1,
        "HellaSwag":28.21,
        "MMLU":26.03,
        "TruthfulQA":46.12,
        "Winogrande":51.54,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":0.04,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"aac86fff08965d84d8bfc3e7c14559d48b8c4c99"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"huggingtweets\/jerma985",
        "Average":28.97,
        "ARC":21.67,
        "HellaSwag":30.91,
        "MMLU":26.57,
        "TruthfulQA":44.01,
        "Winogrande":50.67,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"816206ad02a397161be78dcb70eeda67e0c53132"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"BEE-spoke-data\/smol_llama-101M-GQA",
        "Average":28.97,
        "ARC":23.55,
        "HellaSwag":28.77,
        "MMLU":24.24,
        "TruthfulQA":45.76,
        "Winogrande":50.67,
        "GSM8K":0.83,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.1,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"cac68b3377fd0a1eb1aca92a2e661d81f59d8b08"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BEE-spoke-data\/smol_llama-101M-GQA",
        "Average":28.96,
        "ARC":23.46,
        "HellaSwag":28.73,
        "MMLU":24.35,
        "TruthfulQA":45.8,
        "Winogrande":50.67,
        "GSM8K":0.76,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.1,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"cac68b3377fd0a1eb1aca92a2e661d81f59d8b08"
    },
    {
        "T":"\u2b55",
        "Model":"WizardLM\/WizardLM-30B-V1.0",
        "Average":28.96,
        "ARC":27.39,
        "HellaSwag":25.94,
        "MMLU":23.12,
        "TruthfulQA":48.61,
        "Winogrande":48.7,
        "GSM8K":0.0,
        "Type":"instruction-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":70.0,
        "Available on the Hub":true,
        "Model Sha":"815e2dd7daabe446c429f3c9f70ef01582528f81"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"concedo\/OPT-19M-ChatSalad",
        "Average":28.96,
        "ARC":24.4,
        "HellaSwag":25.15,
        "MMLU":23.12,
        "TruthfulQA":51.36,
        "Winogrande":49.72,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":0.02,
        "Hub \u2764\ufe0f":15.0,
        "Available on the Hub":true,
        "Model Sha":"3930ca6bf3976e9b603815403cb373398ae509e5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"WizardLM\/WizardLM-30B-V1.0",
        "Average":28.95,
        "ARC":27.39,
        "HellaSwag":25.94,
        "MMLU":23.12,
        "TruthfulQA":48.61,
        "Winogrande":48.62,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"8bit",
        "Hub License":"?",
        "#Params (B)":32.32,
        "Hub \u2764\ufe0f":70.0,
        "Available on the Hub":true,
        "Model Sha":"815e2dd7daabe446c429f3c9f70ef01582528f81"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BreadAi\/DiscordPy",
        "Average":28.94,
        "ARC":23.29,
        "HellaSwag":26.15,
        "MMLU":25.04,
        "TruthfulQA":48.16,
        "Winogrande":50.99,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.26,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"a5405585aec0b60c5de7d942ccd58421fe9239be"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/pythia-70m",
        "Average":28.93,
        "ARC":21.59,
        "HellaSwag":27.29,
        "MMLU":25.9,
        "TruthfulQA":47.06,
        "Winogrande":51.46,
        "GSM8K":0.3,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.1,
        "Hub \u2764\ufe0f":14.0,
        "Available on the Hub":true,
        "Model Sha":"2ab25ed47af79376eed2baaf8bbb7a192a0c73ff"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"anton-l\/gpt-j-tiny-random",
        "Average":28.92,
        "ARC":26.37,
        "HellaSwag":25.76,
        "MMLU":24.46,
        "TruthfulQA":47.44,
        "Winogrande":49.49,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPTJForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.05,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"feea91564dac0081f73aeb6744979c6cfe553fff"
    },
    {
        "T":"?",
        "Model":"Corianas\/590m",
        "Average":28.88,
        "ARC":24.15,
        "HellaSwag":31.91,
        "MMLU":26.61,
        "TruthfulQA":42.19,
        "Winogrande":48.38,
        "GSM8K":0.08,
        "Type":"",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.67,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"ec721c97ef0e6ebfc578ab98b3ff6e2bd19b3e27"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BreadAi\/gpt-YA-1-1_70M",
        "Average":28.88,
        "ARC":22.53,
        "HellaSwag":27.37,
        "MMLU":25.38,
        "TruthfulQA":47.09,
        "Winogrande":50.91,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.04,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"218e8da522cf6fb5566314f37624f27412ae2259"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"cyberagent\/open-calm-large",
        "Average":28.88,
        "ARC":20.73,
        "HellaSwag":29.56,
        "MMLU":25.23,
        "TruthfulQA":46.52,
        "Winogrande":51.14,
        "GSM8K":0.08,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":0.76,
        "Hub \u2764\ufe0f":9.0,
        "Available on the Hub":true,
        "Model Sha":"f9b7a3222967b15169a09bcc86b118ac68a1ad62"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"microsoft\/DialoGPT-medium",
        "Average":28.86,
        "ARC":24.49,
        "HellaSwag":26.21,
        "MMLU":25.84,
        "TruthfulQA":47.06,
        "Winogrande":49.57,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":0.36,
        "Hub \u2764\ufe0f":240.0,
        "Available on the Hub":true,
        "Model Sha":"9d5c5fadcc072b693fb5a5e29416bbf3f503c26c"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Quake24\/easyTermsSummerizer",
        "Average":28.86,
        "ARC":25.77,
        "HellaSwag":25.81,
        "MMLU":23.12,
        "TruthfulQA":47.69,
        "Winogrande":50.75,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"BartForConditionalGeneration",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.41,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"8df9f96cc14be8f681c40bd1672b3f3540b70e31"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MayaPH\/FinOPT-Washington",
        "Average":28.85,
        "ARC":25.17,
        "HellaSwag":26.25,
        "MMLU":24.83,
        "TruthfulQA":45.8,
        "Winogrande":51.07,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"cdd8a6cde7902de39757cf31d73af1f51df0d8e8"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"pszemraj\/pythia-31m-goodwiki-deduped-2048-scratch",
        "Average":28.85,
        "ARC":23.12,
        "HellaSwag":25.66,
        "MMLU":23.11,
        "TruthfulQA":51.32,
        "Winogrande":49.88,
        "GSM8K":0.0,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.03,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"01a3cd918dd7c233bc0c3c0c948a9a462a5359d1"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BreadAi\/StoryPy",
        "Average":28.85,
        "ARC":22.35,
        "HellaSwag":26.19,
        "MMLU":24.37,
        "TruthfulQA":49.1,
        "Winogrande":51.07,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.1,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"5c32081bd3bc1404c2f5b8dbb6f888048bcb7cd7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"postbot\/distilgpt2-emailgen",
        "Average":28.84,
        "ARC":21.76,
        "HellaSwag":27.52,
        "MMLU":25.97,
        "TruthfulQA":46.17,
        "Winogrande":51.62,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.09,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"fe96d63cc2edcbd1ae444ada293cc59d1e01a6ad"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"ethzanalytics\/pythia-31m",
        "Average":28.81,
        "ARC":21.84,
        "HellaSwag":27.0,
        "MMLU":24.97,
        "TruthfulQA":49.1,
        "Winogrande":49.72,
        "GSM8K":0.23,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.03,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"eeea0b6b80603d162fe7de4e80a5bf4a8e9c6207"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Yukang\/Llama-2-7b-longlora-16k-ft",
        "Average":28.81,
        "ARC":26.37,
        "HellaSwag":26.37,
        "MMLU":23.75,
        "TruthfulQA":47.76,
        "Winogrande":48.62,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"c86de31b80866d047e680e08dbd3572e2965d4c5"
    },
    {
        "T":"?",
        "Model":"ByteWave\/Yi-8B-Llama",
        "Average":28.78,
        "ARC":25.68,
        "HellaSwag":26.79,
        "MMLU":24.14,
        "TruthfulQA":47.79,
        "Winogrande":48.3,
        "GSM8K":0.0,
        "Type":"",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":8.73,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":false,
        "Model Sha":"4f3f4d73ff3962487d1c51702b02d795bf1f33a4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nthngdy\/pythia-owt2-70m-100k",
        "Average":28.78,
        "ARC":20.9,
        "HellaSwag":28.34,
        "MMLU":25.02,
        "TruthfulQA":45.12,
        "Winogrande":53.28,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.07,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"b288893319b6cdce499148f4482043c350116560"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Tincando\/fiction_story_generator",
        "Average":28.77,
        "ARC":23.29,
        "HellaSwag":28.68,
        "MMLU":26.72,
        "TruthfulQA":43.79,
        "Winogrande":50.12,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"377b080cf96e10d50289aa3e1fd79c330265f45a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Corianas\/256_5epoch",
        "Average":28.76,
        "ARC":22.27,
        "HellaSwag":28.99,
        "MMLU":26.62,
        "TruthfulQA":41.71,
        "Winogrande":52.72,
        "GSM8K":0.23,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":0.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"b1fe75844a07832acd405a4d989a26f6ab7b1c00"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"abhiramtirumala\/DialoGPT-sarcastic-medium",
        "Average":28.73,
        "ARC":23.29,
        "HellaSwag":25.93,
        "MMLU":23.76,
        "TruthfulQA":46.04,
        "Winogrande":53.35,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.14,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"292596e120591887383011c4520bc5b57e7e8993"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"nthngdy\/pythia-owt2-70m-50k",
        "Average":28.71,
        "ARC":21.5,
        "HellaSwag":28.15,
        "MMLU":25.7,
        "TruthfulQA":44.5,
        "Winogrande":52.41,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.07,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":false,
        "Model Sha":"9fce9b8252f7891dbd50299a8c3bd71cd25454db"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"HWERI\/pythia-70m-deduped-cleansharegpt-en",
        "Average":28.71,
        "ARC":21.16,
        "HellaSwag":27.16,
        "MMLU":25.24,
        "TruthfulQA":48.57,
        "Winogrande":50.12,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.04,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"a97ff56bc68a81a9f6147f1590e53511246d1040"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"BEE-spoke-data\/verysmol_llama-v11-KIx2",
        "Average":28.7,
        "ARC":22.7,
        "HellaSwag":27.6,
        "MMLU":25.28,
        "TruthfulQA":44.75,
        "Winogrande":51.54,
        "GSM8K":0.3,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.06,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"1cd271d3d62a9e1dc4b7c2978e54806d74705439"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"RWKV\/rwkv-4-169m-pile",
        "Average":28.64,
        "ARC":23.63,
        "HellaSwag":31.74,
        "MMLU":23.18,
        "TruthfulQA":41.92,
        "Winogrande":50.91,
        "GSM8K":0.45,
        "Type":"pretrained",
        "Architecture":"RwkvForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.13,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"46bdc280eb97b6141d5d51a935e0c4870ecaefcc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"postbot\/distilgpt2-emailgen-V2",
        "Average":28.64,
        "ARC":20.99,
        "HellaSwag":26.78,
        "MMLU":25.53,
        "TruthfulQA":46.51,
        "Winogrande":52.01,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.09,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"9750ba00e79a02e1bf98d3faa3d49b8ae0f8ae63"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"pszemraj\/pythia-31m-simplewiki-scratch-bf16",
        "Average":28.61,
        "ARC":22.78,
        "HellaSwag":25.61,
        "MMLU":23.12,
        "TruthfulQA":49.65,
        "Winogrande":50.51,
        "GSM8K":0.0,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.03,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"4eaec0542e7609fd3f364cb34491f05d7c61a3d0"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"pszemraj\/pythia-31m-simplepile-lite-2048-scratch-2e",
        "Average":28.6,
        "ARC":21.59,
        "HellaSwag":25.79,
        "MMLU":24.99,
        "TruthfulQA":50.62,
        "Winogrande":48.62,
        "GSM8K":0.0,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.03,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"91f011eb99502e667ebc2803f354ce5f5209ccf1"
    },
    {
        "T":"\u2b55",
        "Model":"lgaalves\/gpt2_open-platypus",
        "Average":28.58,
        "ARC":22.18,
        "HellaSwag":31.29,
        "MMLU":26.19,
        "TruthfulQA":40.35,
        "Winogrande":51.3,
        "GSM8K":0.15,
        "Type":"instruction-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"745c1864b752525789cad2b75166c519a327325e"
    },
    {
        "T":"\u2b55",
        "Model":"beomi\/KoAlpaca-KoRWKV-6B",
        "Average":28.57,
        "ARC":23.46,
        "HellaSwag":31.65,
        "MMLU":24.89,
        "TruthfulQA":39.83,
        "Winogrande":51.62,
        "GSM8K":0.0,
        "Type":"instruction-tuned",
        "Architecture":"RwkvForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.53,
        "Hub \u2764\ufe0f":7.0,
        "Available on the Hub":true,
        "Model Sha":"427ee72c4350f26de1b287a0c07b842e7d168dbc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KnutJaegersberg\/RWKV-4-PilePlus-169M-20230520-done-ctx4096",
        "Average":28.57,
        "ARC":23.98,
        "HellaSwag":32.25,
        "MMLU":23.37,
        "TruthfulQA":42.29,
        "Winogrande":49.17,
        "GSM8K":0.38,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.13,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"1134d31db1aee9fc970d3e9dc4e7314fb8bba500"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"yeen214\/llama2_7b_small_tuning_v1",
        "Average":28.56,
        "ARC":22.44,
        "HellaSwag":25.0,
        "MMLU":25.51,
        "TruthfulQA":48.7,
        "Winogrande":49.72,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"3f9b43b4db2da4fe3785071dd52c9fc92aa0801d"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"qiyinmiss\/My_GPT2",
        "Average":28.55,
        "ARC":21.93,
        "HellaSwag":31.59,
        "MMLU":25.84,
        "TruthfulQA":40.73,
        "Winogrande":50.51,
        "GSM8K":0.68,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":0.14,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"4145e280b85ec619906dfc5a624e17cde8ffbea6"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Corianas\/Quokka_590m",
        "Average":28.53,
        "ARC":24.4,
        "HellaSwag":31.61,
        "MMLU":25.36,
        "TruthfulQA":39.59,
        "Winogrande":50.2,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.67,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"ae0ac41e9be016f6dceac06821fbf6ebacc7edb9"
    },
    {
        "T":"\u2b55",
        "Model":"lgaalves\/gpt2_guanaco-dolly-platypus",
        "Average":28.52,
        "ARC":23.55,
        "HellaSwag":31.03,
        "MMLU":26.4,
        "TruthfulQA":40.02,
        "Winogrande":50.12,
        "GSM8K":0.0,
        "Type":"instruction-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"6bf0a8146cf255c829ec2ad83926c8b80945b431"
    },
    {
        "T":"\u2b55",
        "Model":"lgaalves\/gpt2_platypus-dolly-guanaco",
        "Average":28.51,
        "ARC":23.21,
        "HellaSwag":31.04,
        "MMLU":26.16,
        "TruthfulQA":40.31,
        "Winogrande":50.36,
        "GSM8K":0.0,
        "Type":"instruction-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"bfa144d3eb087e54f1798fd2e2fb17e894cc39d3"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"bigcode\/gpt_bigcode-santacoder",
        "Average":28.49,
        "ARC":21.16,
        "HellaSwag":30.84,
        "MMLU":24.97,
        "TruthfulQA":45.64,
        "Winogrande":47.83,
        "GSM8K":0.53,
        "Type":"pretrained",
        "Architecture":"GPTBigCodeForCausalLM",
        "Precision":"float16",
        "Hub License":"openrail",
        "#Params (B)":1.12,
        "Hub \u2764\ufe0f":21.0,
        "Available on the Hub":true,
        "Model Sha":"291931872cae83498cf984b16319f47f5e9e7a07"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MBZUAI\/lamini-cerebras-256m",
        "Average":28.49,
        "ARC":21.76,
        "HellaSwag":28.7,
        "MMLU":26.66,
        "TruthfulQA":41.81,
        "Winogrande":52.01,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.26,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"72df0b6d62d64002575687ea2edbb0df05712678"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"AI-Sweden-Models\/gpt-sw3-126m",
        "Average":28.49,
        "ARC":22.18,
        "HellaSwag":29.54,
        "MMLU":24.43,
        "TruthfulQA":44.03,
        "Winogrande":50.67,
        "GSM8K":0.08,
        "Type":"pretrained",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"bfloat16",
        "Hub License":"other",
        "#Params (B)":0.19,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"9272f5a996cf785b8ab706a27d1e7dff1228dc70"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"blueapple8259\/TinyStories-Alpaca",
        "Average":28.46,
        "ARC":23.98,
        "HellaSwag":24.92,
        "MMLU":23.35,
        "TruthfulQA":46.68,
        "Winogrande":51.85,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.07,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"18e0bde7e72e477757832f0624a0410efc066216"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BreadAi\/gpt-Youtube",
        "Average":28.46,
        "ARC":23.29,
        "HellaSwag":26.34,
        "MMLU":23.54,
        "TruthfulQA":48.63,
        "Winogrande":48.93,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"de88554a0212c16fdfeda030afb58f831ebcd895"
    },
    {
        "T":"?",
        "Model":"Sayan01\/Llama-Flan-XL2base",
        "Average":28.44,
        "ARC":20.65,
        "HellaSwag":25.33,
        "MMLU":23.19,
        "TruthfulQA":50.58,
        "Winogrande":50.91,
        "GSM8K":0.0,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":2.0,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"5ffcaeaf5645d96c3f04ed632a820590d3f87c6c"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"roneneldan\/TinyStories-28M",
        "Average":28.44,
        "ARC":22.78,
        "HellaSwag":25.83,
        "MMLU":23.53,
        "TruthfulQA":48.08,
        "Winogrande":50.43,
        "GSM8K":0.0,
        "Type":"pretrained",
        "Architecture":"GPTNeoForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.05,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"52dabea9997faf578489d619249616926e54ed18"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"EleutherAI\/pythia-70m-deduped",
        "Average":28.44,
        "ARC":21.08,
        "HellaSwag":27.17,
        "MMLU":25.26,
        "TruthfulQA":47.51,
        "Winogrande":49.64,
        "GSM8K":0.0,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.1,
        "Hub \u2764\ufe0f":14.0,
        "Available on the Hub":true,
        "Model Sha":"e93a9faa9c77e5d09219f6c868bfc7a1bd65593c"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"budecosystem\/boomer-1b",
        "Average":28.44,
        "ARC":22.78,
        "HellaSwag":31.58,
        "MMLU":25.66,
        "TruthfulQA":39.17,
        "Winogrande":50.51,
        "GSM8K":0.91,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.94,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"f8f24b5480fa43f23d858f0eb8d1af1b7ad0af59"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"roneneldan\/TinyStories-33M",
        "Average":28.41,
        "ARC":24.23,
        "HellaSwag":25.69,
        "MMLU":23.82,
        "TruthfulQA":47.64,
        "Winogrande":49.09,
        "GSM8K":0.0,
        "Type":"pretrained",
        "Architecture":"GPTNeoForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.07,
        "Hub \u2764\ufe0f":61.0,
        "Available on the Hub":true,
        "Model Sha":"190d22e37cba4b12ddae57d6738a0c65f6ab1aa5"
    },
    {
        "T":"\u2b55",
        "Model":"lgaalves\/gpt2_camel_physics-platypus",
        "Average":28.41,
        "ARC":23.04,
        "HellaSwag":31.32,
        "MMLU":26.91,
        "TruthfulQA":39.56,
        "Winogrande":49.64,
        "GSM8K":0.0,
        "Type":"instruction-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"66165ff32ed8de6c39f3524a810f5e97ba6d3347"
    },
    {
        "T":"\u2b55",
        "Model":"lgaalves\/gpt2_platypus-camel_physics",
        "Average":28.41,
        "ARC":23.04,
        "HellaSwag":31.32,
        "MMLU":26.91,
        "TruthfulQA":39.56,
        "Winogrande":49.64,
        "GSM8K":0.0,
        "Type":"instruction-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"66165ff32ed8de6c39f3524a810f5e97ba6d3347"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dpv\/finetuned-gpt2-tiny",
        "Average":28.4,
        "ARC":21.84,
        "HellaSwag":31.6,
        "MMLU":25.86,
        "TruthfulQA":40.67,
        "Winogrande":50.12,
        "GSM8K":0.3,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"379e02101b4dccba48e7ae792708d2fe7f0bbca2"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"SaylorTwift\/gpt2_test",
        "Average":28.4,
        "ARC":21.84,
        "HellaSwag":31.6,
        "MMLU":25.86,
        "TruthfulQA":40.67,
        "Winogrande":50.12,
        "GSM8K":0.3,
        "Type":"pretrained",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":0.14,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"ef61310a16ffda93bf8f6132e02658482ffc2bcc"
    },
    {
        "T":"\u2b55",
        "Model":"behnamsh\/gpt2_platypus-camel_physics",
        "Average":28.4,
        "ARC":22.78,
        "HellaSwag":31.24,
        "MMLU":25.87,
        "TruthfulQA":38.95,
        "Winogrande":51.54,
        "GSM8K":0.0,
        "Type":"instruction-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"cd4d700d13b3bc9371bf45616ef74ac20d165c3d"
    },
    {
        "T":"?",
        "Model":"MBZUAI\/lamini-cerebras-590m",
        "Average":28.38,
        "ARC":24.32,
        "HellaSwag":31.58,
        "MMLU":25.57,
        "TruthfulQA":40.72,
        "Winogrande":47.91,
        "GSM8K":0.15,
        "Type":"",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.59,
        "Hub \u2764\ufe0f":5.0,
        "Available on the Hub":true,
        "Model Sha":"bab37eb7ba63f6ff9f0eb36a85727146b82ae5ed"
    },
    {
        "T":"?",
        "Model":"mncai\/SGPT-1.3B-insurance-epoch10",
        "Average":28.37,
        "ARC":24.57,
        "HellaSwag":24.25,
        "MMLU":25.23,
        "TruthfulQA":45.24,
        "Winogrande":50.91,
        "GSM8K":0.0,
        "Type":"",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":1.27,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"df685c0bbf838f0627383c28f48e577ee901ba68"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vicgalle\/gpt2-alpaca-gpt4",
        "Average":28.34,
        "ARC":22.61,
        "HellaSwag":31.17,
        "MMLU":25.76,
        "TruthfulQA":38.04,
        "Winogrande":52.17,
        "GSM8K":0.3,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":0.14,
        "Hub \u2764\ufe0f":14.0,
        "Available on the Hub":true,
        "Model Sha":"282e9bd56f0cab5d48e6954793647eecaa0871d9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Corianas\/Quokka_256m",
        "Average":28.32,
        "ARC":22.87,
        "HellaSwag":28.84,
        "MMLU":26.48,
        "TruthfulQA":39.47,
        "Winogrande":52.25,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.32,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"d4e69f714d360d39979eb7b8cbc9decdb7190c88"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"roneneldan\/TinyStories-8M",
        "Average":28.31,
        "ARC":24.66,
        "HellaSwag":25.03,
        "MMLU":23.33,
        "TruthfulQA":46.54,
        "Winogrande":50.28,
        "GSM8K":0.0,
        "Type":"pretrained",
        "Architecture":"GPTNeoForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.02,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"8612e3b15c66ffa94eaa6ee0de5c96edd2d630af"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"ethzanalytics\/pythia-31m",
        "Average":28.3,
        "ARC":19.97,
        "HellaSwag":26.34,
        "MMLU":24.27,
        "TruthfulQA":50.12,
        "Winogrande":49.09,
        "GSM8K":0.0,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.03,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"8a3c2f1555de8a3c53d67d73b5d0d53a66a6c6c2"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aisquared\/dlite-v2-124m",
        "Average":28.3,
        "ARC":23.98,
        "HellaSwag":31.1,
        "MMLU":25.29,
        "TruthfulQA":38.98,
        "Winogrande":50.43,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":4.0,
        "Available on the Hub":true,
        "Model Sha":"bc719f990748ea72be4b6c270df34fc3d37291dc"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"huggingtweets\/gladosystem",
        "Average":28.29,
        "ARC":24.4,
        "HellaSwag":29.71,
        "MMLU":23.18,
        "TruthfulQA":41.78,
        "Winogrande":50.67,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"02a1bbcee7b584ace743b2fe4885cc0eaf2179ac"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MBZUAI\/lamini-cerebras-111m",
        "Average":28.29,
        "ARC":22.1,
        "HellaSwag":27.12,
        "MMLU":25.51,
        "TruthfulQA":43.79,
        "Winogrande":51.22,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.11,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"e8e347b02f9305e4bc144eb9be2821c518d43183"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"gpt2",
        "Average":28.28,
        "ARC":21.59,
        "HellaSwag":31.58,
        "MMLU":25.4,
        "TruthfulQA":41.15,
        "Winogrande":49.57,
        "GSM8K":0.38,
        "Type":"pretrained",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"8bit",
        "Hub License":"mit",
        "#Params (B)":0.14,
        "Hub \u2764\ufe0f":1504.0,
        "Available on the Hub":true,
        "Model Sha":"11c5a3d5811f50298f278a704980280950aedb10"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"pszemraj\/pythia-31m-simplewiki-2048",
        "Average":28.27,
        "ARC":22.18,
        "HellaSwag":25.55,
        "MMLU":23.12,
        "TruthfulQA":49.37,
        "Winogrande":49.41,
        "GSM8K":0.0,
        "Type":"pretrained",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.03,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"95d47818055661250b55144c7d9beaf05dc126d8"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"cyberagent\/open-calm-7b",
        "Average":28.21,
        "ARC":20.48,
        "HellaSwag":30.65,
        "MMLU":25.22,
        "TruthfulQA":44.15,
        "Winogrande":48.54,
        "GSM8K":0.23,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-sa-4.0",
        "#Params (B)":6.66,
        "Hub \u2764\ufe0f":188.0,
        "Available on the Hub":true,
        "Model Sha":"276a5fb67510554e11ef191a2da44c919acccdf5"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"crumb\/gpt2023",
        "Average":28.2,
        "ARC":21.93,
        "HellaSwag":31.11,
        "MMLU":25.05,
        "TruthfulQA":40.71,
        "Winogrande":50.12,
        "GSM8K":0.3,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":0.14,
        "Hub \u2764\ufe0f":12.0,
        "Available on the Hub":true,
        "Model Sha":"e3620b53d164529575db66d9d4f4382311dd713c"
    },
    {
        "T":"\u2b55",
        "Model":"AI-Sweden-Models\/gpt-sw3-126m-instruct",
        "Average":28.2,
        "ARC":23.38,
        "HellaSwag":29.88,
        "MMLU":23.78,
        "TruthfulQA":42.65,
        "Winogrande":48.54,
        "GSM8K":0.99,
        "Type":"instruction-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"bfloat16",
        "Hub License":"other",
        "#Params (B)":0.19,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"5f353e1eb1b579ef62e10302b7c0bb843ee8eba9"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"beomi\/KoRWKV-6B",
        "Average":28.19,
        "ARC":22.1,
        "HellaSwag":32.18,
        "MMLU":24.69,
        "TruthfulQA":39.05,
        "Winogrande":51.14,
        "GSM8K":0.0,
        "Type":"pretrained",
        "Architecture":"RwkvForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":6.53,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"541600070459baf0f1be9560181d5ceb77794085"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"roneneldan\/TinyStories-3M",
        "Average":28.19,
        "ARC":22.01,
        "HellaSwag":25.58,
        "MMLU":24.99,
        "TruthfulQA":47.33,
        "Winogrande":49.25,
        "GSM8K":0.0,
        "Type":"pretrained",
        "Architecture":"GPTNeoForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.01,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"cfaf26ec85ecdfc1bd7c2638104cce55cb67f894"
    },
    {
        "T":"\u2b55",
        "Model":"Locutusque\/TinyMistral-248M-Instruct",
        "Average":28.19,
        "ARC":24.32,
        "HellaSwag":27.52,
        "MMLU":25.18,
        "TruthfulQA":41.94,
        "Winogrande":50.2,
        "GSM8K":0.0,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.25,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"32a9317176bd8562bbb6497eef43a95f2c0261c3"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"pszemraj\/distilgpt2-HC3",
        "Average":28.18,
        "ARC":24.66,
        "HellaSwag":27.99,
        "MMLU":23.95,
        "TruthfulQA":42.1,
        "Winogrande":50.36,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.09,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"6f9ad473a3793d0271df34a55882ad30846a6788"
    },
    {
        "T":"\u2b55",
        "Model":"lgaalves\/gpt2-dolly",
        "Average":28.18,
        "ARC":21.76,
        "HellaSwag":30.77,
        "MMLU":24.66,
        "TruthfulQA":42.22,
        "Winogrande":49.57,
        "GSM8K":0.08,
        "Type":"instruction-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"52fcf61a8eef255a981be6efde187481086e1a48"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"BEE-spoke-data\/smol_llama-81M-tied",
        "Average":28.17,
        "ARC":22.18,
        "HellaSwag":29.33,
        "MMLU":24.06,
        "TruthfulQA":43.97,
        "Winogrande":49.25,
        "GSM8K":0.23,
        "Type":"pretrained",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.08,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"096e543bd36d067a819ea867c66f14d946849053"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"MBZUAI\/LaMini-GPT-124M",
        "Average":28.01,
        "ARC":24.32,
        "HellaSwag":30.82,
        "MMLU":24.99,
        "TruthfulQA":36.57,
        "Winogrande":51.38,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":14.0,
        "Available on the Hub":true,
        "Model Sha":"5c67c8c03c08e82d6138ce2a1eddf5317fac3a6b"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"TurkuNLP\/gpt3-finnish-small",
        "Average":27.95,
        "ARC":20.48,
        "HellaSwag":28.09,
        "MMLU":24.47,
        "TruthfulQA":46.47,
        "Winogrande":48.22,
        "GSM8K":0.0,
        "Type":"pretrained",
        "Architecture":"BloomModel",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.19,
        "Hub \u2764\ufe0f":9.0,
        "Available on the Hub":true,
        "Model Sha":"20a19af481bf59f38610a2977b2b513e9df51e3a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Mikivis\/xuanxuan",
        "Average":27.88,
        "ARC":23.46,
        "HellaSwag":31.12,
        "MMLU":26.27,
        "TruthfulQA":35.97,
        "Winogrande":50.43,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":0.14,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"ba6ae2b347bc613ae38980e059ec8c5ec8b26038"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"vicgalle\/gpt2-alpaca",
        "Average":27.86,
        "ARC":22.87,
        "HellaSwag":31.14,
        "MMLU":26.26,
        "TruthfulQA":36.22,
        "Winogrande":50.67,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":0.14,
        "Hub \u2764\ufe0f":8.0,
        "Available on the Hub":true,
        "Model Sha":"e06875a588f7b3386c18a6efdc8cc7583d95b21b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aisquared\/dlite-v1-124m",
        "Average":27.86,
        "ARC":24.32,
        "HellaSwag":31.16,
        "MMLU":25.08,
        "TruthfulQA":36.38,
        "Winogrande":50.2,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f6fd5f3960f31881e6cee23f5a872ecc80b40283"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"psyche\/kogpt",
        "Average":27.83,
        "ARC":21.16,
        "HellaSwag":28.11,
        "MMLU":26.56,
        "TruthfulQA":42.06,
        "Winogrande":49.09,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.39,
        "Hub \u2764\ufe0f":3.0,
        "Available on the Hub":true,
        "Model Sha":"4c02d48f548103ba53a5e481b8aa81bf7a259287"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"cerebras\/Cerebras-GPT-111M",
        "Average":27.75,
        "ARC":20.22,
        "HellaSwag":26.73,
        "MMLU":25.51,
        "TruthfulQA":46.31,
        "Winogrande":47.75,
        "GSM8K":0.0,
        "Type":"pretrained",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.11,
        "Hub \u2764\ufe0f":62.0,
        "Available on the Hub":true,
        "Model Sha":"d2b54d7af419055f204690fe0385959616a1723e"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"Locutusque\/TinyMistral-248m",
        "Average":27.73,
        "ARC":22.87,
        "HellaSwag":28.02,
        "MMLU":23.15,
        "TruthfulQA":42.52,
        "Winogrande":49.8,
        "GSM8K":0.0,
        "Type":"pretrained",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.25,
        "Hub \u2764\ufe0f":6.0,
        "Available on the Hub":true,
        "Model Sha":"8f03f72bca0542aa164c29ba41f02cba6f9d7748"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"huashiyiqike\/testmodel",
        "Average":27.6,
        "ARC":19.71,
        "HellaSwag":26.68,
        "MMLU":25.28,
        "TruthfulQA":43.72,
        "Winogrande":50.2,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":0.15,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"1ac5d244402e2433b6abfcff1fe65e84af15766b"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Corianas\/111m",
        "Average":27.6,
        "ARC":19.71,
        "HellaSwag":26.68,
        "MMLU":25.28,
        "TruthfulQA":43.72,
        "Winogrande":50.2,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"cc-by-nc-sa-4.0",
        "#Params (B)":0.15,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"ee58d79e27f8b9e3984aab29235c5851d2be01d4"
    },
    {
        "T":"\u2b55",
        "Model":"Felladrin\/TinyMistral-248M-SFT-v3",
        "Average":27.45,
        "ARC":21.93,
        "HellaSwag":28.26,
        "MMLU":22.91,
        "TruthfulQA":40.03,
        "Winogrande":51.54,
        "GSM8K":0.0,
        "Type":"instruction-tuned",
        "Architecture":"MistralForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.25,
        "Hub \u2764\ufe0f":12.0,
        "Available on the Hub":true,
        "Model Sha":"7a4787dfed21a432924d24575e6c65a97e1dd98a"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"gpt2",
        "Average":26.7,
        "ARC":22.1,
        "HellaSwag":31.6,
        "MMLU":25.86,
        "TruthfulQA":40.67,
        "Winogrande":40.0,
        "GSM8K":0.0,
        "Type":"pretrained",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":0.14,
        "Hub \u2764\ufe0f":1422.0,
        "Available on the Hub":true,
        "Model Sha":"11c5a3d5811f50298f278a704980280950aedb10"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"databricks\/dolly-v2-3b",
        "Average":22.83,
        "ARC":25.26,
        "HellaSwag":26.55,
        "MMLU":24.7,
        "TruthfulQA":0.0,
        "Winogrande":59.43,
        "GSM8K":1.06,
        "Type":"fine-tuned",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":232.0,
        "Available on the Hub":true,
        "Model Sha":"f6c9be08f16fe4d3a719bee0a4a7c7415b5c65df"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"FabbriSimo01\/Facebook_opt_1.3b_Quantized",
        "Average":21.78,
        "ARC":22.7,
        "HellaSwag":25.04,
        "MMLU":23.12,
        "TruthfulQA":0.0,
        "Winogrande":59.67,
        "GSM8K":0.15,
        "Type":"pretrained",
        "Architecture":"OPTForCausalLM",
        "Precision":"float16",
        "Hub License":"mit",
        "#Params (B)":1.32,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"7ef72ccee9d91d06967809e4e63ffbef62a9ad4a"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"KevinNi\/mistral-class-bio-tutor",
        "Average":21.59,
        "ARC":28.07,
        "HellaSwag":28.02,
        "MMLU":23.79,
        "TruthfulQA":0.0,
        "Winogrande":49.64,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"MistralModel",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":7.11,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"c0e782c571209e1238e3a3170dcd187f9a436df2"
    },
    {
        "T":"?",
        "Model":"jslin09\/bloom-560m-finetuned-fraud",
        "Average":21.37,
        "ARC":26.96,
        "HellaSwag":28.87,
        "MMLU":24.03,
        "TruthfulQA":0.0,
        "Winogrande":48.38,
        "GSM8K":0.0,
        "Type":"",
        "Architecture":"BloomForCausalLM",
        "Precision":"float16",
        "Hub License":"bigscience-bloom-rail-1.0",
        "#Params (B)":0.56,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"5571f87f557b909e863005c6e3870bc2e77341a7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ahnyeonchan\/OpenOrca-AYT-13B",
        "Average":21.35,
        "ARC":27.22,
        "HellaSwag":26.03,
        "MMLU":25.11,
        "TruthfulQA":0.0,
        "Winogrande":49.72,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"bfloat16",
        "Hub License":"llama2",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"1357abceda30e8389007a023907824cc3a11e397"
    },
    {
        "T":"?",
        "Model":"Andron00e\/YetAnother_Open-Llama-3B-LoRA",
        "Average":21.29,
        "ARC":25.94,
        "HellaSwag":25.76,
        "MMLU":24.65,
        "TruthfulQA":0.0,
        "Winogrande":51.38,
        "GSM8K":0.0,
        "Type":"",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"52c5cb0178831908ed0571f1750fcb0f0fb125f9"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"Andron00e\/YetAnother_Open-Llama-3B-LoRA-OpenOrca",
        "Average":21.2,
        "ARC":25.94,
        "HellaSwag":25.76,
        "MMLU":24.65,
        "TruthfulQA":0.0,
        "Winogrande":50.83,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":3.43,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"07d9d32cd091148295d4e13802ba63486599aff4"
    },
    {
        "T":"?",
        "Model":"Dampish\/Dante-2.8B",
        "Average":21.12,
        "ARC":25.09,
        "HellaSwag":26.05,
        "MMLU":24.51,
        "TruthfulQA":0.0,
        "Winogrande":51.07,
        "GSM8K":0.0,
        "Type":"",
        "Architecture":"GPTNeoXForCausalLM",
        "Precision":"float16",
        "Hub License":"cc-by-nc-4.0",
        "#Params (B)":2.65,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"fb2a8f95c0286f957c830af640fd5c989081e8e4"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"BreadAi\/MuseCan",
        "Average":21.06,
        "ARC":28.07,
        "HellaSwag":25.0,
        "MMLU":24.19,
        "TruthfulQA":0.0,
        "Winogrande":49.09,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"GPT2LMHeadModel",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":0.07,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"f441866d78feaead3dede6efd9e23990bb74c21e"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"team-lucid\/mptk-1b",
        "Average":20.84,
        "ARC":22.7,
        "HellaSwag":25.48,
        "MMLU":27.11,
        "TruthfulQA":0.0,
        "Winogrande":49.72,
        "GSM8K":0.0,
        "Type":"pretrained",
        "Architecture":"MptForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":1.31,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"aea467410ae0cead4fded6b98a3575e92b22862f"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"ceadar-ie\/FinanceConnect-13B",
        "Average":20.07,
        "ARC":22.7,
        "HellaSwag":25.04,
        "MMLU":23.12,
        "TruthfulQA":0.0,
        "Winogrande":49.57,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":13.02,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"adaf830e7362788b73e7ceaf8ec010409774c711"
    },
    {
        "T":"\ud83d\udfe2",
        "Model":"wtang06\/mpt-125m-c4",
        "Average":20.07,
        "ARC":22.7,
        "HellaSwag":25.04,
        "MMLU":23.12,
        "TruthfulQA":0.0,
        "Winogrande":49.57,
        "GSM8K":0.0,
        "Type":"pretrained",
        "Architecture":"MPTForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":0.12,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":true,
        "Model Sha":"55f8f1874aa8bf4fc28c0abc92c7fbd1271ff7d7"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"clibrain\/Llama-2-ft-instruct-es",
        "Average":20.07,
        "ARC":22.7,
        "HellaSwag":25.04,
        "MMLU":23.12,
        "TruthfulQA":0.0,
        "Winogrande":49.57,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"float16",
        "Hub License":"apache-2.0",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":16.0,
        "Available on the Hub":true,
        "Model Sha":"42f07d6a86fac5574febb7b8fa13c3b1e14fcebd"
    },
    {
        "T":"\ud83d\udfe6",
        "Model":"TheTravellingEngineer\/bloom-1b1-RLHF-v2",
        "Average":20.07,
        "ARC":22.7,
        "HellaSwag":25.04,
        "MMLU":23.12,
        "TruthfulQA":0.0,
        "Winogrande":49.57,
        "GSM8K":0.0,
        "Type":"RL-tuned",
        "Architecture":"BloomForCausalLM",
        "Precision":"float16",
        "Hub License":"?",
        "#Params (B)":1.06,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"05f7f0fd82fb3a5798d4bb284b6c10dd9d380f22"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"aiplanet\/panda-coder-13B",
        "Average":20.07,
        "ARC":22.7,
        "HellaSwag":25.04,
        "MMLU":23.12,
        "TruthfulQA":0.0,
        "Winogrande":49.57,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"4bit",
        "Hub License":"apache-2.0",
        "#Params (B)":12.85,
        "Hub \u2764\ufe0f":2.0,
        "Available on the Hub":true,
        "Model Sha":"823a8320224cdac88e927aee00338ffa79395faa"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"APMIC\/caigun-lora-model-33B",
        "Average":20.07,
        "ARC":22.7,
        "HellaSwag":25.04,
        "MMLU":23.12,
        "TruthfulQA":0.0,
        "Winogrande":49.57,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"LlamaForCausalLM",
        "Precision":"8bit",
        "Hub License":"cc-by-nc-nd-4.0",
        "#Params (B)":18.25,
        "Hub \u2764\ufe0f":0.0,
        "Available on the Hub":true,
        "Model Sha":"43789c7afafa495cbcb75185c8f48b11488c0408"
    },
    {
        "T":"\ud83d\udd36",
        "Model":"dfurman\/llama-2-13b-dolphin-peft",
        "Average":20.07,
        "ARC":22.7,
        "HellaSwag":25.04,
        "MMLU":23.12,
        "TruthfulQA":0.0,
        "Winogrande":49.57,
        "GSM8K":0.0,
        "Type":"fine-tuned",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"llama2",
        "#Params (B)":13.0,
        "Hub \u2764\ufe0f":10.0,
        "Available on the Hub":false,
        "Model Sha":"5d17f6b5f394f0745bd4377c8a1290c68051e351"
    },
    {
        "T":"?",
        "Model":"Rardilit\/Panther_v1",
        "Average":20.07,
        "ARC":22.7,
        "HellaSwag":25.04,
        "MMLU":23.12,
        "TruthfulQA":0.0,
        "Winogrande":49.57,
        "GSM8K":0.0,
        "Type":"",
        "Architecture":"?",
        "Precision":"float16",
        "Hub License":"other",
        "#Params (B)":6.61,
        "Hub \u2764\ufe0f":1.0,
        "Available on the Hub":false,
        "Model Sha":""
    }
]