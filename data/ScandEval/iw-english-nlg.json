[
    {
        "rank":1,
        "Model":"gpt-3.5-turbo-0613 (few-shot, val)",
        "num_model_parameters":"unknown",
        "vocabulary_size":100,
        "max_sequence_length":4095,
        "speed":1344,
        "score":62.3,
        "en_score":62.3,
        "conll_en":71.48,
        "sst5":66.41,
        "scala_en":41.43,
        "squad":67.9,
        "cnn_dailymail":69.57,
        "mmlu":43.69,
        "hellaswag":75.6
    },
    {
        "rank":2,
        "Model":"01-ai\/Yi-6B (few-shot)",
        "num_model_parameters":"6061.0",
        "vocabulary_size":64,
        "max_sequence_length":4096,
        "speed":2786,
        "score":58.47,
        "en_score":58.47,
        "conll_en":52.7,
        "sst5":68.66,
        "scala_en":25.29,
        "squad":75.83,
        "cnn_dailymail":67.65,
        "mmlu":50.89,
        "hellaswag":68.29
    },
    {
        "rank":2,
        "Model":"mlabonne\/NeuralBeagle14-7B (few-shot, val)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":8192,
        "speed":2549,
        "score":58.34,
        "en_score":58.34,
        "conll_en":69.16,
        "sst5":63.85,
        "scala_en":28.4,
        "squad":52.69,
        "cnn_dailymail":70.55,
        "mmlu":51.74,
        "hellaswag":71.96
    },
    {
        "rank":3,
        "Model":"mistralai\/Mistral-7B-v0.1 (few-shot)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":2657,
        "score":53.36,
        "en_score":53.36,
        "conll_en":63.4,
        "sst5":68.17,
        "scala_en":30.92,
        "squad":58.79,
        "cnn_dailymail":69.57,
        "mmlu":47.74,
        "hellaswag":34.96
    },
    {
        "rank":4,
        "Model":"mistralai\/Mistral-7B-Instruct-v0.1 (few-shot)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":5443,
        "score":51.92,
        "en_score":51.92,
        "conll_en":57.58,
        "sst5":61.44,
        "scala_en":34.92,
        "squad":65.46,
        "cnn_dailymail":69.9,
        "mmlu":38.4,
        "hellaswag":35.72
    },
    {
        "rank":4,
        "Model":"RuterNorway\/Llama-2-13b-chat-norwegian (few-shot)",
        "num_model_parameters":"unknown",
        "vocabulary_size":32,
        "max_sequence_length":4096,
        "speed":7778,
        "score":51.81,
        "en_score":51.81,
        "conll_en":64.93,
        "sst5":64.14,
        "scala_en":28.08,
        "squad":62.09,
        "cnn_dailymail":68.84,
        "mmlu":36.49,
        "hellaswag":38.09
    },
    {
        "rank":4,
        "Model":"mistralai\/Mistral-7B-Instruct-v0.2 (few-shot)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":2538,
        "score":51.52,
        "en_score":51.52,
        "conll_en":62.11,
        "sst5":59.91,
        "scala_en":30.66,
        "squad":58.3,
        "cnn_dailymail":69.8,
        "mmlu":34.93,
        "hellaswag":44.91
    },
    {
        "rank":5,
        "Model":"meta-llama\/Llama-2-7b-chat-hf (few-shot)",
        "num_model_parameters":"6738.0",
        "vocabulary_size":32,
        "max_sequence_length":4096,
        "speed":2643,
        "score":48.95,
        "en_score":48.95,
        "conll_en":62.53,
        "sst5":62.23,
        "scala_en":22.71,
        "squad":64.54,
        "cnn_dailymail":69.98,
        "mmlu":30.47,
        "hellaswag":30.18
    },
    {
        "rank":6,
        "Model":"Rijgersberg\/GEITje-7B (few-shot)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":10401,
        "score":44.91,
        "en_score":44.91,
        "conll_en":53.39,
        "sst5":65.21,
        "scala_en":12.63,
        "squad":65.74,
        "cnn_dailymail":68.05,
        "mmlu":31.65,
        "hellaswag":17.69
    },
    {
        "rank":7,
        "Model":"meta-llama\/Llama-2-7b-hf (few-shot)",
        "num_model_parameters":"6738.0",
        "vocabulary_size":32,
        "max_sequence_length":4096,
        "speed":2648,
        "score":41.75,
        "en_score":41.75,
        "conll_en":55.27,
        "sst5":65.16,
        "scala_en":20.43,
        "squad":44.64,
        "cnn_dailymail":68.99,
        "mmlu":25.98,
        "hellaswag":11.77
    },
    {
        "rank":8,
        "Model":"AI-Sweden-Models\/gpt-sw3-20b (few-shot)",
        "num_model_parameters":"20918.0",
        "vocabulary_size":64,
        "max_sequence_length":2048,
        "speed":4880,
        "score":34.39,
        "en_score":34.39,
        "conll_en":45.86,
        "sst5":62.08,
        "scala_en":6.62,
        "squad":65.29,
        "cnn_dailymail":43.45,
        "mmlu":9.1,
        "hellaswag":8.35
    },
    {
        "rank":9,
        "Model":"RuterNorway\/Llama-2-7b-chat-norwegian (few-shot)",
        "num_model_parameters":"unknown",
        "vocabulary_size":32,
        "max_sequence_length":4096,
        "speed":10890,
        "score":20.26,
        "en_score":20.26,
        "conll_en":18.69,
        "sst5":21.95,
        "scala_en":0.01,
        "squad":36.7,
        "cnn_dailymail":60.11,
        "mmlu":3.71,
        "hellaswag":0.62
    },
    {
        "rank":10,
        "Model":"RJuro\/kanelsnegl-v0.1 (few-shot)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":512,
        "speed":9757,
        "score":8.86,
        "en_score":8.86,
        "conll_en":0.0,
        "sst5":0.0,
        "scala_en":0.41,
        "squad":0.0,
        "cnn_dailymail":61.26,
        "mmlu":0.0,
        "hellaswag":0.36
    },
    {
        "rank":11,
        "Model":"ai-forever\/mGPT (few-shot)",
        "num_model_parameters":"unknown",
        "vocabulary_size":100,
        "max_sequence_length":1024,
        "speed":13551,
        "score":6.5,
        "en_score":6.5,
        "conll_en":1.55,
        "sst5":3.71,
        "scala_en":-0.42,
        "squad":5.57,
        "cnn_dailymail":34.87,
        "mmlu":0.37,
        "hellaswag":-0.17
    }
]