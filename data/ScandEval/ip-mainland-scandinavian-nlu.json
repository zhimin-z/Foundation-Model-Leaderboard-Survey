[
    {
        "rank":1,
        "Model":"ltg\/norbert3-large",
        "num_model_parameters":"354.0",
        "vocabulary_size":50,
        "max_sequence_length":508,
        "speed":5048,
        "score":67.78,
        "da_score":60.51,
        "no_score":74.76,
        "sv_score":68.08,
        "dansk":73.62,
        "angry_tweets":48.29,
        "scala_da":71.55,
        "scandiqa_da":48.59,
        "norne_nb":93.12,
        "norne_nn":89.39,
        "norec":64.62,
        "scala_nb":77.97,
        "scala_nn":76.3,
        "norquad":66.03,
        "suc3":79.01,
        "swerec":75.32,
        "scala_sv":69.11,
        "scandiqa_sv":48.88
    },
    {
        "rank":1,
        "Model":"gpt-4-0613 (few-shot, val)",
        "num_model_parameters":"unknown",
        "vocabulary_size":100,
        "max_sequence_length":8192,
        "speed":1244,
        "score":67.34,
        "da_score":61.57,
        "no_score":67.09,
        "sv_score":73.37,
        "dansk":64.94,
        "angry_tweets":59.97,
        "scala_da":71.56,
        "scandiqa_da":49.82,
        "norne_nb":81.16,
        "norne_nn":75.75,
        "norec":72.72,
        "scala_nb":77.3,
        "scala_nn":57.18,
        "norquad":49.93,
        "suc3":76.86,
        "swerec":79.19,
        "scala_sv":80.93,
        "scandiqa_sv":56.5
    },
    {
        "rank":1,
        "Model":"AI-Sweden-Models\/roberta-large-550k",
        "num_model_parameters":"355.0",
        "vocabulary_size":50,
        "max_sequence_length":512,
        "speed":5841,
        "score":66.19,
        "da_score":61.39,
        "no_score":67.77,
        "sv_score":69.42,
        "dansk":75.42,
        "angry_tweets":51.02,
        "scala_da":70.43,
        "scandiqa_da":48.69,
        "norne_nb":91.72,
        "norne_nn":87.57,
        "norec":53.96,
        "scala_nb":75.09,
        "scala_nn":64.77,
        "norquad":57.55,
        "suc3":80.09,
        "swerec":77.45,
        "scala_sv":71.86,
        "scandiqa_sv":48.27
    },
    {
        "rank":2,
        "Model":"danish-foundation-models\/encoder-large-v1",
        "num_model_parameters":"355.0",
        "vocabulary_size":50,
        "max_sequence_length":512,
        "speed":6671,
        "score":64.31,
        "da_score":62.39,
        "no_score":65.49,
        "sv_score":65.05,
        "dansk":74.6,
        "angry_tweets":51.42,
        "scala_da":76.11,
        "scandiqa_da":47.42,
        "norne_nb":88.66,
        "norne_nn":84.59,
        "norec":55.59,
        "scala_nb":71.43,
        "scala_nn":53.3,
        "norquad":57.38,
        "suc3":74.18,
        "swerec":75.11,
        "scala_sv":64.11,
        "scandiqa_sv":46.79
    },
    {
        "rank":2,
        "Model":"KennethEnevoldsen\/dfm-sentence-encoder-large-1",
        "num_model_parameters":"355.0",
        "vocabulary_size":50,
        "max_sequence_length":512,
        "speed":6245,
        "score":64.24,
        "da_score":62.35,
        "no_score":66.32,
        "sv_score":64.05,
        "dansk":74.99,
        "angry_tweets":53.85,
        "scala_da":75.71,
        "scandiqa_da":44.85,
        "norne_nb":86.39,
        "norne_nn":83.22,
        "norec":59.61,
        "scala_nb":67.88,
        "scala_nn":62.44,
        "norquad":55.69,
        "suc3":71.65,
        "swerec":74.92,
        "scala_sv":63.43,
        "scandiqa_sv":46.2
    },
    {
        "rank":2,
        "Model":"KennethEnevoldsen\/dfm-sentence-encoder-large-2",
        "num_model_parameters":"355.0",
        "vocabulary_size":50,
        "max_sequence_length":512,
        "speed":6569,
        "score":64.24,
        "da_score":62.98,
        "no_score":66.24,
        "sv_score":63.52,
        "dansk":75.3,
        "angry_tweets":55.12,
        "scala_da":76.34,
        "scandiqa_da":45.15,
        "norne_nb":86.78,
        "norne_nn":83.28,
        "norec":58.73,
        "scala_nb":70.73,
        "scala_nn":59.58,
        "norquad":56.04,
        "suc3":71.86,
        "swerec":74.67,
        "scala_sv":62.77,
        "scandiqa_sv":44.77
    },
    {
        "rank":2,
        "Model":"google\/rembert",
        "num_model_parameters":"576.0",
        "vocabulary_size":250,
        "max_sequence_length":256,
        "speed":3355,
        "score":63.7,
        "da_score":57.49,
        "no_score":65.53,
        "sv_score":68.1,
        "dansk":70.19,
        "angry_tweets":50.19,
        "scala_da":69.72,
        "scandiqa_da":39.85,
        "norne_nb":88.7,
        "norne_nn":86.11,
        "norec":54.19,
        "scala_nb":69.83,
        "scala_nn":54.84,
        "norquad":58.18,
        "suc3":78.23,
        "swerec":75.99,
        "scala_sv":72.17,
        "scandiqa_sv":46.0
    },
    {
        "rank":2,
        "Model":"microsoft\/mdeberta-v3-base",
        "num_model_parameters":"279.0",
        "vocabulary_size":251,
        "max_sequence_length":512,
        "speed":9237,
        "score":62.86,
        "da_score":56.37,
        "no_score":64.44,
        "sv_score":67.78,
        "dansk":72.9,
        "angry_tweets":43.38,
        "scala_da":67.05,
        "scandiqa_da":42.15,
        "norne_nb":91.9,
        "norne_nn":86.81,
        "norec":53.69,
        "scala_nb":70.55,
        "scala_nn":61.21,
        "norquad":48.82,
        "suc3":78.84,
        "swerec":75.24,
        "scala_sv":72.3,
        "scandiqa_sv":44.74
    },
    {
        "rank":3,
        "Model":"NbAiLab\/nb-roberta-base-scandi",
        "num_model_parameters":"278.0",
        "vocabulary_size":250,
        "max_sequence_length":512,
        "speed":15079,
        "score":62.7,
        "da_score":56.44,
        "no_score":66.16,
        "sv_score":65.49,
        "dansk":73.28,
        "angry_tweets":52.08,
        "scala_da":67.99,
        "scandiqa_da":32.39,
        "norne_nb":92.24,
        "norne_nn":87.58,
        "norec":59.98,
        "scala_nb":70.18,
        "scala_nn":70.81,
        "norquad":44.27,
        "suc3":80.02,
        "swerec":76.21,
        "scala_sv":71.92,
        "scandiqa_sv":33.8
    },
    {
        "rank":3,
        "Model":"intfloat\/multilingual-e5-large",
        "num_model_parameters":"560.0",
        "vocabulary_size":250,
        "max_sequence_length":512,
        "speed":6732,
        "score":62.44,
        "da_score":57.24,
        "no_score":62.56,
        "sv_score":67.54,
        "dansk":69.5,
        "angry_tweets":55.07,
        "scala_da":57.67,
        "scandiqa_da":46.71,
        "norne_nb":89.86,
        "norne_nn":84.32,
        "norec":61.52,
        "scala_nb":62.34,
        "scala_nn":34.88,
        "norquad":53.01,
        "suc3":80.36,
        "swerec":79.65,
        "scala_sv":63.15,
        "scandiqa_sv":46.99
    },
    {
        "rank":3,
        "Model":"NbAiLab\/nb-roberta-base-scandi-1e4",
        "num_model_parameters":"278.0",
        "vocabulary_size":250,
        "max_sequence_length":512,
        "speed":15074,
        "score":61.93,
        "da_score":53.96,
        "no_score":66.29,
        "sv_score":65.53,
        "dansk":72.16,
        "angry_tweets":51.7,
        "scala_da":62.03,
        "scandiqa_da":29.95,
        "norne_nb":92.09,
        "norne_nn":86.85,
        "norec":59.84,
        "scala_nb":73.33,
        "scala_nn":71.06,
        "norquad":43.67,
        "suc3":79.9,
        "swerec":76.2,
        "scala_sv":73.62,
        "scandiqa_sv":32.38
    },
    {
        "rank":3,
        "Model":"ltg\/norbert3-base",
        "num_model_parameters":"124.0",
        "vocabulary_size":50,
        "max_sequence_length":508,
        "speed":11405,
        "score":61.4,
        "da_score":52.38,
        "no_score":69.86,
        "sv_score":61.95,
        "dansk":73.26,
        "angry_tweets":43.94,
        "scala_da":51.62,
        "scandiqa_da":40.7,
        "norne_nb":92.36,
        "norne_nn":88.49,
        "norec":59.73,
        "scala_nb":74.4,
        "scala_nn":68.85,
        "norquad":57.67,
        "suc3":78.21,
        "swerec":71.05,
        "scala_sv":56.02,
        "scandiqa_sv":42.52
    },
    {
        "rank":3,
        "Model":"KennethEnevoldsen\/dfm-sentence-encoder-medium-3",
        "num_model_parameters":"178.0",
        "vocabulary_size":120,
        "max_sequence_length":512,
        "speed":14050,
        "score":61.28,
        "da_score":56.45,
        "no_score":64.01,
        "sv_score":63.39,
        "dansk":71.21,
        "angry_tweets":47.55,
        "scala_da":68.72,
        "scandiqa_da":38.33,
        "norne_nb":91.17,
        "norne_nn":87.3,
        "norec":59.1,
        "scala_nb":74.32,
        "scala_nn":72.94,
        "norquad":34.06,
        "suc3":81.35,
        "swerec":71.16,
        "scala_sv":63.89,
        "scandiqa_sv":37.18
    },
    {
        "rank":3,
        "Model":"vesteinn\/ScandiBERT-no-faroese",
        "num_model_parameters":"124.0",
        "vocabulary_size":50,
        "max_sequence_length":512,
        "speed":15436,
        "score":61.23,
        "da_score":54.43,
        "no_score":63.89,
        "sv_score":65.39,
        "dansk":69.79,
        "angry_tweets":47.73,
        "scala_da":68.28,
        "scandiqa_da":31.9,
        "norne_nb":91.09,
        "norne_nn":85.72,
        "norec":50.9,
        "scala_nb":69.34,
        "scala_nn":66.24,
        "norquad":48.45,
        "suc3":79.08,
        "swerec":72.53,
        "scala_sv":73.01,
        "scandiqa_sv":36.92
    },
    {
        "rank":4,
        "Model":"sentence-transformers\/use-cmlm-multilingual",
        "num_model_parameters":"471.0",
        "vocabulary_size":501,
        "max_sequence_length":512,
        "speed":13305,
        "score":60.83,
        "da_score":53.71,
        "no_score":63.11,
        "sv_score":65.66,
        "dansk":69.17,
        "angry_tweets":48.03,
        "scala_da":55.31,
        "scandiqa_da":42.34,
        "norne_nb":90.08,
        "norne_nn":86.04,
        "norec":56.35,
        "scala_nb":59.38,
        "scala_nn":46.54,
        "norquad":55.05,
        "suc3":80.05,
        "swerec":75.09,
        "scala_sv":61.83,
        "scandiqa_sv":45.69
    },
    {
        "rank":4,
        "Model":"NbAiLab\/nb-bert-base",
        "num_model_parameters":"178.0",
        "vocabulary_size":120,
        "max_sequence_length":512,
        "speed":14050,
        "score":60.67,
        "da_score":54.88,
        "no_score":64.39,
        "sv_score":62.74,
        "dansk":70.36,
        "angry_tweets":46.32,
        "scala_da":66.41,
        "scandiqa_da":36.42,
        "norne_nb":93.01,
        "norne_nn":88.43,
        "norec":60.84,
        "scala_nb":73.89,
        "scala_nn":72.1,
        "norquad":33.01,
        "suc3":80.38,
        "swerec":71.21,
        "scala_sv":64.03,
        "scandiqa_sv":35.33
    },
    {
        "rank":4,
        "Model":"vesteinn\/FoBERT",
        "num_model_parameters":"124.0",
        "vocabulary_size":50,
        "max_sequence_length":512,
        "speed":15623,
        "score":60.15,
        "da_score":54.17,
        "no_score":62.6,
        "sv_score":63.69,
        "dansk":69.65,
        "angry_tweets":49.18,
        "scala_da":65.45,
        "scandiqa_da":32.4,
        "norne_nb":90.65,
        "norne_nn":84.88,
        "norec":52.44,
        "scala_nb":68.77,
        "scala_nn":65.4,
        "norquad":43.13,
        "suc3":78.58,
        "swerec":73.41,
        "scala_sv":71.14,
        "scandiqa_sv":31.62
    },
    {
        "rank":4,
        "Model":"xlm-roberta-large",
        "num_model_parameters":"560.0",
        "vocabulary_size":250,
        "max_sequence_length":512,
        "speed":6663,
        "score":60.14,
        "da_score":55.48,
        "no_score":61.61,
        "sv_score":63.33,
        "dansk":72.74,
        "angry_tweets":48.33,
        "scala_da":57.3,
        "scandiqa_da":43.57,
        "norne_nb":91.66,
        "norne_nn":86.19,
        "norec":50.25,
        "scala_nb":55.51,
        "scala_nn":43.89,
        "norquad":57.57,
        "suc3":80.33,
        "swerec":76.63,
        "scala_sv":49.72,
        "scandiqa_sv":46.64
    },
    {
        "rank":4,
        "Model":"pere\/roberta-debug-8",
        "num_model_parameters":"278.0",
        "vocabulary_size":250,
        "max_sequence_length":512,
        "speed":15103,
        "score":59.95,
        "da_score":54.32,
        "no_score":63.08,
        "sv_score":62.45,
        "dansk":71.34,
        "angry_tweets":49.77,
        "scala_da":64.31,
        "scandiqa_da":31.86,
        "norne_nb":91.16,
        "norne_nn":84.75,
        "norec":55.25,
        "scala_nb":68.03,
        "scala_nn":66.9,
        "norquad":41.65,
        "suc3":74.48,
        "swerec":74.58,
        "scala_sv":69.07,
        "scandiqa_sv":31.66
    },
    {
        "rank":4,
        "Model":"setu4993\/LaBSE",
        "num_model_parameters":"471.0",
        "vocabulary_size":501,
        "max_sequence_length":512,
        "speed":13386,
        "score":58.93,
        "da_score":52.69,
        "no_score":60.74,
        "sv_score":63.36,
        "dansk":71.24,
        "angry_tweets":46.5,
        "scala_da":52.92,
        "scandiqa_da":40.08,
        "norne_nb":90.58,
        "norne_nn":85.21,
        "norec":54.26,
        "scala_nb":59.44,
        "scala_nn":49.3,
        "norquad":46.42,
        "suc3":77.78,
        "swerec":73.58,
        "scala_sv":60.36,
        "scandiqa_sv":41.71
    },
    {
        "rank":5,
        "Model":"gpt-3.5-turbo-0613 (few-shot, val)",
        "num_model_parameters":"unknown",
        "vocabulary_size":100,
        "max_sequence_length":4095,
        "speed":1344,
        "score":58.75,
        "da_score":54.7,
        "no_score":56.17,
        "sv_score":65.37,
        "dansk":59.61,
        "angry_tweets":50.54,
        "scala_da":57.57,
        "scandiqa_da":51.09,
        "norne_nb":77.7,
        "norne_nn":73.92,
        "norec":58.88,
        "scala_nb":54.29,
        "scala_nn":32.82,
        "norquad":46.44,
        "suc3":73.04,
        "swerec":72.77,
        "scala_sv":58.06,
        "scandiqa_sv":57.59
    },
    {
        "rank":5,
        "Model":"pere\/roberta-base-exp-8",
        "num_model_parameters":"278.0",
        "vocabulary_size":250,
        "max_sequence_length":512,
        "speed":15112,
        "score":58.74,
        "da_score":52.79,
        "no_score":63.83,
        "sv_score":59.59,
        "dansk":68.77,
        "angry_tweets":49.66,
        "scala_da":60.13,
        "scandiqa_da":32.6,
        "norne_nb":88.99,
        "norne_nn":82.99,
        "norec":57.37,
        "scala_nb":69.92,
        "scala_nn":70.05,
        "norquad":41.98,
        "suc3":73.44,
        "swerec":73.63,
        "scala_sv":58.91,
        "scandiqa_sv":32.39
    },
    {
        "rank":5,
        "Model":"pere\/roberta-debug-32",
        "num_model_parameters":"278.0",
        "vocabulary_size":250,
        "max_sequence_length":512,
        "speed":14958,
        "score":58.74,
        "da_score":53.4,
        "no_score":60.5,
        "sv_score":62.34,
        "dansk":68.46,
        "angry_tweets":50.48,
        "scala_da":64.34,
        "scandiqa_da":30.3,
        "norne_nb":89.07,
        "norne_nn":83.27,
        "norec":53.23,
        "scala_nb":70.06,
        "scala_nn":66.81,
        "norquad":34.17,
        "suc3":72.25,
        "swerec":75.04,
        "scala_sv":70.16,
        "scandiqa_sv":31.89
    },
    {
        "rank":5,
        "Model":"gpt-3.5-turbo-0613 (few-shot)",
        "num_model_parameters":"unknown",
        "vocabulary_size":100,
        "max_sequence_length":4096,
        "speed":1344,
        "score":58.47,
        "da_score":55.49,
        "no_score":54.81,
        "sv_score":65.09,
        "dansk":59.4,
        "angry_tweets":51.8,
        "scala_da":54.22,
        "scandiqa_da":56.55,
        "norne_nb":74.92,
        "norne_nn":75.34,
        "norec":57.64,
        "scala_nb":49.93,
        "scala_nn":34.22,
        "norquad":44.39,
        "suc3":71.43,
        "swerec":77.5,
        "scala_sv":55.99,
        "scandiqa_sv":55.46
    },
    {
        "rank":5,
        "Model":"pere\/roberta-base-exp-32",
        "num_model_parameters":"278.0",
        "vocabulary_size":250,
        "max_sequence_length":512,
        "speed":15081,
        "score":57.64,
        "da_score":50.05,
        "no_score":62.81,
        "sv_score":60.06,
        "dansk":71.9,
        "angry_tweets":51.33,
        "scala_da":44.45,
        "scandiqa_da":32.51,
        "norne_nb":91.66,
        "norne_nn":87.74,
        "norec":57.43,
        "scala_nb":63.31,
        "scala_nn":62.79,
        "norquad":41.05,
        "suc3":79.75,
        "swerec":74.73,
        "scala_sv":53.55,
        "scandiqa_sv":32.2
    },
    {
        "rank":5,
        "Model":"AI-Sweden-Models\/bert-large-nordic-pile-1M-steps",
        "num_model_parameters":"369.0",
        "vocabulary_size":64,
        "max_sequence_length":512,
        "speed":6571,
        "score":56.03,
        "da_score":46.96,
        "no_score":52.1,
        "sv_score":69.05,
        "dansk":67.4,
        "angry_tweets":41.53,
        "scala_da":41.62,
        "scandiqa_da":37.3,
        "norne_nb":87.5,
        "norne_nn":80.57,
        "norec":47.11,
        "scala_nb":52.62,
        "scala_nn":25.06,
        "norquad":38.4,
        "suc3":80.65,
        "swerec":77.43,
        "scala_sv":76.56,
        "scandiqa_sv":41.54
    },
    {
        "rank":6,
        "Model":"pere\/roberta-base-exp-32B",
        "num_model_parameters":"278.0",
        "vocabulary_size":250,
        "max_sequence_length":512,
        "speed":15103,
        "score":55.06,
        "da_score":51.14,
        "no_score":56.67,
        "sv_score":57.38,
        "dansk":71.81,
        "angry_tweets":47.83,
        "scala_da":54.99,
        "scandiqa_da":29.92,
        "norne_nb":90.6,
        "norne_nn":86.76,
        "norec":52.19,
        "scala_nb":54.98,
        "scala_nn":58.33,
        "norquad":29.17,
        "suc3":77.97,
        "swerec":73.27,
        "scala_sv":47.19,
        "scandiqa_sv":31.07
    },
    {
        "rank":6,
        "Model":"vesteinn\/DanskBERT",
        "num_model_parameters":"124.0",
        "vocabulary_size":50,
        "max_sequence_length":512,
        "speed":15749,
        "score":54.51,
        "da_score":59.57,
        "no_score":52.31,
        "sv_score":51.65,
        "dansk":72.55,
        "angry_tweets":52.86,
        "scala_da":75.2,
        "scandiqa_da":37.65,
        "norne_nb":86.82,
        "norne_nn":79.91,
        "norec":47.84,
        "scala_nb":51.99,
        "scala_nn":30.57,
        "norquad":36.75,
        "suc3":72.33,
        "swerec":67.77,
        "scala_sv":33.79,
        "scandiqa_sv":32.71
    },
    {
        "rank":6,
        "Model":"ltg\/norbert3-small",
        "num_model_parameters":"41.0",
        "vocabulary_size":50,
        "max_sequence_length":508,
        "speed":13515,
        "score":54.2,
        "da_score":48.24,
        "no_score":62.56,
        "sv_score":51.81,
        "dansk":67.89,
        "angry_tweets":39.34,
        "scala_da":50.9,
        "scandiqa_da":34.82,
        "norne_nb":90.02,
        "norne_nn":86.52,
        "norec":51.36,
        "scala_nb":67.29,
        "scala_nn":56.67,
        "norquad":48.63,
        "suc3":74.22,
        "swerec":63.8,
        "scala_sv":37.77,
        "scandiqa_sv":31.45
    },
    {
        "rank":6,
        "Model":"KBLab\/megatron-bert-large-swedish-cased-165k",
        "num_model_parameters":"370.0",
        "vocabulary_size":64,
        "max_sequence_length":512,
        "speed":7138,
        "score":52.91,
        "da_score":41.65,
        "no_score":46.69,
        "sv_score":70.39,
        "dansk":58.5,
        "angry_tweets":41.02,
        "scala_da":27.1,
        "scandiqa_da":39.99,
        "norne_nb":85.99,
        "norne_nn":79.47,
        "norec":39.53,
        "scala_nb":27.39,
        "scala_nn":23.56,
        "norquad":39.01,
        "suc3":81.05,
        "swerec":78.0,
        "scala_sv":76.79,
        "scandiqa_sv":45.71
    },
    {
        "rank":6,
        "Model":"AI-Nordics\/bert-large-swedish-cased",
        "num_model_parameters":"335.0",
        "vocabulary_size":31,
        "max_sequence_length":512,
        "speed":7199,
        "score":52.54,
        "da_score":42.27,
        "no_score":47.34,
        "sv_score":68.02,
        "dansk":60.66,
        "angry_tweets":38.46,
        "scala_da":32.29,
        "scandiqa_da":37.68,
        "norne_nb":83.32,
        "norne_nn":77.97,
        "norec":38.44,
        "scala_nb":37.54,
        "scala_nn":23.1,
        "norquad":39.97,
        "suc3":78.61,
        "swerec":77.47,
        "scala_sv":72.87,
        "scandiqa_sv":43.11
    },
    {
        "rank":7,
        "Model":"cardiffnlp\/twitter-xlm-roberta-base",
        "num_model_parameters":"278.0",
        "vocabulary_size":250,
        "max_sequence_length":512,
        "speed":14837,
        "score":52.05,
        "da_score":47.29,
        "no_score":50.94,
        "sv_score":57.92,
        "dansk":70.1,
        "angry_tweets":45.3,
        "scala_da":51.74,
        "scandiqa_da":22.01,
        "norne_nb":87.7,
        "norne_nn":81.41,
        "norec":48.34,
        "scala_nb":55.3,
        "scala_nn":37.46,
        "norquad":24.49,
        "suc3":72.49,
        "swerec":70.69,
        "scala_sv":56.6,
        "scandiqa_sv":31.89
    },
    {
        "rank":7,
        "Model":"KBLab\/megatron-bert-large-swedish-cased-110k",
        "num_model_parameters":"370.0",
        "vocabulary_size":64,
        "max_sequence_length":512,
        "speed":7075,
        "score":51.65,
        "da_score":41.35,
        "no_score":43.68,
        "sv_score":69.92,
        "dansk":60.18,
        "angry_tweets":39.2,
        "scala_da":26.68,
        "scandiqa_da":39.34,
        "norne_nb":84.03,
        "norne_nn":77.98,
        "norec":39.15,
        "scala_nb":21.39,
        "scala_nn":17.1,
        "norquad":35.32,
        "suc3":80.39,
        "swerec":78.45,
        "scala_sv":76.28,
        "scandiqa_sv":44.56
    },
    {
        "rank":8,
        "Model":"bert-base-multilingual-uncased",
        "num_model_parameters":"167.0",
        "vocabulary_size":106,
        "max_sequence_length":512,
        "speed":13993,
        "score":50.63,
        "da_score":45.57,
        "no_score":51.06,
        "sv_score":55.28,
        "dansk":64.92,
        "angry_tweets":33.5,
        "scala_da":46.75,
        "scandiqa_da":37.09,
        "norne_nb":82.9,
        "norne_nn":77.33,
        "norec":37.28,
        "scala_nb":49.41,
        "scala_nn":43.58,
        "norquad":40.35,
        "suc3":70.85,
        "swerec":63.3,
        "scala_sv":48.97,
        "scandiqa_sv":38.0
    },
    {
        "rank":9,
        "Model":"KB\/bert-base-swedish-cased",
        "num_model_parameters":"125.0",
        "vocabulary_size":50,
        "max_sequence_length":512,
        "speed":16181,
        "score":50.33,
        "da_score":39.21,
        "no_score":43.04,
        "sv_score":68.74,
        "dansk":61.74,
        "angry_tweets":33.28,
        "scala_da":33.15,
        "scandiqa_da":28.67,
        "norne_nb":85.91,
        "norne_nn":79.67,
        "norec":38.7,
        "scala_nb":39.13,
        "scala_nn":24.13,
        "norquad":19.04,
        "suc3":81.95,
        "swerec":75.58,
        "scala_sv":78.86,
        "scandiqa_sv":38.56
    },
    {
        "rank":9,
        "Model":"KBLab\/bert-base-swedish-cased",
        "num_model_parameters":"125.0",
        "vocabulary_size":50,
        "max_sequence_length":512,
        "speed":16164,
        "score":50.13,
        "da_score":39.27,
        "no_score":42.61,
        "sv_score":68.53,
        "dansk":61.74,
        "angry_tweets":33.31,
        "scala_da":33.35,
        "scandiqa_da":28.67,
        "norne_nb":85.33,
        "norne_nn":79.44,
        "norec":38.17,
        "scala_nb":39.49,
        "scala_nn":22.17,
        "norquad":19.04,
        "suc3":81.23,
        "swerec":75.73,
        "scala_sv":78.6,
        "scandiqa_sv":38.56
    },
    {
        "rank":9,
        "Model":"jonfd\/electra-small-nordic",
        "num_model_parameters":"22.0",
        "vocabulary_size":96,
        "max_sequence_length":128,
        "speed":5989,
        "score":49.76,
        "da_score":43.43,
        "no_score":51.22,
        "sv_score":54.63,
        "dansk":65.4,
        "angry_tweets":34.43,
        "scala_da":67.27,
        "scandiqa_da":6.6,
        "norne_nb":84.95,
        "norne_nn":79.57,
        "norec":40.15,
        "scala_nb":72.87,
        "scala_nn":63.77,
        "norquad":14.16,
        "suc3":71.07,
        "swerec":66.42,
        "scala_sv":69.19,
        "scandiqa_sv":11.85
    },
    {
        "rank":9,
        "Model":"bert-base-multilingual-cased",
        "num_model_parameters":"178.0",
        "vocabulary_size":120,
        "max_sequence_length":512,
        "speed":14083,
        "score":49.52,
        "da_score":40.76,
        "no_score":51.05,
        "sv_score":56.75,
        "dansk":63.17,
        "angry_tweets":32.38,
        "scala_da":27.93,
        "scandiqa_da":39.57,
        "norne_nb":88.72,
        "norne_nn":83.08,
        "norec":35.87,
        "scala_nb":44.22,
        "scala_nn":39.55,
        "norquad":40.55,
        "suc3":76.29,
        "swerec":61.78,
        "scala_sv":47.74,
        "scandiqa_sv":41.17
    },
    {
        "rank":9,
        "Model":"KBLab\/megatron-bert-base-swedish-cased-600k",
        "num_model_parameters":"135.0",
        "vocabulary_size":64,
        "max_sequence_length":512,
        "speed":15726,
        "score":49.26,
        "da_score":38.19,
        "no_score":43.03,
        "sv_score":66.55,
        "dansk":57.97,
        "angry_tweets":39.4,
        "scala_da":23.5,
        "scandiqa_da":31.87,
        "norne_nb":82.2,
        "norne_nn":76.64,
        "norec":40.2,
        "scala_nb":24.45,
        "scala_nn":19.18,
        "norquad":30.69,
        "suc3":78.91,
        "swerec":76.09,
        "scala_sv":70.08,
        "scandiqa_sv":41.14
    },
    {
        "rank":9,
        "Model":"Geotrend\/bert-base-en-fr-de-no-da-cased",
        "num_model_parameters":"118.0",
        "vocabulary_size":42,
        "max_sequence_length":512,
        "speed":13973,
        "score":49.23,
        "da_score":44.89,
        "no_score":49.07,
        "sv_score":53.73,
        "dansk":63.38,
        "angry_tweets":34.78,
        "scala_da":41.08,
        "scandiqa_da":40.32,
        "norne_nb":88.05,
        "norne_nn":83.08,
        "norec":35.34,
        "scala_nb":31.45,
        "scala_nn":36.12,
        "norquad":41.59,
        "suc3":76.55,
        "swerec":61.6,
        "scala_sv":37.44,
        "scandiqa_sv":39.32
    },
    {
        "rank":9,
        "Model":"Geotrend\/bert-base-en-no-cased",
        "num_model_parameters":"111.0",
        "vocabulary_size":33,
        "max_sequence_length":512,
        "speed":14081,
        "score":49.11,
        "da_score":44.37,
        "no_score":49.54,
        "sv_score":53.42,
        "dansk":62.66,
        "angry_tweets":33.91,
        "scala_da":40.96,
        "scandiqa_da":39.93,
        "norne_nb":89.07,
        "norne_nn":82.69,
        "norec":34.97,
        "scala_nb":39.58,
        "scala_nn":31.27,
        "norquad":41.89,
        "suc3":75.33,
        "swerec":61.8,
        "scala_sv":36.62,
        "scandiqa_sv":39.95
    },
    {
        "rank":9,
        "Model":"facebook\/xlm-v-base",
        "num_model_parameters":"778.0",
        "vocabulary_size":902,
        "max_sequence_length":512,
        "speed":13135,
        "score":49.09,
        "da_score":47.72,
        "no_score":43.3,
        "sv_score":56.24,
        "dansk":71.42,
        "angry_tweets":31.86,
        "scala_da":52.95,
        "scandiqa_da":34.66,
        "norne_nb":89.99,
        "norne_nn":78.6,
        "norec":17.93,
        "scala_nb":43.46,
        "scala_nn":10.97,
        "norquad":43.74,
        "suc3":68.39,
        "swerec":73.43,
        "scala_sv":45.09,
        "scandiqa_sv":38.04
    },
    {
        "rank":9,
        "Model":"Geotrend\/bert-base-25lang-cased",
        "num_model_parameters":"151.0",
        "vocabulary_size":85,
        "max_sequence_length":512,
        "speed":13908,
        "score":48.84,
        "da_score":40.98,
        "no_score":51.22,
        "sv_score":54.32,
        "dansk":62.53,
        "angry_tweets":32.88,
        "scala_da":29.01,
        "scandiqa_da":39.51,
        "norne_nb":87.99,
        "norne_nn":83.1,
        "norec":36.21,
        "scala_nb":46.43,
        "scala_nn":39.82,
        "norquad":40.01,
        "suc3":75.62,
        "swerec":62.5,
        "scala_sv":38.18,
        "scandiqa_sv":40.96
    },
    {
        "rank":10,
        "Model":"microsoft\/infoxlm-large",
        "num_model_parameters":"560.0",
        "vocabulary_size":250,
        "max_sequence_length":512,
        "speed":6696,
        "score":48.48,
        "da_score":42.97,
        "no_score":47.09,
        "sv_score":55.39,
        "dansk":74.42,
        "angry_tweets":37.94,
        "scala_da":15.26,
        "scandiqa_da":44.25,
        "norne_nb":91.9,
        "norne_nn":86.59,
        "norec":30.56,
        "scala_nb":9.79,
        "scala_nn":6.36,
        "norquad":60.47,
        "suc3":79.53,
        "swerec":75.42,
        "scala_sv":18.44,
        "scandiqa_sv":48.19
    },
    {
        "rank":11,
        "Model":"Geotrend\/bert-base-en-da-cased",
        "num_model_parameters":"111.0",
        "vocabulary_size":33,
        "max_sequence_length":512,
        "speed":14062,
        "score":48.38,
        "da_score":42.7,
        "no_score":48.2,
        "sv_score":54.23,
        "dansk":62.57,
        "angry_tweets":33.67,
        "scala_da":35.79,
        "scandiqa_da":38.77,
        "norne_nb":88.55,
        "norne_nn":83.09,
        "norec":35.16,
        "scala_nb":31.82,
        "scala_nn":32.94,
        "norquad":39.46,
        "suc3":74.88,
        "swerec":61.89,
        "scala_sv":40.22,
        "scandiqa_sv":39.95
    },
    {
        "rank":11,
        "Model":"RJuro\/munin-neuralbeagle-7b (few-shot, val)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":2493,
        "score":48.07,
        "da_score":46.45,
        "no_score":44.18,
        "sv_score":53.56,
        "dansk":51.44,
        "angry_tweets":54.91,
        "scala_da":22.77,
        "scandiqa_da":56.7,
        "norne_nb":61.18,
        "norne_nn":65.16,
        "norec":55.61,
        "scala_nb":20.84,
        "scala_nn":9.12,
        "norquad":42.98,
        "suc3":62.96,
        "swerec":77.13,
        "scala_sv":15.73,
        "scandiqa_sv":58.43
    },
    {
        "rank":11,
        "Model":"Geotrend\/bert-base-da-cased",
        "num_model_parameters":"104.0",
        "vocabulary_size":23,
        "max_sequence_length":512,
        "speed":15432,
        "score":46.94,
        "da_score":40.89,
        "no_score":47.23,
        "sv_score":52.71,
        "dansk":62.76,
        "angry_tweets":32.06,
        "scala_da":30.95,
        "scandiqa_da":37.79,
        "norne_nb":87.52,
        "norne_nn":82.66,
        "norec":32.73,
        "scala_nb":36.41,
        "scala_nn":30.37,
        "norquad":37.71,
        "suc3":74.13,
        "swerec":62.18,
        "scala_sv":36.93,
        "scandiqa_sv":37.59
    },
    {
        "rank":11,
        "Model":"timpal0l\/BeagleCatMunin (few-shot, val)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":2495,
        "score":46.89,
        "da_score":45.38,
        "no_score":41.38,
        "sv_score":53.92,
        "dansk":47.62,
        "angry_tweets":54.73,
        "scala_da":21.8,
        "scandiqa_da":57.39,
        "norne_nb":54.04,
        "norne_nn":62.21,
        "norec":54.74,
        "scala_nb":14.51,
        "scala_nn":5.38,
        "norquad":42.71,
        "suc3":50.53,
        "swerec":77.37,
        "scala_sv":27.84,
        "scandiqa_sv":59.92
    },
    {
        "rank":12,
        "Model":"microsoft\/xlm-align-base",
        "num_model_parameters":"278.0",
        "vocabulary_size":250,
        "max_sequence_length":512,
        "speed":14744,
        "score":46.85,
        "da_score":39.98,
        "no_score":50.53,
        "sv_score":50.02,
        "dansk":70.36,
        "angry_tweets":47.83,
        "scala_da":11.87,
        "scandiqa_da":29.87,
        "norne_nb":90.07,
        "norne_nn":85.65,
        "norec":54.46,
        "scala_nb":12.16,
        "scala_nn":8.99,
        "norquad":49.24,
        "suc3":78.6,
        "swerec":73.67,
        "scala_sv":15.41,
        "scandiqa_sv":32.41
    },
    {
        "rank":13,
        "Model":"merge-crew\/da-sv-dare-ties-density-0.9 (few-shot, val)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":2443,
        "score":46.66,
        "da_score":43.27,
        "no_score":42.72,
        "sv_score":53.98,
        "dansk":45.61,
        "angry_tweets":53.73,
        "scala_da":17.08,
        "scandiqa_da":56.67,
        "norne_nb":48.24,
        "norne_nn":61.5,
        "norec":49.4,
        "scala_nb":24.12,
        "scala_nn":13.2,
        "norquad":47.93,
        "suc3":46.61,
        "swerec":76.38,
        "scala_sv":34.16,
        "scandiqa_sv":58.77
    },
    {
        "rank":13,
        "Model":"merge-crew\/da-sv-slerp (few-shot, val)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":2467,
        "score":46.46,
        "da_score":45.84,
        "no_score":39.43,
        "sv_score":54.1,
        "dansk":45.94,
        "angry_tweets":51.75,
        "scala_da":28.04,
        "scandiqa_da":57.65,
        "norne_nb":49.67,
        "norne_nn":61.11,
        "norec":56.07,
        "scala_nb":3.81,
        "scala_nn":-1.29,
        "norquad":44.98,
        "suc3":46.57,
        "swerec":76.53,
        "scala_sv":33.43,
        "scandiqa_sv":59.87
    },
    {
        "rank":13,
        "Model":"merge-crew\/da-sv-task-arithmetic (few-shot, val)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":2500,
        "score":46.44,
        "da_score":45.76,
        "no_score":39.27,
        "sv_score":54.28,
        "dansk":46.06,
        "angry_tweets":51.51,
        "scala_da":27.68,
        "scandiqa_da":57.78,
        "norne_nb":49.69,
        "norne_nn":61.78,
        "norec":55.87,
        "scala_nb":2.99,
        "scala_nn":-1.29,
        "norquad":44.62,
        "suc3":47.28,
        "swerec":76.62,
        "scala_sv":33.23,
        "scandiqa_sv":60.0
    },
    {
        "rank":13,
        "Model":"Mabeck\/Heidrun-Mistral-7B-chat (few-shot)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":5822,
        "score":46.41,
        "da_score":44.16,
        "no_score":42.9,
        "sv_score":52.16,
        "dansk":50.8,
        "angry_tweets":42.79,
        "scala_da":23.25,
        "scandiqa_da":59.82,
        "norne_nb":61.41,
        "norne_nn":59.49,
        "norec":49.19,
        "scala_nb":15.17,
        "scala_nn":10.78,
        "norquad":48.98,
        "suc3":55.06,
        "swerec":77.5,
        "scala_sv":17.47,
        "scandiqa_sv":58.6
    },
    {
        "rank":13,
        "Model":"KBLab\/megatron-bert-base-swedish-cased-125k",
        "num_model_parameters":"135.0",
        "vocabulary_size":64,
        "max_sequence_length":512,
        "speed":15763,
        "score":46.4,
        "da_score":35.39,
        "no_score":38.03,
        "sv_score":65.78,
        "dansk":53.93,
        "angry_tweets":36.31,
        "scala_da":23.46,
        "scandiqa_da":27.85,
        "norne_nb":77.98,
        "norne_nn":75.0,
        "norec":33.88,
        "scala_nb":24.23,
        "scala_nn":18.18,
        "norquad":20.56,
        "suc3":79.29,
        "swerec":75.85,
        "scala_sv":70.43,
        "scandiqa_sv":37.56
    },
    {
        "rank":13,
        "Model":"birgermoell\/Flashback-Bellman (few-shot, val)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":2887,
        "score":45.83,
        "da_score":43.44,
        "no_score":41.0,
        "sv_score":53.05,
        "dansk":47.71,
        "angry_tweets":48.21,
        "scala_da":19.55,
        "scandiqa_da":58.27,
        "norne_nb":56.44,
        "norne_nn":66.56,
        "norec":53.24,
        "scala_nb":11.96,
        "scala_nn":2.5,
        "norquad":42.02,
        "suc3":55.29,
        "swerec":78.29,
        "scala_sv":18.45,
        "scandiqa_sv":60.18
    },
    {
        "rank":13,
        "Model":"mlabonne\/NeuralBeagle14-7B (few-shot, val)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":8192,
        "speed":2549,
        "score":45.69,
        "da_score":43.95,
        "no_score":41.99,
        "sv_score":51.13,
        "dansk":53.02,
        "angry_tweets":51.29,
        "scala_da":19.73,
        "scandiqa_da":51.75,
        "norne_nb":62.47,
        "norne_nn":66.69,
        "norec":54.04,
        "scala_nb":16.75,
        "scala_nn":13.0,
        "norquad":34.48,
        "suc3":61.25,
        "swerec":76.03,
        "scala_sv":16.28,
        "scandiqa_sv":50.96
    },
    {
        "rank":13,
        "Model":"birgermoell\/BeagleCatMunin-Flashback-Bellman (few-shot, val)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":2890,
        "score":45.68,
        "da_score":45.56,
        "no_score":40.41,
        "sv_score":51.08,
        "dansk":50.4,
        "angry_tweets":52.3,
        "scala_da":21.3,
        "scandiqa_da":58.23,
        "norne_nb":53.96,
        "norne_nn":63.45,
        "norec":52.7,
        "scala_nb":14.87,
        "scala_nn":2.48,
        "norquad":41.56,
        "suc3":52.96,
        "swerec":76.99,
        "scala_sv":14.27,
        "scandiqa_sv":60.1
    },
    {
        "rank":13,
        "Model":"timpal0l\/BeagleCatMunin2 (few-shot, val)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":2477,
        "score":45.57,
        "da_score":42.97,
        "no_score":43.73,
        "sv_score":50.02,
        "dansk":51.53,
        "angry_tweets":47.95,
        "scala_da":14.1,
        "scandiqa_da":58.28,
        "norne_nb":61.17,
        "norne_nn":65.44,
        "norec":58.69,
        "scala_nb":15.03,
        "scala_nn":5.95,
        "norquad":42.42,
        "suc3":60.87,
        "swerec":73.72,
        "scala_sv":6.78,
        "scandiqa_sv":58.69
    },
    {
        "rank":13,
        "Model":"birgermoell\/Rapid-Cycling (few-shot, val)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":2346,
        "score":45.44,
        "da_score":44.68,
        "no_score":39.79,
        "sv_score":51.85,
        "dansk":49.99,
        "angry_tweets":51.25,
        "scala_da":20.66,
        "scandiqa_da":56.81,
        "norne_nb":55.93,
        "norne_nn":63.85,
        "norec":50.41,
        "scala_nb":15.74,
        "scala_nn":2.23,
        "norquad":39.87,
        "suc3":53.66,
        "swerec":77.72,
        "scala_sv":16.22,
        "scandiqa_sv":59.81
    },
    {
        "rank":14,
        "Model":"jhu-clsp\/bernice",
        "num_model_parameters":"278.0",
        "vocabulary_size":250,
        "max_sequence_length":512,
        "speed":5567,
        "score":45.09,
        "da_score":40.81,
        "no_score":41.42,
        "sv_score":53.05,
        "dansk":61.98,
        "angry_tweets":47.2,
        "scala_da":40.52,
        "scandiqa_da":13.53,
        "norne_nb":84.11,
        "norne_nn":77.82,
        "norec":39.63,
        "scala_nb":45.75,
        "scala_nn":33.74,
        "norquad":5.35,
        "suc3":71.34,
        "swerec":70.91,
        "scala_sv":53.52,
        "scandiqa_sv":16.41
    },
    {
        "rank":14,
        "Model":"flax-community\/nordic-roberta-wiki",
        "num_model_parameters":"125.0",
        "vocabulary_size":50,
        "max_sequence_length":512,
        "speed":16227,
        "score":44.99,
        "da_score":41.0,
        "no_score":39.45,
        "sv_score":54.52,
        "dansk":60.82,
        "angry_tweets":34.45,
        "scala_da":41.89,
        "scandiqa_da":26.83,
        "norne_nb":85.42,
        "norne_nn":78.92,
        "norec":36.27,
        "scala_nb":48.07,
        "scala_nn":29.81,
        "norquad":0.44,
        "suc3":72.9,
        "swerec":61.11,
        "scala_sv":55.05,
        "scandiqa_sv":29.04
    },
    {
        "rank":15,
        "Model":"RJuro\/munin-neuralbeagle-SkoleGPTOpenOrca-7b (few-shot, val)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":3008,
        "score":44.85,
        "da_score":42.96,
        "no_score":38.65,
        "sv_score":52.95,
        "dansk":50.83,
        "angry_tweets":43.41,
        "scala_da":19.72,
        "scandiqa_da":57.88,
        "norne_nb":53.68,
        "norne_nn":61.92,
        "norec":47.78,
        "scala_nb":0.91,
        "scala_nn":1.24,
        "norquad":47.95,
        "suc3":59.36,
        "swerec":72.04,
        "scala_sv":22.38,
        "scandiqa_sv":58.03
    },
    {
        "rank":15,
        "Model":"merge-crew\/da-sv-ties (few-shot, val)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":2457,
        "score":44.78,
        "da_score":42.27,
        "no_score":40.84,
        "sv_score":51.23,
        "dansk":45.39,
        "angry_tweets":51.95,
        "scala_da":13.25,
        "scandiqa_da":58.51,
        "norne_nb":47.61,
        "norne_nn":60.57,
        "norec":44.46,
        "scala_nb":23.99,
        "scala_nn":11.6,
        "norquad":47.02,
        "suc3":48.36,
        "swerec":76.57,
        "scala_sv":20.94,
        "scandiqa_sv":59.07
    },
    {
        "rank":16,
        "Model":"microsoft\/infoxlm-base",
        "num_model_parameters":"278.0",
        "vocabulary_size":250,
        "max_sequence_length":512,
        "speed":14918,
        "score":44.7,
        "da_score":39.03,
        "no_score":47.1,
        "sv_score":47.97,
        "dansk":69.78,
        "angry_tweets":46.78,
        "scala_da":11.27,
        "scandiqa_da":28.28,
        "norne_nb":90.14,
        "norne_nn":84.12,
        "norec":44.42,
        "scala_nb":11.2,
        "scala_nn":7.12,
        "norquad":47.69,
        "suc3":79.43,
        "swerec":71.48,
        "scala_sv":7.26,
        "scandiqa_sv":33.72
    },
    {
        "rank":16,
        "Model":"birgermoell\/Munin-NeuralBeagle-NorskGPT (few-shot, val)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":2903,
        "score":44.59,
        "da_score":38.62,
        "no_score":45.91,
        "sv_score":49.25,
        "dansk":51.85,
        "angry_tweets":44.02,
        "scala_da":1.22,
        "scandiqa_da":57.38,
        "norne_nb":63.33,
        "norne_nn":68.84,
        "norec":58.28,
        "scala_nb":18.65,
        "scala_nn":10.72,
        "norquad":44.57,
        "suc3":63.85,
        "swerec":73.72,
        "scala_sv":-0.56,
        "scandiqa_sv":59.98
    },
    {
        "rank":16,
        "Model":"birgermoell\/WestLake-Munin-Cat-NorskGPT (few-shot, val)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":2856,
        "score":44.59,
        "da_score":38.62,
        "no_score":45.91,
        "sv_score":49.25,
        "dansk":51.85,
        "angry_tweets":44.02,
        "scala_da":1.22,
        "scandiqa_da":57.38,
        "norne_nb":63.33,
        "norne_nn":68.84,
        "norec":58.28,
        "scala_nb":18.65,
        "scala_nn":10.72,
        "norquad":44.57,
        "suc3":63.85,
        "swerec":73.72,
        "scala_sv":-0.56,
        "scandiqa_sv":59.98
    },
    {
        "rank":16,
        "Model":"KBLab\/bert-base-swedish-cased-new",
        "num_model_parameters":"135.0",
        "vocabulary_size":64,
        "max_sequence_length":512,
        "speed":15933,
        "score":44.21,
        "da_score":31.39,
        "no_score":36.21,
        "sv_score":65.04,
        "dansk":59.37,
        "angry_tweets":38.46,
        "scala_da":4.61,
        "scandiqa_da":23.13,
        "norne_nb":83.23,
        "norne_nn":79.16,
        "norec":33.94,
        "scala_nb":9.56,
        "scala_nn":4.16,
        "norquad":22.84,
        "suc3":79.99,
        "swerec":76.04,
        "scala_sv":73.52,
        "scandiqa_sv":30.6
    },
    {
        "rank":17,
        "Model":"merge-crew\/da-sv-dare-ties-density-0.6 (few-shot, val)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":2515,
        "score":44.2,
        "da_score":41.34,
        "no_score":40.33,
        "sv_score":50.94,
        "dansk":46.03,
        "angry_tweets":49.59,
        "scala_da":12.72,
        "scandiqa_da":57.03,
        "norne_nb":47.26,
        "norne_nn":59.35,
        "norec":54.93,
        "scala_nb":9.0,
        "scala_nn":5.26,
        "norquad":45.95,
        "suc3":45.12,
        "swerec":78.74,
        "scala_sv":19.74,
        "scandiqa_sv":60.15
    },
    {
        "rank":18,
        "Model":"clips\/mfaq",
        "num_model_parameters":"278.0",
        "vocabulary_size":250,
        "max_sequence_length":128,
        "speed":5591,
        "score":43.83,
        "da_score":39.17,
        "no_score":42.81,
        "sv_score":49.51,
        "dansk":68.49,
        "angry_tweets":45.6,
        "scala_da":28.26,
        "scandiqa_da":14.34,
        "norne_nb":89.46,
        "norne_nn":79.71,
        "norec":52.91,
        "scala_nb":27.55,
        "scala_nn":15.2,
        "norquad":12.36,
        "suc3":76.31,
        "swerec":73.32,
        "scala_sv":32.29,
        "scandiqa_sv":16.12
    },
    {
        "rank":18,
        "Model":"sentence-transformers\/paraphrase-xlm-r-multilingual-v1",
        "num_model_parameters":"278.0",
        "vocabulary_size":250,
        "max_sequence_length":512,
        "speed":14994,
        "score":43.76,
        "da_score":41.52,
        "no_score":39.83,
        "sv_score":49.95,
        "dansk":61.17,
        "angry_tweets":46.39,
        "scala_da":38.61,
        "scandiqa_da":19.9,
        "norne_nb":81.26,
        "norne_nn":74.05,
        "norec":49.93,
        "scala_nb":38.26,
        "scala_nn":25.17,
        "norquad":0.0,
        "suc3":70.22,
        "swerec":71.33,
        "scala_sv":39.6,
        "scandiqa_sv":18.65
    },
    {
        "rank":19,
        "Model":"ThatsGroes\/munin-SkoleGPTOpenOrca-7b-16bit (few-shot, val)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":3002,
        "score":43.65,
        "da_score":42.93,
        "no_score":38.63,
        "sv_score":49.39,
        "dansk":43.61,
        "angry_tweets":42.31,
        "scala_da":24.82,
        "scandiqa_da":60.99,
        "norne_nb":48.64,
        "norne_nn":58.21,
        "norec":51.25,
        "scala_nb":1.6,
        "scala_nn":0.0,
        "norquad":49.04,
        "suc3":47.73,
        "swerec":75.86,
        "scala_sv":15.31,
        "scandiqa_sv":58.64
    },
    {
        "rank":19,
        "Model":"flax-community\/swe-roberta-wiki-oscar",
        "num_model_parameters":"125.0",
        "vocabulary_size":50,
        "max_sequence_length":512,
        "speed":15437,
        "score":43.53,
        "da_score":35.03,
        "no_score":33.88,
        "sv_score":61.67,
        "dansk":55.98,
        "angry_tweets":36.66,
        "scala_da":22.69,
        "scandiqa_da":24.81,
        "norne_nb":79.25,
        "norne_nn":75.39,
        "norec":36.56,
        "scala_nb":22.02,
        "scala_nn":19.72,
        "norquad":0.78,
        "suc3":75.4,
        "swerec":76.22,
        "scala_sv":65.73,
        "scandiqa_sv":29.34
    },
    {
        "rank":19,
        "Model":"bineric\/NorskGPT-Mistral-7b (few-shot)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":2443,
        "score":43.37,
        "da_score":37.1,
        "no_score":45.06,
        "sv_score":47.96,
        "dansk":50.76,
        "angry_tweets":40.41,
        "scala_da":0.0,
        "scandiqa_da":57.24,
        "norne_nb":63.28,
        "norne_nn":61.25,
        "norec":56.9,
        "scala_nb":13.86,
        "scala_nn":10.17,
        "norquad":49.06,
        "suc3":58.4,
        "swerec":74.3,
        "scala_sv":0.0,
        "scandiqa_sv":59.13
    },
    {
        "rank":20,
        "Model":"distilbert-base-multilingual-cased",
        "num_model_parameters":"135.0",
        "vocabulary_size":120,
        "max_sequence_length":512,
        "speed":26355,
        "score":43.08,
        "da_score":38.59,
        "no_score":41.92,
        "sv_score":48.73,
        "dansk":58.12,
        "angry_tweets":32.53,
        "scala_da":35.53,
        "scandiqa_da":28.19,
        "norne_nb":83.62,
        "norne_nn":80.69,
        "norec":33.16,
        "scala_nb":36.1,
        "scala_nn":30.1,
        "norquad":19.26,
        "suc3":70.08,
        "swerec":59.66,
        "scala_sv":33.71,
        "scandiqa_sv":31.48
    },
    {
        "rank":20,
        "Model":"sentence-transformers\/paraphrase-multilingual-mpnet-base-v2",
        "num_model_parameters":"278.0",
        "vocabulary_size":250,
        "max_sequence_length":512,
        "speed":15100,
        "score":42.8,
        "da_score":39.99,
        "no_score":39.95,
        "sv_score":48.47,
        "dansk":61.18,
        "angry_tweets":49.13,
        "scala_da":29.66,
        "scandiqa_da":19.99,
        "norne_nb":81.94,
        "norne_nn":75.56,
        "norec":55.53,
        "scala_nb":36.01,
        "scala_nn":14.99,
        "norquad":0.0,
        "suc3":65.14,
        "swerec":73.47,
        "scala_sv":36.62,
        "scandiqa_sv":18.65
    },
    {
        "rank":21,
        "Model":"ThatsGroes\/munin-SkoleGPTOpenOrca-7b-16bit (few-shot)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":3006,
        "score":42.77,
        "da_score":41.28,
        "no_score":37.91,
        "sv_score":49.13,
        "dansk":45.37,
        "angry_tweets":39.63,
        "scala_da":21.77,
        "scandiqa_da":58.35,
        "norne_nb":51.99,
        "norne_nn":52.74,
        "norec":50.39,
        "scala_nb":0.99,
        "scala_nn":1.27,
        "norquad":47.77,
        "suc3":44.64,
        "swerec":77.98,
        "scala_sv":16.57,
        "scandiqa_sv":57.33
    },
    {
        "rank":21,
        "Model":"sentence-transformers\/stsb-xlm-r-multilingual",
        "num_model_parameters":"278.0",
        "vocabulary_size":250,
        "max_sequence_length":512,
        "speed":15040,
        "score":42.66,
        "da_score":38.79,
        "no_score":38.69,
        "sv_score":50.5,
        "dansk":58.52,
        "angry_tweets":42.26,
        "scala_da":34.8,
        "scandiqa_da":19.6,
        "norne_nb":80.08,
        "norne_nn":74.59,
        "norec":52.16,
        "scala_nb":36.3,
        "scala_nn":14.21,
        "norquad":0.0,
        "suc3":68.94,
        "swerec":72.77,
        "scala_sv":40.21,
        "scandiqa_sv":20.09
    },
    {
        "rank":21,
        "Model":"timpal0l\/Mistral-7B-v0.1-flashback-v2 (few-shot)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":2505,
        "score":42.66,
        "da_score":39.45,
        "no_score":35.76,
        "sv_score":52.77,
        "dansk":41.66,
        "angry_tweets":47.52,
        "scala_da":17.36,
        "scandiqa_da":51.28,
        "norne_nb":48.28,
        "norne_nn":50.51,
        "norec":49.76,
        "scala_nb":14.54,
        "scala_nn":9.16,
        "norquad":32.04,
        "suc3":44.16,
        "swerec":80.29,
        "scala_sv":34.8,
        "scandiqa_sv":51.82
    },
    {
        "rank":22,
        "Model":"DDSC\/roberta-base-scandinavian",
        "num_model_parameters":"125.0",
        "vocabulary_size":50,
        "max_sequence_length":512,
        "speed":14491,
        "score":42.6,
        "da_score":36.91,
        "no_score":41.07,
        "sv_score":49.83,
        "dansk":43.9,
        "angry_tweets":44.48,
        "scala_da":30.37,
        "scandiqa_da":28.89,
        "norne_nb":71.73,
        "norne_nn":79.8,
        "norec":46.74,
        "scala_nb":8.02,
        "scala_nn":17.04,
        "norquad":29.26,
        "suc3":58.84,
        "swerec":72.28,
        "scala_sv":37.61,
        "scandiqa_sv":30.59
    },
    {
        "rank":23,
        "Model":"Geotrend\/distilbert-base-25lang-cased",
        "num_model_parameters":"109.0",
        "vocabulary_size":85,
        "max_sequence_length":512,
        "speed":26099,
        "score":42.44,
        "da_score":37.99,
        "no_score":40.96,
        "sv_score":48.37,
        "dansk":58.44,
        "angry_tweets":31.81,
        "scala_da":34.13,
        "scandiqa_da":27.6,
        "norne_nb":83.59,
        "norne_nn":80.29,
        "norec":33.19,
        "scala_nb":32.6,
        "scala_nn":24.97,
        "norquad":19.93,
        "suc3":70.56,
        "swerec":60.69,
        "scala_sv":30.83,
        "scandiqa_sv":31.41
    },
    {
        "rank":23,
        "Model":"Geotrend\/distilbert-base-en-fr-de-no-da-cased",
        "num_model_parameters":"76.0",
        "vocabulary_size":42,
        "max_sequence_length":512,
        "speed":26081,
        "score":42.4,
        "da_score":38.22,
        "no_score":41.29,
        "sv_score":47.68,
        "dansk":58.78,
        "angry_tweets":31.3,
        "scala_da":34.92,
        "scandiqa_da":27.86,
        "norne_nb":83.49,
        "norne_nn":80.23,
        "norec":32.66,
        "scala_nb":33.65,
        "scala_nn":29.07,
        "norquad":19.29,
        "suc3":69.94,
        "swerec":59.83,
        "scala_sv":29.82,
        "scandiqa_sv":31.13
    },
    {
        "rank":23,
        "Model":"Geotrend\/distilbert-base-en-no-cased",
        "num_model_parameters":"69.0",
        "vocabulary_size":33,
        "max_sequence_length":512,
        "speed":26597,
        "score":42.23,
        "da_score":37.83,
        "no_score":41.71,
        "sv_score":47.15,
        "dansk":57.53,
        "angry_tweets":32.95,
        "scala_da":33.63,
        "scandiqa_da":27.21,
        "norne_nb":83.93,
        "norne_nn":79.39,
        "norec":32.32,
        "scala_nb":36.15,
        "scala_nn":30.17,
        "norquad":19.71,
        "suc3":69.28,
        "swerec":59.53,
        "scala_sv":29.36,
        "scandiqa_sv":30.42
    },
    {
        "rank":23,
        "Model":"Geotrend\/distilbert-base-en-da-cased",
        "num_model_parameters":"69.0",
        "vocabulary_size":33,
        "max_sequence_length":512,
        "speed":26196,
        "score":41.91,
        "da_score":38.95,
        "no_score":39.3,
        "sv_score":47.47,
        "dansk":59.5,
        "angry_tweets":31.89,
        "scala_da":36.0,
        "scandiqa_da":28.41,
        "norne_nb":83.27,
        "norne_nn":79.59,
        "norec":29.37,
        "scala_nb":31.5,
        "scala_nn":24.06,
        "norquad":18.62,
        "suc3":69.62,
        "swerec":59.42,
        "scala_sv":29.01,
        "scandiqa_sv":31.82
    },
    {
        "rank":23,
        "Model":"Geotrend\/distilbert-base-da-cased",
        "num_model_parameters":"61.0",
        "vocabulary_size":23,
        "max_sequence_length":512,
        "speed":28950,
        "score":41.63,
        "da_score":38.19,
        "no_score":39.67,
        "sv_score":47.03,
        "dansk":58.36,
        "angry_tweets":32.13,
        "scala_da":34.75,
        "scandiqa_da":27.5,
        "norne_nb":82.84,
        "norne_nn":78.83,
        "norec":30.7,
        "scala_nb":34.24,
        "scala_nn":27.2,
        "norquad":16.44,
        "suc3":69.25,
        "swerec":58.47,
        "scala_sv":29.8,
        "scandiqa_sv":30.61
    },
    {
        "rank":23,
        "Model":"birgermoell\/NeuralBeagle-Flashback (few-shot, val)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":2904,
        "score":41.63,
        "da_score":43.36,
        "no_score":39.98,
        "sv_score":41.56,
        "dansk":48.28,
        "angry_tweets":44.2,
        "scala_da":22.79,
        "scandiqa_da":58.16,
        "norne_nb":51.78,
        "norne_nn":61.22,
        "norec":53.06,
        "scala_nb":10.27,
        "scala_nn":8.06,
        "norquad":41.18,
        "suc3":51.73,
        "swerec":36.06,
        "scala_sv":19.42,
        "scandiqa_sv":59.03
    },
    {
        "rank":24,
        "Model":"sarnikowski\/convbert-medium-small-da-cased",
        "num_model_parameters":"24.0",
        "vocabulary_size":29,
        "max_sequence_length":512,
        "speed":13821,
        "score":40.91,
        "da_score":47.3,
        "no_score":36.92,
        "sv_score":38.5,
        "dansk":64.28,
        "angry_tweets":36.85,
        "scala_da":63.55,
        "scandiqa_da":24.52,
        "norne_nb":79.5,
        "norne_nn":73.03,
        "norec":32.4,
        "scala_nb":41.65,
        "scala_nn":25.53,
        "norquad":5.41,
        "suc3":58.01,
        "swerec":57.67,
        "scala_sv":13.4,
        "scandiqa_sv":24.92
    },
    {
        "rank":24,
        "Model":"RuterNorway\/Llama-2-13b-chat-norwegian (few-shot)",
        "num_model_parameters":"unknown",
        "vocabulary_size":32,
        "max_sequence_length":4096,
        "speed":7778,
        "score":40.78,
        "da_score":38.61,
        "no_score":36.26,
        "sv_score":47.46,
        "dansk":43.17,
        "angry_tweets":43.4,
        "scala_da":11.08,
        "scandiqa_da":56.81,
        "norne_nb":58.61,
        "norne_nn":60.4,
        "norec":41.36,
        "scala_nb":6.52,
        "scala_nn":3.95,
        "norquad":38.93,
        "suc3":50.85,
        "swerec":74.17,
        "scala_sv":7.51,
        "scandiqa_sv":57.32
    },
    {
        "rank":24,
        "Model":"mistralai\/Mistral-7B-Instruct-v0.2 (few-shot)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":2538,
        "score":40.76,
        "da_score":40.88,
        "no_score":35.59,
        "sv_score":45.82,
        "dansk":44.89,
        "angry_tweets":48.09,
        "scala_da":19.06,
        "scandiqa_da":51.49,
        "norne_nb":53.42,
        "norne_nn":54.34,
        "norec":38.79,
        "scala_nb":17.06,
        "scala_nn":11.0,
        "norquad":35.68,
        "suc3":47.92,
        "swerec":62.9,
        "scala_sv":19.95,
        "scandiqa_sv":52.5
    },
    {
        "rank":25,
        "Model":"Mabeck\/Heidrun-Mistral-7B-base (few-shot)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":5161,
        "score":40.72,
        "da_score":39.63,
        "no_score":33.37,
        "sv_score":49.16,
        "dansk":46.3,
        "angry_tweets":45.44,
        "scala_da":8.46,
        "scandiqa_da":58.33,
        "norne_nb":54.34,
        "norne_nn":55.98,
        "norec":45.04,
        "scala_nb":2.22,
        "scala_nn":2.52,
        "norquad":30.9,
        "suc3":54.45,
        "swerec":80.2,
        "scala_sv":3.78,
        "scandiqa_sv":58.21
    },
    {
        "rank":26,
        "Model":"Addedk\/kbbert-distilled-cased",
        "num_model_parameters":"82.0",
        "vocabulary_size":50,
        "max_sequence_length":512,
        "speed":29698,
        "score":40.23,
        "da_score":31.25,
        "no_score":31.65,
        "sv_score":57.79,
        "dansk":57.84,
        "angry_tweets":31.18,
        "scala_da":13.25,
        "scandiqa_da":22.73,
        "norne_nb":81.82,
        "norne_nn":75.89,
        "norec":33.42,
        "scala_nb":14.99,
        "scala_nn":13.63,
        "norquad":0.0,
        "suc3":80.12,
        "swerec":71.28,
        "scala_sv":51.58,
        "scandiqa_sv":28.16
    },
    {
        "rank":26,
        "Model":"Twitter\/twhin-bert-large",
        "num_model_parameters":"561.0",
        "vocabulary_size":250,
        "max_sequence_length":512,
        "speed":5299,
        "score":39.54,
        "da_score":36.67,
        "no_score":34.32,
        "sv_score":47.61,
        "dansk":66.39,
        "angry_tweets":39.36,
        "scala_da":7.06,
        "scandiqa_da":33.88,
        "norne_nb":86.26,
        "norne_nn":80.1,
        "norec":34.17,
        "scala_nb":12.11,
        "scala_nn":4.28,
        "norquad":11.74,
        "suc3":74.26,
        "swerec":63.35,
        "scala_sv":16.07,
        "scandiqa_sv":36.77
    },
    {
        "rank":27,
        "Model":"mistralai\/Mistral-7B-Instruct-v0.1 (few-shot)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":5443,
        "score":39.39,
        "da_score":36.98,
        "no_score":35.7,
        "sv_score":45.5,
        "dansk":37.93,
        "angry_tweets":44.49,
        "scala_da":14.09,
        "scandiqa_da":51.42,
        "norne_nb":50.08,
        "norne_nn":51.27,
        "norec":43.65,
        "scala_nb":14.09,
        "scala_nn":8.28,
        "norquad":37.31,
        "suc3":45.01,
        "swerec":73.33,
        "scala_sv":11.59,
        "scandiqa_sv":52.05
    },
    {
        "rank":27,
        "Model":"sentence-transformers\/paraphrase-multilingual-MiniLM-L12-v2",
        "num_model_parameters":"118.0",
        "vocabulary_size":250,
        "max_sequence_length":512,
        "speed":17428,
        "score":39.39,
        "da_score":36.46,
        "no_score":35.88,
        "sv_score":45.84,
        "dansk":56.75,
        "angry_tweets":44.48,
        "scala_da":26.74,
        "scandiqa_da":17.89,
        "norne_nb":78.31,
        "norne_nn":72.13,
        "norec":47.53,
        "scala_nb":26.92,
        "scala_nn":14.63,
        "norquad":0.0,
        "suc3":66.5,
        "swerec":72.19,
        "scala_sv":28.75,
        "scandiqa_sv":15.91
    },
    {
        "rank":27,
        "Model":"mistralai\/Mistral-7B-v0.1 (few-shot)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":2657,
        "score":39.19,
        "da_score":36.47,
        "no_score":34.5,
        "sv_score":46.6,
        "dansk":45.42,
        "angry_tweets":43.16,
        "scala_da":8.79,
        "scandiqa_da":48.51,
        "norne_nb":52.0,
        "norne_nn":55.12,
        "norec":47.25,
        "scala_nb":8.66,
        "scala_nn":6.8,
        "norquad":29.44,
        "suc3":53.34,
        "swerec":80.0,
        "scala_sv":4.61,
        "scandiqa_sv":48.43
    },
    {
        "rank":27,
        "Model":"neph1\/bellman-7b-mistral-instruct-v0.2 (few-shot)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":2518,
        "score":38.91,
        "da_score":39.76,
        "no_score":33.3,
        "sv_score":43.68,
        "dansk":46.11,
        "angry_tweets":47.58,
        "scala_da":18.41,
        "scandiqa_da":46.93,
        "norne_nb":57.01,
        "norne_nn":56.77,
        "norec":38.81,
        "scala_nb":14.16,
        "scala_nn":9.29,
        "norquad":25.79,
        "suc3":54.38,
        "swerec":55.84,
        "scala_sv":16.05,
        "scandiqa_sv":48.44
    },
    {
        "rank":27,
        "Model":"danish-foundation-models\/encoder-medium-v1",
        "num_model_parameters":"111.0",
        "vocabulary_size":32,
        "max_sequence_length":512,
        "speed":16130,
        "score":38.23,
        "da_score":45.02,
        "no_score":35.66,
        "sv_score":34.0,
        "dansk":63.42,
        "angry_tweets":39.91,
        "scala_da":51.01,
        "scandiqa_da":25.76,
        "norne_nb":68.66,
        "norne_nn":61.77,
        "norec":36.56,
        "scala_nb":31.23,
        "scala_nn":5.4,
        "norquad":22.56,
        "suc3":49.62,
        "swerec":58.7,
        "scala_sv":2.23,
        "scandiqa_sv":25.45
    },
    {
        "rank":28,
        "Model":"Addedk\/mbert-swedish-distilled-cased",
        "num_model_parameters":"135.0",
        "vocabulary_size":120,
        "max_sequence_length":512,
        "speed":26091,
        "score":38.07,
        "da_score":32.06,
        "no_score":35.05,
        "sv_score":47.12,
        "dansk":56.36,
        "angry_tweets":31.16,
        "scala_da":21.08,
        "scandiqa_da":19.63,
        "norne_nb":82.98,
        "norne_nn":76.65,
        "norec":30.38,
        "scala_nb":21.99,
        "scala_nn":19.06,
        "norquad":9.47,
        "suc3":73.41,
        "swerec":62.1,
        "scala_sv":34.86,
        "scandiqa_sv":18.1
    },
    {
        "rank":29,
        "Model":"jannikskytt\/MeDa-Bert",
        "num_model_parameters":"111.0",
        "vocabulary_size":32,
        "max_sequence_length":511,
        "speed":16114,
        "score":38.05,
        "da_score":44.97,
        "no_score":36.99,
        "sv_score":32.2,
        "dansk":64.64,
        "angry_tweets":44.62,
        "scala_da":47.47,
        "scandiqa_da":23.14,
        "norne_nb":71.69,
        "norne_nn":60.0,
        "norec":38.94,
        "scala_nb":30.32,
        "scala_nn":7.99,
        "norquad":24.02,
        "suc3":48.32,
        "swerec":53.98,
        "scala_sv":3.33,
        "scandiqa_sv":23.15
    },
    {
        "rank":29,
        "Model":"sarnikowski\/electra-small-discriminator-da-256-cased",
        "num_model_parameters":"13.0",
        "vocabulary_size":29,
        "max_sequence_length":512,
        "speed":20340,
        "score":37.93,
        "da_score":43.66,
        "no_score":33.65,
        "sv_score":36.49,
        "dansk":60.63,
        "angry_tweets":24.38,
        "scala_da":68.58,
        "scandiqa_da":21.03,
        "norne_nb":73.15,
        "norne_nn":66.34,
        "norec":29.97,
        "scala_nb":40.79,
        "scala_nn":25.08,
        "norquad":1.93,
        "suc3":52.79,
        "swerec":57.93,
        "scala_sv":14.72,
        "scandiqa_sv":20.54
    },
    {
        "rank":29,
        "Model":"birgermoell\/roberta-swedish-scandi",
        "num_model_parameters":"125.0",
        "vocabulary_size":50,
        "max_sequence_length":512,
        "speed":15385,
        "score":37.74,
        "da_score":29.82,
        "no_score":28.56,
        "sv_score":54.84,
        "dansk":49.22,
        "angry_tweets":33.51,
        "scala_da":12.08,
        "scandiqa_da":24.49,
        "norne_nb":72.74,
        "norne_nn":69.74,
        "norec":29.68,
        "scala_nb":15.83,
        "scala_nn":8.7,
        "norquad":1.04,
        "suc3":68.55,
        "swerec":69.96,
        "scala_sv":52.88,
        "scandiqa_sv":27.99
    },
    {
        "rank":29,
        "Model":"Maltehb\/danish-bert-botxo",
        "num_model_parameters":"111.0",
        "vocabulary_size":32,
        "max_sequence_length":512,
        "speed":16091,
        "score":37.33,
        "da_score":45.69,
        "no_score":32.11,
        "sv_score":34.2,
        "dansk":66.71,
        "angry_tweets":43.79,
        "scala_da":45.96,
        "scandiqa_da":26.29,
        "norne_nb":72.62,
        "norne_nn":58.73,
        "norec":40.65,
        "scala_nb":29.47,
        "scala_nn":12.95,
        "norquad":0.91,
        "suc3":50.29,
        "swerec":57.42,
        "scala_sv":4.94,
        "scandiqa_sv":24.16
    },
    {
        "rank":29,
        "Model":"sarnikowski\/convbert-small-da-cased",
        "num_model_parameters":"13.0",
        "vocabulary_size":29,
        "max_sequence_length":512,
        "speed":14273,
        "score":37.26,
        "da_score":41.84,
        "no_score":34.03,
        "sv_score":35.92,
        "dansk":60.59,
        "angry_tweets":29.52,
        "scala_da":57.1,
        "scandiqa_da":20.16,
        "norne_nb":76.07,
        "norne_nn":70.94,
        "norec":32.49,
        "scala_nb":35.43,
        "scala_nn":21.11,
        "norquad":1.84,
        "suc3":55.06,
        "swerec":53.7,
        "scala_sv":12.38,
        "scandiqa_sv":22.53
    },
    {
        "rank":30,
        "Model":"ltg\/norbert3-xs",
        "num_model_parameters":"15.0",
        "vocabulary_size":50,
        "max_sequence_length":508,
        "speed":14208,
        "score":36.87,
        "da_score":31.49,
        "no_score":40.7,
        "sv_score":38.44,
        "dansk":59.94,
        "angry_tweets":39.16,
        "scala_da":2.16,
        "scandiqa_da":24.69,
        "norne_nb":87.63,
        "norne_nn":80.19,
        "norec":49.92,
        "scala_nb":7.93,
        "scala_nn":5.06,
        "norquad":22.46,
        "suc3":67.53,
        "swerec":59.27,
        "scala_sv":2.83,
        "scandiqa_sv":24.11
    },
    {
        "rank":30,
        "Model":"DDSC\/roberta-base-danish",
        "num_model_parameters":"125.0",
        "vocabulary_size":50,
        "max_sequence_length":512,
        "speed":15004,
        "score":36.83,
        "da_score":37.96,
        "no_score":32.72,
        "sv_score":39.81,
        "dansk":63.84,
        "angry_tweets":43.9,
        "scala_da":17.16,
        "scandiqa_da":26.94,
        "norne_nb":76.14,
        "norne_nn":72.88,
        "norec":32.29,
        "scala_nb":0.45,
        "scala_nn":-0.08,
        "norquad":23.91,
        "suc3":65.95,
        "swerec":64.02,
        "scala_sv":0.8,
        "scandiqa_sv":28.46
    },
    {
        "rank":30,
        "Model":"meta-llama\/Llama-2-7b-chat-hf (few-shot)",
        "num_model_parameters":"6738.0",
        "vocabulary_size":32,
        "max_sequence_length":4096,
        "speed":2643,
        "score":36.47,
        "da_score":36.27,
        "no_score":31.47,
        "sv_score":41.68,
        "dansk":35.44,
        "angry_tweets":44.88,
        "scala_da":9.74,
        "scandiqa_da":55.0,
        "norne_nb":44.99,
        "norne_nn":49.09,
        "norec":41.56,
        "scala_nb":3.04,
        "scala_nn":4.03,
        "norquad":33.76,
        "suc3":39.72,
        "swerec":66.18,
        "scala_sv":6.74,
        "scandiqa_sv":54.07
    },
    {
        "rank":30,
        "Model":"merge-crew\/da-sv-dare-ties-density-0.3 (few-shot, val)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":2461,
        "score":36.39,
        "da_score":34.15,
        "no_score":31.64,
        "sv_score":43.37,
        "dansk":30.16,
        "angry_tweets":48.49,
        "scala_da":5.52,
        "scandiqa_da":52.44,
        "norne_nb":35.98,
        "norne_nn":47.39,
        "norec":38.98,
        "scala_nb":11.54,
        "scala_nn":5.2,
        "norquad":37.54,
        "suc3":32.37,
        "swerec":75.33,
        "scala_sv":12.73,
        "scandiqa_sv":53.05
    },
    {
        "rank":30,
        "Model":"AI-Sweden-Models\/gpt-sw3-20b (few-shot)",
        "num_model_parameters":"20918.0",
        "vocabulary_size":64,
        "max_sequence_length":2048,
        "speed":4880,
        "score":35.39,
        "da_score":30.45,
        "no_score":31.58,
        "sv_score":44.14,
        "dansk":27.41,
        "angry_tweets":30.24,
        "scala_da":11.34,
        "scandiqa_da":52.8,
        "norne_nb":30.82,
        "norne_nn":39.56,
        "norec":34.51,
        "scala_nb":15.17,
        "scala_nn":12.46,
        "norquad":42.81,
        "suc3":31.86,
        "swerec":78.88,
        "scala_sv":12.26,
        "scandiqa_sv":53.58
    },
    {
        "rank":30,
        "Model":"Maltehb\/aelaectra-danish-electra-small-cased",
        "num_model_parameters":"14.0",
        "vocabulary_size":32,
        "max_sequence_length":128,
        "speed":6035,
        "score":35.23,
        "da_score":40.94,
        "no_score":31.55,
        "sv_score":33.19,
        "dansk":63.31,
        "angry_tweets":32.72,
        "scala_da":67.74,
        "scandiqa_da":0.0,
        "norne_nb":71.85,
        "norne_nn":67.14,
        "norec":29.0,
        "scala_nb":33.57,
        "scala_nn":21.79,
        "norquad":0.03,
        "suc3":57.82,
        "swerec":55.68,
        "scala_sv":19.26,
        "scandiqa_sv":0.0
    },
    {
        "rank":30,
        "Model":"norallm\/normistral-7b-warm (few-shot)",
        "num_model_parameters":"7248.0",
        "vocabulary_size":33,
        "max_sequence_length":2048,
        "speed":3175,
        "score":34.97,
        "da_score":32.69,
        "no_score":28.16,
        "sv_score":44.08,
        "dansk":37.8,
        "angry_tweets":40.51,
        "scala_da":3.35,
        "scandiqa_da":49.08,
        "norne_nb":42.29,
        "norne_nn":46.29,
        "norec":27.05,
        "scala_nb":1.63,
        "scala_nn":2.57,
        "norquad":39.18,
        "suc3":48.78,
        "swerec":76.09,
        "scala_sv":2.53,
        "scandiqa_sv":48.93
    },
    {
        "rank":31,
        "Model":"danish-foundation-models\/munin-7b-alpha (few-shot)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":3019,
        "score":34.76,
        "da_score":35.42,
        "no_score":25.42,
        "sv_score":43.43,
        "dansk":38.31,
        "angry_tweets":37.13,
        "scala_da":26.46,
        "scandiqa_da":39.77,
        "norne_nb":46.32,
        "norne_nn":48.2,
        "norec":20.46,
        "scala_nb":4.5,
        "scala_nn":1.1,
        "norquad":31.16,
        "suc3":39.55,
        "swerec":78.79,
        "scala_sv":15.77,
        "scandiqa_sv":39.62
    },
    {
        "rank":31,
        "Model":"dbmdz\/bert-base-historic-multilingual-cased",
        "num_model_parameters":"111.0",
        "vocabulary_size":32,
        "max_sequence_length":512,
        "speed":15165,
        "score":34.75,
        "da_score":26.28,
        "no_score":30.36,
        "sv_score":47.62,
        "dansk":47.61,
        "angry_tweets":24.17,
        "scala_da":8.14,
        "scandiqa_da":25.19,
        "norne_nb":68.63,
        "norne_nn":67.7,
        "norec":25.68,
        "scala_nb":6.73,
        "scala_nn":3.35,
        "norquad":22.57,
        "suc3":68.83,
        "swerec":64.25,
        "scala_sv":28.62,
        "scandiqa_sv":28.78
    },
    {
        "rank":31,
        "Model":"sentence-transformers\/distilbert-multilingual-nli-stsb-quora-ranking",
        "num_model_parameters":"135.0",
        "vocabulary_size":120,
        "max_sequence_length":512,
        "speed":26151,
        "score":33.92,
        "da_score":28.84,
        "no_score":31.74,
        "sv_score":41.19,
        "dansk":54.48,
        "angry_tweets":36.6,
        "scala_da":8.84,
        "scandiqa_da":15.42,
        "norne_nb":77.81,
        "norne_nn":72.22,
        "norec":44.59,
        "scala_nb":8.98,
        "scala_nn":5.72,
        "norquad":0.0,
        "suc3":65.5,
        "swerec":68.33,
        "scala_sv":14.81,
        "scandiqa_sv":16.11
    },
    {
        "rank":31,
        "Model":"sentence-transformers\/quora-distilbert-multilingual",
        "num_model_parameters":"135.0",
        "vocabulary_size":120,
        "max_sequence_length":512,
        "speed":26458,
        "score":33.8,
        "da_score":28.47,
        "no_score":31.74,
        "sv_score":41.2,
        "dansk":54.48,
        "angry_tweets":36.6,
        "scala_da":8.84,
        "scandiqa_da":13.97,
        "norne_nb":77.81,
        "norne_nn":72.22,
        "norec":44.59,
        "scala_nb":8.98,
        "scala_nn":5.72,
        "norquad":0.0,
        "suc3":65.5,
        "swerec":68.36,
        "scala_sv":14.81,
        "scandiqa_sv":16.11
    },
    {
        "rank":31,
        "Model":"01-ai\/Yi-6B (few-shot)",
        "num_model_parameters":"6061.0",
        "vocabulary_size":64,
        "max_sequence_length":4096,
        "speed":2786,
        "score":33.57,
        "da_score":24.44,
        "no_score":31.27,
        "sv_score":45.01,
        "dansk":35.08,
        "angry_tweets":4.0,
        "scala_da":3.68,
        "scandiqa_da":55.0,
        "norne_nb":43.44,
        "norne_nn":46.33,
        "norec":38.96,
        "scala_nb":0.75,
        "scala_nn":1.04,
        "norquad":40.33,
        "suc3":46.69,
        "swerec":75.39,
        "scala_sv":2.91,
        "scandiqa_sv":55.05
    },
    {
        "rank":31,
        "Model":"dbmdz\/bert-medium-historic-multilingual-cased",
        "num_model_parameters":"42.0",
        "vocabulary_size":32,
        "max_sequence_length":512,
        "speed":24291,
        "score":33.23,
        "da_score":26.54,
        "no_score":29.05,
        "sv_score":44.1,
        "dansk":49.88,
        "angry_tweets":27.93,
        "scala_da":5.42,
        "scandiqa_da":22.93,
        "norne_nb":69.65,
        "norne_nn":66.78,
        "norec":26.33,
        "scala_nb":6.62,
        "scala_nn":5.16,
        "norquad":15.75,
        "suc3":66.11,
        "swerec":59.66,
        "scala_sv":26.28,
        "scandiqa_sv":24.36
    },
    {
        "rank":31,
        "Model":"meta-llama\/Llama-2-7b-hf (few-shot)",
        "num_model_parameters":"6738.0",
        "vocabulary_size":32,
        "max_sequence_length":4096,
        "speed":2648,
        "score":33.23,
        "da_score":30.35,
        "no_score":25.85,
        "sv_score":43.48,
        "dansk":31.77,
        "angry_tweets":43.91,
        "scala_da":0.31,
        "scandiqa_da":45.42,
        "norne_nb":42.13,
        "norne_nn":43.8,
        "norec":41.74,
        "scala_nb":0.0,
        "scala_nn":0.02,
        "norquad":18.67,
        "suc3":44.11,
        "swerec":79.05,
        "scala_sv":7.34,
        "scandiqa_sv":43.42
    },
    {
        "rank":31,
        "Model":"Maltehb\/aelaectra-danish-electra-small-uncased",
        "num_model_parameters":"14.0",
        "vocabulary_size":32,
        "max_sequence_length":128,
        "speed":5995,
        "score":32.85,
        "da_score":41.16,
        "no_score":28.87,
        "sv_score":28.52,
        "dansk":62.52,
        "angry_tweets":34.45,
        "scala_da":65.15,
        "scandiqa_da":2.51,
        "norne_nb":59.76,
        "norne_nn":51.44,
        "norec":33.41,
        "scala_nb":32.87,
        "scala_nn":20.09,
        "norquad":0.0,
        "suc3":39.17,
        "swerec":57.71,
        "scala_sv":17.1,
        "scandiqa_sv":0.11
    },
    {
        "rank":32,
        "Model":"Rijgersberg\/GEITje-7B (few-shot)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":32768,
        "speed":10401,
        "score":31.97,
        "da_score":27.79,
        "no_score":25.65,
        "sv_score":42.46,
        "dansk":33.41,
        "angry_tweets":26.08,
        "scala_da":-0.22,
        "scandiqa_da":51.9,
        "norne_nb":41.44,
        "norne_nn":45.09,
        "norec":34.51,
        "scala_nb":0.0,
        "scala_nn":0.56,
        "norquad":24.53,
        "suc3":41.08,
        "swerec":76.38,
        "scala_sv":-0.21,
        "scandiqa_sv":52.58
    },
    {
        "rank":33,
        "Model":"jjzha\/dajobbert-base-uncased",
        "num_model_parameters":"110.0",
        "vocabulary_size":32,
        "max_sequence_length":512,
        "speed":16243,
        "score":31.59,
        "da_score":38.38,
        "no_score":27.03,
        "sv_score":29.35,
        "dansk":60.78,
        "angry_tweets":39.65,
        "scala_da":37.67,
        "scandiqa_da":15.41,
        "norne_nb":65.95,
        "norne_nn":55.29,
        "norec":33.31,
        "scala_nb":20.34,
        "scala_nn":8.07,
        "norquad":0.0,
        "suc3":42.99,
        "swerec":55.49,
        "scala_sv":4.69,
        "scandiqa_sv":14.22
    },
    {
        "rank":34,
        "Model":"AI-Sweden-Models\/gpt-sw3-6.7b-v2 (few-shot)",
        "num_model_parameters":"7111.0",
        "vocabulary_size":64,
        "max_sequence_length":2048,
        "speed":2351,
        "score":28.15,
        "da_score":22.16,
        "no_score":24.59,
        "sv_score":37.69,
        "dansk":20.84,
        "angry_tweets":18.07,
        "scala_da":10.54,
        "scandiqa_da":39.18,
        "norne_nb":29.62,
        "norne_nn":32.3,
        "norec":34.67,
        "scala_nb":8.37,
        "scala_nn":7.76,
        "norquad":24.67,
        "suc3":28.73,
        "swerec":77.47,
        "scala_sv":8.78,
        "scandiqa_sv":35.78
    },
    {
        "rank":34,
        "Model":"sentence-transformers\/distiluse-base-multilingual-cased-v1",
        "num_model_parameters":"135.0",
        "vocabulary_size":120,
        "max_sequence_length":512,
        "speed":26344,
        "score":25.98,
        "da_score":23.28,
        "no_score":22.36,
        "sv_score":32.3,
        "dansk":46.78,
        "angry_tweets":27.78,
        "scala_da":3.04,
        "scandiqa_da":15.52,
        "norne_nb":60.76,
        "norne_nn":59.62,
        "norec":25.98,
        "scala_nb":2.65,
        "scala_nn":3.47,
        "norquad":0.2,
        "suc3":49.86,
        "swerec":60.06,
        "scala_sv":3.18,
        "scandiqa_sv":16.08
    },
    {
        "rank":35,
        "Model":"KBLab\/albert-base-swedish-cased-alpha",
        "num_model_parameters":"14.0",
        "vocabulary_size":50,
        "max_sequence_length":512,
        "speed":15925,
        "score":25.8,
        "da_score":17.95,
        "no_score":22.3,
        "sv_score":37.13,
        "dansk":29.9,
        "angry_tweets":19.79,
        "scala_da":6.15,
        "scandiqa_da":15.96,
        "norne_nb":66.97,
        "norne_nn":63.9,
        "norec":18.85,
        "scala_nb":5.83,
        "scala_nn":4.02,
        "norquad":0.0,
        "suc3":47.19,
        "swerec":56.57,
        "scala_sv":20.92,
        "scandiqa_sv":23.86
    },
    {
        "rank":35,
        "Model":"dbmdz\/bert-mini-historic-multilingual-cased",
        "num_model_parameters":"12.0",
        "vocabulary_size":32,
        "max_sequence_length":512,
        "speed":47122,
        "score":25.15,
        "da_score":20.94,
        "no_score":23.1,
        "sv_score":31.43,
        "dansk":41.7,
        "angry_tweets":26.03,
        "scala_da":2.19,
        "scandiqa_da":13.82,
        "norne_nb":61.55,
        "norne_nn":59.9,
        "norec":24.59,
        "scala_nb":3.45,
        "scala_nn":2.72,
        "norquad":3.99,
        "suc3":50.07,
        "swerec":56.1,
        "scala_sv":5.05,
        "scandiqa_sv":14.49
    },
    {
        "rank":36,
        "Model":"AI-Sweden-Models\/gpt-sw3-6.7b-v2-instruct (few-shot)",
        "num_model_parameters":"7111.0",
        "vocabulary_size":64,
        "max_sequence_length":2048,
        "speed":2383,
        "score":24.54,
        "da_score":18.31,
        "no_score":24.1,
        "sv_score":31.21,
        "dansk":15.35,
        "angry_tweets":2.85,
        "scala_da":10.99,
        "scandiqa_da":44.04,
        "norne_nb":24.67,
        "norne_nn":29.03,
        "norec":34.39,
        "scala_nb":2.42,
        "scala_nn":5.11,
        "norquad":31.39,
        "suc3":14.58,
        "swerec":56.6,
        "scala_sv":10.92,
        "scandiqa_sv":42.72
    },
    {
        "rank":36,
        "Model":"norallm\/normistral-7b-scratch (few-shot)",
        "num_model_parameters":"7248.0",
        "vocabulary_size":33,
        "max_sequence_length":2048,
        "speed":3192,
        "score":24.06,
        "da_score":23.0,
        "no_score":18.49,
        "sv_score":30.7,
        "dansk":14.88,
        "angry_tweets":34.66,
        "scala_da":0.29,
        "scandiqa_da":42.16,
        "norne_nb":14.58,
        "norne_nn":21.06,
        "norec":32.02,
        "scala_nb":1.49,
        "scala_nn":0.98,
        "norquad":22.87,
        "suc3":13.79,
        "swerec":71.59,
        "scala_sv":-0.89,
        "scandiqa_sv":38.33
    },
    {
        "rank":36,
        "Model":"AI-Sweden-Models\/gpt-sw3-1.3b (few-shot)",
        "num_model_parameters":"1445.0",
        "vocabulary_size":64,
        "max_sequence_length":2048,
        "speed":4608,
        "score":24.04,
        "da_score":21.4,
        "no_score":19.69,
        "sv_score":31.04,
        "dansk":8.8,
        "angry_tweets":28.65,
        "scala_da":2.84,
        "scandiqa_da":45.31,
        "norne_nb":13.49,
        "norne_nn":14.74,
        "norec":27.28,
        "scala_nb":3.09,
        "scala_nn":1.86,
        "norquad":34.9,
        "suc3":6.08,
        "swerec":71.38,
        "scala_sv":1.17,
        "scandiqa_sv":45.53
    },
    {
        "rank":36,
        "Model":"jannesg\/bertsson",
        "num_model_parameters":"124.0",
        "vocabulary_size":50,
        "max_sequence_length":512,
        "speed":15314,
        "score":23.36,
        "da_score":18.76,
        "no_score":18.1,
        "sv_score":33.23,
        "dansk":32.63,
        "angry_tweets":24.11,
        "scala_da":2.91,
        "scandiqa_da":15.37,
        "norne_nb":49.3,
        "norne_nn":46.11,
        "norec":23.21,
        "scala_nb":2.26,
        "scala_nn":-0.66,
        "norquad":0.68,
        "suc3":51.13,
        "swerec":61.67,
        "scala_sv":2.87,
        "scandiqa_sv":17.24
    },
    {
        "rank":36,
        "Model":"NbAiLab\/nb-gpt-j-6B-alpaca (few-shot)",
        "num_model_parameters":"6055.0",
        "vocabulary_size":50,
        "max_sequence_length":1024,
        "speed":2607,
        "score":22.77,
        "da_score":20.21,
        "no_score":20.04,
        "sv_score":28.05,
        "dansk":12.95,
        "angry_tweets":27.68,
        "scala_da":1.65,
        "scandiqa_da":38.57,
        "norne_nb":23.82,
        "norne_nn":26.04,
        "norec":32.6,
        "scala_nb":0.34,
        "scala_nn":2.26,
        "norquad":21.34,
        "suc3":13.28,
        "swerec":60.17,
        "scala_sv":1.52,
        "scandiqa_sv":37.22
    },
    {
        "rank":36,
        "Model":"AI-Sweden-Models\/gpt-sw3-356m (few-shot)",
        "num_model_parameters":"471.0",
        "vocabulary_size":64,
        "max_sequence_length":2048,
        "speed":5758,
        "score":21.42,
        "da_score":20.13,
        "no_score":20.78,
        "sv_score":23.34,
        "dansk":16.13,
        "angry_tweets":27.61,
        "scala_da":1.96,
        "scandiqa_da":34.81,
        "norne_nb":27.37,
        "norne_nn":31.22,
        "norec":34.21,
        "scala_nb":0.92,
        "scala_nn":1.25,
        "norquad":18.54,
        "suc3":23.77,
        "swerec":34.29,
        "scala_sv":1.57,
        "scandiqa_sv":33.71
    },
    {
        "rank":37,
        "Model":"3ebdola\/Dialectal-Arabic-XLM-R-Base",
        "num_model_parameters":"278.0",
        "vocabulary_size":250,
        "max_sequence_length":512,
        "speed":15177,
        "score":19.22,
        "da_score":15.82,
        "no_score":17.36,
        "sv_score":24.47,
        "dansk":36.51,
        "angry_tweets":22.07,
        "scala_da":1.63,
        "scandiqa_da":3.09,
        "norne_nb":55.55,
        "norne_nn":53.53,
        "norec":12.69,
        "scala_nb":2.79,
        "scala_nn":1.66,
        "norquad":0.0,
        "suc3":42.78,
        "swerec":44.95,
        "scala_sv":1.43,
        "scandiqa_sv":8.71
    },
    {
        "rank":37,
        "Model":"alexanderfalk\/danbert-small-cased",
        "num_model_parameters":"83.0",
        "vocabulary_size":52,
        "max_sequence_length":512,
        "speed":30013,
        "score":18.87,
        "da_score":19.57,
        "no_score":17.28,
        "sv_score":19.75,
        "dansk":33.05,
        "angry_tweets":30.67,
        "scala_da":13.01,
        "scandiqa_da":1.56,
        "norne_nb":42.18,
        "norne_nn":37.39,
        "norec":24.39,
        "scala_nb":7.29,
        "scala_nn":2.57,
        "norquad":0.0,
        "suc3":22.47,
        "swerec":53.88,
        "scala_sv":1.55,
        "scandiqa_sv":1.12
    },
    {
        "rank":37,
        "Model":"mhenrichsen\/danskgpt-tiny-chat (few-shot)",
        "num_model_parameters":"1100.0",
        "vocabulary_size":32,
        "max_sequence_length":2048,
        "speed":1745,
        "score":18.72,
        "da_score":18.96,
        "no_score":15.36,
        "sv_score":21.84,
        "dansk":22.31,
        "angry_tweets":34.05,
        "scala_da":0.7,
        "scandiqa_da":18.78,
        "norne_nb":28.74,
        "norne_nn":30.34,
        "norec":27.49,
        "scala_nb":-2.17,
        "scala_nn":0.26,
        "norquad":5.35,
        "suc3":27.31,
        "swerec":45.94,
        "scala_sv":-0.97,
        "scandiqa_sv":15.08
    },
    {
        "rank":37,
        "Model":"RuterNorway\/Llama-2-7b-chat-norwegian (few-shot)",
        "num_model_parameters":"unknown",
        "vocabulary_size":32,
        "max_sequence_length":4096,
        "speed":10890,
        "score":16.9,
        "da_score":11.58,
        "no_score":14.62,
        "sv_score":24.49,
        "dansk":10.12,
        "angry_tweets":10.65,
        "scala_da":-0.66,
        "scandiqa_da":26.21,
        "norne_nb":21.04,
        "norne_nn":18.71,
        "norec":12.22,
        "scala_nb":-1.18,
        "scala_nn":0.36,
        "norquad":26.79,
        "suc3":22.38,
        "swerec":31.11,
        "scala_sv":0.09,
        "scandiqa_sv":44.37
    },
    {
        "rank":37,
        "Model":"mhenrichsen\/danskgpt-tiny (few-shot)",
        "num_model_parameters":"1100.0",
        "vocabulary_size":32,
        "max_sequence_length":2048,
        "speed":8597,
        "score":14.93,
        "da_score":13.52,
        "no_score":11.81,
        "sv_score":19.47,
        "dansk":14.13,
        "angry_tweets":26.31,
        "scala_da":-0.54,
        "scandiqa_da":14.16,
        "norne_nb":27.37,
        "norne_nn":27.59,
        "norec":18.09,
        "scala_nb":-0.19,
        "scala_nn":-0.8,
        "norquad":2.15,
        "suc3":23.92,
        "swerec":31.93,
        "scala_sv":0.46,
        "scandiqa_sv":21.56
    },
    {
        "rank":37,
        "Model":"RabotaRu\/HRBert-mini",
        "num_model_parameters":"80.0",
        "vocabulary_size":200,
        "max_sequence_length":512,
        "speed":54951,
        "score":14.62,
        "da_score":11.54,
        "no_score":12.03,
        "sv_score":20.27,
        "dansk":22.21,
        "angry_tweets":20.33,
        "scala_da":0.9,
        "scandiqa_da":2.73,
        "norne_nb":31.87,
        "norne_nn":32.47,
        "norec":15.07,
        "scala_nb":1.26,
        "scala_nn":0.49,
        "norquad":0.0,
        "suc3":24.61,
        "swerec":52.31,
        "scala_sv":1.32,
        "scandiqa_sv":2.86
    },
    {
        "rank":37,
        "Model":"fresh-xlm-roberta-base",
        "num_model_parameters":"278.0",
        "vocabulary_size":250,
        "max_sequence_length":512,
        "speed":1319,
        "score":11.81,
        "da_score":9.08,
        "no_score":9.87,
        "sv_score":16.47,
        "dansk":16.04,
        "angry_tweets":17.37,
        "scala_da":1.34,
        "scandiqa_da":1.58,
        "norne_nb":25.49,
        "norne_nn":25.94,
        "norec":12.6,
        "scala_nb":0.5,
        "scala_nn":1.83,
        "norquad":0.0,
        "suc3":11.91,
        "swerec":51.11,
        "scala_sv":0.86,
        "scandiqa_sv":2.0
    },
    {
        "rank":37,
        "Model":"fresh-electra-small",
        "num_model_parameters":"14.0",
        "vocabulary_size":31,
        "max_sequence_length":512,
        "speed":7219,
        "score":10.74,
        "da_score":7.94,
        "no_score":7.78,
        "sv_score":16.49,
        "dansk":12.87,
        "angry_tweets":18.61,
        "scala_da":0.3,
        "scandiqa_da":0.0,
        "norne_nb":18.38,
        "norne_nn":12.76,
        "norec":15.29,
        "scala_nb":0.17,
        "scala_nn":0.37,
        "norquad":0.0,
        "suc3":10.54,
        "swerec":55.54,
        "scala_sv":-0.15,
        "scandiqa_sv":0.02
    },
    {
        "rank":38,
        "Model":"NbAiLab\/nb-gpt-j-6B-v2 (few-shot)",
        "num_model_parameters":"6051.0",
        "vocabulary_size":50,
        "max_sequence_length":1024,
        "speed":2556,
        "score":9.23,
        "da_score":8.86,
        "no_score":7.45,
        "sv_score":11.4,
        "dansk":0.24,
        "angry_tweets":27.8,
        "scala_da":0.56,
        "scandiqa_da":6.82,
        "norne_nb":5.29,
        "norne_nn":6.77,
        "norec":20.84,
        "scala_nb":0.45,
        "scala_nn":0.48,
        "norquad":2.45,
        "suc3":0.31,
        "swerec":27.42,
        "scala_sv":0.07,
        "scandiqa_sv":17.79
    },
    {
        "rank":38,
        "Model":"NbAiLab\/nb-gpt-j-6B@sharded (few-shot)",
        "num_model_parameters":"unknown",
        "vocabulary_size":50,
        "max_sequence_length":1024,
        "speed":2630,
        "score":6.3,
        "da_score":4.1,
        "no_score":5.21,
        "sv_score":9.58,
        "dansk":0.36,
        "angry_tweets":11.0,
        "scala_da":-0.11,
        "scandiqa_da":5.16,
        "norne_nb":0.22,
        "norne_nn":0.24,
        "norec":20.64,
        "scala_nb":-0.99,
        "scala_nn":-0.15,
        "norquad":0.55,
        "suc3":0.01,
        "swerec":33.5,
        "scala_sv":-0.02,
        "scandiqa_sv":4.82
    },
    {
        "rank":38,
        "Model":"AI-Sweden-Models\/gpt-sw3-126m (few-shot)",
        "num_model_parameters":"186.0",
        "vocabulary_size":64,
        "max_sequence_length":2048,
        "speed":8958,
        "score":4.89,
        "da_score":5.02,
        "no_score":4.48,
        "sv_score":5.16,
        "dansk":3.43,
        "angry_tweets":9.18,
        "scala_da":-0.22,
        "scandiqa_da":7.7,
        "norne_nb":13.55,
        "norne_nn":9.38,
        "norec":7.78,
        "scala_nb":-1.46,
        "scala_nn":-2.97,
        "norquad":0.9,
        "suc3":5.66,
        "swerec":8.15,
        "scala_sv":-0.81,
        "scandiqa_sv":7.64
    },
    {
        "rank":39,
        "Model":"RJuro\/kanelsnegl-v0.1 (few-shot)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":512,
        "speed":9757,
        "score":4.05,
        "da_score":3.25,
        "no_score":0.24,
        "sv_score":8.66,
        "dansk":0.0,
        "angry_tweets":13.0,
        "scala_da":0.0,
        "scandiqa_da":0.0,
        "norne_nb":0.0,
        "norne_nn":0.0,
        "norec":0.95,
        "scala_nb":0.0,
        "scala_nn":0.0,
        "norquad":0.0,
        "suc3":0.0,
        "swerec":34.63,
        "scala_sv":0.0,
        "scandiqa_sv":0.0
    },
    {
        "rank":39,
        "Model":"RJuro\/kanelsnegl-v0.2 (few-shot)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":512,
        "speed":1373,
        "score":2.89,
        "da_score":1.2,
        "no_score":0.32,
        "sv_score":7.16,
        "dansk":0.0,
        "angry_tweets":4.81,
        "scala_da":0.0,
        "scandiqa_da":0.0,
        "norne_nb":0.0,
        "norne_nn":0.0,
        "norec":1.27,
        "scala_nb":0.0,
        "scala_nn":0.0,
        "norquad":0.0,
        "suc3":0.0,
        "swerec":28.62,
        "scala_sv":0.0,
        "scandiqa_sv":0.0
    },
    {
        "rank":39,
        "Model":"RJuro\/kanelsnegl-v0.2 (few-shot, val)",
        "num_model_parameters":"7242.0",
        "vocabulary_size":32,
        "max_sequence_length":512,
        "speed":2982,
        "score":2.75,
        "da_score":1.15,
        "no_score":0.06,
        "sv_score":7.03,
        "dansk":0.0,
        "angry_tweets":4.6,
        "scala_da":0.0,
        "scandiqa_da":0.0,
        "norne_nb":0.0,
        "norne_nn":0.0,
        "norec":0.25,
        "scala_nb":0.0,
        "scala_nn":0.0,
        "norquad":0.0,
        "suc3":0.0,
        "swerec":28.13,
        "scala_sv":0.0,
        "scandiqa_sv":0.0
    },
    {
        "rank":39,
        "Model":"ai-forever\/mGPT (few-shot)",
        "num_model_parameters":"unknown",
        "vocabulary_size":100,
        "max_sequence_length":1024,
        "speed":13551,
        "score":1.33,
        "da_score":1.13,
        "no_score":1.17,
        "sv_score":1.68,
        "dansk":0.65,
        "angry_tweets":2.61,
        "scala_da":-0.73,
        "scandiqa_da":2.0,
        "norne_nb":0.08,
        "norne_nn":0.0,
        "norec":4.76,
        "scala_nb":0.67,
        "scala_nn":-0.88,
        "norquad":0.0,
        "suc3":0.0,
        "swerec":0.0,
        "scala_sv":0.49,
        "scandiqa_sv":6.24
    },
    {
        "rank":39,
        "Model":"peter-sk\/gpt-neox-da (few-shot)",
        "num_model_parameters":"1515.0",
        "vocabulary_size":50,
        "max_sequence_length":1024,
        "speed":6025,
        "score":0.35,
        "da_score":0.14,
        "no_score":-0.2,
        "sv_score":1.12,
        "dansk":0.64,
        "angry_tweets":-0.52,
        "scala_da":-0.02,
        "scandiqa_da":0.48,
        "norne_nb":0.29,
        "norne_nn":0.25,
        "norec":-1.43,
        "scala_nb":-0.42,
        "scala_nn":1.11,
        "norquad":0.0,
        "suc3":0.26,
        "swerec":4.75,
        "scala_sv":-0.6,
        "scandiqa_sv":0.06
    }
]