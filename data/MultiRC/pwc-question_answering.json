[
    {
        "table_id":44,
        "row_id":51238,
        "rank":1,
        "method":"PaLM 540B (finetuned) ",
        "mlmodel":{

        },
        "method_short":"PaLM 540B ",
        "method_details":"finetuned",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-05",
        "metrics":{
            "F1":"90.1",
            "EM":"69.2"
        },
        "raw_metrics":{
            "F1":90.1,
            "EM":69.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":989558,
            "title":"PaLM: Scaling Language Modeling with Pathways",
            "url":"\/paper\/palm-scaling-language-modeling-with-pathways-1",
            "published":"2022-04-05T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":184,
                "name":"fine-tuned",
                "color":"#e56666"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":44,
        "row_id":24571,
        "rank":2,
        "method":"DeBERTa-1.5B",
        "mlmodel":{

        },
        "method_short":"DeBERTa-1.5B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-05",
        "metrics":{
            "F1":"88.2",
            "EM":"63.7"
        },
        "raw_metrics":{
            "F1":88.2,
            "EM":63.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":201217,
            "title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention",
            "url":"\/paper\/deberta-decoding-enhanced-bert-with",
            "published":"2020-06-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deberta-decoding-enhanced-bert-with\/review\/?hl=24571"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":44,
        "row_id":102574,
        "rank":3,
        "method":"PaLM 2-L (one-shot)",
        "mlmodel":{

        },
        "method_short":"PaLM 2-L ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-17",
        "metrics":{
            "F1":"88.2",
            "EM":null
        },
        "raw_metrics":{
            "F1":88.2,
            "EM":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1210556,
            "title":"PaLM 2 Technical Report",
            "url":"\/paper\/palm-2-technical-report-1",
            "published":"2023-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/palm-2-technical-report-1\/review\/?hl=102574"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":44,
        "row_id":8379,
        "rank":4,
        "method":"T5-11B",
        "mlmodel":{

        },
        "method_short":"T5-11B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-10-23",
        "metrics":{
            "F1":"88.1",
            "EM":"63.3"
        },
        "raw_metrics":{
            "F1":88.1,
            "EM":63.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":166345,
            "title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "url":"\/paper\/exploring-the-limits-of-transfer-learning",
            "published":"2019-10-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/exploring-the-limits-of-transfer-learning\/review\/?hl=8379"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":44,
        "row_id":102573,
        "rank":5,
        "method":"PaLM 2-M (one-shot)",
        "mlmodel":{

        },
        "method_short":"PaLM 2-M ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-17",
        "metrics":{
            "F1":"84.1",
            "EM":null
        },
        "raw_metrics":{
            "F1":84.1,
            "EM":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1210556,
            "title":"PaLM 2 Technical Report",
            "url":"\/paper\/palm-2-technical-report-1",
            "published":"2023-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/palm-2-technical-report-1\/review\/?hl=102573"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":44,
        "row_id":102572,
        "rank":6,
        "method":"PaLM 2-S (one-shot)",
        "mlmodel":{

        },
        "method_short":"PaLM 2-S ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-17",
        "metrics":{
            "F1":"84.0",
            "EM":null
        },
        "raw_metrics":{
            "F1":84.0,
            "EM":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1210556,
            "title":"PaLM 2 Technical Report",
            "url":"\/paper\/palm-2-technical-report-1",
            "published":"2023-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/palm-2-technical-report-1\/review\/?hl=102572"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":44,
        "row_id":39030,
        "rank":7,
        "method":"FLAN 137B (zero-shot)",
        "mlmodel":{

        },
        "method_short":"FLAN 137B ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-03",
        "metrics":{
            "F1":"77.5",
            "EM":null
        },
        "raw_metrics":{
            "F1":77.5,
            "EM":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":861409,
            "title":"Finetuned Language Models Are Zero-Shot Learners",
            "url":"\/paper\/finetuned-language-models-are-zero-shot",
            "published":"2021-09-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/finetuned-language-models-are-zero-shot\/review\/?hl=39030"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":188,
                "name":"zero-shot",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":44,
        "row_id":16820,
        "rank":8,
        "method":"GPT-3 175B (Few-Shot)",
        "mlmodel":{

        },
        "method_short":"GPT-3 175B ",
        "method_details":"Few-Shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-05-28",
        "metrics":{
            "F1":"75.4",
            "EM":null
        },
        "raw_metrics":{
            "F1":75.4,
            "EM":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":198147,
            "title":"Language Models are Few-Shot Learners",
            "url":"\/paper\/language-models-are-few-shot-learners",
            "published":"2020-05-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/language-models-are-few-shot-learners\/review\/?hl=16820"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":44,
        "row_id":39231,
        "rank":9,
        "method":"KELM (finetuning BERT-large based single model)",
        "mlmodel":{

        },
        "method_short":"KELM ",
        "method_details":"finetuning BERT-large based single model",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-09",
        "metrics":{
            "F1":"70.8",
            "EM":"27.2"
        },
        "raw_metrics":{
            "F1":70.8,
            "EM":27.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":864209,
            "title":"KELM: Knowledge Enhanced Pre-Trained Language Representations with Message Passing on Hierarchical Relational Graphs",
            "url":"\/paper\/kelm-knowledge-enhanced-pre-trained-language",
            "published":"2021-09-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/kelm-knowledge-enhanced-pre-trained-language\/review\/?hl=39231"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":184,
                "name":"fine-tuned",
                "color":"#e56666"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":44,
        "row_id":39232,
        "rank":10,
        "method":"BERT-large(single model)",
        "mlmodel":{

        },
        "method_short":"BERT-large",
        "method_details":"single model",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-10-11",
        "metrics":{
            "F1":"70.0",
            "EM":"24.1"
        },
        "raw_metrics":{
            "F1":70.0,
            "EM":24.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":59204,
            "title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "url":"\/paper\/bert-pre-training-of-deep-bidirectional",
            "published":"2018-10-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/bert-pre-training-of-deep-bidirectional\/review\/?hl=39232"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":44,
        "row_id":71004,
        "rank":11,
        "method":"Neo-6B (QA + WS)",
        "mlmodel":{

        },
        "method_short":"Neo-6B ",
        "method_details":"QA + WS",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-05",
        "metrics":{
            "F1":" 63.8",
            "EM":null
        },
        "raw_metrics":{
            "F1":63.8,
            "EM":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1087115,
            "title":"Ask Me Anything: A simple strategy for prompting language models",
            "url":"\/paper\/ask-me-anything-a-simple-strategy-for",
            "published":"2022-10-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ask-me-anything-a-simple-strategy-for\/review\/?hl=71004"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":44,
        "row_id":100796,
        "rank":12,
        "method":"Bloomberg GPT (one-shot)",
        "mlmodel":{

        },
        "method_short":"Bloomberg GPT ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "F1":"62.29",
            "EM":null
        },
        "raw_metrics":{
            "F1":62.29,
            "EM":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100796"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":44,
        "row_id":60038,
        "rank":13,
        "method":"N-Grammer",
        "mlmodel":{

        },
        "method_short":"N-Grammer",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-13",
        "metrics":{
            "F1":"61.95",
            "EM":"11.33"
        },
        "raw_metrics":{
            "F1":61.95,
            "EM":11.33
        },
        "uses_additional_data":false,
        "paper":{
            "id":1043167,
            "title":"N-Grammer: Augmenting Transformers with latent n-grams",
            "url":"\/paper\/n-grammer-augmenting-transformers-with-latent-1",
            "published":"2022-07-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/n-grammer-augmenting-transformers-with-latent-1\/review\/?hl=60038"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":44,
        "row_id":71002,
        "rank":14,
        "method":"Neo-6B (few-shot)",
        "mlmodel":{

        },
        "method_short":"Neo-6B ",
        "method_details":"few-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-05",
        "metrics":{
            "F1":"60.8",
            "EM":null
        },
        "raw_metrics":{
            "F1":60.8,
            "EM":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1087115,
            "title":"Ask Me Anything: A simple strategy for prompting language models",
            "url":"\/paper\/ask-me-anything-a-simple-strategy-for",
            "published":"2022-10-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ask-me-anything-a-simple-strategy-for\/review\/?hl=71002"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":183,
                "name":"few-shot",
                "color":"#a1df95"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":44,
        "row_id":96240,
        "rank":15,
        "method":"AlexaTM 20B",
        "mlmodel":{

        },
        "method_short":"AlexaTM 20B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-08-02",
        "metrics":{
            "F1":"59.57",
            "EM":null
        },
        "raw_metrics":{
            "F1":59.57,
            "EM":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1053502,
            "title":"AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model",
            "url":"\/paper\/alexatm-20b-few-shot-learning-using-a-large",
            "published":"2022-08-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/alexatm-20b-few-shot-learning-using-a-large\/review\/?hl=96240"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":44,
        "row_id":71003,
        "rank":16,
        "method":"Neo-6B (QA)",
        "mlmodel":{

        },
        "method_short":"Neo-6B ",
        "method_details":"QA",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-05",
        "metrics":{
            "F1":"58.8",
            "EM":null
        },
        "raw_metrics":{
            "F1":58.8,
            "EM":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1087115,
            "title":"Ask Me Anything: A simple strategy for prompting language models",
            "url":"\/paper\/ask-me-anything-a-simple-strategy-for",
            "published":"2022-10-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ask-me-anything-a-simple-strategy-for\/review\/?hl=71003"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":44,
        "row_id":100799,
        "rank":17,
        "method":"BLOOM 176B (one-shot)",
        "mlmodel":{

        },
        "method_short":"BLOOM 176B ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "F1":"26.65",
            "EM":null
        },
        "raw_metrics":{
            "F1":26.65,
            "EM":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100799"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":44,
        "row_id":100797,
        "rank":18,
        "method":"GPT-NeoX (one-shot)",
        "mlmodel":{

        },
        "method_short":"GPT-NeoX ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "F1":"22.86",
            "EM":null
        },
        "raw_metrics":{
            "F1":22.86,
            "EM":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100797"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":44,
        "row_id":100798,
        "rank":19,
        "method":"OPT 66B (one-shot)",
        "mlmodel":{

        },
        "method_short":"OPT 66B ",
        "method_details":"one-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-30",
        "metrics":{
            "F1":"18.80",
            "EM":null
        },
        "raw_metrics":{
            "F1":18.8,
            "EM":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1183339,
            "title":"BloombergGPT: A Large Language Model for Finance",
            "url":"\/paper\/bloomberggpt-a-large-language-model-for",
            "published":"2023-03-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/bloomberggpt-a-large-language-model-for\/review\/?hl=100798"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]