[
    {
        "table_id":2553,
        "row_id":109450,
        "rank":1,
        "method":"GPT4RoI",
        "mlmodel":{

        },
        "method_short":"GPT4RoI",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-07-07",
        "metrics":{
            "Accuracy":"89.4"
        },
        "raw_metrics":{
            "Accuracy":89.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":1242643,
            "title":"GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest",
            "url":"\/paper\/gpt4roi-instruction-tuning-large-language",
            "published":"2023-07-07T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2553,
        "row_id":24386,
        "rank":2,
        "method":"ERNIE-ViL-large(ensemble of 15 models)",
        "mlmodel":{

        },
        "method_short":"ERNIE-ViL-large",
        "method_details":"ensemble of 15 models",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-30",
        "metrics":{
            "Accuracy":"81.6"
        },
        "raw_metrics":{
            "Accuracy":81.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":206405,
            "title":"ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph",
            "url":"\/paper\/ernie-vil-knowledge-enhanced-vision-language",
            "published":"2020-06-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/ernie-vil-knowledge-enhanced-vision-language\/review\/?hl=24386"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2553,
        "row_id":24385,
        "rank":3,
        "method":"UNITER-large (10 ensemble)",
        "mlmodel":{

        },
        "method_short":"UNITER-large ",
        "method_details":"10 ensemble",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-09-25",
        "metrics":{
            "Accuracy":"79.8"
        },
        "raw_metrics":{
            "Accuracy":79.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":156206,
            "title":"UNITER: UNiversal Image-TExt Representation Learning",
            "url":"\/paper\/uniter-learning-universal-image-text-1",
            "published":"2019-09-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/uniter-learning-universal-image-text-1\/review\/?hl=24385"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2553,
        "row_id":69533,
        "rank":4,
        "method":"MAD (Single Model, Formerly CLIP-TD)",
        "mlmodel":{

        },
        "method_short":"MAD ",
        "method_details":"Single Model, Formerly CLIP-TD",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-22",
        "metrics":{
            "Accuracy":"79.6"
        },
        "raw_metrics":{
            "Accuracy":79.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":998392,
            "title":"Multimodal Adaptive Distillation for Leveraging Unimodal Encoders for Vision-Language Tasks",
            "url":"\/paper\/multimodal-adaptive-distillation-for",
            "published":"2022-04-22T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/multimodal-adaptive-distillation-for\/review\/?hl=69533"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2553,
        "row_id":14798,
        "rank":5,
        "method":"UNITER (Large)",
        "mlmodel":{

        },
        "method_short":"UNITER ",
        "method_details":"Large",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-09-25",
        "metrics":{
            "Accuracy":"77.3"
        },
        "raw_metrics":{
            "Accuracy":77.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":156206,
            "title":"UNITER: UNiversal Image-TExt Representation Learning",
            "url":"\/paper\/uniter-learning-universal-image-text-1",
            "published":"2019-09-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/uniter-learning-universal-image-text-1\/review\/?hl=14798"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2553,
        "row_id":40437,
        "rank":6,
        "method":"KVL-BERTLARGE",
        "mlmodel":{

        },
        "method_short":"KVL-BERTLARGE",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-13",
        "metrics":{
            "Accuracy":"76.4"
        },
        "raw_metrics":{
            "Accuracy":76.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":727505,
            "title":"KVL-BERT: Knowledge Enhanced Visual-and-Linguistic BERT for Visual Commonsense Reasoning",
            "url":"\/paper\/kvl-bert-knowledge-enhanced-visual-and",
            "published":"2020-12-13T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/kvl-bert-knowledge-enhanced-visual-and\/review\/?hl=40437"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2553,
        "row_id":14799,
        "rank":7,
        "method":"VL-BERTLARGE",
        "mlmodel":{

        },
        "method_short":"VL-BERTLARGE",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-22",
        "metrics":{
            "Accuracy":"75.8"
        },
        "raw_metrics":{
            "Accuracy":75.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":150926,
            "title":"VL-BERT: Pre-training of Generic Visual-Linguistic Representations",
            "url":"\/paper\/vl-bert-pre-training-of-generic-visual",
            "published":"2019-08-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vl-bert-pre-training-of-generic-visual\/review\/?hl=14799"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2553,
        "row_id":50619,
        "rank":8,
        "method":"VL-T5",
        "mlmodel":{

        },
        "method_short":"VL-T5",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-04",
        "metrics":{
            "Accuracy":"75.3"
        },
        "raw_metrics":{
            "Accuracy":75.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":742287,
            "title":"Unifying Vision-and-Language Tasks via Text Generation",
            "url":"\/paper\/unifying-vision-and-language-tasks-via-text",
            "published":"2021-02-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/unifying-vision-and-language-tasks-via-text\/review\/?hl=50619"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2553,
        "row_id":14800,
        "rank":9,
        "method":"VisualBERT",
        "mlmodel":{

        },
        "method_short":"VisualBERT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-09",
        "metrics":{
            "Accuracy":"71.6"
        },
        "raw_metrics":{
            "Accuracy":71.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":149599,
            "title":"VisualBERT: A Simple and Performant Baseline for Vision and Language",
            "url":"\/paper\/visualbert-a-simple-and-performant-baseline",
            "published":"2019-08-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/visualbert-a-simple-and-performant-baseline\/review\/?hl=14800"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2553,
        "row_id":88759,
        "rank":10,
        "method":"OFA-X",
        "mlmodel":{

        },
        "method_short":"OFA-X",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-08",
        "metrics":{
            "Accuracy":"71.2"
        },
        "raw_metrics":{
            "Accuracy":71.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":1125614,
            "title":"Harnessing the Power of Multi-Task Pretraining for Ground-Truth Level Natural Language Explanations",
            "url":"\/paper\/harnessing-the-power-of-multi-task",
            "published":"2022-12-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/harnessing-the-power-of-multi-task\/review\/?hl=88759"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":2553,
        "row_id":88760,
        "rank":11,
        "method":"OFA-X-MT",
        "mlmodel":{

        },
        "method_short":"OFA-X-MT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-08",
        "metrics":{
            "Accuracy":"62"
        },
        "raw_metrics":{
            "Accuracy":62.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1125614,
            "title":"Harnessing the Power of Multi-Task Pretraining for Ground-Truth Level Natural Language Explanations",
            "url":"\/paper\/harnessing-the-power-of-multi-task",
            "published":"2022-12-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/harnessing-the-power-of-multi-task\/review\/?hl=88760"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]