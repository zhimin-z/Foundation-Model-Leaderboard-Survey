[
    {
        "table_id":24593,
        "row_id":112991,
        "rank":1,
        "method":"VTimeLLM",
        "mlmodel":{

        },
        "Model":"VTimeLLM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-30",
        "metrics":{
            "gpt-score":"3.10"
        },
        "raw_metrics":{
            "gpt-score":3.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":1333323,
            "title":"VTimeLLM: Empower LLM to Grasp Video Moments",
            "url":"\/paper\/vtimellm-empower-llm-to-grasp-video-moments",
            "published":"2023-11-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vtimellm-empower-llm-to-grasp-video-moments\/review\/?hl=112991"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24593,
        "row_id":112653,
        "rank":2,
        "method":"MovieChat",
        "mlmodel":{

        },
        "Model":"MovieChat",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-07-31",
        "metrics":{
            "gpt-score":"2.93"
        },
        "raw_metrics":{
            "gpt-score":2.93
        },
        "uses_additional_data":false,
        "paper":{
            "id":1255388,
            "title":"MovieChat: From Dense Token to Sparse Memory for Long Video Understanding",
            "url":"\/paper\/moviechat-from-dense-token-to-sparse-memory",
            "published":"2023-07-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/moviechat-from-dense-token-to-sparse-memory\/review\/?hl=112653"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24593,
        "row_id":112504,
        "rank":3,
        "method":"Chat-UniVi",
        "mlmodel":{

        },
        "Model":"Chat-UniVi",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-14",
        "metrics":{
            "gpt-score":"2.91"
        },
        "raw_metrics":{
            "gpt-score":2.91
        },
        "uses_additional_data":false,
        "paper":{
            "id":1320706,
            "title":"Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding",
            "url":"\/paper\/chat-univi-unified-visual-representation",
            "published":"2023-11-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/chat-univi-unified-visual-representation\/review\/?hl=112504"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24593,
        "row_id":112848,
        "rank":4,
        "method":"VideoChat2",
        "mlmodel":{

        },
        "Model":"VideoChat2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-28",
        "metrics":{
            "gpt-score":"2.88"
        },
        "raw_metrics":{
            "gpt-score":2.88
        },
        "uses_additional_data":false,
        "paper":{
            "id":1329478,
            "title":"MVBench: A Comprehensive Multi-modal Video Understanding Benchmark",
            "url":"\/paper\/mvbench-a-comprehensive-multi-modal-video",
            "published":"2023-11-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mvbench-a-comprehensive-multi-modal-video\/review\/?hl=112848"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24593,
        "row_id":109675,
        "rank":5,
        "method":"BT-Adapter",
        "mlmodel":{

        },
        "Model":"BT-Adapter",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-09-27",
        "metrics":{
            "gpt-score":"2.69"
        },
        "raw_metrics":{
            "gpt-score":2.69
        },
        "uses_additional_data":false,
        "paper":{
            "id":1289477,
            "title":"One For All: Video Conversation is Feasible Without Video Instruction Tuning",
            "url":"\/paper\/one-for-all-video-conversation-is-feasible",
            "published":"2023-09-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/one-for-all-video-conversation-is-feasible\/review\/?hl=109675"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24593,
        "row_id":106231,
        "rank":6,
        "method":"Video-ChatGPT",
        "mlmodel":{

        },
        "Model":"Video-ChatGPT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-06-08",
        "metrics":{
            "gpt-score":"2.52"
        },
        "raw_metrics":{
            "gpt-score":2.52
        },
        "uses_additional_data":false,
        "paper":{
            "id":1225920,
            "title":"Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models",
            "url":"\/paper\/video-chatgpt-towards-detailed-video",
            "published":"2023-06-08T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24593,
        "row_id":106232,
        "rank":7,
        "method":"Video Chat",
        "mlmodel":{

        },
        "Model":"Video Chat",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-10",
        "metrics":{
            "gpt-score":"2.50"
        },
        "raw_metrics":{
            "gpt-score":2.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":1205778,
            "title":"VideoChat: Chat-Centric Video Understanding",
            "url":"\/paper\/videochat-chat-centric-video-understanding",
            "published":"2023-05-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/videochat-chat-centric-video-understanding\/review\/?hl=106232"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24593,
        "row_id":109676,
        "rank":8,
        "method":"BT-Adapter (zero-shot)",
        "mlmodel":{

        },
        "Model":"BT-Adapter ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-09-27",
        "metrics":{
            "gpt-score":"2.46"
        },
        "raw_metrics":{
            "gpt-score":2.46
        },
        "uses_additional_data":false,
        "paper":{
            "id":1289477,
            "title":"One For All: Video Conversation is Feasible Without Video Instruction Tuning",
            "url":"\/paper\/one-for-all-video-conversation-is-feasible",
            "published":"2023-09-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/one-for-all-video-conversation-is-feasible\/review\/?hl=109676"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24593,
        "row_id":106233,
        "rank":9,
        "method":"LLaMA Adapter",
        "mlmodel":{

        },
        "Model":"LLaMA Adapter",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-28",
        "metrics":{
            "gpt-score":"2.32"
        },
        "raw_metrics":{
            "gpt-score":2.32
        },
        "uses_additional_data":false,
        "paper":{
            "id":1199360,
            "title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model",
            "url":"\/paper\/llama-adapter-v2-parameter-efficient-visual",
            "published":"2023-04-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-adapter-v2-parameter-efficient-visual\/review\/?hl=106233"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24593,
        "row_id":106234,
        "rank":10,
        "method":"Video LLaMA",
        "mlmodel":{

        },
        "Model":"Video LLaMA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-06-05",
        "metrics":{
            "gpt-score":"2.18"
        },
        "raw_metrics":{
            "gpt-score":2.18
        },
        "uses_additional_data":false,
        "paper":{
            "id":1223066,
            "title":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding",
            "url":"\/paper\/video-llama-an-instruction-tuned-audio-visual",
            "published":"2023-06-05T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]