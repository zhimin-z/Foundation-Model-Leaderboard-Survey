[
    {
        "table_id":24594,
        "row_id":112506,
        "rank":1,
        "method":"Chat-UniVi",
        "mlmodel":{

        },
        "Model":"Chat-UniVi",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-14",
        "metrics":{
            "gpt-score":"2.89"
        },
        "raw_metrics":{
            "gpt-score":2.89
        },
        "uses_additional_data":false,
        "paper":{
            "id":1320706,
            "title":"Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding",
            "url":"\/paper\/chat-univi-unified-visual-representation",
            "published":"2023-11-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/chat-univi-unified-visual-representation\/review\/?hl=112506"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24594,
        "row_id":112850,
        "rank":2,
        "method":"VideoChat2",
        "mlmodel":{

        },
        "Model":"VideoChat2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-28",
        "metrics":{
            "gpt-score":"2.66"
        },
        "raw_metrics":{
            "gpt-score":2.66
        },
        "uses_additional_data":false,
        "paper":{
            "id":1329478,
            "title":"MVBench: A Comprehensive Multi-modal Video Understanding Benchmark",
            "url":"\/paper\/mvbench-a-comprehensive-multi-modal-video",
            "published":"2023-11-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mvbench-a-comprehensive-multi-modal-video\/review\/?hl=112850"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24594,
        "row_id":112989,
        "rank":3,
        "method":"VTimeLLM",
        "mlmodel":{

        },
        "Model":"VTimeLLM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-30",
        "metrics":{
            "gpt-score":"2.49"
        },
        "raw_metrics":{
            "gpt-score":2.49
        },
        "uses_additional_data":false,
        "paper":{
            "id":1333323,
            "title":"VTimeLLM: Empower LLM to Grasp Video Moments",
            "url":"\/paper\/vtimellm-empower-llm-to-grasp-video-moments",
            "published":"2023-11-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vtimellm-empower-llm-to-grasp-video-moments\/review\/?hl=112989"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24594,
        "row_id":109679,
        "rank":4,
        "method":"BT-Adapter",
        "mlmodel":{

        },
        "Model":"BT-Adapter",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-09-27",
        "metrics":{
            "gpt-score":"2.34"
        },
        "raw_metrics":{
            "gpt-score":2.34
        },
        "uses_additional_data":false,
        "paper":{
            "id":1289477,
            "title":"One For All: Video Conversation is Feasible Without Video Instruction Tuning",
            "url":"\/paper\/one-for-all-video-conversation-is-feasible",
            "published":"2023-09-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/one-for-all-video-conversation-is-feasible\/review\/?hl=109679"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24594,
        "row_id":112655,
        "rank":5,
        "method":"MovieChat",
        "mlmodel":{

        },
        "Model":"MovieChat",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-07-31",
        "metrics":{
            "gpt-score":"2.24"
        },
        "raw_metrics":{
            "gpt-score":2.24
        },
        "uses_additional_data":false,
        "paper":{
            "id":1255388,
            "title":"MovieChat: From Dense Token to Sparse Memory for Long Video Understanding",
            "url":"\/paper\/moviechat-from-dense-token-to-sparse-memory",
            "published":"2023-07-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/moviechat-from-dense-token-to-sparse-memory\/review\/?hl=112655"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24594,
        "row_id":109680,
        "rank":6,
        "method":"BT-Adapter (zero-shot)",
        "mlmodel":{

        },
        "Model":"BT-Adapter ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-09-27",
        "metrics":{
            "gpt-score":"2.13"
        },
        "raw_metrics":{
            "gpt-score":2.13
        },
        "uses_additional_data":false,
        "paper":{
            "id":1289477,
            "title":"One For All: Video Conversation is Feasible Without Video Instruction Tuning",
            "url":"\/paper\/one-for-all-video-conversation-is-feasible",
            "published":"2023-09-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/one-for-all-video-conversation-is-feasible\/review\/?hl=109680"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24594,
        "row_id":106235,
        "rank":7,
        "method":"Video-ChatGPT",
        "mlmodel":{

        },
        "Model":"Video-ChatGPT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-06-08",
        "metrics":{
            "gpt-score":"1.98"
        },
        "raw_metrics":{
            "gpt-score":1.98
        },
        "uses_additional_data":false,
        "paper":{
            "id":1225920,
            "title":"Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models",
            "url":"\/paper\/video-chatgpt-towards-detailed-video",
            "published":"2023-06-08T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24594,
        "row_id":106237,
        "rank":8,
        "method":"LLaMA Adapter",
        "mlmodel":{

        },
        "Model":"LLaMA Adapter",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-28",
        "metrics":{
            "gpt-score":"1.98"
        },
        "raw_metrics":{
            "gpt-score":1.98
        },
        "uses_additional_data":false,
        "paper":{
            "id":1199360,
            "title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model",
            "url":"\/paper\/llama-adapter-v2-parameter-efficient-visual",
            "published":"2023-04-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-adapter-v2-parameter-efficient-visual\/review\/?hl=106237"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24594,
        "row_id":106236,
        "rank":9,
        "method":"Video Chat",
        "mlmodel":{

        },
        "Model":"Video Chat",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-10",
        "metrics":{
            "gpt-score":"1.94"
        },
        "raw_metrics":{
            "gpt-score":1.94
        },
        "uses_additional_data":false,
        "paper":{
            "id":1205778,
            "title":"VideoChat: Chat-Centric Video Understanding",
            "url":"\/paper\/videochat-chat-centric-video-understanding",
            "published":"2023-05-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/videochat-chat-centric-video-understanding\/review\/?hl=106236"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24594,
        "row_id":106238,
        "rank":10,
        "method":"Video LLaMA",
        "mlmodel":{

        },
        "Model":"Video LLaMA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-06-05",
        "metrics":{
            "gpt-score":"1.82"
        },
        "raw_metrics":{
            "gpt-score":1.82
        },
        "uses_additional_data":false,
        "paper":{
            "id":1223066,
            "title":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding",
            "url":"\/paper\/video-llama-an-instruction-tuned-audio-visual",
            "published":"2023-06-05T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]