[
    {
        "table_id":24590,
        "row_id":112847,
        "rank":1,
        "method":"VideoChat2",
        "mlmodel":{

        },
        "method_short":"VideoChat2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-28",
        "metrics":{
            "gpt-score":"3.02"
        },
        "raw_metrics":{
            "gpt-score":3.02
        },
        "uses_additional_data":false,
        "paper":{
            "id":1329478,
            "title":"MVBench: A Comprehensive Multi-modal Video Understanding Benchmark",
            "url":"\/paper\/mvbench-a-comprehensive-multi-modal-video",
            "published":"2023-11-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mvbench-a-comprehensive-multi-modal-video\/review\/?hl=112847"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24590,
        "row_id":112508,
        "rank":2,
        "method":"Chat-UniVi",
        "mlmodel":{

        },
        "method_short":"Chat-UniVi",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-14",
        "metrics":{
            "gpt-score":"2.89"
        },
        "raw_metrics":{
            "gpt-score":2.89
        },
        "uses_additional_data":false,
        "paper":{
            "id":1320706,
            "title":"Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding",
            "url":"\/paper\/chat-univi-unified-visual-representation",
            "published":"2023-11-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/chat-univi-unified-visual-representation\/review\/?hl=112508"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24590,
        "row_id":112990,
        "rank":3,
        "method":"VTimeLLM",
        "mlmodel":{

        },
        "method_short":"VTimeLLM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-30",
        "metrics":{
            "gpt-score":"2.78"
        },
        "raw_metrics":{
            "gpt-score":2.78
        },
        "uses_additional_data":false,
        "paper":{
            "id":1333323,
            "title":"VTimeLLM: Empower LLM to Grasp Video Moments",
            "url":"\/paper\/vtimellm-empower-llm-to-grasp-video-moments",
            "published":"2023-11-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vtimellm-empower-llm-to-grasp-video-moments\/review\/?hl=112990"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24590,
        "row_id":112652,
        "rank":4,
        "method":"MovieChat",
        "mlmodel":{

        },
        "method_short":"MovieChat",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-07-31",
        "metrics":{
            "gpt-score":"2.76"
        },
        "raw_metrics":{
            "gpt-score":2.76
        },
        "uses_additional_data":false,
        "paper":{
            "id":1255388,
            "title":"MovieChat: From Dense Token to Sparse Memory for Long Video Understanding",
            "url":"\/paper\/moviechat-from-dense-token-to-sparse-memory",
            "published":"2023-07-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/moviechat-from-dense-token-to-sparse-memory\/review\/?hl=112652"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24590,
        "row_id":109670,
        "rank":5,
        "method":"BT-Adapter",
        "mlmodel":{

        },
        "method_short":"BT-Adapter",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-09-27",
        "metrics":{
            "gpt-score":"2.68"
        },
        "raw_metrics":{
            "gpt-score":2.68
        },
        "uses_additional_data":false,
        "paper":{
            "id":1289477,
            "title":"One For All: Video Conversation is Feasible Without Video Instruction Tuning",
            "url":"\/paper\/one-for-all-video-conversation-is-feasible",
            "published":"2023-09-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/one-for-all-video-conversation-is-feasible\/review\/?hl=109670"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24590,
        "row_id":106219,
        "rank":6,
        "method":"Video-ChatGPT",
        "mlmodel":{

        },
        "method_short":"Video-ChatGPT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-06-08",
        "metrics":{
            "gpt-score":"2.40"
        },
        "raw_metrics":{
            "gpt-score":2.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":1225920,
            "title":"Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models",
            "url":"\/paper\/video-chatgpt-towards-detailed-video",
            "published":"2023-06-08T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24590,
        "row_id":106220,
        "rank":7,
        "method":"Video Chat",
        "mlmodel":{

        },
        "method_short":"Video Chat",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-10",
        "metrics":{
            "gpt-score":"2.32"
        },
        "raw_metrics":{
            "gpt-score":2.32
        },
        "uses_additional_data":false,
        "paper":{
            "id":1205778,
            "title":"VideoChat: Chat-Centric Video Understanding",
            "url":"\/paper\/videochat-chat-centric-video-understanding",
            "published":"2023-05-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/videochat-chat-centric-video-understanding\/review\/?hl=106220"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24590,
        "row_id":109669,
        "rank":8,
        "method":"BT-Adapter (zero-shot)",
        "mlmodel":{

        },
        "method_short":"BT-Adapter ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-09-27",
        "metrics":{
            "gpt-score":"2.16"
        },
        "raw_metrics":{
            "gpt-score":2.16
        },
        "uses_additional_data":false,
        "paper":{
            "id":1289477,
            "title":"One For All: Video Conversation is Feasible Without Video Instruction Tuning",
            "url":"\/paper\/one-for-all-video-conversation-is-feasible",
            "published":"2023-09-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/one-for-all-video-conversation-is-feasible\/review\/?hl=109669"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24590,
        "row_id":106221,
        "rank":9,
        "method":"LLaMA Adapter",
        "mlmodel":{

        },
        "method_short":"LLaMA Adapter",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-28",
        "metrics":{
            "gpt-score":"2.03"
        },
        "raw_metrics":{
            "gpt-score":2.03
        },
        "uses_additional_data":false,
        "paper":{
            "id":1199360,
            "title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model",
            "url":"\/paper\/llama-adapter-v2-parameter-efficient-visual",
            "published":"2023-04-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-adapter-v2-parameter-efficient-visual\/review\/?hl=106221"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24590,
        "row_id":106222,
        "rank":10,
        "method":"Video LLaMA",
        "mlmodel":{

        },
        "method_short":"Video LLaMA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-06-05",
        "metrics":{
            "gpt-score":"1.96"
        },
        "raw_metrics":{
            "gpt-score":1.96
        },
        "uses_additional_data":false,
        "paper":{
            "id":1223066,
            "title":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding",
            "url":"\/paper\/video-llama-an-instruction-tuned-audio-visual",
            "published":"2023-06-05T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]