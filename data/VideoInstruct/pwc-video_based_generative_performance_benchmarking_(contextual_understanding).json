[
    {
        "table_id":24592,
        "row_id":112849,
        "rank":1,
        "Model":"VideoChat2",
        "mlmodel":{

        },
        "method_short":"VideoChat2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-28",
        "metrics":{
            "gpt-score":"3.51"
        },
        "raw_metrics":{
            "gpt-score":3.51
        },
        "uses_additional_data":false,
        "paper":{
            "id":1329478,
            "title":"MVBench: A Comprehensive Multi-modal Video Understanding Benchmark",
            "url":"\/paper\/mvbench-a-comprehensive-multi-modal-video",
            "published":"2023-11-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mvbench-a-comprehensive-multi-modal-video\/review\/?hl=112849"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24592,
        "row_id":112505,
        "rank":2,
        "Model":"Chat-UniVi",
        "mlmodel":{

        },
        "method_short":"Chat-UniVi",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-14",
        "metrics":{
            "gpt-score":"3.46"
        },
        "raw_metrics":{
            "gpt-score":3.46
        },
        "uses_additional_data":false,
        "paper":{
            "id":1320706,
            "title":"Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding",
            "url":"\/paper\/chat-univi-unified-visual-representation",
            "published":"2023-11-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/chat-univi-unified-visual-representation\/review\/?hl=112505"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24592,
        "row_id":112992,
        "rank":3,
        "Model":"VTimeLLM",
        "mlmodel":{

        },
        "method_short":"VTimeLLM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-30",
        "metrics":{
            "gpt-score":"3.40"
        },
        "raw_metrics":{
            "gpt-score":3.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":1333323,
            "title":"VTimeLLM: Empower LLM to Grasp Video Moments",
            "url":"\/paper\/vtimellm-empower-llm-to-grasp-video-moments",
            "published":"2023-11-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vtimellm-empower-llm-to-grasp-video-moments\/review\/?hl=112992"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24592,
        "row_id":109674,
        "rank":4,
        "Model":"BT-Adapter",
        "mlmodel":{

        },
        "method_short":"BT-Adapter",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-09-27",
        "metrics":{
            "gpt-score":"3.27"
        },
        "raw_metrics":{
            "gpt-score":3.27
        },
        "uses_additional_data":false,
        "paper":{
            "id":1289477,
            "title":"One For All: Video Conversation is Feasible Without Video Instruction Tuning",
            "url":"\/paper\/one-for-all-video-conversation-is-feasible",
            "published":"2023-09-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/one-for-all-video-conversation-is-feasible\/review\/?hl=109674"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24592,
        "row_id":112654,
        "rank":5,
        "Model":"MovieChat",
        "mlmodel":{

        },
        "method_short":"MovieChat",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-07-31",
        "metrics":{
            "gpt-score":"3.01"
        },
        "raw_metrics":{
            "gpt-score":3.01
        },
        "uses_additional_data":false,
        "paper":{
            "id":1255388,
            "title":"MovieChat: From Dense Token to Sparse Memory for Long Video Understanding",
            "url":"\/paper\/moviechat-from-dense-token-to-sparse-memory",
            "published":"2023-07-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/moviechat-from-dense-token-to-sparse-memory\/review\/?hl=112654"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24592,
        "row_id":109673,
        "rank":6,
        "Model":"BT-Adapter (zero-shot)",
        "mlmodel":{

        },
        "method_short":"BT-Adapter ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-09-27",
        "metrics":{
            "gpt-score":"2.89"
        },
        "raw_metrics":{
            "gpt-score":2.89
        },
        "uses_additional_data":false,
        "paper":{
            "id":1289477,
            "title":"One For All: Video Conversation is Feasible Without Video Instruction Tuning",
            "url":"\/paper\/one-for-all-video-conversation-is-feasible",
            "published":"2023-09-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/one-for-all-video-conversation-is-feasible\/review\/?hl=109673"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24592,
        "row_id":106227,
        "rank":7,
        "Model":"Video-ChatGPT",
        "mlmodel":{

        },
        "method_short":"Video-ChatGPT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-06-08",
        "metrics":{
            "gpt-score":"2.62"
        },
        "raw_metrics":{
            "gpt-score":2.62
        },
        "uses_additional_data":false,
        "paper":{
            "id":1225920,
            "title":"Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models",
            "url":"\/paper\/video-chatgpt-towards-detailed-video",
            "published":"2023-06-08T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24592,
        "row_id":106228,
        "rank":8,
        "Model":"Video Chat",
        "mlmodel":{

        },
        "method_short":"Video Chat",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-10",
        "metrics":{
            "gpt-score":"2.53"
        },
        "raw_metrics":{
            "gpt-score":2.53
        },
        "uses_additional_data":false,
        "paper":{
            "id":1205778,
            "title":"VideoChat: Chat-Centric Video Understanding",
            "url":"\/paper\/videochat-chat-centric-video-understanding",
            "published":"2023-05-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/videochat-chat-centric-video-understanding\/review\/?hl=106228"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24592,
        "row_id":106229,
        "rank":9,
        "Model":"LLaMA Adapter",
        "mlmodel":{

        },
        "method_short":"LLaMA Adapter",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-28",
        "metrics":{
            "gpt-score":"2.30"
        },
        "raw_metrics":{
            "gpt-score":2.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":1199360,
            "title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model",
            "url":"\/paper\/llama-adapter-v2-parameter-efficient-visual",
            "published":"2023-04-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llama-adapter-v2-parameter-efficient-visual\/review\/?hl=106229"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":24592,
        "row_id":106230,
        "rank":10,
        "Model":"Video LLaMA",
        "mlmodel":{

        },
        "method_short":"Video LLaMA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-06-05",
        "metrics":{
            "gpt-score":"2.16"
        },
        "raw_metrics":{
            "gpt-score":2.16
        },
        "uses_additional_data":false,
        "paper":{
            "id":1223066,
            "title":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding",
            "url":"\/paper\/video-llama-an-instruction-tuned-audio-visual",
            "published":"2023-06-05T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]