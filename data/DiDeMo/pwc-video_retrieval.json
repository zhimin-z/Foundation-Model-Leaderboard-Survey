[
    {
        "table_id":1138,
        "row_id":104628,
        "rank":1,
        "method":"VAST",
        "mlmodel":{

        },
        "Model":"VAST",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "text-to-video R@1":"72.0",
            "text-to-video R@5":"89.0",
            "text-to-video R@10":"91.4",
            "text-to-video R@50":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":72.0,
            "text-to-video R@5":89.0,
            "text-to-video R@10":91.4,
            "text-to-video R@50":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":100388,
        "rank":2,
        "method":"UMT-L (ViT-L\/16)",
        "mlmodel":{

        },
        "Model":"UMT-L ",
        "method_details":"ViT-L\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-28",
        "metrics":{
            "text-to-video R@1":"70.4",
            "text-to-video R@5":"90.1",
            "text-to-video R@10":"93.5",
            "text-to-video R@50":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":"65.7",
            "video-to-text R@5":"89.6",
            "video-to-text R@10":"93.3",
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":70.4,
            "text-to-video R@5":90.1,
            "text-to-video R@10":93.5,
            "text-to-video R@50":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":65.7,
            "video-to-text R@5":89.6,
            "video-to-text R@10":93.3,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1181934,
            "title":"Unmasked Teacher: Towards Training-Efficient Video Foundation Models",
            "url":"\/paper\/unmasked-teacher-towards-training-efficient",
            "published":"2023-03-28T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":101408,
        "rank":3,
        "method":"VALOR",
        "mlmodel":{

        },
        "Model":"VALOR",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-17",
        "metrics":{
            "text-to-video R@1":"61.5",
            "text-to-video R@5":"85.3",
            "text-to-video R@10":"90.4",
            "text-to-video R@50":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":61.5,
            "text-to-video R@5":85.3,
            "text-to-video R@10":90.4,
            "text-to-video R@50":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1191887,
            "title":"VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset",
            "url":"\/paper\/valor-vision-audio-language-omni-perception",
            "published":"2023-04-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/valor-vision-audio-language-omni-perception\/review\/?hl=101408"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":111065,
        "rank":4,
        "method":"TESTA (ViT-B\/16)",
        "mlmodel":{

        },
        "Model":"TESTA ",
        "method_details":"ViT-B\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-10-29",
        "metrics":{
            "text-to-video R@1":"61.2",
            "text-to-video R@5":"87.2",
            "text-to-video R@10":"91.5",
            "text-to-video R@50":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":61.2,
            "text-to-video R@5":87.2,
            "text-to-video R@10":91.5,
            "text-to-video R@50":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1310474,
            "title":"TESTA: Temporal-Spatial Token Aggregation for Long-form Video-Language Understanding",
            "url":"\/paper\/testa-temporal-spatial-token-aggregation-for",
            "published":"2023-10-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/testa-temporal-spatial-token-aggregation-for\/review\/?hl=111065"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":86725,
        "rank":5,
        "method":"InternVideo",
        "mlmodel":{

        },
        "Model":"InternVideo",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-06",
        "metrics":{
            "text-to-video R@1":"57.9",
            "text-to-video R@5":null,
            "text-to-video R@10":null,
            "text-to-video R@50":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":"59.1",
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":57.9,
            "text-to-video R@5":null,
            "text-to-video R@10":null,
            "text-to-video R@50":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":59.1,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1124231,
            "title":"InternVideo: General Video Foundation Models via Generative and Discriminative Learning",
            "url":"\/paper\/internvideo-general-video-foundation-models",
            "published":"2022-12-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/internvideo-general-video-foundation-models\/review\/?hl=86725"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":103557,
        "rank":6,
        "method":"VLAB",
        "mlmodel":{

        },
        "Model":"VLAB",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-22",
        "metrics":{
            "text-to-video R@1":"56.8",
            "text-to-video R@5":"81.6",
            "text-to-video R@10":"88.7",
            "text-to-video R@50":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":56.8,
            "text-to-video R@5":81.6,
            "text-to-video R@10":88.7,
            "text-to-video R@50":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1212983,
            "title":"VLAB: Enhancing Video Language Pre-training by Feature Adapting and Blending",
            "url":"\/paper\/vlab-enhancing-video-language-pre-training-by",
            "published":"2023-05-22T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/vlab-enhancing-video-language-pre-training-by\/review\/?hl=103557"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":88513,
        "rank":7,
        "method":"HiTeA",
        "mlmodel":{

        },
        "Model":"HiTeA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-30",
        "metrics":{
            "text-to-video R@1":"56.5",
            "text-to-video R@5":"81.7",
            "text-to-video R@10":"89.7",
            "text-to-video R@50":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":56.5,
            "text-to-video R@5":81.7,
            "text-to-video R@10":89.7,
            "text-to-video R@50":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1136078,
            "title":"HiTeA: Hierarchical Temporal-Aware Video-Language Pre-training",
            "url":"\/paper\/hitea-hierarchical-temporal-aware-video",
            "published":"2022-12-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/hitea-hierarchical-temporal-aware-video\/review\/?hl=88513"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":99061,
        "rank":8,
        "method":"MuLTI",
        "mlmodel":{

        },
        "Model":"MuLTI",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-10",
        "metrics":{
            "text-to-video R@1":"56.5",
            "text-to-video R@5":"80.2",
            "text-to-video R@10":"87.0",
            "text-to-video R@50":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":56.5,
            "text-to-video R@5":80.2,
            "text-to-video R@10":87.0,
            "text-to-video R@50":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1171789,
            "title":"MuLTI: Efficient Video-and-Language Understanding with MultiWay-Sampler and Multiple Choice Modeling",
            "url":"\/paper\/multi-efficient-video-and-language",
            "published":"2023-03-10T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/multi-efficient-video-and-language\/review\/?hl=99061"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":96439,
        "rank":9,
        "method":"mPLUG-2",
        "mlmodel":{

        },
        "Model":"mPLUG-2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-01",
        "metrics":{
            "text-to-video R@1":"56.4",
            "text-to-video R@5":"79.1",
            "text-to-video R@10":"85.2",
            "text-to-video R@50":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":56.4,
            "text-to-video R@5":79.1,
            "text-to-video R@10":85.2,
            "text-to-video R@50":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1151002,
            "title":"mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video",
            "url":"\/paper\/mplug-2-a-modularized-multi-modal-foundation",
            "published":"2023-02-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mplug-2-a-modularized-multi-modal-foundation\/review\/?hl=96439"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":68569,
        "rank":10,
        "method":"CLIP-ViP",
        "mlmodel":{

        },
        "Model":"CLIP-ViP",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-14",
        "metrics":{
            "text-to-video R@1":"55.3",
            "text-to-video R@5":"82",
            "text-to-video R@10":"89.3",
            "text-to-video R@50":null,
            "text-to-video Median Rank":"1",
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":55.3,
            "text-to-video R@5":82.0,
            "text-to-video R@10":89.3,
            "text-to-video R@50":null,
            "text-to-video Median Rank":1.0,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1074966,
            "title":"CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Representation Alignment",
            "url":"\/paper\/clip-vip-adapting-pre-trained-image-text",
            "published":"2022-09-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/clip-vip-adapting-pre-trained-image-text\/review\/?hl=68569"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":97496,
        "rank":11,
        "method":"STAN",
        "mlmodel":{

        },
        "Model":"STAN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-26",
        "metrics":{
            "text-to-video R@1":"54.6",
            "text-to-video R@5":"78.4",
            "text-to-video R@10":"85.1",
            "text-to-video R@50":null,
            "text-to-video Median Rank":"1",
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":54.6,
            "text-to-video R@5":78.4,
            "text-to-video R@10":85.1,
            "text-to-video R@50":null,
            "text-to-video Median Rank":1.0,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1147773,
            "title":"Revisiting Temporal Modeling for CLIP-based Image-to-Video Knowledge Transferring",
            "url":"\/paper\/revisiting-temporal-modeling-for-clip-based",
            "published":"2023-01-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/revisiting-temporal-modeling-for-clip-based\/review\/?hl=97496"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":56543,
        "rank":12,
        "method":"Singularity",
        "mlmodel":{

        },
        "Model":"Singularity",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-07",
        "metrics":{
            "text-to-video R@1":"53.9",
            "text-to-video R@5":"79.4",
            "text-to-video R@10":"86.9",
            "text-to-video R@50":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":53.9,
            "text-to-video R@5":79.4,
            "text-to-video R@10":86.9,
            "text-to-video R@50":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1023192,
            "title":"Revealing Single Frame Bias for Video-and-Language Learning",
            "url":"\/paper\/revealing-single-frame-bias-for-video-and",
            "published":"2022-06-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/revealing-single-frame-bias-for-video-and\/review\/?hl=56543"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":109129,
        "rank":13,
        "method":"DMAE (ViT-B\/32)",
        "mlmodel":{

        },
        "Model":"DMAE ",
        "method_details":"ViT-B\/32",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-09-20",
        "metrics":{
            "text-to-video R@1":"52.7",
            "text-to-video R@5":"79.3",
            "text-to-video R@10":"86.6",
            "text-to-video R@50":null,
            "text-to-video Median Rank":"1.0",
            "text-to-video Mean Rank":"10.5",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":52.7,
            "text-to-video R@5":79.3,
            "text-to-video R@10":86.6,
            "text-to-video R@50":null,
            "text-to-video Median Rank":1.0,
            "text-to-video Mean Rank":10.5,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1284092,
            "title":"Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning",
            "url":"\/paper\/dual-modal-attention-enhanced-text-video",
            "published":"2023-09-20T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/dual-modal-attention-enhanced-text-video\/review\/?hl=109129"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":70643,
        "rank":14,
        "method":"HunYuan_tvr (huge)",
        "mlmodel":{

        },
        "Model":"HunYuan_tvr ",
        "method_details":"huge",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-07",
        "metrics":{
            "text-to-video R@1":"52.7",
            "text-to-video R@5":"77.8",
            "text-to-video R@10":"85.2",
            "text-to-video R@50":null,
            "text-to-video Median Rank":"1.0",
            "text-to-video Mean Rank":"13.7",
            "video-to-text R@1":"54.1",
            "video-to-text R@5":"78.3",
            "video-to-text R@10":"86.8",
            "video-to-text Median Rank":"1.0",
            "video-to-text Mean Rank":"9.1",
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":52.7,
            "text-to-video R@5":77.8,
            "text-to-video R@10":85.2,
            "text-to-video R@50":null,
            "text-to-video Median Rank":1.0,
            "text-to-video Mean Rank":13.7,
            "video-to-text R@1":54.1,
            "video-to-text R@5":78.3,
            "video-to-text R@10":86.8,
            "video-to-text Median Rank":1.0,
            "video-to-text Mean Rank":9.1,
            "text-to-videoR@1":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":990765,
            "title":"Tencent Text-Video Retrieval: Hierarchical Cross-Modal Interactions with Multi-Level Representations",
            "url":"\/paper\/hunyuan-tvr-for-text-video-retrivial",
            "published":"2022-04-07T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":69051,
        "rank":15,
        "method":"OmniVL",
        "mlmodel":{

        },
        "Model":"OmniVL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-15",
        "metrics":{
            "text-to-video R@1":"52.4",
            "text-to-video R@5":"79.5",
            "text-to-video R@10":"85.4",
            "text-to-video R@50":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":52.4,
            "text-to-video R@5":79.5,
            "text-to-video R@10":85.4,
            "text-to-video R@50":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1075042,
            "title":"OmniVL:One Foundation Model for Image-Language and Video-Language Tasks",
            "url":"\/paper\/omnivl-one-foundation-model-for-image",
            "published":"2022-09-15T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/omnivl-one-foundation-model-for-image\/review\/?hl=69051"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":51594,
        "rank":16,
        "method":"HunYuan_tvr",
        "mlmodel":{

        },
        "Model":"HunYuan_tvr",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-07",
        "metrics":{
            "text-to-video R@1":"52.1",
            "text-to-video R@5":"78.2",
            "text-to-video R@10":"85.7",
            "text-to-video R@50":null,
            "text-to-video Median Rank":"1",
            "text-to-video Mean Rank":"11.1",
            "video-to-text R@1":"54.8",
            "video-to-text R@5":"79.9",
            "video-to-text R@10":"87.2",
            "video-to-text Median Rank":"1",
            "video-to-text Mean Rank":"7.1",
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":52.1,
            "text-to-video R@5":78.2,
            "text-to-video R@10":85.7,
            "text-to-video R@50":null,
            "text-to-video Median Rank":1.0,
            "text-to-video Mean Rank":11.1,
            "video-to-text R@1":54.8,
            "video-to-text R@5":79.9,
            "video-to-text R@10":87.2,
            "video-to-text Median Rank":1.0,
            "video-to-text Mean Rank":7.1,
            "text-to-videoR@1":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":990765,
            "title":"Tencent Text-Video Retrieval: Hierarchical Cross-Modal Interactions with Multi-Level Representations",
            "url":"\/paper\/hunyuan-tvr-for-text-video-retrivial",
            "published":"2022-04-07T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":88699,
        "rank":17,
        "method":"Cap4Video",
        "mlmodel":{

        },
        "Model":"Cap4Video",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-31",
        "metrics":{
            "text-to-video R@1":"52.0",
            "text-to-video R@5":"79.4",
            "text-to-video R@10":"87.5",
            "text-to-video R@50":null,
            "text-to-video Median Rank":"1",
            "text-to-video Mean Rank":"10.5",
            "video-to-text R@1":"51.2",
            "video-to-text R@5":"78.5",
            "video-to-text R@10":"87.4",
            "video-to-text Median Rank":"1",
            "video-to-text Mean Rank":"7.3",
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":52.0,
            "text-to-video R@5":79.4,
            "text-to-video R@10":87.5,
            "text-to-video R@50":null,
            "text-to-video Median Rank":1.0,
            "text-to-video Mean Rank":10.5,
            "video-to-text R@1":51.2,
            "video-to-text R@5":78.5,
            "video-to-text R@10":87.4,
            "video-to-text Median Rank":1.0,
            "video-to-text Mean Rank":7.3,
            "text-to-videoR@1":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1136847,
            "title":"Cap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval?",
            "url":"\/paper\/cap4video-what-can-auxiliary-captions-do-for",
            "published":"2022-12-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/cap4video-what-can-auxiliary-captions-do-for\/review\/?hl=88699"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":88010,
        "rank":18,
        "method":"Clover",
        "mlmodel":{

        },
        "Model":"Clover",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-16",
        "metrics":{
            "text-to-video R@1":"50.1",
            "text-to-video R@5":"76.7",
            "text-to-video R@10":"85.6",
            "text-to-video R@50":null,
            "text-to-video Median Rank":"1",
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":50.1,
            "text-to-video R@5":76.7,
            "text-to-video R@10":85.6,
            "text-to-video R@50":null,
            "text-to-video Median Rank":1.0,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1045027,
            "title":"Clover: Towards A Unified Video-Language Alignment and Fusion Model",
            "url":"\/paper\/clover-towards-a-unified-video-language",
            "published":"2022-07-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/clover-towards-a-unified-video-language\/review\/?hl=88010"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":49617,
        "rank":19,
        "method":"DRL",
        "mlmodel":{

        },
        "Model":"DRL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-14",
        "metrics":{
            "text-to-video R@1":"49.0",
            "text-to-video R@5":"76.5",
            "text-to-video R@10":"84.5",
            "text-to-video R@50":null,
            "text-to-video Median Rank":"2.0",
            "text-to-video Mean Rank":"11.5",
            "video-to-text R@1":"49.9",
            "video-to-text R@5":null,
            "video-to-text R@10":"83.3",
            "video-to-text Median Rank":"2",
            "video-to-text Mean Rank":"7.9",
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":49.0,
            "text-to-video R@5":76.5,
            "text-to-video R@10":84.5,
            "text-to-video R@50":null,
            "text-to-video Median Rank":2.0,
            "text-to-video Mean Rank":11.5,
            "video-to-text R@1":49.9,
            "video-to-text R@5":null,
            "video-to-text R@10":83.3,
            "video-to-text Median Rank":2.0,
            "video-to-text Mean Rank":7.9,
            "text-to-videoR@1":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":976257,
            "title":"Disentangled Representation Learning for Text-Video Retrieval",
            "url":"\/paper\/disentangled-representation-learning-for-text",
            "published":"2022-03-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/disentangled-representation-learning-for-text\/review\/?hl=49617"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":100108,
        "rank":20,
        "method":"DiffusionRet+QB-Norm",
        "mlmodel":{

        },
        "Model":"DiffusionRet+QB-Norm",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-17",
        "metrics":{
            "text-to-video R@1":"48.9",
            "text-to-video R@5":"75.5",
            "text-to-video R@10":"83.3",
            "text-to-video R@50":null,
            "text-to-video Median Rank":"2.0",
            "text-to-video Mean Rank":"14.1",
            "video-to-text R@1":"50.3",
            "video-to-text R@5":"75.1",
            "video-to-text R@10":"82.9",
            "video-to-text Median Rank":"1.0",
            "video-to-text Mean Rank":"10.3",
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":48.9,
            "text-to-video R@5":75.5,
            "text-to-video R@10":83.3,
            "text-to-video R@50":null,
            "text-to-video Median Rank":2.0,
            "text-to-video Mean Rank":14.1,
            "video-to-text R@1":50.3,
            "video-to-text R@5":75.1,
            "video-to-text R@10":82.9,
            "video-to-text Median Rank":1.0,
            "video-to-text Mean Rank":10.3,
            "text-to-videoR@1":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1175924,
            "title":"DiffusionRet: Generative Text-Video Retrieval with Diffusion Model",
            "url":"\/paper\/diffusionret-generative-text-video-retrieval",
            "published":"2023-03-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/diffusionret-generative-text-video-retrieval\/review\/?hl=100108"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":96081,
        "rank":21,
        "method":"VIOLETv2",
        "mlmodel":{

        },
        "Model":"VIOLETv2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-04",
        "metrics":{
            "text-to-video R@1":"47.9",
            "text-to-video R@5":"76.5",
            "text-to-video R@10":"84.1",
            "text-to-video R@50":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":47.9,
            "text-to-video R@5":76.5,
            "text-to-video R@10":84.1,
            "text-to-video R@50":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1069338,
            "title":"An Empirical Study of End-to-End Video-Language Transformers with Masked Visual Modeling",
            "url":"\/paper\/an-empirical-study-of-end-to-end-video",
            "published":"2022-09-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/an-empirical-study-of-end-to-end-video\/review\/?hl=96081"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":65462,
        "rank":22,
        "method":"X-CLIP",
        "mlmodel":{

        },
        "Model":"X-CLIP",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-15",
        "metrics":{
            "text-to-video R@1":"47.8",
            "text-to-video R@5":"79.3",
            "text-to-video R@10":null,
            "text-to-video R@50":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":"12.6",
            "video-to-text R@1":"47.8",
            "video-to-text R@5":null,
            "video-to-text R@10":"76.8",
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":"10.5",
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":47.8,
            "text-to-video R@5":79.3,
            "text-to-video R@10":null,
            "text-to-video R@50":null,
            "text-to-video Median Rank":null,
            "text-to-video Mean Rank":12.6,
            "video-to-text R@1":47.8,
            "video-to-text R@5":null,
            "video-to-text R@10":76.8,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":10.5,
            "text-to-videoR@1":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1044539,
            "title":"X-CLIP: End-to-End Multi-grained Contrastive Learning for Video-Text Retrieval",
            "url":"\/paper\/x-clip-end-to-end-multi-grained-contrastive",
            "published":"2022-07-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/x-clip-end-to-end-multi-grained-contrastive\/review\/?hl=65462"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":100337,
        "rank":23,
        "method":"HBI",
        "mlmodel":{

        },
        "Model":"HBI",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-25",
        "metrics":{
            "text-to-video R@1":"46.9",
            "text-to-video R@5":"74.9",
            "text-to-video R@10":"82.7",
            "text-to-video R@50":null,
            "text-to-video Median Rank":"2.0",
            "text-to-video Mean Rank":"12.1",
            "video-to-text R@1":"46.2",
            "video-to-text R@5":"73.0",
            "video-to-text R@10":"82.7",
            "video-to-text Median Rank":"2.0",
            "video-to-text Mean Rank":"8.7",
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":46.9,
            "text-to-video R@5":74.9,
            "text-to-video R@10":82.7,
            "text-to-video R@50":null,
            "text-to-video Median Rank":2.0,
            "text-to-video Mean Rank":12.1,
            "video-to-text R@1":46.2,
            "video-to-text R@5":73.0,
            "video-to-text R@10":82.7,
            "video-to-text Median Rank":2.0,
            "video-to-text Mean Rank":8.7,
            "text-to-videoR@1":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1180602,
            "title":"Video-Text as Game Players: Hierarchical Banzhaf Interaction for Cross-Modal Representation Learning",
            "url":"\/paper\/video-text-as-game-players-hierarchical",
            "published":"2023-03-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/video-text-as-game-players-hierarchical\/review\/?hl=100337"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":100107,
        "rank":24,
        "method":"DiffusionRet",
        "mlmodel":{

        },
        "Model":"DiffusionRet",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-17",
        "metrics":{
            "text-to-video R@1":"46.7",
            "text-to-video R@5":"74.7",
            "text-to-video R@10":"82.7",
            "text-to-video R@50":null,
            "text-to-video Median Rank":"2.0",
            "text-to-video Mean Rank":"14.3",
            "video-to-text R@1":"46.2",
            "video-to-text R@5":"74.3",
            "video-to-text R@10":"82.2",
            "video-to-text Median Rank":"2.0",
            "video-to-text Mean Rank":"10.7",
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":46.7,
            "text-to-video R@5":74.7,
            "text-to-video R@10":82.7,
            "text-to-video R@50":null,
            "text-to-video Median Rank":2.0,
            "text-to-video Mean Rank":14.3,
            "video-to-text R@1":46.2,
            "video-to-text R@5":74.3,
            "video-to-text R@10":82.2,
            "video-to-text Median Rank":2.0,
            "video-to-text Mean Rank":10.7,
            "text-to-videoR@1":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1175924,
            "title":"DiffusionRet: Generative Text-Video Retrieval with Diffusion Model",
            "url":"\/paper\/diffusionret-generative-text-video-retrieval",
            "published":"2023-03-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/diffusionret-generative-text-video-retrieval\/review\/?hl=100107"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":49618,
        "rank":25,
        "method":"CAMoE",
        "mlmodel":{

        },
        "Model":"CAMoE",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-09",
        "metrics":{
            "text-to-video R@1":"43.8",
            "text-to-video R@5":"71.4",
            "text-to-video R@10":"79.9",
            "text-to-video R@50":null,
            "text-to-video Median Rank":"2.0",
            "text-to-video Mean Rank":"16.3",
            "video-to-text R@1":"45.5",
            "video-to-text R@5":null,
            "video-to-text R@10":"80.5",
            "video-to-text Median Rank":"2",
            "video-to-text Mean Rank":"10.2",
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":43.8,
            "text-to-video R@5":71.4,
            "text-to-video R@10":79.9,
            "text-to-video R@50":null,
            "text-to-video Median Rank":2.0,
            "text-to-video Mean Rank":16.3,
            "video-to-text R@1":45.5,
            "video-to-text R@5":null,
            "video-to-text R@10":80.5,
            "video-to-text Median Rank":2.0,
            "video-to-text Mean Rank":10.2,
            "text-to-videoR@1":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":864259,
            "title":"Improving Video-Text Retrieval by Multi-Stream Corpus Alignment and Dual Softmax Loss",
            "url":"\/paper\/improving-video-text-retrieval-by-multi",
            "published":"2021-09-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improving-video-text-retrieval-by-multi\/review\/?hl=49618"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":45085,
        "rank":26,
        "method":"QB-Norm+CLIP4Clip",
        "mlmodel":{

        },
        "Model":"QB-Norm+CLIP4Clip",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-23",
        "metrics":{
            "text-to-video R@1":"43.5",
            "text-to-video R@5":"71.4",
            "text-to-video R@10":"80.9",
            "text-to-video R@50":null,
            "text-to-video Median Rank":"2.0",
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":43.5,
            "text-to-video R@5":71.4,
            "text-to-video R@10":80.9,
            "text-to-video R@50":null,
            "text-to-video Median Rank":2.0,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":933091,
            "title":"Cross Modal Retrieval with Querybank Normalisation",
            "url":"\/paper\/cross-modal-retrieval-with-querybank",
            "published":"2021-12-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/cross-modal-retrieval-with-querybank\/review\/?hl=45085"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":39886,
        "rank":27,
        "method":"CLIP4Clip",
        "mlmodel":{

        },
        "Model":"CLIP4Clip",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-18",
        "metrics":{
            "text-to-video R@1":"43.4",
            "text-to-video R@5":"70.2",
            "text-to-video R@10":"80.6",
            "text-to-video R@50":null,
            "text-to-video Median Rank":"2.0",
            "text-to-video Mean Rank":"17.5",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":43.4,
            "text-to-video R@5":70.2,
            "text-to-video R@10":80.6,
            "text-to-video R@50":null,
            "text-to-video Median Rank":2.0,
            "text-to-video Mean Rank":17.5,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":784398,
            "title":"CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval",
            "url":"\/paper\/clip4clip-an-empirical-study-of-clip-for-end",
            "published":"2021-04-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/clip4clip-an-empirical-study-of-clip-for-end\/review\/?hl=39886"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":60791,
        "rank":28,
        "method":"ALPRO",
        "mlmodel":{

        },
        "Model":"ALPRO",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-17",
        "metrics":{
            "text-to-video R@1":"35.9",
            "text-to-video R@5":"67.5",
            "text-to-video R@10":"78.8",
            "text-to-video R@50":null,
            "text-to-video Median Rank":"3",
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":35.9,
            "text-to-video R@5":67.5,
            "text-to-video R@10":78.8,
            "text-to-video R@50":null,
            "text-to-video Median Rank":3.0,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":932382,
            "title":"Align and Prompt: Video-and-Language Pre-training with Entity Prompts",
            "url":"\/paper\/align-and-prompt-video-and-language-pre",
            "published":"2021-12-17T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":29237,
        "rank":29,
        "method":"FROZEN",
        "mlmodel":{

        },
        "Model":"FROZEN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-01",
        "metrics":{
            "text-to-video R@1":"31.0",
            "text-to-video R@5":"59.8",
            "text-to-video R@10":"72.4",
            "text-to-video R@50":null,
            "text-to-video Median Rank":"3",
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":31.0,
            "text-to-video R@5":59.8,
            "text-to-video R@10":72.4,
            "text-to-video R@50":null,
            "text-to-video Median Rank":3.0,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":773452,
            "title":"Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval",
            "url":"\/paper\/frozen-in-time-a-joint-video-and-image",
            "published":"2021-04-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/frozen-in-time-a-joint-video-and-image\/review\/?hl=29237"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":110583,
        "rank":30,
        "method":"HD-VILA",
        "mlmodel":{

        },
        "Model":"HD-VILA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-19",
        "metrics":{
            "text-to-video R@1":"28.8",
            "text-to-video R@5":"57.4",
            "text-to-video R@10":"69.1",
            "text-to-video R@50":null,
            "text-to-video Median Rank":"4",
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":28.8,
            "text-to-video R@5":57.4,
            "text-to-video R@10":69.1,
            "text-to-video R@50":null,
            "text-to-video Median Rank":4.0,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":912961,
            "title":"Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions",
            "url":"\/paper\/advancing-high-resolution-video-language",
            "published":"2021-11-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/advancing-high-resolution-video-language\/review\/?hl=110583"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":37689,
        "rank":31,
        "method":"PO Loss",
        "mlmodel":{

        },
        "Model":"PO Loss",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-09",
        "metrics":{
            "text-to-video R@1":"16.3",
            "text-to-video R@5":null,
            "text-to-video R@10":"56.5",
            "text-to-video R@50":null,
            "text-to-video Median Rank":"8",
            "text-to-video Mean Rank":"40.2",
            "video-to-text R@1":"15",
            "video-to-text R@5":null,
            "video-to-text R@10":"54.9",
            "video-to-text Median Rank":"8",
            "video-to-text Mean Rank":"39.6",
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":16.3,
            "text-to-video R@5":null,
            "text-to-video R@10":56.5,
            "text-to-video R@50":null,
            "text-to-video Median Rank":8.0,
            "text-to-video Mean Rank":40.2,
            "video-to-text R@1":15.0,
            "video-to-text R@5":null,
            "video-to-text R@10":54.9,
            "video-to-text Median Rank":8.0,
            "video-to-text Mean Rank":39.6,
            "text-to-videoR@1":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":752396,
            "title":"Rudder: A Cross Lingual Video and Text Retrieval Dataset",
            "url":"\/paper\/rudder-a-cross-lingual-video-and-text",
            "published":"2021-03-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rudder-a-cross-lingual-video-and-text\/review\/?hl=37689"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":12881,
        "rank":32,
        "method":"Collaborative Experts",
        "mlmodel":{

        },
        "Model":"Collaborative Experts",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-07-31",
        "metrics":{
            "text-to-video R@1":"16.1",
            "text-to-video R@5":"41.1",
            "text-to-video R@10":"54.4",
            "text-to-video R@50":"82.7",
            "text-to-video Median Rank":"8.3",
            "text-to-video Mean Rank":"43.7",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "raw_metrics":{
            "text-to-video R@1":16.1,
            "text-to-video R@5":41.1,
            "text-to-video R@10":54.4,
            "text-to-video R@50":82.7,
            "text-to-video Median Rank":8.3,
            "text-to-video Mean Rank":43.7,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":148748,
            "title":"Use What You Have: Video Retrieval Using Representations From Collaborative Experts",
            "url":"\/paper\/use-what-you-have-video-retrieval-using",
            "published":"2019-07-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/use-what-you-have-video-retrieval-using\/review\/?hl=12881"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":1138,
        "row_id":111273,
        "rank":33,
        "method":"Aurora (ours, r=64)",
        "mlmodel":{

        },
        "Model":"Aurora ",
        "method_details":"ours, r=64",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "text-to-video R@1":null,
            "text-to-video R@5":"77.4",
            "text-to-video R@10":"85.3",
            "text-to-video R@50":null,
            "text-to-video Median Rank":"1",
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":"53.1"
        },
        "raw_metrics":{
            "text-to-video R@1":null,
            "text-to-video R@5":77.4,
            "text-to-video R@10":85.3,
            "text-to-video R@50":null,
            "text-to-video Median Rank":1.0,
            "text-to-video Mean Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null,
            "video-to-text Median Rank":null,
            "video-to-text Mean Rank":null,
            "text-to-videoR@1":53.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]