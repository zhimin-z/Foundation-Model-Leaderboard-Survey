[
    {
        "table_id":20341,
        "row_id":104635,
        "rank":1,
        "method":"VAST",
        "mlmodel":{

        },
        "method_short":"VAST",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "text-to-video R@1":"55.5",
            "text-to-video R@5":"74.3",
            "text-to-video R@10":"79.6",
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null
        },
        "raw_metrics":{
            "text-to-video R@1":55.5,
            "text-to-video R@5":74.3,
            "text-to-video R@10":79.6,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20341,
        "row_id":100387,
        "rank":2,
        "method":"UMT-L (ViT-L\/16)",
        "mlmodel":{

        },
        "method_short":"UMT-L ",
        "method_details":"ViT-L\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-28",
        "metrics":{
            "text-to-video R@1":"48.6",
            "text-to-video R@5":"72.9",
            "text-to-video R@10":"79.0",
            "text-to-video Median Rank":null,
            "video-to-text R@1":"49.9",
            "video-to-text R@5":"74.8",
            "video-to-text R@10":"81.4"
        },
        "raw_metrics":{
            "text-to-video R@1":48.6,
            "text-to-video R@5":72.9,
            "text-to-video R@10":79.0,
            "text-to-video Median Rank":null,
            "video-to-text R@1":49.9,
            "video-to-text R@5":74.8,
            "video-to-text R@10":81.4
        },
        "uses_additional_data":true,
        "paper":{
            "id":1181934,
            "title":"Unmasked Teacher: Towards Training-Efficient Video Foundation Models",
            "url":"\/paper\/unmasked-teacher-towards-training-efficient",
            "published":"2023-03-28T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20341,
        "row_id":102763,
        "rank":3,
        "method":"mPLUG-2",
        "mlmodel":{

        },
        "method_short":"mPLUG-2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-01",
        "metrics":{
            "text-to-video R@1":"45.7",
            "text-to-video R@5":"71.1",
            "text-to-video R@10":"79.2",
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null
        },
        "raw_metrics":{
            "text-to-video R@1":45.7,
            "text-to-video R@5":71.1,
            "text-to-video R@10":79.2,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1151002,
            "title":"mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video",
            "url":"\/paper\/mplug-2-a-modularized-multi-modal-foundation",
            "published":"2023-02-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mplug-2-a-modularized-multi-modal-foundation\/review\/?hl=102763"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20341,
        "row_id":88511,
        "rank":4,
        "method":"HiTeA-17M",
        "mlmodel":{

        },
        "method_short":"HiTeA-17M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-30",
        "metrics":{
            "text-to-video R@1":"43.2",
            "text-to-video R@5":"69.3",
            "text-to-video R@10":"79.0",
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null
        },
        "raw_metrics":{
            "text-to-video R@1":43.2,
            "text-to-video R@5":69.3,
            "text-to-video R@10":79.0,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1136078,
            "title":"HiTeA: Hierarchical Temporal-Aware Video-Language Pre-training",
            "url":"\/paper\/hitea-hierarchical-temporal-aware-video",
            "published":"2022-12-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/hitea-hierarchical-temporal-aware-video\/review\/?hl=88511"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20341,
        "row_id":110313,
        "rank":5,
        "method":"LanguageBind",
        "mlmodel":{

        },
        "method_short":"LanguageBind",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-10-03",
        "metrics":{
            "text-to-video R@1":"39.9",
            "text-to-video R@5":"66.1",
            "text-to-video R@10":"74.6",
            "text-to-video Median Rank":"2",
            "video-to-text R@1":"39.8",
            "video-to-text R@5":"67.8",
            "video-to-text R@10":"76.2"
        },
        "raw_metrics":{
            "text-to-video R@1":39.9,
            "text-to-video R@5":66.1,
            "text-to-video R@10":74.6,
            "text-to-video Median Rank":2.0,
            "video-to-text R@1":39.8,
            "video-to-text R@5":67.8,
            "video-to-text R@10":76.2
        },
        "uses_additional_data":true,
        "paper":{
            "id":1292471,
            "title":"LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment",
            "url":"\/paper\/languagebind-extending-video-language",
            "published":"2023-10-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/languagebind-extending-video-language\/review\/?hl=110313"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20341,
        "row_id":102851,
        "rank":6,
        "method":"Singularity-17M",
        "mlmodel":{

        },
        "method_short":"Singularity-17M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-07",
        "metrics":{
            "text-to-video R@1":"37.1",
            "text-to-video R@5":"61.7",
            "text-to-video R@10":"69.9",
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null
        },
        "raw_metrics":{
            "text-to-video R@1":37.1,
            "text-to-video R@5":61.7,
            "text-to-video R@10":69.9,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1023192,
            "title":"Revealing Single Frame Bias for Video-and-Language Learning",
            "url":"\/paper\/revealing-single-frame-bias-for-video-and",
            "published":"2022-06-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/revealing-single-frame-bias-for-video-and\/review\/?hl=102851"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20341,
        "row_id":102850,
        "rank":7,
        "method":"Singularity-5M",
        "mlmodel":{

        },
        "method_short":"Singularity-5M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-07",
        "metrics":{
            "text-to-video R@1":"36.9",
            "text-to-video R@5":"61.1",
            "text-to-video R@10":"69.3",
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null
        },
        "raw_metrics":{
            "text-to-video R@1":36.9,
            "text-to-video R@5":61.1,
            "text-to-video R@10":69.3,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1023192,
            "title":"Revealing Single Frame Bias for Video-and-Language Learning",
            "url":"\/paper\/revealing-single-frame-bias-for-video-and",
            "published":"2022-06-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/revealing-single-frame-bias-for-video-and\/review\/?hl=102850"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20341,
        "row_id":102854,
        "rank":8,
        "method":"HiTeA-5M",
        "mlmodel":{

        },
        "method_short":"HiTeA-5M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-30",
        "metrics":{
            "text-to-video R@1":"36.1",
            "text-to-video R@5":"60.1",
            "text-to-video R@10":"70.3",
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null
        },
        "raw_metrics":{
            "text-to-video R@1":36.1,
            "text-to-video R@5":60.1,
            "text-to-video R@10":70.3,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1136078,
            "title":"HiTeA: Hierarchical Temporal-Aware Video-Language Pre-training",
            "url":"\/paper\/hitea-hierarchical-temporal-aware-video",
            "published":"2022-12-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/hitea-hierarchical-temporal-aware-video\/review\/?hl=102854"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20341,
        "row_id":109689,
        "rank":9,
        "method":"BT-Adapter",
        "mlmodel":{

        },
        "method_short":"BT-Adapter",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-09-27",
        "metrics":{
            "text-to-video R@1":"35.6",
            "text-to-video R@5":"61.9",
            "text-to-video R@10":"72.6",
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null
        },
        "raw_metrics":{
            "text-to-video R@1":35.6,
            "text-to-video R@5":61.9,
            "text-to-video R@10":72.6,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1289477,
            "title":"One For All: Video Conversation is Feasible Without Video Instruction Tuning",
            "url":"\/paper\/one-for-all-video-conversation-is-feasible",
            "published":"2023-09-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/one-for-all-video-conversation-is-feasible\/review\/?hl=109689"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20341,
        "row_id":69052,
        "rank":10,
        "method":"OmniVL",
        "mlmodel":{

        },
        "method_short":"OmniVL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-15",
        "metrics":{
            "text-to-video R@1":"33.3",
            "text-to-video R@5":"58.7",
            "text-to-video R@10":"68.5",
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null
        },
        "raw_metrics":{
            "text-to-video R@1":33.3,
            "text-to-video R@5":58.7,
            "text-to-video R@10":68.5,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1075042,
            "title":"OmniVL:One Foundation Model for Image-Language and Video-Language Tasks",
            "url":"\/paper\/omnivl-one-foundation-model-for-image",
            "published":"2022-09-15T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/omnivl-one-foundation-model-for-image\/review\/?hl=69052"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20341,
        "row_id":86737,
        "rank":11,
        "method":"InternVideo",
        "mlmodel":{

        },
        "method_short":"InternVideo",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-06",
        "metrics":{
            "text-to-video R@1":"31.5",
            "text-to-video R@5":"57.6",
            "text-to-video R@10":"68.2",
            "text-to-video Median Rank":null,
            "video-to-text R@1":"33.5",
            "video-to-text R@5":"60.3",
            "video-to-text R@10":"71.1"
        },
        "raw_metrics":{
            "text-to-video R@1":31.5,
            "text-to-video R@5":57.6,
            "text-to-video R@10":68.2,
            "text-to-video Median Rank":null,
            "video-to-text R@1":33.5,
            "video-to-text R@5":60.3,
            "video-to-text R@10":71.1
        },
        "uses_additional_data":true,
        "paper":{
            "id":1124231,
            "title":"InternVideo: General Video Foundation Models via Generative and Discriminative Learning",
            "url":"\/paper\/internvideo-general-video-foundation-models",
            "published":"2022-12-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/internvideo-general-video-foundation-models\/review\/?hl=86737"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20341,
        "row_id":88013,
        "rank":12,
        "method":"Clover",
        "mlmodel":{

        },
        "method_short":"Clover",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-16",
        "metrics":{
            "text-to-video R@1":"29.5",
            "text-to-video R@5":"55.2",
            "text-to-video R@10":"66.3",
            "text-to-video Median Rank":"4",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null
        },
        "raw_metrics":{
            "text-to-video R@1":29.5,
            "text-to-video R@5":55.2,
            "text-to-video R@10":66.3,
            "text-to-video Median Rank":4.0,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1045027,
            "title":"Clover: Towards A Unified Video-Language Alignment and Fusion Model",
            "url":"\/paper\/clover-towards-a-unified-video-language",
            "published":"2022-07-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/clover-towards-a-unified-video-language\/review\/?hl=88013"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20341,
        "row_id":102866,
        "rank":13,
        "method":"Y. Ge et. al.",
        "mlmodel":{

        },
        "method_short":"Y. Ge et. al.",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-13",
        "metrics":{
            "text-to-video R@1":"25.6",
            "text-to-video R@5":"50.6",
            "text-to-video R@10":"61.1",
            "text-to-video Median Rank":"5.0",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null
        },
        "raw_metrics":{
            "text-to-video R@1":25.6,
            "text-to-video R@5":50.6,
            "text-to-video R@10":61.1,
            "text-to-video Median Rank":5.0,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":944982,
            "title":"Bridging Video-text Retrieval with Multiple Choice Questions",
            "url":"\/paper\/bridgeformer-bridging-video-text-retrieval",
            "published":"2022-01-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/bridgeformer-bridging-video-text-retrieval\/review\/?hl=102866"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20341,
        "row_id":102877,
        "rank":14,
        "method":"ALPRO",
        "mlmodel":{

        },
        "method_short":"ALPRO",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-17",
        "metrics":{
            "text-to-video R@1":"23.8",
            "text-to-video R@5":"47.3",
            "text-to-video R@10":"57.9",
            "text-to-video Median Rank":"6",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null
        },
        "raw_metrics":{
            "text-to-video R@1":23.8,
            "text-to-video R@5":47.3,
            "text-to-video R@10":57.9,
            "text-to-video Median Rank":6.0,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":932382,
            "title":"Align and Prompt: Video-and-Language Pre-training with Entity Prompts",
            "url":"\/paper\/align-and-prompt-video-and-language-pre",
            "published":"2021-12-17T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20341,
        "row_id":69063,
        "rank":15,
        "method":"FROZEN",
        "mlmodel":{

        },
        "method_short":"FROZEN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-01",
        "metrics":{
            "text-to-video R@1":"21.1",
            "text-to-video R@5":"46.0",
            "text-to-video R@10":"56.2",
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null
        },
        "raw_metrics":{
            "text-to-video R@1":21.1,
            "text-to-video R@5":46.0,
            "text-to-video R@10":56.2,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":773452,
            "title":"Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval",
            "url":"\/paper\/frozen-in-time-a-joint-video-and-image",
            "published":"2021-04-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/frozen-in-time-a-joint-video-and-image\/review\/?hl=69063"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20341,
        "row_id":102874,
        "rank":16,
        "method":"M. Bain et. al.",
        "mlmodel":{

        },
        "method_short":"M. Bain et. al.",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-01",
        "metrics":{
            "text-to-video R@1":"20.2",
            "text-to-video R@5":"46.4",
            "text-to-video R@10":"58.5",
            "text-to-video Median Rank":"7",
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null
        },
        "raw_metrics":{
            "text-to-video R@1":20.2,
            "text-to-video R@5":46.4,
            "text-to-video R@10":58.5,
            "text-to-video Median Rank":7.0,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":773452,
            "title":"Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval",
            "url":"\/paper\/frozen-in-time-a-joint-video-and-image",
            "published":"2021-04-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/frozen-in-time-a-joint-video-and-image\/review\/?hl=102874"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":20341,
        "row_id":102871,
        "rank":17,
        "method":"VideoCLIP",
        "mlmodel":{

        },
        "method_short":"VideoCLIP",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-28",
        "metrics":{
            "text-to-video R@1":"16.6",
            "text-to-video R@5":"46.9",
            "text-to-video R@10":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null
        },
        "raw_metrics":{
            "text-to-video R@1":16.6,
            "text-to-video R@5":46.9,
            "text-to-video R@10":null,
            "text-to-video Median Rank":null,
            "video-to-text R@1":null,
            "video-to-text R@5":null,
            "video-to-text R@10":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":875851,
            "title":"VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding",
            "url":"\/paper\/videoclip-contrastive-pre-training-for-zero",
            "published":"2021-09-28T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]