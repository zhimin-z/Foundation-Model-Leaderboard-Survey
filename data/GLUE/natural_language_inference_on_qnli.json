[
    {
        "table_id":962,
        "row_id":7987,
        "rank":1,
        "method":"ALBERT",
        "mlmodel":{

        },
        "method_short":"ALBERT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-09-26",
        "metrics":{
            "Accuracy":"99.2%"
        },
        "raw_metrics":{
            "Accuracy":99.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":156146,
            "title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
            "url":"\/paper\/albert-a-lite-bert-for-self-supervised",
            "published":"2019-09-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/albert-a-lite-bert-for-self-supervised\/review\/?hl=7987"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":30639,
        "rank":2,
        "method":"StructBERTRoBERTa ensemble",
        "mlmodel":{

        },
        "method_short":"StructBERTRoBERTa ensemble",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-13",
        "metrics":{
            "Accuracy":"99.2%"
        },
        "raw_metrics":{
            "Accuracy":99.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":149844,
            "title":"StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding",
            "url":"\/paper\/structbert-incorporating-language-structures",
            "published":"2019-08-13T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/structbert-incorporating-language-structures\/review\/?hl=30639"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":86562,
        "rank":3,
        "method":"ALICE",
        "mlmodel":{

        },
        "method_short":"ALICE",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-08",
        "metrics":{
            "Accuracy":"99.2%"
        },
        "raw_metrics":{
            "Accuracy":99.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":169701,
            "title":"SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization",
            "url":"\/paper\/smart-robust-and-efficient-fine-tuning-for",
            "published":"2019-11-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/smart-robust-and-efficient-fine-tuning-for\/review\/?hl=86562"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":86567,
        "rank":4,
        "method":"MT-DNN-SMART",
        "mlmodel":{

        },
        "method_short":"MT-DNN-SMART",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-08",
        "metrics":{
            "Accuracy":"99.2%"
        },
        "raw_metrics":{
            "Accuracy":99.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":169701,
            "title":"SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization",
            "url":"\/paper\/smart-robust-and-efficient-fine-tuning-for",
            "published":"2019-11-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/smart-robust-and-efficient-fine-tuning-for\/review\/?hl=86567"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":6020,
        "rank":5,
        "method":"RoBERTa",
        "mlmodel":{

        },
        "method_short":"RoBERTa",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-07-26",
        "metrics":{
            "Accuracy":"98.9%"
        },
        "raw_metrics":{
            "Accuracy":98.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":148282,
            "title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "url":"\/paper\/roberta-a-robustly-optimized-bert-pretraining",
            "published":"2019-07-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/roberta-a-robustly-optimized-bert-pretraining\/review\/?hl=6020"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":8352,
        "rank":6,
        "method":"T5-11B",
        "mlmodel":{

        },
        "method_short":"T5-11B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-10-23",
        "metrics":{
            "Accuracy":"96.7%"
        },
        "raw_metrics":{
            "Accuracy":96.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":166345,
            "title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "url":"\/paper\/exploring-the-limits-of-transfer-learning",
            "published":"2019-10-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/exploring-the-limits-of-transfer-learning\/review\/?hl=8352"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":8353,
        "rank":7,
        "method":"T5-3B",
        "mlmodel":{

        },
        "method_short":"T5-3B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-10-23",
        "metrics":{
            "Accuracy":"96.3%"
        },
        "raw_metrics":{
            "Accuracy":96.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":166345,
            "title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "url":"\/paper\/exploring-the-limits-of-transfer-learning",
            "published":"2019-10-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/exploring-the-limits-of-transfer-learning\/review\/?hl=8353"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":60699,
        "rank":8,
        "method":"DeBERTaV3large",
        "mlmodel":{

        },
        "method_short":"DeBERTaV3large",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-18",
        "metrics":{
            "Accuracy":"96%"
        },
        "raw_metrics":{
            "Accuracy":96.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":912443,
            "title":"DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing",
            "url":"\/paper\/debertav3-improving-deberta-using-electra",
            "published":"2021-11-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/debertav3-improving-deberta-using-electra\/review\/?hl=60699"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":7932,
        "rank":9,
        "method":"ELECTRA",
        "mlmodel":{

        },
        "method_short":"ELECTRA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "Accuracy":"95.4%"
        },
        "raw_metrics":{
            "Accuracy":95.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":24166,
        "rank":10,
        "method":"DeBERTa (large)",
        "mlmodel":{

        },
        "method_short":"DeBERTa ",
        "method_details":"large",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-05",
        "metrics":{
            "Accuracy":"95.3%"
        },
        "raw_metrics":{
            "Accuracy":95.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":201217,
            "title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention",
            "url":"\/paper\/deberta-decoding-enhanced-bert-with",
            "published":"2020-06-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deberta-decoding-enhanced-bert-with\/review\/?hl=24166"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":12716,
        "rank":11,
        "method":"XLNet (single model)",
        "mlmodel":{

        },
        "method_short":"XLNet ",
        "method_details":"single model",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-19",
        "metrics":{
            "Accuracy":"94.9%"
        },
        "raw_metrics":{
            "Accuracy":94.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":143172,
            "title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding",
            "url":"\/paper\/xlnet-generalized-autoregressive-pretraining",
            "published":"2019-06-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/xlnet-generalized-autoregressive-pretraining\/review\/?hl=12716"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":8354,
        "rank":12,
        "method":"T5-Large",
        "mlmodel":{

        },
        "method_short":"T5-Large",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-10-23",
        "metrics":{
            "Accuracy":"94.8%"
        },
        "raw_metrics":{
            "Accuracy":94.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":166345,
            "title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "url":"\/paper\/exploring-the-limits-of-transfer-learning",
            "published":"2019-10-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/exploring-the-limits-of-transfer-learning\/review\/?hl=8354"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":63589,
        "rank":13,
        "method":"Vector-wise",
        "mlmodel":{

        },
        "method_short":"Vector-wise",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-08-15",
        "metrics":{
            "Accuracy":"94.7%"
        },
        "raw_metrics":{
            "Accuracy":94.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":1058964,
            "title":"LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale",
            "url":"\/paper\/llm-int8-8-bit-matrix-multiplication-for",
            "published":"2022-08-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/llm-int8-8-bit-matrix-multiplication-for\/review\/?hl=63589"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":6079,
        "rank":14,
        "method":"ERNIE 2.0 Large",
        "mlmodel":{

        },
        "method_short":"ERNIE 2.0 Large",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-07-29",
        "metrics":{
            "Accuracy":"94.6%"
        },
        "raw_metrics":{
            "Accuracy":94.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":148407,
            "title":"ERNIE 2.0: A Continual Pre-training Framework for Language Understanding",
            "url":"\/paper\/ernie-20-a-continual-pre-training-framework",
            "published":"2019-07-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ernie-20-a-continual-pre-training-framework\/review\/?hl=6079"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":31487,
        "rank":15,
        "method":"EFL",
        "mlmodel":{

        },
        "method_short":"EFL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-29",
        "metrics":{
            "Accuracy":"94.5%"
        },
        "raw_metrics":{
            "Accuracy":94.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":791525,
            "title":"Entailment as Few-Shot Learner",
            "url":"\/paper\/entailment-as-few-shot-learner",
            "published":"2021-04-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/entailment-as-few-shot-learner\/review\/?hl=31487"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":7933,
        "rank":16,
        "method":"SpanBERT",
        "mlmodel":{

        },
        "method_short":"SpanBERT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-07-24",
        "metrics":{
            "Accuracy":"94.3%"
        },
        "raw_metrics":{
            "Accuracy":94.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":146696,
            "title":"SpanBERT: Improving Pre-training by Representing and Predicting Spans",
            "url":"\/paper\/spanbert-improving-pre-training-by",
            "published":"2019-07-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/spanbert-improving-pre-training-by\/review\/?hl=7933"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":54749,
        "rank":17,
        "method":"TRANS-BLSTM",
        "mlmodel":{

        },
        "method_short":"TRANS-BLSTM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-16",
        "metrics":{
            "Accuracy":"94.08%"
        },
        "raw_metrics":{
            "Accuracy":94.08
        },
        "uses_additional_data":false,
        "paper":{
            "id":187073,
            "title":"TRANS-BLSTM: Transformer with Bidirectional LSTM for Language Understanding",
            "url":"\/paper\/trans-blstm-transformer-with-bidirectional",
            "published":"2020-03-16T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/trans-blstm-transformer-with-bidirectional\/review\/?hl=54749"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":8355,
        "rank":18,
        "method":"T5-Base",
        "mlmodel":{

        },
        "method_short":"T5-Base",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-10-23",
        "metrics":{
            "Accuracy":"93.7%"
        },
        "raw_metrics":{
            "Accuracy":93.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":166345,
            "title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "url":"\/paper\/exploring-the-limits-of-transfer-learning",
            "published":"2019-10-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/exploring-the-limits-of-transfer-learning\/review\/?hl=8355"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":98896,
        "rank":19,
        "method":"ASA + RoBERTa",
        "mlmodel":{

        },
        "method_short":"ASA + RoBERTa",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-25",
        "metrics":{
            "Accuracy":"93.6%"
        },
        "raw_metrics":{
            "Accuracy":93.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":1033489,
            "title":"Adversarial Self-Attention for Language Understanding",
            "url":"\/paper\/adversarial-self-attention-for-language",
            "published":"2022-06-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/adversarial-self-attention-for-language\/review\/?hl=98896"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":24197,
        "rank":20,
        "method":"MLM+ subs+ del-span",
        "mlmodel":{

        },
        "method_short":"MLM+ subs+ del-span",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-31",
        "metrics":{
            "Accuracy":"93.4%"
        },
        "raw_metrics":{
            "Accuracy":93.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":732544,
            "title":"CLEAR: Contrastive Learning for Sentence Representation",
            "url":"\/paper\/clear-contrastive-learning-for-sentence",
            "published":"2020-12-31T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/clear-contrastive-learning-for-sentence\/review\/?hl=24197"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":6078,
        "rank":21,
        "method":"ERNIE 2.0 Base",
        "mlmodel":{

        },
        "method_short":"ERNIE 2.0 Base",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-07-29",
        "metrics":{
            "Accuracy":"92.9%"
        },
        "raw_metrics":{
            "Accuracy":92.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":148407,
            "title":"ERNIE 2.0: A Continual Pre-training Framework for Language Understanding",
            "url":"\/paper\/ernie-20-a-continual-pre-training-framework",
            "published":"2019-07-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ernie-20-a-continual-pre-training-framework\/review\/?hl=6078"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":86505,
        "rank":22,
        "method":"BERT-LARGE",
        "mlmodel":{

        },
        "method_short":"BERT-LARGE",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-10-11",
        "metrics":{
            "Accuracy":"92.7%"
        },
        "raw_metrics":{
            "Accuracy":92.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":59204,
            "title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "url":"\/paper\/bert-pre-training-of-deep-bidirectional",
            "published":"2018-10-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/bert-pre-training-of-deep-bidirectional\/review\/?hl=86505"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":18890,
        "rank":23,
        "method":"BigBird",
        "mlmodel":{

        },
        "method_short":"BigBird",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-07-28",
        "metrics":{
            "Accuracy":"92.2%"
        },
        "raw_metrics":{
            "Accuracy":92.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":210706,
            "title":"Big Bird: Transformers for Longer Sequences",
            "url":"\/paper\/big-bird-transformers-for-longer-sequences",
            "published":"2020-07-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/big-bird-transformers-for-longer-sequences\/review\/?hl=18890"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":30625,
        "rank":24,
        "method":"RealFormer",
        "mlmodel":{

        },
        "method_short":"RealFormer",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-21",
        "metrics":{
            "Accuracy":"91.89%"
        },
        "raw_metrics":{
            "Accuracy":91.89
        },
        "uses_additional_data":false,
        "paper":{
            "id":730853,
            "title":"RealFormer: Transformer Likes Residual Attention",
            "url":"\/paper\/informer-transformer-likes-informed-attention",
            "published":"2020-12-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/informer-transformer-likes-informed-attention\/review\/?hl=30625"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":98889,
        "rank":25,
        "method":"ASA + BERT-base",
        "mlmodel":{

        },
        "method_short":"ASA + BERT-base",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-25",
        "metrics":{
            "Accuracy":"91.4%"
        },
        "raw_metrics":{
            "Accuracy":91.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":1033489,
            "title":"Adversarial Self-Attention for Language Understanding",
            "url":"\/paper\/adversarial-self-attention-for-language",
            "published":"2022-06-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/adversarial-self-attention-for-language\/review\/?hl=98889"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":12717,
        "rank":26,
        "method":"ERNIE",
        "mlmodel":{

        },
        "method_short":"ERNIE",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-05-17",
        "metrics":{
            "Accuracy":"91.3%"
        },
        "raw_metrics":{
            "Accuracy":91.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":114976,
            "title":"ERNIE: Enhanced Language Representation with Informative Entities",
            "url":"\/paper\/ernie-enhanced-language-representation-with",
            "published":"2019-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ernie-enhanced-language-representation-with\/review\/?hl=12717"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":52591,
        "rank":27,
        "method":"data2vec",
        "mlmodel":{

        },
        "method_short":"data2vec",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-07",
        "metrics":{
            "Accuracy":"91.1%"
        },
        "raw_metrics":{
            "Accuracy":91.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":957898,
            "title":"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",
            "url":"\/paper\/data2vec-a-general-framework-for-self-1",
            "published":"2022-02-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/data2vec-a-general-framework-for-self-1\/review\/?hl=52591"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":36315,
        "rank":28,
        "method":"Charformer-Tall",
        "mlmodel":{

        },
        "method_short":"Charformer-Tall",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-23",
        "metrics":{
            "Accuracy":"91.0%"
        },
        "raw_metrics":{
            "Accuracy":91.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":824266,
            "title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization",
            "url":"\/paper\/charformer-fast-character-transformers-via",
            "published":"2021-06-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/charformer-fast-character-transformers-via\/review\/?hl=36315"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":30346,
        "rank":29,
        "method":"24hBERT",
        "mlmodel":{

        },
        "method_short":"24hBERT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-15",
        "metrics":{
            "Accuracy":"90.6"
        },
        "raw_metrics":{
            "Accuracy":90.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":783560,
            "title":"How to Train BERT with an Academic Budget",
            "url":"\/paper\/how-to-train-bert-with-an-academic-budget",
            "published":"2021-04-15T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":8356,
        "rank":30,
        "method":"T5-Small",
        "mlmodel":{

        },
        "method_short":"T5-Small",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-10-23",
        "metrics":{
            "Accuracy":"90.3%"
        },
        "raw_metrics":{
            "Accuracy":90.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":166345,
            "title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "url":"\/paper\/exploring-the-limits-of-transfer-learning",
            "published":"2019-10-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/exploring-the-limits-of-transfer-learning\/review\/?hl=8356"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":8114,
        "rank":31,
        "method":"DistilBERT",
        "mlmodel":{

        },
        "method_short":"DistilBERT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-10-02",
        "metrics":{
            "Accuracy":"90.2%"
        },
        "raw_metrics":{
            "Accuracy":90.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":156821,
            "title":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
            "url":"\/paper\/distilbert-a-distilled-version-of-bert",
            "published":"2019-10-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/distilbert-a-distilled-version-of-bert\/review\/?hl=8114"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":71975,
        "rank":32,
        "method":"SqueezeBERT",
        "mlmodel":{

        },
        "method_short":"SqueezeBERT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-19",
        "metrics":{
            "Accuracy":"90.1%"
        },
        "raw_metrics":{
            "Accuracy":90.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":205166,
            "title":"SqueezeBERT: What can computer vision teach NLP about efficient neural networks?",
            "url":"\/paper\/squeezebert-what-can-computer-vision-teach",
            "published":"2020-06-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/squeezebert-what-can-computer-vision-teach\/review\/?hl=71975"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":25280,
        "rank":33,
        "method":"Nystr\u00f6mformer",
        "mlmodel":{

        },
        "method_short":"Nystr\u00f6mformer",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-07",
        "metrics":{
            "Accuracy":"88.7%"
        },
        "raw_metrics":{
            "Accuracy":88.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":743243,
            "title":"Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention",
            "url":"\/paper\/nystromformer-a-nystrom-based-algorithm-for",
            "published":"2021-02-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/nystromformer-a-nystrom-based-algorithm-for\/review\/?hl=25280"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":12718,
        "rank":34,
        "method":"TinyBERT",
        "mlmodel":{

        },
        "method_short":"TinyBERT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-09-23",
        "metrics":{
            "Accuracy":"87.7%"
        },
        "raw_metrics":{
            "Accuracy":87.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":154692,
            "title":"TinyBERT: Distilling BERT for Natural Language Understanding",
            "url":"\/paper\/190910351",
            "published":"2019-09-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/190910351\/review\/?hl=12718"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":33085,
        "rank":35,
        "method":"FNet-Large",
        "mlmodel":{

        },
        "method_short":"FNet-Large",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-09",
        "metrics":{
            "Accuracy":"85%"
        },
        "raw_metrics":{
            "Accuracy":85.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":796253,
            "title":"FNet: Mixing Tokens with Fourier Transforms",
            "url":"\/paper\/fnet-mixing-tokens-with-fourier-transforms",
            "published":"2021-05-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fnet-mixing-tokens-with-fourier-transforms\/review\/?hl=33085"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":962,
        "row_id":104268,
        "rank":36,
        "method":"LM-CPPF RoBERTa-base",
        "mlmodel":{

        },
        "method_short":"LM-CPPF RoBERTa-base",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-29",
        "metrics":{
            "Accuracy":"70.2%"
        },
        "raw_metrics":{
            "Accuracy":70.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":1218461,
            "title":"LM-CPPF: Paraphrasing-Guided Data Augmentation for Contrastive Prompt-Based Few-Shot Fine-Tuning",
            "url":"\/paper\/lm-cppf-paraphrasing-guided-data-augmentation",
            "published":"2023-05-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lm-cppf-paraphrasing-guided-data-augmentation\/review\/?hl=104268"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]