[
    {
        "Model":"Palmyra X (43B)",
        "Mean win rate":0.971,
        "MMLU - EM":0.609,
        "BoolQ - EM":0.896,
        "NarrativeQA - F1":"0.742",
        "NaturalQuestions (closed-book) - F1":0.413,
        "NaturalQuestions (open-book) - F1":null,
        "QuAC - F1":"0.473",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.616
    },
    {
        "Model":"text-davinci-003",
        "Mean win rate":0.957,
        "MMLU - EM":0.569,
        "BoolQ - EM":0.881,
        "NarrativeQA - F1":"0.727",
        "NaturalQuestions (closed-book) - F1":0.406,
        "NaturalQuestions (open-book) - F1":"0.77",
        "QuAC - F1":"0.525",
        "HellaSwag - EM":"0.822",
        "OpenbookQA - EM":"0.646",
        "TruthfulQA - EM":0.593
    },
    {
        "Model":"Llama 2 (70B)",
        "Mean win rate":0.953,
        "MMLU - EM":0.582,
        "BoolQ - EM":0.886,
        "NarrativeQA - F1":"0.77",
        "NaturalQuestions (closed-book) - F1":0.458,
        "NaturalQuestions (open-book) - F1":"0.674",
        "QuAC - F1":"0.484",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.554
    },
    {
        "Model":"text-davinci-002",
        "Mean win rate":0.929,
        "MMLU - EM":0.568,
        "BoolQ - EM":0.877,
        "NarrativeQA - F1":"0.727",
        "NaturalQuestions (closed-book) - F1":0.383,
        "NaturalQuestions (open-book) - F1":"0.713",
        "QuAC - F1":"0.445",
        "HellaSwag - EM":"0.815",
        "OpenbookQA - EM":"0.594",
        "TruthfulQA - EM":0.61
    },
    {
        "Model":"LLaMA (65B)",
        "Mean win rate":0.913,
        "MMLU - EM":0.584,
        "BoolQ - EM":0.871,
        "NarrativeQA - F1":"0.755",
        "NaturalQuestions (closed-book) - F1":0.431,
        "NaturalQuestions (open-book) - F1":"0.672",
        "QuAC - F1":"0.401",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.508
    },
    {
        "Model":"Cohere Command beta (52.4B)",
        "Mean win rate":0.858,
        "MMLU - EM":0.452,
        "BoolQ - EM":0.856,
        "NarrativeQA - F1":"0.752",
        "NaturalQuestions (closed-book) - F1":0.372,
        "NaturalQuestions (open-book) - F1":"0.76",
        "QuAC - F1":"0.432",
        "HellaSwag - EM":"0.811",
        "OpenbookQA - EM":"0.582",
        "TruthfulQA - EM":0.269
    },
    {
        "Model":"LLaMA (30B)",
        "Mean win rate":0.858,
        "MMLU - EM":0.531,
        "BoolQ - EM":0.861,
        "NarrativeQA - F1":"0.752",
        "NaturalQuestions (closed-book) - F1":0.408,
        "NaturalQuestions (open-book) - F1":"0.666",
        "QuAC - F1":"0.39",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.344
    },
    {
        "Model":"Jurassic-2 Jumbo (178B)",
        "Mean win rate":0.837,
        "MMLU - EM":0.48,
        "BoolQ - EM":0.829,
        "NarrativeQA - F1":"0.733",
        "NaturalQuestions (closed-book) - F1":0.385,
        "NaturalQuestions (open-book) - F1":"0.669",
        "QuAC - F1":"0.435",
        "HellaSwag - EM":"0.788",
        "OpenbookQA - EM":"0.558",
        "TruthfulQA - EM":0.437
    },
    {
        "Model":"Llama 2 (13B)",
        "Mean win rate":0.814,
        "MMLU - EM":0.507,
        "BoolQ - EM":0.811,
        "NarrativeQA - F1":"0.744",
        "NaturalQuestions (closed-book) - F1":0.376,
        "NaturalQuestions (open-book) - F1":"0.637",
        "QuAC - F1":"0.424",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.33
    },
    {
        "Model":"Anthropic-LM v4-s3 (52B)",
        "Mean win rate":0.804,
        "MMLU - EM":0.481,
        "BoolQ - EM":0.815,
        "NarrativeQA - F1":"0.728",
        "NaturalQuestions (closed-book) - F1":0.288,
        "NaturalQuestions (open-book) - F1":"0.686",
        "QuAC - F1":"0.431",
        "HellaSwag - EM":"0.807",
        "OpenbookQA - EM":"0.558",
        "TruthfulQA - EM":0.368
    },
    {
        "Model":"Vicuna v1.3 (13B)",
        "Mean win rate":0.789,
        "MMLU - EM":0.462,
        "BoolQ - EM":0.808,
        "NarrativeQA - F1":"0.691",
        "NaturalQuestions (closed-book) - F1":0.346,
        "NaturalQuestions (open-book) - F1":"0.686",
        "QuAC - F1":"0.403",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.385
    },
    {
        "Model":"gpt-3.5-turbo-0301",
        "Mean win rate":0.775,
        "MMLU - EM":0.59,
        "BoolQ - EM":0.74,
        "NarrativeQA - F1":"0.663",
        "NaturalQuestions (closed-book) - F1":0.39,
        "NaturalQuestions (open-book) - F1":"0.624",
        "QuAC - F1":"0.512",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.609
    },
    {
        "Model":"Jurassic-2 Grande (17B)",
        "Mean win rate":0.773,
        "MMLU - EM":0.475,
        "BoolQ - EM":0.826,
        "NarrativeQA - F1":"0.737",
        "NaturalQuestions (closed-book) - F1":0.356,
        "NaturalQuestions (open-book) - F1":"0.639",
        "QuAC - F1":"0.418",
        "HellaSwag - EM":"0.781",
        "OpenbookQA - EM":"0.542",
        "TruthfulQA - EM":0.348
    },
    {
        "Model":"TNLG v2 (530B)",
        "Mean win rate":0.764,
        "MMLU - EM":0.469,
        "BoolQ - EM":0.809,
        "NarrativeQA - F1":"0.722",
        "NaturalQuestions (closed-book) - F1":0.384,
        "NaturalQuestions (open-book) - F1":"0.642",
        "QuAC - F1":"0.39",
        "HellaSwag - EM":"0.799",
        "OpenbookQA - EM":"0.562",
        "TruthfulQA - EM":0.251
    },
    {
        "Model":"gpt-3.5-turbo-0613",
        "Mean win rate":0.761,
        "MMLU - EM":0.391,
        "BoolQ - EM":0.87,
        "NarrativeQA - F1":"0.625",
        "NaturalQuestions (closed-book) - F1":0.348,
        "NaturalQuestions (open-book) - F1":"0.675",
        "QuAC - F1":"0.485",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.339
    },
    {
        "Model":"Falcon-Instruct (40B)",
        "Mean win rate":0.751,
        "MMLU - EM":0.497,
        "BoolQ - EM":0.829,
        "NarrativeQA - F1":"0.625",
        "NaturalQuestions (closed-book) - F1":0.377,
        "NaturalQuestions (open-book) - F1":"0.666",
        "QuAC - F1":"0.371",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.384
    },
    {
        "Model":"Falcon (40B)",
        "Mean win rate":0.744,
        "MMLU - EM":0.509,
        "BoolQ - EM":0.819,
        "NarrativeQA - F1":"0.673",
        "NaturalQuestions (closed-book) - F1":0.392,
        "NaturalQuestions (open-book) - F1":"0.675",
        "QuAC - F1":"0.307",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.353
    },
    {
        "Model":"J1-Grande v2 beta (17B)",
        "Mean win rate":0.723,
        "MMLU - EM":0.445,
        "BoolQ - EM":0.812,
        "NarrativeQA - F1":"0.725",
        "NaturalQuestions (closed-book) - F1":0.337,
        "NaturalQuestions (open-book) - F1":"0.625",
        "QuAC - F1":"0.392",
        "HellaSwag - EM":"0.764",
        "OpenbookQA - EM":"0.56",
        "TruthfulQA - EM":0.306
    },
    {
        "Model":"MPT-Instruct (30B)",
        "Mean win rate":0.706,
        "MMLU - EM":0.444,
        "BoolQ - EM":0.85,
        "NarrativeQA - F1":"0.733",
        "NaturalQuestions (closed-book) - F1":0.304,
        "NaturalQuestions (open-book) - F1":"0.697",
        "QuAC - F1":"0.327",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.234
    },
    {
        "Model":"MPT (30B)",
        "Mean win rate":0.67,
        "MMLU - EM":0.437,
        "BoolQ - EM":0.704,
        "NarrativeQA - F1":"0.732",
        "NaturalQuestions (closed-book) - F1":0.347,
        "NaturalQuestions (open-book) - F1":"0.673",
        "QuAC - F1":"0.393",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.231
    },
    {
        "Model":"Llama 2 (7B)",
        "Mean win rate":0.67,
        "MMLU - EM":0.431,
        "BoolQ - EM":0.762,
        "NarrativeQA - F1":"0.691",
        "NaturalQuestions (closed-book) - F1":0.337,
        "NaturalQuestions (open-book) - F1":"0.611",
        "QuAC - F1":"0.406",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.272
    },
    {
        "Model":"Cohere xlarge v20221108 (52.4B)",
        "Mean win rate":0.636,
        "MMLU - EM":0.382,
        "BoolQ - EM":0.762,
        "NarrativeQA - F1":"0.672",
        "NaturalQuestions (closed-book) - F1":0.361,
        "NaturalQuestions (open-book) - F1":"0.628",
        "QuAC - F1":"0.374",
        "HellaSwag - EM":"0.81",
        "OpenbookQA - EM":"0.588",
        "TruthfulQA - EM":0.169
    },
    {
        "Model":"OPT (175B)",
        "Mean win rate":0.633,
        "MMLU - EM":0.318,
        "BoolQ - EM":0.793,
        "NarrativeQA - F1":"0.671",
        "NaturalQuestions (closed-book) - F1":0.297,
        "NaturalQuestions (open-book) - F1":"0.615",
        "QuAC - F1":"0.36",
        "HellaSwag - EM":"0.791",
        "OpenbookQA - EM":"0.586",
        "TruthfulQA - EM":0.25
    },
    {
        "Model":"Cohere Command beta (6.1B)",
        "Mean win rate":0.629,
        "MMLU - EM":0.406,
        "BoolQ - EM":0.798,
        "NarrativeQA - F1":"0.709",
        "NaturalQuestions (closed-book) - F1":0.229,
        "NaturalQuestions (open-book) - F1":"0.717",
        "QuAC - F1":"0.375",
        "HellaSwag - EM":"0.752",
        "OpenbookQA - EM":"0.55",
        "TruthfulQA - EM":0.203
    },
    {
        "Model":"Vicuna v1.3 (7B)",
        "Mean win rate":0.627,
        "MMLU - EM":0.434,
        "BoolQ - EM":0.76,
        "NarrativeQA - F1":"0.643",
        "NaturalQuestions (closed-book) - F1":0.287,
        "NaturalQuestions (open-book) - F1":"0.634",
        "QuAC - F1":"0.392",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.292
    },
    {
        "Model":"Luminous Supreme (70B)",
        "Mean win rate":0.621,
        "MMLU - EM":0.38,
        "BoolQ - EM":0.775,
        "NarrativeQA - F1":"0.711",
        "NaturalQuestions (closed-book) - F1":0.293,
        "NaturalQuestions (open-book) - F1":"0.649",
        "QuAC - F1":"0.37",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.222
    },
    {
        "Model":"LLaMA (13B)",
        "Mean win rate":0.611,
        "MMLU - EM":0.422,
        "BoolQ - EM":0.714,
        "NarrativeQA - F1":"0.711",
        "NaturalQuestions (closed-book) - F1":0.346,
        "NaturalQuestions (open-book) - F1":"0.614",
        "QuAC - F1":"0.347",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.324
    },
    {
        "Model":"davinci (175B)",
        "Mean win rate":0.597,
        "MMLU - EM":0.422,
        "BoolQ - EM":0.722,
        "NarrativeQA - F1":"0.687",
        "NaturalQuestions (closed-book) - F1":0.329,
        "NaturalQuestions (open-book) - F1":"0.625",
        "QuAC - F1":"0.36",
        "HellaSwag - EM":"0.775",
        "OpenbookQA - EM":"0.586",
        "TruthfulQA - EM":0.194
    },
    {
        "Model":"InstructPalmyra (30B)",
        "Mean win rate":0.55,
        "MMLU - EM":0.403,
        "BoolQ - EM":0.751,
        "NarrativeQA - F1":"0.496",
        "NaturalQuestions (closed-book) - F1":0.33,
        "NaturalQuestions (open-book) - F1":"0.682",
        "QuAC - F1":"0.433",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.185
    },
    {
        "Model":"Cohere xlarge v20220609 (52.4B)",
        "Mean win rate":0.538,
        "MMLU - EM":0.353,
        "BoolQ - EM":0.718,
        "NarrativeQA - F1":"0.65",
        "NaturalQuestions (closed-book) - F1":0.312,
        "NaturalQuestions (open-book) - F1":"0.595",
        "QuAC - F1":"0.361",
        "HellaSwag - EM":"0.811",
        "OpenbookQA - EM":"0.55",
        "TruthfulQA - EM":0.198
    },
    {
        "Model":"LLaMA (7B)",
        "Mean win rate":0.53,
        "MMLU - EM":0.321,
        "BoolQ - EM":0.756,
        "NarrativeQA - F1":"0.669",
        "NaturalQuestions (closed-book) - F1":0.297,
        "NaturalQuestions (open-book) - F1":"0.589",
        "QuAC - F1":"0.338",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.28
    },
    {
        "Model":"Luminous Extended (30B)",
        "Mean win rate":0.508,
        "MMLU - EM":0.321,
        "BoolQ - EM":0.767,
        "NarrativeQA - F1":"0.665",
        "NaturalQuestions (closed-book) - F1":0.254,
        "NaturalQuestions (open-book) - F1":"0.609",
        "QuAC - F1":"0.349",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.221
    },
    {
        "Model":"GLM (130B)",
        "Mean win rate":0.478,
        "MMLU - EM":0.344,
        "BoolQ - EM":0.784,
        "NarrativeQA - F1":"0.706",
        "NaturalQuestions (closed-book) - F1":0.148,
        "NaturalQuestions (open-book) - F1":"0.642",
        "QuAC - F1":"0.272",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.218
    },
    {
        "Model":"J1-Jumbo v1 (178B)",
        "Mean win rate":0.476,
        "MMLU - EM":0.259,
        "BoolQ - EM":0.776,
        "NarrativeQA - F1":"0.695",
        "NaturalQuestions (closed-book) - F1":0.293,
        "NaturalQuestions (open-book) - F1":"0.595",
        "QuAC - F1":"0.358",
        "HellaSwag - EM":"0.765",
        "OpenbookQA - EM":"0.534",
        "TruthfulQA - EM":0.175
    },
    {
        "Model":"Jurassic-2 Large (7.5B)",
        "Mean win rate":0.473,
        "MMLU - EM":0.339,
        "BoolQ - EM":0.742,
        "NarrativeQA - F1":null,
        "NaturalQuestions (closed-book) - F1":0.274,
        "NaturalQuestions (open-book) - F1":"0.589",
        "QuAC - F1":null,
        "HellaSwag - EM":"0.729",
        "OpenbookQA - EM":"0.53",
        "TruthfulQA - EM":0.245
    },
    {
        "Model":"RedPajama-INCITE-Instruct (7B)",
        "Mean win rate":0.459,
        "MMLU - EM":0.363,
        "BoolQ - EM":0.705,
        "NarrativeQA - F1":"0.638",
        "NaturalQuestions (closed-book) - F1":0.232,
        "NaturalQuestions (open-book) - F1":"0.659",
        "QuAC - F1":"0.26",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.243
    },
    {
        "Model":"OPT (66B)",
        "Mean win rate":0.455,
        "MMLU - EM":0.276,
        "BoolQ - EM":0.76,
        "NarrativeQA - F1":"0.638",
        "NaturalQuestions (closed-book) - F1":0.258,
        "NaturalQuestions (open-book) - F1":"0.596",
        "QuAC - F1":"0.357",
        "HellaSwag - EM":"0.745",
        "OpenbookQA - EM":"0.534",
        "TruthfulQA - EM":0.201
    },
    {
        "Model":"BLOOM (176B)",
        "Mean win rate":0.451,
        "MMLU - EM":0.299,
        "BoolQ - EM":0.704,
        "NarrativeQA - F1":"0.662",
        "NaturalQuestions (closed-book) - F1":0.216,
        "NaturalQuestions (open-book) - F1":"0.621",
        "QuAC - F1":"0.361",
        "HellaSwag - EM":"0.744",
        "OpenbookQA - EM":"0.534",
        "TruthfulQA - EM":0.205
    },
    {
        "Model":"Falcon (7B)",
        "Mean win rate":0.434,
        "MMLU - EM":0.286,
        "BoolQ - EM":0.753,
        "NarrativeQA - F1":"0.621",
        "NaturalQuestions (closed-book) - F1":0.285,
        "NaturalQuestions (open-book) - F1":"0.579",
        "QuAC - F1":"0.332",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.234
    },
    {
        "Model":"Alpaca (7B)",
        "Mean win rate":0.427,
        "MMLU - EM":0.385,
        "BoolQ - EM":0.778,
        "NarrativeQA - F1":"0.396",
        "NaturalQuestions (closed-book) - F1":0.266,
        "NaturalQuestions (open-book) - F1":"0.592",
        "QuAC - F1":"0.27",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.243
    },
    {
        "Model":"J1-Grande v1 (17B)",
        "Mean win rate":0.408,
        "MMLU - EM":0.27,
        "BoolQ - EM":0.722,
        "NarrativeQA - F1":"0.672",
        "NaturalQuestions (closed-book) - F1":0.233,
        "NaturalQuestions (open-book) - F1":"0.578",
        "QuAC - F1":"0.362",
        "HellaSwag - EM":"0.739",
        "OpenbookQA - EM":"0.52",
        "TruthfulQA - EM":0.193
    },
    {
        "Model":"Cohere large v20220720 (13.1B)",
        "Mean win rate":0.403,
        "MMLU - EM":0.324,
        "BoolQ - EM":0.725,
        "NarrativeQA - F1":"0.625",
        "NaturalQuestions (closed-book) - F1":0.232,
        "NaturalQuestions (open-book) - F1":"0.573",
        "QuAC - F1":"0.338",
        "HellaSwag - EM":"0.736",
        "OpenbookQA - EM":"0.542",
        "TruthfulQA - EM":0.181
    },
    {
        "Model":"RedPajama-INCITE-Base (7B)",
        "Mean win rate":0.377,
        "MMLU - EM":0.302,
        "BoolQ - EM":0.713,
        "NarrativeQA - F1":"0.617",
        "NaturalQuestions (closed-book) - F1":0.25,
        "NaturalQuestions (open-book) - F1":"0.586",
        "QuAC - F1":"0.336",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.205
    },
    {
        "Model":"GPT-NeoX (20B)",
        "Mean win rate":0.331,
        "MMLU - EM":0.276,
        "BoolQ - EM":0.683,
        "NarrativeQA - F1":"0.599",
        "NaturalQuestions (closed-book) - F1":0.193,
        "NaturalQuestions (open-book) - F1":"0.596",
        "QuAC - F1":"0.326",
        "HellaSwag - EM":"0.718",
        "OpenbookQA - EM":"0.524",
        "TruthfulQA - EM":0.216
    },
    {
        "Model":"RedPajama-INCITE-Instruct-v1 (3B)",
        "Mean win rate":0.32,
        "MMLU - EM":0.257,
        "BoolQ - EM":0.677,
        "NarrativeQA - F1":"0.638",
        "NaturalQuestions (closed-book) - F1":0.203,
        "NaturalQuestions (open-book) - F1":"0.637",
        "QuAC - F1":"0.259",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.208
    },
    {
        "Model":"Cohere medium v20221108 (6.1B)",
        "Mean win rate":0.314,
        "MMLU - EM":0.254,
        "BoolQ - EM":0.7,
        "NarrativeQA - F1":"0.61",
        "NaturalQuestions (closed-book) - F1":0.199,
        "NaturalQuestions (open-book) - F1":"0.517",
        "QuAC - F1":"0.314",
        "HellaSwag - EM":"0.726",
        "OpenbookQA - EM":"0.538",
        "TruthfulQA - EM":0.215
    },
    {
        "Model":"RedPajama-INCITE-Base-v1 (3B)",
        "Mean win rate":0.305,
        "MMLU - EM":0.263,
        "BoolQ - EM":0.685,
        "NarrativeQA - F1":"0.555",
        "NaturalQuestions (closed-book) - F1":0.207,
        "NaturalQuestions (open-book) - F1":"0.52",
        "QuAC - F1":"0.309",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.277
    },
    {
        "Model":"Luminous Base (13B)",
        "Mean win rate":0.287,
        "MMLU - EM":0.27,
        "BoolQ - EM":0.719,
        "NarrativeQA - F1":"0.605",
        "NaturalQuestions (closed-book) - F1":0.202,
        "NaturalQuestions (open-book) - F1":"0.568",
        "QuAC - F1":"0.334",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.182
    },
    {
        "Model":"text-curie-001",
        "Mean win rate":0.282,
        "MMLU - EM":0.237,
        "BoolQ - EM":0.62,
        "NarrativeQA - F1":"0.582",
        "NaturalQuestions (closed-book) - F1":0.175,
        "NaturalQuestions (open-book) - F1":"0.571",
        "QuAC - F1":"0.358",
        "HellaSwag - EM":"0.676",
        "OpenbookQA - EM":"0.514",
        "TruthfulQA - EM":0.257
    },
    {
        "Model":"T0pp (11B)",
        "Mean win rate":0.264,
        "MMLU - EM":0.407,
        "BoolQ - EM":0.0,
        "NarrativeQA - F1":"0.151",
        "NaturalQuestions (closed-book) - F1":0.039,
        "NaturalQuestions (open-book) - F1":"0.19",
        "QuAC - F1":"0.121",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.377
    },
    {
        "Model":"TNLG v2 (6.7B)",
        "Mean win rate":0.259,
        "MMLU - EM":0.242,
        "BoolQ - EM":0.698,
        "NarrativeQA - F1":"0.631",
        "NaturalQuestions (closed-book) - F1":0.21,
        "NaturalQuestions (open-book) - F1":"0.561",
        "QuAC - F1":"0.345",
        "HellaSwag - EM":"0.704",
        "OpenbookQA - EM":"0.478",
        "TruthfulQA - EM":0.167
    },
    {
        "Model":"curie (6.7B)",
        "Mean win rate":0.257,
        "MMLU - EM":0.243,
        "BoolQ - EM":0.656,
        "NarrativeQA - F1":"0.604",
        "NaturalQuestions (closed-book) - F1":0.199,
        "NaturalQuestions (open-book) - F1":"0.552",
        "QuAC - F1":"0.321",
        "HellaSwag - EM":"0.682",
        "OpenbookQA - EM":"0.502",
        "TruthfulQA - EM":0.232
    },
    {
        "Model":"Falcon-Instruct (7B)",
        "Mean win rate":0.257,
        "MMLU - EM":0.275,
        "BoolQ - EM":0.72,
        "NarrativeQA - F1":"0.476",
        "NaturalQuestions (closed-book) - F1":0.194,
        "NaturalQuestions (open-book) - F1":"0.449",
        "QuAC - F1":"0.311",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.213
    },
    {
        "Model":"J1-Large v1 (7.5B)",
        "Mean win rate":0.239,
        "MMLU - EM":0.241,
        "BoolQ - EM":0.683,
        "NarrativeQA - F1":"0.623",
        "NaturalQuestions (closed-book) - F1":0.19,
        "NaturalQuestions (open-book) - F1":"0.532",
        "QuAC - F1":"0.328",
        "HellaSwag - EM":"0.7",
        "OpenbookQA - EM":"0.514",
        "TruthfulQA - EM":0.197
    },
    {
        "Model":"Pythia (12B)",
        "Mean win rate":0.224,
        "MMLU - EM":0.274,
        "BoolQ - EM":0.662,
        "NarrativeQA - F1":"0.596",
        "NaturalQuestions (closed-book) - F1":0.175,
        "NaturalQuestions (open-book) - F1":"0.581",
        "QuAC - F1":"0.313",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.177
    },
    {
        "Model":"GPT-J (6B)",
        "Mean win rate":0.214,
        "MMLU - EM":0.249,
        "BoolQ - EM":0.649,
        "NarrativeQA - F1":"0.545",
        "NaturalQuestions (closed-book) - F1":0.156,
        "NaturalQuestions (open-book) - F1":"0.559",
        "QuAC - F1":"0.33",
        "HellaSwag - EM":"0.663",
        "OpenbookQA - EM":"0.514",
        "TruthfulQA - EM":0.199
    },
    {
        "Model":"Cohere medium v20220720 (6.1B)",
        "Mean win rate":0.212,
        "MMLU - EM":0.279,
        "BoolQ - EM":0.659,
        "NarrativeQA - F1":"0.559",
        "NaturalQuestions (closed-book) - F1":0.177,
        "NaturalQuestions (open-book) - F1":"0.504",
        "QuAC - F1":"0.279",
        "HellaSwag - EM":"0.706",
        "OpenbookQA - EM":"0.496",
        "TruthfulQA - EM":0.19
    },
    {
        "Model":"UL2 (20B)",
        "Mean win rate":0.208,
        "MMLU - EM":0.291,
        "BoolQ - EM":0.746,
        "NarrativeQA - F1":"0.083",
        "NaturalQuestions (closed-book) - F1":0.204,
        "NaturalQuestions (open-book) - F1":"0.349",
        "QuAC - F1":"0.144",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.193
    },
    {
        "Model":"T5 (11B)",
        "Mean win rate":0.191,
        "MMLU - EM":0.29,
        "BoolQ - EM":0.761,
        "NarrativeQA - F1":"0.086",
        "NaturalQuestions (closed-book) - F1":0.194,
        "NaturalQuestions (open-book) - F1":"0.477",
        "QuAC - F1":"0.116",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.133
    },
    {
        "Model":"Pythia (6.9B)",
        "Mean win rate":0.171,
        "MMLU - EM":0.236,
        "BoolQ - EM":0.631,
        "NarrativeQA - F1":"0.528",
        "NaturalQuestions (closed-book) - F1":0.142,
        "NaturalQuestions (open-book) - F1":"0.539",
        "QuAC - F1":"0.296",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.213
    },
    {
        "Model":"text-babbage-001",
        "Mean win rate":0.135,
        "MMLU - EM":0.229,
        "BoolQ - EM":0.451,
        "NarrativeQA - F1":"0.429",
        "NaturalQuestions (closed-book) - F1":0.07,
        "NaturalQuestions (open-book) - F1":"0.33",
        "QuAC - F1":"0.284",
        "HellaSwag - EM":"0.561",
        "OpenbookQA - EM":"0.452",
        "TruthfulQA - EM":0.233
    },
    {
        "Model":"Cohere small v20220720 (410M)",
        "Mean win rate":0.114,
        "MMLU - EM":0.264,
        "BoolQ - EM":0.457,
        "NarrativeQA - F1":"0.294",
        "NaturalQuestions (closed-book) - F1":0.078,
        "NaturalQuestions (open-book) - F1":"0.309",
        "QuAC - F1":"0.219",
        "HellaSwag - EM":"0.483",
        "OpenbookQA - EM":"0.348",
        "TruthfulQA - EM":0.217
    },
    {
        "Model":"ada (350M)",
        "Mean win rate":0.11,
        "MMLU - EM":0.243,
        "BoolQ - EM":0.581,
        "NarrativeQA - F1":"0.326",
        "NaturalQuestions (closed-book) - F1":0.082,
        "NaturalQuestions (open-book) - F1":"0.365",
        "QuAC - F1":"0.242",
        "HellaSwag - EM":"0.435",
        "OpenbookQA - EM":"0.38",
        "TruthfulQA - EM":0.215
    },
    {
        "Model":"babbage (1.3B)",
        "Mean win rate":0.103,
        "MMLU - EM":0.235,
        "BoolQ - EM":0.574,
        "NarrativeQA - F1":"0.491",
        "NaturalQuestions (closed-book) - F1":0.119,
        "NaturalQuestions (open-book) - F1":"0.451",
        "QuAC - F1":"0.273",
        "HellaSwag - EM":"0.555",
        "OpenbookQA - EM":"0.438",
        "TruthfulQA - EM":0.188
    },
    {
        "Model":"YaLM (100B)",
        "Mean win rate":0.097,
        "MMLU - EM":0.243,
        "BoolQ - EM":0.634,
        "NarrativeQA - F1":"0.252",
        "NaturalQuestions (closed-book) - F1":0.068,
        "NaturalQuestions (open-book) - F1":"0.227",
        "QuAC - F1":"0.162",
        "HellaSwag - EM":null,
        "OpenbookQA - EM":null,
        "TruthfulQA - EM":0.202
    },
    {
        "Model":"text-ada-001",
        "Mean win rate":0.079,
        "MMLU - EM":0.238,
        "BoolQ - EM":0.464,
        "NarrativeQA - F1":"0.238",
        "NaturalQuestions (closed-book) - F1":0.025,
        "NaturalQuestions (open-book) - F1":"0.149",
        "QuAC - F1":"0.176",
        "HellaSwag - EM":"0.429",
        "OpenbookQA - EM":"0.346",
        "TruthfulQA - EM":0.232
    }
]