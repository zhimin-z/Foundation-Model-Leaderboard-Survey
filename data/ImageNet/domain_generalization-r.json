[
    {
        "table_id":3369,
        "row_id":57556,
        "rank":1,
        "method":"Model soups (BASIC-L)",
        "mlmodel":{

        },
        "method_short":"Model soups ",
        "method_details":"BASIC-L",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-10",
        "metrics":{
            "Top-1 Error Rate":"3.90"
        },
        "raw_metrics":{
            "Top-1 Error Rate":3.9
        },
        "uses_additional_data":true,
        "paper":{
            "id":974897,
            "title":"Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
            "url":"\/paper\/model-soups-averaging-weights-of-multiple",
            "published":"2022-03-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/model-soups-averaging-weights-of-multiple\/review\/?hl=57556"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3369,
        "row_id":48977,
        "rank":2,
        "method":"Model soups (ViT-G\/14)",
        "mlmodel":{

        },
        "method_short":"Model soups ",
        "method_details":"ViT-G\/14",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-10",
        "metrics":{
            "Top-1 Error Rate":"4.54"
        },
        "raw_metrics":{
            "Top-1 Error Rate":4.54
        },
        "uses_additional_data":true,
        "paper":{
            "id":974897,
            "title":"Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
            "url":"\/paper\/model-soups-averaging-weights-of-multiple",
            "published":"2022-03-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/model-soups-averaging-weights-of-multiple\/review\/?hl=48977"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3369,
        "row_id":86648,
        "rank":3,
        "method":"CAR-FT (CLIP, ViT-L\/14@336px)",
        "mlmodel":{

        },
        "method_short":"CAR-FT ",
        "method_details":"CLIP, ViT-L\/14@336px",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-29",
        "metrics":{
            "Top-1 Error Rate":"10.3"
        },
        "raw_metrics":{
            "Top-1 Error Rate":10.3
        },
        "uses_additional_data":true,
        "paper":{
            "id":1120059,
            "title":"Context-Aware Robust Fine-Tuning",
            "url":"\/paper\/context-aware-robust-fine-tuning",
            "published":"2022-11-29T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/context-aware-robust-fine-tuning\/review\/?hl=86648"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3369,
        "row_id":54314,
        "rank":4,
        "method":"FAN-Hybrid-L(IN-21K, 384))",
        "mlmodel":{

        },
        "method_short":"FAN-Hybrid-L",
        "method_details":"IN-21K, 384",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-26",
        "metrics":{
            "Top-1 Error Rate":"28.9"
        },
        "raw_metrics":{
            "Top-1 Error Rate":28.9
        },
        "uses_additional_data":true,
        "paper":{
            "id":999980,
            "title":"Understanding The Robustness in Vision Transformers",
            "url":"\/paper\/understanding-the-robustness-in-vision",
            "published":"2022-04-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/understanding-the-robustness-in-vision\/review\/?hl=54314"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3369,
        "row_id":87289,
        "rank":5,
        "method":"LLE (ViT-B\/16, SWAG, Edge Aug)",
        "mlmodel":{

        },
        "method_short":"LLE ",
        "method_details":"ViT-B\/16, SWAG, Edge Aug",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-09",
        "metrics":{
            "Top-1 Error Rate":"31.3"
        },
        "raw_metrics":{
            "Top-1 Error Rate":31.3
        },
        "uses_additional_data":true,
        "paper":{
            "id":1126199,
            "title":"A Whac-A-Mole Dilemma: Shortcuts Come in Multiples Where Mitigating One Amplifies Others",
            "url":"\/paper\/a-whac-a-mole-dilemma-shortcuts-come-in",
            "published":"2022-12-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/a-whac-a-mole-dilemma-shortcuts-come-in\/review\/?hl=87289"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3369,
        "row_id":45428,
        "rank":6,
        "method":"ConvNeXt-XL (Im21k, 384)",
        "mlmodel":{

        },
        "method_short":"ConvNeXt-XL ",
        "method_details":"Im21k, 384",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-10",
        "metrics":{
            "Top-1 Error Rate":"31.8"
        },
        "raw_metrics":{
            "Top-1 Error Rate":31.8
        },
        "uses_additional_data":true,
        "paper":{
            "id":942596,
            "title":"A ConvNet for the 2020s",
            "url":"\/paper\/a-convnet-for-the-2020s",
            "published":"2022-01-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/a-convnet-for-the-2020s\/review\/?hl=45428"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3369,
        "row_id":87288,
        "rank":7,
        "method":"LLE (ViT-H\/14, MAE, Edge Aug)",
        "mlmodel":{

        },
        "method_short":"LLE ",
        "method_details":"ViT-H\/14, MAE, Edge Aug",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-09",
        "metrics":{
            "Top-1 Error Rate":"33.1"
        },
        "raw_metrics":{
            "Top-1 Error Rate":33.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":1126199,
            "title":"A Whac-A-Mole Dilemma: Shortcuts Come in Multiples Where Mitigating One Amplifies Others",
            "url":"\/paper\/a-whac-a-mole-dilemma-shortcuts-come-in",
            "published":"2022-12-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/a-whac-a-mole-dilemma-shortcuts-come-in\/review\/?hl=87288"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3369,
        "row_id":45123,
        "rank":8,
        "method":"MAE (ViT-H, 448)",
        "mlmodel":{

        },
        "method_short":"MAE ",
        "method_details":"ViT-H, 448",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-11",
        "metrics":{
            "Top-1 Error Rate":"33.5"
        },
        "raw_metrics":{
            "Top-1 Error Rate":33.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":908690,
            "title":"Masked Autoencoders Are Scalable Vision Learners",
            "url":"\/paper\/masked-autoencoders-are-scalable-vision",
            "published":"2021-11-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-autoencoders-are-scalable-vision\/review\/?hl=45123"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3369,
        "row_id":68559,
        "rank":9,
        "method":"MAE+DAT (ViT-H)",
        "mlmodel":{

        },
        "method_short":"MAE+DAT ",
        "method_details":"ViT-H",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-16",
        "metrics":{
            "Top-1 Error Rate":"34.39"
        },
        "raw_metrics":{
            "Top-1 Error Rate":34.39
        },
        "uses_additional_data":false,
        "paper":{
            "id":1075790,
            "title":"Enhance the Visual Representation via Discrete Adversarial Training",
            "url":"\/paper\/enhance-the-visual-representation-via",
            "published":"2022-09-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/enhance-the-visual-representation-via\/review\/?hl=68559"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3369,
        "row_id":70352,
        "rank":10,
        "method":"GPaCo (ViT-L)",
        "mlmodel":{

        },
        "method_short":"GPaCo ",
        "method_details":"ViT-L",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-26",
        "metrics":{
            "Top-1 Error Rate":"39.7"
        },
        "raw_metrics":{
            "Top-1 Error Rate":39.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":1080045,
            "title":"Generalized Parametric Contrastive Learning",
            "url":"\/paper\/generalized-parametric-contrastive-learning",
            "published":"2022-09-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/generalized-parametric-contrastive-learning\/review\/?hl=70352"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3369,
        "row_id":55840,
        "rank":11,
        "method":"VOLO-D5+HAT",
        "mlmodel":{

        },
        "method_short":"VOLO-D5+HAT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-03",
        "metrics":{
            "Top-1 Error Rate":"40.3"
        },
        "raw_metrics":{
            "Top-1 Error Rate":40.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":988272,
            "title":"Improving Vision Transformers by Revisiting High-frequency Components",
            "url":"\/paper\/improving-vision-transformers-by-revisiting",
            "published":"2022-04-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improving-vision-transformers-by-revisiting\/review\/?hl=55840"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3369,
        "row_id":43621,
        "rank":12,
        "method":"Pyramid Adversarial Training Improves ViT (Im21k)",
        "mlmodel":{

        },
        "method_short":"Pyramid Adversarial Training Improves ViT ",
        "method_details":"Im21k",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-30",
        "metrics":{
            "Top-1 Error Rate":"42.16"
        },
        "raw_metrics":{
            "Top-1 Error Rate":42.16
        },
        "uses_additional_data":true,
        "paper":{
            "id":923009,
            "title":"Pyramid Adversarial Training Improves ViT Performance",
            "url":"\/paper\/pyramid-adversarial-training-improves-vit",
            "published":"2021-11-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/pyramid-adversarial-training-improves-vit\/review\/?hl=43621"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3369,
        "row_id":109652,
        "rank":13,
        "method":"FAN-L-Hybrid+STL",
        "mlmodel":{

        },
        "method_short":"FAN-L-Hybrid+STL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-10-02",
        "metrics":{
            "Top-1 Error Rate":"43.4"
        },
        "raw_metrics":{
            "Top-1 Error Rate":43.4
        },
        "uses_additional_data":false,
        "paper":{
            "id":1289297,
            "title":"Fully Attentional Networks with Self-emerging Token Labeling",
            "url":"\/paper\/fully-attentional-networks-with-self-emerging",
            "published":"2023-01-01T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":145,
                "name":"ViT",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":3369,
        "row_id":47970,
        "rank":14,
        "method":"SEER (RegNet10B)",
        "mlmodel":{

        },
        "method_short":"SEER ",
        "method_details":"RegNet10B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-16",
        "metrics":{
            "Top-1 Error Rate":"43.9"
        },
        "raw_metrics":{
            "Top-1 Error Rate":43.9
        },
        "uses_additional_data":true,
        "paper":{
            "id":963673,
            "title":"Vision Models Are More Robust And Fair When Pretrained On Uncurated Images Without Supervision",
            "url":"\/paper\/vision-models-are-more-robust-and-fair-when",
            "published":"2022-02-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vision-models-are-more-robust-and-fair-when\/review\/?hl=47970"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3369,
        "row_id":44731,
        "rank":15,
        "method":"DiscreteViT",
        "mlmodel":{

        },
        "method_short":"DiscreteViT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-20",
        "metrics":{
            "Top-1 Error Rate":"44.74"
        },
        "raw_metrics":{
            "Top-1 Error Rate":44.74
        },
        "uses_additional_data":false,
        "paper":{
            "id":914582,
            "title":"Discrete Representations Strengthen Vision Transformer Robustness",
            "url":"\/paper\/discrete-representations-strengthen-vision-1",
            "published":"2021-11-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/discrete-representations-strengthen-vision-1\/review\/?hl=44731"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3369,
        "row_id":43620,
        "rank":16,
        "method":"Pyramid Adversarial Training Improves ViT",
        "mlmodel":{

        },
        "method_short":"Pyramid Adversarial Training Improves ViT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-30",
        "metrics":{
            "Top-1 Error Rate":"46.08"
        },
        "raw_metrics":{
            "Top-1 Error Rate":46.08
        },
        "uses_additional_data":false,
        "paper":{
            "id":923009,
            "title":"Pyramid Adversarial Training Improves ViT Performance",
            "url":"\/paper\/pyramid-adversarial-training-improves-vit",
            "published":"2021-11-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/pyramid-adversarial-training-improves-vit\/review\/?hl=43620"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3369,
        "row_id":33633,
        "rank":17,
        "method":"RVT-B*",
        "mlmodel":{

        },
        "method_short":"RVT-B*",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-17",
        "metrics":{
            "Top-1 Error Rate":"51.3"
        },
        "raw_metrics":{
            "Top-1 Error Rate":51.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":800173,
            "title":"Towards Robust Vision Transformer",
            "url":"\/paper\/rethinking-the-design-principles-of-robust",
            "published":"2021-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-the-design-principles-of-robust\/review\/?hl=33633"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3369,
        "row_id":53858,
        "rank":18,
        "method":"Sequencer2D-L",
        "mlmodel":{

        },
        "method_short":"Sequencer2D-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-04",
        "metrics":{
            "Top-1 Error Rate":"51.9"
        },
        "raw_metrics":{
            "Top-1 Error Rate":51.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":1004136,
            "title":"Sequencer: Deep LSTM for Image Classification",
            "url":"\/paper\/sequencer-deep-lstm-for-image-classification",
            "published":"2022-05-04T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3369,
        "row_id":33632,
        "rank":19,
        "method":"RVT-S*",
        "mlmodel":{

        },
        "method_short":"RVT-S*",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-17",
        "metrics":{
            "Top-1 Error Rate":"52.3"
        },
        "raw_metrics":{
            "Top-1 Error Rate":52.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":800173,
            "title":"Towards Robust Vision Transformer",
            "url":"\/paper\/rethinking-the-design-principles-of-robust",
            "published":"2021-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-the-design-principles-of-robust\/review\/?hl=33632"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3369,
        "row_id":17686,
        "rank":20,
        "method":"DeepAugment+AugMix (ResNet-50)",
        "mlmodel":{

        },
        "method_short":"DeepAugment+AugMix ",
        "method_details":"ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-29",
        "metrics":{
            "Top-1 Error Rate":" 53.2 "
        },
        "raw_metrics":{
            "Top-1 Error Rate":53.2
        },
        "uses_additional_data":false,
        "paper":{
            "id":205961,
            "title":"The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization",
            "url":"\/paper\/the-many-faces-of-robustness-a-critical",
            "published":"2020-06-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/the-many-faces-of-robustness-a-critical\/review\/?hl=17686"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":3369,
        "row_id":44978,
        "rank":21,
        "method":"PRIME with JSD (ResNet-50)",
        "mlmodel":{

        },
        "method_short":"PRIME with JSD ",
        "method_details":"ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-27",
        "metrics":{
            "Top-1 Error Rate":"53.7"
        },
        "raw_metrics":{
            "Top-1 Error Rate":53.7
        },
        "uses_additional_data":false,
        "paper":{
            "id":933290,
            "title":"PRIME: A few primitives can boost robustness to common corruptions",
            "url":"\/paper\/prime-a-few-primitives-can-boost-robustness",
            "published":"2021-12-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/prime-a-few-primitives-can-boost-robustness\/review\/?hl=44978"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":3369,
        "row_id":33631,
        "rank":22,
        "method":"RVT-Ti*",
        "mlmodel":{

        },
        "method_short":"RVT-Ti*",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-17",
        "metrics":{
            "Top-1 Error Rate":"56.1"
        },
        "raw_metrics":{
            "Top-1 Error Rate":56.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":800173,
            "title":"Towards Robust Vision Transformer",
            "url":"\/paper\/rethinking-the-design-principles-of-robust",
            "published":"2021-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-the-design-principles-of-robust\/review\/?hl=33631"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3369,
        "row_id":44945,
        "rank":23,
        "method":"PRIME (ResNet-50)",
        "mlmodel":{

        },
        "method_short":"PRIME ",
        "method_details":"ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-27",
        "metrics":{
            "Top-1 Error Rate":"57.1"
        },
        "raw_metrics":{
            "Top-1 Error Rate":57.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":933290,
            "title":"PRIME: A few primitives can boost robustness to common corruptions",
            "url":"\/paper\/prime-a-few-primitives-can-boost-robustness",
            "published":"2021-12-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/prime-a-few-primitives-can-boost-robustness\/review\/?hl=44945"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":3369,
        "row_id":17685,
        "rank":24,
        "method":"DeepAugment (ResNet-50)",
        "mlmodel":{

        },
        "method_short":"DeepAugment ",
        "method_details":"ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-29",
        "metrics":{
            "Top-1 Error Rate":"57.8"
        },
        "raw_metrics":{
            "Top-1 Error Rate":57.8
        },
        "uses_additional_data":false,
        "paper":{
            "id":205961,
            "title":"The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization",
            "url":"\/paper\/the-many-faces-of-robustness-a-critical",
            "published":"2020-06-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/the-many-faces-of-robustness-a-critical\/review\/?hl=17685"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":3369,
        "row_id":17683,
        "rank":25,
        "method":"Stylized ImageNet (ResNet-50)",
        "mlmodel":{

        },
        "method_short":"Stylized ImageNet ",
        "method_details":"ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-11-29",
        "metrics":{
            "Top-1 Error Rate":"58.5 "
        },
        "raw_metrics":{
            "Top-1 Error Rate":58.5
        },
        "uses_additional_data":true,
        "paper":{
            "id":63747,
            "title":"ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness",
            "url":"\/paper\/imagenet-trained-cnns-are-biased-towards",
            "published":"2018-11-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/imagenet-trained-cnns-are-biased-towards\/review\/?hl=17683"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":3369,
        "row_id":17684,
        "rank":26,
        "method":"AugMix (ResNet-50)",
        "mlmodel":{

        },
        "method_short":"AugMix ",
        "method_details":"ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-12-05",
        "metrics":{
            "Top-1 Error Rate":"58.9"
        },
        "raw_metrics":{
            "Top-1 Error Rate":58.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":175569,
            "title":"AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty",
            "url":"\/paper\/augmix-a-simple-data-processing-method-to",
            "published":"2019-12-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/augmix-a-simple-data-processing-method-to\/review\/?hl=17684"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":3369,
        "row_id":17682,
        "rank":27,
        "method":"ResNet-50",
        "mlmodel":{

        },
        "method_short":"ResNet-50",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2015-12-10",
        "metrics":{
            "Top-1 Error Rate":"63.9"
        },
        "raw_metrics":{
            "Top-1 Error Rate":63.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":37118,
            "title":"Deep Residual Learning for Image Recognition",
            "url":"\/paper\/deep-residual-learning-for-image-recognition",
            "published":"2015-12-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deep-residual-learning-for-image-recognition\/review\/?hl=17682"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":3369,
        "row_id":34308,
        "rank":28,
        "method":"ResNet-152x2-SAM",
        "mlmodel":{

        },
        "method_short":"ResNet-152x2-SAM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-03",
        "metrics":{
            "Top-1 Error Rate":"71.9"
        },
        "raw_metrics":{
            "Top-1 Error Rate":71.9
        },
        "uses_additional_data":false,
        "paper":{
            "id":810994,
            "title":"When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations",
            "url":"\/paper\/when-vision-transformers-outperform-resnets",
            "published":"2021-06-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/when-vision-transformers-outperform-resnets\/review\/?hl=34308"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":3369,
        "row_id":34313,
        "rank":29,
        "method":"ViT-B\/16-SAM",
        "mlmodel":{

        },
        "method_short":"ViT-B\/16-SAM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-03",
        "metrics":{
            "Top-1 Error Rate":"73.6"
        },
        "raw_metrics":{
            "Top-1 Error Rate":73.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":810994,
            "title":"When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations",
            "url":"\/paper\/when-vision-transformers-outperform-resnets",
            "published":"2021-06-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/when-vision-transformers-outperform-resnets\/review\/?hl=34313"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":3369,
        "row_id":34318,
        "rank":30,
        "method":"Mixer-B\/8-SAM",
        "mlmodel":{

        },
        "method_short":"Mixer-B\/8-SAM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-03",
        "metrics":{
            "Top-1 Error Rate":"76.5"
        },
        "raw_metrics":{
            "Top-1 Error Rate":76.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":810994,
            "title":"When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations",
            "url":"\/paper\/when-vision-transformers-outperform-resnets",
            "published":"2021-06-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/when-vision-transformers-outperform-resnets\/review\/?hl=34318"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]