[
    {
        "table_id":116,
        "row_id":112988,
        "rank":1,
        "Model":"OmniVec",
        "mlmodel":{

        },
        "method_short":"OmniVec",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-07",
        "metrics":{
            "Top 1 Accuracy":"92.4%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":92.4,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1319233,
            "title":"OmniVec: Learning robust representations with cross modal sharing",
            "url":"\/paper\/omnivec-learning-robust-representations-with",
            "published":"2023-11-07T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/omnivec-learning-robust-representations-with\/review\/?hl=112988"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":97249,
        "rank":2,
        "Model":"BASIC-L (Lion, fine-tuned)",
        "mlmodel":{

        },
        "method_short":"BASIC-L ",
        "method_details":"Lion, fine-tuned",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "Top 1 Accuracy":"91.1%",
            "Number of params":"2440M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":91.1,
            "Number of params":2440000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":98,
                "name":"Conv+Transformer",
                "color":"#ff2600"
            },
            {
                "id":247,
                "name":"ALIGN",
                "color":"#2771D3"
            },
            {
                "id":103,
                "name":"JFT-3B",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":53996,
        "rank":3,
        "Model":"CoCa (finetuned)",
        "mlmodel":{

        },
        "method_short":"CoCa ",
        "method_details":"finetuned",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-04",
        "metrics":{
            "Top 1 Accuracy":"91.0%",
            "Number of params":"2100M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":91.0,
            "Number of params":2100000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1004211,
            "title":"CoCa: Contrastive Captioners are Image-Text Foundation Models",
            "url":"\/paper\/coca-contrastive-captioners-are-image-text",
            "published":"2022-05-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/coca-contrastive-captioners-are-image-text\/review\/?hl=53996"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":247,
                "name":"ALIGN",
                "color":"#2771D3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":103,
                "name":"JFT-3B",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":57543,
        "rank":4,
        "Model":"Model soups (BASIC-L)",
        "mlmodel":{

        },
        "method_short":"Model soups ",
        "method_details":"BASIC-L",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-10",
        "metrics":{
            "Top 1 Accuracy":"90.98%",
            "Number of params":"2440M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":90.98,
            "Number of params":2440000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":974897,
            "title":"Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
            "url":"\/paper\/model-soups-averaging-weights-of-multiple",
            "published":"2022-03-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/model-soups-averaging-weights-of-multiple\/review\/?hl=57543"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":247,
                "name":"ALIGN",
                "color":"#2771D3"
            },
            {
                "id":103,
                "name":"JFT-3B",
                "color":"#2771D3"
            },
            {
                "id":98,
                "name":"Conv+Transformer",
                "color":"#ff2600"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":48974,
        "rank":5,
        "Model":"Model soups (ViT-G\/14)",
        "mlmodel":{

        },
        "method_short":"Model soups ",
        "method_details":"ViT-G\/14",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-10",
        "metrics":{
            "Top 1 Accuracy":"90.94%",
            "Number of params":"1843M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":90.94,
            "Number of params":1843000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":974897,
            "title":"Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
            "url":"\/paper\/model-soups-averaging-weights-of-multiple",
            "published":"2022-03-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/model-soups-averaging-weights-of-multiple\/review\/?hl=48974"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":103,
                "name":"JFT-3B",
                "color":"#2771D3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":70405,
        "rank":6,
        "Model":"ViT-e",
        "mlmodel":{

        },
        "method_short":"ViT-e",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-14",
        "metrics":{
            "Top 1 Accuracy":"90.9%",
            "Number of params":"3900M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":90.9,
            "Number of params":3900000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1074922,
            "title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model",
            "url":"\/paper\/pali-a-jointly-scaled-multilingual-language",
            "published":"2022-09-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/pali-a-jointly-scaled-multilingual-language\/review\/?hl=70405"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":103,
                "name":"JFT-3B",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":34580,
        "rank":7,
        "Model":"CoAtNet-7",
        "mlmodel":{

        },
        "method_short":"CoAtNet-7",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-09",
        "metrics":{
            "Top 1 Accuracy":"90.88%",
            "Number of params":"2440M",
            "GFLOPs":"2586",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":90.88,
            "Number of params":2440000000.0,
            "GFLOPs":2586.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":814553,
            "title":"CoAtNet: Marrying Convolution and Attention for All Data Sizes",
            "url":"\/paper\/coatnet-marrying-convolution-and-attention",
            "published":"2021-06-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/coatnet-marrying-convolution-and-attention\/review\/?hl=34580"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":98,
                "name":"Conv+Transformer",
                "color":"#ff2600"
            },
            {
                "id":103,
                "name":"JFT-3B",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":97331,
        "rank":8,
        "Model":"ViT-G\/14 (Lion)",
        "mlmodel":{

        },
        "method_short":"ViT-G\/14 ",
        "method_details":"Lion",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "Top 1 Accuracy":"90.71%",
            "Number of params":"1843M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":90.71,
            "Number of params":1843000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":103,
                "name":"JFT-3B",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":53995,
        "rank":9,
        "Model":"CoCa (frozen)",
        "mlmodel":{

        },
        "method_short":"CoCa ",
        "method_details":"frozen",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-04",
        "metrics":{
            "Top 1 Accuracy":"90.60%",
            "Number of params":"2100M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":90.6,
            "Number of params":2100000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1004211,
            "title":"CoCa: Contrastive Captioners are Image-Text Foundation Models",
            "url":"\/paper\/coca-contrastive-captioners-are-image-text",
            "published":"2022-05-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/coca-contrastive-captioners-are-image-text\/review\/?hl=53995"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":247,
                "name":"ALIGN",
                "color":"#2771D3"
            },
            {
                "id":103,
                "name":"JFT-3B",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":40143,
        "rank":10,
        "Model":"CoAtNet-6",
        "mlmodel":{

        },
        "method_short":"CoAtNet-6",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-09",
        "metrics":{
            "Top 1 Accuracy":"90.45%",
            "Number of params":"1470M",
            "GFLOPs":"1521",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":90.45,
            "Number of params":1470000000.0,
            "GFLOPs":1521.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":814553,
            "title":"CoAtNet: Marrying Convolution and Attention for All Data Sizes",
            "url":"\/paper\/coatnet-marrying-convolution-and-attention",
            "published":"2021-06-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/coatnet-marrying-convolution-and-attention\/review\/?hl=40143"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":98,
                "name":"Conv+Transformer",
                "color":"#ff2600"
            },
            {
                "id":103,
                "name":"JFT-3B",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":34458,
        "rank":11,
        "Model":"ViT-G\/14",
        "mlmodel":{

        },
        "method_short":"ViT-G\/14",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-08",
        "metrics":{
            "Top 1 Accuracy":"90.45%",
            "Number of params":"1843M",
            "GFLOPs":"2859.9",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":90.45,
            "Number of params":1843000000.0,
            "GFLOPs":2859.9,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":813674,
            "title":"Scaling Vision Transformers",
            "url":"\/paper\/scaling-vision-transformers",
            "published":"2021-06-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-vision-transformers\/review\/?hl=34458"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":103,
                "name":"JFT-3B",
                "color":"#2771D3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":51577,
        "rank":12,
        "Model":"DaViT-G",
        "mlmodel":{

        },
        "method_short":"DaViT-G",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-07",
        "metrics":{
            "Top 1 Accuracy":"90.4%",
            "Number of params":"1437M",
            "GFLOPs":"1038",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":90.4,
            "Number of params":1437000000.0,
            "GFLOPs":1038.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":990738,
            "title":"DaViT: Dual Attention Vision Transformers",
            "url":"\/paper\/davit-dual-attention-vision-transformers",
            "published":"2022-04-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/davit-dual-attention-vision-transformers\/review\/?hl=51577"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":51583,
        "rank":13,
        "Model":"DaViT-H",
        "mlmodel":{

        },
        "method_short":"DaViT-H",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-07",
        "metrics":{
            "Top 1 Accuracy":"90.2%",
            "Number of params":"362M",
            "GFLOPs":"334",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":90.2,
            "Number of params":362000000.0,
            "GFLOPs":334.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":990738,
            "title":"DaViT: Dual Attention Vision Transformers",
            "url":"\/paper\/davit-dual-attention-vision-transformers",
            "published":"2022-04-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/davit-dual-attention-vision-transformers\/review\/?hl=51583"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":24286,
        "rank":14,
        "Model":"Meta Pseudo Labels (EfficientNet-L2)",
        "mlmodel":{

        },
        "method_short":"Meta Pseudo Labels ",
        "method_details":"EfficientNet-L2",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-23",
        "metrics":{
            "Top 1 Accuracy":"90.2%",
            "Number of params":"480M",
            "GFLOPs":null,
            "Hardware Burden":"95040G",
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":90.2,
            "Number of params":480000000.0,
            "GFLOPs":null,
            "Hardware Burden":95040.0,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":188189,
            "title":"Meta Pseudo Labels",
            "url":"\/paper\/meta-pseudo-labels",
            "published":"2020-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/meta-pseudo-labels\/review\/?hl=24286"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":42925,
        "rank":15,
        "Model":"SwinV2-G",
        "mlmodel":{

        },
        "method_short":"SwinV2-G",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-18",
        "metrics":{
            "Top 1 Accuracy":"90.17%",
            "Number of params":"3000M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":90.17,
            "Number of params":3000000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":912369,
            "title":"Swin Transformer V2: Scaling Up Capacity and Resolution",
            "url":"\/paper\/swin-transformer-v2-scaling-up-capacity-and",
            "published":"2021-11-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/swin-transformer-v2-scaling-up-capacity-and\/review\/?hl=42925"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":99183,
        "rank":16,
        "Model":"InternImage-DCNv3-G (M3I Pre-training)",
        "mlmodel":{

        },
        "method_short":"InternImage-DCNv3-G ",
        "method_details":"M3I Pre-training",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-10",
        "metrics":{
            "Top 1 Accuracy":"90.1%",
            "Number of params":"3000M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":90.1,
            "Number of params":3000000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1109218,
            "title":"InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions",
            "url":"\/paper\/internimage-exploring-large-scale-vision",
            "published":"2022-11-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/internimage-exploring-large-scale-vision\/review\/?hl=99183"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":43156,
        "rank":17,
        "Model":"Florence-CoSwin-H",
        "mlmodel":{

        },
        "method_short":"Florence-CoSwin-H",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-22",
        "metrics":{
            "Top 1 Accuracy":"90.05%",
            "Number of params":"893M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":90.05,
            "Number of params":893000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":914419,
            "title":"Florence: A New Foundation Model for Computer Vision",
            "url":"\/paper\/florence-a-new-foundation-model-for-computer",
            "published":"2021-11-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/florence-a-new-foundation-model-for-computer\/review\/?hl=43156"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":248,
                "name":"FLD-900M",
                "color":"#2771D3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":24284,
        "rank":18,
        "Model":"Meta Pseudo Labels (EfficientNet-B6-Wide)",
        "mlmodel":{

        },
        "method_short":"Meta Pseudo Labels ",
        "method_details":"EfficientNet-B6-Wide",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-23",
        "metrics":{
            "Top 1 Accuracy":"90%",
            "Number of params":"390M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":90.0,
            "Number of params":390000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":188189,
            "title":"Meta Pseudo Labels",
            "url":"\/paper\/meta-pseudo-labels",
            "published":"2020-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/meta-pseudo-labels\/review\/?hl=24284"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":89208,
        "rank":19,
        "Model":"RevCol-H",
        "mlmodel":{

        },
        "method_short":"RevCol-H",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-22",
        "metrics":{
            "Top 1 Accuracy":"90.0%",
            "Number of params":"2158M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":90.0,
            "Number of params":2158000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1132613,
            "title":"Reversible Column Networks",
            "url":"\/paper\/reversible-column-networks",
            "published":"2022-12-22T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":370,
                "name":"Pure CNN",
                "color":"#ff4013"
            },
            {
                "id":283,
                "name":"Reversible",
                "color":"#3227d3"
            },
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":102987,
        "rank":20,
        "Model":"ONE-PEACE",
        "mlmodel":{

        },
        "method_short":"ONE-PEACE",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-18",
        "metrics":{
            "Top 1 Accuracy":"89.8%",
            "Number of params":"1520M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":89.8,
            "Number of params":1520000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1211430,
            "title":"ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities",
            "url":"\/paper\/one-peace-exploring-one-general",
            "published":"2023-05-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/one-peace-exploring-one-general\/review\/?hl=102987"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":86823,
        "rank":21,
        "Model":"EVA",
        "mlmodel":{

        },
        "method_short":"EVA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-14",
        "metrics":{
            "Top 1 Accuracy":"89.7%",
            "Number of params":"1000M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":89.7,
            "Number of params":1000000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1110592,
            "title":"EVA: Exploring the Limits of Masked Visual Representation Learning at Scale",
            "url":"\/paper\/eva-exploring-the-limits-of-masked-visual",
            "published":"2022-11-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/eva-exploring-the-limits-of-masked-visual\/review\/?hl=86823"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":100961,
        "rank":22,
        "Model":"MAWS (ViT-2B)",
        "mlmodel":{

        },
        "method_short":"MAWS ",
        "method_details":"ViT-2B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-23",
        "metrics":{
            "Top 1 Accuracy":"89.7%",
            "Number of params":"2000M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":89.7,
            "Number of params":2000000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1179156,
            "title":"The effectiveness of MAE pre-pretraining for billion-scale pretraining",
            "url":"\/paper\/the-effectiveness-of-mae-pre-pretraining-for",
            "published":"2023-03-23T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":80334,
        "rank":23,
        "Model":"M3I Pre-training (InternImage-H)",
        "mlmodel":{

        },
        "method_short":"M3I Pre-training ",
        "method_details":"InternImage-H",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-17",
        "metrics":{
            "Top 1 Accuracy":"89.6%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":89.6,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1113300,
            "title":"Towards All-in-one Pre-training via Maximizing Multi-modal Mutual Information",
            "url":"\/paper\/towards-all-in-one-pre-training-via",
            "published":"2022-11-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/towards-all-in-one-pre-training-via\/review\/?hl=80334"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":345,
                "name":"YFCC-15M",
                "color":"#2771D3"
            },
            {
                "id":344,
                "name":"Laion-400M",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":100145,
        "rank":24,
        "Model":"ViT-L\/16 (384res, distilled from ViT-22B)",
        "mlmodel":{

        },
        "method_short":"ViT-L\/16 ",
        "method_details":"384res, distilled from ViT-22B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-04",
        "metrics":{
            "Top 1 Accuracy":"89.6%",
            "Number of params":"307M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":89.6,
            "Number of params":307000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1155994,
            "title":"Scaling Vision Transformers to 22 Billion Parameters",
            "url":"\/paper\/scaling-vision-transformers-to-22-billion",
            "published":"2023-02-10T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":102365,
        "rank":25,
        "Model":"InternImage-H",
        "mlmodel":{

        },
        "method_short":"InternImage-H",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-10",
        "metrics":{
            "Top 1 Accuracy":"89.6%",
            "Number of params":"1080M",
            "GFLOPs":"1478",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":89.6,
            "Number of params":1080000000.0,
            "GFLOPs":1478.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1109218,
            "title":"InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions",
            "url":"\/paper\/internimage-exploring-large-scale-vision",
            "published":"2022-11-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/internimage-exploring-large-scale-vision\/review\/?hl=102365"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":61437,
        "rank":26,
        "Model":"MaxViT-XL (512res, JFT)",
        "mlmodel":{

        },
        "method_short":"MaxViT-XL ",
        "method_details":"512res, JFT",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-04",
        "metrics":{
            "Top 1 Accuracy":"89.53%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":89.53,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":988369,
            "title":"MaxViT: Multi-Axis Vision Transformer",
            "url":"\/paper\/maxvit-multi-axis-vision-transformer",
            "published":"2022-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/maxvit-multi-axis-vision-transformer\/review\/?hl=61437"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":61433,
        "rank":27,
        "Model":"MaxViT-XL (384res, JFT)",
        "mlmodel":{

        },
        "method_short":"MaxViT-XL ",
        "method_details":"384res, JFT",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-04",
        "metrics":{
            "Top 1 Accuracy":"89.41%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":89.41,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":988369,
            "title":"MaxViT: Multi-Axis Vision Transformer",
            "url":"\/paper\/maxvit-multi-axis-vision-transformer",
            "published":"2022-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/maxvit-multi-axis-vision-transformer\/review\/?hl=61433"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":71568,
        "rank":28,
        "Model":"MaxViT-L (512res, JFT)",
        "mlmodel":{

        },
        "method_short":"MaxViT-L ",
        "method_details":"512res, JFT",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-04",
        "metrics":{
            "Top 1 Accuracy":"89.41%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":89.41,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":988369,
            "title":"MaxViT: Multi-Axis Vision Transformer",
            "url":"\/paper\/maxvit-multi-axis-vision-transformer",
            "published":"2022-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/maxvit-multi-axis-vision-transformer\/review\/?hl=71568"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":25384,
        "rank":29,
        "Model":"NFNet-F4+",
        "mlmodel":{

        },
        "method_short":"NFNet-F4+",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-11",
        "metrics":{
            "Top 1 Accuracy":"89.2%",
            "Number of params":"527M",
            "GFLOPs":"367",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":89.2,
            "Number of params":527000000.0,
            "GFLOPs":367.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":744370,
            "title":"High-Performance Large-Scale Image Recognition Without Normalization",
            "url":"\/paper\/high-performance-large-scale-image",
            "published":"2021-02-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/high-performance-large-scale-image\/review\/?hl=25384"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":71564,
        "rank":30,
        "Model":"MaxViT-L (384res, JFT)",
        "mlmodel":{

        },
        "method_short":"MaxViT-L ",
        "method_details":"384res, JFT",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-04",
        "metrics":{
            "Top 1 Accuracy":"89.12%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":89.12,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":988369,
            "title":"MaxViT: Multi-Axis Vision Transformer",
            "url":"\/paper\/maxvit-multi-axis-vision-transformer",
            "published":"2022-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/maxvit-multi-axis-vision-transformer\/review\/?hl=71564"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":71508,
        "rank":31,
        "Model":"MOAT-4 22K+1K",
        "mlmodel":{

        },
        "method_short":"MOAT-4 22K+1K",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-04",
        "metrics":{
            "Top 1 Accuracy":"89.1%",
            "Number of params":"483.2M",
            "GFLOPs":"648.5",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":89.1,
            "Number of params":483200000.0,
            "GFLOPs":648.5,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1087041,
            "title":"MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models",
            "url":"\/paper\/moat-alternating-mobile-convolution-and",
            "published":"2022-10-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/moat-alternating-mobile-convolution-and\/review\/?hl=71508"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":55918,
        "rank":32,
        "Model":"FD (CLIP ViT-L-336)",
        "mlmodel":{

        },
        "method_short":"FD ",
        "method_details":"CLIP ViT-L-336",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-27",
        "metrics":{
            "Top 1 Accuracy":"89.0%",
            "Number of params":"307M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":89.0,
            "Number of params":307000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1017290,
            "title":"Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation",
            "url":"\/paper\/contrastive-learning-rivals-masked-image",
            "published":"2022-05-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/contrastive-learning-rivals-masked-image\/review\/?hl=55918"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            },
            {
                "id":300,
                "name":"CLIP data ",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":86602,
        "rank":33,
        "Model":"Last Layer Tuning with Newton Step (ViT-G\/14))",
        "mlmodel":{

        },
        "method_short":"Last Layer Tuning with Newton Step ",
        "method_details":"ViT-G\/14",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-24",
        "metrics":{
            "Top 1 Accuracy":"88.9%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.9,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1117685,
            "title":"Differentially Private Image Classification from Features",
            "url":"\/paper\/differentially-private-image-classification",
            "published":"2022-11-24T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/differentially-private-image-classification\/review\/?hl=86602"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":40306,
        "rank":34,
        "Model":"TokenLearner L\/8 (24+11)",
        "mlmodel":{

        },
        "method_short":"TokenLearner L\/8 ",
        "method_details":"24+11",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-21",
        "metrics":{
            "Top 1 Accuracy":"88.87%",
            "Number of params":"460M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.87,
            "Number of params":460000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":821783,
            "title":"TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?",
            "url":"\/paper\/tokenlearner-what-can-8-learned-tokens-do-for",
            "published":"2021-06-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/tokenlearner-what-can-8-learned-tokens-do-for\/review\/?hl=40306"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":61436,
        "rank":35,
        "Model":"MaxViT-B (512res, JFT)",
        "mlmodel":{

        },
        "method_short":"MaxViT-B ",
        "method_details":"512res, JFT",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-04",
        "metrics":{
            "Top 1 Accuracy":"88.82%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.82,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":988369,
            "title":"MaxViT: Multi-Axis Vision Transformer",
            "url":"\/paper\/maxvit-multi-axis-vision-transformer",
            "published":"2022-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/maxvit-multi-axis-vision-transformer\/review\/?hl=61436"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":53598,
        "rank":36,
        "Model":"MViTv2-H (512 res, ImageNet-21k pretrain)",
        "mlmodel":{

        },
        "method_short":"MViTv2-H ",
        "method_details":"512 res, ImageNet-21k pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-02",
        "metrics":{
            "Top 1 Accuracy":"88.8%",
            "Number of params":"667M",
            "GFLOPs":"763.5",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.8,
            "Number of params":667000000.0,
            "GFLOPs":763.5,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":924692,
            "title":"MViTv2: Improved Multiscale Vision Transformers for Classification and Detection",
            "url":"\/paper\/improved-multiscale-vision-transformers-for",
            "published":"2021-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improved-multiscale-vision-transformers-for\/review\/?hl=53598"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":71569,
        "rank":37,
        "Model":"MaxViT-XL (512res, 21K)",
        "mlmodel":{

        },
        "method_short":"MaxViT-XL ",
        "method_details":"512res, 21K",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-04",
        "metrics":{
            "Top 1 Accuracy":"88.7%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.7,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":988369,
            "title":"MaxViT: Multi-Axis Vision Transformer",
            "url":"\/paper\/maxvit-multi-axis-vision-transformer",
            "published":"2022-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/maxvit-multi-axis-vision-transformer\/review\/?hl=71569"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":71562,
        "rank":38,
        "Model":"MaxViT-B (384res, JFT)",
        "mlmodel":{

        },
        "method_short":"MaxViT-B ",
        "method_details":"384res, JFT",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-04",
        "metrics":{
            "Top 1 Accuracy":"88.69%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.69,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":988369,
            "title":"MaxViT: Multi-Axis Vision Transformer",
            "url":"\/paper\/maxvit-multi-axis-vision-transformer",
            "published":"2022-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/maxvit-multi-axis-vision-transformer\/review\/?hl=71562"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":25440,
        "rank":39,
        "Model":"ALIGN (EfficientNet-L2)",
        "mlmodel":{

        },
        "method_short":"ALIGN ",
        "method_details":"EfficientNet-L2",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-11",
        "metrics":{
            "Top 1 Accuracy":"88.64%",
            "Number of params":"480M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.64,
            "Number of params":480000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":744362,
            "title":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision",
            "url":"\/paper\/scaling-up-visual-and-vision-language",
            "published":"2021-02-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-up-visual-and-vision-language\/review\/?hl=25440"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":247,
                "name":"ALIGN",
                "color":"#2771D3"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":23974,
        "rank":40,
        "Model":"EfficientNet-L2-475 (SAM)",
        "mlmodel":{

        },
        "method_short":"EfficientNet-L2-475 ",
        "method_details":"SAM",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-10-03",
        "metrics":{
            "Top 1 Accuracy":"88.61%",
            "Number of params":"480M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.61,
            "Number of params":480000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":225198,
            "title":"Sharpness-Aware Minimization for Efficiently Improving Generalization",
            "url":"\/paper\/sharpness-aware-minimization-for-efficiently-1",
            "published":"2020-10-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/sharpness-aware-minimization-for-efficiently-1\/review\/?hl=23974"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":100144,
        "rank":41,
        "Model":"ViT-B\/16 (384res, distilled from ViT-22B)",
        "mlmodel":{

        },
        "method_short":"ViT-B\/16 ",
        "method_details":"384res, distilled from ViT-22B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-04",
        "metrics":{
            "Top 1 Accuracy":"88.6%",
            "Number of params":"86M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.6,
            "Number of params":86000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1155994,
            "title":"Scaling Vision Transformers to 22 Billion Parameters",
            "url":"\/paper\/scaling-vision-transformers-to-22-billion",
            "published":"2023-02-10T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":36894,
        "rank":42,
        "Model":"BEiT-L (ViT; ImageNet-22K pretrain)",
        "mlmodel":{

        },
        "method_short":"BEiT-L ",
        "method_details":"ViT; ImageNet-22K pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-15",
        "metrics":{
            "Top 1 Accuracy":"88.60%",
            "Number of params":"331M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.6,
            "Number of params":331000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":818838,
            "title":"BEiT: BERT Pre-Training of Image Transformers",
            "url":"\/paper\/beit-bert-pre-training-of-image-transformers",
            "published":"2021-06-15T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":45645,
        "rank":43,
        "Model":"SWAG (ViT H\/14)",
        "mlmodel":{

        },
        "method_short":"SWAG ",
        "method_details":"ViT H\/14",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-20",
        "metrics":{
            "Top 1 Accuracy":"88.6%",
            "Number of params":"633.5M",
            "GFLOPs":"1018.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.6,
            "Number of params":633500000.0,
            "GFLOPs":1018.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":948291,
            "title":"Revisiting Weakly Supervised Pre-Training of Visual Perception Models",
            "url":"\/paper\/revisiting-weakly-supervised-pre-training-of",
            "published":"2022-01-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/revisiting-weakly-supervised-pre-training-of\/review\/?hl=45645"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":20349,
        "rank":44,
        "Model":"ViT-H\/14",
        "mlmodel":{

        },
        "method_short":"ViT-H\/14",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-10-22",
        "metrics":{
            "Top 1 Accuracy":"88.55%",
            "Number of params":"632M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.55,
            "Number of params":632000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":229828,
            "title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
            "url":"\/paper\/an-image-is-worth-16x16-words-transformers-1",
            "published":"2020-10-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/an-image-is-worth-16x16-words-transformers-1\/review\/?hl=20349"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":63447,
        "rank":45,
        "Model":"CoAtNet-3 @384",
        "mlmodel":{

        },
        "method_short":"CoAtNet-3 @384",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-09",
        "metrics":{
            "Top 1 Accuracy":"88.52%",
            "Number of params":"168M",
            "GFLOPs":"114",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.52,
            "Number of params":168000000.0,
            "GFLOPs":114.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":814553,
            "title":"CoAtNet: Marrying Convolution and Attention for All Data Sizes",
            "url":"\/paper\/coatnet-marrying-convolution-and-attention",
            "published":"2021-06-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/coatnet-marrying-convolution-and-attention\/review\/?hl=63447"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":71565,
        "rank":46,
        "Model":"MaxViT-XL (384res, 21K)",
        "mlmodel":{

        },
        "method_short":"MaxViT-XL ",
        "method_details":"384res, 21K",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-04",
        "metrics":{
            "Top 1 Accuracy":"88.51%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.51,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":988369,
            "title":"MaxViT: Multi-Axis Vision Transformer",
            "url":"\/paper\/maxvit-multi-axis-vision-transformer",
            "published":"2022-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/maxvit-multi-axis-vision-transformer\/review\/?hl=71565"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":96436,
        "rank":47,
        "Model":"mPLUG-2",
        "mlmodel":{

        },
        "method_short":"mPLUG-2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-01",
        "metrics":{
            "Top 1 Accuracy":"88.5%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.5,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1151002,
            "title":"mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video",
            "url":"\/paper\/mplug-2-a-modularized-multi-modal-foundation",
            "published":"2023-02-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mplug-2-a-modularized-multi-modal-foundation\/review\/?hl=96436"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":103939,
        "rank":48,
        "Model":"OpenCLIP ViT-H\/14",
        "mlmodel":{

        },
        "method_short":"OpenCLIP ViT-H\/14",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-14",
        "metrics":{
            "Top 1 Accuracy":"88.5%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.5,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1128601,
            "title":"Reproducible scaling laws for contrastive language-image learning",
            "url":"\/paper\/reproducible-scaling-laws-for-contrastive",
            "published":"2022-12-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/reproducible-scaling-laws-for-contrastive\/review\/?hl=103939"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":10363,
        "rank":49,
        "Model":"FixEfficientNet-L2",
        "mlmodel":{

        },
        "method_short":"FixEfficientNet-L2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-18",
        "metrics":{
            "Top 1 Accuracy":"88.5%",
            "Number of params":"480M",
            "GFLOPs":"585",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.5,
            "Number of params":480000000.0,
            "GFLOPs":585.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":187431,
            "title":"Fixing the train-test resolution discrepancy: FixEfficientNet",
            "url":"\/paper\/fixing-the-train-test-resolution-discrepancy-2",
            "published":"2020-03-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fixing-the-train-test-resolution-discrepancy-2\/review\/?hl=10363"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":48208,
        "rank":50,
        "Model":"ViTAE-H + MAE (448)",
        "mlmodel":{

        },
        "method_short":"ViTAE-H + MAE ",
        "method_details":"448",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-21",
        "metrics":{
            "Top 1 Accuracy":"88.5%",
            "Number of params":"644M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.5,
            "Number of params":644000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":964748,
            "title":"ViTAEv2: Vision Transformer Advanced by Exploring Inductive Bias for Image Recognition and Beyond",
            "url":"\/paper\/vitaev2-vision-transformer-advanced-by",
            "published":"2022-02-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vitaev2-vision-transformer-advanced-by\/review\/?hl=48208"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":71567,
        "rank":51,
        "Model":"MaxViT-L (512res, 21K)",
        "mlmodel":{

        },
        "method_short":"MaxViT-L ",
        "method_details":"512res, 21K",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-04",
        "metrics":{
            "Top 1 Accuracy":"88.46%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.46,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":988369,
            "title":"MaxViT: Multi-Axis Vision Transformer",
            "url":"\/paper\/maxvit-multi-axis-vision-transformer",
            "published":"2022-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/maxvit-multi-axis-vision-transformer\/review\/?hl=71567"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":53596,
        "rank":52,
        "Model":"MViTv2-L (384 res, ImageNet-21k pretrain)",
        "mlmodel":{

        },
        "method_short":"MViTv2-L ",
        "method_details":"384 res, ImageNet-21k pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-02",
        "metrics":{
            "Top 1 Accuracy":"88.4%",
            "Number of params":"218M",
            "GFLOPs":"140.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.4,
            "Number of params":218000000.0,
            "GFLOPs":140.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":924692,
            "title":"MViTv2: Improved Multiscale Vision Transformers for Classification and Detection",
            "url":"\/paper\/improved-multiscale-vision-transformers-for",
            "published":"2021-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improved-multiscale-vision-transformers-for\/review\/?hl=53596"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":9589,
        "rank":53,
        "Model":"NoisyStudent (EfficientNet-L2)",
        "mlmodel":{

        },
        "method_short":"NoisyStudent ",
        "method_details":"EfficientNet-L2",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-01-07",
        "metrics":{
            "Top 1 Accuracy":"88.4%",
            "Number of params":"480M",
            "GFLOPs":null,
            "Hardware Burden":"51800G",
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.4,
            "Number of params":480000000.0,
            "GFLOPs":null,
            "Hardware Burden":51800.0,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":170047,
            "title":"Self-training with Noisy Student improves ImageNet classification",
            "url":"\/paper\/self-training-with-noisy-student-improves",
            "published":"2019-11-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/self-training-with-noisy-student-improves\/review\/?hl=9589"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[
            {
                "url":"https:\/\/github.com\/facebookresearch\/AugLy\/blob\/main\/examples\/imagenet\/evaluate_robustness_imagenet_efficientnet_l2.ipynb"
            }
        ]
    },
    {
        "table_id":116,
        "row_id":71566,
        "rank":54,
        "Model":"MaxViT-B (512res, 21K)",
        "mlmodel":{

        },
        "method_short":"MaxViT-B ",
        "method_details":"512res, 21K",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-04",
        "metrics":{
            "Top 1 Accuracy":"88.38%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.38,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":988369,
            "title":"MaxViT: Multi-Axis Vision Transformer",
            "url":"\/paper\/maxvit-multi-axis-vision-transformer",
            "published":"2022-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/maxvit-multi-axis-vision-transformer\/review\/?hl=71566"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":89004,
        "rank":55,
        "Model":"Top-k DiffSortNets (EfficientNet-L2)",
        "mlmodel":{

        },
        "method_short":"Top-k DiffSortNets ",
        "method_details":"EfficientNet-L2",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-15",
        "metrics":{
            "Top 1 Accuracy":"88.37%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.37,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1027734,
            "title":"Differentiable Top-k Classification Learning",
            "url":"\/paper\/differentiable-top-k-classification-learning-1",
            "published":"2022-06-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/differentiable-top-k-classification-learning-1\/review\/?hl=89004"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            },
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":87709,
        "rank":56,
        "Model":"Adlik-ViT-SG+Swin_large+Convnext_xlarge(384)",
        "mlmodel":{

        },
        "method_short":"Adlik-ViT-SG+Swin_large+Convnext_xlarge",
        "method_details":"384",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-19",
        "metrics":{
            "Top 1 Accuracy":"88.36%",
            "Number of params":"1827M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.36,
            "Number of params":1827000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":942596,
            "title":"A ConvNet for the 2020s",
            "url":"\/paper\/a-convnet-for-the-2020s",
            "published":"2022-01-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/a-convnet-for-the-2020s\/review\/?hl=87709"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":34957,
        "rank":57,
        "Model":"V-MoE-H\/14 (Every-2)",
        "mlmodel":{

        },
        "method_short":"V-MoE-H\/14 ",
        "method_details":"Every-2",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-10",
        "metrics":{
            "Top 1 Accuracy":"88.36%",
            "Number of params":"7200M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.36,
            "Number of params":7200000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":816109,
            "title":"Scaling Vision with Sparse Mixture of Experts",
            "url":"\/paper\/scaling-vision-with-sparse-mixture-of-experts",
            "published":"2021-06-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-vision-with-sparse-mixture-of-experts\/review\/?hl=34957"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":71563,
        "rank":58,
        "Model":"MaxViT-L (384res, 21K)",
        "mlmodel":{

        },
        "method_short":"MaxViT-L ",
        "method_details":"384res, 21K",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-04",
        "metrics":{
            "Top 1 Accuracy":"88.32%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.32,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":988369,
            "title":"MaxViT: Multi-Axis Vision Transformer",
            "url":"\/paper\/maxvit-multi-axis-vision-transformer",
            "published":"2022-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/maxvit-multi-axis-vision-transformer\/review\/?hl=71563"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":101657,
        "rank":59,
        "Model":"Unicom (ViT-L\/14@336px) (Finetuned)",
        "mlmodel":{

        },
        "method_short":"Unicom ",
        "method_details":"ViT-L\/14@336px",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-12",
        "metrics":{
            "Top 1 Accuracy":"88.3",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.3,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1190273,
            "title":"Unicom: Universal and Compact Representation Learning for Image Retrieval",
            "url":"\/paper\/unicom-universal-and-compact-representation",
            "published":"2023-04-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/unicom-universal-and-compact-representation\/review\/?hl=101657"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":45189,
        "rank":60,
        "Model":"PeCo (ViT-H, 448)",
        "mlmodel":{

        },
        "method_short":"PeCo ",
        "method_details":"ViT-H, 448",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-24",
        "metrics":{
            "Top 1 Accuracy":"88.3%",
            "Number of params":"656M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.3,
            "Number of params":656000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":916192,
            "title":"PeCo: Perceptual Codebook for BERT Pre-training of Vision Transformers",
            "url":"\/paper\/peco-perceptual-codebook-for-bert-pre",
            "published":"2021-11-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/peco-perceptual-codebook-for-bert-pre\/review\/?hl=45189"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":71561,
        "rank":61,
        "Model":"MaxViT-B (384res, 21K)",
        "mlmodel":{

        },
        "method_short":"MaxViT-B ",
        "method_details":"384res, 21K",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-04",
        "metrics":{
            "Top 1 Accuracy":"88.24%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.24,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":988369,
            "title":"MaxViT: Multi-Axis Vision Transformer",
            "url":"\/paper\/maxvit-multi-axis-vision-transformer",
            "published":"2022-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/maxvit-multi-axis-vision-transformer\/review\/?hl=71561"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":34952,
        "rank":62,
        "Model":"V-MoE-H\/14 (Last-5)",
        "mlmodel":{

        },
        "method_short":"V-MoE-H\/14 ",
        "method_details":"Last-5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-10",
        "metrics":{
            "Top 1 Accuracy":"88.23%",
            "Number of params":"2700M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.23,
            "Number of params":2700000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":816109,
            "title":"Scaling Vision with Sparse Mixture of Experts",
            "url":"\/paper\/scaling-vision-with-sparse-mixture-of-experts",
            "published":"2021-06-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-vision-with-sparse-mixture-of-experts\/review\/?hl=34952"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":69824,
        "rank":63,
        "Model":"dBOT ViT-H (CLIP as Teacher)",
        "mlmodel":{

        },
        "method_short":"dBOT ViT-H ",
        "method_details":"CLIP as Teacher",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-08",
        "metrics":{
            "Top 1 Accuracy":"88.2%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.2,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1071401,
            "title":"Exploring Target Representations for Masked Autoencoders",
            "url":"\/paper\/exploring-target-representations-for-masked",
            "published":"2022-09-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/exploring-target-representations-for-masked\/review\/?hl=69824"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":300,
                "name":"CLIP data ",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":73460,
        "rank":64,
        "Model":"CAFormer-B36 (384 res, 21K)",
        "mlmodel":{

        },
        "method_short":"CAFormer-B36 ",
        "method_details":"384 res, 21K",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"88.1%",
            "Number of params":"99M",
            "GFLOPs":"72.2",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.1,
            "Number of params":99000000.0,
            "GFLOPs":72.2,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=73460"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":34942,
        "rank":65,
        "Model":"VIT-H\/14",
        "mlmodel":{

        },
        "method_short":"VIT-H\/14",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-10",
        "metrics":{
            "Top 1 Accuracy":"88.08%",
            "Number of params":"656M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.08,
            "Number of params":656000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":816109,
            "title":"Scaling Vision with Sparse Mixture of Experts",
            "url":"\/paper\/scaling-vision-with-sparse-mixture-of-experts",
            "published":"2021-06-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-vision-with-sparse-mixture-of-experts\/review\/?hl=34942"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":87342,
        "rank":66,
        "Model":"ViT-H@224 (cosub)",
        "mlmodel":{

        },
        "method_short":"ViT-H@224 ",
        "method_details":"cosub",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-09",
        "metrics":{
            "Top 1 Accuracy":"88.0%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.0,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1126263,
            "title":"Co-training $2^L$ Submodels for Visual Recognition",
            "url":"\/paper\/co-training-2-l-submodels-for-visual",
            "published":"2022-12-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/co-training-2-l-submodels-for-visual\/review\/?hl=87342"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":112833,
        "rank":67,
        "Model":"UniRepLKNet-XL++",
        "mlmodel":{

        },
        "method_short":"UniRepLKNet-XL++",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-27",
        "metrics":{
            "Top 1 Accuracy":"88%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.0,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1327838,
            "title":"UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition",
            "url":"\/paper\/unireplknet-a-universal-perception-large",
            "published":"2023-11-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/unireplknet-a-universal-perception-large\/review\/?hl=112833"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":102364,
        "rank":68,
        "Model":"InternImage-XL",
        "mlmodel":{

        },
        "method_short":"InternImage-XL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-10",
        "metrics":{
            "Top 1 Accuracy":"88%",
            "Number of params":"335M",
            "GFLOPs":"163",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.0,
            "Number of params":335000000.0,
            "GFLOPs":163.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1109218,
            "title":"InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions",
            "url":"\/paper\/internimage-exploring-large-scale-vision",
            "published":"2022-11-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/internimage-exploring-large-scale-vision\/review\/?hl=102364"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":53595,
        "rank":69,
        "Model":"MViTv2-H (mageNet-21k pretrain)",
        "mlmodel":{

        },
        "method_short":"MViTv2-H ",
        "method_details":"mageNet-21k pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-02",
        "metrics":{
            "Top 1 Accuracy":"88%",
            "Number of params":"667M",
            "GFLOPs":"120.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.0,
            "Number of params":667000000.0,
            "GFLOPs":120.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":924692,
            "title":"MViTv2: Improved Multiscale Vision Transformers for Classification and Detection",
            "url":"\/paper\/improved-multiscale-vision-transformers-for",
            "published":"2021-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improved-multiscale-vision-transformers-for\/review\/?hl=53595"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":31606,
        "rank":70,
        "Model":"Mixer-H\/14 (JFT-300M pre-train)",
        "mlmodel":{

        },
        "method_short":"Mixer-H\/14 ",
        "method_details":"JFT-300M pre-train",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-04",
        "metrics":{
            "Top 1 Accuracy":"87.94%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.94,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":793349,
            "title":"MLP-Mixer: An all-MLP Architecture for Vision",
            "url":"\/paper\/mlp-mixer-an-all-mlp-architecture-for-vision",
            "published":"2021-05-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mlp-mixer-an-all-mlp-architecture-for-vision\/review\/?hl=31606"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            },
            {
                "id":18,
                "name":"MLP",
                "color":"#ffae00"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":112832,
        "rank":71,
        "Model":"UniRepLKNet-L++",
        "mlmodel":{

        },
        "method_short":"UniRepLKNet-L++",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-27",
        "metrics":{
            "Top 1 Accuracy":"87.9%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.9,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1327838,
            "title":"UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition",
            "url":"\/paper\/unireplknet-a-universal-perception-large",
            "published":"2023-11-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/unireplknet-a-universal-perception-large\/review\/?hl=112832"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":69823,
        "rank":72,
        "Model":"dBOT ViT-L (CLIP as Teacher)",
        "mlmodel":{

        },
        "method_short":"dBOT ViT-L ",
        "method_details":"CLIP as Teacher",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-08",
        "metrics":{
            "Top 1 Accuracy":"87.8%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.8,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1071401,
            "title":"Exploring Target Representations for Masked Autoencoders",
            "url":"\/paper\/exploring-target-representations-for-masked",
            "published":"2022-09-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/exploring-target-representations-for-masked\/review\/?hl=69823"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":300,
                "name":"CLIP data ",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":99769,
        "rank":73,
        "Model":"MogaNet-XL (384res)",
        "mlmodel":{

        },
        "method_short":"MogaNet-XL ",
        "method_details":"384res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-07",
        "metrics":{
            "Top 1 Accuracy":"87.8%",
            "Number of params":"181M",
            "GFLOPs":"102",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.8,
            "Number of params":181000000.0,
            "GFLOPs":102.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1106694,
            "title":"Efficient Multi-order Gated Aggregation Network",
            "url":"\/paper\/efficient-multi-order-gated-aggregation",
            "published":"2022-11-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/efficient-multi-order-gated-aggregation\/review\/?hl=99769"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":75979,
        "rank":74,
        "Model":"VAN-B6 (22K, 384res)",
        "mlmodel":{

        },
        "method_short":"VAN-B6 ",
        "method_details":"22K, 384res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-20",
        "metrics":{
            "Top 1 Accuracy":"87.8%",
            "Number of params":"200M",
            "GFLOPs":"114.3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.8,
            "Number of params":200000000.0,
            "GFLOPs":114.3,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":964888,
            "title":"Visual Attention Network",
            "url":"\/paper\/visual-attention-network",
            "published":"2022-02-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/visual-attention-network\/review\/?hl=75979"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":63755,
        "rank":75,
        "Model":"RepLKNet-XL",
        "mlmodel":{

        },
        "method_short":"RepLKNet-XL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-13",
        "metrics":{
            "Top 1 Accuracy":"87.8%",
            "Number of params":"335M",
            "GFLOPs":"128.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.8,
            "Number of params":335000000.0,
            "GFLOPs":128.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":976188,
            "title":"Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs",
            "url":"\/paper\/scaling-up-your-kernels-to-31x31-revisiting",
            "published":"2022-03-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-up-your-kernels-to-31x31-revisiting\/review\/?hl=63755"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":45304,
        "rank":76,
        "Model":"ConvNeXt-XL (ImageNet-22k)",
        "mlmodel":{

        },
        "method_short":"ConvNeXt-XL ",
        "method_details":"ImageNet-22k",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-10",
        "metrics":{
            "Top 1 Accuracy":"87.8%",
            "Number of params":"350M",
            "GFLOPs":"179",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.8,
            "Number of params":350000000.0,
            "GFLOPs":179.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":942596,
            "title":"A ConvNet for the 2020s",
            "url":"\/paper\/a-convnet-for-the-2020s",
            "published":"2022-01-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/a-convnet-for-the-2020s\/review\/?hl=45304"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":44087,
        "rank":77,
        "Model":"MAE (ViT-H, 448)",
        "mlmodel":{

        },
        "method_short":"MAE ",
        "method_details":"ViT-H, 448",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-11",
        "metrics":{
            "Top 1 Accuracy":"87.8%",
            "Number of params":"656M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.8,
            "Number of params":656000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":908690,
            "title":"Masked Autoencoders Are Scalable Vision Learners",
            "url":"\/paper\/masked-autoencoders-are-scalable-vision",
            "published":"2021-11-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-autoencoders-are-scalable-vision\/review\/?hl=44087"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":20350,
        "rank":78,
        "Model":"ViT-L\/16",
        "mlmodel":{

        },
        "method_short":"ViT-L\/16",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-10-22",
        "metrics":{
            "Top 1 Accuracy":"87.76%",
            "Number of params":"307M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.76,
            "Number of params":307000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":229828,
            "title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
            "url":"\/paper\/an-image-is-worth-16x16-words-transformers-1",
            "published":"2020-10-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/an-image-is-worth-16x16-words-transformers-1\/review\/?hl=20350"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60810,
        "rank":79,
        "Model":"HorNet-L (GF)",
        "mlmodel":{

        },
        "method_short":"HorNet-L ",
        "method_details":"GF",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-28",
        "metrics":{
            "Top 1 Accuracy":"87.7%",
            "Number of params":null,
            "GFLOPs":"101.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.7,
            "Number of params":null,
            "GFLOPs":101.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1051716,
            "title":"HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions",
            "url":"\/paper\/hornet-efficient-high-order-spatial",
            "published":"2022-07-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/hornet-efficient-high-order-spatial\/review\/?hl=60810"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":102363,
        "rank":80,
        "Model":"InternImage-L",
        "mlmodel":{

        },
        "method_short":"InternImage-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-10",
        "metrics":{
            "Top 1 Accuracy":"87.7%",
            "Number of params":"223M",
            "GFLOPs":"108",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.7,
            "Number of params":223000000.0,
            "GFLOPs":108.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1109218,
            "title":"InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions",
            "url":"\/paper\/internimage-exploring-large-scale-vision",
            "published":"2022-11-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/internimage-exploring-large-scale-vision\/review\/?hl=102363"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":73463,
        "rank":81,
        "Model":"ConvFormer-B36 (384 res, 21K)",
        "mlmodel":{

        },
        "method_short":"ConvFormer-B36 ",
        "method_details":"384 res, 21K",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"87.6%",
            "Number of params":"100M",
            "GFLOPs":"66.5",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.6,
            "Number of params":100000000.0,
            "GFLOPs":66.5,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=73463"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":9478,
        "rank":82,
        "Model":"BiT-L (ResNet)",
        "mlmodel":{

        },
        "method_short":"BiT-L ",
        "method_details":"ResNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-12-24",
        "metrics":{
            "Top 1 Accuracy":"87.54%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.54,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":178162,
            "title":"Big Transfer (BiT): General Visual Representation Learning",
            "url":"\/paper\/large-scale-learning-of-general-visual",
            "published":"2019-12-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/large-scale-learning-of-general-visual\/review\/?hl=9478"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":45188,
        "rank":83,
        "Model":"PeCo (ViT-H, 224)",
        "mlmodel":{

        },
        "method_short":"PeCo ",
        "method_details":"ViT-H, 224",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-24",
        "metrics":{
            "Top 1 Accuracy":"87.5%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.5,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":916192,
            "title":"PeCo: Perceptual Codebook for BERT Pre-training of Vision Transformers",
            "url":"\/paper\/peco-perceptual-codebook-for-bert-pre",
            "published":"2021-11-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/peco-perceptual-codebook-for-bert-pre\/review\/?hl=45188"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":87343,
        "rank":84,
        "Model":"ViT-L@224 (cosub)",
        "mlmodel":{

        },
        "method_short":"ViT-L@224 ",
        "method_details":"cosub",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-09",
        "metrics":{
            "Top 1 Accuracy":"87.5%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.5,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1126263,
            "title":"Co-training $2^L$ Submodels for Visual Recognition",
            "url":"\/paper\/co-training-2-l-submodels-for-visual",
            "published":"2022-12-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/co-training-2-l-submodels-for-visual\/review\/?hl=87343"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":88310,
        "rank":85,
        "Model":"CAFormer-M36 (384 res, 21K)",
        "mlmodel":{

        },
        "method_short":"CAFormer-M36 ",
        "method_details":"384 res, 21K",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"87.5%",
            "Number of params":"56M",
            "GFLOPs":"42",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.5,
            "Number of params":56000000.0,
            "GFLOPs":42.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=88310"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":36295,
        "rank":86,
        "Model":"CSWin-L (384 res,ImageNet-22k pretrain)",
        "mlmodel":{

        },
        "method_short":"CSWin-L ",
        "method_details":"384 res,ImageNet-22k pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-01",
        "metrics":{
            "Top 1 Accuracy":"87.5%",
            "Number of params":"173M",
            "GFLOPs":"96.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.5,
            "Number of params":173000000.0,
            "GFLOPs":96.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":828741,
            "title":"CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows",
            "url":"\/paper\/cswin-transformer-a-general-vision",
            "published":"2021-07-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/cswin-transformer-a-general-vision\/review\/?hl=36295"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60763,
        "rank":87,
        "Model":"DaViT-L (ImageNet-22k)",
        "mlmodel":{

        },
        "method_short":"DaViT-L ",
        "method_details":"ImageNet-22k",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-07",
        "metrics":{
            "Top 1 Accuracy":"87.5%",
            "Number of params":"196.8M",
            "GFLOPs":"103",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.5,
            "Number of params":196800000.0,
            "GFLOPs":103.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":990738,
            "title":"DaViT: Dual Attention Vision Transformers",
            "url":"\/paper\/davit-dual-attention-vision-transformers",
            "published":"2022-04-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/davit-dual-attention-vision-transformers\/review\/?hl=60763"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":73672,
        "rank":88,
        "Model":"DiNAT-Large (11x11ks; 384res; Pretrained on IN22K@224)",
        "mlmodel":{

        },
        "method_short":"DiNAT-Large ",
        "method_details":"11x11ks; 384res; Pretrained on IN22K@224",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-29",
        "metrics":{
            "Top 1 Accuracy":"87.5%",
            "Number of params":"200M",
            "GFLOPs":"92.4",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.5,
            "Number of params":200000000.0,
            "GFLOPs":92.4,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1082646,
            "title":"Dilated Neighborhood Attention Transformer",
            "url":"\/paper\/dilated-neighborhood-attention-transformer",
            "published":"2022-09-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dilated-neighborhood-attention-transformer\/review\/?hl=73672"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":34947,
        "rank":89,
        "Model":"V-MoE-L\/16 (Every-2)",
        "mlmodel":{

        },
        "method_short":"V-MoE-L\/16 ",
        "method_details":"Every-2",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-10",
        "metrics":{
            "Top 1 Accuracy":"87.41%",
            "Number of params":"3400M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.41,
            "Number of params":3400000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":816109,
            "title":"Scaling Vision with Sparse Mixture of Experts",
            "url":"\/paper\/scaling-vision-with-sparse-mixture-of-experts",
            "published":"2021-06-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-vision-with-sparse-mixture-of-experts\/review\/?hl=34947"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":73671,
        "rank":90,
        "Model":"DiNAT-Large (384x384; Pretrained on ImageNet-22K @ 224x224)",
        "mlmodel":{

        },
        "method_short":"DiNAT-Large ",
        "method_details":"384x384; Pretrained on ImageNet-22K @ 224x224",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-29",
        "metrics":{
            "Top 1 Accuracy":"87.4%",
            "Number of params":null,
            "GFLOPs":"89.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.4,
            "Number of params":null,
            "GFLOPs":89.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1082646,
            "title":"Dilated Neighborhood Attention Transformer",
            "url":"\/paper\/dilated-neighborhood-attention-transformer",
            "published":"2022-09-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dilated-neighborhood-attention-transformer\/review\/?hl=73671"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":105283,
        "rank":91,
        "Model":"data2vec 2.0",
        "mlmodel":{

        },
        "method_short":"data2vec 2.0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-14",
        "metrics":{
            "Top 1 Accuracy":"87.4%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.4,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1128901,
            "title":"Efficient Self-supervised Learning with Contextualized Target Representations for Vision, Speech and Language",
            "url":"\/paper\/efficient-self-supervised-learning-with",
            "published":"2022-12-14T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":112831,
        "rank":92,
        "Model":"UniRepLKNet-B++",
        "mlmodel":{

        },
        "method_short":"UniRepLKNet-B++",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-27",
        "metrics":{
            "Top 1 Accuracy":"87.4%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.4,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1327838,
            "title":"UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition",
            "url":"\/paper\/unireplknet-a-universal-perception-large",
            "published":"2023-11-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/unireplknet-a-universal-perception-large\/review\/?hl=112831"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":88311,
        "rank":93,
        "Model":"CAFormer-B36 (224 res, 21K)",
        "mlmodel":{

        },
        "method_short":"CAFormer-B36 ",
        "method_details":"224 res, 21K",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"87.4%",
            "Number of params":"99M",
            "GFLOPs":"23.2",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.4,
            "Number of params":99000000.0,
            "GFLOPs":23.2,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=88311"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60742,
        "rank":94,
        "Model":"UniNet-B6",
        "mlmodel":{

        },
        "method_short":"UniNet-B6",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-12",
        "metrics":{
            "Top 1 Accuracy":"87.4%",
            "Number of params":"117M",
            "GFLOPs":"51",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.4,
            "Number of params":117000000.0,
            "GFLOPs":51.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1042502,
            "title":"UniNet: Unified Architecture Search with Convolution, Transformer, and MLP",
            "url":"\/paper\/uninet-unified-architecture-search-with-1",
            "published":"2022-07-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/uninet-unified-architecture-search-with-1\/review\/?hl=60742"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":73674,
        "rank":95,
        "Model":"DiNAT_s-Large (384res; Pretrained on IN22K@224)",
        "mlmodel":{

        },
        "method_short":"DiNAT_s-Large ",
        "method_details":"384res; Pretrained on IN22K@224",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-29",
        "metrics":{
            "Top 1 Accuracy":"87.4%",
            "Number of params":"197M",
            "GFLOPs":"101.5",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.4,
            "Number of params":197000000.0,
            "GFLOPs":101.5,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1082646,
            "title":"Dilated Neighborhood Attention Transformer",
            "url":"\/paper\/dilated-neighborhood-attention-transformer",
            "published":"2022-09-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dilated-neighborhood-attention-transformer\/review\/?hl=73674"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":28884,
        "rank":96,
        "Model":"Swin-L (384 res, ImageNet-22k pretrain)",
        "mlmodel":{

        },
        "method_short":"Swin-L ",
        "method_details":"384 res, ImageNet-22k pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-25",
        "metrics":{
            "Top 1 Accuracy":"87.3%",
            "Number of params":"197M",
            "GFLOPs":"103.9",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.3,
            "Number of params":197000000.0,
            "GFLOPs":103.9,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":757245,
            "title":"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
            "url":"\/paper\/swin-transformer-hierarchical-vision",
            "published":"2021-03-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/swin-transformer-hierarchical-vision\/review\/?hl=28884"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":55835,
        "rank":97,
        "Model":"VOLO-D5+HAT",
        "mlmodel":{

        },
        "method_short":"VOLO-D5+HAT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-03",
        "metrics":{
            "Top 1 Accuracy":"87.3%",
            "Number of params":"295.5M",
            "GFLOPs":"412",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.3,
            "Number of params":295500000.0,
            "GFLOPs":412.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":988272,
            "title":"Improving Vision Transformers by Revisiting High-frequency Components",
            "url":"\/paper\/improving-vision-transformers-by-revisiting",
            "published":"2022-04-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improving-vision-transformers-by-revisiting\/review\/?hl=55835"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":47814,
        "rank":98,
        "Model":"EfficientNetV2 (PolyLoss)",
        "mlmodel":{

        },
        "method_short":"EfficientNetV2 ",
        "method_details":"PolyLoss",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-26",
        "metrics":{
            "Top 1 Accuracy":"87.2%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.2,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1000670,
            "title":"PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions",
            "url":"\/paper\/polyloss-a-polynomial-expansion-perspective-1",
            "published":"2022-04-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/polyloss-a-polynomial-expansion-perspective-1\/review\/?hl=47814"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":44846,
        "rank":99,
        "Model":"ELSA-VOLO-D5 (512512)",
        "mlmodel":{

        },
        "method_short":"ELSA-VOLO-D5 ",
        "method_details":"512*512",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-23",
        "metrics":{
            "Top 1 Accuracy":"87.2%",
            "Number of params":"298M",
            "GFLOPs":"437",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.2,
            "Number of params":298000000.0,
            "GFLOPs":437.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":932838,
            "title":"ELSA: Enhanced Local Self-Attention for Vision Transformer",
            "url":"\/paper\/elsa-enhanced-local-self-attention-for-vision",
            "published":"2021-12-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/elsa-enhanced-local-self-attention-for-vision\/review\/?hl=44846"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":55712,
        "rank":100,
        "Model":"Bamboo (Bamboo-H)",
        "mlmodel":{

        },
        "method_short":"Bamboo ",
        "method_details":"Bamboo-H",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-21",
        "metrics":{
            "Top 1 Accuracy":"87.1",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.1,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1014071,
            "title":"A Study on Transformer Configuration and Training Objective",
            "url":"\/paper\/deeper-vs-wider-a-revisit-of-transformer",
            "published":"2022-05-21T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/deeper-vs-wider-a-revisit-of-transformer\/review\/?hl=55712"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":87347,
        "rank":101,
        "Model":"Swin-L@224 (cosub)",
        "mlmodel":{

        },
        "method_short":"Swin-L@224 ",
        "method_details":"cosub",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-09",
        "metrics":{
            "Top 1 Accuracy":"87.1%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.1,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1126263,
            "title":"Co-training $2^L$ Submodels for Visual Recognition",
            "url":"\/paper\/co-training-2-l-submodels-for-visual",
            "published":"2022-12-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/co-training-2-l-submodels-for-visual\/review\/?hl=87347"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":10364,
        "rank":102,
        "Model":"FixEfficientNet-B7",
        "mlmodel":{

        },
        "method_short":"FixEfficientNet-B7",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-18",
        "metrics":{
            "Top 1 Accuracy":"87.1%",
            "Number of params":"66M",
            "GFLOPs":" 82",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.1,
            "Number of params":66000000.0,
            "GFLOPs":82.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":187431,
            "title":"Fixing the train-test resolution discrepancy: FixEfficientNet",
            "url":"\/paper\/fixing-the-train-test-resolution-discrepancy-2",
            "published":"2020-03-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fixing-the-train-test-resolution-discrepancy-2\/review\/?hl=10364"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":53383,
        "rank":103,
        "Model":"FAN-L-Hybrid++",
        "mlmodel":{

        },
        "method_short":"FAN-L-Hybrid++",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-26",
        "metrics":{
            "Top 1 Accuracy":"87.1%",
            "Number of params":"76.8M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.1,
            "Number of params":76800000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":999980,
            "title":"Understanding The Robustness in Vision Transformers",
            "url":"\/paper\/understanding-the-robustness-in-vision",
            "published":"2022-04-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/understanding-the-robustness-in-vision\/review\/?hl=53383"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":61470,
        "rank":104,
        "Model":"SwinV2-B",
        "mlmodel":{

        },
        "method_short":"SwinV2-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-18",
        "metrics":{
            "Top 1 Accuracy":"87.1%",
            "Number of params":"88M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.1,
            "Number of params":88000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":912369,
            "title":"Swin Transformer V2: Scaling Up Capacity and Resolution",
            "url":"\/paper\/swin-transformer-v2-scaling-up-capacity-and",
            "published":"2021-11-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/swin-transformer-v2-scaling-up-capacity-and\/review\/?hl=61470"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":35495,
        "rank":105,
        "Model":"VOLO-D5",
        "mlmodel":{

        },
        "method_short":"VOLO-D5",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-24",
        "metrics":{
            "Top 1 Accuracy":"87.1%",
            "Number of params":"296M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.1,
            "Number of params":296000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":824294,
            "title":"VOLO: Vision Outlooker for Visual Recognition",
            "url":"\/paper\/volo-vision-outlooker-for-visual-recognition",
            "published":"2021-06-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/volo-vision-outlooker-for-visual-recognition\/review\/?hl=35495"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":44966,
        "rank":106,
        "Model":"PatchConvNet-L120-21k-384",
        "mlmodel":{

        },
        "method_short":"PatchConvNet-L120-21k-384",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-27",
        "metrics":{
            "Top 1 Accuracy":"87.1%",
            "Number of params":"334.3M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.1,
            "Number of params":334300000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":933338,
            "title":"Augmenting Convolutional networks with attention-based aggregation",
            "url":"\/paper\/augmenting-convolutional-networks-with",
            "published":"2021-12-27T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":167,
                "name":"PatchConvnet",
                "color":"#e0edd4"
            },
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":35379,
        "rank":107,
        "Model":"16-TokenLearner B\/16 (21)",
        "mlmodel":{

        },
        "method_short":"16-TokenLearner B\/16 ",
        "method_details":"21",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-21",
        "metrics":{
            "Top 1 Accuracy":"87.07%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.07,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":821783,
            "title":"TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?",
            "url":"\/paper\/tokenlearner-what-can-8-learned-tokens-do-for",
            "published":"2021-06-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/tokenlearner-what-can-8-learned-tokens-do-for\/review\/?hl=35379"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":68557,
        "rank":108,
        "Model":"MAE+DAT (ViT-H)",
        "mlmodel":{

        },
        "method_short":"MAE+DAT ",
        "method_details":"ViT-H",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-16",
        "metrics":{
            "Top 1 Accuracy":"87.02%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.02,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1075790,
            "title":"Enhance the Visual Representation via Discrete Adversarial Training",
            "url":"\/paper\/enhance-the-visual-representation-via",
            "published":"2022-09-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/enhance-the-visual-representation-via\/review\/?hl=68557"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60741,
        "rank":109,
        "Model":"UniNet-B5",
        "mlmodel":{

        },
        "method_short":"UniNet-B5",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-12",
        "metrics":{
            "Top 1 Accuracy":"87%",
            "Number of params":"72.9M",
            "GFLOPs":"20.4",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.0,
            "Number of params":72900000.0,
            "GFLOPs":20.4,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1042502,
            "title":"UniNet: Unified Architecture Search with Convolution, Transformer, and MLP",
            "url":"\/paper\/uninet-unified-architecture-search-with-1",
            "published":"2022-07-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/uninet-unified-architecture-search-with-1\/review\/?hl=60741"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":75978,
        "rank":110,
        "Model":"VAN-B5 (22K, 384res)",
        "mlmodel":{

        },
        "method_short":"VAN-B5 ",
        "method_details":"22K, 384res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-20",
        "metrics":{
            "Top 1 Accuracy":"87%",
            "Number of params":"90M",
            "GFLOPs":"50.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.0,
            "Number of params":90000000.0,
            "GFLOPs":50.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":964888,
            "title":"Visual Attention Network",
            "url":"\/paper\/visual-attention-network",
            "published":"2022-02-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/visual-attention-network\/review\/?hl=75978"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":73462,
        "rank":111,
        "Model":"ConvFormer-B36 (224 res, 21K)",
        "mlmodel":{

        },
        "method_short":"ConvFormer-B36 ",
        "method_details":"224 res, 21K",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"87.0%",
            "Number of params":"100M",
            "GFLOPs":"22.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.0,
            "Number of params":100000000.0,
            "GFLOPs":22.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=73462"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":44086,
        "rank":112,
        "Model":"MAE (ViT-H)",
        "mlmodel":{

        },
        "method_short":"MAE ",
        "method_details":"ViT-H",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-11",
        "metrics":{
            "Top 1 Accuracy":"86.9%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.9,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":908690,
            "title":"Masked Autoencoders Are Scalable Vision Learners",
            "url":"\/paper\/masked-autoencoders-are-scalable-vision",
            "published":"2021-11-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-autoencoders-are-scalable-vision\/review\/?hl=44086"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":104380,
        "rank":113,
        "Model":"Hiera-H",
        "mlmodel":{

        },
        "method_short":"Hiera-H",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-06-01",
        "metrics":{
            "Top 1 Accuracy":"86.9%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.9,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1221231,
            "title":"Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles",
            "url":"\/paper\/hiera-a-hierarchical-vision-transformer",
            "published":"2023-06-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/hiera-a-hierarchical-vision-transformer\/review\/?hl=104380"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":88306,
        "rank":114,
        "Model":"CAFormer-S36 (384 res, 21K)",
        "mlmodel":{

        },
        "method_short":"CAFormer-S36 ",
        "method_details":"384 res, 21K",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"86.9%",
            "Number of params":"39M",
            "GFLOPs":"26.0",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.9,
            "Number of params":39000000.0,
            "GFLOPs":26.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=88306"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":88308,
        "rank":115,
        "Model":"ConvFormer-M36 (384 res, 21K)",
        "mlmodel":{

        },
        "method_short":"ConvFormer-M36 ",
        "method_details":"384 res, 21K",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"86.9%",
            "Number of params":"57M",
            "GFLOPs":"37.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.9,
            "Number of params":57000000.0,
            "GFLOPs":37.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=88308"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":11196,
        "rank":116,
        "Model":"NoisyStudent (EfficientNet-B7)",
        "mlmodel":{

        },
        "method_short":"NoisyStudent ",
        "method_details":"EfficientNet-B7",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-11",
        "metrics":{
            "Top 1 Accuracy":"86.9%",
            "Number of params":"66M",
            "GFLOPs":"37",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.9,
            "Number of params":66000000.0,
            "GFLOPs":37.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":170047,
            "title":"Self-training with Noisy Student improves ImageNet classification",
            "url":"\/paper\/self-training-with-noisy-student-improves",
            "published":"2019-11-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/self-training-with-noisy-student-improves\/review\/?hl=11196"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60762,
        "rank":117,
        "Model":"DaViT-B (ImageNet-22k)",
        "mlmodel":{

        },
        "method_short":"DaViT-B ",
        "method_details":"ImageNet-22k",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-07",
        "metrics":{
            "Top 1 Accuracy":"86.9%",
            "Number of params":"87.9M",
            "GFLOPs":"46.4",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.9,
            "Number of params":87900000.0,
            "GFLOPs":46.4,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":990738,
            "title":"DaViT: Dual Attention Vision Transformers",
            "url":"\/paper\/davit-dual-attention-vision-transformers",
            "published":"2022-04-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/davit-dual-attention-vision-transformers\/review\/?hl=60762"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29230,
        "rank":118,
        "Model":"EfficientNetV2-L (21k)",
        "mlmodel":{

        },
        "method_short":"EfficientNetV2-L ",
        "method_details":"21k",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-01",
        "metrics":{
            "Top 1 Accuracy":"86.8%",
            "Number of params":"121M",
            "GFLOPs":"53",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.8,
            "Number of params":121000000.0,
            "GFLOPs":53.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":773495,
            "title":"EfficientNetV2: Smaller Models and Faster Training",
            "url":"\/paper\/efficientnetv2-smaller-models-and-faster",
            "published":"2021-04-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/efficientnetv2-smaller-models-and-faster\/review\/?hl=29230"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":35496,
        "rank":119,
        "Model":"VOLO-D4",
        "mlmodel":{

        },
        "method_short":"VOLO-D4",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-24",
        "metrics":{
            "Top 1 Accuracy":"86.8%",
            "Number of params":"193M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.8,
            "Number of params":193000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":824294,
            "title":"VOLO: Vision Outlooker for Visual Recognition",
            "url":"\/paper\/volo-vision-outlooker-for-visual-recognition",
            "published":"2021-06-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/volo-vision-outlooker-for-visual-recognition\/review\/?hl=35496"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":33897,
        "rank":120,
        "Model":"NFNet-F5 w\/ SAM w\/ augmult=16",
        "mlmodel":{

        },
        "method_short":"NFNet-F5 w\/ SAM w\/ augmult=16",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-27",
        "metrics":{
            "Top 1 Accuracy":"86.78%",
            "Number of params":"377.2M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.78,
            "Number of params":377200000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":806611,
            "title":"Drawing Multiple Augmentation Samples Per Image During Training Efficiently Decreases Test Error",
            "url":"\/paper\/drawing-multiple-augmentation-samples-per",
            "published":"2021-05-27T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/drawing-multiple-augmentation-samples-per\/review\/?hl=33897"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":70807,
        "rank":121,
        "Model":"\u00b52Net (ViT-L\/16)",
        "mlmodel":{

        },
        "method_short":"\u00b52Net ",
        "method_details":"ViT-L\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-25",
        "metrics":{
            "Top 1 Accuracy":"86.74%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.74,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1015923,
            "title":"An Evolutionary Approach to Dynamic Introduction of Tasks in Large-scale Multitask Learning Systems",
            "url":"\/paper\/an-evolutionary-approach-to-dynamic",
            "published":"2022-05-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/an-evolutionary-approach-to-dynamic\/review\/?hl=70807"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":52545,
        "rank":122,
        "Model":"ViT-B @384 (DeiT III, 21k)",
        "mlmodel":{

        },
        "method_short":"ViT-B @384 ",
        "method_details":"DeiT III, 21k",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-14",
        "metrics":{
            "Top 1 Accuracy":"86.7%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.7,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":994466,
            "title":"DeiT III: Revenge of the ViT",
            "url":"\/paper\/deit-iii-revenge-of-the-vit",
            "published":"2022-04-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deit-iii-revenge-of-the-vit\/review\/?hl=52545"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60633,
        "rank":123,
        "Model":"MaxViT-L (512res)",
        "mlmodel":{

        },
        "method_short":"MaxViT-L ",
        "method_details":"512res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-04",
        "metrics":{
            "Top 1 Accuracy":"86.7%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.7,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":988369,
            "title":"MaxViT: Multi-Axis Vision Transformer",
            "url":"\/paper\/maxvit-multi-axis-vision-transformer",
            "published":"2022-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/maxvit-multi-axis-vision-transformer\/review\/?hl=60633"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":18577,
        "rank":124,
        "Model":"FixEfficientNet-B6",
        "mlmodel":{

        },
        "method_short":"FixEfficientNet-B6",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-18",
        "metrics":{
            "Top 1 Accuracy":"86.7%",
            "Number of params":"43M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.7,
            "Number of params":43000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":187431,
            "title":"Fixing the train-test resolution discrepancy: FixEfficientNet",
            "url":"\/paper\/fixing-the-train-test-resolution-discrepancy-2",
            "published":"2020-03-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fixing-the-train-test-resolution-discrepancy-2\/review\/?hl=18577"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":71507,
        "rank":125,
        "Model":"MOAT-3 1K only",
        "mlmodel":{

        },
        "method_short":"MOAT-3 1K only",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-04",
        "metrics":{
            "Top 1 Accuracy":"86.7%",
            "Number of params":"190M",
            "GFLOPs":"271",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.7,
            "Number of params":190000000.0,
            "GFLOPs":271.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1087041,
            "title":"MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models",
            "url":"\/paper\/moat-alternating-mobile-convolution-and",
            "published":"2022-10-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/moat-alternating-mobile-convolution-and\/review\/?hl=71507"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":81779,
        "rank":126,
        "Model":"Heinsen Routing + BEiT-large 16 224",
        "mlmodel":{

        },
        "method_short":"Heinsen Routing + BEiT-large 16 224",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-20",
        "metrics":{
            "Top 1 Accuracy":"86.7%",
            "Number of params":"312.8M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.7,
            "Number of params":312800000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1116016,
            "title":"An Algorithm for Routing Vectors in Sequences",
            "url":"\/paper\/an-algorithm-for-routing-vectors-in-sequences",
            "published":"2022-11-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/an-algorithm-for-routing-vectors-in-sequences\/review\/?hl=81779"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":64411,
        "rank":127,
        "Model":"CLCNet (S:ViT+D:EffNet-B7) (retrain)",
        "mlmodel":{

        },
        "method_short":"CLCNet ",
        "method_details":"S:ViT+D:EffNet-B7",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-19",
        "metrics":{
            "Top 1 Accuracy":"86.61%",
            "Number of params":null,
            "GFLOPs":"51.93",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.61,
            "Number of params":null,
            "GFLOPs":51.93,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1012740,
            "title":"CLCNet: Rethinking of Ensemble Modeling with Classification Confidence Network",
            "url":"\/paper\/clcnet-rethinking-of-ensemble-modeling-with",
            "published":"2022-05-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/clcnet-rethinking-of-ensemble-modeling-with\/review\/?hl=64411"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":88309,
        "rank":128,
        "Model":"CAFormer-M36 (224 res, 21K)",
        "mlmodel":{

        },
        "method_short":"CAFormer-M36 ",
        "method_details":"224 res, 21K",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"86.6%",
            "Number of params":"56M",
            "GFLOPs":"13.2",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.6,
            "Number of params":56000000.0,
            "GFLOPs":13.2,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=88309"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":75976,
        "rank":129,
        "Model":"VAN-B4 (22K, 384res)",
        "mlmodel":{

        },
        "method_short":"VAN-B4 ",
        "method_details":"22K, 384res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-20",
        "metrics":{
            "Top 1 Accuracy":"86.6%",
            "Number of params":"60M",
            "GFLOPs":"35.9",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.6,
            "Number of params":60000000.0,
            "GFLOPs":35.9,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":964888,
            "title":"Visual Attention Network",
            "url":"\/paper\/visual-attention-network",
            "published":"2022-02-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/visual-attention-network\/review\/?hl=75976"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":52585,
        "rank":130,
        "Model":"data2vec (ViT-H)",
        "mlmodel":{

        },
        "method_short":"data2vec ",
        "method_details":"ViT-H",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-07",
        "metrics":{
            "Top 1 Accuracy":"86.6%",
            "Number of params":"656M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.6,
            "Number of params":656000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":957898,
            "title":"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",
            "url":"\/paper\/data2vec-a-general-framework-for-self-1",
            "published":"2022-02-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/data2vec-a-general-framework-for-self-1\/review\/?hl=52585"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":73673,
        "rank":131,
        "Model":"DiNAT_s-Large (224x224; Pretrained on ImageNet-22K @ 224x224)",
        "mlmodel":{

        },
        "method_short":"DiNAT_s-Large ",
        "method_details":"224x224; Pretrained on ImageNet-22K @ 224x224",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-29",
        "metrics":{
            "Top 1 Accuracy":"86.5%",
            "Number of params":null,
            "GFLOPs":"34.5",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.5,
            "Number of params":null,
            "GFLOPs":34.5,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1082646,
            "title":"Dilated Neighborhood Attention Transformer",
            "url":"\/paper\/dilated-neighborhood-attention-transformer",
            "published":"2022-09-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dilated-neighborhood-attention-transformer\/review\/?hl=73673"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":76000,
        "rank":132,
        "Model":"MKD ViT-L",
        "mlmodel":{

        },
        "method_short":"MKD ViT-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-16",
        "metrics":{
            "Top 1 Accuracy":"86.5%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.5,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":962993,
            "title":"Meta Knowledge Distillation",
            "url":"\/paper\/meta-knowledge-distillation",
            "published":"2022-02-16T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/meta-knowledge-distillation\/review\/?hl=76000"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":326,
                "name":"Teacher-22k",
                "color":"#a96800"
            },
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60264,
        "rank":133,
        "Model":"TinyViT-21M-512-distill (512 res, 21k)",
        "mlmodel":{

        },
        "method_short":"TinyViT-21M-512-distill ",
        "method_details":"512 res, 21k",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-21",
        "metrics":{
            "Top 1 Accuracy":"86.5%",
            "Number of params":"21M",
            "GFLOPs":"27.0",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.5,
            "Number of params":21000000.0,
            "GFLOPs":27.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1047551,
            "title":"TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
            "url":"\/paper\/tinyvit-fast-pretraining-distillation-for",
            "published":"2022-07-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/tinyvit-fast-pretraining-distillation-for\/review\/?hl=60264"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":44965,
        "rank":134,
        "Model":"PatchConvNet-B60-21k-384",
        "mlmodel":{

        },
        "method_short":"PatchConvNet-B60-21k-384",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-27",
        "metrics":{
            "Top 1 Accuracy":"86.5%",
            "Number of params":"99.4M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.5,
            "Number of params":99400000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":933338,
            "title":"Augmenting Convolutional networks with attention-based aggregation",
            "url":"\/paper\/augmenting-convolutional-networks-with",
            "published":"2021-12-27T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":167,
                "name":"PatchConvnet",
                "color":"#e0edd4"
            },
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29676,
        "rank":135,
        "Model":"CaiT-M-48-448",
        "mlmodel":{

        },
        "method_short":"CaiT-M-48-448",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-31",
        "metrics":{
            "Top 1 Accuracy":"86.5%",
            "Number of params":"438M",
            "GFLOPs":"377.3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.5,
            "Number of params":438000000.0,
            "GFLOPs":377.3,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":772635,
            "title":"Going deeper with Image Transformers",
            "url":"\/paper\/going-deeper-with-image-transformers",
            "published":"2021-03-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/going-deeper-with-image-transformers\/review\/?hl=29676"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":25383,
        "rank":136,
        "Model":"NFNet-F6 w\/ SAM",
        "mlmodel":{

        },
        "method_short":"NFNet-F6 w\/ SAM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-11",
        "metrics":{
            "Top 1 Accuracy":"86.5%",
            "Number of params":"438.4M",
            "GFLOPs":"377.28",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.5,
            "Number of params":438400000.0,
            "GFLOPs":377.28,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":744370,
            "title":"High-Performance Large-Scale Image Recognition Without Normalization",
            "url":"\/paper\/high-performance-large-scale-image",
            "published":"2021-02-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/high-performance-large-scale-image\/review\/?hl=25383"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":64413,
        "rank":137,
        "Model":"CLCNet (S:ViT+D:VOLO-D3) (retrain)",
        "mlmodel":{

        },
        "method_short":"CLCNet ",
        "method_details":"S:ViT+D:VOLO-D3",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-19",
        "metrics":{
            "Top 1 Accuracy":"86.46%",
            "Number of params":null,
            "GFLOPs":"57.46",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.46,
            "Number of params":null,
            "GFLOPs":57.46,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1012740,
            "title":"CLCNet: Rethinking of Ensemble Modeling with Classification Confidence Network",
            "url":"\/paper\/clcnet-rethinking-of-ensemble-modeling-with",
            "published":"2022-05-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/clcnet-rethinking-of-ensemble-modeling-with\/review\/?hl=64413"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":64412,
        "rank":138,
        "Model":"CLCNet (S:ConvNeXt-L+D:EffNet-B7) (retrain)",
        "mlmodel":{

        },
        "method_short":"CLCNet ",
        "method_details":"S:ConvNeXt-L+D:EffNet-B7",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-19",
        "metrics":{
            "Top 1 Accuracy":"86.42%",
            "Number of params":null,
            "GFLOPs":"45.43",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.42,
            "Number of params":null,
            "GFLOPs":45.43,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1012740,
            "title":"CLCNet: Rethinking of Ensemble Modeling with Classification Confidence Network",
            "url":"\/paper\/clcnet-rethinking-of-ensemble-modeling-with",
            "published":"2022-05-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/clcnet-rethinking-of-ensemble-modeling-with\/review\/?hl=64412"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":71544,
        "rank":139,
        "Model":"MaxViT-L (384res)",
        "mlmodel":{

        },
        "method_short":"MaxViT-L ",
        "method_details":"384res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-04",
        "metrics":{
            "Top 1 Accuracy":"86.4%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.4,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":988369,
            "title":"MaxViT: Multi-Axis Vision Transformer",
            "url":"\/paper\/maxvit-multi-axis-vision-transformer",
            "published":"2022-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/maxvit-multi-axis-vision-transformer\/review\/?hl=71544"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":112830,
        "rank":140,
        "Model":"UniRepLKNet-S++",
        "mlmodel":{

        },
        "method_short":"UniRepLKNet-S++",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-27",
        "metrics":{
            "Top 1 Accuracy":"86.4%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.4,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1327838,
            "title":"UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition",
            "url":"\/paper\/unireplknet-a-universal-perception-large",
            "published":"2023-11-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/unireplknet-a-universal-perception-large\/review\/?hl=112830"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":10366,
        "rank":141,
        "Model":"FixEfficientNet-B5",
        "mlmodel":{

        },
        "method_short":"FixEfficientNet-B5",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-18",
        "metrics":{
            "Top 1 Accuracy":"86.4%",
            "Number of params":"30M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.4,
            "Number of params":30000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":187431,
            "title":"Fixing the train-test resolution discrepancy: FixEfficientNet",
            "url":"\/paper\/fixing-the-train-test-resolution-discrepancy-2",
            "published":"2020-03-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fixing-the-train-test-resolution-discrepancy-2\/review\/?hl=10366"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":88304,
        "rank":142,
        "Model":"ConvFormer-S36 (384 res, 21K)",
        "mlmodel":{

        },
        "method_short":"ConvFormer-S36 ",
        "method_details":"384 res, 21K",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"86.4%",
            "Number of params":"40M",
            "GFLOPs":"22.4",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.4,
            "Number of params":40000000.0,
            "GFLOPs":22.4,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=88304"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":11197,
        "rank":143,
        "Model":"NoisyStudent (EfficientNet-B6)",
        "mlmodel":{

        },
        "method_short":"NoisyStudent ",
        "method_details":"EfficientNet-B6",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-11",
        "metrics":{
            "Top 1 Accuracy":"86.4%",
            "Number of params":"43M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.4,
            "Number of params":43000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":170047,
            "title":"Self-training with Noisy Student improves ImageNet classification",
            "url":"\/paper\/self-training-with-noisy-student-improves",
            "published":"2019-11-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/self-training-with-noisy-student-improves\/review\/?hl=11197"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":28808,
        "rank":144,
        "Model":"Swin-B (384 res, ImageNet-22k pretrain)",
        "mlmodel":{

        },
        "method_short":"Swin-B ",
        "method_details":"384 res, ImageNet-22k pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-25",
        "metrics":{
            "Top 1 Accuracy":"86.4%",
            "Number of params":"88M",
            "GFLOPs":"47",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.4,
            "Number of params":88000000.0,
            "GFLOPs":47.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":757245,
            "title":"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
            "url":"\/paper\/swin-transformer-hierarchical-vision",
            "published":"2021-03-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/swin-transformer-hierarchical-vision\/review\/?hl=28808"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[
            {
                "url":"https:\/\/github.com\/facebookresearch\/AugLy\/blob\/main\/examples\/imagenet\/evaluate_robustness_imagenet_swin_base_patch4_window12_384.ipynb"
            }
        ]
    },
    {
        "table_id":116,
        "row_id":73455,
        "rank":145,
        "Model":"CAFormer-B36 (384 res)",
        "mlmodel":{

        },
        "method_short":"CAFormer-B36 ",
        "method_details":"384 res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"86.4%",
            "Number of params":"99M",
            "GFLOPs":"72.2",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.4,
            "Number of params":99000000.0,
            "GFLOPs":72.2,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=73455"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":30486,
        "rank":146,
        "Model":"LV-ViT-L",
        "mlmodel":{

        },
        "method_short":"LV-ViT-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-22",
        "metrics":{
            "Top 1 Accuracy":"86.4%",
            "Number of params":"151M",
            "GFLOPs":"214.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.4,
            "Number of params":151000000.0,
            "GFLOPs":214.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":787097,
            "title":"All Tokens Matter: Token Labeling for Training Better Vision Transformers",
            "url":"\/paper\/token-labeling-training-a-85-5-top-1-accuracy",
            "published":"2021-04-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/token-labeling-training-a-85-5-top-1-accuracy\/review\/?hl=30486"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":5974,
        "rank":147,
        "Model":"FixResNeXt-101 32x48d",
        "mlmodel":{

        },
        "method_short":"FixResNeXt-101 32x48d",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-14",
        "metrics":{
            "Top 1 Accuracy":"86.4%",
            "Number of params":"829M",
            "GFLOPs":null,
            "Hardware Burden":"62G",
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.4,
            "Number of params":829000000.0,
            "GFLOPs":null,
            "Hardware Burden":62.0,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":142997,
            "title":"Fixing the train-test resolution discrepancy",
            "url":"\/paper\/fixing-the-train-test-resolution-discrepancy",
            "published":"2019-06-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fixing-the-train-test-resolution-discrepancy\/review\/?hl=5974"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":6,
                "name":"ResNeXt",
                "color":"#86960b"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60630,
        "rank":148,
        "Model":"MaxViT-B (384res)",
        "mlmodel":{

        },
        "method_short":"MaxViT-B ",
        "method_details":"384res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-04",
        "metrics":{
            "Top 1 Accuracy":"86.34%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.34,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":988369,
            "title":"MaxViT: Multi-Axis Vision Transformer",
            "url":"\/paper\/maxvit-multi-axis-vision-transformer",
            "published":"2022-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/maxvit-multi-axis-vision-transformer\/review\/?hl=60630"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":55711,
        "rank":149,
        "Model":"Bamboo (Bamboo-L)",
        "mlmodel":{

        },
        "method_short":"Bamboo ",
        "method_details":"Bamboo-L",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-21",
        "metrics":{
            "Top 1 Accuracy":"86.3",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.3,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1014071,
            "title":"A Study on Transformer Configuration and Training Objective",
            "url":"\/paper\/deeper-vs-wider-a-revisit-of-transformer",
            "published":"2022-05-21T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/deeper-vs-wider-a-revisit-of-transformer\/review\/?hl=55711"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":87344,
        "rank":150,
        "Model":"ViT-B@224 (cosub)",
        "mlmodel":{

        },
        "method_short":"ViT-B@224 ",
        "method_details":"cosub",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-09",
        "metrics":{
            "Top 1 Accuracy":"86.3%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.3,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1126263,
            "title":"Co-training $2^L$ Submodels for Visual Recognition",
            "url":"\/paper\/co-training-2-l-submodels-for-visual",
            "published":"2022-12-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/co-training-2-l-submodels-for-visual\/review\/?hl=87344"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":99854,
        "rank":151,
        "Model":"Our SP-ViT-L|384",
        "mlmodel":{

        },
        "method_short":"Our SP-ViT-L|384",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-15",
        "metrics":{
            "Top 1 Accuracy":"86.3%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.3,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1027709,
            "title":"SP-ViT: Learning 2D Spatial Priors for Vision Transformers",
            "url":"\/paper\/sp-vit-learning-2d-spatial-priors-for-vision",
            "published":"2022-06-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/sp-vit-learning-2d-spatial-priors-for-vision\/review\/?hl=99854"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":35025,
        "rank":152,
        "Model":"BEiT-L (ViT; ImageNet 1k pretrain)",
        "mlmodel":{

        },
        "method_short":"BEiT-L ",
        "method_details":"ViT; ImageNet 1k pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-15",
        "metrics":{
            "Top 1 Accuracy":"86.3%",
            "Number of params":"86M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.3,
            "Number of params":86000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":818838,
            "title":"BEiT: BERT Pre-Training of Image Transformers",
            "url":"\/paper\/beit-bert-pre-training-of-image-transformers",
            "published":"2021-06-15T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":35497,
        "rank":153,
        "Model":"VOLO-D3",
        "mlmodel":{

        },
        "method_short":"VOLO-D3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-24",
        "metrics":{
            "Top 1 Accuracy":"86.3%",
            "Number of params":"86M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.3,
            "Number of params":86000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":824294,
            "title":"VOLO: Vision Outlooker for Visual Recognition",
            "url":"\/paper\/volo-vision-outlooker-for-visual-recognition",
            "published":"2021-06-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/volo-vision-outlooker-for-visual-recognition\/review\/?hl=35497"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":75977,
        "rank":154,
        "Model":"VAN-B5 (22K)",
        "mlmodel":{

        },
        "method_short":"VAN-B5 ",
        "method_details":"22K",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-20",
        "metrics":{
            "Top 1 Accuracy":"86.3%",
            "Number of params":"90M",
            "GFLOPs":"17.2",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.3,
            "Number of params":90000000.0,
            "GFLOPs":17.2,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":964888,
            "title":"Visual Attention Network",
            "url":"\/paper\/visual-attention-network",
            "published":"2022-02-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/visual-attention-network\/review\/?hl=75977"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60755,
        "rank":155,
        "Model":"UniFormer-L (384 res)",
        "mlmodel":{

        },
        "method_short":"UniFormer-L ",
        "method_details":"384 res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-24",
        "metrics":{
            "Top 1 Accuracy":"86.3%",
            "Number of params":"100M",
            "GFLOPs":"39.2",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.3,
            "Number of params":100000000.0,
            "GFLOPs":39.2,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":949483,
            "title":"UniFormer: Unifying Convolution and Self-attention for Visual Recognition",
            "url":"\/paper\/uniformer-unifying-convolution-and-self",
            "published":"2022-01-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/uniformer-unifying-convolution-and-self\/review\/?hl=60755"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":53593,
        "rank":156,
        "Model":"MViTv2-L (384 res)",
        "mlmodel":{

        },
        "method_short":"MViTv2-L ",
        "method_details":"384 res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-02",
        "metrics":{
            "Top 1 Accuracy":"86.3%",
            "Number of params":"218M",
            "GFLOPs":"140.2",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.3,
            "Number of params":218000000.0,
            "GFLOPs":140.2,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":924692,
            "title":"MViTv2: Improved Multiscale Vision Transformers for Classification and Detection",
            "url":"\/paper\/improved-multiscale-vision-transformers-for",
            "published":"2021-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improved-multiscale-vision-transformers-for\/review\/?hl=53593"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29200,
        "rank":157,
        "Model":"CAIT-M36-448",
        "mlmodel":{

        },
        "method_short":"CAIT-M36-448",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-31",
        "metrics":{
            "Top 1 Accuracy":"86.3%",
            "Number of params":"271M",
            "GFLOPs":"247.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.3,
            "Number of params":271000000.0,
            "GFLOPs":247.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":772635,
            "title":"Going deeper with Image Transformers",
            "url":"\/paper\/going-deeper-with-image-transformers",
            "published":"2021-03-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/going-deeper-with-image-transformers\/review\/?hl=29200"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":25385,
        "rank":158,
        "Model":"NFNet-F5 w\/ SAM",
        "mlmodel":{

        },
        "method_short":"NFNet-F5 w\/ SAM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-11",
        "metrics":{
            "Top 1 Accuracy":"86.3%",
            "Number of params":"377.2M",
            "GFLOPs":"289.76",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.3,
            "Number of params":377200000.0,
            "GFLOPs":289.76,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":744370,
            "title":"High-Performance Large-Scale Image Recognition Without Normalization",
            "url":"\/paper\/high-performance-large-scale-image",
            "published":"2021-02-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/high-performance-large-scale-image\/review\/?hl=25385"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":87348,
        "rank":159,
        "Model":"Swin-B@224 (cosub)",
        "mlmodel":{

        },
        "method_short":"Swin-B@224 ",
        "method_details":"cosub",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-09",
        "metrics":{
            "Top 1 Accuracy":"86.2%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.2,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1126263,
            "title":"Co-training $2^L$ Submodels for Visual Recognition",
            "url":"\/paper\/co-training-2-l-submodels-for-visual",
            "published":"2022-12-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/co-training-2-l-submodels-for-visual\/review\/?hl=87348"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60265,
        "rank":160,
        "Model":"TinyViT-21M-384-distill (384 res, 21k)",
        "mlmodel":{

        },
        "method_short":"TinyViT-21M-384-distill ",
        "method_details":"384 res, 21k",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-21",
        "metrics":{
            "Top 1 Accuracy":"86.2%",
            "Number of params":"21M",
            "GFLOPs":"13.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.2,
            "Number of params":21000000.0,
            "GFLOPs":13.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1047551,
            "title":"TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
            "url":"\/paper\/tinyvit-fast-pretraining-distillation-for",
            "published":"2022-07-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/tinyvit-fast-pretraining-distillation-for\/review\/?hl=60265"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":73464,
        "rank":161,
        "Model":"CAFormer-M36 (384 res)",
        "mlmodel":{

        },
        "method_short":"CAFormer-M36 ",
        "method_details":"384 res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"86.2%",
            "Number of params":"56M",
            "GFLOPs":"42.0",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.2,
            "Number of params":56000000.0,
            "GFLOPs":42.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=73464"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":113269,
        "rank":162,
        "Model":"TransNeXt-Base (IN-1K supervised, 384)",
        "mlmodel":{

        },
        "method_short":"TransNeXt-Base ",
        "method_details":"IN-1K supervised, 384",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-28",
        "metrics":{
            "Top 1 Accuracy":"86.2%",
            "Number of params":"89.7M",
            "GFLOPs":"56.3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.2,
            "Number of params":89700000.0,
            "GFLOPs":56.3,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1331523,
            "title":"TransNeXt: Robust Foveal Visual Perception for Vision Transformers",
            "url":"\/paper\/transnext-robust-foveal-visual-perception-for",
            "published":"2023-11-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/transnext-robust-foveal-visual-perception-for\/review\/?hl=113269"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":113321,
        "rank":163,
        "Model":"MIRL (ViT-B-48)",
        "mlmodel":{

        },
        "method_short":"MIRL ",
        "method_details":"ViT-B-48",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-09-25",
        "metrics":{
            "Top 1 Accuracy":"86.2%",
            "Number of params":"341M",
            "GFLOPs":"67.0",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.2,
            "Number of params":341000000.0,
            "GFLOPs":67.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1333554,
            "title":"Masked Image Residual Learning for Scaling Deeper Vision Transformers",
            "url":"\/paper\/masked-image-residual-learning-for-scaling-1",
            "published":"2023-09-25T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60631,
        "rank":164,
        "Model":"MaxViT-S (512res)",
        "mlmodel":{

        },
        "method_short":"MaxViT-S ",
        "method_details":"512res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-04",
        "metrics":{
            "Top 1 Accuracy":"86.19%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.19,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":988369,
            "title":"MaxViT: Multi-Axis Vision Transformer",
            "url":"\/paper\/maxvit-multi-axis-vision-transformer",
            "published":"2022-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/maxvit-multi-axis-vision-transformer\/review\/?hl=60631"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":11198,
        "rank":165,
        "Model":"NoisyStudent (EfficientNet-B5)",
        "mlmodel":{

        },
        "method_short":"NoisyStudent ",
        "method_details":"EfficientNet-B5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-11",
        "metrics":{
            "Top 1 Accuracy":"86.1%",
            "Number of params":"30M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.1,
            "Number of params":30000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":170047,
            "title":"Self-training with Noisy Student improves ImageNet classification",
            "url":"\/paper\/self-training-with-noisy-student-improves",
            "published":"2019-11-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/self-training-with-noisy-student-improves\/review\/?hl=11198"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29229,
        "rank":166,
        "Model":"EfficientNetV2-M (21k)",
        "mlmodel":{

        },
        "method_short":"EfficientNetV2-M ",
        "method_details":"21k",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-01",
        "metrics":{
            "Top 1 Accuracy":"86.1%",
            "Number of params":"55M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.1,
            "Number of params":55000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":773495,
            "title":"EfficientNetV2: Smaller Models and Faster Training",
            "url":"\/paper\/efficientnetv2-smaller-models-and-faster",
            "published":"2021-04-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/efficientnetv2-smaller-models-and-faster\/review\/?hl=29229"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":88307,
        "rank":167,
        "Model":"ConvFormer-M36 (224 res, 21K)",
        "mlmodel":{

        },
        "method_short":"ConvFormer-M36 ",
        "method_details":"224 res, 21K",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"86.1%",
            "Number of params":"57M",
            "GFLOPs":"12.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.1,
            "Number of params":57000000.0,
            "GFLOPs":12.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=88307"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29201,
        "rank":168,
        "Model":"CAIT-M-36",
        "mlmodel":{

        },
        "method_short":"CAIT-M-36",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-31",
        "metrics":{
            "Top 1 Accuracy":"86.1%",
            "Number of params":"270.9M",
            "GFLOPs":"173.3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.1,
            "Number of params":270900000.0,
            "GFLOPs":173.3,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":772635,
            "title":"Going deeper with Image Transformers",
            "url":"\/paper\/going-deeper-with-image-transformers",
            "published":"2021-03-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/going-deeper-with-image-transformers\/review\/?hl=29201"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":35035,
        "rank":169,
        "Model":"Refiner-ViT-L",
        "mlmodel":{

        },
        "method_short":"Refiner-ViT-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-07",
        "metrics":{
            "Top 1 Accuracy":"86.03",
            "Number of params":"81M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.03,
            "Number of params":81000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":812454,
            "title":"Refiner: Refining Self-attention for Vision Transformers",
            "url":"\/paper\/refiner-refining-self-attention-for-vision",
            "published":"2021-06-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/refiner-refining-self-attention-for-vision\/review\/?hl=35035"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":70344,
        "rank":170,
        "Model":"GPaCo (ViT-L)",
        "mlmodel":{

        },
        "method_short":"GPaCo ",
        "method_details":"ViT-L",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-26",
        "metrics":{
            "Top 1 Accuracy":"86.01%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.01,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1080045,
            "title":"Generalized Parametric Contrastive Learning",
            "url":"\/paper\/generalized-parametric-contrastive-learning",
            "published":"2022-09-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/generalized-parametric-contrastive-learning\/review\/?hl=70344"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":25,
                "name":"Contrastive",
                "color":"#995d0b"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":45688,
        "rank":171,
        "Model":"Omnivore (Swin-L)",
        "mlmodel":{

        },
        "method_short":"Omnivore ",
        "method_details":"Swin-L",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-20",
        "metrics":{
            "Top 1 Accuracy":"86.0%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.0,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":948292,
            "title":"Omnivore: A Single Model for Many Visual Modalities",
            "url":"\/paper\/omnivore-a-single-model-for-many-visual",
            "published":"2022-01-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/omnivore-a-single-model-for-many-visual\/review\/?hl=45688"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":99853,
        "rank":172,
        "Model":"Our SP-ViT-M|384",
        "mlmodel":{

        },
        "method_short":"Our SP-ViT-M|384",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-15",
        "metrics":{
            "Top 1 Accuracy":"86%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.0,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1027709,
            "title":"SP-ViT: Learning 2D Spatial Priors for Vision Transformers",
            "url":"\/paper\/sp-vit-learning-2d-spatial-priors-for-vision",
            "published":"2022-06-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/sp-vit-learning-2d-spatial-priors-for-vision\/review\/?hl=99853"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":113272,
        "rank":173,
        "Model":"TransNeXt-Small (IN-1K supervised, 384)",
        "mlmodel":{

        },
        "method_short":"TransNeXt-Small ",
        "method_details":"IN-1K supervised, 384",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-28",
        "metrics":{
            "Top 1 Accuracy":"86.0%",
            "Number of params":"49.7M",
            "GFLOPs":"32.1",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.0,
            "Number of params":49700000.0,
            "GFLOPs":32.1,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1331523,
            "title":"TransNeXt: Robust Foveal Visual Perception for Vision Transformers",
            "url":"\/paper\/transnext-robust-foveal-visual-perception-for",
            "published":"2023-11-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/transnext-robust-foveal-visual-perception-for\/review\/?hl=113272"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":35498,
        "rank":174,
        "Model":"VOLO-D2",
        "mlmodel":{

        },
        "method_short":"VOLO-D2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-24",
        "metrics":{
            "Top 1 Accuracy":"86%",
            "Number of params":"59M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.0,
            "Number of params":59000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":824294,
            "title":"VOLO: Vision Outlooker for Visual Recognition",
            "url":"\/paper\/volo-vision-outlooker-for-visual-recognition",
            "published":"2021-06-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/volo-vision-outlooker-for-visual-recognition\/review\/?hl=35498"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":35219,
        "rank":175,
        "Model":"XCiT-L24",
        "mlmodel":{

        },
        "method_short":"XCiT-L24",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-17",
        "metrics":{
            "Top 1 Accuracy":"86%",
            "Number of params":"189M",
            "GFLOPs":"417.9",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.0,
            "Number of params":189000000.0,
            "GFLOPs":417.9,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":820317,
            "title":"XCiT: Cross-Covariance Image Transformers",
            "url":"\/paper\/xcit-cross-covariance-image-transformers",
            "published":"2021-06-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/xcit-cross-covariance-image-transformers\/review\/?hl=35219"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":111051,
        "rank":176,
        "Model":"SparK (ConvNeXt-Large, 384)",
        "mlmodel":{

        },
        "method_short":"SparK ",
        "method_details":"ConvNeXt-Large, 384",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-09",
        "metrics":{
            "Top 1 Accuracy":"86.0%",
            "Number of params":"198M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.0,
            "Number of params":198000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1139501,
            "title":"Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling",
            "url":"\/paper\/designing-bert-for-convolutional-networks",
            "published":"2023-01-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/designing-bert-for-convolutional-networks\/review\/?hl=111051"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":212,
                "name":"ConvNeXt",
                "color":"#77bb41"
            },
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":25386,
        "rank":177,
        "Model":"NFNet-F5",
        "mlmodel":{

        },
        "method_short":"NFNet-F5",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-11",
        "metrics":{
            "Top 1 Accuracy":"86.0%",
            "Number of params":"377.2M",
            "GFLOPs":"289.76",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.0,
            "Number of params":377200000.0,
            "GFLOPs":289.76,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":744370,
            "title":"High-Performance Large-Scale Image Recognition Without Normalization",
            "url":"\/paper\/high-performance-large-scale-image",
            "published":"2021-02-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/high-performance-large-scale-image\/review\/?hl=25386"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":44085,
        "rank":178,
        "Model":"MAE (ViT-L)",
        "mlmodel":{

        },
        "method_short":"MAE ",
        "method_details":"ViT-L",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-11",
        "metrics":{
            "Top 1 Accuracy":"85.9%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.9,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":908690,
            "title":"Masked Autoencoders Are Scalable Vision Learners",
            "url":"\/paper\/masked-autoencoders-are-scalable-vision",
            "published":"2021-11-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-autoencoders-are-scalable-vision\/review\/?hl=44085"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":10367,
        "rank":179,
        "Model":"FixEfficientNet-B4",
        "mlmodel":{

        },
        "method_short":"FixEfficientNet-B4",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-18",
        "metrics":{
            "Top 1 Accuracy":"85.9%",
            "Number of params":"19M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.9,
            "Number of params":19000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":187431,
            "title":"Fixing the train-test resolution discrepancy: FixEfficientNet",
            "url":"\/paper\/fixing-the-train-test-resolution-discrepancy-2",
            "published":"2020-03-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fixing-the-train-test-resolution-discrepancy-2\/review\/?hl=10367"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":25397,
        "rank":180,
        "Model":"NFNet-F4",
        "mlmodel":{

        },
        "method_short":"NFNet-F4",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-11",
        "metrics":{
            "Top 1 Accuracy":"85.9%",
            "Number of params":"316.1M",
            "GFLOPs":"215.24",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.9,
            "Number of params":316100000.0,
            "GFLOPs":215.24,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":744370,
            "title":"High-Performance Large-Scale Image Recognition Without Normalization",
            "url":"\/paper\/high-performance-large-scale-image",
            "published":"2021-02-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/high-performance-large-scale-image\/review\/?hl=25397"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":87349,
        "rank":181,
        "Model":"ConvNeXt-B@224 (cosub)",
        "mlmodel":{

        },
        "method_short":"ConvNeXt-B@224 ",
        "method_details":"cosub",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-09",
        "metrics":{
            "Top 1 Accuracy":"85.8%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.8,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1126263,
            "title":"Co-training $2^L$ Submodels for Visual Recognition",
            "url":"\/paper\/co-training-2-l-submodels-for-visual",
            "published":"2022-12-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/co-training-2-l-submodels-for-visual\/review\/?hl=87349"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":87351,
        "rank":182,
        "Model":"PiT-B@224 (cosub)",
        "mlmodel":{

        },
        "method_short":"PiT-B@224 ",
        "method_details":"cosub",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-09",
        "metrics":{
            "Top 1 Accuracy":"85.8%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.8,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1126263,
            "title":"Co-training $2^L$ Submodels for Visual Recognition",
            "url":"\/paper\/co-training-2-l-submodels-for-visual",
            "published":"2022-12-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/co-training-2-l-submodels-for-visual\/review\/?hl=87351"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":88305,
        "rank":183,
        "Model":"CAFormer-S36 (224 res, 21K)",
        "mlmodel":{

        },
        "method_short":"CAFormer-S36 ",
        "method_details":"224 res, 21K",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"85.8%",
            "Number of params":"39M",
            "GFLOPs":"8.0",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.8,
            "Number of params":39000000.0,
            "GFLOPs":8.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=88305"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":35218,
        "rank":184,
        "Model":"XCiT-M24",
        "mlmodel":{

        },
        "method_short":"XCiT-M24",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-17",
        "metrics":{
            "Top 1 Accuracy":"85.8%",
            "Number of params":"84M",
            "GFLOPs":"188",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.8,
            "Number of params":84000000.0,
            "GFLOPs":188.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":820317,
            "title":"XCiT: Cross-Covariance Image Transformers",
            "url":"\/paper\/xcit-cross-covariance-image-transformers",
            "published":"2021-06-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/xcit-cross-covariance-image-transformers\/review\/?hl=35218"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":15817,
        "rank":185,
        "Model":"Fix-EfficientNet-B8 (MaxUp + CutMix)",
        "mlmodel":{

        },
        "method_short":"Fix-EfficientNet-B8 ",
        "method_details":"MaxUp + CutMix",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-02-20",
        "metrics":{
            "Top 1 Accuracy":"85.8%",
            "Number of params":"87.42M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.8,
            "Number of params":87420000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":184301,
            "title":"MaxUp: A Simple Way to Improve Generalization of Neural Network Training",
            "url":"\/paper\/maxup-a-simple-way-to-improve-generalization",
            "published":"2020-02-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/maxup-a-simple-way-to-improve-generalization\/review\/?hl=15817"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":10515,
        "rank":186,
        "Model":"KDforAA (EfficientNet-B8)",
        "mlmodel":{

        },
        "method_short":"KDforAA ",
        "method_details":"EfficientNet-B8",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-25",
        "metrics":{
            "Top 1 Accuracy":"85.8%",
            "Number of params":"88M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.8,
            "Number of params":88000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":188259,
            "title":"Circumventing Outliers of AutoAugment with Knowledge Distillation",
            "url":"\/paper\/circumventing-outliers-of-autoaugment-with",
            "published":"2020-03-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/circumventing-outliers-of-autoaugment-with\/review\/?hl=10515"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29202,
        "rank":187,
        "Model":"CAIT-M-24",
        "mlmodel":{

        },
        "method_short":"CAIT-M-24",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-31",
        "metrics":{
            "Top 1 Accuracy":"85.8%",
            "Number of params":"185.9M",
            "GFLOPs":"116.1",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.8,
            "Number of params":185900000.0,
            "GFLOPs":116.1,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":772635,
            "title":"Going deeper with Image Transformers",
            "url":"\/paper\/going-deeper-with-image-transformers",
            "published":"2021-03-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/going-deeper-with-image-transformers\/review\/?hl=29202"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":52541,
        "rank":188,
        "Model":"ViT-L @384 (DeiT III)",
        "mlmodel":{

        },
        "method_short":"ViT-L @384 ",
        "method_details":"DeiT III",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-14",
        "metrics":{
            "Top 1 Accuracy":"85.8%",
            "Number of params":"304.8M",
            "GFLOPs":"191.2",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.8,
            "Number of params":304800000.0,
            "GFLOPs":191.2,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":994466,
            "title":"DeiT III: Revenge of the ViT",
            "url":"\/paper\/deit-iii-revenge-of-the-vit",
            "published":"2022-04-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deit-iii-revenge-of-the-vit\/review\/?hl=52541"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":47951,
        "rank":189,
        "Model":"SEER (RG-10B)",
        "mlmodel":{

        },
        "method_short":"SEER ",
        "method_details":"RG-10B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-16",
        "metrics":{
            "Top 1 Accuracy":"85.8%",
            "Number of params":"10000M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.8,
            "Number of params":10000000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":963673,
            "title":"Vision Models Are More Robust And Fair When Pretrained On Uncurated Images Without Supervision",
            "url":"\/paper\/vision-models-are-more-robust-and-fair-when",
            "published":"2022-02-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vision-models-are-more-robust-and-fair-when\/review\/?hl=47951"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":67,
                "name":"Self-Supervised Learning",
                "color":"#d72727"
            },
            {
                "id":108,
                "name":"RegNet",
                "color":"#2771D3"
            },
            {
                "id":107,
                "name":"IG-1B",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":71545,
        "rank":190,
        "Model":"MaxViT-T(512res)",
        "mlmodel":{

        },
        "method_short":"MaxViT-T",
        "method_details":"512res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-04",
        "metrics":{
            "Top 1 Accuracy":"85.72%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.72,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":988369,
            "title":"MaxViT: Multi-Axis Vision Transformer",
            "url":"\/paper\/maxvit-multi-axis-vision-transformer",
            "published":"2022-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/maxvit-multi-axis-vision-transformer\/review\/?hl=71545"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":10368,
        "rank":191,
        "Model":"FixEfficientNet-B8",
        "mlmodel":{

        },
        "method_short":"FixEfficientNet-B8",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-18",
        "metrics":{
            "Top 1 Accuracy":"85.7%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.7,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":187431,
            "title":"Fixing the train-test resolution discrepancy: FixEfficientNet",
            "url":"\/paper\/fixing-the-train-test-resolution-discrepancy-2",
            "published":"2020-03-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fixing-the-train-test-resolution-discrepancy-2\/review\/?hl=10368"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29227,
        "rank":192,
        "Model":"EfficientNetV2-L",
        "mlmodel":{

        },
        "method_short":"EfficientNetV2-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-01",
        "metrics":{
            "Top 1 Accuracy":"85.7%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.7,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":773495,
            "title":"EfficientNetV2: Smaller Models and Faster Training",
            "url":"\/paper\/efficientnetv2-smaller-models-and-faster",
            "published":"2021-04-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/efficientnetv2-smaller-models-and-faster\/review\/?hl=29227"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":52544,
        "rank":193,
        "Model":"ViT-B @224 (DeiT III, 21k)",
        "mlmodel":{

        },
        "method_short":"ViT-B @224 ",
        "method_details":"DeiT III, 21k",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-14",
        "metrics":{
            "Top 1 Accuracy":"85.7%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.7,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":994466,
            "title":"DeiT III: Revenge of the ViT",
            "url":"\/paper\/deit-iii-revenge-of-the-vit",
            "published":"2022-04-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deit-iii-revenge-of-the-vit\/review\/?hl=52544"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":69822,
        "rank":194,
        "Model":"dBOT ViT-B (CLIP as Teacher)",
        "mlmodel":{

        },
        "method_short":"dBOT ViT-B ",
        "method_details":"CLIP as Teacher",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-08",
        "metrics":{
            "Top 1 Accuracy":"85.7%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.7,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1071401,
            "title":"Exploring Target Representations for Masked Autoencoders",
            "url":"\/paper\/exploring-target-representations-for-masked",
            "published":"2022-09-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/exploring-target-representations-for-masked\/review\/?hl=69822"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":300,
                "name":"CLIP data ",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":73465,
        "rank":195,
        "Model":"CAFormer-S36 (384 res)",
        "mlmodel":{

        },
        "method_short":"CAFormer-S36 ",
        "method_details":"384 res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"85.7%",
            "Number of params":"39M",
            "GFLOPs":"26.0",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.7,
            "Number of params":39000000.0,
            "GFLOPs":26.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=73465"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":76001,
        "rank":196,
        "Model":"VAN-B4 (22K)",
        "mlmodel":{

        },
        "method_short":"VAN-B4 ",
        "method_details":"22K",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-20",
        "metrics":{
            "Top 1 Accuracy":"85.7%",
            "Number of params":"60M",
            "GFLOPs":"12.2",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.7,
            "Number of params":60000000.0,
            "GFLOPs":12.2,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":964888,
            "title":"Visual Attention Network",
            "url":"\/paper\/visual-attention-network",
            "published":"2022-02-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/visual-attention-network\/review\/?hl=76001"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":73456,
        "rank":197,
        "Model":"ConvFormer-B36 (384 res)",
        "mlmodel":{

        },
        "method_short":"ConvFormer-B36 ",
        "method_details":"384 res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"85.7%",
            "Number of params":"100M",
            "GFLOPs":"66.5",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.7,
            "Number of params":100000000.0,
            "GFLOPs":66.5,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=73456"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":25396,
        "rank":198,
        "Model":"NFNet-F3",
        "mlmodel":{

        },
        "method_short":"NFNet-F3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-11",
        "metrics":{
            "Top 1 Accuracy":"85.7%",
            "Number of params":"254.9M",
            "GFLOPs":"114.76",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.7,
            "Number of params":254900000.0,
            "GFLOPs":114.76,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":744370,
            "title":"High-Performance Large-Scale Image Recognition Without Normalization",
            "url":"\/paper\/high-performance-large-scale-image",
            "published":"2021-02-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/high-performance-large-scale-image\/review\/?hl=25396"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":105531,
        "rank":199,
        "Model":"ViT-H @224 (DeiT-III + AugSub)",
        "mlmodel":{

        },
        "method_short":"ViT-H @224 ",
        "method_details":"DeiT-III + AugSub",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-06-20",
        "metrics":{
            "Top 1 Accuracy":"85.7%",
            "Number of params":"632M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.7,
            "Number of params":632000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1231534,
            "title":"Augmenting Sub-model to Improve Main Model",
            "url":"\/paper\/augmenting-sub-model-to-improve-main-model",
            "published":"2023-06-20T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":35217,
        "rank":200,
        "Model":"XCiT-S24",
        "mlmodel":{

        },
        "method_short":"XCiT-S24",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-17",
        "metrics":{
            "Top 1 Accuracy":"85.6%",
            "Number of params":"48M",
            "GFLOPs":"106",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.6,
            "Number of params":48000000.0,
            "GFLOPs":106.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":820317,
            "title":"XCiT: Cross-Covariance Image Transformers",
            "url":"\/paper\/xcit-cross-covariance-image-transformers",
            "published":"2021-06-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/xcit-cross-covariance-image-transformers\/review\/?hl=35217"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":73466,
        "rank":201,
        "Model":"ConvFormer-M36 (384 res)",
        "mlmodel":{

        },
        "method_short":"ConvFormer-M36 ",
        "method_details":"384 res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"85.6%",
            "Number of params":"57M",
            "GFLOPs":"37.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.6,
            "Number of params":57000000.0,
            "GFLOPs":37.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=73466"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":46097,
        "rank":202,
        "Model":"UniFormer-L",
        "mlmodel":{

        },
        "method_short":"UniFormer-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-24",
        "metrics":{
            "Top 1 Accuracy":"85.6%",
            "Number of params":"100M",
            "GFLOPs":"12.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.6,
            "Number of params":100000000.0,
            "GFLOPs":12.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":949483,
            "title":"UniFormer: Unifying Convolution and Self-attention for Visual Recognition",
            "url":"\/paper\/uniformer-unifying-convolution-and-self",
            "published":"2022-01-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/uniformer-unifying-convolution-and-self\/review\/?hl=46097"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":49540,
        "rank":203,
        "Model":"ViT-L@384 (attn finetune)",
        "mlmodel":{

        },
        "method_short":"ViT-L@384 ",
        "method_details":"attn finetune",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-18",
        "metrics":{
            "Top 1 Accuracy":"85.5%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.5,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":979672,
            "title":"Three things everyone should know about Vision Transformers",
            "url":"\/paper\/three-things-everyone-should-know-about",
            "published":"2022-03-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/three-things-everyone-should-know-about\/review\/?hl=49540"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":99851,
        "rank":204,
        "Model":"Our SP-ViT-L",
        "mlmodel":{

        },
        "method_short":"Our SP-ViT-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-15",
        "metrics":{
            "Top 1 Accuracy":"85.5%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.5,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1027709,
            "title":"SP-ViT: Learning 2D Spatial Priors for Vision Transformers",
            "url":"\/paper\/sp-vit-learning-2d-spatial-priors-for-vision",
            "published":"2022-06-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/sp-vit-learning-2d-spatial-priors-for-vision\/review\/?hl=99851"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":53640,
        "rank":205,
        "Model":"Mini-Swin-B@384",
        "mlmodel":{

        },
        "method_short":"Mini-Swin-B@384",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-14",
        "metrics":{
            "Top 1 Accuracy":"85.5%",
            "Number of params":"47M",
            "GFLOPs":"98.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.5,
            "Number of params":47000000.0,
            "GFLOPs":98.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":994462,
            "title":"MiniViT: Compressing Vision Transformers with Weight Multiplexing",
            "url":"\/paper\/minivit-compressing-vision-transformers-with",
            "published":"2022-04-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/minivit-compressing-vision-transformers-with\/review\/?hl=53640"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":98,
                "name":"Conv+Transformer",
                "color":"#ff2600"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":63779,
        "rank":206,
        "Model":"Wave-ViT-L",
        "mlmodel":{

        },
        "method_short":"Wave-ViT-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-11",
        "metrics":{
            "Top 1 Accuracy":"85.5%",
            "Number of params":"57.5M",
            "GFLOPs":"14.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.5,
            "Number of params":57500000.0,
            "GFLOPs":14.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1042136,
            "title":"Wave-ViT: Unifying Wavelet and Transformers for Visual Representation Learning",
            "url":"\/paper\/wave-vit-unifying-wavelet-and-transformers",
            "published":"2022-07-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/wave-vit-unifying-wavelet-and-transformers\/review\/?hl=63779"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":10516,
        "rank":207,
        "Model":"KDforAA (EfficientNet-B7)",
        "mlmodel":{

        },
        "method_short":"KDforAA ",
        "method_details":"EfficientNet-B7",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-25",
        "metrics":{
            "Top 1 Accuracy":"85.5%",
            "Number of params":"66M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.5,
            "Number of params":66000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":188259,
            "title":"Circumventing Outliers of AutoAugment with Knowledge Distillation",
            "url":"\/paper\/circumventing-outliers-of-autoaugment-with",
            "published":"2020-03-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/circumventing-outliers-of-autoaugment-with\/review\/?hl=10516"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":28724,
        "rank":208,
        "Model":"HaloNet4 (base 128, Conv-12)",
        "mlmodel":{

        },
        "method_short":"HaloNet4 ",
        "method_details":"base 128, Conv-12",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-23",
        "metrics":{
            "Top 1 Accuracy":"85.5%",
            "Number of params":"87M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.5,
            "Number of params":87000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":756297,
            "title":"Scaling Local Self-Attention for Parameter Efficient Visual Backbones",
            "url":"\/paper\/scaling-local-self-attention-for-parameter",
            "published":"2021-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-local-self-attention-for-parameter\/review\/?hl=28724"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":8898,
        "rank":209,
        "Model":"AdvProp (EfficientNet-B8)",
        "mlmodel":{

        },
        "method_short":"AdvProp ",
        "method_details":"EfficientNet-B8",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-21",
        "metrics":{
            "Top 1 Accuracy":"85.5%",
            "Number of params":"88M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.5,
            "Number of params":88000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":173727,
            "title":"Adversarial Examples Improve Image Recognition",
            "url":"\/paper\/adversarial-examples-improve-image",
            "published":"2019-11-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/adversarial-examples-improve-image\/review\/?hl=8898"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":73457,
        "rank":210,
        "Model":"CAFormer-B36 (224 res)",
        "mlmodel":{

        },
        "method_short":"CAFormer-B36 ",
        "method_details":"224 res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"85.5%",
            "Number of params":"99M",
            "GFLOPs":"23.2",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.5,
            "Number of params":99000000.0,
            "GFLOPs":23.2,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=73457"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":61425,
        "rank":211,
        "Model":"ConvNeXt-L (384 res)",
        "mlmodel":{

        },
        "method_short":"ConvNeXt-L ",
        "method_details":"384 res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-10",
        "metrics":{
            "Top 1 Accuracy":"85.5%",
            "Number of params":"198M",
            "GFLOPs":"101",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.5,
            "Number of params":198000000.0,
            "GFLOPs":101.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":942596,
            "title":"A ConvNet for the 2020s",
            "url":"\/paper\/a-convnet-for-the-2020s",
            "published":"2022-01-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/a-convnet-for-the-2020s\/review\/?hl=61425"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":16783,
        "rank":212,
        "Model":"EfficientNet-B8 (RandAugment)",
        "mlmodel":{

        },
        "method_short":"EfficientNet-B8 ",
        "method_details":"RandAugment",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-09-30",
        "metrics":{
            "Top 1 Accuracy":"85.4%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.4,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":156406,
            "title":"RandAugment: Practical automated data augmentation with a reduced search space",
            "url":"\/paper\/randaugment-practical-data-augmentation-with",
            "published":"2019-09-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/randaugment-practical-data-augmentation-with\/review\/?hl=16783"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":99524,
        "rank":213,
        "Model":"BiFormer-B (IN1k ptretrain)",
        "mlmodel":{

        },
        "method_short":"BiFormer-B* ",
        "method_details":"IN1k ptretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-15",
        "metrics":{
            "Top 1 Accuracy":"85.4%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.4,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1174425,
            "title":"BiFormer: Vision Transformer with Bi-Level Routing Attention",
            "url":"\/paper\/biformer-vision-transformer-with-bi-level",
            "published":"2023-03-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/biformer-vision-transformer-with-bi-level\/review\/?hl=99524"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":44964,
        "rank":214,
        "Model":"PatchConvNet-S60-21k-512",
        "mlmodel":{

        },
        "method_short":"PatchConvNet-S60-21k-512",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-27",
        "metrics":{
            "Top 1 Accuracy":"85.4%",
            "Number of params":"25.2M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.4,
            "Number of params":25200000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":933338,
            "title":"Augmenting Convolutional networks with attention-based aggregation",
            "url":"\/paper\/augmenting-convolutional-networks-with",
            "published":"2021-12-27T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            },
            {
                "id":167,
                "name":"PatchConvnet",
                "color":"#e0edd4"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":88302,
        "rank":215,
        "Model":"CAFormer-S18 (384 res, 21K)",
        "mlmodel":{

        },
        "method_short":"CAFormer-S18 ",
        "method_details":"384 res, 21K",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"85.4%",
            "Number of params":"26M",
            "GFLOPs":"13.4",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.4,
            "Number of params":26000000.0,
            "GFLOPs":13.4,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=88302"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":88303,
        "rank":216,
        "Model":"ConvFormer-S36 (224 res, 21K)",
        "mlmodel":{

        },
        "method_short":"ConvFormer-S36 ",
        "method_details":"224 res, 21K",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"85.4%",
            "Number of params":"40M",
            "GFLOPs":"7.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.4,
            "Number of params":40000000.0,
            "GFLOPs":7.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=88303"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":73467,
        "rank":217,
        "Model":"ConvFormer-S36 (384 res)",
        "mlmodel":{

        },
        "method_short":"ConvFormer-S36 ",
        "method_details":"384 res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"85.4%",
            "Number of params":"40M",
            "GFLOPs":"22.4",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.4,
            "Number of params":40000000.0,
            "GFLOPs":22.4,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=73467"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29204,
        "rank":218,
        "Model":"CAIT-S-36",
        "mlmodel":{

        },
        "method_short":"CAIT-S-36",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-31",
        "metrics":{
            "Top 1 Accuracy":"85.4%",
            "Number of params":"68.2M",
            "GFLOPs":"48",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.4,
            "Number of params":68200000.0,
            "GFLOPs":48.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":772635,
            "title":"Going deeper with Image Transformers",
            "url":"\/paper\/going-deeper-with-image-transformers",
            "published":"2021-03-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/going-deeper-with-image-transformers\/review\/?hl=29204"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":5615,
        "rank":219,
        "Model":"ResNeXt-101 32x48d",
        "mlmodel":{

        },
        "method_short":"ResNeXt-101 32x48d",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-05-02",
        "metrics":{
            "Top 1 Accuracy":"85.4%",
            "Number of params":"829M",
            "GFLOPs":"306",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.4,
            "Number of params":829000000.0,
            "GFLOPs":306.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":4635,
            "title":"Exploring the Limits of Weakly Supervised Pretraining",
            "url":"\/paper\/exploring-the-limits-of-weakly-supervised",
            "published":"2018-05-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/exploring-the-limits-of-weakly-supervised\/review\/?hl=5615"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":107,
                "name":"IG-1B",
                "color":"#2771D3"
            },
            {
                "id":6,
                "name":"ResNeXt",
                "color":"#86960b"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":9482,
        "rank":220,
        "Model":"BiT-M (ResNet)",
        "mlmodel":{

        },
        "method_short":"BiT-M ",
        "method_details":"ResNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-12-24",
        "metrics":{
            "Top 1 Accuracy":"85.39%",
            "Number of params":"928M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.39,
            "Number of params":928000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":178162,
            "title":"Big Transfer (BiT): General Visual Representation Learning",
            "url":"\/paper\/large-scale-learning-of-general-visual",
            "published":"2019-12-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/large-scale-learning-of-general-visual\/review\/?hl=9482"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":32048,
        "rank":221,
        "Model":"ViT-L\/16 Dosovitskiy et al. (2021)",
        "mlmodel":{

        },
        "method_short":"ViT-L\/16 Dosovitskiy et al. ",
        "method_details":"2021",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-04",
        "metrics":{
            "Top 1 Accuracy":"85.3%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.3,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":793349,
            "title":"MLP-Mixer: An all-MLP Architecture for Vision",
            "url":"\/paper\/mlp-mixer-an-all-mlp-architecture-for-vision",
            "published":"2021-05-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mlp-mixer-an-all-mlp-architecture-for-vision\/review\/?hl=32048"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":45687,
        "rank":222,
        "Model":"Omnivore (Swin-B)",
        "mlmodel":{

        },
        "method_short":"Omnivore ",
        "method_details":"Swin-B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-20",
        "metrics":{
            "Top 1 Accuracy":"85.3%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.3,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":948292,
            "title":"Omnivore: A Single Model for Many Visual Modalities",
            "url":"\/paper\/omnivore-a-single-model-for-many-visual",
            "published":"2022-01-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/omnivore-a-single-model-for-many-visual\/review\/?hl=45687"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":11199,
        "rank":223,
        "Model":"NoisyStudent (EfficientNet-B4)",
        "mlmodel":{

        },
        "method_short":"NoisyStudent ",
        "method_details":"EfficientNet-B4",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-11",
        "metrics":{
            "Top 1 Accuracy":"85.3%",
            "Number of params":"19M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.3,
            "Number of params":19000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":170047,
            "title":"Self-training with Noisy Student improves ImageNet classification",
            "url":"\/paper\/self-training-with-noisy-student-improves",
            "published":"2019-11-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/self-training-with-noisy-student-improves\/review\/?hl=11199"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29203,
        "rank":224,
        "Model":"CAIT-S-48",
        "mlmodel":{

        },
        "method_short":"CAIT-S-48",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-31",
        "metrics":{
            "Top 1 Accuracy":"85.3%",
            "Number of params":"89.5M",
            "GFLOPs":"63.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.3,
            "Number of params":89500000.0,
            "GFLOPs":63.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":772635,
            "title":"Going deeper with Image Transformers",
            "url":"\/paper\/going-deeper-with-image-transformers",
            "published":"2021-03-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/going-deeper-with-image-transformers\/review\/?hl=29203"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":105530,
        "rank":225,
        "Model":"ViT-L @224 (DeiT-III + AugSub)",
        "mlmodel":{

        },
        "method_short":"ViT-L @224 ",
        "method_details":"DeiT-III + AugSub",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-06-20",
        "metrics":{
            "Top 1 Accuracy":"85.3%",
            "Number of params":"304M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.3,
            "Number of params":304000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1231534,
            "title":"Augmenting Sub-model to Improve Main Model",
            "url":"\/paper\/augmenting-sub-model-to-improve-main-model",
            "published":"2023-06-20T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":55045,
        "rank":226,
        "Model":"CLCNet (S:D1+D:D5)",
        "mlmodel":{

        },
        "method_short":"CLCNet ",
        "method_details":"S:D1+D:D5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-19",
        "metrics":{
            "Top 1 Accuracy":"85.28%",
            "Number of params":null,
            "GFLOPs":"47.43",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.28,
            "Number of params":null,
            "GFLOPs":47.43,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1012740,
            "title":"CLCNet: Rethinking of Ensemble Modeling with Classification Confidence Network",
            "url":"\/paper\/clcnet-rethinking-of-ensemble-modeling-with",
            "published":"2022-05-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/clcnet-rethinking-of-ensemble-modeling-with\/review\/?hl=55045"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":71542,
        "rank":227,
        "Model":"MaxViT-T (384res)",
        "mlmodel":{

        },
        "method_short":"MaxViT-T ",
        "method_details":"384res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-04",
        "metrics":{
            "Top 1 Accuracy":"85.24%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.24,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":988369,
            "title":"MaxViT: Multi-Axis Vision Transformer",
            "url":"\/paper\/maxvit-multi-axis-vision-transformer",
            "published":"2022-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/maxvit-multi-axis-vision-transformer\/review\/?hl=71542"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":52542,
        "rank":228,
        "Model":"ViT-H @224 (DeiT III)",
        "mlmodel":{

        },
        "method_short":"ViT-H @224 ",
        "method_details":"DeiT III",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-14",
        "metrics":{
            "Top 1 Accuracy":"85.2%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.2,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":994466,
            "title":"DeiT III: Revenge of the ViT",
            "url":"\/paper\/deit-iii-revenge-of-the-vit",
            "published":"2022-04-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deit-iii-revenge-of-the-vit\/review\/?hl=52542"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":35499,
        "rank":229,
        "Model":"VOLO-D1",
        "mlmodel":{

        },
        "method_short":"VOLO-D1",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-24",
        "metrics":{
            "Top 1 Accuracy":"85.2%",
            "Number of params":"27M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.2,
            "Number of params":27000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":824294,
            "title":"VOLO: Vision Outlooker for Visual Recognition",
            "url":"\/paper\/volo-vision-outlooker-for-visual-recognition",
            "published":"2021-06-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/volo-vision-outlooker-for-visual-recognition\/review\/?hl=35499"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":73468,
        "rank":230,
        "Model":"CAFormer-M36 (224 res)",
        "mlmodel":{

        },
        "method_short":"CAFormer-M36 ",
        "method_details":"224 res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"85.2%",
            "Number of params":"56M",
            "GFLOPs":"13.2",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.2,
            "Number of params":56000000.0,
            "GFLOPs":13.2,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=73468"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":8899,
        "rank":231,
        "Model":"AdvProp (EfficientNet-B7)",
        "mlmodel":{

        },
        "method_short":"AdvProp ",
        "method_details":"EfficientNet-B7",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-21",
        "metrics":{
            "Top 1 Accuracy":"85.2%",
            "Number of params":"66M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.2,
            "Number of params":66000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":173727,
            "title":"Adversarial Examples Improve Image Recognition",
            "url":"\/paper\/adversarial-examples-improve-image",
            "published":"2019-11-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/adversarial-examples-improve-image\/review\/?hl=8899"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":48309,
        "rank":232,
        "Model":"UniNet-B5",
        "mlmodel":{

        },
        "method_short":"UniNet-B5",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-08",
        "metrics":{
            "Top 1 Accuracy":"85.2%",
            "Number of params":"73.5M",
            "GFLOPs":"23.2",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.2,
            "Number of params":73500000.0,
            "GFLOPs":23.2,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":881795,
            "title":"UniNet: Unified Architecture Search with Convolution, Transformer, and MLP",
            "url":"\/paper\/uninet-unified-architecture-search-with",
            "published":"2021-10-08T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/uninet-unified-architecture-search-with\/review\/?hl=48309"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":23829,
        "rank":233,
        "Model":"DeiT-B 384",
        "mlmodel":{

        },
        "method_short":"DeiT-B 384",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-23",
        "metrics":{
            "Top 1 Accuracy":"85.2%",
            "Number of params":"87M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.2,
            "Number of params":87000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":731001,
            "title":"Training data-efficient image transformers & distillation through attention",
            "url":"\/paper\/training-data-efficient-image-transformers",
            "published":"2020-12-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/training-data-efficient-image-transformers\/review\/?hl=23829"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29226,
        "rank":234,
        "Model":"EfficientNetV2-M",
        "mlmodel":{

        },
        "method_short":"EfficientNetV2-M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-01",
        "metrics":{
            "Top 1 Accuracy":"85.1%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.1,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":773495,
            "title":"EfficientNetV2: Smaller Models and Faster Training",
            "url":"\/paper\/efficientnetv2-smaller-models-and-faster",
            "published":"2021-04-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/efficientnetv2-smaller-models-and-faster\/review\/?hl=29226"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":75999,
        "rank":235,
        "Model":"MKD ViT-B",
        "mlmodel":{

        },
        "method_short":"MKD ViT-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-16",
        "metrics":{
            "Top 1 Accuracy":"85.1%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.1,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":962993,
            "title":"Meta Knowledge Distillation",
            "url":"\/paper\/meta-knowledge-distillation",
            "published":"2022-02-16T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/meta-knowledge-distillation\/review\/?hl=75999"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":326,
                "name":"Teacher-22k",
                "color":"#a96800"
            },
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":99852,
        "rank":236,
        "Model":"SP-ViT-S|384",
        "mlmodel":{

        },
        "method_short":"SP-ViT-S|384",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-15",
        "metrics":{
            "Top 1 Accuracy":"85.1%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.1,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1027709,
            "title":"SP-ViT: Learning 2D Spatial Priors for Vision Transformers",
            "url":"\/paper\/sp-vit-learning-2d-spatial-priors-for-vision",
            "published":"2022-06-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/sp-vit-learning-2d-spatial-priors-for-vision\/review\/?hl=99852"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":35216,
        "rank":237,
        "Model":"XCiT-S12",
        "mlmodel":{

        },
        "method_short":"XCiT-S12",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-17",
        "metrics":{
            "Top 1 Accuracy":"85.1%",
            "Number of params":"26M",
            "GFLOPs":"55.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.1,
            "Number of params":26000000.0,
            "GFLOPs":55.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":820317,
            "title":"XCiT: Cross-Covariance Image Transformers",
            "url":"\/paper\/xcit-cross-covariance-image-transformers",
            "published":"2021-06-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/xcit-cross-covariance-image-transformers\/review\/?hl=35216"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29205,
        "rank":238,
        "Model":"CAIT-S-24",
        "mlmodel":{

        },
        "method_short":"CAIT-S-24",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-31",
        "metrics":{
            "Top 1 Accuracy":"85.1%",
            "Number of params":"46.9M",
            "GFLOPs":"32.2",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.1,
            "Number of params":46900000.0,
            "GFLOPs":32.2,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":772635,
            "title":"Going deeper with Image Transformers",
            "url":"\/paper\/going-deeper-with-image-transformers",
            "published":"2021-03-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/going-deeper-with-image-transformers\/review\/?hl=29205"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":18191,
        "rank":239,
        "Model":"ResNet200_vd_26w_4s_ssld",
        "mlmodel":{

        },
        "method_short":"ResNet200_vd_26w_4s_ssld",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-18",
        "metrics":{
            "Top 1 Accuracy":"85.1%",
            "Number of params":"76M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.1,
            "Number of params":76000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":203194,
            "title":"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset",
            "url":"\/paper\/semi-supervised-recognition-under-a-noisy-and",
            "published":"2020-06-18T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":57828,
        "rank":240,
        "Model":"MixMIM-B",
        "mlmodel":{

        },
        "method_short":"MixMIM-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-26",
        "metrics":{
            "Top 1 Accuracy":"85.1%",
            "Number of params":"88M",
            "GFLOPs":"16.3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.1,
            "Number of params":88000000.0,
            "GFLOPs":16.3,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1016710,
            "title":"MixMAE: Mixed and Masked Autoencoder for Efficient Pretraining of Hierarchical Vision Transformers",
            "url":"\/paper\/mixmim-mixed-and-masked-image-modeling-for",
            "published":"2022-05-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mixmim-mixed-and-masked-image-modeling-for\/review\/?hl=57828"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":25395,
        "rank":241,
        "Model":"NFNet-F2",
        "mlmodel":{

        },
        "method_short":"NFNet-F2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-11",
        "metrics":{
            "Top 1 Accuracy":"85.1%",
            "Number of params":"193.8M",
            "GFLOPs":"62.59",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.1,
            "Number of params":193800000.0,
            "GFLOPs":62.59,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":744370,
            "title":"High-Performance Large-Scale Image Recognition Without Normalization",
            "url":"\/paper\/high-performance-large-scale-image",
            "published":"2021-02-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/high-performance-large-scale-image\/review\/?hl=25395"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6127,
        "rank":242,
        "Model":"ResNeXt-101 32x32d",
        "mlmodel":{

        },
        "method_short":"ResNeXt-101 32x32d",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-05-02",
        "metrics":{
            "Top 1 Accuracy":"85.1%",
            "Number of params":"466M",
            "GFLOPs":"174",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.1,
            "Number of params":466000000.0,
            "GFLOPs":174.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":4635,
            "title":"Exploring the Limits of Weakly Supervised Pretraining",
            "url":"\/paper\/exploring-the-limits-of-weakly-supervised",
            "published":"2018-05-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/exploring-the-limits-of-weakly-supervised\/review\/?hl=6127"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":6,
                "name":"ResNeXt",
                "color":"#86960b"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":44733,
        "rank":243,
        "Model":"DiscreteViT",
        "mlmodel":{

        },
        "method_short":"DiscreteViT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-20",
        "metrics":{
            "Top 1 Accuracy":"85.07%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.07,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":914582,
            "title":"Discrete Representations Strengthen Vision Transformer Robustness",
            "url":"\/paper\/discrete-representations-strengthen-vision-1",
            "published":"2021-11-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/discrete-representations-strengthen-vision-1\/review\/?hl=44733"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29228,
        "rank":244,
        "Model":"EfficientNetV2-S (21k)",
        "mlmodel":{

        },
        "method_short":"EfficientNetV2-S ",
        "method_details":"21k",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-01",
        "metrics":{
            "Top 1 Accuracy":"85.0%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.0,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":773495,
            "title":"EfficientNetV2: Smaller Models and Faster Training",
            "url":"\/paper\/efficientnetv2-smaller-models-and-faster",
            "published":"2021-04-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/efficientnetv2-smaller-models-and-faster\/review\/?hl=29228"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":87345,
        "rank":245,
        "Model":"ViT-M@224 (cosub)",
        "mlmodel":{

        },
        "method_short":"ViT-M@224 ",
        "method_details":"cosub",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-09",
        "metrics":{
            "Top 1 Accuracy":"85.0%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.0,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1126263,
            "title":"Co-training $2^L$ Submodels for Visual Recognition",
            "url":"\/paper\/co-training-2-l-submodels-for-visual",
            "published":"2022-12-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/co-training-2-l-submodels-for-visual\/review\/?hl=87345"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":112984,
        "rank":246,
        "Model":"ViC-MAE (ViT-L)",
        "mlmodel":{

        },
        "method_short":"ViC-MAE ",
        "method_details":"ViT-L",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-21",
        "metrics":{
            "Top 1 Accuracy":"85%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.0,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1177739,
            "title":"ViC-MAE: Self-Supervised Representation Learning from Images and Video with Contrastive Masked Autoencoders",
            "url":"\/paper\/visual-representation-learning-from-unlabeled",
            "published":"2023-03-21T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/visual-representation-learning-from-unlabeled\/review\/?hl=112984"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":10369,
        "rank":247,
        "Model":"FixEfficientNet-B3",
        "mlmodel":{

        },
        "method_short":"FixEfficientNet-B3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-18",
        "metrics":{
            "Top 1 Accuracy":"85%",
            "Number of params":"12M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.0,
            "Number of params":12000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":187431,
            "title":"Fixing the train-test resolution discrepancy: FixEfficientNet",
            "url":"\/paper\/fixing-the-train-test-resolution-discrepancy-2",
            "published":"2020-03-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fixing-the-train-test-resolution-discrepancy-2\/review\/?hl=10369"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":73469,
        "rank":248,
        "Model":"CAFormer-S18 (384 res)",
        "mlmodel":{

        },
        "method_short":"CAFormer-S18 ",
        "method_details":"384 res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"85.0%",
            "Number of params":"26M",
            "GFLOPs":"13.4",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.0,
            "Number of params":26000000.0,
            "GFLOPs":13.4,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=73469"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":88300,
        "rank":249,
        "Model":"ConvFormer-S18 (384 res, 21K)",
        "mlmodel":{

        },
        "method_short":"ConvFormer-S18 ",
        "method_details":"384 res, 21K",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"85.0%",
            "Number of params":"27M",
            "GFLOPs":"11.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.0,
            "Number of params":27000000.0,
            "GFLOPs":11.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=88300"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":16786,
        "rank":250,
        "Model":"EfficientNet-B7 (RandAugment)",
        "mlmodel":{

        },
        "method_short":"EfficientNet-B7 ",
        "method_details":"RandAugment",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-09-30",
        "metrics":{
            "Top 1 Accuracy":"85%",
            "Number of params":"66M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.0,
            "Number of params":66000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":156406,
            "title":"RandAugment: Practical automated data augmentation with a reduced search space",
            "url":"\/paper\/randaugment-practical-data-augmentation-with",
            "published":"2019-09-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/randaugment-practical-data-augmentation-with\/review\/?hl=16786"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":52539,
        "rank":251,
        "Model":"ViT-B @384 (DeiT III)",
        "mlmodel":{

        },
        "method_short":"ViT-B @384 ",
        "method_details":"DeiT III",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-14",
        "metrics":{
            "Top 1 Accuracy":"85.0%",
            "Number of params":"87M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.0,
            "Number of params":87000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":994466,
            "title":"DeiT III: Revenge of the ViT",
            "url":"\/paper\/deit-iii-revenge-of-the-vit",
            "published":"2022-04-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deit-iii-revenge-of-the-vit\/review\/?hl=52539"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":71540,
        "rank":252,
        "Model":"MaxViT-B (224res)",
        "mlmodel":{

        },
        "method_short":"MaxViT-B ",
        "method_details":"224res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-04",
        "metrics":{
            "Top 1 Accuracy":"84.95%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.95,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":988369,
            "title":"MaxViT: Multi-Axis Vision Transformer",
            "url":"\/paper\/maxvit-multi-axis-vision-transformer",
            "published":"2022-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/maxvit-multi-axis-vision-transformer\/review\/?hl=71540"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":108044,
        "rank":253,
        "Model":"CaiT-S24",
        "mlmodel":{

        },
        "method_short":"CaiT-S24",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-18",
        "metrics":{
            "Top 1 Accuracy":"84.91%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.91,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1265182,
            "title":"Which Transformer to Favor: A Comparative Analysis of Efficiency in Vision Transformers",
            "url":"\/paper\/which-transformer-to-favor-a-comparative",
            "published":"2023-08-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/which-transformer-to-favor-a-comparative\/review\/?hl=108044"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":52540,
        "rank":254,
        "Model":"ViT-L @224 (DeiT III)",
        "mlmodel":{

        },
        "method_short":"ViT-L @224 ",
        "method_details":"DeiT III",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-14",
        "metrics":{
            "Top 1 Accuracy":"84.9%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.9,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":994466,
            "title":"DeiT III: Revenge of the ViT",
            "url":"\/paper\/deit-iii-revenge-of-the-vit",
            "published":"2022-04-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deit-iii-revenge-of-the-vit\/review\/?hl=52540"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":99850,
        "rank":255,
        "Model":"Our SP-ViT-M",
        "mlmodel":{

        },
        "method_short":"Our SP-ViT-M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-15",
        "metrics":{
            "Top 1 Accuracy":"84.9%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.9,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1027709,
            "title":"SP-ViT: Learning 2D Spatial Priors for Vision Transformers",
            "url":"\/paper\/sp-vit-learning-2d-spatial-priors-for-vision",
            "published":"2022-06-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/sp-vit-learning-2d-spatial-priors-for-vision\/review\/?hl=99850"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29253,
        "rank":256,
        "Model":"CvT-21 (384 res, ImageNet-22k pretrain)",
        "mlmodel":{

        },
        "method_short":"CvT-21 ",
        "method_details":"384 res, ImageNet-22k pretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-29",
        "metrics":{
            "Top 1 Accuracy":"84.9%",
            "Number of params":"32M",
            "GFLOPs":"25",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.9,
            "Number of params":32000000.0,
            "GFLOPs":25.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":758429,
            "title":"CvT: Introducing Convolutions to Vision Transformers",
            "url":"\/paper\/cvt-introducing-convolutions-to-vision",
            "published":"2021-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/cvt-introducing-convolutions-to-vision\/review\/?hl=29253"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":102362,
        "rank":257,
        "Model":"InternImage-B",
        "mlmodel":{

        },
        "method_short":"InternImage-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-10",
        "metrics":{
            "Top 1 Accuracy":"84.9%",
            "Number of params":"97M",
            "GFLOPs":"16",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.9,
            "Number of params":97000000.0,
            "GFLOPs":16.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1109218,
            "title":"InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions",
            "url":"\/paper\/internimage-exploring-large-scale-vision",
            "published":"2022-11-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/internimage-exploring-large-scale-vision\/review\/?hl=102362"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60266,
        "rank":258,
        "Model":"TinyViT-21M-distill (21k)",
        "mlmodel":{

        },
        "method_short":"TinyViT-21M-distill ",
        "method_details":"21k",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-21",
        "metrics":{
            "Top 1 Accuracy":"84.8%",
            "Number of params":"21M",
            "GFLOPs":"4.3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.8,
            "Number of params":21000000.0,
            "GFLOPs":4.3,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1047551,
            "title":"TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
            "url":"\/paper\/tinyvit-fast-pretraining-distillation-for",
            "published":"2022-07-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/tinyvit-fast-pretraining-distillation-for\/review\/?hl=60266"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":63781,
        "rank":259,
        "Model":"Wave-ViT-B",
        "mlmodel":{

        },
        "method_short":"Wave-ViT-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-11",
        "metrics":{
            "Top 1 Accuracy":"84.8%",
            "Number of params":"33.5M",
            "GFLOPs":"7.2",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.8,
            "Number of params":33500000.0,
            "GFLOPs":7.2,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1042136,
            "title":"Wave-ViT: Unifying Wavelet and Transformers for Visual Representation Learning",
            "url":"\/paper\/wave-vit-unifying-wavelet-and-transformers",
            "published":"2022-07-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/wave-vit-unifying-wavelet-and-transformers\/review\/?hl=63781"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29206,
        "rank":260,
        "Model":"CAIT-XS-36",
        "mlmodel":{

        },
        "method_short":"CAIT-XS-36",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-31",
        "metrics":{
            "Top 1 Accuracy":"84.8%",
            "Number of params":"38.6M",
            "GFLOPs":"28.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.8,
            "Number of params":38600000.0,
            "GFLOPs":28.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":772635,
            "title":"Going deeper with Image Transformers",
            "url":"\/paper\/going-deeper-with-image-transformers",
            "published":"2021-03-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/going-deeper-with-image-transformers\/review\/?hl=29206"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":44122,
        "rank":261,
        "Model":"SReT-B (384 res, ImageNet-1K only)",
        "mlmodel":{

        },
        "method_short":"SReT-B ",
        "method_details":"384 res, ImageNet-1K only",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-09",
        "metrics":{
            "Top 1 Accuracy":"84.8%",
            "Number of params":"71.2M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.8,
            "Number of params":71200000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":907412,
            "title":"Sliced Recursive Transformer",
            "url":"\/paper\/sliced-recursive-transformer-1",
            "published":"2021-11-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/sliced-recursive-transformer-1\/review\/?hl=44122"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":38423,
        "rank":262,
        "Model":"MViT-B-24",
        "mlmodel":{

        },
        "method_short":"MViT-B-24",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-22",
        "metrics":{
            "Top 1 Accuracy":"84.8%",
            "Number of params":"72.9M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.8,
            "Number of params":72900000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":787025,
            "title":"Multiscale Vision Transformers",
            "url":"\/paper\/multiscale-vision-transformers",
            "published":"2021-04-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multiscale-vision-transformers\/review\/?hl=38423"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":49599,
        "rank":263,
        "Model":"ActiveMLP-L",
        "mlmodel":{

        },
        "method_short":"ActiveMLP-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-11",
        "metrics":{
            "Top 1 Accuracy":"84.8%",
            "Number of params":"76.4M",
            "GFLOPs":"36.4",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.8,
            "Number of params":76400000.0,
            "GFLOPs":36.4,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":975658,
            "title":"Active Token Mixer",
            "url":"\/paper\/activemlp-an-mlp-like-architecture-with",
            "published":"2022-03-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/activemlp-an-mlp-like-architecture-with\/review\/?hl=49599"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":45406,
        "rank":264,
        "Model":"DAT-B (384 res, IN-1K only)",
        "mlmodel":{

        },
        "method_short":"DAT-B ",
        "method_details":"384 res, IN-1K only",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-03",
        "metrics":{
            "Top 1 Accuracy":"84.8%",
            "Number of params":"88M",
            "GFLOPs":"49.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.8,
            "Number of params":88000000.0,
            "GFLOPs":49.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":934161,
            "title":"Vision Transformer with Deformable Attention",
            "url":"\/paper\/vision-transformer-with-deformable-attention",
            "published":"2022-01-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vision-transformer-with-deformable-attention\/review\/?hl=45406"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":113322,
        "rank":265,
        "Model":"MIRL(ViT-S-54)",
        "mlmodel":{

        },
        "method_short":"MIRL",
        "method_details":"ViT-S-54",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-09-22",
        "metrics":{
            "Top 1 Accuracy":"84.8%",
            "Number of params":"96M",
            "GFLOPs":"18.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.8,
            "Number of params":96000000.0,
            "GFLOPs":18.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1333554,
            "title":"Masked Image Residual Learning for Scaling Deeper Vision Transformers",
            "url":"\/paper\/masked-image-residual-learning-for-scaling-1",
            "published":"2023-09-25T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":73458,
        "rank":266,
        "Model":"ConvFormer-B36 (224 res)",
        "mlmodel":{

        },
        "method_short":"ConvFormer-B36 ",
        "method_details":"224 res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"84.8%",
            "Number of params":"100M",
            "GFLOPs":"22.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.8,
            "Number of params":100000000.0,
            "GFLOPs":22.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=73458"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":8298,
        "rank":267,
        "Model":"ResNeXt-101 32x16d (semi-weakly sup.)",
        "mlmodel":{

        },
        "method_short":"ResNeXt-101 32x16d ",
        "method_details":"semi-weakly sup.",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-05-02",
        "metrics":{
            "Top 1 Accuracy":"84.8%",
            "Number of params":"193M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.8,
            "Number of params":193000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":113449,
            "title":"Billion-scale semi-supervised learning for image classification",
            "url":"\/paper\/billion-scale-semi-supervised-learning-for",
            "published":"2019-05-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/billion-scale-semi-supervised-learning-for\/review\/?hl=8298"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":107,
                "name":"IG-1B",
                "color":"#2771D3"
            },
            {
                "id":6,
                "name":"ResNeXt",
                "color":"#86960b"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":44845,
        "rank":268,
        "Model":"ELSA-VOLO-D1",
        "mlmodel":{

        },
        "method_short":"ELSA-VOLO-D1",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-23",
        "metrics":{
            "Top 1 Accuracy":"84.7%",
            "Number of params":"27M",
            "GFLOPs":"8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.7,
            "Number of params":27000000.0,
            "GFLOPs":8.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":932838,
            "title":"ELSA: Enhanced Local Self-Attention for Vision Transformer",
            "url":"\/paper\/elsa-enhanced-local-self-attention-for-vision",
            "published":"2021-12-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/elsa-enhanced-local-self-attention-for-vision\/review\/?hl=44845"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":113277,
        "rank":269,
        "Model":"TransNeXt-Small (IN-1K supervised, 224)",
        "mlmodel":{

        },
        "method_short":"TransNeXt-Small ",
        "method_details":"IN-1K supervised, 224",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-28",
        "metrics":{
            "Top 1 Accuracy":"84.7%",
            "Number of params":"49.7M",
            "GFLOPs":"10.3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.7,
            "Number of params":49700000.0,
            "GFLOPs":10.3,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1331523,
            "title":"TransNeXt: Robust Foveal Visual Perception for Vision Transformers",
            "url":"\/paper\/transnext-robust-foveal-visual-perception-for",
            "published":"2023-11-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/transnext-robust-foveal-visual-perception-for\/review\/?hl=113277"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":63703,
        "rank":270,
        "Model":"Next-ViT-L @384",
        "mlmodel":{

        },
        "method_short":"Next-ViT-L @384",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-12",
        "metrics":{
            "Top 1 Accuracy":"84.7%",
            "Number of params":"57.8M",
            "GFLOPs":"32",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.7,
            "Number of params":57800000.0,
            "GFLOPs":32.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1042514,
            "title":"Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios",
            "url":"\/paper\/next-vit-next-generation-vision-transformer",
            "published":"2022-07-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/next-vit-next-generation-vision-transformer\/review\/?hl=63703"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":24943,
        "rank":271,
        "Model":"BoTNet T7",
        "mlmodel":{

        },
        "method_short":"BoTNet T7",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-27",
        "metrics":{
            "Top 1 Accuracy":"84.7%",
            "Number of params":"75.1M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.7,
            "Number of params":75100000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":739590,
            "title":"Bottleneck Transformers for Visual Recognition",
            "url":"\/paper\/bottleneck-transformers-for-visual",
            "published":"2021-01-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/bottleneck-transformers-for-visual\/review\/?hl=24943"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":78961,
        "rank":272,
        "Model":"MogaNet-L",
        "mlmodel":{

        },
        "method_short":"MogaNet-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-07",
        "metrics":{
            "Top 1 Accuracy":"84.7%",
            "Number of params":"83M",
            "GFLOPs":"15.9",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.7,
            "Number of params":83000000.0,
            "GFLOPs":15.9,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1106694,
            "title":"Efficient Multi-order Gated Aggregation Network",
            "url":"\/paper\/efficient-multi-order-gated-aggregation",
            "published":"2022-11-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/efficient-multi-order-gated-aggregation\/review\/?hl=78961"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":55678,
        "rank":273,
        "Model":"LITv2-B|384",
        "mlmodel":{

        },
        "method_short":"LITv2-B|384",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-26",
        "metrics":{
            "Top 1 Accuracy":"84.7%",
            "Number of params":"87M",
            "GFLOPs":"39.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.7,
            "Number of params":87000000.0,
            "GFLOPs":39.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1016651,
            "title":"Fast Vision Transformers with HiLo Attention",
            "url":"\/paper\/fast-vision-transformers-with-hilo-attention",
            "published":"2022-05-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fast-vision-transformers-with-hilo-attention\/review\/?hl=55678"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":25394,
        "rank":274,
        "Model":"NFNet-F1",
        "mlmodel":{

        },
        "method_short":"NFNet-F1",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-11",
        "metrics":{
            "Top 1 Accuracy":"84.7%",
            "Number of params":"132.6M",
            "GFLOPs":"35.54",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.7,
            "Number of params":132600000.0,
            "GFLOPs":35.54,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":744370,
            "title":"High-Performance Large-Scale Image Recognition Without Normalization",
            "url":"\/paper\/high-performance-large-scale-image",
            "published":"2021-02-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/high-performance-large-scale-image\/review\/?hl=25394"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":53852,
        "rank":275,
        "Model":"Sequencer2D-L\u2191392",
        "mlmodel":{

        },
        "method_short":"Sequencer2D-L\u2191392",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-04",
        "metrics":{
            "Top 1 Accuracy":"84.6%",
            "Number of params":"54M",
            "GFLOPs":"50.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.6,
            "Number of params":54000000.0,
            "GFLOPs":50.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1004136,
            "title":"Sequencer: Deep LSTM for Image Classification",
            "url":"\/paper\/sequencer-deep-lstm-for-image-classification",
            "published":"2022-05-04T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":63784,
        "rank":276,
        "Model":"SE-CoTNetD-152",
        "mlmodel":{

        },
        "method_short":"SE-CoTNetD-152",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-26",
        "metrics":{
            "Top 1 Accuracy":"84.6%",
            "Number of params":"55.8M",
            "GFLOPs":"26.5",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.6,
            "Number of params":55800000.0,
            "GFLOPs":26.5,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":841137,
            "title":"Contextual Transformer Networks for Visual Recognition",
            "url":"\/paper\/contextual-transformer-networks-for-visual",
            "published":"2021-07-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/contextual-transformer-networks-for-visual\/review\/?hl=63784"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":51585,
        "rank":277,
        "Model":"DaViT-B",
        "mlmodel":{

        },
        "method_short":"DaViT-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-07",
        "metrics":{
            "Top 1 Accuracy":"84.6%",
            "Number of params":"87.9M",
            "GFLOPs":"15.5",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.6,
            "Number of params":87900000.0,
            "GFLOPs":15.5,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":990738,
            "title":"DaViT: Dual Attention Vision Transformers",
            "url":"\/paper\/davit-dual-attention-vision-transformers",
            "published":"2022-04-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/davit-dual-attention-vision-transformers\/review\/?hl=51585"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":111671,
        "rank":278,
        "Model":"ReXNet-R_3.0",
        "mlmodel":{

        },
        "method_short":"ReXNet-R_3.0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-07-02",
        "metrics":{
            "Top 1 Accuracy":"84.5%",
            "Number of params":"34.8M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.5,
            "Number of params":34800000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":206716,
            "title":"Rethinking Channel Dimensions for Efficient Model Design",
            "url":"\/paper\/rexnet-diminishing-representational",
            "published":"2020-07-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rexnet-diminishing-representational\/review\/?hl=111671"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":73470,
        "rank":279,
        "Model":"CAFormer-S36 (224 res)",
        "mlmodel":{

        },
        "method_short":"CAFormer-S36 ",
        "method_details":"224 res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"84.5%",
            "Number of params":"39M",
            "GFLOPs":"8.0",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.5,
            "Number of params":39000000.0,
            "GFLOPs":8.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=73470"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":73471,
        "rank":280,
        "Model":"ConvFormer-M36 (224 res)",
        "mlmodel":{

        },
        "method_short":"ConvFormer-M36 ",
        "method_details":"224 res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"84.5%",
            "Number of params":"57M",
            "GFLOPs":"12.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.5,
            "Number of params":57000000.0,
            "GFLOPs":12.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=73471"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":59086,
        "rank":281,
        "Model":"GC ViT-B",
        "mlmodel":{

        },
        "method_short":"GC ViT-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-20",
        "metrics":{
            "Top 1 Accuracy":"84.5%",
            "Number of params":"90M",
            "GFLOPs":"14.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.5,
            "Number of params":90000000.0,
            "GFLOPs":14.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1029616,
            "title":"Global Context Vision Transformers",
            "url":"\/paper\/global-context-vision-transformers",
            "published":"2022-06-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/global-context-vision-transformers\/review\/?hl=59086"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":10906,
        "rank":282,
        "Model":"ResNeSt-269",
        "mlmodel":{

        },
        "method_short":"ResNeSt-269",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-04-19",
        "metrics":{
            "Top 1 Accuracy":"84.5%",
            "Number of params":"111M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.5,
            "Number of params":111000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":191448,
            "title":"ResNeSt: Split-Attention Networks",
            "url":"\/paper\/resnest-split-attention-networks",
            "published":"2020-04-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/resnest-split-attention-networks\/review\/?hl=10906"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":71539,
        "rank":283,
        "Model":"MaxViT-S (224res)",
        "mlmodel":{

        },
        "method_short":"MaxViT-S ",
        "method_details":"224res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-04",
        "metrics":{
            "Top 1 Accuracy":"84.45%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.45,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":988369,
            "title":"MaxViT: Multi-Axis Vision Transformer",
            "url":"\/paper\/maxvit-multi-axis-vision-transformer",
            "published":"2022-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/maxvit-multi-axis-vision-transformer\/review\/?hl=71539"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":5154,
        "rank":284,
        "Model":"GPIPE",
        "mlmodel":{

        },
        "method_short":"GPIPE",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-11-16",
        "metrics":{
            "Top 1 Accuracy":"84.4%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.4,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":62516,
            "title":"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism",
            "url":"\/paper\/gpipe-efficient-training-of-giant-neural",
            "published":"2018-11-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/gpipe-efficient-training-of-giant-neural\/review\/?hl=5154"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":73472,
        "rank":285,
        "Model":"ConvFormer-S18 (384 res)",
        "mlmodel":{

        },
        "method_short":"ConvFormer-S18 ",
        "method_details":"384 res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"84.4%",
            "Number of params":"27M",
            "GFLOPs":"11.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.4,
            "Number of params":27000000.0,
            "GFLOPs":11.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=73472"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":3084,
        "rank":286,
        "Model":"EfficientNet-B7",
        "mlmodel":{

        },
        "method_short":"EfficientNet-B7",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-05-28",
        "metrics":{
            "Top 1 Accuracy":"84.4%",
            "Number of params":"66M",
            "GFLOPs":"37",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.4,
            "Number of params":66000000.0,
            "GFLOPs":37.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":117456,
            "title":"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
            "url":"\/paper\/efficientnet-rethinking-model-scaling-for",
            "published":"2019-05-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/efficientnet-rethinking-model-scaling-for\/review\/?hl=3084"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":70628,
        "rank":287,
        "Model":"DiNAT-Base",
        "mlmodel":{

        },
        "method_short":"DiNAT-Base",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-29",
        "metrics":{
            "Top 1 Accuracy":"84.4%",
            "Number of params":"90M",
            "GFLOPs":"13.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.4,
            "Number of params":90000000.0,
            "GFLOPs":13.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1082646,
            "title":"Dilated Neighborhood Attention Transformer",
            "url":"\/paper\/dilated-neighborhood-attention-transformer",
            "published":"2022-09-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dilated-neighborhood-attention-transformer\/review\/?hl=70628"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":265,
                "name":"Neighborhood Attention",
                "color":"#ba1330"
            },
            {
                "id":266,
                "name":"NAT Transformer",
                "color":"#574896"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":28377,
        "rank":288,
        "Model":"ResNet-RS-50 (160 image res)",
        "mlmodel":{

        },
        "method_short":"ResNet-RS-50 ",
        "method_details":"160 image res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-13",
        "metrics":{
            "Top 1 Accuracy":"84.4%",
            "Number of params":"192M",
            "GFLOPs":"4.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.4,
            "Number of params":192000000.0,
            "GFLOPs":4.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":754198,
            "title":"Revisiting ResNets: Improved Training and Scaling Strategies",
            "url":"\/paper\/revisiting-resnets-improved-training-and",
            "published":"2021-03-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/revisiting-resnets-improved-training-and\/review\/?hl=28377"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":75981,
        "rank":289,
        "Model":"ColorNet (RHYLH with Conv Layer)",
        "mlmodel":{

        },
        "method_short":"ColorNet ",
        "method_details":"RHYLH with Conv Layer",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-02-01",
        "metrics":{
            "Top 1 Accuracy":"84.32%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.32,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":93126,
            "title":"ColorNet: Investigating the importance of color spaces for image classification",
            "url":"\/paper\/colornet-investigating-the-importance-of",
            "published":"2019-02-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/colornet-investigating-the-importance-of\/review\/?hl=75981"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":49539,
        "rank":290,
        "Model":"ViT-B@384 (attn finetune)",
        "mlmodel":{

        },
        "method_short":"ViT-B@384 ",
        "method_details":"attn finetune",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-18",
        "metrics":{
            "Top 1 Accuracy":"84.3%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.3,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":979672,
            "title":"Three things everyone should know about Vision Transformers",
            "url":"\/paper\/three-things-everyone-should-know-about",
            "published":"2022-03-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/three-things-everyone-should-know-about\/review\/?hl=49539"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":99523,
        "rank":291,
        "Model":"BiFormer-S (IN1k ptretrain)",
        "mlmodel":{

        },
        "method_short":"BiFormer-S* ",
        "method_details":"IN1k ptretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-15",
        "metrics":{
            "Top 1 Accuracy":"84.3%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.3,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1174425,
            "title":"BiFormer: Vision Transformer with Bi-Level Routing Attention",
            "url":"\/paper\/biformer-vision-transformer-with-bi-level",
            "published":"2023-03-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/biformer-vision-transformer-with-bi-level\/review\/?hl=99523"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":44121,
        "rank":292,
        "Model":"SReT-S (512 res, ImageNet-1K only)",
        "mlmodel":{

        },
        "method_short":"SReT-S ",
        "method_details":"512 res, ImageNet-1K only",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-09",
        "metrics":{
            "Top 1 Accuracy":"84.3%",
            "Number of params":"21.3M",
            "GFLOPs":"42.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.3,
            "Number of params":21300000.0,
            "GFLOPs":42.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":907412,
            "title":"Sliced Recursive Transformer",
            "url":"\/paper\/sliced-recursive-transformer-1",
            "published":"2021-11-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/sliced-recursive-transformer-1\/review\/?hl=44121"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":21051,
        "rank":293,
        "Model":"LambdaResNet200",
        "mlmodel":{

        },
        "method_short":"LambdaResNet200",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-17",
        "metrics":{
            "Top 1 Accuracy":"84.3%",
            "Number of params":"42M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.3,
            "Number of params":42000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":746268,
            "title":"LambdaNetworks: Modeling Long-Range Interactions Without Attention",
            "url":"\/paper\/lambdanetworks-modeling-long-range-1",
            "published":"2021-02-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lambdanetworks-modeling-long-range-1\/review\/?hl=21051"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":78960,
        "rank":294,
        "Model":"MogaNet-B",
        "mlmodel":{

        },
        "method_short":"MogaNet-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-07",
        "metrics":{
            "Top 1 Accuracy":"84.3%",
            "Number of params":"44M",
            "GFLOPs":"9.9",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.3,
            "Number of params":44000000.0,
            "GFLOPs":9.9,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1106694,
            "title":"Efficient Multi-order Gated Aggregation Network",
            "url":"\/paper\/efficient-multi-order-gated-aggregation",
            "published":"2022-11-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/efficient-multi-order-gated-aggregation\/review\/?hl=78960"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":10496,
        "rank":295,
        "Model":"TResNet-XL",
        "mlmodel":{

        },
        "method_short":"TResNet-XL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-30",
        "metrics":{
            "Top 1 Accuracy":"84.3%",
            "Number of params":"77M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.3,
            "Number of params":77000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":188636,
            "title":"TResNet: High Performance GPU-Dedicated Architecture",
            "url":"\/paper\/tresnet-high-performance-gpu-dedicated",
            "published":"2020-03-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/tresnet-high-performance-gpu-dedicated\/review\/?hl=10496"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":8299,
        "rank":296,
        "Model":"ResNeXt-101 32x8d (semi-weakly sup.)",
        "mlmodel":{

        },
        "method_short":"ResNeXt-101 32x8d ",
        "method_details":"semi-weakly sup.",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-05-02",
        "metrics":{
            "Top 1 Accuracy":"84.3%",
            "Number of params":"88M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.3,
            "Number of params":88000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":113449,
            "title":"Billion-scale semi-supervised learning for image classification",
            "url":"\/paper\/billion-scale-semi-supervised-learning-for",
            "published":"2019-05-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/billion-scale-semi-supervised-learning-for\/review\/?hl=8299"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":6,
                "name":"ResNeXt",
                "color":"#86960b"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":52492,
        "rank":297,
        "Model":"NAT-Base",
        "mlmodel":{

        },
        "method_short":"NAT-Base",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-14",
        "metrics":{
            "Top 1 Accuracy":"84.3%",
            "Number of params":"90M",
            "GFLOPs":"13.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.3,
            "Number of params":90000000.0,
            "GFLOPs":13.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":994419,
            "title":"Neighborhood Attention Transformer",
            "url":"\/paper\/neighborhood-attention-transformer",
            "published":"2022-04-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/neighborhood-attention-transformer\/review\/?hl=52492"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":265,
                "name":"Neighborhood Attention",
                "color":"#ba1330"
            },
            {
                "id":266,
                "name":"NAT Transformer",
                "color":"#574896"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":11200,
        "rank":298,
        "Model":"Assemble-ResNet152",
        "mlmodel":{

        },
        "method_short":"Assemble-ResNet152",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-01-17",
        "metrics":{
            "Top 1 Accuracy":"84.2%",
            "Number of params":null,
            "GFLOPs":"15.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.2,
            "Number of params":null,
            "GFLOPs":15.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":180140,
            "title":"Compounding the Performance Improvements of Assembled Techniques in a Convolutional Neural Network",
            "url":"\/paper\/compounding-the-performance-improvements-of",
            "published":"2020-01-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/compounding-the-performance-improvements-of\/review\/?hl=11200"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":24941,
        "rank":299,
        "Model":"BoTNet T7-320",
        "mlmodel":{

        },
        "method_short":"BoTNet T7-320",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-27",
        "metrics":{
            "Top 1 Accuracy":"84.2%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.2,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":739590,
            "title":"Bottleneck Transformers for Visual Recognition",
            "url":"\/paper\/bottleneck-transformers-for-visual",
            "published":"2021-01-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/bottleneck-transformers-for-visual\/review\/?hl=24941"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":38754,
        "rank":300,
        "Model":"ViP-B|384",
        "mlmodel":{

        },
        "method_short":"ViP-B|384",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-13",
        "metrics":{
            "Top 1 Accuracy":"84.2%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.2,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":834924,
            "title":"Visual Parser: Representing Part-whole Hierarchies with Transformers",
            "url":"\/paper\/visual-parser-representing-part-whole",
            "published":"2021-07-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/visual-parser-representing-part-whole\/review\/?hl=38754"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":55710,
        "rank":301,
        "Model":"Bamboo (Bamboo-B)",
        "mlmodel":{

        },
        "method_short":"Bamboo ",
        "method_details":"Bamboo-B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-21",
        "metrics":{
            "Top 1 Accuracy":"84.2",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.2,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1014071,
            "title":"A Study on Transformer Configuration and Training Objective",
            "url":"\/paper\/deeper-vs-wider-a-revisit-of-transformer",
            "published":"2022-05-21T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/deeper-vs-wider-a-revisit-of-transformer\/review\/?hl=55710"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":87350,
        "rank":302,
        "Model":"RegnetY16GF@224 (cosub)",
        "mlmodel":{

        },
        "method_short":"RegnetY16GF@224 ",
        "method_details":"cosub",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-09",
        "metrics":{
            "Top 1 Accuracy":"84.2%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.2,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1126263,
            "title":"Co-training $2^L$ Submodels for Visual Recognition",
            "url":"\/paper\/co-training-2-l-submodels-for-visual",
            "published":"2022-12-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/co-training-2-l-submodels-for-visual\/review\/?hl=87350"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":108798,
        "rank":303,
        "Model":"EfficientViT-B3 (r288)",
        "mlmodel":{

        },
        "method_short":"EfficientViT-B3 ",
        "method_details":"r288",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-29",
        "metrics":{
            "Top 1 Accuracy":"84.2%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.2,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1017975,
            "title":"EfficientViT: Multi-Scale Linear Attention for High-Resolution Dense Prediction",
            "url":"\/paper\/efficientvit-enhanced-linear-attention-for",
            "published":"2022-05-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/efficientvit-enhanced-linear-attention-for\/review\/?hl=108798"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":51586,
        "rank":304,
        "Model":"DaViT-S",
        "mlmodel":{

        },
        "method_short":"DaViT-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-07",
        "metrics":{
            "Top 1 Accuracy":"84.2%",
            "Number of params":"49.7M",
            "GFLOPs":"8.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.2,
            "Number of params":49700000.0,
            "GFLOPs":8.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":990738,
            "title":"DaViT: Dual Attention Vision Transformers",
            "url":"\/paper\/davit-dual-attention-vision-transformers",
            "published":"2022-04-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/davit-dual-attention-vision-transformers\/review\/?hl=51586"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":102361,
        "rank":305,
        "Model":"InternImage-S",
        "mlmodel":{

        },
        "method_short":"InternImage-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-10",
        "metrics":{
            "Top 1 Accuracy":"84.2%",
            "Number of params":"50M",
            "GFLOPs":"8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.2,
            "Number of params":50000000.0,
            "GFLOPs":8.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1109218,
            "title":"InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions",
            "url":"\/paper\/internimage-exploring-large-scale-vision",
            "published":"2022-11-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/internimage-exploring-large-scale-vision\/review\/?hl=102361"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":48308,
        "rank":306,
        "Model":"UniNet-B4",
        "mlmodel":{

        },
        "method_short":"UniNet-B4",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-08",
        "metrics":{
            "Top 1 Accuracy":"84.2%",
            "Number of params":"73.5M",
            "GFLOPs":"9.9",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.2,
            "Number of params":73500000.0,
            "GFLOPs":9.9,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":881795,
            "title":"UniNet: Unified Architecture Search with Convolution, Transformer, and MLP",
            "url":"\/paper\/uninet-unified-architecture-search-with",
            "published":"2021-10-08T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/uninet-unified-architecture-search-with\/review\/?hl=48308"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":23828,
        "rank":307,
        "Model":"DeiT-B",
        "mlmodel":{

        },
        "method_short":"DeiT-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-23",
        "metrics":{
            "Top 1 Accuracy":"84.2%",
            "Number of params":"86M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.2,
            "Number of params":86000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":731001,
            "title":"Training data-efficient image transformers & distillation through attention",
            "url":"\/paper\/training-data-efficient-image-transformers",
            "published":"2020-12-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/training-data-efficient-image-transformers\/review\/?hl=23828"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":105529,
        "rank":308,
        "Model":"ViT-B @224 (DeiT-III + AugSub)",
        "mlmodel":{

        },
        "method_short":"ViT-B @224 ",
        "method_details":"DeiT-III + AugSub",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-06-20",
        "metrics":{
            "Top 1 Accuracy":"84.2%",
            "Number of params":"86.6M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.2,
            "Number of params":86600000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1231534,
            "title":"Augmenting Sub-model to Improve Main Model",
            "url":"\/paper\/augmenting-sub-model-to-improve-main-model",
            "published":"2023-06-20T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":64206,
        "rank":309,
        "Model":"RevBiFPN-S6",
        "mlmodel":{

        },
        "method_short":"RevBiFPN-S6",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-28",
        "metrics":{
            "Top 1 Accuracy":"84.2%",
            "Number of params":"142.3M",
            "GFLOPs":"38.1",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.2,
            "Number of params":142300000.0,
            "GFLOPs":38.1,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1034397,
            "title":"RevBiFPN: The Fully Reversible Bidirectional Feature Pyramid Network",
            "url":"\/paper\/revbifpn-the-fully-reversible-bidirectional",
            "published":"2022-06-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/revbifpn-the-fully-reversible-bidirectional\/review\/?hl=64206"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":283,
                "name":"Reversible",
                "color":"#3227d3"
            },
            {
                "id":13,
                "name":"FPN",
                "color":"#009481"
            },
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60761,
        "rank":310,
        "Model":"ResNeXt-101 32\u00d716d",
        "mlmodel":{

        },
        "method_short":"ResNeXt-101 32\u00d716d",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-05-02",
        "metrics":{
            "Top 1 Accuracy":"84.2%",
            "Number of params":"194M",
            "GFLOPs":"72",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.2,
            "Number of params":194000000.0,
            "GFLOPs":72.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":4635,
            "title":"Exploring the Limits of Weakly Supervised Pretraining",
            "url":"\/paper\/exploring-the-limits-of-weakly-supervised",
            "published":"2018-05-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/exploring-the-limits-of-weakly-supervised\/review\/?hl=60761"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60748,
        "rank":311,
        "Model":"FBNetV5-F-CLS",
        "mlmodel":{

        },
        "method_short":"FBNetV5-F-CLS",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-19",
        "metrics":{
            "Top 1 Accuracy":"84.1%",
            "Number of params":null,
            "GFLOPs":"2.1",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.1,
            "Number of params":null,
            "GFLOPs":2.1,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":912986,
            "title":"FBNetV5: Neural Architecture Search for Multiple Tasks in One Run",
            "url":"\/paper\/fbnetv5-neural-architecture-search-for",
            "published":"2021-11-19T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/fbnetv5-neural-architecture-search-for\/review\/?hl=60748"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":49535,
        "rank":312,
        "Model":"ViT-B-36x1",
        "mlmodel":{

        },
        "method_short":"ViT-B-36x1",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-18",
        "metrics":{
            "Top 1 Accuracy":"84.1%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.1,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":979672,
            "title":"Three things everyone should know about Vision Transformers",
            "url":"\/paper\/three-things-everyone-should-know-about",
            "published":"2022-03-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/three-things-everyone-should-know-about\/review\/?hl=49535"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":49536,
        "rank":313,
        "Model":"ViT-B-18x2",
        "mlmodel":{

        },
        "method_short":"ViT-B-18x2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-18",
        "metrics":{
            "Top 1 Accuracy":"84.1%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.1,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":979672,
            "title":"Three things everyone should know about Vision Transformers",
            "url":"\/paper\/three-things-everyone-should-know-about",
            "published":"2022-03-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/three-things-everyone-should-know-about\/review\/?hl=49536"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":108988,
        "rank":314,
        "Model":"XCiT-M (+MixPro)",
        "mlmodel":{

        },
        "method_short":"XCiT-M ",
        "method_details":"+MixPro",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-24",
        "metrics":{
            "Top 1 Accuracy":"84.1%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.1,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1195795,
            "title":"MixPro: Data Augmentation with MaskMix and Progressive Attention Labeling for Vision Transformer",
            "url":"\/paper\/mixpro-data-augmentation-with-maskmix-and",
            "published":"2023-04-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mixpro-data-augmentation-with-maskmix-and\/review\/?hl=108988"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":11201,
        "rank":315,
        "Model":"NoisyStudent (EfficientNet-B3)",
        "mlmodel":{

        },
        "method_short":"NoisyStudent ",
        "method_details":"EfficientNet-B3",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-11",
        "metrics":{
            "Top 1 Accuracy":"84.1%",
            "Number of params":"12M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.1,
            "Number of params":12000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":170047,
            "title":"Self-training with Noisy Student improves ImageNet classification",
            "url":"\/paper\/self-training-with-noisy-student-improves",
            "published":"2019-11-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/self-training-with-noisy-student-improves\/review\/?hl=11201"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":88301,
        "rank":316,
        "Model":"CAFormer-S18 (224 res, 21K)",
        "mlmodel":{

        },
        "method_short":"CAFormer-S18 ",
        "method_details":"224 res, 21K",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"84.1%",
            "Number of params":"26M",
            "GFLOPs":"4.1",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.1,
            "Number of params":26000000.0,
            "GFLOPs":4.1,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=88301"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29207,
        "rank":317,
        "Model":"CAIT-XS-24",
        "mlmodel":{

        },
        "method_short":"CAIT-XS-24",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-31",
        "metrics":{
            "Top 1 Accuracy":"84.1%",
            "Number of params":"26.6M",
            "GFLOPs":"19.3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.1,
            "Number of params":26600000.0,
            "GFLOPs":19.3,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":772635,
            "title":"Going deeper with Image Transformers",
            "url":"\/paper\/going-deeper-with-image-transformers",
            "published":"2021-03-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/going-deeper-with-image-transformers\/review\/?hl=29207"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":73473,
        "rank":318,
        "Model":"ConvFormer-S36 (224 res)",
        "mlmodel":{

        },
        "method_short":"ConvFormer-S36 ",
        "method_details":"224 res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"84.1%",
            "Number of params":"40M",
            "GFLOPs":"7.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.1,
            "Number of params":40000000.0,
            "GFLOPs":7.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=73473"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":30487,
        "rank":319,
        "Model":"LV-ViT-M",
        "mlmodel":{

        },
        "method_short":"LV-ViT-M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-22",
        "metrics":{
            "Top 1 Accuracy":"84.1%",
            "Number of params":"56M",
            "GFLOPs":"16",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.1,
            "Number of params":56000000.0,
            "GFLOPs":16.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":787097,
            "title":"All Tokens Matter: Token Labeling for Training Better Vision Transformers",
            "url":"\/paper\/token-labeling-training-a-85-5-top-1-accuracy",
            "published":"2021-04-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/token-labeling-training-a-85-5-top-1-accuracy\/review\/?hl=30487"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":32090,
        "rank":320,
        "Model":"Conformer-B",
        "mlmodel":{

        },
        "method_short":"Conformer-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-09",
        "metrics":{
            "Top 1 Accuracy":"84.1%",
            "Number of params":"83.3M",
            "GFLOPs":"46.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.1,
            "Number of params":83300000.0,
            "GFLOPs":46.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":796024,
            "title":"Conformer: Local Features Coupling Global Representations for Visual Recognition",
            "url":"\/paper\/conformer-local-features-coupling-global",
            "published":"2021-05-09T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":44963,
        "rank":321,
        "Model":"PatchConvNet-B120",
        "mlmodel":{

        },
        "method_short":"PatchConvNet-B120",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-27",
        "metrics":{
            "Top 1 Accuracy":"84.1%",
            "Number of params":"188.6M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.1,
            "Number of params":188600000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":933338,
            "title":"Augmenting Convolutional networks with attention-based aggregation",
            "url":"\/paper\/augmenting-convolutional-networks-with",
            "published":"2021-12-27T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":167,
                "name":"PatchConvnet",
                "color":"#e0edd4"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":70345,
        "rank":322,
        "Model":"GPaCo (Vit-B)",
        "mlmodel":{

        },
        "method_short":"GPaCo ",
        "method_details":"Vit-B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-26",
        "metrics":{
            "Top 1 Accuracy":"84.0%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.0,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1080045,
            "title":"Generalized Parametric Contrastive Learning",
            "url":"\/paper\/generalized-parametric-contrastive-learning",
            "published":"2022-09-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/generalized-parametric-contrastive-learning\/review\/?hl=70345"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":10365,
        "rank":323,
        "Model":"FixEfficientNetB4",
        "mlmodel":{

        },
        "method_short":"FixEfficientNetB4",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-18",
        "metrics":{
            "Top 1 Accuracy":"84.0%",
            "Number of params":"19M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.0,
            "Number of params":19000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":187431,
            "title":"Fixing the train-test resolution discrepancy: FixEfficientNet",
            "url":"\/paper\/fixing-the-train-test-resolution-discrepancy-2",
            "published":"2020-03-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fixing-the-train-test-resolution-discrepancy-2\/review\/?hl=10365"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":18195,
        "rank":324,
        "Model":"Fix_ResNet50_vd_ssld",
        "mlmodel":{

        },
        "method_short":"Fix_ResNet50_vd_ssld",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-18",
        "metrics":{
            "Top 1 Accuracy":"84.0%",
            "Number of params":"25.58M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.0,
            "Number of params":25580000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":203194,
            "title":"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset",
            "url":"\/paper\/semi-supervised-recognition-under-a-noisy-and",
            "published":"2020-06-18T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":113276,
        "rank":325,
        "Model":"TransNeXt-Tiny (IN-1K supervised, 224)",
        "mlmodel":{

        },
        "method_short":"TransNeXt-Tiny ",
        "method_details":"IN-1K supervised, 224",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-28",
        "metrics":{
            "Top 1 Accuracy":"84.0%",
            "Number of params":"28.2M",
            "GFLOPs":"5.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.0,
            "Number of params":28200000.0,
            "GFLOPs":5.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1331523,
            "title":"TransNeXt: Robust Foveal Visual Perception for Vision Transformers",
            "url":"\/paper\/transnext-robust-foveal-visual-perception-for",
            "published":"2023-11-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/transnext-robust-foveal-visual-perception-for\/review\/?hl=113276"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":21050,
        "rank":326,
        "Model":"LambdaResNet152",
        "mlmodel":{

        },
        "method_short":"LambdaResNet152",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-17",
        "metrics":{
            "Top 1 Accuracy":"84.0%",
            "Number of params":"35M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.0,
            "Number of params":35000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":746268,
            "title":"LambdaNetworks: Modeling Long-Range Interactions Without Attention",
            "url":"\/paper\/lambdanetworks-modeling-long-range-1",
            "published":"2021-02-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lambdanetworks-modeling-long-range-1\/review\/?hl=21050"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6181,
        "rank":327,
        "Model":"EfficientNet-B6",
        "mlmodel":{

        },
        "method_short":"EfficientNet-B6",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-05-28",
        "metrics":{
            "Top 1 Accuracy":"84%",
            "Number of params":"43M",
            "GFLOPs":"19",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.0,
            "Number of params":43000000.0,
            "GFLOPs":19.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":117456,
            "title":"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
            "url":"\/paper\/efficientnet-rethinking-model-scaling-for",
            "published":"2019-05-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/efficientnet-rethinking-model-scaling-for\/review\/?hl=6181"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":59085,
        "rank":328,
        "Model":"GC ViT-S",
        "mlmodel":{

        },
        "method_short":"GC ViT-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-20",
        "metrics":{
            "Top 1 Accuracy":"84.0%",
            "Number of params":"51M",
            "GFLOPs":"8.5",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.0,
            "Number of params":51000000.0,
            "GFLOPs":8.5,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1029616,
            "title":"Global Context Vision Transformers",
            "url":"\/paper\/global-context-vision-transformers",
            "published":"2022-06-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/global-context-vision-transformers\/review\/?hl=59085"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":24940,
        "rank":329,
        "Model":"BoTNet T6",
        "mlmodel":{

        },
        "method_short":"BoTNet T6",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-27",
        "metrics":{
            "Top 1 Accuracy":"84%",
            "Number of params":"53.9M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.0,
            "Number of params":53900000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":739590,
            "title":"Bottleneck Transformers for Visual Recognition",
            "url":"\/paper\/bottleneck-transformers-for-visual",
            "published":"2021-01-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/bottleneck-transformers-for-visual\/review\/?hl=24940"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29410,
        "rank":330,
        "Model":"PiT-B",
        "mlmodel":{

        },
        "method_short":"PiT-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-30",
        "metrics":{
            "Top 1 Accuracy":"84%",
            "Number of params":"73.8M",
            "GFLOPs":"12.5",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.0,
            "Number of params":73800000.0,
            "GFLOPs":12.5,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":771780,
            "title":"Rethinking Spatial Dimensions of Vision Transformers",
            "url":"\/paper\/rethinking-spatial-dimensions-of-vision",
            "published":"2021-03-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-spatial-dimensions-of-vision\/review\/?hl=29410"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":98905,
        "rank":331,
        "Model":"DeepMAD-89M",
        "mlmodel":{

        },
        "method_short":"DeepMAD-89M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-05",
        "metrics":{
            "Top 1 Accuracy":"84%",
            "Number of params":"89M",
            "GFLOPs":"15.4",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.0,
            "Number of params":89000000.0,
            "GFLOPs":15.4,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1168443,
            "title":"DeepMAD: Mathematical Architecture Design for Deep Convolutional Neural Network",
            "url":"\/paper\/deepmad-mathematical-architecture-design-for",
            "published":"2023-03-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deepmad-mathematical-architecture-design-for\/review\/?hl=98905"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":99849,
        "rank":332,
        "Model":"Our SP-ViT-S",
        "mlmodel":{

        },
        "method_short":"Our SP-ViT-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-15",
        "metrics":{
            "Top 1 Accuracy":"83.9%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.9,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1027709,
            "title":"SP-ViT: Learning 2D Spatial Priors for Vision Transformers",
            "url":"\/paper\/sp-vit-learning-2d-spatial-priors-for-vision",
            "published":"2022-06-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/sp-vit-learning-2d-spatial-priors-for-vision\/review\/?hl=99849"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":112829,
        "rank":333,
        "Model":"UniRepLKNet-S",
        "mlmodel":{

        },
        "method_short":"UniRepLKNet-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-27",
        "metrics":{
            "Top 1 Accuracy":"83.9%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.9,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1327838,
            "title":"UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition",
            "url":"\/paper\/unireplknet-a-universal-perception-large",
            "published":"2023-11-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/unireplknet-a-universal-perception-large\/review\/?hl=112829"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":63782,
        "rank":334,
        "Model":"Wave-ViT-S",
        "mlmodel":{

        },
        "method_short":"Wave-ViT-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-11",
        "metrics":{
            "Top 1 Accuracy":"83.9%",
            "Number of params":"22.7M",
            "GFLOPs":"4.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.9,
            "Number of params":22700000.0,
            "GFLOPs":4.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1042136,
            "title":"Wave-ViT: Unifying Wavelet and Transformers for Visual Representation Learning",
            "url":"\/paper\/wave-vit-unifying-wavelet-and-transformers",
            "published":"2022-07-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/wave-vit-unifying-wavelet-and-transformers\/review\/?hl=63782"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29225,
        "rank":335,
        "Model":"EfficientNetV2-S",
        "mlmodel":{

        },
        "method_short":"EfficientNetV2-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-01",
        "metrics":{
            "Top 1 Accuracy":"83.9%",
            "Number of params":"24M",
            "GFLOPs":"8.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.9,
            "Number of params":24000000.0,
            "GFLOPs":8.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":773495,
            "title":"EfficientNetV2: Smaller Models and Faster Training",
            "url":"\/paper\/efficientnetv2-smaller-models-and-faster",
            "published":"2021-04-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/efficientnetv2-smaller-models-and-faster\/review\/?hl=29225"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":102752,
        "rank":336,
        "Model":"ASF-former-B",
        "mlmodel":{

        },
        "method_short":"ASF-former-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-26",
        "metrics":{
            "Top 1 Accuracy":"83.9%",
            "Number of params":"56.7M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.9,
            "Number of params":56700000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":999999,
            "title":"Adaptive Split-Fusion Transformer",
            "url":"\/paper\/adaptive-split-fusion-transformer",
            "published":"2022-04-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/adaptive-split-fusion-transformer\/review\/?hl=102752"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":34303,
        "rank":337,
        "Model":"DynamicViT-LV-M\/0.8",
        "mlmodel":{

        },
        "method_short":"DynamicViT-LV-M\/0.8",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-03",
        "metrics":{
            "Top 1 Accuracy":"83.9",
            "Number of params":"57.1M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.9,
            "Number of params":57100000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":810914,
            "title":"DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification",
            "url":"\/paper\/dynamicvit-efficient-vision-transformers-with",
            "published":"2021-06-03T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":27416,
        "rank":338,
        "Model":"TNT-B",
        "mlmodel":{

        },
        "method_short":"TNT-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-27",
        "metrics":{
            "Top 1 Accuracy":"83.9%",
            "Number of params":"65.6M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.9,
            "Number of params":65600000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":749686,
            "title":"Transformer in Transformer",
            "url":"\/paper\/transformer-in-transformer",
            "published":"2021-02-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/transformer-in-transformer\/review\/?hl=27416"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":10905,
        "rank":339,
        "Model":"ResNeSt-200",
        "mlmodel":{

        },
        "method_short":"ResNeSt-200",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-04-19",
        "metrics":{
            "Top 1 Accuracy":"83.9%",
            "Number of params":"70M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.9,
            "Number of params":70000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":191448,
            "title":"ResNeSt: Split-Attention Networks",
            "url":"\/paper\/resnest-split-attention-networks",
            "published":"2020-04-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/resnest-split-attention-networks\/review\/?hl=10905"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":4109,
        "rank":340,
        "Model":"AmoebaNet-A",
        "mlmodel":{

        },
        "method_short":"AmoebaNet-A",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-02-05",
        "metrics":{
            "Top 1 Accuracy":"83.9%",
            "Number of params":"469M",
            "GFLOPs":"208",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.9,
            "Number of params":469000000.0,
            "GFLOPs":208.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":9162,
            "title":"Regularized Evolution for Image Classifier Architecture Search",
            "url":"\/paper\/regularized-evolution-for-image-classifier",
            "published":"2018-02-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/regularized-evolution-for-image-classifier\/review\/?hl=4109"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":55044,
        "rank":341,
        "Model":"CLCNet (S:B4+D:B7)",
        "mlmodel":{

        },
        "method_short":"CLCNet ",
        "method_details":"S:B4+D:B7",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-19",
        "metrics":{
            "Top 1 Accuracy":"83.88%",
            "Number of params":null,
            "GFLOPs":"18.58",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.88,
            "Number of params":null,
            "GFLOPs":18.58,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1012740,
            "title":"CLCNet: Rethinking of Ensemble Modeling with Classification Confidence Network",
            "url":"\/paper\/clcnet-rethinking-of-ensemble-modeling-with",
            "published":"2022-05-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/clcnet-rethinking-of-ensemble-modeling-with\/review\/?hl=55044"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":28383,
        "rank":342,
        "Model":"ResNet-RS-270 (256 image res)",
        "mlmodel":{

        },
        "method_short":"ResNet-RS-270 ",
        "method_details":"256 image res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-13",
        "metrics":{
            "Top 1 Accuracy":"83.8%",
            "Number of params":null,
            "GFLOPs":"54",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.8,
            "Number of params":null,
            "GFLOPs":54.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":754198,
            "title":"Revisiting ResNets: Improved Training and Scaling Strategies",
            "url":"\/paper\/revisiting-resnets-improved-training-and",
            "published":"2021-03-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/revisiting-resnets-improved-training-and\/review\/?hl=28383"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":24939,
        "rank":343,
        "Model":"SENet-350",
        "mlmodel":{

        },
        "method_short":"SENet-350",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-27",
        "metrics":{
            "Top 1 Accuracy":"83.8%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.8,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":739590,
            "title":"Bottleneck Transformers for Visual Recognition",
            "url":"\/paper\/bottleneck-transformers-for-visual",
            "published":"2021-01-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/bottleneck-transformers-for-visual\/review\/?hl=24939"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":52538,
        "rank":344,
        "Model":"ViT-B @224 (DeiT III)",
        "mlmodel":{

        },
        "method_short":"ViT-B @224 ",
        "method_details":"DeiT III",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-14",
        "metrics":{
            "Top 1 Accuracy":"83.8%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.8,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":994466,
            "title":"DeiT III: Revenge of the ViT",
            "url":"\/paper\/deit-iii-revenge-of-the-vit",
            "published":"2022-04-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deit-iii-revenge-of-the-vit\/review\/?hl=52538"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":44120,
        "rank":345,
        "Model":"SReT-S (384 res, ImageNet-1K only)",
        "mlmodel":{

        },
        "method_short":"SReT-S ",
        "method_details":"384 res, ImageNet-1K only",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-09",
        "metrics":{
            "Top 1 Accuracy":"83.8%",
            "Number of params":"21M",
            "GFLOPs":"18.5",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.8,
            "Number of params":21000000.0,
            "GFLOPs":18.5,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":907412,
            "title":"Sliced Recursive Transformer",
            "url":"\/paper\/sliced-recursive-transformer-1",
            "published":"2021-11-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/sliced-recursive-transformer-1\/review\/?hl=44120"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":70627,
        "rank":346,
        "Model":"DiNAT-Small",
        "mlmodel":{

        },
        "method_short":"DiNAT-Small",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-29",
        "metrics":{
            "Top 1 Accuracy":"83.8%",
            "Number of params":"51M",
            "GFLOPs":"7.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.8,
            "Number of params":51000000.0,
            "GFLOPs":7.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1082646,
            "title":"Dilated Neighborhood Attention Transformer",
            "url":"\/paper\/dilated-neighborhood-attention-transformer",
            "published":"2022-09-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dilated-neighborhood-attention-transformer\/review\/?hl=70627"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":265,
                "name":"Neighborhood Attention",
                "color":"#ba1330"
            },
            {
                "id":266,
                "name":"NAT Transformer",
                "color":"#574896"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":33775,
        "rank":347,
        "Model":"Transformer local-attention (NesT-B)",
        "mlmodel":{

        },
        "method_short":"Transformer local-attention ",
        "method_details":"NesT-B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-26",
        "metrics":{
            "Top 1 Accuracy":"83.8%",
            "Number of params":"68M",
            "GFLOPs":"17.9",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.8,
            "Number of params":68000000.0,
            "GFLOPs":17.9,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":805906,
            "title":"Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding",
            "url":"\/paper\/aggregating-nested-transformers",
            "published":"2021-05-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/aggregating-nested-transformers\/review\/?hl=33775"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":36913,
        "rank":348,
        "Model":"PVTv2-B4",
        "mlmodel":{

        },
        "method_short":"PVTv2-B4",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-25",
        "metrics":{
            "Top 1 Accuracy":"83.8%",
            "Number of params":"82M",
            "GFLOPs":"11.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.8,
            "Number of params":82000000.0,
            "GFLOPs":11.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":825108,
            "title":"PVT v2: Improved Baselines with Pyramid Vision Transformer",
            "url":"\/paper\/pvtv2-improved-baselines-with-pyramid-vision",
            "published":"2021-06-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/pvtv2-improved-baselines-with-pyramid-vision\/review\/?hl=36913"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":108985,
        "rank":349,
        "Model":"CA-Swin-S (+MixPro)",
        "mlmodel":{

        },
        "method_short":"CA-Swin-S ",
        "method_details":"+MixPro",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-24",
        "metrics":{
            "Top 1 Accuracy":"83.7%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.7,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1195795,
            "title":"MixPro: Data Augmentation with MaskMix and Progressive Attention Labeling for Vision Transformer",
            "url":"\/paper\/mixpro-data-augmentation-with-maskmix-and",
            "published":"2023-04-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mixpro-data-augmentation-with-maskmix-and\/review\/?hl=108985"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":88299,
        "rank":350,
        "Model":"ConvFormer-S18 (224 res, 21K)",
        "mlmodel":{

        },
        "method_short":"ConvFormer-S18 ",
        "method_details":"224 res, 21K",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"83.7%",
            "Number of params":"27M",
            "GFLOPs":"3.9",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.7,
            "Number of params":27000000.0,
            "GFLOPs":3.9,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=88299"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":71445,
        "rank":351,
        "Model":"DAT-S",
        "mlmodel":{

        },
        "method_short":"DAT-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-03",
        "metrics":{
            "Top 1 Accuracy":"83.7%",
            "Number of params":"50M",
            "GFLOPs":"9.0",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.7,
            "Number of params":50000000.0,
            "GFLOPs":9.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":934161,
            "title":"Vision Transformer with Deformable Attention",
            "url":"\/paper\/vision-transformer-with-deformable-attention",
            "published":"2022-01-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vision-transformer-with-deformable-attention\/review\/?hl=71445"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":52491,
        "rank":352,
        "Model":"NAT-Small",
        "mlmodel":{

        },
        "method_short":"NAT-Small",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-14",
        "metrics":{
            "Top 1 Accuracy":"83.7%",
            "Number of params":"51M",
            "GFLOPs":"7.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.7,
            "Number of params":51000000.0,
            "GFLOPs":7.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":994419,
            "title":"Neighborhood Attention Transformer",
            "url":"\/paper\/neighborhood-attention-transformer",
            "published":"2022-04-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/neighborhood-attention-transformer\/review\/?hl=52491"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":265,
                "name":"Neighborhood Attention",
                "color":"#ba1330"
            },
            {
                "id":266,
                "name":"NAT Transformer",
                "color":"#574896"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":52669,
        "rank":353,
        "Model":"QnA-ViT-Base",
        "mlmodel":{

        },
        "method_short":"QnA-ViT-Base",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-21",
        "metrics":{
            "Top 1 Accuracy":"83.7%",
            "Number of params":"56M",
            "GFLOPs":"9.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.7,
            "Number of params":56000000.0,
            "GFLOPs":9.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":933167,
            "title":"Learned Queries for Efficient Local Attention",
            "url":"\/paper\/learned-queries-for-efficient-local-attention",
            "published":"2021-12-21T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":64205,
        "rank":354,
        "Model":"RevBiFPN-S5",
        "mlmodel":{

        },
        "method_short":"RevBiFPN-S5",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-28",
        "metrics":{
            "Top 1 Accuracy":"83.7%",
            "Number of params":"82M",
            "GFLOPs":"21.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.7,
            "Number of params":82000000.0,
            "GFLOPs":21.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1034397,
            "title":"RevBiFPN: The Fully Reversible Bidirectional Feature Pyramid Network",
            "url":"\/paper\/revbifpn-the-fully-reversible-bidirectional",
            "published":"2022-06-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/revbifpn-the-fully-reversible-bidirectional\/review\/?hl=64205"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":283,
                "name":"Reversible",
                "color":"#3227d3"
            },
            {
                "id":13,
                "name":"FPN",
                "color":"#009481"
            },
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":77457,
        "rank":355,
        "Model":"Pyramid ViG-B",
        "mlmodel":{

        },
        "method_short":"Pyramid ViG-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-01",
        "metrics":{
            "Top 1 Accuracy":"83.7%",
            "Number of params":"92.6M",
            "GFLOPs":"16.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.7,
            "Number of params":92600000.0,
            "GFLOPs":16.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1019895,
            "title":"Vision GNN: An Image is Worth Graph of Nodes",
            "url":"\/paper\/vision-gnn-an-image-is-worth-graph-of-nodes",
            "published":"2022-06-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vision-gnn-an-image-is-worth-graph-of-nodes\/review\/?hl=77457"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":32184,
        "rank":356,
        "Model":"Twins-SVT-L",
        "mlmodel":{

        },
        "method_short":"Twins-SVT-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-28",
        "metrics":{
            "Top 1 Accuracy":"83.7%",
            "Number of params":"99.2M",
            "GFLOPs":"15.1",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.7,
            "Number of params":99200000.0,
            "GFLOPs":15.1,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":788788,
            "title":"Twins: Revisiting the Design of Spatial Attention in Vision Transformers",
            "url":"\/paper\/twins-revisiting-spatial-attention-design-in",
            "published":"2021-04-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/twins-revisiting-spatial-attention-design-in\/review\/?hl=32184"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":71930,
        "rank":357,
        "Model":"TransBoost-ViT-S",
        "mlmodel":{

        },
        "method_short":"TransBoost-ViT-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-26",
        "metrics":{
            "Top 1 Accuracy":"83.67%",
            "Number of params":"22.05M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.67,
            "Number of params":22050000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1016638,
            "title":"TransBoost: Improving the Best ImageNet Performance using Deep Transduction",
            "url":"\/paper\/transboost-improving-the-best-imagenet",
            "published":"2022-05-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/transboost-improving-the-best-imagenet\/review\/?hl=71930"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":108030,
        "rank":358,
        "Model":"XCiT-S",
        "mlmodel":{

        },
        "method_short":"XCiT-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-18",
        "metrics":{
            "Top 1 Accuracy":"83.65%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.65,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1265182,
            "title":"Which Transformer to Favor: A Comparative Analysis of Efficiency in Vision Transformers",
            "url":"\/paper\/which-transformer-to-favor-a-comparative",
            "published":"2023-08-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/which-transformer-to-favor-a-comparative\/review\/?hl=108030"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":71538,
        "rank":359,
        "Model":"MaxViT-T (224res)",
        "mlmodel":{

        },
        "method_short":"MaxViT-T ",
        "method_details":"224res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-04",
        "metrics":{
            "Top 1 Accuracy":"83.62%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.62,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":988369,
            "title":"MaxViT: Multi-Axis Vision Transformer",
            "url":"\/paper\/maxvit-multi-axis-vision-transformer",
            "published":"2022-04-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/maxvit-multi-axis-vision-transformer\/review\/?hl=71538"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":108033,
        "rank":360,
        "Model":"Wave-ViT-S",
        "mlmodel":{

        },
        "method_short":"Wave-ViT-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-18",
        "metrics":{
            "Top 1 Accuracy":"83.61%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.61,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1265182,
            "title":"Which Transformer to Favor: A Comparative Analysis of Efficiency in Vision Transformers",
            "url":"\/paper\/which-transformer-to-favor-a-comparative",
            "published":"2023-08-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/which-transformer-to-favor-a-comparative\/review\/?hl=108033"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":55677,
        "rank":361,
        "Model":"LITv2-B",
        "mlmodel":{

        },
        "method_short":"LITv2-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-26",
        "metrics":{
            "Top 1 Accuracy":"83.6%",
            "Number of params":null,
            "GFLOPs":"13.2",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.6,
            "Number of params":null,
            "GFLOPs":13.2,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1016651,
            "title":"Fast Vision Transformers with HiLo Attention",
            "url":"\/paper\/fast-vision-transformers-with-hilo-attention",
            "published":"2022-05-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fast-vision-transformers-with-hilo-attention\/review\/?hl=55677"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":5008,
        "rank":362,
        "Model":"MultiGrain PNASNet (500px)",
        "mlmodel":{

        },
        "method_short":"MultiGrain PNASNet ",
        "method_details":"500px",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-02-14",
        "metrics":{
            "Top 1 Accuracy":"83.6%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.6,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":105902,
            "title":"MultiGrain: a unified image embedding for classes and instances",
            "url":"\/paper\/multigrain-a-unified-image-embedding-for",
            "published":"2019-02-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multigrain-a-unified-image-embedding-for\/review\/?hl=5008"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":44084,
        "rank":363,
        "Model":"MAE (ViT-L)",
        "mlmodel":{

        },
        "method_short":"MAE ",
        "method_details":"ViT-L",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-11",
        "metrics":{
            "Top 1 Accuracy":"83.6%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.6,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":908690,
            "title":"Masked Autoencoders Are Scalable Vision Learners",
            "url":"\/paper\/masked-autoencoders-are-scalable-vision",
            "published":"2021-11-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-autoencoders-are-scalable-vision\/review\/?hl=44084"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":98865,
        "rank":364,
        "Model":"PAT-B",
        "mlmodel":{

        },
        "method_short":"PAT-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-30",
        "metrics":{
            "Top 1 Accuracy":"83.6%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.6,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1120892,
            "title":"Pattern Attention Transformer with Doughnut Kernel",
            "url":"\/paper\/pattern-attention-transformer-with-doughnut",
            "published":"2022-11-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/pattern-attention-transformer-with-doughnut\/review\/?hl=98865"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":10370,
        "rank":365,
        "Model":"FixEfficientNet-B2",
        "mlmodel":{

        },
        "method_short":"FixEfficientNet-B2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-18",
        "metrics":{
            "Top 1 Accuracy":"83.6%",
            "Number of params":"9.2M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.6,
            "Number of params":9200000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":187431,
            "title":"Fixing the train-test resolution discrepancy: FixEfficientNet",
            "url":"\/paper\/fixing-the-train-test-resolution-discrepancy-2",
            "published":"2020-03-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fixing-the-train-test-resolution-discrepancy-2\/review\/?hl=10370"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":88185,
        "rank":366,
        "Model":"CAFormer-S18 (224 res)",
        "mlmodel":{

        },
        "method_short":"CAFormer-S18 ",
        "method_details":"224 res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"83.6%",
            "Number of params":"26M",
            "GFLOPs":"4.1",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.6,
            "Number of params":26000000.0,
            "GFLOPs":4.1,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=88185"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":86745,
        "rank":367,
        "Model":"IPT-B",
        "mlmodel":{

        },
        "method_short":"IPT-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-06",
        "metrics":{
            "Top 1 Accuracy":"83.6%",
            "Number of params":"39.3M",
            "GFLOPs":"7.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.6,
            "Number of params":39300000.0,
            "GFLOPs":7.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1124238,
            "title":"IncepFormer: Efficient Inception Transformer with Pyramid Pooling for Semantic Segmentation",
            "url":"\/paper\/incepformer-efficient-inception-transformer",
            "published":"2022-12-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/incepformer-efficient-inception-transformer\/review\/?hl=86745"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":36768,
        "rank":368,
        "Model":"ViTAE-B-Stage",
        "mlmodel":{

        },
        "method_short":"ViTAE-B-Stage",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-07",
        "metrics":{
            "Top 1 Accuracy":"83.6%",
            "Number of params":"48.5M",
            "GFLOPs":"27.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.6,
            "Number of params":48500000.0,
            "GFLOPs":27.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":812332,
            "title":"ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias",
            "url":"\/paper\/vitae-vision-transformer-advanced-by",
            "published":"2021-06-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vitae-vision-transformer-advanced-by\/review\/?hl=36768"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":34008,
        "rank":369,
        "Model":"ResT-Large",
        "mlmodel":{

        },
        "method_short":"ResT-Large",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-28",
        "metrics":{
            "Top 1 Accuracy":"83.6%",
            "Number of params":"51.63M",
            "GFLOPs":"7.9",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.6,
            "Number of params":51630000.0,
            "GFLOPs":7.9,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":807341,
            "title":"ResT: An Efficient Transformer for Visual Recognition",
            "url":"\/paper\/rest-an-efficient-transformer-for-visual",
            "published":"2021-05-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rest-an-efficient-transformer-for-visual\/review\/?hl=34008"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":25393,
        "rank":370,
        "Model":"NFNet-F0",
        "mlmodel":{

        },
        "method_short":"NFNet-F0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-11",
        "metrics":{
            "Top 1 Accuracy":"83.6%",
            "Number of params":"71.5M",
            "GFLOPs":"12.38",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.6,
            "Number of params":71500000.0,
            "GFLOPs":12.38,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":744370,
            "title":"High-Performance Large-Scale Image Recognition Without Normalization",
            "url":"\/paper\/high-performance-large-scale-image",
            "published":"2021-02-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/high-performance-large-scale-image\/review\/?hl=25393"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60627,
        "rank":371,
        "Model":"SE-ResNeXt-101, 64x4d, S=2(320px)",
        "mlmodel":{

        },
        "method_short":"SE-ResNeXt-101, 64x4d, S=2",
        "method_details":"320px",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-11-30",
        "metrics":{
            "Top 1 Accuracy":"83.6%",
            "Number of params":"98M",
            "GFLOPs":"38.2",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.6,
            "Number of params":98000000.0,
            "GFLOPs":38.2,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":237979,
            "title":"Towards Better Accuracy-efficiency Trade-offs: Divide and Co-training",
            "url":"\/paper\/splitnet-divide-and-co-training",
            "published":"2020-11-30T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":34939,
        "rank":372,
        "Model":"ResMLP-B24\/8",
        "mlmodel":{

        },
        "method_short":"ResMLP-B24\/8",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-07",
        "metrics":{
            "Top 1 Accuracy":"83.6%",
            "Number of params":"116M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.6,
            "Number of params":116000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":795412,
            "title":"ResMLP: Feedforward networks for image classification with data-efficient training",
            "url":"\/paper\/resmlp-feedforward-networks-for-image",
            "published":"2021-05-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/resmlp-feedforward-networks-for-image\/review\/?hl=34939"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":18,
                "name":"MLP",
                "color":"#ffae00"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":24938,
        "rank":373,
        "Model":"BoTNet T5",
        "mlmodel":{

        },
        "method_short":"BoTNet T5",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-27",
        "metrics":{
            "Top 1 Accuracy":"83.5%",
            "Number of params":null,
            "GFLOPs":"19.3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.5,
            "Number of params":null,
            "GFLOPs":19.3,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":739590,
            "title":"Bottleneck Transformers for Visual Recognition",
            "url":"\/paper\/bottleneck-transformers-for-visual",
            "published":"2021-01-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/bottleneck-transformers-for-visual\/review\/?hl=24938"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":102360,
        "rank":374,
        "Model":"InternImage-T",
        "mlmodel":{

        },
        "method_short":"InternImage-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-10",
        "metrics":{
            "Top 1 Accuracy":"83.5%",
            "Number of params":"30M",
            "GFLOPs":"5",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.5,
            "Number of params":30000000.0,
            "GFLOPs":5.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1109218,
            "title":"InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions",
            "url":"\/paper\/internimage-exploring-large-scale-vision",
            "published":"2022-11-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/internimage-exploring-large-scale-vision\/review\/?hl=102360"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":44962,
        "rank":375,
        "Model":"PatchConvNet-B60",
        "mlmodel":{

        },
        "method_short":"PatchConvNet-B60",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-27",
        "metrics":{
            "Top 1 Accuracy":"83.5%",
            "Number of params":"99.4M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.5,
            "Number of params":99400000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":933338,
            "title":"Augmenting Convolutional networks with attention-based aggregation",
            "url":"\/paper\/augmenting-convolutional-networks-with",
            "published":"2021-12-27T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":167,
                "name":"PatchConvnet",
                "color":"#e0edd4"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":49541,
        "rank":376,
        "Model":"ViT-B (hMLP + BeiT)",
        "mlmodel":{

        },
        "method_short":"ViT-B ",
        "method_details":"hMLP + BeiT",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-18",
        "metrics":{
            "Top 1 Accuracy":"83.4%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.4,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":979672,
            "title":"Three things everyone should know about Vision Transformers",
            "url":"\/paper\/three-things-everyone-should-know-about",
            "published":"2022-03-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/three-things-everyone-should-know-about\/review\/?hl=49541"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60754,
        "rank":377,
        "Model":"UniFormer-S",
        "mlmodel":{

        },
        "method_short":"UniFormer-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-24",
        "metrics":{
            "Top 1 Accuracy":"83.4%",
            "Number of params":"22M",
            "GFLOPs":"3.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.4,
            "Number of params":22000000.0,
            "GFLOPs":3.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":949483,
            "title":"UniFormer: Unifying Convolution and Self-attention for Visual Recognition",
            "url":"\/paper\/uniformer-unifying-convolution-and-self",
            "published":"2022-01-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/uniformer-unifying-convolution-and-self\/review\/?hl=60754"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":52537,
        "rank":378,
        "Model":"ViT-S @384 (DeiT III)",
        "mlmodel":{

        },
        "method_short":"ViT-S @384 ",
        "method_details":"DeiT III",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-14",
        "metrics":{
            "Top 1 Accuracy":"83.4%",
            "Number of params":"22M",
            "GFLOPs":"15.5",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.4,
            "Number of params":22000000.0,
            "GFLOPs":15.5,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":994466,
            "title":"DeiT III: Revenge of the ViT",
            "url":"\/paper\/deit-iii-revenge-of-the-vit",
            "published":"2022-04-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deit-iii-revenge-of-the-vit\/review\/?hl=52537"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":78959,
        "rank":379,
        "Model":"MogaNet-S",
        "mlmodel":{

        },
        "method_short":"MogaNet-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-07",
        "metrics":{
            "Top 1 Accuracy":"83.4%",
            "Number of params":"25M",
            "GFLOPs":"5",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.4,
            "Number of params":25000000.0,
            "GFLOPs":5.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1106694,
            "title":"Efficient Multi-order Gated Aggregation Network",
            "url":"\/paper\/efficient-multi-order-gated-aggregation",
            "published":"2022-11-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/efficient-multi-order-gated-aggregation\/review\/?hl=78959"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":59084,
        "rank":380,
        "Model":"GC ViT-T",
        "mlmodel":{

        },
        "method_short":"GC ViT-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-20",
        "metrics":{
            "Top 1 Accuracy":"83.4%",
            "Number of params":"28M",
            "GFLOPs":"4.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.4,
            "Number of params":28000000.0,
            "GFLOPs":4.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1029616,
            "title":"Global Context Vision Transformers",
            "url":"\/paper\/global-context-vision-transformers",
            "published":"2022-06-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/global-context-vision-transformers\/review\/?hl=59084"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":8300,
        "rank":381,
        "Model":"ResNeXt-101 32x4d (semi-weakly sup.)",
        "mlmodel":{

        },
        "method_short":"ResNeXt-101 32x4d ",
        "method_details":"semi-weakly sup.",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-05-02",
        "metrics":{
            "Top 1 Accuracy":"83.4%",
            "Number of params":"42M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.4,
            "Number of params":42000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":113449,
            "title":"Billion-scale semi-supervised learning for image classification",
            "url":"\/paper\/billion-scale-semi-supervised-learning-for",
            "published":"2019-05-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/billion-scale-semi-supervised-learning-for\/review\/?hl=8300"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":6,
                "name":"ResNeXt",
                "color":"#86960b"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":53849,
        "rank":382,
        "Model":"Sequencer2D-L",
        "mlmodel":{

        },
        "method_short":"Sequencer2D-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-04",
        "metrics":{
            "Top 1 Accuracy":"83.4%",
            "Number of params":"54M",
            "GFLOPs":"16.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.4,
            "Number of params":54000000.0,
            "GFLOPs":16.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1004136,
            "title":"Sequencer: Deep LSTM for Image Classification",
            "url":"\/paper\/sequencer-deep-lstm-for-image-classification",
            "published":"2022-05-04T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":39382,
        "rank":383,
        "Model":"sMLPNet-B (ImageNet-1k)",
        "mlmodel":{

        },
        "method_short":"sMLPNet-B ",
        "method_details":"ImageNet-1k",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-12",
        "metrics":{
            "Top 1 Accuracy":"83.4%",
            "Number of params":"65.9M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.4,
            "Number of params":65900000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":865635,
            "title":"Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?",
            "url":"\/paper\/sparse-mlp-for-image-recognition-is-self",
            "published":"2021-09-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/sparse-mlp-for-image-recognition-is-self\/review\/?hl=39382"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":18,
                "name":"MLP",
                "color":"#ffae00"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":22069,
        "rank":384,
        "Model":"SE-ResNeXt-101, 64x4d, S=2(416px)",
        "mlmodel":{

        },
        "method_short":"SE-ResNeXt-101, 64x4d, S=2",
        "method_details":"416px",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-11-30",
        "metrics":{
            "Top 1 Accuracy":"83.34%",
            "Number of params":"98M",
            "GFLOPs":"61.1",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.34,
            "Number of params":98000000.0,
            "GFLOPs":61.1,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":237979,
            "title":"Towards Better Accuracy-efficiency Trade-offs: Divide and Co-training",
            "url":"\/paper\/splitnet-divide-and-co-training",
            "published":"2020-11-30T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":6,
                "name":"ResNeXt",
                "color":"#86960b"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29250,
        "rank":385,
        "Model":"CvT-21 (384 res)",
        "mlmodel":{

        },
        "method_short":"CvT-21 ",
        "method_details":"384 res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-29",
        "metrics":{
            "Top 1 Accuracy":"83.3%",
            "Number of params":null,
            "GFLOPs":"24.9",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.3,
            "Number of params":null,
            "GFLOPs":24.9,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":758429,
            "title":"CvT: Introducing Convolutions to Vision Transformers",
            "url":"\/paper\/cvt-introducing-convolutions-to-vision",
            "published":"2021-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/cvt-introducing-convolutions-to-vision\/review\/?hl=29250"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29399,
        "rank":386,
        "Model":"T2T-ViT-14|384",
        "mlmodel":{

        },
        "method_short":"T2T-ViT-14|384",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-28",
        "metrics":{
            "Top 1 Accuracy":"83.3%",
            "Number of params":null,
            "GFLOPs":"34.2",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.3,
            "Number of params":null,
            "GFLOPs":34.2,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":740117,
            "title":"Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet",
            "url":"\/paper\/tokens-to-token-vit-training-vision",
            "published":"2021-01-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/tokens-to-token-vit-training-vision\/review\/?hl=29399"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":28491,
        "rank":387,
        "Model":"CeiT-S (384 finetune res)",
        "mlmodel":{

        },
        "method_short":"CeiT-S ",
        "method_details":"384 finetune res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-22",
        "metrics":{
            "Top 1 Accuracy":"83.3%",
            "Number of params":"24.2M",
            "GFLOPs":"12.9",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.3,
            "Number of params":24200000.0,
            "GFLOPs":12.9,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755729,
            "title":"Incorporating Convolution Designs into Visual Transformers",
            "url":"\/paper\/incorporating-convolution-designs-into-visual",
            "published":"2021-03-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/incorporating-convolution-designs-into-visual\/review\/?hl=28491"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60759,
        "rank":388,
        "Model":"LV-ViT-S",
        "mlmodel":{

        },
        "method_short":"LV-ViT-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-22",
        "metrics":{
            "Top 1 Accuracy":"83.3%",
            "Number of params":"26M",
            "GFLOPs":"6.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.3,
            "Number of params":26000000.0,
            "GFLOPs":6.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":787097,
            "title":"All Tokens Matter: Token Labeling for Training Better Vision Transformers",
            "url":"\/paper\/token-labeling-training-a-85-5-top-1-accuracy",
            "published":"2021-04-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/token-labeling-training-a-85-5-top-1-accuracy\/review\/?hl=60759"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":75969,
        "rank":389,
        "Model":"MOAT-0 1K only",
        "mlmodel":{

        },
        "method_short":"MOAT-0 1K only",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-04",
        "metrics":{
            "Top 1 Accuracy":"83.3%",
            "Number of params":"27.8M",
            "GFLOPs":"5.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.3,
            "Number of params":27800000.0,
            "GFLOPs":5.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1087041,
            "title":"MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models",
            "url":"\/paper\/moat-alternating-mobile-convolution-and",
            "published":"2022-10-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/moat-alternating-mobile-convolution-and\/review\/?hl=75969"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6182,
        "rank":390,
        "Model":"EfficientNet-B5",
        "mlmodel":{

        },
        "method_short":"EfficientNet-B5",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-05-28",
        "metrics":{
            "Top 1 Accuracy":"83.3%",
            "Number of params":"30M",
            "GFLOPs":"9.9",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.3,
            "Number of params":30000000.0,
            "GFLOPs":9.9,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":117456,
            "title":"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
            "url":"\/paper\/efficientnet-rethinking-model-scaling-for",
            "published":"2019-05-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/efficientnet-rethinking-model-scaling-for\/review\/?hl=6182"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":33774,
        "rank":391,
        "Model":"Transformer local-attention (NesT-S)",
        "mlmodel":{

        },
        "method_short":"Transformer local-attention ",
        "method_details":"NesT-S",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-26",
        "metrics":{
            "Top 1 Accuracy":"83.3%",
            "Number of params":"38M",
            "GFLOPs":"10.4",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.3,
            "Number of params":38000000.0,
            "GFLOPs":10.4,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":805906,
            "title":"Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding",
            "url":"\/paper\/aggregating-nested-transformers",
            "published":"2021-05-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/aggregating-nested-transformers\/review\/?hl=33774"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29123,
        "rank":392,
        "Model":"ViL-Medium-D",
        "mlmodel":{

        },
        "method_short":"ViL-Medium-D",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-29",
        "metrics":{
            "Top 1 Accuracy":"83.3%",
            "Number of params":"39.7M",
            "GFLOPs":"8.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.3,
            "Number of params":39700000.0,
            "GFLOPs":8.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":758437,
            "title":"Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding",
            "url":"\/paper\/2103-15358",
            "published":"2021-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/2103-15358\/review\/?hl=29123"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":55676,
        "rank":393,
        "Model":"LITv2-M",
        "mlmodel":{

        },
        "method_short":"LITv2-M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-26",
        "metrics":{
            "Top 1 Accuracy":"83.3%",
            "Number of params":"49M",
            "GFLOPs":"7.5",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.3,
            "Number of params":49000000.0,
            "GFLOPs":7.5,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1016651,
            "title":"Fast Vision Transformers with HiLo Attention",
            "url":"\/paper\/fast-vision-transformers-with-hilo-attention",
            "published":"2022-05-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fast-vision-transformers-with-hilo-attention\/review\/?hl=55676"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60628,
        "rank":394,
        "Model":"Shift-B",
        "mlmodel":{

        },
        "method_short":"Shift-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-26",
        "metrics":{
            "Top 1 Accuracy":"83.3%",
            "Number of params":"88M",
            "GFLOPs":"15.2",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.3,
            "Number of params":88000000.0,
            "GFLOPs":15.2,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":951213,
            "title":"When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism",
            "url":"\/paper\/when-shift-operation-meets-vision-transformer",
            "published":"2022-01-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/when-shift-operation-meets-vision-transformer\/review\/?hl=60628"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":11202,
        "rank":395,
        "Model":"MultiGrain PNASNet (450px)",
        "mlmodel":{

        },
        "method_short":"MultiGrain PNASNet ",
        "method_details":"450px",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-02-14",
        "metrics":{
            "Top 1 Accuracy":"83.2%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.2,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":105902,
            "title":"MultiGrain: a unified image embedding for classes and instances",
            "url":"\/paper\/multigrain-a-unified-image-embedding-for",
            "published":"2019-02-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multigrain-a-unified-image-embedding-for\/review\/?hl=11202"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":57971,
        "rank":396,
        "Model":"Meta Pseudo Labels (ResNet-50)",
        "mlmodel":{

        },
        "method_short":"Meta Pseudo Labels ",
        "method_details":"ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-23",
        "metrics":{
            "Top 1 Accuracy":"83.2%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.2,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":188189,
            "title":"Meta Pseudo Labels",
            "url":"\/paper\/meta-pseudo-labels",
            "published":"2020-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/meta-pseudo-labels\/review\/?hl=57971"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":112828,
        "rank":397,
        "Model":"UniRepLKNet-T",
        "mlmodel":{

        },
        "method_short":"UniRepLKNet-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-27",
        "metrics":{
            "Top 1 Accuracy":"83.2%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.2,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1327838,
            "title":"UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition",
            "url":"\/paper\/unireplknet-a-universal-perception-large",
            "published":"2023-11-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/unireplknet-a-universal-perception-large\/review\/?hl=112828"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60267,
        "rank":398,
        "Model":"TinyViT-11M-distill (21k)",
        "mlmodel":{

        },
        "method_short":"TinyViT-11M-distill ",
        "method_details":"21k",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-21",
        "metrics":{
            "Top 1 Accuracy":"83.2%",
            "Number of params":"11M",
            "GFLOPs":"2.0",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.2,
            "Number of params":11000000.0,
            "GFLOPs":2.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1047551,
            "title":"TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
            "url":"\/paper\/tinyvit-fast-pretraining-distillation-for",
            "published":"2022-07-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/tinyvit-fast-pretraining-distillation-for\/review\/?hl=60267"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":111672,
        "rank":399,
        "Model":"ReXNet-R_2.0",
        "mlmodel":{

        },
        "method_short":"ReXNet-R_2.0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-07-02",
        "metrics":{
            "Top 1 Accuracy":"83.2%",
            "Number of params":"16.5M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.2,
            "Number of params":16500000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":206716,
            "title":"Rethinking Channel Dimensions for Efficient Model Design",
            "url":"\/paper\/rexnet-diminishing-representational",
            "published":"2020-07-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rexnet-diminishing-representational\/review\/?hl=111672"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":52668,
        "rank":400,
        "Model":"QnA-ViT-Small",
        "mlmodel":{

        },
        "method_short":"QnA-ViT-Small",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-21",
        "metrics":{
            "Top 1 Accuracy":"83.2%",
            "Number of params":"25M",
            "GFLOPs":"4.4",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.2,
            "Number of params":25000000.0,
            "GFLOPs":4.4,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":933167,
            "title":"Learned Queries for Efficient Local Attention",
            "url":"\/paper\/learned-queries-for-efficient-local-attention",
            "published":"2021-12-21T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":52490,
        "rank":401,
        "Model":"NAT-Tiny",
        "mlmodel":{

        },
        "method_short":"NAT-Tiny",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-14",
        "metrics":{
            "Top 1 Accuracy":"83.2%",
            "Number of params":"28M",
            "GFLOPs":"4.3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.2,
            "Number of params":28000000.0,
            "GFLOPs":4.3,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":994419,
            "title":"Neighborhood Attention Transformer",
            "url":"\/paper\/neighborhood-attention-transformer",
            "published":"2022-04-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/neighborhood-attention-transformer\/review\/?hl=52490"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":265,
                "name":"Neighborhood Attention",
                "color":"#ba1330"
            },
            {
                "id":266,
                "name":"NAT Transformer",
                "color":"#574896"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":63783,
        "rank":402,
        "Model":"SE-CoTNetD-101",
        "mlmodel":{

        },
        "method_short":"SE-CoTNetD-101",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-26",
        "metrics":{
            "Top 1 Accuracy":"83.2%",
            "Number of params":"40.9M",
            "GFLOPs":"8.5",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.2,
            "Number of params":40900000.0,
            "GFLOPs":8.5,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":841137,
            "title":"Contextual Transformer Networks for Visual Recognition",
            "url":"\/paper\/contextual-transformer-networks-for-visual",
            "published":"2021-07-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/contextual-transformer-networks-for-visual\/review\/?hl=63783"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":63702,
        "rank":403,
        "Model":"Next-ViT-B",
        "mlmodel":{

        },
        "method_short":"Next-ViT-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-12",
        "metrics":{
            "Top 1 Accuracy":"83.2%",
            "Number of params":"44.8M",
            "GFLOPs":"8.3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.2,
            "Number of params":44800000.0,
            "GFLOPs":8.3,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1042514,
            "title":"Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios",
            "url":"\/paper\/next-vit-next-generation-vision-transformer",
            "published":"2022-07-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/next-vit-next-generation-vision-transformer\/review\/?hl=63702"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":36912,
        "rank":404,
        "Model":"PVTv2-B3",
        "mlmodel":{

        },
        "method_short":"PVTv2-B3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-25",
        "metrics":{
            "Top 1 Accuracy":"83.2%",
            "Number of params":"45.2M",
            "GFLOPs":"6.9",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.2,
            "Number of params":45200000.0,
            "GFLOPs":6.9,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":825108,
            "title":"PVT v2: Improved Baselines with Pyramid Vision Transformer",
            "url":"\/paper\/pvtv2-improved-baselines-with-pyramid-vision",
            "published":"2021-06-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/pvtv2-improved-baselines-with-pyramid-vision\/review\/?hl=36912"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":44961,
        "rank":405,
        "Model":"PatchConvNet-S120",
        "mlmodel":{

        },
        "method_short":"PatchConvNet-S120",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-27",
        "metrics":{
            "Top 1 Accuracy":"83.2%",
            "Number of params":"47.7M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.2,
            "Number of params":47700000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":933338,
            "title":"Augmenting Convolutional networks with attention-based aggregation",
            "url":"\/paper\/augmenting-convolutional-networks-with",
            "published":"2021-12-27T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":167,
                "name":"PatchConvnet",
                "color":"#e0edd4"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29125,
        "rank":406,
        "Model":"ViL-Base-D",
        "mlmodel":{

        },
        "method_short":"ViL-Base-D",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-29",
        "metrics":{
            "Top 1 Accuracy":"83.2%",
            "Number of params":"55.7M",
            "GFLOPs":"13.4",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.2,
            "Number of params":55700000.0,
            "GFLOPs":13.4,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":758437,
            "title":"Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding",
            "url":"\/paper\/2103-15358",
            "published":"2021-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/2103-15358\/review\/?hl=29125"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":37317,
        "rank":407,
        "Model":"CycleMLP-B5",
        "mlmodel":{

        },
        "method_short":"CycleMLP-B5",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-21",
        "metrics":{
            "Top 1 Accuracy":"83.2%",
            "Number of params":"76M",
            "GFLOPs":"12.3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.2,
            "Number of params":76000000.0,
            "GFLOPs":12.3,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":839264,
            "title":"CycleMLP: A MLP-like Architecture for Dense Prediction",
            "url":"\/paper\/cyclemlp-a-mlp-like-architecture-for-dense",
            "published":"2021-07-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/cyclemlp-a-mlp-like-architecture-for-dense\/review\/?hl=37317"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":18,
                "name":"MLP",
                "color":"#ffae00"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":11203,
        "rank":408,
        "Model":"MultiGrain SENet154 (450px)",
        "mlmodel":{

        },
        "method_short":"MultiGrain SENet154 ",
        "method_details":"450px",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-02-14",
        "metrics":{
            "Top 1 Accuracy":"83.1%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.1,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":105902,
            "title":"MultiGrain: a unified image embedding for classes and instances",
            "url":"\/paper\/multigrain-a-unified-image-embedding-for",
            "published":"2019-02-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multigrain-a-unified-image-embedding-for\/review\/?hl=11203"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":28547,
        "rank":409,
        "Model":"DeepVit-L (DeiT training recipe)",
        "mlmodel":{

        },
        "method_short":"DeepVit-L* ",
        "method_details":"DeiT training recipe",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-22",
        "metrics":{
            "Top 1 Accuracy":"83.1%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.1,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755709,
            "title":"DeepViT: Towards Deeper Vision Transformer",
            "url":"\/paper\/deepvit-towards-deeper-vision-transformer",
            "published":"2021-03-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deepvit-towards-deeper-vision-transformer\/review\/?hl=28547"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":52543,
        "rank":410,
        "Model":"ViT-S @224 (DeiT III, 21k)",
        "mlmodel":{

        },
        "method_short":"ViT-S @224 ",
        "method_details":"DeiT III, 21k",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-14",
        "metrics":{
            "Top 1 Accuracy":"83.1%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.1,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":994466,
            "title":"DeiT III: Revenge of the ViT",
            "url":"\/paper\/deit-iii-revenge-of-the-vit",
            "published":"2022-04-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deit-iii-revenge-of-the-vit\/review\/?hl=52543"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":75998,
        "rank":411,
        "Model":"MKD ViT-S",
        "mlmodel":{

        },
        "method_short":"MKD ViT-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-16",
        "metrics":{
            "Top 1 Accuracy":"83.1%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.1,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":962993,
            "title":"Meta Knowledge Distillation",
            "url":"\/paper\/meta-knowledge-distillation",
            "published":"2022-02-16T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/meta-knowledge-distillation\/review\/?hl=75998"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":87346,
        "rank":412,
        "Model":"ViT-S@224 (cosub)",
        "mlmodel":{

        },
        "method_short":"ViT-S@224 ",
        "method_details":"cosub",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-09",
        "metrics":{
            "Top 1 Accuracy":"83.1%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.1,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1126263,
            "title":"Co-training $2^L$ Submodels for Visual Recognition",
            "url":"\/paper\/co-training-2-l-submodels-for-visual",
            "published":"2022-12-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/co-training-2-l-submodels-for-visual\/review\/?hl=87346"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":98864,
        "rank":413,
        "Model":"PAT-S",
        "mlmodel":{

        },
        "method_short":"PAT-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-30",
        "metrics":{
            "Top 1 Accuracy":"83.1%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.1,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1120892,
            "title":"Pattern Attention Transformer with Doughnut Kernel",
            "url":"\/paper\/pattern-attention-transformer-with-doughnut",
            "published":"2022-11-30T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/pattern-attention-transformer-with-doughnut\/review\/?hl=98864"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60260,
        "rank":414,
        "Model":"TinyViT-21M",
        "mlmodel":{

        },
        "method_short":"TinyViT-21M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-21",
        "metrics":{
            "Top 1 Accuracy":"83.1%",
            "Number of params":"21M",
            "GFLOPs":"4.3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.1,
            "Number of params":21000000.0,
            "GFLOPs":4.3,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1047551,
            "title":"TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
            "url":"\/paper\/tinyvit-fast-pretraining-distillation-for",
            "published":"2022-07-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/tinyvit-fast-pretraining-distillation-for\/review\/?hl=60260"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":39381,
        "rank":415,
        "Model":"sMLPNet-S (ImageNet-1k)",
        "mlmodel":{

        },
        "method_short":"sMLPNet-S ",
        "method_details":"ImageNet-1k",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-12",
        "metrics":{
            "Top 1 Accuracy":"83.1%",
            "Number of params":"48.6M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.1,
            "Number of params":48600000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":865635,
            "title":"Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?",
            "url":"\/paper\/sparse-mlp-for-image-recognition-is-self",
            "published":"2021-09-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/sparse-mlp-for-image-recognition-is-self\/review\/?hl=39381"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":18,
                "name":"MLP",
                "color":"#ffae00"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":77456,
        "rank":416,
        "Model":"Pyramid ViG-M",
        "mlmodel":{

        },
        "method_short":"Pyramid ViG-M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-01",
        "metrics":{
            "Top 1 Accuracy":"83.1%",
            "Number of params":"51.7M",
            "GFLOPs":"8.9",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.1,
            "Number of params":51700000.0,
            "GFLOPs":8.9,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1019895,
            "title":"Vision GNN: An Image is Worth Graph of Nodes",
            "url":"\/paper\/vision-gnn-an-image-is-worth-graph-of-nodes",
            "published":"2022-06-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vision-gnn-an-image-is-worth-graph-of-nodes\/review\/?hl=77456"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":108032,
        "rank":417,
        "Model":"SwinV2-Ti",
        "mlmodel":{

        },
        "method_short":"SwinV2-Ti",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-18",
        "metrics":{
            "Top 1 Accuracy":"83.09%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.09,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1265182,
            "title":"Which Transformer to Favor: A Comparative Analysis of Efficiency in Vision Transformers",
            "url":"\/paper\/which-transformer-to-favor-a-comparative",
            "published":"2023-08-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/which-transformer-to-favor-a-comparative\/review\/?hl=108032"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":66005,
        "rank":418,
        "Model":"gSwin-S",
        "mlmodel":{

        },
        "method_short":"gSwin-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-08-24",
        "metrics":{
            "Top 1 Accuracy":"83.01%",
            "Number of params":"39.8M",
            "GFLOPs":"7.0",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.01,
            "Number of params":39800000.0,
            "GFLOPs":7.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1064636,
            "title":"gSwin: Gated MLP Vision Model with Hierarchical Structure of Shifted Window",
            "url":"\/paper\/gswin-gated-mlp-vision-model-with",
            "published":"2022-08-24T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/gswin-gated-mlp-vision-model-with\/review\/?hl=66005"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":48,
                "name":"Swin-Transformer",
                "color":"#f75c2f"
            },
            {
                "id":18,
                "name":"MLP",
                "color":"#ffae00"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":5009,
        "rank":419,
        "Model":"MultiGrain SENet154 (400px)",
        "mlmodel":{

        },
        "method_short":"MultiGrain SENet154 ",
        "method_details":"400px",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-02-14",
        "metrics":{
            "Top 1 Accuracy":"83.0%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.0,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":105902,
            "title":"MultiGrain: a unified image embedding for classes and instances",
            "url":"\/paper\/multigrain-a-unified-image-embedding-for",
            "published":"2019-02-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multigrain-a-unified-image-embedding-for\/review\/?hl=5009"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29249,
        "rank":420,
        "Model":"CvT-13 (384 res)",
        "mlmodel":{

        },
        "method_short":"CvT-13 ",
        "method_details":"384 res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-29",
        "metrics":{
            "Top 1 Accuracy":"83%",
            "Number of params":"20M",
            "GFLOPs":"16.3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.0,
            "Number of params":20000000.0,
            "GFLOPs":16.3,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":758429,
            "title":"CvT: Introducing Convolutions to Vision Transformers",
            "url":"\/paper\/cvt-introducing-convolutions-to-vision",
            "published":"2021-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/cvt-introducing-convolutions-to-vision\/review\/?hl=29249"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":18193,
        "rank":421,
        "Model":"ResNet50_vd_ssld",
        "mlmodel":{

        },
        "method_short":"ResNet50_vd_ssld",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-18",
        "metrics":{
            "Top 1 Accuracy":"83.0%",
            "Number of params":"25.58M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.0,
            "Number of params":25580000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":203194,
            "title":"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset",
            "url":"\/paper\/semi-supervised-recognition-under-a-noisy-and",
            "published":"2020-06-18T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":73475,
        "rank":422,
        "Model":"ConvFormer-S18 (224 res)",
        "mlmodel":{

        },
        "method_short":"ConvFormer-S18 ",
        "method_details":"224 res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-24",
        "metrics":{
            "Top 1 Accuracy":"83.0%",
            "Number of params":"27M",
            "GFLOPs":"3.9",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.0,
            "Number of params":27000000.0,
            "GFLOPs":3.9,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1098433,
            "title":"MetaFormer Baselines for Vision",
            "url":"\/paper\/metaformer-baselines-for-vision",
            "published":"2022-10-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-baselines-for-vision\/review\/?hl=73475"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":38422,
        "rank":423,
        "Model":"MViT-B-16",
        "mlmodel":{

        },
        "method_short":"MViT-B-16",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-22",
        "metrics":{
            "Top 1 Accuracy":"83.0%",
            "Number of params":"37.0M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.0,
            "Number of params":37000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":787025,
            "title":"Multiscale Vision Transformers",
            "url":"\/paper\/multiscale-vision-transformers",
            "published":"2021-04-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multiscale-vision-transformers\/review\/?hl=38422"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":10904,
        "rank":424,
        "Model":"ResNeSt-101",
        "mlmodel":{

        },
        "method_short":"ResNeSt-101",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-04-19",
        "metrics":{
            "Top 1 Accuracy":"83.0%",
            "Number of params":"48M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.0,
            "Number of params":48000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":191448,
            "title":"ResNeSt: Split-Attention Networks",
            "url":"\/paper\/resnest-split-attention-networks",
            "published":"2020-04-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/resnest-split-attention-networks\/review\/?hl=10904"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":64204,
        "rank":425,
        "Model":"RevBiFPN-S4",
        "mlmodel":{

        },
        "method_short":"RevBiFPN-S4",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-28",
        "metrics":{
            "Top 1 Accuracy":"83%",
            "Number of params":"48.7M",
            "GFLOPs":"10.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.0,
            "Number of params":48700000.0,
            "GFLOPs":10.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1034397,
            "title":"RevBiFPN: The Fully Reversible Bidirectional Feature Pyramid Network",
            "url":"\/paper\/revbifpn-the-fully-reversible-bidirectional",
            "published":"2022-06-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/revbifpn-the-fully-reversible-bidirectional\/review\/?hl=64204"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":283,
                "name":"Reversible",
                "color":"#3227d3"
            },
            {
                "id":13,
                "name":"FPN",
                "color":"#009481"
            },
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":54294,
        "rank":426,
        "Model":"ZenNAS (0.8ms)",
        "mlmodel":{

        },
        "method_short":"ZenNAS ",
        "method_details":"0.8ms",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-01",
        "metrics":{
            "Top 1 Accuracy":"83.0%",
            "Number of params":"183M",
            "GFLOPs":"13.9",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.0,
            "Number of params":183000000.0,
            "GFLOPs":13.9,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":740994,
            "title":"Zen-NAS: A Zero-Shot NAS for High-Performance Deep Image Recognition",
            "url":"\/paper\/zen-nas-a-zero-shot-nas-for-high-performance",
            "published":"2021-02-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/zen-nas-a-zero-shot-nas-for-high-performance\/review\/?hl=54294"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60738,
        "rank":427,
        "Model":"NASViT (supernet)",
        "mlmodel":{

        },
        "method_short":"NASViT ",
        "method_details":"supernet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-29",
        "metrics":{
            "Top 1 Accuracy":"82.9%",
            "Number of params":null,
            "GFLOPs":"1.881",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.9,
            "Number of params":null,
            "GFLOPs":1.881,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":882976,
            "title":"NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training",
            "url":"\/paper\/nasvit-neural-architecture-search-for",
            "published":"2021-09-29T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":108984,
        "rank":428,
        "Model":"DeiT-B (+MixPro)",
        "mlmodel":{

        },
        "method_short":"DeiT-B ",
        "method_details":"+MixPro",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-24",
        "metrics":{
            "Top 1 Accuracy":"82.9%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.9,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1195795,
            "title":"MixPro: Data Augmentation with MaskMix and Progressive Attention Labeling for Vision Transformer",
            "url":"\/paper\/mixpro-data-augmentation-with-maskmix-and",
            "published":"2023-04-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mixpro-data-augmentation-with-maskmix-and\/review\/?hl=108984"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":86744,
        "rank":429,
        "Model":"IPT-S",
        "mlmodel":{

        },
        "method_short":"IPT-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-06",
        "metrics":{
            "Top 1 Accuracy":"82.9%",
            "Number of params":"24.3M",
            "GFLOPs":"4.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.9,
            "Number of params":24300000.0,
            "GFLOPs":4.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1124238,
            "title":"IncepFormer: Efficient Inception Transformer with Pyramid Pooling for Semantic Segmentation",
            "url":"\/paper\/incepformer-efficient-inception-transformer",
            "published":"2022-12-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/incepformer-efficient-inception-transformer\/review\/?hl=86744"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29122,
        "rank":430,
        "Model":"ViL-Medium-W",
        "mlmodel":{

        },
        "method_short":"ViL-Medium-W",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-29",
        "metrics":{
            "Top 1 Accuracy":"82.9%",
            "Number of params":"39.8M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.9,
            "Number of params":39800000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":758437,
            "title":"Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding",
            "url":"\/paper\/2103-15358",
            "published":"2021-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/2103-15358\/review\/?hl=29122"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":36274,
        "rank":431,
        "Model":"GFNet-H-B",
        "mlmodel":{

        },
        "method_short":"GFNet-H-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-01",
        "metrics":{
            "Top 1 Accuracy":"82.9%",
            "Number of params":"54M",
            "GFLOPs":"8.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.9,
            "Number of params":54000000.0,
            "GFLOPs":8.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":828703,
            "title":"Global Filter Networks for Image Classification",
            "url":"\/paper\/global-filter-networks-for-image",
            "published":"2021-07-01T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":4708,
        "rank":432,
        "Model":"Oct-ResNet-152 (SE)",
        "mlmodel":{

        },
        "method_short":"Oct-ResNet-152 ",
        "method_details":"SE",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-10",
        "metrics":{
            "Top 1 Accuracy":"82.9%",
            "Number of params":"66.8M",
            "GFLOPs":"22.2",
            "Hardware Burden":"20771G",
            "Operations per network pass":"2.22G",
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.9,
            "Number of params":66800000.0,
            "GFLOPs":22.2,
            "Hardware Burden":20771.0,
            "Operations per network pass":2.22,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":111242,
            "title":"Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution",
            "url":"\/paper\/drop-an-octave-reducing-spatial-redundancy-in",
            "published":"2019-04-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/drop-an-octave-reducing-spatial-redundancy-in\/review\/?hl=4708"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6126,
        "rank":433,
        "Model":"PNASNet-5",
        "mlmodel":{

        },
        "method_short":"PNASNet-5",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-12-02",
        "metrics":{
            "Top 1 Accuracy":"82.9%",
            "Number of params":"86.1M",
            "GFLOPs":"50",
            "Hardware Burden":null,
            "Operations per network pass":"2.5G",
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.9,
            "Number of params":86100000.0,
            "GFLOPs":50.0,
            "Hardware Burden":null,
            "Operations per network pass":2.5,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":7580,
            "title":"Progressive Neural Architecture Search",
            "url":"\/paper\/progressive-neural-architecture-search",
            "published":"2017-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/progressive-neural-architecture-search\/review\/?hl=6126"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":9794,
        "rank":434,
        "Model":"Harm-SE-RNX-101 64x4d (320x320, Mean-Max Pooling)",
        "mlmodel":{

        },
        "method_short":"Harm-SE-RNX-101 64x4d ",
        "method_details":"320x320, Mean-Max Pooling",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-01-18",
        "metrics":{
            "Top 1 Accuracy":"82.85%",
            "Number of params":"88.2M",
            "GFLOPs":"31.4",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.85,
            "Number of params":88200000.0,
            "GFLOPs":31.4,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":180364,
            "title":"Harmonic Convolutional Networks based on Discrete Cosine Transform",
            "url":"\/paper\/harmonic-convolutional-networks-based-on",
            "published":"2020-01-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/harmonic-convolutional-networks-based-on\/review\/?hl=9794"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":34577,
        "rank":435,
        "Model":"FunMatch - T384+224 (ResNet-50)",
        "mlmodel":{

        },
        "method_short":"FunMatch - T384+224 ",
        "method_details":"ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-09",
        "metrics":{
            "Top 1 Accuracy":"82.8%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.8,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":814470,
            "title":"Knowledge distillation: A good teacher is patient and consistent",
            "url":"\/paper\/knowledge-distillation-a-good-teacher-is",
            "published":"2021-06-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/knowledge-distillation-a-good-teacher-is\/review\/?hl=34577"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":108980,
        "rank":436,
        "Model":"CA-Swin-T (+MixPro)",
        "mlmodel":{

        },
        "method_short":"CA-Swin-T ",
        "method_details":"+MixPro",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-24",
        "metrics":{
            "Top 1 Accuracy":"82.8%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.8,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1195795,
            "title":"MixPro: Data Augmentation with MaskMix and Progressive Attention Labeling for Vision Transformer",
            "url":"\/paper\/mixpro-data-augmentation-with-maskmix-and",
            "published":"2023-04-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mixpro-data-augmentation-with-maskmix-and\/review\/?hl=108980"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":61463,
        "rank":437,
        "Model":"VAN-B2",
        "mlmodel":{

        },
        "method_short":"VAN-B2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-20",
        "metrics":{
            "Top 1 Accuracy":"82.8%",
            "Number of params":"26.6M",
            "GFLOPs":"5",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.8,
            "Number of params":26600000.0,
            "GFLOPs":5.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":964888,
            "title":"Visual Attention Network",
            "url":"\/paper\/visual-attention-network",
            "published":"2022-02-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/visual-attention-network\/review\/?hl=61463"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":51587,
        "rank":438,
        "Model":"DaViT-T",
        "mlmodel":{

        },
        "method_short":"DaViT-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-07",
        "metrics":{
            "Top 1 Accuracy":"82.8%",
            "Number of params":"28.3M",
            "GFLOPs":"4.5",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.8,
            "Number of params":28300000.0,
            "GFLOPs":4.5,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":990738,
            "title":"DaViT: Dual Attention Vision Transformers",
            "url":"\/paper\/davit-dual-attention-vision-transformers",
            "published":"2022-04-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/davit-dual-attention-vision-transformers\/review\/?hl=51587"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":49038,
        "rank":439,
        "Model":"ReXNet_3.0",
        "mlmodel":{

        },
        "method_short":"ReXNet_3.0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-07-02",
        "metrics":{
            "Top 1 Accuracy":"82.8%",
            "Number of params":"34.7M",
            "GFLOPs":"3.4",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.8,
            "Number of params":34700000.0,
            "GFLOPs":3.4,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":206716,
            "title":"Rethinking Channel Dimensions for Efficient Model Design",
            "url":"\/paper\/rexnet-diminishing-representational",
            "published":"2020-07-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rexnet-diminishing-representational\/review\/?hl=49038"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":53850,
        "rank":440,
        "Model":"Sequencer2D-M",
        "mlmodel":{

        },
        "method_short":"Sequencer2D-M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-04",
        "metrics":{
            "Top 1 Accuracy":"82.8%",
            "Number of params":"38M",
            "GFLOPs":"11.1",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.8,
            "Number of params":38000000.0,
            "GFLOPs":11.1,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1004136,
            "title":"Sequencer: Deep LSTM for Image Classification",
            "url":"\/paper\/sequencer-deep-lstm-for-image-classification",
            "published":"2022-05-04T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29110,
        "rank":441,
        "Model":"CrossViT-18+",
        "mlmodel":{

        },
        "method_short":"CrossViT-18+",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-27",
        "metrics":{
            "Top 1 Accuracy":"82.8%",
            "Number of params":"44.3M",
            "GFLOPs":"9.5",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.8,
            "Number of params":44300000.0,
            "GFLOPs":9.5,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":758441,
            "title":"CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification",
            "url":"\/paper\/2103-14899",
            "published":"2021-03-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/2103-14899\/review\/?hl=29110"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":46844,
        "rank":442,
        "Model":"Shift-S",
        "mlmodel":{

        },
        "method_short":"Shift-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-26",
        "metrics":{
            "Top 1 Accuracy":"82.8%",
            "Number of params":"50M",
            "GFLOPs":"8.5",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.8,
            "Number of params":50000000.0,
            "GFLOPs":8.5,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":951213,
            "title":"When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism",
            "url":"\/paper\/when-shift-operation-meets-vision-transformer",
            "published":"2022-01-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/when-shift-operation-meets-vision-transformer\/review\/?hl=46844"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":58456,
        "rank":443,
        "Model":"HRFormer-B",
        "mlmodel":{

        },
        "method_short":"HRFormer-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-18",
        "metrics":{
            "Top 1 Accuracy":"82.8%",
            "Number of params":"50.3M",
            "GFLOPs":"13.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.8,
            "Number of params":50300000.0,
            "GFLOPs":13.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":890071,
            "title":"HRFormer: High-Resolution Transformer for Dense Prediction",
            "url":"\/paper\/hrformer-high-resolution-transformer-for",
            "published":"2021-10-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/hrformer-high-resolution-transformer-for\/review\/?hl=58456"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":24937,
        "rank":444,
        "Model":"BoTNet T4",
        "mlmodel":{

        },
        "method_short":"BoTNet T4",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-27",
        "metrics":{
            "Top 1 Accuracy":"82.8%",
            "Number of params":"54.7M",
            "GFLOPs":"10.9",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.8,
            "Number of params":54700000.0,
            "GFLOPs":10.9,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":739590,
            "title":"Bottleneck Transformers for Visual Recognition",
            "url":"\/paper\/bottleneck-transformers-for-visual",
            "published":"2021-01-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/bottleneck-transformers-for-visual\/review\/?hl=24937"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":38753,
        "rank":445,
        "Model":"CCT-14\/7x2 | 384",
        "mlmodel":{

        },
        "method_short":"CCT-14\/7x2 | 384",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-12",
        "metrics":{
            "Top 1 Accuracy":"82.71%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.71,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":778781,
            "title":"Escaping the Big Data Paradigm with Compact Transformers",
            "url":"\/paper\/escaping-the-big-data-paradigm-with-compact",
            "published":"2021-04-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/escaping-the-big-data-paradigm-with-compact\/review\/?hl=38753"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":11205,
        "rank":446,
        "Model":"MultiGrain SENet154 (500px)",
        "mlmodel":{

        },
        "method_short":"MultiGrain SENet154 ",
        "method_details":"500px",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-02-14",
        "metrics":{
            "Top 1 Accuracy":"82.7%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.7,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":105902,
            "title":"MultiGrain: a unified image embedding for classes and instances",
            "url":"\/paper\/multigrain-a-unified-image-embedding-for",
            "published":"2019-02-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multigrain-a-unified-image-embedding-for\/review\/?hl=11205"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":108986,
        "rank":447,
        "Model":"PVT-M (+MixPro)",
        "mlmodel":{

        },
        "method_short":"PVT-M ",
        "method_details":"+MixPro",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-24",
        "metrics":{
            "Top 1 Accuracy":"82.7%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.7,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1195795,
            "title":"MixPro: Data Augmentation with MaskMix and Progressive Attention Labeling for Vision Transformer",
            "url":"\/paper\/mixpro-data-augmentation-with-maskmix-and",
            "published":"2023-04-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mixpro-data-augmentation-with-maskmix-and\/review\/?hl=108986"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":102751,
        "rank":448,
        "Model":"ASF-former-S",
        "mlmodel":{

        },
        "method_short":"ASF-former-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-26",
        "metrics":{
            "Top 1 Accuracy":"82.7%",
            "Number of params":"19.3M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.7,
            "Number of params":19300000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":999999,
            "title":"Adaptive Split-Fusion Transformer",
            "url":"\/paper\/adaptive-split-fusion-transformer",
            "published":"2022-04-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/adaptive-split-fusion-transformer\/review\/?hl=102751"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60639,
        "rank":449,
        "Model":"Container Container",
        "mlmodel":{

        },
        "method_short":"Container Container",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-02",
        "metrics":{
            "Top 1 Accuracy":"82.7%",
            "Number of params":"22.1M",
            "GFLOPs":"8.1",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.7,
            "Number of params":22100000.0,
            "GFLOPs":8.1,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":811008,
            "title":"Container: Context Aggregation Network",
            "url":"\/paper\/container-context-aggregation-network",
            "published":"2021-06-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/container-context-aggregation-network\/review\/?hl=60639"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":48307,
        "rank":450,
        "Model":"UniNet-B2",
        "mlmodel":{

        },
        "method_short":"UniNet-B2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-08",
        "metrics":{
            "Top 1 Accuracy":"82.7%",
            "Number of params":"22.5M",
            "GFLOPs":"2.4",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.7,
            "Number of params":22500000.0,
            "GFLOPs":2.4,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":881795,
            "title":"UniNet: Unified Architecture Search with Convolution, Transformer, and MLP",
            "url":"\/paper\/uninet-unified-architecture-search-with",
            "published":"2021-10-08T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/uninet-unified-architecture-search-with\/review\/?hl=48307"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":70626,
        "rank":451,
        "Model":"DiNAT-Tiny",
        "mlmodel":{

        },
        "method_short":"DiNAT-Tiny",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-29",
        "metrics":{
            "Top 1 Accuracy":"82.7%",
            "Number of params":"28M",
            "GFLOPs":"4.3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.7,
            "Number of params":28000000.0,
            "GFLOPs":4.3,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1082646,
            "title":"Dilated Neighborhood Attention Transformer",
            "url":"\/paper\/dilated-neighborhood-attention-transformer",
            "published":"2022-09-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dilated-neighborhood-attention-transformer\/review\/?hl=70626"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":265,
                "name":"Neighborhood Attention",
                "color":"#ba1330"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":266,
                "name":"NAT Transformer",
                "color":"#574896"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":61467,
        "rank":452,
        "Model":"ELSA-Swin-T",
        "mlmodel":{

        },
        "method_short":"ELSA-Swin-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-23",
        "metrics":{
            "Top 1 Accuracy":"82.7%",
            "Number of params":"28M",
            "GFLOPs":"4.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.7,
            "Number of params":28000000.0,
            "GFLOPs":4.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":932838,
            "title":"ELSA: Enhanced Local Self-Attention for Vision Transformer",
            "url":"\/paper\/elsa-enhanced-local-self-attention-for-vision",
            "published":"2021-12-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/elsa-enhanced-local-self-attention-for-vision\/review\/?hl=61467"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":2049,
        "rank":453,
        "Model":"NASNET-A(6)",
        "mlmodel":{

        },
        "method_short":"NASNET-A",
        "method_details":"6",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-07-21",
        "metrics":{
            "Top 1 Accuracy":"82.7%",
            "Number of params":"88.9M",
            "GFLOPs":"23.8",
            "Hardware Burden":"1648G",
            "Operations per network pass":"2.38G",
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.7,
            "Number of params":88900000.0,
            "GFLOPs":23.8,
            "Hardware Burden":1648.0,
            "Operations per network pass":2.38,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":6283,
            "title":"Learning Transferable Architectures for Scalable Image Recognition",
            "url":"\/paper\/learning-transferable-architectures-for",
            "published":"2017-07-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learning-transferable-architectures-for\/review\/?hl=2049"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":33621,
        "rank":454,
        "Model":"RVT-B",
        "mlmodel":{

        },
        "method_short":"RVT-B*",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-17",
        "metrics":{
            "Top 1 Accuracy":"82.7%",
            "Number of params":"91.8M",
            "GFLOPs":"17.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.7,
            "Number of params":91800000.0,
            "GFLOPs":17.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":800173,
            "title":"Towards Robust Vision Transformer",
            "url":"\/paper\/rethinking-the-design-principles-of-robust",
            "published":"2021-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-the-design-principles-of-robust\/review\/?hl=33621"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60747,
        "rank":455,
        "Model":"FBNetV5-C-CLS",
        "mlmodel":{

        },
        "method_short":"FBNetV5-C-CLS",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-19",
        "metrics":{
            "Top 1 Accuracy":"82.6%",
            "Number of params":null,
            "GFLOPs":"1",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.6,
            "Number of params":null,
            "GFLOPs":1.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":912986,
            "title":"FBNetV5: Neural Architecture Search for Multiple Tasks in One Run",
            "url":"\/paper\/fbnetv5-neural-architecture-search-for",
            "published":"2021-11-19T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/fbnetv5-neural-architecture-search-for\/review\/?hl=60747"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":11206,
        "rank":456,
        "Model":"MultiGrain PNASNet (400px)",
        "mlmodel":{

        },
        "method_short":"MultiGrain PNASNet ",
        "method_details":"400px",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-02-14",
        "metrics":{
            "Top 1 Accuracy":"82.6%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.6,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":105902,
            "title":"MultiGrain: a unified image embedding for classes and instances",
            "url":"\/paper\/multigrain-a-unified-image-embedding-for",
            "published":"2019-02-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multigrain-a-unified-image-embedding-for\/review\/?hl=11206"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":49537,
        "rank":457,
        "Model":"ViT-S-24x2",
        "mlmodel":{

        },
        "method_short":"ViT-S-24x2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-18",
        "metrics":{
            "Top 1 Accuracy":"82.6%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.6,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":979672,
            "title":"Three things everyone should know about Vision Transformers",
            "url":"\/paper\/three-things-everyone-should-know-about",
            "published":"2022-03-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/three-things-everyone-should-know-about\/review\/?hl=49537"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":10371,
        "rank":458,
        "Model":"FixEfficientNet-B1",
        "mlmodel":{

        },
        "method_short":"FixEfficientNet-B1",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-18",
        "metrics":{
            "Top 1 Accuracy":"82.6%",
            "Number of params":"7.8M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.6,
            "Number of params":7800000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":187431,
            "title":"Fixing the train-test resolution discrepancy: FixEfficientNet",
            "url":"\/paper\/fixing-the-train-test-resolution-discrepancy-2",
            "published":"2020-03-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fixing-the-train-test-resolution-discrepancy-2\/review\/?hl=10371"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6183,
        "rank":459,
        "Model":"EfficientNet-B4",
        "mlmodel":{

        },
        "method_short":"EfficientNet-B4",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-05-28",
        "metrics":{
            "Top 1 Accuracy":"82.6%",
            "Number of params":"19M",
            "GFLOPs":"4.2",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.6,
            "Number of params":19000000.0,
            "GFLOPs":4.2,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":117456,
            "title":"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
            "url":"\/paper\/efficientnet-rethinking-model-scaling-for",
            "published":"2019-05-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/efficientnet-rethinking-model-scaling-for\/review\/?hl=6183"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[
            {
                "url":"https:\/\/github.com\/facebookresearch\/AugLy\/blob\/main\/examples\/imagenet\/evaluate_robustness_imagenet_efficientnet_b4.ipynb"
            }
        ]
    },
    {
        "table_id":116,
        "row_id":23827,
        "rank":460,
        "Model":"DeiT-B",
        "mlmodel":{

        },
        "method_short":"DeiT-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-23",
        "metrics":{
            "Top 1 Accuracy":"82.6%",
            "Number of params":"22M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.6,
            "Number of params":22000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":731001,
            "title":"Training data-efficient image transformers & distillation through attention",
            "url":"\/paper\/training-data-efficient-image-transformers",
            "published":"2020-12-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/training-data-efficient-image-transformers\/review\/?hl=23827"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":25003,
        "rank":461,
        "Model":"T2T-ViTt-24",
        "mlmodel":{

        },
        "method_short":"T2T-ViTt-24",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-28",
        "metrics":{
            "Top 1 Accuracy":"82.6%",
            "Number of params":"64.4M",
            "GFLOPs":"30",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.6,
            "Number of params":64400000.0,
            "GFLOPs":30.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":740117,
            "title":"Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet",
            "url":"\/paper\/tokens-to-token-vit-training-vision",
            "published":"2021-01-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/tokens-to-token-vit-training-vision\/review\/?hl=25003"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":108029,
        "rank":462,
        "Model":"ViT-S",
        "mlmodel":{

        },
        "method_short":"ViT-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-18",
        "metrics":{
            "Top 1 Accuracy":"82.54%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.54,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1265182,
            "title":"Which Transformer to Favor: A Comparative Analysis of Efficiency in Vision Transformers",
            "url":"\/paper\/which-transformer-to-favor-a-comparative",
            "published":"2023-08-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/which-transformer-to-favor-a-comparative\/review\/?hl=108029"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29248,
        "rank":463,
        "Model":"CvT-21",
        "mlmodel":{

        },
        "method_short":"CvT-21",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-29",
        "metrics":{
            "Top 1 Accuracy":"82.5%",
            "Number of params":null,
            "GFLOPs":"7.1",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.5,
            "Number of params":null,
            "GFLOPs":7.1,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":758429,
            "title":"CvT: Introducing Convolutions to Vision Transformers",
            "url":"\/paper\/cvt-introducing-convolutions-to-vision",
            "published":"2021-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/cvt-introducing-convolutions-to-vision\/review\/?hl=29248"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":113275,
        "rank":464,
        "Model":"TransNeXt-Micro (IN-1K supervised, 224)",
        "mlmodel":{

        },
        "method_short":"TransNeXt-Micro ",
        "method_details":"IN-1K supervised, 224",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-28",
        "metrics":{
            "Top 1 Accuracy":"82.5%",
            "Number of params":"12.8M",
            "GFLOPs":"2.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.5,
            "Number of params":12800000.0,
            "GFLOPs":2.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1331523,
            "title":"TransNeXt: Robust Foveal Visual Perception for Vision Transformers",
            "url":"\/paper\/transnext-robust-foveal-visual-perception-for",
            "published":"2023-11-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/transnext-robust-foveal-visual-perception-for\/review\/?hl=113275"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60622,
        "rank":465,
        "Model":"FixResNet-50 Billion-scale@224",
        "mlmodel":{

        },
        "method_short":"FixResNet-50 Billion-scale@224",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-14",
        "metrics":{
            "Top 1 Accuracy":"82.5%",
            "Number of params":"25.6M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.5,
            "Number of params":25600000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":142997,
            "title":"Fixing the train-test resolution discrepancy",
            "url":"\/paper\/fixing-the-train-test-resolution-discrepancy",
            "published":"2019-06-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fixing-the-train-test-resolution-discrepancy\/review\/?hl=60622"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":63701,
        "rank":466,
        "Model":"Next-ViT-S",
        "mlmodel":{

        },
        "method_short":"Next-ViT-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-12",
        "metrics":{
            "Top 1 Accuracy":"82.5%",
            "Number of params":"31.7M",
            "GFLOPs":"5.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.5,
            "Number of params":31700000.0,
            "GFLOPs":5.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1042514,
            "title":"Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios",
            "url":"\/paper\/next-vit-next-generation-vision-transformer",
            "published":"2022-07-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/next-vit-next-generation-vision-transformer\/review\/?hl=63701"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29461,
        "rank":467,
        "Model":"LeViT-384",
        "mlmodel":{

        },
        "method_short":"LeViT-384",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-02",
        "metrics":{
            "Top 1 Accuracy":"82.5%",
            "Number of params":"39.4M",
            "GFLOPs":"2.334",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.5,
            "Number of params":39400000.0,
            "GFLOPs":2.334,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":774248,
            "title":"LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference",
            "url":"\/paper\/levit-a-vision-transformer-in-convnet-s",
            "published":"2021-04-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/levit-a-vision-transformer-in-convnet-s\/review\/?hl=29461"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29109,
        "rank":468,
        "Model":"CrossViT-18",
        "mlmodel":{

        },
        "method_short":"CrossViT-18",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-27",
        "metrics":{
            "Top 1 Accuracy":"82.5%",
            "Number of params":"43.3M",
            "GFLOPs":"9",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.5,
            "Number of params":43300000.0,
            "GFLOPs":9.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":758441,
            "title":"CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification",
            "url":"\/paper\/2103-14899",
            "published":"2021-03-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/2103-14899\/review\/?hl=29109"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":43405,
        "rank":469,
        "Model":"MetaFormer PoolFormer-M48",
        "mlmodel":{

        },
        "method_short":"MetaFormer PoolFormer-M48",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-22",
        "metrics":{
            "Top 1 Accuracy":"82.5%",
            "Number of params":"73M",
            "GFLOPs":"23.2",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.5,
            "Number of params":73000000.0,
            "GFLOPs":23.2,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":914420,
            "title":"MetaFormer Is Actually What You Need for Vision",
            "url":"\/paper\/metaformer-is-actually-what-you-need-for",
            "published":"2021-11-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/metaformer-is-actually-what-you-need-for\/review\/?hl=43405"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":28363,
        "rank":470,
        "Model":"ConViT-B+",
        "mlmodel":{

        },
        "method_short":"ConViT-B+",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-19",
        "metrics":{
            "Top 1 Accuracy":"82.5%",
            "Number of params":"152M",
            "GFLOPs":"30",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.5,
            "Number of params":152000000.0,
            "GFLOPs":30.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755434,
            "title":"ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases",
            "url":"\/paper\/convit-improving-vision-transformers-with",
            "published":"2021-03-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/convit-improving-vision-transformers-with\/review\/?hl=28363"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":71929,
        "rank":471,
        "Model":"TransBoost-ConvNext-T",
        "mlmodel":{

        },
        "method_short":"TransBoost-ConvNext-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-26",
        "metrics":{
            "Top 1 Accuracy":"82.46%",
            "Number of params":"28.59M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.46,
            "Number of params":28590000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1016638,
            "title":"TransBoost: Improving the Best ImageNet Performance using Deep Transduction",
            "url":"\/paper\/transboost-improving-the-best-imagenet",
            "published":"2022-05-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/transboost-improving-the-best-imagenet\/review\/?hl=71929"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":11207,
        "rank":472,
        "Model":"NoisyStudent (EfficientNet-B2)",
        "mlmodel":{

        },
        "method_short":"NoisyStudent ",
        "method_details":"EfficientNet-B2",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-11",
        "metrics":{
            "Top 1 Accuracy":"82.4%",
            "Number of params":"9.2M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.4,
            "Number of params":9200000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":170047,
            "title":"Self-training with Noisy Student improves ImageNet classification",
            "url":"\/paper\/self-training-with-noisy-student-improves",
            "published":"2019-11-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/self-training-with-noisy-student-improves\/review\/?hl=11207"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":37555,
        "rank":473,
        "Model":"AutoFormer-base",
        "mlmodel":{

        },
        "method_short":"AutoFormer-base",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-01",
        "metrics":{
            "Top 1 Accuracy":"82.4%",
            "Number of params":"54M",
            "GFLOPs":"11",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.4,
            "Number of params":54000000.0,
            "GFLOPs":11.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":828740,
            "title":"AutoFormer: Searching Transformers for Visual Recognition",
            "url":"\/paper\/autoformer-searching-transformers-for-visual",
            "published":"2021-07-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/autoformer-searching-transformers-for-visual\/review\/?hl=37555"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":40188,
        "rank":474,
        "Model":"ResNet-152 (A2 + reg)",
        "mlmodel":{

        },
        "method_short":"ResNet-152 ",
        "method_details":"A2 + reg",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-01",
        "metrics":{
            "Top 1 Accuracy":"82.4%",
            "Number of params":"60.2M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.4,
            "Number of params":60200000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":877343,
            "title":"ResNet strikes back: An improved training procedure in timm",
            "url":"\/paper\/resnet-strikes-back-an-improved-training",
            "published":"2021-10-01T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":28362,
        "rank":475,
        "Model":"ConViT-B",
        "mlmodel":{

        },
        "method_short":"ConViT-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-19",
        "metrics":{
            "Top 1 Accuracy":"82.4%",
            "Number of params":"86M",
            "GFLOPs":"17",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.4,
            "Number of params":86000000.0,
            "GFLOPs":17.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755434,
            "title":"ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases",
            "url":"\/paper\/convit-improving-vision-transformers-with",
            "published":"2021-03-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/convit-improving-vision-transformers-with\/review\/?hl=28362"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":37857,
        "rank":476,
        "Model":"DeiT-B with iRPE-K",
        "mlmodel":{

        },
        "method_short":"DeiT-B with iRPE-K",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-29",
        "metrics":{
            "Top 1 Accuracy":"82.4%",
            "Number of params":"87M",
            "GFLOPs":"35.368",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.4,
            "Number of params":87000000.0,
            "GFLOPs":35.368,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":843509,
            "title":"Rethinking and Improving Relative Position Encoding for Vision Transformer",
            "url":"\/paper\/rethinking-and-improving-relative-position",
            "published":"2021-07-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-and-improving-relative-position\/review\/?hl=37857"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":70543,
        "rank":477,
        "Model":"Mega",
        "mlmodel":{

        },
        "method_short":"Mega",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-21",
        "metrics":{
            "Top 1 Accuracy":"82.4%",
            "Number of params":"90M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.4,
            "Number of params":90000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1078900,
            "title":"Mega: Moving Average Equipped Gated Attention",
            "url":"\/paper\/mega-moving-average-equipped-gated-attention",
            "published":"2022-09-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mega-moving-average-equipped-gated-attention\/review\/?hl=70543"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":105116,
        "rank":478,
        "Model":"ResMLP-B24 + STD",
        "mlmodel":{

        },
        "method_short":"ResMLP-B24 + STD",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-23",
        "metrics":{
            "Top 1 Accuracy":"82.4%",
            "Number of params":"122.6M",
            "GFLOPs":"24.1",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.4,
            "Number of params":122600000.0,
            "GFLOPs":24.1,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1230624,
            "title":"Spatial-Channel Token Distillation for Vision MLPs",
            "url":"\/paper\/spatial-channel-token-distillation-for-vision",
            "published":"2022-07-23T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":97471,
        "rank":479,
        "Model":"ViT-B\/16-224+HTM",
        "mlmodel":{

        },
        "method_short":"ViT-B\/16-224+HTM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-14",
        "metrics":{
            "Top 1 Accuracy":"82.37%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.37,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1093473,
            "title":"TokenMixup: Efficient Attention-guided Token-level Data Augmentation for Transformers",
            "url":"\/paper\/tokenmixup-efficient-attention-guided-token",
            "published":"2022-10-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/tokenmixup-efficient-attention-guided-token\/review\/?hl=97471"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":21465,
        "rank":480,
        "Model":"ColorNet",
        "mlmodel":{

        },
        "method_short":"ColorNet",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-02-01",
        "metrics":{
            "Top 1 Accuracy":"82.35%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.35,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":93126,
            "title":"ColorNet: Investigating the importance of color spaces for image classification",
            "url":"\/paper\/colornet-investigating-the-importance-of",
            "published":"2019-02-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/colornet-investigating-the-importance-of\/review\/?hl=21465"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":25002,
        "rank":481,
        "Model":"T2T-ViT-24",
        "mlmodel":{

        },
        "method_short":"T2T-ViT-24",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-28",
        "metrics":{
            "Top 1 Accuracy":"82.3%",
            "Number of params":null,
            "GFLOPs":"27.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.3,
            "Number of params":null,
            "GFLOPs":27.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":740117,
            "title":"Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet",
            "url":"\/paper\/tokens-to-token-vit-training-vision",
            "published":"2021-01-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/tokens-to-token-vit-training-vision\/review\/?hl=25002"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":49538,
        "rank":482,
        "Model":"ViT-S-48x1",
        "mlmodel":{

        },
        "method_short":"ViT-S-48x1",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-18",
        "metrics":{
            "Top 1 Accuracy":"82.3%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.3,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":979672,
            "title":"Three things everyone should know about Vision Transformers",
            "url":"\/paper\/three-things-everyone-should-know-about",
            "published":"2022-03-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/three-things-everyone-should-know-about\/review\/?hl=49538"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60756,
        "rank":483,
        "Model":"MViTv2-T",
        "mlmodel":{

        },
        "method_short":"MViTv2-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-02",
        "metrics":{
            "Top 1 Accuracy":"82.3%",
            "Number of params":"24M",
            "GFLOPs":"4.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.3,
            "Number of params":24000000.0,
            "GFLOPs":4.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":924692,
            "title":"MViTv2: Improved Multiscale Vision Transformers for Classification and Detection",
            "url":"\/paper\/improved-multiscale-vision-transformers-for",
            "published":"2021-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improved-multiscale-vision-transformers-for\/review\/?hl=60756"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":11208,
        "rank":484,
        "Model":"SCARLET-A4",
        "mlmodel":{

        },
        "method_short":"SCARLET-A4",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-16",
        "metrics":{
            "Top 1 Accuracy":"82.3%",
            "Number of params":"27.8M",
            "GFLOPs":"8.4",
            "Hardware Burden":"12G",
            "Operations per network pass":"0.42G",
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.3,
            "Number of params":27800000.0,
            "GFLOPs":8.4,
            "Hardware Burden":12.0,
            "Operations per network pass":0.42,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":150158,
            "title":"SCARLET-NAS: Bridging the Gap between Stability and Scalability in Weight-sharing Neural Architecture Search",
            "url":"\/paper\/scarletnas-bridging-the-gap-between",
            "published":"2019-08-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scarletnas-bridging-the-gap-between\/review\/?hl=11208"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":53851,
        "rank":485,
        "Model":"Sequencer2D-S",
        "mlmodel":{

        },
        "method_short":"Sequencer2D-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-04",
        "metrics":{
            "Top 1 Accuracy":"82.3%",
            "Number of params":"28M",
            "GFLOPs":"8.4",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.3,
            "Number of params":28000000.0,
            "GFLOPs":8.4,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1004136,
            "title":"Sequencer: Deep LSTM for Image Classification",
            "url":"\/paper\/sequencer-deep-lstm-for-image-classification",
            "published":"2022-05-04T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29108,
        "rank":486,
        "Model":"CrossViT-15+",
        "mlmodel":{

        },
        "method_short":"CrossViT-15+",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-27",
        "metrics":{
            "Top 1 Accuracy":"82.3%",
            "Number of params":"28.2M",
            "GFLOPs":"6.1",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.3,
            "Number of params":28200000.0,
            "GFLOPs":6.1,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":758441,
            "title":"CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification",
            "url":"\/paper\/2103-14899",
            "published":"2021-03-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/2103-14899\/review\/?hl=29108"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":37538,
        "rank":487,
        "Model":"GLiT-Bases",
        "mlmodel":{

        },
        "method_short":"GLiT-Bases",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-07",
        "metrics":{
            "Top 1 Accuracy":"82.3%",
            "Number of params":"96.1M",
            "GFLOPs":"17",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.3,
            "Number of params":96100000.0,
            "GFLOPs":17.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":831816,
            "title":"GLiT: Neural Architecture Search for Global and Local Image Transformer",
            "url":"\/paper\/glit-neural-architecture-search-for-global",
            "published":"2021-07-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/glit-neural-architecture-search-for-global\/review\/?hl=37538"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":108041,
        "rank":488,
        "Model":"EViT (delete)",
        "mlmodel":{

        },
        "method_short":"EViT ",
        "method_details":"delete",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-18",
        "metrics":{
            "Top 1 Accuracy":"82.29%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.29,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1265182,
            "title":"Which Transformer to Favor: A Comparative Analysis of Efficiency in Vision Transformers",
            "url":"\/paper\/which-transformer-to-favor-a-comparative",
            "published":"2023-08-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/which-transformer-to-favor-a-comparative\/review\/?hl=108041"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":108046,
        "rank":489,
        "Model":"STViT-Swin-Ti",
        "mlmodel":{

        },
        "method_short":"STViT-Swin-Ti",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-18",
        "metrics":{
            "Top 1 Accuracy":"82.22%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.22,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1265182,
            "title":"Which Transformer to Favor: A Comparative Analysis of Efficiency in Vision Transformers",
            "url":"\/paper\/which-transformer-to-favor-a-comparative",
            "published":"2023-08-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/which-transformer-to-favor-a-comparative\/review\/?hl=108046"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":75982,
        "rank":490,
        "Model":"BossNet-T1",
        "mlmodel":{

        },
        "method_short":"BossNet-T1",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-23",
        "metrics":{
            "Top 1 Accuracy":"82.2%",
            "Number of params":null,
            "GFLOPs":"15.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.2,
            "Number of params":null,
            "GFLOPs":15.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":756376,
            "title":"BossNAS: Exploring Hybrid CNN-transformers with Block-wisely Self-supervised Neural Architecture Search",
            "url":"\/paper\/bossnas-exploring-hybrid-cnn-transformers",
            "published":"2021-03-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/bossnas-exploring-hybrid-cnn-transformers\/review\/?hl=75982"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29208,
        "rank":491,
        "Model":"CAIT-XXS-36",
        "mlmodel":{

        },
        "method_short":"CAIT-XXS-36",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-31",
        "metrics":{
            "Top 1 Accuracy":"82.2%",
            "Number of params":"17.3M",
            "GFLOPs":"14.3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.2,
            "Number of params":17300000.0,
            "GFLOPs":14.3,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":772635,
            "title":"Going deeper with Image Transformers",
            "url":"\/paper\/going-deeper-with-image-transformers",
            "published":"2021-03-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/going-deeper-with-image-transformers\/review\/?hl=29208"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29251,
        "rank":492,
        "Model":"CvT-13-NAS",
        "mlmodel":{

        },
        "method_short":"CvT-13-NAS",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-29",
        "metrics":{
            "Top 1 Accuracy":"82.2%",
            "Number of params":"18M",
            "GFLOPs":"4.1",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.2,
            "Number of params":18000000.0,
            "GFLOPs":4.1,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":758429,
            "title":"CvT: Introducing Convolutions to Vision Transformers",
            "url":"\/paper\/cvt-introducing-convolutions-to-vision",
            "published":"2021-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/cvt-introducing-convolutions-to-vision\/review\/?hl=29251"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":36767,
        "rank":493,
        "Model":"ViTAE-S-Stage",
        "mlmodel":{

        },
        "method_short":"ViTAE-S-Stage",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-07",
        "metrics":{
            "Top 1 Accuracy":"82.2%",
            "Number of params":"19.2M",
            "GFLOPs":"12.0",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.2,
            "Number of params":19200000.0,
            "GFLOPs":12.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":812332,
            "title":"ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias",
            "url":"\/paper\/vitae-vision-transformer-advanced-by",
            "published":"2021-06-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vitae-vision-transformer-advanced-by\/review\/?hl=36767"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":25000,
        "rank":494,
        "Model":"T2T-ViTt-19",
        "mlmodel":{

        },
        "method_short":"T2T-ViTt-19",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-28",
        "metrics":{
            "Top 1 Accuracy":"82.2%",
            "Number of params":"39.2M",
            "GFLOPs":"19.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.2,
            "Number of params":39200000.0,
            "GFLOPs":19.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":740117,
            "title":"Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet",
            "url":"\/paper\/tokens-to-token-vit-training-vision",
            "published":"2021-01-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/tokens-to-token-vit-training-vision\/review\/?hl=25000"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":39473,
        "rank":495,
        "Model":"Evo-LeViT-384",
        "mlmodel":{

        },
        "method_short":"Evo-LeViT-384*",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-08-03",
        "metrics":{
            "Top 1 Accuracy":"82.2%",
            "Number of params":"39.6M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.2,
            "Number of params":39600000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":846027,
            "title":"Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer",
            "url":"\/paper\/evo-vit-slow-fast-token-evolution-for-dynamic",
            "published":"2021-08-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/evo-vit-slow-fast-token-evolution-for-dynamic\/review\/?hl=39473"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":30930,
        "rank":496,
        "Model":"Visformer-S",
        "mlmodel":{

        },
        "method_short":"Visformer-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-26",
        "metrics":{
            "Top 1 Accuracy":"82.2%",
            "Number of params":"40.2M",
            "GFLOPs":"4.9",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.2,
            "Number of params":40200000.0,
            "GFLOPs":4.9,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":788363,
            "title":"Visformer: The Vision-friendly Transformer",
            "url":"\/paper\/visformer-the-vision-friendly-transformer",
            "published":"2021-04-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/visformer-the-vision-friendly-transformer\/review\/?hl=30930"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":28361,
        "rank":497,
        "Model":"ConViT-S+",
        "mlmodel":{

        },
        "method_short":"ConViT-S+",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-19",
        "metrics":{
            "Top 1 Accuracy":"82.2%",
            "Number of params":"48M",
            "GFLOPs":"10",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.2,
            "Number of params":48000000.0,
            "GFLOPs":10.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755434,
            "title":"ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases",
            "url":"\/paper\/convit-improving-vision-transformers-with",
            "published":"2021-03-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/convit-improving-vision-transformers-with\/review\/?hl=28361"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":45782,
        "rank":498,
        "Model":"ConvMixer-1536\/20",
        "mlmodel":{

        },
        "method_short":"ConvMixer-1536\/20",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-29",
        "metrics":{
            "Top 1 Accuracy":"82.20",
            "Number of params":"51.6M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.2,
            "Number of params":51600000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":949389,
            "title":"Patches Are All You Need?",
            "url":"\/paper\/patches-are-all-you-need-1",
            "published":"2022-01-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/patches-are-all-you-need-1\/review\/?hl=45782"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":179,
                "name":"Mixer",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":28546,
        "rank":499,
        "Model":"DeepVit-L",
        "mlmodel":{

        },
        "method_short":"DeepVit-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-22",
        "metrics":{
            "Top 1 Accuracy":"82.2%",
            "Number of params":"55M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.2,
            "Number of params":55000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755709,
            "title":"DeepViT: Towards Deeper Vision Transformer",
            "url":"\/paper\/deepvit-towards-deeper-vision-transformer",
            "published":"2021-03-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deepvit-towards-deeper-vision-transformer\/review\/?hl=28546"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":24936,
        "rank":500,
        "Model":"SENet-152",
        "mlmodel":{

        },
        "method_short":"SENet-152",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-27",
        "metrics":{
            "Top 1 Accuracy":"82.2%",
            "Number of params":"66.6M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.2,
            "Number of params":66600000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":739590,
            "title":"Bottleneck Transformers for Visual Recognition",
            "url":"\/paper\/bottleneck-transformers-for-visual",
            "published":"2021-01-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/bottleneck-transformers-for-visual\/review\/?hl=24936"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":8176,
        "rank":501,
        "Model":"ResNeXt-101 32x8d",
        "mlmodel":{

        },
        "method_short":"ResNeXt-101 32x8d",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-05-02",
        "metrics":{
            "Top 1 Accuracy":"82.2%",
            "Number of params":"88M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.2,
            "Number of params":88000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":4635,
            "title":"Exploring the Limits of Weakly Supervised Pretraining",
            "url":"\/paper\/exploring-the-limits-of-weakly-supervised",
            "published":"2018-05-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/exploring-the-limits-of-weakly-supervised\/review\/?hl=8176"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":6,
                "name":"ResNeXt",
                "color":"#86960b"
            }
        ],
        "reports":[
            {
                "url":"https:\/\/github.com\/facebookresearch\/AugLy\/blob\/main\/examples\/imagenet\/evaluate_robustness_imagenet_resnext101_32x8d.ipynb"
            }
        ]
    },
    {
        "table_id":116,
        "row_id":71931,
        "rank":502,
        "Model":"TransBoost-Swin-T",
        "mlmodel":{

        },
        "method_short":"TransBoost-Swin-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-26",
        "metrics":{
            "Top 1 Accuracy":"82.16%",
            "Number of params":"71.71M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.16,
            "Number of params":71710000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1016638,
            "title":"TransBoost: Improving the Best ImageNet Performance using Deep Transduction",
            "url":"\/paper\/transboost-improving-the-best-imagenet",
            "published":"2022-05-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/transboost-improving-the-best-imagenet\/review\/?hl=71931"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":22070,
        "rank":503,
        "Model":"ResNeXt-101, 64x4d, S=2(224px)",
        "mlmodel":{

        },
        "method_short":"ResNeXt-101, 64x4d, S=2",
        "method_details":"224px",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-11-30",
        "metrics":{
            "Top 1 Accuracy":"82.13%",
            "Number of params":"88.6M",
            "GFLOPs":"18.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.13,
            "Number of params":88600000.0,
            "GFLOPs":18.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":237979,
            "title":"Towards Better Accuracy-efficiency Trade-offs: Divide and Co-training",
            "url":"\/paper\/splitnet-divide-and-co-training",
            "published":"2020-11-30T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":6,
                "name":"ResNeXt",
                "color":"#86960b"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":108043,
        "rank":504,
        "Model":"ToMe-ViT-S",
        "mlmodel":{

        },
        "method_short":"ToMe-ViT-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-18",
        "metrics":{
            "Top 1 Accuracy":"82.11%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.11,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1265182,
            "title":"Which Transformer to Favor: A Comparative Analysis of Efficiency in Vision Transformers",
            "url":"\/paper\/which-transformer-to-favor-a-comparative",
            "published":"2023-08-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/which-transformer-to-favor-a-comparative\/review\/?hl=108043"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":44960,
        "rank":505,
        "Model":"PatchConvNet-S60",
        "mlmodel":{

        },
        "method_short":"PatchConvNet-S60",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-27",
        "metrics":{
            "Top 1 Accuracy":"82.1%",
            "Number of params":"25.2M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.1,
            "Number of params":25200000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":933338,
            "title":"Augmenting Convolutional networks with attention-based aggregation",
            "url":"\/paper\/augmenting-convolutional-networks-with",
            "published":"2021-12-27T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":167,
                "name":"PatchConvnet",
                "color":"#e0edd4"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":77455,
        "rank":506,
        "Model":"Pyramid ViG-S",
        "mlmodel":{

        },
        "method_short":"Pyramid ViG-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-01",
        "metrics":{
            "Top 1 Accuracy":"82.1%",
            "Number of params":"27.3M",
            "GFLOPs":"4.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.1,
            "Number of params":27300000.0,
            "GFLOPs":4.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1019895,
            "title":"Vision GNN: An Image is Worth Graph of Nodes",
            "url":"\/paper\/vision-gnn-an-image-is-worth-graph-of-nodes",
            "published":"2022-06-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vision-gnn-an-image-is-worth-graph-of-nodes\/review\/?hl=77455"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":61424,
        "rank":507,
        "Model":"ConvNeXt-T",
        "mlmodel":{

        },
        "method_short":"ConvNeXt-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-10",
        "metrics":{
            "Top 1 Accuracy":"82.1%",
            "Number of params":"29M",
            "GFLOPs":"4.5",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.1,
            "Number of params":29000000.0,
            "GFLOPs":4.5,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":942596,
            "title":"A ConvNet for the 2020s",
            "url":"\/paper\/a-convnet-for-the-2020s",
            "published":"2022-01-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/a-convnet-for-the-2020s\/review\/?hl=61424"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":105115,
        "rank":508,
        "Model":"CycleMLP-B2 + STD",
        "mlmodel":{

        },
        "method_short":"CycleMLP-B2 + STD",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-23",
        "metrics":{
            "Top 1 Accuracy":"82.1%",
            "Number of params":"30.1M",
            "GFLOPs":"4.0",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.1,
            "Number of params":30100000.0,
            "GFLOPs":4.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1230624,
            "title":"Spatial-Channel Token Distillation for Vision MLPs",
            "url":"\/paper\/spatial-channel-token-distillation-for-vision",
            "published":"2022-07-23T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":28488,
        "rank":509,
        "Model":"CeiT-S",
        "mlmodel":{

        },
        "method_short":"CeiT-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-22",
        "metrics":{
            "Top 1 Accuracy":"82%",
            "Number of params":null,
            "GFLOPs":"4.5",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.0,
            "Number of params":null,
            "GFLOPs":4.5,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755729,
            "title":"Incorporating Convolution Designs into Visual Transformers",
            "url":"\/paper\/incorporating-convolution-designs-into-visual",
            "published":"2021-03-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/incorporating-convolution-designs-into-visual\/review\/?hl=28488"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":30878,
        "rank":510,
        "Model":"DIFFQ (\u03bb=1e\u22122)",
        "mlmodel":{

        },
        "method_short":"DIFFQ ",
        "method_details":"\u03bb=1e\u22122",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-20",
        "metrics":{
            "Top 1 Accuracy":"82.0",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.0,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":785665,
            "title":"Differentiable Model Compression via Pseudo Quantization Noise",
            "url":"\/paper\/differentiable-model-compression-via-pseudo",
            "published":"2021-04-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/differentiable-model-compression-via-pseudo\/review\/?hl=30878"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":87843,
        "rank":511,
        "Model":"NEXcepTion-S",
        "mlmodel":{

        },
        "method_short":"NEXcepTion-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-16",
        "metrics":{
            "Top 1 Accuracy":"82%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.0,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1129870,
            "title":"From Xception to NEXcepTion: New Design Decisions and Neural Architecture Search",
            "url":"\/paper\/from-xception-to-nexception-new-design",
            "published":"2022-12-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/from-xception-to-nexception-new-design\/review\/?hl=87843"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":59083,
        "rank":512,
        "Model":"GC ViT-XT",
        "mlmodel":{

        },
        "method_short":"GC ViT-XT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-20",
        "metrics":{
            "Top 1 Accuracy":"82.0%",
            "Number of params":"20M",
            "GFLOPs":"2.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.0,
            "Number of params":20000000.0,
            "GFLOPs":2.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1029616,
            "title":"Global Context Vision Transformers",
            "url":"\/paper\/global-context-vision-transformers",
            "published":"2022-06-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/global-context-vision-transformers\/review\/?hl=59083"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":34346,
        "rank":513,
        "Model":"Container-Light",
        "mlmodel":{

        },
        "method_short":"Container-Light",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-02",
        "metrics":{
            "Top 1 Accuracy":"82%",
            "Number of params":"20M",
            "GFLOPs":"3.2",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.0,
            "Number of params":20000000.0,
            "GFLOPs":3.2,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":811008,
            "title":"Container: Context Aggregation Network",
            "url":"\/paper\/container-context-aggregation-network",
            "published":"2021-06-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/container-context-aggregation-network\/review\/?hl=34346"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29121,
        "rank":514,
        "Model":"ViL-Small",
        "mlmodel":{

        },
        "method_short":"ViL-Small",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-29",
        "metrics":{
            "Top 1 Accuracy":"82%",
            "Number of params":"24.6M",
            "GFLOPs":"4.86",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.0,
            "Number of params":24600000.0,
            "GFLOPs":4.86,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":758437,
            "title":"Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding",
            "url":"\/paper\/2103-15358",
            "published":"2021-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/2103-15358\/review\/?hl=29121"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":36911,
        "rank":515,
        "Model":"PVTv2-B2",
        "mlmodel":{

        },
        "method_short":"PVTv2-B2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-25",
        "metrics":{
            "Top 1 Accuracy":"82%",
            "Number of params":"25.4M",
            "GFLOPs":"4",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.0,
            "Number of params":25400000.0,
            "GFLOPs":4.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":825108,
            "title":"PVT v2: Improved Baselines with Pyramid Vision Transformer",
            "url":"\/paper\/pvtv2-improved-baselines-with-pyramid-vision",
            "published":"2021-06-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/pvtv2-improved-baselines-with-pyramid-vision\/review\/?hl=36911"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":61466,
        "rank":516,
        "Model":"ActiveMLP-T",
        "mlmodel":{

        },
        "method_short":"ActiveMLP-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-11",
        "metrics":{
            "Top 1 Accuracy":"82%",
            "Number of params":"27.2M",
            "GFLOPs":"4",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.0,
            "Number of params":27200000.0,
            "GFLOPs":4.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":975658,
            "title":"Active Token Mixer",
            "url":"\/paper\/activemlp-an-mlp-like-architecture-with",
            "published":"2022-03-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/activemlp-an-mlp-like-architecture-with\/review\/?hl=61466"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":55675,
        "rank":517,
        "Model":"LITv2-S",
        "mlmodel":{

        },
        "method_short":"LITv2-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-26",
        "metrics":{
            "Top 1 Accuracy":"82%",
            "Number of params":"28M",
            "GFLOPs":"3.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.0,
            "Number of params":28000000.0,
            "GFLOPs":3.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1016651,
            "title":"Fast Vision Transformers with HiLo Attention",
            "url":"\/paper\/fast-vision-transformers-with-hilo-attention",
            "published":"2022-05-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fast-vision-transformers-with-hilo-attention\/review\/?hl=55675"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":71446,
        "rank":518,
        "Model":"DAT-T",
        "mlmodel":{

        },
        "method_short":"DAT-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-03",
        "metrics":{
            "Top 1 Accuracy":"82.0%",
            "Number of params":"29M",
            "GFLOPs":"4.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.0,
            "Number of params":29000000.0,
            "GFLOPs":4.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":934161,
            "title":"Vision Transformer with Deformable Attention",
            "url":"\/paper\/vision-transformer-with-deformable-attention",
            "published":"2022-01-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vision-transformer-with-deformable-attention\/review\/?hl=71446"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":79154,
        "rank":519,
        "Model":"Swin-T (SAMix+DM)",
        "mlmodel":{

        },
        "method_short":"Swin-T ",
        "method_details":"SAMix+DM",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "Top 1 Accuracy":"81.97%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.97,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":108042,
        "rank":520,
        "Model":"EViT (fuse)",
        "mlmodel":{

        },
        "method_short":"EViT ",
        "method_details":"fuse",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-18",
        "metrics":{
            "Top 1 Accuracy":"81.96%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.96,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1265182,
            "title":"Which Transformer to Favor: A Comparative Analysis of Efficiency in Vision Transformers",
            "url":"\/paper\/which-transformer-to-favor-a-comparative",
            "published":"2023-08-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/which-transformer-to-favor-a-comparative\/review\/?hl=108042"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":79152,
        "rank":521,
        "Model":"Swin-T (AutoMix+DM)",
        "mlmodel":{

        },
        "method_short":"Swin-T ",
        "method_details":"AutoMix+DM",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "Top 1 Accuracy":"81.92%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.92,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29398,
        "rank":522,
        "Model":"T2T-ViT-19",
        "mlmodel":{

        },
        "method_short":"T2T-ViT-19",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-28",
        "metrics":{
            "Top 1 Accuracy":"81.9%",
            "Number of params":null,
            "GFLOPs":"17.0",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.9,
            "Number of params":null,
            "GFLOPs":17.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":740117,
            "title":"Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet",
            "url":"\/paper\/tokens-to-token-vit-training-vision",
            "published":"2021-01-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/tokens-to-token-vit-training-vision\/review\/?hl=29398"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":44126,
        "rank":523,
        "Model":"ResNet-101 (224 res, Fast Knowledge Distillation)",
        "mlmodel":{

        },
        "method_short":"ResNet-101 ",
        "method_details":"224 res, Fast Knowledge Distillation",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-02",
        "metrics":{
            "Top 1 Accuracy":"81.9%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.9,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":924618,
            "title":"A Fast Knowledge Distillation Framework for Visual Recognition",
            "url":"\/paper\/a-fast-knowledge-distillation-framework-for",
            "published":"2021-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/a-fast-knowledge-distillation-framework-for\/review\/?hl=44126"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":33620,
        "rank":524,
        "Model":"RVT-S",
        "mlmodel":{

        },
        "method_short":"RVT-S*",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-17",
        "metrics":{
            "Top 1 Accuracy":"81.9%",
            "Number of params":"23.3M",
            "GFLOPs":"4.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.9,
            "Number of params":23300000.0,
            "GFLOPs":4.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":800173,
            "title":"Towards Robust Vision Transformer",
            "url":"\/paper\/rethinking-the-design-principles-of-robust",
            "published":"2021-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-the-design-principles-of-robust\/review\/?hl=33620"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29409,
        "rank":525,
        "Model":"PiT-S",
        "mlmodel":{

        },
        "method_short":"PiT-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-30",
        "metrics":{
            "Top 1 Accuracy":"81.9%",
            "Number of params":"23.5M",
            "GFLOPs":"2.9",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.9,
            "Number of params":23500000.0,
            "GFLOPs":2.9,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":771780,
            "title":"Rethinking Spatial Dimensions of Vision Transformers",
            "url":"\/paper\/rethinking-spatial-dimensions-of-vision",
            "published":"2021-03-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-spatial-dimensions-of-vision\/review\/?hl=29409"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":39380,
        "rank":526,
        "Model":"sMLPNet-T (ImageNet-1k)",
        "mlmodel":{

        },
        "method_short":"sMLPNet-T ",
        "method_details":"ImageNet-1k",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-12",
        "metrics":{
            "Top 1 Accuracy":"81.9%",
            "Number of params":"24.1M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.9,
            "Number of params":24100000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":865635,
            "title":"Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?",
            "url":"\/paper\/sparse-mlp-for-image-recognition-is-self",
            "published":"2021-09-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/sparse-mlp-for-image-recognition-is-self\/review\/?hl=39380"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":18,
                "name":"MLP",
                "color":"#ffae00"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29124,
        "rank":527,
        "Model":"ViL-Base-W",
        "mlmodel":{

        },
        "method_short":"ViL-Base-W",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-29",
        "metrics":{
            "Top 1 Accuracy":"81.9%",
            "Number of params":"79M",
            "GFLOPs":"6.74",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.9,
            "Number of params":79000000.0,
            "GFLOPs":6.74,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":758437,
            "title":"Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding",
            "url":"\/paper\/2103-15358",
            "published":"2021-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/2103-15358\/review\/?hl=29124"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":105987,
        "rank":528,
        "Model":"Swin-T+SSA",
        "mlmodel":{

        },
        "method_short":"Swin-T+SSA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-06-02",
        "metrics":{
            "Top 1 Accuracy":"81.89%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.89,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1222131,
            "title":"The Information Pathways Hypothesis: Transformers are Dynamic Self-Ensembles",
            "url":"\/paper\/the-information-pathways-hypothesis",
            "published":"2023-06-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/the-information-pathways-hypothesis\/review\/?hl=105987"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":11209,
        "rank":529,
        "Model":"AOGNet-40M-AN",
        "mlmodel":{

        },
        "method_short":"AOGNet-40M-AN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-04",
        "metrics":{
            "Top 1 Accuracy":"81.87%",
            "Number of params":null,
            "GFLOPs":"7.51",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.87,
            "Number of params":null,
            "GFLOPs":7.51,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":149150,
            "title":"Attentive Normalization",
            "url":"\/paper\/attentive-normalization",
            "published":"2019-08-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/attentive-normalization\/review\/?hl=11209"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":43136,
        "rank":530,
        "Model":"FBNetV5",
        "mlmodel":{

        },
        "method_short":"FBNetV5",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-19",
        "metrics":{
            "Top 1 Accuracy":"81.8%",
            "Number of params":null,
            "GFLOPs":"0.726",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.8,
            "Number of params":null,
            "GFLOPs":0.726,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":912986,
            "title":"FBNetV5: Neural Architecture Search for Multiple Tasks in One Run",
            "url":"\/paper\/fbnetv5-neural-architecture-search-for",
            "published":"2021-11-19T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/fbnetv5-neural-architecture-search-for\/review\/?hl=43136"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60737,
        "rank":531,
        "Model":"NASViT-A5",
        "mlmodel":{

        },
        "method_short":"NASViT-A5",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-29",
        "metrics":{
            "Top 1 Accuracy":"81.8%",
            "Number of params":null,
            "GFLOPs":"0.757",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.8,
            "Number of params":null,
            "GFLOPs":0.757,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":882976,
            "title":"NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training",
            "url":"\/paper\/nasvit-neural-architecture-search-for",
            "published":"2021-09-29T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":37571,
        "rank":532,
        "Model":"ResNet-200",
        "mlmodel":{

        },
        "method_short":"ResNet-200",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-26",
        "metrics":{
            "Top 1 Accuracy":"81.8%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.8,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":841133,
            "title":"Parametric Contrastive Learning",
            "url":"\/paper\/parametric-contrastive-learning",
            "published":"2021-07-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/parametric-contrastive-learning\/review\/?hl=37571"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":44842,
        "rank":533,
        "Model":"RepMLPNet-L256",
        "mlmodel":{

        },
        "method_short":"RepMLPNet-L256",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-21",
        "metrics":{
            "Top 1 Accuracy":"81.8%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.8,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":932764,
            "title":"RepMLPNet: Hierarchical Vision MLP with Re-parameterized Locality",
            "url":"\/paper\/repmlpnet-hierarchical-vision-mlp-with-re",
            "published":"2021-12-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/repmlpnet-hierarchical-vision-mlp-with-re\/review\/?hl=44842"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":18,
                "name":"MLP",
                "color":"#ffae00"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":87841,
        "rank":534,
        "Model":"NEXcepTion-TP",
        "mlmodel":{

        },
        "method_short":"NEXcepTion-TP",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-16",
        "metrics":{
            "Top 1 Accuracy":"81.8%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.8,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1129870,
            "title":"From Xception to NEXcepTion: New Design Decisions and Neural Architecture Search",
            "url":"\/paper\/from-xception-to-nexception-new-design",
            "published":"2022-12-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/from-xception-to-nexception-new-design\/review\/?hl=87841"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":52489,
        "rank":535,
        "Model":"NAT-Mini",
        "mlmodel":{

        },
        "method_short":"NAT-Mini",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-14",
        "metrics":{
            "Top 1 Accuracy":"81.8%",
            "Number of params":"20M",
            "GFLOPs":"2.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.8,
            "Number of params":20000000.0,
            "GFLOPs":2.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":994419,
            "title":"Neighborhood Attention Transformer",
            "url":"\/paper\/neighborhood-attention-transformer",
            "published":"2022-04-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/neighborhood-attention-transformer\/review\/?hl=52489"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":265,
                "name":"Neighborhood Attention",
                "color":"#ba1330"
            },
            {
                "id":266,
                "name":"NAT Transformer",
                "color":"#574896"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":70625,
        "rank":536,
        "Model":"DiNAT-Mini",
        "mlmodel":{

        },
        "method_short":"DiNAT-Mini",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-29",
        "metrics":{
            "Top 1 Accuracy":"81.8%",
            "Number of params":"20M",
            "GFLOPs":"2.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.8,
            "Number of params":20000000.0,
            "GFLOPs":2.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1082646,
            "title":"Dilated Neighborhood Attention Transformer",
            "url":"\/paper\/dilated-neighborhood-attention-transformer",
            "published":"2022-09-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dilated-neighborhood-attention-transformer\/review\/?hl=70625"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":265,
                "name":"Neighborhood Attention",
                "color":"#ba1330"
            },
            {
                "id":266,
                "name":"NAT Transformer",
                "color":"#574896"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":40189,
        "rank":537,
        "Model":"ResNet-152 (A2)",
        "mlmodel":{

        },
        "method_short":"ResNet-152 ",
        "method_details":"A2",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-01",
        "metrics":{
            "Top 1 Accuracy":"81.8%",
            "Number of params":"60.2M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.8,
            "Number of params":60200000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":877343,
            "title":"ResNet strikes back: An improved training procedure in timm",
            "url":"\/paper\/resnet-strikes-back-an-improved-training",
            "published":"2021-10-01T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":57973,
        "rank":538,
        "Model":"MEAL V2 (ResNet-50) (380 res)",
        "mlmodel":{

        },
        "method_short":"MEAL V2 ",
        "method_details":"ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-09-17",
        "metrics":{
            "Top 1 Accuracy":"81.72%",
            "Number of params":"25.6M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.72,
            "Number of params":25600000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":218270,
            "title":"MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks",
            "url":"\/paper\/meal-v2-boosting-vanilla-resnet-50-to-80-top",
            "published":"2020-09-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/meal-v2-boosting-vanilla-resnet-50-to-80-top\/review\/?hl=57973"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":66004,
        "rank":539,
        "Model":"gSwin-T",
        "mlmodel":{

        },
        "method_short":"gSwin-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-08-24",
        "metrics":{
            "Top 1 Accuracy":"81.71%",
            "Number of params":"21.8M",
            "GFLOPs":"3.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.71,
            "Number of params":21800000.0,
            "GFLOPs":3.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1064636,
            "title":"gSwin: Gated MLP Vision Model with Hierarchical Structure of Shifted Window",
            "url":"\/paper\/gswin-gated-mlp-vision-model-with",
            "published":"2022-08-24T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/gswin-gated-mlp-vision-model-with\/review\/?hl=66004"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":48,
                "name":"Swin-Transformer",
                "color":"#f75c2f"
            },
            {
                "id":18,
                "name":"MLP",
                "color":"#ffae00"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60746,
        "rank":540,
        "Model":"FBNetV5-A-CLS",
        "mlmodel":{

        },
        "method_short":"FBNetV5-A-CLS",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-19",
        "metrics":{
            "Top 1 Accuracy":"81.7%",
            "Number of params":null,
            "GFLOPs":"0.685",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.7,
            "Number of params":null,
            "GFLOPs":0.685,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":912986,
            "title":"FBNetV5: Neural Architecture Search for Multiple Tasks in One Run",
            "url":"\/paper\/fbnetv5-neural-architecture-search-for",
            "published":"2021-11-19T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/fbnetv5-neural-architecture-search-for\/review\/?hl=60746"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":31942,
        "rank":541,
        "Model":"T2T-ViT-14",
        "mlmodel":{

        },
        "method_short":"T2T-ViT-14",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-05",
        "metrics":{
            "Top 1 Accuracy":"81.7%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.7,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":794722,
            "title":"Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks",
            "url":"\/paper\/beyond-self-attention-external-attention",
            "published":"2021-05-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/beyond-self-attention-external-attention\/review\/?hl=31942"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":52667,
        "rank":542,
        "Model":"QnA-ViT-Tiny",
        "mlmodel":{

        },
        "method_short":"QnA-ViT-Tiny",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-21",
        "metrics":{
            "Top 1 Accuracy":"81.7%",
            "Number of params":"16M",
            "GFLOPs":"2.5",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.7,
            "Number of params":16000000.0,
            "GFLOPs":2.5,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":933167,
            "title":"Learned Queries for Efficient Local Attention",
            "url":"\/paper\/learned-queries-for-efficient-local-attention",
            "published":"2021-12-21T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":37554,
        "rank":543,
        "Model":"AutoFormer-small",
        "mlmodel":{

        },
        "method_short":"AutoFormer-small",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-01",
        "metrics":{
            "Top 1 Accuracy":"81.7%",
            "Number of params":"22.9M",
            "GFLOPs":"5.1",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.7,
            "Number of params":22900000.0,
            "GFLOPs":5.1,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":828740,
            "title":"AutoFormer: Searching Transformers for Visual Recognition",
            "url":"\/paper\/autoformer-searching-transformers-for-visual",
            "published":"2021-07-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/autoformer-searching-transformers-for-visual\/review\/?hl=37554"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":46840,
        "rank":544,
        "Model":"Shift-T",
        "mlmodel":{

        },
        "method_short":"Shift-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-01-26",
        "metrics":{
            "Top 1 Accuracy":"81.7%",
            "Number of params":"28M",
            "GFLOPs":"4.4",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.7,
            "Number of params":28000000.0,
            "GFLOPs":4.4,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":951213,
            "title":"When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism",
            "url":"\/paper\/when-shift-operation-meets-vision-transformer",
            "published":"2022-01-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/when-shift-operation-meets-vision-transformer\/review\/?hl=46840"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":24934,
        "rank":545,
        "Model":"BoTNet T3",
        "mlmodel":{

        },
        "method_short":"BoTNet T3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-27",
        "metrics":{
            "Top 1 Accuracy":"81.7%",
            "Number of params":"33.5M",
            "GFLOPs":"7.3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.7,
            "Number of params":33500000.0,
            "GFLOPs":7.3,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":739590,
            "title":"Bottleneck Transformers for Visual Recognition",
            "url":"\/paper\/bottleneck-transformers-for-visual",
            "published":"2021-01-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/bottleneck-transformers-for-visual\/review\/?hl=24934"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29247,
        "rank":546,
        "Model":"CvT-13",
        "mlmodel":{

        },
        "method_short":"CvT-13",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-29",
        "metrics":{
            "Top 1 Accuracy":"81.6%",
            "Number of params":null,
            "GFLOPs":"4.5",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.6,
            "Number of params":null,
            "GFLOPs":4.5,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":758429,
            "title":"CvT: Introducing Convolutions to Vision Transformers",
            "url":"\/paper\/cvt-introducing-convolutions-to-vision",
            "published":"2021-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/cvt-introducing-convolutions-to-vision\/review\/?hl=29247"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":21312,
        "rank":547,
        "Model":"ResNet-152 (SAM)",
        "mlmodel":{

        },
        "method_short":"ResNet-152 ",
        "method_details":"SAM",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-10-03",
        "metrics":{
            "Top 1 Accuracy":"81.6%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.6,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":225198,
            "title":"Sharpness-Aware Minimization for Efficiently Improving Generalization",
            "url":"\/paper\/sharpness-aware-minimization-for-efficiently-1",
            "published":"2020-10-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/sharpness-aware-minimization-for-efficiently-1\/review\/?hl=21312"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":112827,
        "rank":548,
        "Model":"UniRepLKNet-N",
        "mlmodel":{

        },
        "method_short":"UniRepLKNet-N",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-27",
        "metrics":{
            "Top 1 Accuracy":"81.6%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.6,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1327838,
            "title":"UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition",
            "url":"\/paper\/unireplknet-a-universal-perception-large",
            "published":"2023-11-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/unireplknet-a-universal-perception-large\/review\/?hl=112827"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":101975,
        "rank":549,
        "Model":"CloFormer-S",
        "mlmodel":{

        },
        "method_short":"CloFormer-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-31",
        "metrics":{
            "Top 1 Accuracy":"81.6%",
            "Number of params":"12.3M",
            "GFLOPs":"2",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.6,
            "Number of params":12300000.0,
            "GFLOPs":2.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1184169,
            "title":"Rethinking Local Perception in Lightweight Vision Transformer",
            "url":"\/paper\/rethinking-local-perception-in-lightweight",
            "published":"2023-03-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-local-perception-in-lightweight\/review\/?hl=101975"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29470,
        "rank":550,
        "Model":"LeViT-256",
        "mlmodel":{

        },
        "method_short":"LeViT-256",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-02",
        "metrics":{
            "Top 1 Accuracy":"81.6%",
            "Number of params":"17.8M",
            "GFLOPs":"1.066",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.6,
            "Number of params":17800000.0,
            "GFLOPs":1.066,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":774248,
            "title":"LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference",
            "url":"\/paper\/levit-a-vision-transformer-in-convnet-s",
            "published":"2021-04-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/levit-a-vision-transformer-in-convnet-s\/review\/?hl=29470"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":49041,
        "rank":551,
        "Model":"ReXNet_2.0",
        "mlmodel":{

        },
        "method_short":"ReXNet_2.0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-07-02",
        "metrics":{
            "Top 1 Accuracy":"81.6%",
            "Number of params":"19M",
            "GFLOPs":"1.5",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.6,
            "Number of params":19000000.0,
            "GFLOPs":1.5,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":206716,
            "title":"Rethinking Channel Dimensions for Efficient Model Design",
            "url":"\/paper\/rexnet-diminishing-representational",
            "published":"2020-07-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rexnet-diminishing-representational\/review\/?hl=49041"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":63785,
        "rank":552,
        "Model":"SE-CoTNetD-50",
        "mlmodel":{

        },
        "method_short":"SE-CoTNetD-50",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-26",
        "metrics":{
            "Top 1 Accuracy":"81.6%",
            "Number of params":"23.1M",
            "GFLOPs":"4.1",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.6,
            "Number of params":23100000.0,
            "GFLOPs":4.1,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":841137,
            "title":"Contextual Transformer Networks for Visual Recognition",
            "url":"\/paper\/contextual-transformer-networks-for-visual",
            "published":"2021-07-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/contextual-transformer-networks-for-visual\/review\/?hl=63785"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":33111,
        "rank":553,
        "Model":"gMLP-B",
        "mlmodel":{

        },
        "method_short":"gMLP-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-17",
        "metrics":{
            "Top 1 Accuracy":"81.6%",
            "Number of params":"73M",
            "GFLOPs":"31.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.6,
            "Number of params":73000000.0,
            "GFLOPs":31.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":800166,
            "title":"Pay Attention to MLPs",
            "url":"\/paper\/pay-attention-to-mlps",
            "published":"2021-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/pay-attention-to-mlps\/review\/?hl=33111"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":18,
                "name":"MLP",
                "color":"#ffae00"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":38562,
        "rank":554,
        "Model":"CoE-Large + CondConv",
        "mlmodel":{

        },
        "method_short":"CoE-Large + CondConv",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-08",
        "metrics":{
            "Top 1 Accuracy":"81.5%",
            "Number of params":null,
            "GFLOPs":"0.214",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.5,
            "Number of params":null,
            "GFLOPs":0.214,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":832426,
            "title":"Collaboration of Experts: Achieving 80% Top-1 Accuracy on ImageNet with 100M FLOPs",
            "url":"\/paper\/collaboration-of-experts-achieving-80-top-1",
            "published":"2021-07-08T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/collaboration-of-experts-achieving-80-top-1\/review\/?hl=38562"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":87842,
        "rank":555,
        "Model":"NEXcepTion-T",
        "mlmodel":{

        },
        "method_short":"NEXcepTion-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-16",
        "metrics":{
            "Top 1 Accuracy":"81.5%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.5,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1129870,
            "title":"From Xception to NEXcepTion: New Design Decisions and Neural Architecture Search",
            "url":"\/paper\/from-xception-to-nexception-new-design",
            "published":"2022-12-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/from-xception-to-nexception-new-design\/review\/?hl=87842"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":11211,
        "rank":556,
        "Model":"NoisyStudent (EfficientNet-B1)",
        "mlmodel":{

        },
        "method_short":"NoisyStudent ",
        "method_details":"EfficientNet-B1",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-11",
        "metrics":{
            "Top 1 Accuracy":"81.5%",
            "Number of params":"7.8M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.5,
            "Number of params":7800000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":170047,
            "title":"Self-training with Noisy Student improves ImageNet classification",
            "url":"\/paper\/self-training-with-noisy-student-improves",
            "published":"2019-11-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/self-training-with-noisy-student-improves\/review\/?hl=11211"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60261,
        "rank":557,
        "Model":"TinyViT-11M",
        "mlmodel":{

        },
        "method_short":"TinyViT-11M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-21",
        "metrics":{
            "Top 1 Accuracy":"81.5%",
            "Number of params":"11M",
            "GFLOPs":"2.0",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.5,
            "Number of params":11000000.0,
            "GFLOPs":2.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1047551,
            "title":"TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
            "url":"\/paper\/tinyvit-fast-pretraining-distillation-for",
            "published":"2022-07-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/tinyvit-fast-pretraining-distillation-for\/review\/?hl=60261"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":33773,
        "rank":558,
        "Model":"Transformer local-attention (NesT-T)",
        "mlmodel":{

        },
        "method_short":"Transformer local-attention ",
        "method_details":"NesT-T",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-26",
        "metrics":{
            "Top 1 Accuracy":"81.5%",
            "Number of params":"17M",
            "GFLOPs":"5.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.5,
            "Number of params":17000000.0,
            "GFLOPs":5.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":805906,
            "title":"Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding",
            "url":"\/paper\/aggregating-nested-transformers",
            "published":"2021-05-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/aggregating-nested-transformers\/review\/?hl=33773"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":24998,
        "rank":559,
        "Model":"T2T-ViT-14",
        "mlmodel":{

        },
        "method_short":"T2T-ViT-14",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-28",
        "metrics":{
            "Top 1 Accuracy":"81.5%",
            "Number of params":"21.5M",
            "GFLOPs":"9.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.5,
            "Number of params":21500000.0,
            "GFLOPs":9.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":740117,
            "title":"Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet",
            "url":"\/paper\/tokens-to-token-vit-training-vision",
            "published":"2021-01-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/tokens-to-token-vit-training-vision\/review\/?hl=24998"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29107,
        "rank":560,
        "Model":"CrossViT-15",
        "mlmodel":{

        },
        "method_short":"CrossViT-15",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-27",
        "metrics":{
            "Top 1 Accuracy":"81.5%",
            "Number of params":"27.4M",
            "GFLOPs":"5.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.5,
            "Number of params":27400000.0,
            "GFLOPs":5.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":758441,
            "title":"CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification",
            "url":"\/paper\/2103-14899",
            "published":"2021-03-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/2103-14899\/review\/?hl=29107"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":17490,
        "rank":561,
        "Model":"PyConvResNet-101",
        "mlmodel":{

        },
        "method_short":"PyConvResNet-101",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-20",
        "metrics":{
            "Top 1 Accuracy":"81.49%",
            "Number of params":"42.3M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.49,
            "Number of params":42300000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":205105,
            "title":"Pyramidal Convolution: Rethinking Convolutional Neural Networks for Visual Recognition",
            "url":"\/paper\/pyramidal-convolution-rethinking",
            "published":"2020-06-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/pyramidal-convolution-rethinking\/review\/?hl=17490"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60736,
        "rank":562,
        "Model":"NASViT-A4",
        "mlmodel":{

        },
        "method_short":"NASViT-A4",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-29",
        "metrics":{
            "Top 1 Accuracy":"81.4%",
            "Number of params":null,
            "GFLOPs":"0.591",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.4,
            "Number of params":null,
            "GFLOPs":0.591,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":882976,
            "title":"NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training",
            "url":"\/paper\/nasvit-neural-architecture-search-for",
            "published":"2021-09-29T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":37856,
        "rank":563,
        "Model":"DeiT-S with iRPE-QKV",
        "mlmodel":{

        },
        "method_short":"DeiT-S with iRPE-QKV",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-29",
        "metrics":{
            "Top 1 Accuracy":"81.4%",
            "Number of params":null,
            "GFLOPs":"9.770",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.4,
            "Number of params":null,
            "GFLOPs":9.77,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":843509,
            "title":"Rethinking and Improving Relative Position Encoding for Vision Transformer",
            "url":"\/paper\/rethinking-and-improving-relative-position",
            "published":"2021-07-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-and-improving-relative-position\/review\/?hl=37856"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":52536,
        "rank":564,
        "Model":"ViT-S @224 (DeiT III)",
        "mlmodel":{

        },
        "method_short":"ViT-S @224 ",
        "method_details":"DeiT III",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-14",
        "metrics":{
            "Top 1 Accuracy":"81.4%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.4,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":994466,
            "title":"DeiT III: Revenge of the ViT",
            "url":"\/paper\/deit-iii-revenge-of-the-vit",
            "published":"2022-04-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deit-iii-revenge-of-the-vit\/review\/?hl=52536"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":99522,
        "rank":565,
        "Model":"BiFormer-T (IN1k ptretrain)",
        "mlmodel":{

        },
        "method_short":"BiFormer-T ",
        "method_details":"IN1k ptretrain",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-15",
        "metrics":{
            "Top 1 Accuracy":"81.4%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.4,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1174425,
            "title":"BiFormer: Vision Transformer with Bi-Level Routing Attention",
            "url":"\/paper\/biformer-vision-transformer-with-bi-level",
            "published":"2023-03-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/biformer-vision-transformer-with-bi-level\/review\/?hl=99522"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":105111,
        "rank":566,
        "Model":"MobileOne-S4 (distill)",
        "mlmodel":{

        },
        "method_short":"MobileOne-S4 ",
        "method_details":"distill",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-08",
        "metrics":{
            "Top 1 Accuracy":"81.4%",
            "Number of params":"14.8M",
            "GFLOPs":"2.9",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.4,
            "Number of params":14800000.0,
            "GFLOPs":2.9,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1023967,
            "title":"MobileOne: An Improved One millisecond Mobile Backbone",
            "url":"\/paper\/an-improved-one-millisecond-mobile-backbone",
            "published":"2022-06-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/an-improved-one-millisecond-mobile-backbone\/review\/?hl=105111"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":24935,
        "rank":567,
        "Model":"SENet-101",
        "mlmodel":{

        },
        "method_short":"SENet-101",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-27",
        "metrics":{
            "Top 1 Accuracy":"81.4%",
            "Number of params":"49.2M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.4,
            "Number of params":49200000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":739590,
            "title":"Bottleneck Transformers for Visual Recognition",
            "url":"\/paper\/bottleneck-transformers-for-visual",
            "published":"2021-01-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/bottleneck-transformers-for-visual\/review\/?hl=24935"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":61472,
        "rank":568,
        "Model":"CCT-14\/7x2",
        "mlmodel":{

        },
        "method_short":"CCT-14\/7x2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-12",
        "metrics":{
            "Top 1 Accuracy":"81.34%",
            "Number of params":"22.36M",
            "GFLOPs":"11.06",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.34,
            "Number of params":22360000.0,
            "GFLOPs":11.06,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":778781,
            "title":"Escaping the Big Data Paradigm with Compact Transformers",
            "url":"\/paper\/escaping-the-big-data-paradigm-with-compact",
            "published":"2021-04-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/escaping-the-big-data-paradigm-with-compact\/review\/?hl=61472"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":108038,
        "rank":569,
        "Model":"GFNet-S",
        "mlmodel":{

        },
        "method_short":"GFNet-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-18",
        "metrics":{
            "Top 1 Accuracy":"81.33%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.33,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1265182,
            "title":"Which Transformer to Favor: A Comparative Analysis of Efficiency in Vision Transformers",
            "url":"\/paper\/which-transformer-to-favor-a-comparative",
            "published":"2023-08-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/which-transformer-to-favor-a-comparative\/review\/?hl=108038"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":22144,
        "rank":570,
        "Model":"ResNet-200 (Adversarial Autoaugment)",
        "mlmodel":{

        },
        "method_short":"ResNet-200 ",
        "method_details":"Adversarial Autoaugment",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-12-24",
        "metrics":{
            "Top 1 Accuracy":"81.32%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.32,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":178148,
            "title":"Adversarial AutoAugment",
            "url":"\/paper\/adversarial-autoaugment-1",
            "published":"2019-12-24T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/adversarial-autoaugment-1\/review\/?hl=22144"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":11212,
        "rank":571,
        "Model":"MultiGrain PNASNet (300px)",
        "mlmodel":{

        },
        "method_short":"MultiGrain PNASNet ",
        "method_details":"300px",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-02-14",
        "metrics":{
            "Top 1 Accuracy":"81.3%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.3,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":105902,
            "title":"MultiGrain: a unified image embedding for classes and instances",
            "url":"\/paper\/multigrain-a-unified-image-embedding-for",
            "published":"2019-02-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multigrain-a-unified-image-embedding-for\/review\/?hl=11212"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":37508,
        "rank":572,
        "Model":"ResNet-152",
        "mlmodel":{

        },
        "method_short":"ResNet-152",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-26",
        "metrics":{
            "Top 1 Accuracy":"81.3%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.3,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":841133,
            "title":"Parametric Contrastive Learning",
            "url":"\/paper\/parametric-contrastive-learning",
            "published":"2021-07-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/parametric-contrastive-learning\/review\/?hl=37508"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[
            {
                "url":"https:\/\/github.com\/facebookresearch\/AugLy\/blob\/main\/examples\/imagenet\/evaluate_robustness_imagenet_resnet152.ipynb"
            }
        ]
    },
    {
        "table_id":116,
        "row_id":28360,
        "rank":573,
        "Model":"ConViT-S",
        "mlmodel":{

        },
        "method_short":"ConViT-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-19",
        "metrics":{
            "Top 1 Accuracy":"81.3%",
            "Number of params":"27M",
            "GFLOPs":"5.4",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.3,
            "Number of params":27000000.0,
            "GFLOPs":5.4,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755434,
            "title":"ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases",
            "url":"\/paper\/convit-improving-vision-transformers-with",
            "published":"2021-03-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/convit-improving-vision-transformers-with\/review\/?hl=28360"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60638,
        "rank":574,
        "Model":"Swin-T",
        "mlmodel":{

        },
        "method_short":"Swin-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-25",
        "metrics":{
            "Top 1 Accuracy":"81.3%",
            "Number of params":"29M",
            "GFLOPs":"4.5",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.3,
            "Number of params":29000000.0,
            "GFLOPs":4.5,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":757245,
            "title":"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
            "url":"\/paper\/swin-transformer-hierarchical-vision",
            "published":"2021-03-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/swin-transformer-hierarchical-vision\/review\/?hl=60638"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":97061,
        "rank":575,
        "Model":"SimpleNetV1-9m-correct-labels",
        "mlmodel":{

        },
        "method_short":"SimpleNetV1-9m-correct-labels",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-08-22",
        "metrics":{
            "Top 1 Accuracy":"81.24",
            "Number of params":"9.5M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.24,
            "Number of params":9500000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":10271,
            "title":"Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures",
            "url":"\/paper\/lets-keep-it-simple-using-simple",
            "published":"2016-08-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lets-keep-it-simple-using-simple\/review\/?hl=97061"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":10252,
        "rank":576,
        "Model":"Res2Net-101",
        "mlmodel":{

        },
        "method_short":"Res2Net-101",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-02",
        "metrics":{
            "Top 1 Accuracy":"81.23%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.23,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":110193,
            "title":"Res2Net: A New Multi-scale Backbone Architecture",
            "url":"\/paper\/res2net-a-new-multi-scale-backbone",
            "published":"2019-04-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/res2net-a-new-multi-scale-backbone\/review\/?hl=10252"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":20874,
        "rank":577,
        "Model":"ResNeXt-101 (Debiased+CutMix)",
        "mlmodel":{

        },
        "method_short":"ResNeXt-101 ",
        "method_details":"Debiased+CutMix",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-10-04",
        "metrics":{
            "Top 1 Accuracy":"81.2",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.2,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":227347,
            "title":"Shape-Texture Debiased Neural Network Training",
            "url":"\/paper\/shape-texture-debiased-neural-network-1",
            "published":"2020-10-12T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":6,
                "name":"ResNeXt",
                "color":"#86960b"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":108979,
        "rank":578,
        "Model":"PVT-S (+MixPro)",
        "mlmodel":{

        },
        "method_short":"PVT-S ",
        "method_details":"+MixPro",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-24",
        "metrics":{
            "Top 1 Accuracy":"81.2%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.2,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1195795,
            "title":"MixPro: Data Augmentation with MaskMix and Progressive Attention Labeling for Vision Transformer",
            "url":"\/paper\/mixpro-data-augmentation-with-maskmix-and",
            "published":"2023-04-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mixpro-data-augmentation-with-maskmix-and\/review\/?hl=108979"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":79150,
        "rank":579,
        "Model":"Swin-T (PuzzleMix+DM)",
        "mlmodel":{

        },
        "method_short":"Swin-T ",
        "method_details":"PuzzleMix+DM",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "Top 1 Accuracy":"81.16%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.16,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":71928,
        "rank":580,
        "Model":"TransBoost-ResNet50-StrikesBack",
        "mlmodel":{

        },
        "method_short":"TransBoost-ResNet50-StrikesBack",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-26",
        "metrics":{
            "Top 1 Accuracy":"81.15%",
            "Number of params":"25.56M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.15,
            "Number of params":25560000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1016638,
            "title":"TransBoost: Improving the Best ImageNet Performance using Deep Transduction",
            "url":"\/paper\/transboost-improving-the-best-imagenet",
            "published":"2022-05-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/transboost-improving-the-best-imagenet\/review\/?hl=71928"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60624,
        "rank":581,
        "Model":"ResNeSt-50",
        "mlmodel":{

        },
        "method_short":"ResNeSt-50",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-04-19",
        "metrics":{
            "Top 1 Accuracy":"81.13%",
            "Number of params":"27.5M",
            "GFLOPs":"5.39",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.13,
            "Number of params":27500000.0,
            "GFLOPs":5.39,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":191448,
            "title":"ResNeSt: Split-Attention Networks",
            "url":"\/paper\/resnest-split-attention-networks",
            "published":"2020-04-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/resnest-split-attention-networks\/review\/?hl=60624"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":79153,
        "rank":582,
        "Model":"DeiT-S (SAMix+DM)",
        "mlmodel":{

        },
        "method_short":"DeiT-S ",
        "method_details":"SAMix+DM",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "Top 1 Accuracy":"81.12%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.12,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":37855,
        "rank":583,
        "Model":"DeiT-S with iRPE-QK",
        "mlmodel":{

        },
        "method_short":"DeiT-S with iRPE-QK",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-29",
        "metrics":{
            "Top 1 Accuracy":"81.1%",
            "Number of params":null,
            "GFLOPs":"9.412",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.1,
            "Number of params":null,
            "GFLOPs":9.412,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":843509,
            "title":"Rethinking and Improving Relative Position Encoding for Vision Transformer",
            "url":"\/paper\/rethinking-and-improving-relative-position",
            "published":"2021-07-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-and-improving-relative-position\/review\/?hl=37855"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6184,
        "rank":584,
        "Model":"EfficientNet-B3",
        "mlmodel":{

        },
        "method_short":"EfficientNet-B3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-05-28",
        "metrics":{
            "Top 1 Accuracy":"81.1%",
            "Number of params":"12M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.1,
            "Number of params":12000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":117456,
            "title":"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
            "url":"\/paper\/efficientnet-rethinking-model-scaling-for",
            "published":"2019-05-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/efficientnet-rethinking-model-scaling-for\/review\/?hl=6184"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":61462,
        "rank":585,
        "Model":"VAN-B1",
        "mlmodel":{

        },
        "method_short":"VAN-B1",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-20",
        "metrics":{
            "Top 1 Accuracy":"81.1%",
            "Number of params":"13.9M",
            "GFLOPs":"2.5",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.1,
            "Number of params":13900000.0,
            "GFLOPs":2.5,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":964888,
            "title":"Visual Attention Network",
            "url":"\/paper\/visual-attention-network",
            "published":"2022-02-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/visual-attention-network\/review\/?hl=61462"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":64203,
        "rank":586,
        "Model":"RevBiFPN-S3",
        "mlmodel":{

        },
        "method_short":"RevBiFPN-S3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-28",
        "metrics":{
            "Top 1 Accuracy":"81.1%",
            "Number of params":"19.6M",
            "GFLOPs":"3.33",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.1,
            "Number of params":19600000.0,
            "GFLOPs":3.33,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1034397,
            "title":"RevBiFPN: The Fully Reversible Bidirectional Feature Pyramid Network",
            "url":"\/paper\/revbifpn-the-fully-reversible-bidirectional",
            "published":"2022-06-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/revbifpn-the-fully-reversible-bidirectional\/review\/?hl=64203"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":283,
                "name":"Reversible",
                "color":"#3227d3"
            },
            {
                "id":13,
                "name":"FPN",
                "color":"#009481"
            },
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":34305,
        "rank":587,
        "Model":"ResNet-152x2-SAM",
        "mlmodel":{

        },
        "method_short":"ResNet-152x2-SAM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-03",
        "metrics":{
            "Top 1 Accuracy":"81.1%",
            "Number of params":"236M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.1,
            "Number of params":236000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":810994,
            "title":"When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations",
            "url":"\/paper\/when-vision-transformers-outperform-resnets",
            "published":"2021-06-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/when-vision-transformers-outperform-resnets\/review\/?hl=34305"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":108040,
        "rank":588,
        "Model":"DynamicViT-S",
        "mlmodel":{

        },
        "method_short":"DynamicViT-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-18",
        "metrics":{
            "Top 1 Accuracy":"81.09%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.09,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1265182,
            "title":"Which Transformer to Favor: A Comparative Analysis of Efficiency in Vision Transformers",
            "url":"\/paper\/which-transformer-to-favor-a-comparative",
            "published":"2023-08-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/which-transformer-to-favor-a-comparative\/review\/?hl=108040"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":45001,
        "rank":589,
        "Model":"ResNet-101 (SAMix)",
        "mlmodel":{

        },
        "method_short":"ResNet-101 ",
        "method_details":"SAMix",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-30",
        "metrics":{
            "Top 1 Accuracy":"81.08%",
            "Number of params":"44.6M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.08,
            "Number of params":44600000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":923048,
            "title":"Boosting Discriminative Visual Representation Learning with Scenario-Agnostic Mixup",
            "url":"\/paper\/boosting-discriminative-visual-representation",
            "published":"2021-11-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/boosting-discriminative-visual-representation\/review\/?hl=45001"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60735,
        "rank":590,
        "Model":"NASViT-A3",
        "mlmodel":{

        },
        "method_short":"NASViT-A3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-29",
        "metrics":{
            "Top 1 Accuracy":"81.0%",
            "Number of params":null,
            "GFLOPs":"0.528",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.0,
            "Number of params":null,
            "GFLOPs":0.528,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":882976,
            "title":"NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training",
            "url":"\/paper\/nasvit-neural-architecture-search-for",
            "published":"2021-09-29T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":35599,
        "rank":591,
        "Model":"ViTAE-13M",
        "mlmodel":{

        },
        "method_short":"ViTAE-13M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-07",
        "metrics":{
            "Top 1 Accuracy":"81%",
            "Number of params":"13.2M",
            "GFLOPs":"6.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.0,
            "Number of params":13200000.0,
            "GFLOPs":6.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":812332,
            "title":"ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias",
            "url":"\/paper\/vitae-vision-transformer-advanced-by",
            "published":"2021-06-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vitae-vision-transformer-advanced-by\/review\/?hl=35599"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":44995,
        "rank":592,
        "Model":"ResNet-101 (AutoMix)",
        "mlmodel":{

        },
        "method_short":"ResNet-101 ",
        "method_details":"AutoMix",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-24",
        "metrics":{
            "Top 1 Accuracy":"80.98%",
            "Number of params":"44.6M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.98,
            "Number of params":44600000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":756879,
            "title":"AutoMix: Unveiling the Power of Mixup for Stronger Classifiers",
            "url":"\/paper\/automix-unveiling-the-power-of-mixup",
            "published":"2021-03-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/automix-unveiling-the-power-of-mixup\/review\/?hl=44995"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":79151,
        "rank":593,
        "Model":"DeiT-S (AutoMix+DM)",
        "mlmodel":{

        },
        "method_short":"DeiT-S ",
        "method_details":"AutoMix+DM",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "Top 1 Accuracy":"80.91%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.91,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":37509,
        "rank":594,
        "Model":"ResNet-101",
        "mlmodel":{

        },
        "method_short":"ResNet-101",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-26",
        "metrics":{
            "Top 1 Accuracy":"80.9%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.9,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":841133,
            "title":"Parametric Contrastive Learning",
            "url":"\/paper\/parametric-contrastive-learning",
            "published":"2021-07-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/parametric-contrastive-learning\/review\/?hl=37509"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29209,
        "rank":595,
        "Model":"CAIT-XXS-24",
        "mlmodel":{

        },
        "method_short":"CAIT-XXS-24",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-31",
        "metrics":{
            "Top 1 Accuracy":"80.9%",
            "Number of params":"12M",
            "GFLOPs":"9.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.9,
            "Number of params":12000000.0,
            "GFLOPs":9.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":772635,
            "title":"Going deeper with Image Transformers",
            "url":"\/paper\/going-deeper-with-image-transformers",
            "published":"2021-03-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/going-deeper-with-image-transformers\/review\/?hl=29209"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":37854,
        "rank":596,
        "Model":"DeiT-S with iRPE-K",
        "mlmodel":{

        },
        "method_short":"DeiT-S with iRPE-K",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-29",
        "metrics":{
            "Top 1 Accuracy":"80.9%",
            "Number of params":"22M",
            "GFLOPs":"9.318",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.9,
            "Number of params":22000000.0,
            "GFLOPs":9.318,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":843509,
            "title":"Rethinking and Improving Relative Position Encoding for Vision Transformer",
            "url":"\/paper\/rethinking-and-improving-relative-position",
            "published":"2021-07-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-and-improving-relative-position\/review\/?hl=37854"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":61473,
        "rank":597,
        "Model":"CentroidViT-S (arXiv, 2021-02)",
        "mlmodel":{

        },
        "method_short":"CentroidViT-S ",
        "method_details":"arXiv, 2021-02",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-17",
        "metrics":{
            "Top 1 Accuracy":"80.9%",
            "Number of params":"22.3M",
            "GFLOPs":"9.4",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.9,
            "Number of params":22300000.0,
            "GFLOPs":9.4,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":746312,
            "title":"Centroid Transformers: Learning to Abstract with Attention",
            "url":"\/paper\/centroid-transformers-learning-to-abstract",
            "published":"2021-02-17T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/centroid-transformers-learning-to-abstract\/review\/?hl=61473"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":2053,
        "rank":598,
        "Model":"ResNeXt-101  64x4",
        "mlmodel":{

        },
        "method_short":"ResNeXt-101  64x4",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-11-16",
        "metrics":{
            "Top 1 Accuracy":"80.9%",
            "Number of params":"83.6M",
            "GFLOPs":"31.5",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.9,
            "Number of params":83600000.0,
            "GFLOPs":31.5,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":24264,
            "title":"Aggregated Residual Transformations for Deep Neural Networks",
            "url":"\/paper\/aggregated-residual-transformations-for-deep",
            "published":"2016-11-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/aggregated-residual-transformations-for-deep\/review\/?hl=2053"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":6,
                "name":"ResNeXt",
                "color":"#86960b"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":57967,
        "rank":599,
        "Model":"AlphaNet-A6",
        "mlmodel":{

        },
        "method_short":"AlphaNet-A6",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-16",
        "metrics":{
            "Top 1 Accuracy":"80.8%",
            "Number of params":null,
            "GFLOPs":"0.709",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.8,
            "Number of params":null,
            "GFLOPs":0.709,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":745792,
            "title":"AlphaNet: Improved Training of Supernets with Alpha-Divergence",
            "url":"\/paper\/alphanet-improved-training-of-supernet-with",
            "published":"2021-02-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/alphanet-improved-training-of-supernet-with\/review\/?hl=57967"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":15976,
        "rank":600,
        "Model":"ResNet-200 (Supervised Contrastive)",
        "mlmodel":{

        },
        "method_short":"ResNet-200 ",
        "method_details":"Supervised Contrastive",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-04-23",
        "metrics":{
            "Top 1 Accuracy":"80.8%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.8,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":191948,
            "title":"Supervised Contrastive Learning",
            "url":"\/paper\/supervised-contrastive-learning",
            "published":"2020-04-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/supervised-contrastive-learning\/review\/?hl=15976"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60743,
        "rank":601,
        "Model":"UniNet-B0",
        "mlmodel":{

        },
        "method_short":"UniNet-B0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-12",
        "metrics":{
            "Top 1 Accuracy":"80.8%",
            "Number of params":"11.5M",
            "GFLOPs":"0.555",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.8,
            "Number of params":11500000.0,
            "GFLOPs":0.555,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1042502,
            "title":"UniNet: Unified Architecture Search with Convolution, Transformer, and MLP",
            "url":"\/paper\/uninet-unified-architecture-search-with-1",
            "published":"2022-07-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/uninet-unified-architecture-search-with-1\/review\/?hl=60743"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29951,
        "rank":602,
        "Model":"LocalViT-S",
        "mlmodel":{

        },
        "method_short":"LocalViT-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-12",
        "metrics":{
            "Top 1 Accuracy":"80.8%",
            "Number of params":"22.4M",
            "GFLOPs":"4.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.8,
            "Number of params":22400000.0,
            "GFLOPs":4.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":778779,
            "title":"LocalViT: Bringing Locality to Vision Transformers",
            "url":"\/paper\/localvit-bringing-locality-to-vision",
            "published":"2021-04-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/localvit-bringing-locality-to-vision\/review\/?hl=29951"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":98882,
        "rank":603,
        "Model":"DAFT-conv (384 heads, 300 epochs)",
        "mlmodel":{

        },
        "method_short":"DAFT-conv ",
        "method_details":"384 heads, 300 epochs",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-29",
        "metrics":{
            "Top 1 Accuracy":"80.8%",
            "Number of params":"23M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.8,
            "Number of params":23000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":884638,
            "title":"A Dot Product Attention Free Transformer",
            "url":"\/paper\/a-dot-product-attention-free-transformer",
            "published":"2021-09-29T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":34941,
        "rank":604,
        "Model":"ResMLP-S24",
        "mlmodel":{

        },
        "method_short":"ResMLP-S24",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-07",
        "metrics":{
            "Top 1 Accuracy":"80.8%",
            "Number of params":"30M",
            "GFLOPs":"6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.8,
            "Number of params":30000000.0,
            "GFLOPs":6.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":795412,
            "title":"ResMLP: Feedforward networks for image classification with data-efficient training",
            "url":"\/paper\/resmlp-feedforward-networks-for-image",
            "published":"2021-05-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/resmlp-feedforward-networks-for-image\/review\/?hl=34941"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":18,
                "name":"MLP",
                "color":"#ffae00"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60268,
        "rank":605,
        "Model":"TinyViT-5M-distill (21k)",
        "mlmodel":{

        },
        "method_short":"TinyViT-5M-distill ",
        "method_details":"21k",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-21",
        "metrics":{
            "Top 1 Accuracy":"80.7%",
            "Number of params":"5.4M",
            "GFLOPs":"1.3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.7,
            "Number of params":5400000.0,
            "GFLOPs":1.3,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1047551,
            "title":"TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
            "url":"\/paper\/tinyvit-fast-pretraining-distillation-for",
            "published":"2022-07-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/tinyvit-fast-pretraining-distillation-for\/review\/?hl=60268"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":105,
                "name":"ImageNet-22k",
                "color":"#bc51bd"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":38561,
        "rank":606,
        "Model":"CoE-Large",
        "mlmodel":{

        },
        "method_short":"CoE-Large",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-08",
        "metrics":{
            "Top 1 Accuracy":"80.7%",
            "Number of params":"95.3M",
            "GFLOPs":"0.194",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.7,
            "Number of params":95300000.0,
            "GFLOPs":0.194,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":832426,
            "title":"Collaboration of Experts: Achieving 80% Top-1 Accuracy on ImageNet with 100M FLOPs",
            "url":"\/paper\/collaboration-of-experts-achieving-80-top-1",
            "published":"2021-07-08T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/collaboration-of-experts-achieving-80-top-1\/review\/?hl=38561"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":57972,
        "rank":607,
        "Model":"MEAL V2 (ResNet-50) (224 res)",
        "mlmodel":{

        },
        "method_short":"MEAL V2 ",
        "method_details":"ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-09-17",
        "metrics":{
            "Top 1 Accuracy":"80.67%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.67,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":218270,
            "title":"MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks",
            "url":"\/paper\/meal-v2-boosting-vanilla-resnet-50-to-80-top",
            "published":"2020-09-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/meal-v2-boosting-vanilla-resnet-50-to-80-top\/review\/?hl=57972"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":108045,
        "rank":608,
        "Model":"TokenLearner-ViT-8",
        "mlmodel":{

        },
        "method_short":"TokenLearner-ViT-8",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-18",
        "metrics":{
            "Top 1 Accuracy":"80.66%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.66,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1265182,
            "title":"Which Transformer to Favor: A Comparative Analysis of Efficiency in Vision Transformers",
            "url":"\/paper\/which-transformer-to-favor-a-comparative",
            "published":"2023-08-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/which-transformer-to-favor-a-comparative\/review\/?hl=108045"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60625,
        "rank":609,
        "Model":"ResNeSt-50-fast",
        "mlmodel":{

        },
        "method_short":"ResNeSt-50-fast",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-04-19",
        "metrics":{
            "Top 1 Accuracy":"80.64%",
            "Number of params":"27.5M",
            "GFLOPs":"4.34",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.64,
            "Number of params":27500000.0,
            "GFLOPs":4.34,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":191448,
            "title":"ResNeSt: Split-Attention Networks",
            "url":"\/paper\/resnest-split-attention-networks",
            "published":"2020-04-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/resnest-split-attention-networks\/review\/?hl=60625"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":71925,
        "rank":610,
        "Model":"TransBoost-ResNet152",
        "mlmodel":{

        },
        "method_short":"TransBoost-ResNet152",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-26",
        "metrics":{
            "Top 1 Accuracy":"80.64%",
            "Number of params":"60.19M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.64,
            "Number of params":60190000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1016638,
            "title":"TransBoost: Improving the Best ImageNet Performance using Deep Transduction",
            "url":"\/paper\/transboost-improving-the-best-imagenet",
            "published":"2022-05-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/transboost-improving-the-best-imagenet\/review\/?hl=71925"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":9313,
        "rank":611,
        "Model":"ResNet-200 (Fast AA)",
        "mlmodel":{

        },
        "method_short":"ResNet-200 ",
        "method_details":"Fast AA",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-05-01",
        "metrics":{
            "Top 1 Accuracy":"80.6%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.6,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":113336,
            "title":"Fast AutoAugment",
            "url":"\/paper\/fast-autoaugment",
            "published":"2019-05-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fast-autoaugment\/review\/?hl=9313"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":108981,
        "rank":612,
        "Model":"CaiT-XXS (+MixPro)",
        "mlmodel":{

        },
        "method_short":"CaiT-XXS ",
        "method_details":"+MixPro",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-24",
        "metrics":{
            "Top 1 Accuracy":"80.6%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.6,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1195795,
            "title":"MixPro: Data Augmentation with MaskMix and Progressive Attention Labeling for Vision Transformer",
            "url":"\/paper\/mixpro-data-augmentation-with-maskmix-and",
            "published":"2023-04-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mixpro-data-augmentation-with-maskmix-and\/review\/?hl=108981"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":11210,
        "rank":613,
        "Model":"ResNeXt-101 (CutMix)",
        "mlmodel":{

        },
        "method_short":"ResNeXt-101 ",
        "method_details":"CutMix",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-05-13",
        "metrics":{
            "Top 1 Accuracy":"80.53%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.53,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":114316,
            "title":"CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features",
            "url":"\/paper\/cutmix-regularization-strategy-to-train",
            "published":"2019-05-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/cutmix-regularization-strategy-to-train\/review\/?hl=11210"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":6,
                "name":"ResNeXt",
                "color":"#86960b"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60734,
        "rank":614,
        "Model":"NASViT-A2",
        "mlmodel":{

        },
        "method_short":"NASViT-A2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-29",
        "metrics":{
            "Top 1 Accuracy":"80.5%",
            "Number of params":null,
            "GFLOPs":"0.421",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.5,
            "Number of params":null,
            "GFLOPs":0.421,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":882976,
            "title":"NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training",
            "url":"\/paper\/nasvit-neural-architecture-search-for",
            "published":"2021-09-29T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":21729,
        "rank":615,
        "Model":"Attention-92",
        "mlmodel":{

        },
        "method_short":"Attention-92",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-04-23",
        "metrics":{
            "Top 1 Accuracy":"80.5%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.5,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":23736,
            "title":"Residual Attention Network for Image Classification",
            "url":"\/paper\/residual-attention-network-for-image",
            "published":"2017-04-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/residual-attention-network-for-image\/review\/?hl=21729"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":16312,
        "rank":616,
        "Model":"NAT-M4",
        "mlmodel":{

        },
        "method_short":"NAT-M4",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-05-12",
        "metrics":{
            "Top 1 Accuracy":"80.5%",
            "Number of params":"9.1M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.5,
            "Number of params":9100000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":194717,
            "title":"Neural Architecture Transfer",
            "url":"\/paper\/neural-architecture-transfer",
            "published":"2020-05-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/neural-architecture-transfer\/review\/?hl=16312"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":86743,
        "rank":617,
        "Model":"IPT-T",
        "mlmodel":{

        },
        "method_short":"IPT-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-06",
        "metrics":{
            "Top 1 Accuracy":"80.5%",
            "Number of params":"14.0M",
            "GFLOPs":"2.3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.5,
            "Number of params":14000000.0,
            "GFLOPs":2.3,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1124238,
            "title":"IncepFormer: Efficient Inception Transformer with Pyramid Pooling for Semantic Segmentation",
            "url":"\/paper\/incepformer-efficient-inception-transformer",
            "published":"2022-12-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/incepformer-efficient-inception-transformer\/review\/?hl=86743"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":37537,
        "rank":618,
        "Model":"GLiT-Smalls",
        "mlmodel":{

        },
        "method_short":"GLiT-Smalls",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-07",
        "metrics":{
            "Top 1 Accuracy":"80.5%",
            "Number of params":"24.6M",
            "GFLOPs":"4.4",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.5,
            "Number of params":24600000.0,
            "GFLOPs":4.4,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":831816,
            "title":"GLiT: Neural Architecture Search for Global and Local Image Transformer",
            "url":"\/paper\/glit-neural-architecture-search-for-global",
            "published":"2021-07-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/glit-neural-architecture-search-for-global\/review\/?hl=37537"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":39448,
        "rank":619,
        "Model":"HCGNet-C",
        "mlmodel":{

        },
        "method_short":"HCGNet-C",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-26",
        "metrics":{
            "Top 1 Accuracy":"80.5%",
            "Number of params":"42.2M",
            "GFLOPs":"7.1",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.5,
            "Number of params":42200000.0,
            "GFLOPs":7.1,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":151373,
            "title":"Gated Convolutional Networks with Hybrid Connectivity for Image Classification",
            "url":"\/paper\/gated-convolutional-networks-with-hybrid",
            "published":"2019-08-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/gated-convolutional-networks-with-hybrid\/review\/?hl=39448"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":34135,
        "rank":620,
        "Model":"DVT (T2T-ViT-12)",
        "mlmodel":{

        },
        "method_short":"DVT ",
        "method_details":"T2T-ViT-12",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-31",
        "metrics":{
            "Top 1 Accuracy":"80.43%",
            "Number of params":null,
            "GFLOPs":"1.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.43,
            "Number of params":null,
            "GFLOPs":1.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":808027,
            "title":"Not All Images are Worth 16x16 Words: Dynamic Transformers for Efficient Image Recognition",
            "url":"\/paper\/not-all-images-are-worth-16x16-words-dynamic",
            "published":"2021-05-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/not-all-images-are-worth-16x16-words-dynamic\/review\/?hl=34135"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":48306,
        "rank":621,
        "Model":"UniNet-B1",
        "mlmodel":{

        },
        "method_short":"UniNet-B1",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-08",
        "metrics":{
            "Top 1 Accuracy":"80.4%",
            "Number of params":"14M",
            "GFLOPs":"0.99",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.4,
            "Number of params":14000000.0,
            "GFLOPs":0.99,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":881795,
            "title":"UniNet: Unified Architecture Search with Convolution, Transformer, and MLP",
            "url":"\/paper\/uninet-unified-architecture-search-with",
            "published":"2021-10-08T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/uninet-unified-architecture-search-with\/review\/?hl=48306"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":40187,
        "rank":622,
        "Model":"DeiT-S (T2)",
        "mlmodel":{

        },
        "method_short":"DeiT-S ",
        "method_details":"T2",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-01",
        "metrics":{
            "Top 1 Accuracy":"80.4%",
            "Number of params":"22M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.4,
            "Number of params":22000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":877343,
            "title":"ResNet strikes back: An improved training procedure in timm",
            "url":"\/paper\/resnet-strikes-back-an-improved-training",
            "published":"2021-10-01T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":40185,
        "rank":623,
        "Model":"ResNet50 (A1)",
        "mlmodel":{

        },
        "method_short":"ResNet50 ",
        "method_details":"A1",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-01",
        "metrics":{
            "Top 1 Accuracy":"80.4%",
            "Number of params":"25M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.4,
            "Number of params":25000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":877343,
            "title":"ResNet strikes back: An improved training procedure in timm",
            "url":"\/paper\/resnet-strikes-back-an-improved-training",
            "published":"2021-10-01T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":66003,
        "rank":624,
        "Model":"gSwin-VT",
        "mlmodel":{

        },
        "method_short":"gSwin-VT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-08-24",
        "metrics":{
            "Top 1 Accuracy":"80.32%",
            "Number of params":"15.5M",
            "GFLOPs":"2.3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.32,
            "Number of params":15500000.0,
            "GFLOPs":2.3,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1064636,
            "title":"gSwin: Gated MLP Vision Model with Hierarchical Structure of Shifted Window",
            "url":"\/paper\/gswin-gated-mlp-vision-model-with",
            "published":"2022-08-24T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/gswin-gated-mlp-vision-model-with\/review\/?hl=66003"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":48,
                "name":"Swin-Transformer",
                "color":"#f75c2f"
            },
            {
                "id":18,
                "name":"MLP",
                "color":"#ffae00"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":57966,
        "rank":625,
        "Model":"AlphaNet-A5",
        "mlmodel":{

        },
        "method_short":"AlphaNet-A5",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-16",
        "metrics":{
            "Top 1 Accuracy":"80.3%",
            "Number of params":null,
            "GFLOPs":"0.491",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.3,
            "Number of params":null,
            "GFLOPs":0.491,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":745792,
            "title":"AlphaNet: Improved Training of Supernets with Alpha-Divergence",
            "url":"\/paper\/alphanet-improved-training-of-supernet-with",
            "published":"2021-02-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/alphanet-improved-training-of-supernet-with\/review\/?hl=57966"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":24596,
        "rank":626,
        "Model":"ResNet-50+AutoDropout+RandAugment",
        "mlmodel":{

        },
        "method_short":"ResNet-50+AutoDropout+RandAugment",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-05",
        "metrics":{
            "Top 1 Accuracy":"80.3%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.3,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":734166,
            "title":"AutoDropout: Learning Dropout Patterns to Regularize Deep Networks",
            "url":"\/paper\/autodropout-learning-dropout-patterns-to",
            "published":"2021-01-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/autodropout-learning-dropout-patterns-to\/review\/?hl=24596"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":49042,
        "rank":627,
        "Model":"ReXNet_1.5",
        "mlmodel":{

        },
        "method_short":"ReXNet_1.5",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-07-02",
        "metrics":{
            "Top 1 Accuracy":"80.3%",
            "Number of params":"9.7M",
            "GFLOPs":"0.86",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.3,
            "Number of params":9700000.0,
            "GFLOPs":0.86,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":206716,
            "title":"Rethinking Channel Dimensions for Efficient Model Design",
            "url":"\/paper\/rexnet-diminishing-representational",
            "published":"2020-07-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rexnet-diminishing-representational\/review\/?hl=49042"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":36261,
        "rank":628,
        "Model":"CCT-16\/7x2",
        "mlmodel":{

        },
        "method_short":"CCT-16\/7x2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-12",
        "metrics":{
            "Top 1 Accuracy":"80.28%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.28,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":778781,
            "title":"Escaping the Big Data Paradigm with Compact Transformers",
            "url":"\/paper\/escaping-the-big-data-paradigm-with-compact",
            "published":"2021-04-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/escaping-the-big-data-paradigm-with-compact\/review\/?hl=36261"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":79149,
        "rank":629,
        "Model":"DeiT-S (PuzzleMix+DM)",
        "mlmodel":{

        },
        "method_short":"DeiT-S ",
        "method_details":"PuzzleMix+DM",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "Top 1 Accuracy":"80.25%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.25,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":20325,
        "rank":630,
        "Model":"iAFF-ResNeXt-50-32x4d",
        "mlmodel":{

        },
        "method_short":"iAFF-ResNeXt-50-32x4d",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-09-29",
        "metrics":{
            "Top 1 Accuracy":"80.22%",
            "Number of params":"34.7M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.22,
            "Number of params":34700000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":219902,
            "title":"Attentional Feature Fusion",
            "url":"\/paper\/attentional-feature-fusion",
            "published":"2020-09-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/attentional-feature-fusion\/review\/?hl=20325"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":6,
                "name":"ResNeXt",
                "color":"#86960b"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":112826,
        "rank":631,
        "Model":"UniRepLKNet-P",
        "mlmodel":{

        },
        "method_short":"UniRepLKNet-P",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-27",
        "metrics":{
            "Top 1 Accuracy":"80.2%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.2,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1327838,
            "title":"UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition",
            "url":"\/paper\/unireplknet-a-universal-perception-large",
            "published":"2023-11-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/unireplknet-a-universal-perception-large\/review\/?hl=112826"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":10372,
        "rank":632,
        "Model":"FixEfficientNet-B0",
        "mlmodel":{

        },
        "method_short":"FixEfficientNet-B0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-18",
        "metrics":{
            "Top 1 Accuracy":"80.2%",
            "Number of params":"5.3M",
            "GFLOPs":"1.60",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.2,
            "Number of params":5300000.0,
            "GFLOPs":1.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":187431,
            "title":"Fixing the train-test resolution discrepancy: FixEfficientNet",
            "url":"\/paper\/fixing-the-train-test-resolution-discrepancy-2",
            "published":"2020-03-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fixing-the-train-test-resolution-discrepancy-2\/review\/?hl=10372"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":98881,
        "rank":633,
        "Model":"DAFT-conv (16 heads)",
        "mlmodel":{

        },
        "method_short":"DAFT-conv ",
        "method_details":"16 heads",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-29",
        "metrics":{
            "Top 1 Accuracy":"80.2%",
            "Number of params":"20.3M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.2,
            "Number of params":20300000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":884638,
            "title":"A Dot Product Attention Free Transformer",
            "url":"\/paper\/a-dot-product-attention-free-transformer",
            "published":"2021-09-29T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":39229,
        "rank":634,
        "Model":"ConvMLP-L",
        "mlmodel":{

        },
        "method_short":"ConvMLP-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-09",
        "metrics":{
            "Top 1 Accuracy":"80.2%",
            "Number of params":"42.7M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.2,
            "Number of params":42700000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":864188,
            "title":"ConvMLP: Hierarchical Convolutional MLPs for Vision",
            "url":"\/paper\/convmlp-hierarchical-convolutional-mlps-for",
            "published":"2021-09-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/convmlp-hierarchical-convolutional-mlps-for\/review\/?hl=39229"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":18,
                "name":"MLP",
                "color":"#ffae00"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":44125,
        "rank":635,
        "Model":"ResNet-50 (224 res, Fast Knowledge Distillation)",
        "mlmodel":{

        },
        "method_short":"ResNet-50 ",
        "method_details":"224 res, Fast Knowledge Distillation",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-02",
        "metrics":{
            "Top 1 Accuracy":"80.1%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.1,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":924618,
            "title":"A Fast Knowledge Distillation Framework for Visual Recognition",
            "url":"\/paper\/a-fast-knowledge-distillation-framework-for",
            "published":"2021-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/a-fast-knowledge-distillation-framework-for\/review\/?hl=44125"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":98883,
        "rank":636,
        "Model":"DAFT-conv (384 heads, 200 epochs)",
        "mlmodel":{

        },
        "method_short":"DAFT-conv ",
        "method_details":"384 heads, 200 epochs",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-29",
        "metrics":{
            "Top 1 Accuracy":"80.1%",
            "Number of params":"23M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.1,
            "Number of params":23000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":884638,
            "title":"A Dot Product Attention Free Transformer",
            "url":"\/paper\/a-dot-product-attention-free-transformer",
            "published":"2021-09-29T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":2054,
        "rank":637,
        "Model":"Inception ResNet V2",
        "mlmodel":{

        },
        "method_short":"Inception ResNet V2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-02-23",
        "metrics":{
            "Top 1 Accuracy":"80.1%",
            "Number of params":"55.8M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.1,
            "Number of params":55800000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":31162,
            "title":"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",
            "url":"\/paper\/inception-v4-inception-resnet-and-the-impact",
            "published":"2016-02-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/inception-v4-inception-resnet-and-the-impact\/review\/?hl=2054"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":4576,
        "rank":638,
        "Model":"RandWire-WS",
        "mlmodel":{

        },
        "method_short":"RandWire-WS",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-02",
        "metrics":{
            "Top 1 Accuracy":"80.1%",
            "Number of params":"61.5M",
            "GFLOPs":"7.9",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.1,
            "Number of params":61500000.0,
            "GFLOPs":7.9,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":110191,
            "title":"Exploring Randomly Wired Neural Networks for Image Recognition",
            "url":"\/paper\/exploring-randomly-wired-neural-networks-for",
            "published":"2019-04-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/exploring-randomly-wired-neural-networks-for\/review\/?hl=4576"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":37529,
        "rank":639,
        "Model":"WideNet-H",
        "mlmodel":{

        },
        "method_short":"WideNet-H",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-25",
        "metrics":{
            "Top 1 Accuracy":"80.09%",
            "Number of params":"63M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.09,
            "Number of params":63000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":841171,
            "title":"Go Wider Instead of Deeper",
            "url":"\/paper\/go-wider-instead-of-deeper",
            "published":"2021-07-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/go-wider-instead-of-deeper\/review\/?hl=37529"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":38560,
        "rank":640,
        "Model":"CoE-Small + CondConv + PWLU",
        "mlmodel":{

        },
        "method_short":"CoE-Small + CondConv + PWLU",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-08",
        "metrics":{
            "Top 1 Accuracy":"80%",
            "Number of params":null,
            "GFLOPs":"0.100",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.0,
            "Number of params":null,
            "GFLOPs":0.1,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":832426,
            "title":"Collaboration of Experts: Achieving 80% Top-1 Accuracy on ImageNet with 100M FLOPs",
            "url":"\/paper\/collaboration-of-experts-achieving-80-top-1",
            "published":"2021-07-08T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/collaboration-of-experts-achieving-80-top-1\/review\/?hl=38560"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":61474,
        "rank":641,
        "Model":"BasisNet-MV3",
        "mlmodel":{

        },
        "method_short":"BasisNet-MV3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-07",
        "metrics":{
            "Top 1 Accuracy":"80%",
            "Number of params":null,
            "GFLOPs":"0.198",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.0,
            "Number of params":null,
            "GFLOPs":0.198,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":795440,
            "title":"BasisNet: Two-stage Model Synthesis for Efficient Inference",
            "url":"\/paper\/basisnet-two-stage-model-synthesis-for-1",
            "published":"2021-05-07T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/basisnet-two-stage-model-synthesis-for-1\/review\/?hl=61474"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":57965,
        "rank":642,
        "Model":"AlphaNet-A4",
        "mlmodel":{

        },
        "method_short":"AlphaNet-A4",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-16",
        "metrics":{
            "Top 1 Accuracy":"80.0%",
            "Number of params":null,
            "GFLOPs":"0.444",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.0,
            "Number of params":null,
            "GFLOPs":0.444,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":745792,
            "title":"AlphaNet: Improved Training of Supernets with Alpha-Divergence",
            "url":"\/paper\/alphanet-improved-training-of-supernet-with",
            "published":"2021-02-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/alphanet-improved-training-of-supernet-with\/review\/?hl=57965"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":78963,
        "rank":643,
        "Model":"MogaNet-T (256res)",
        "mlmodel":{

        },
        "method_short":"MogaNet-T ",
        "method_details":"256res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-07",
        "metrics":{
            "Top 1 Accuracy":"80%",
            "Number of params":"5.2M",
            "GFLOPs":"1.44",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.0,
            "Number of params":5200000.0,
            "GFLOPs":1.44,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1106694,
            "title":"Efficient Multi-order Gated Aggregation Network",
            "url":"\/paper\/efficient-multi-order-gated-aggregation",
            "published":"2022-11-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/efficient-multi-order-gated-aggregation\/review\/?hl=78963"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":105110,
        "rank":644,
        "Model":"MobileOne-S3 (distill)",
        "mlmodel":{

        },
        "method_short":"MobileOne-S3 ",
        "method_details":"distill",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-08",
        "metrics":{
            "Top 1 Accuracy":"80.0%",
            "Number of params":"10.1M",
            "GFLOPs":"1.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.0,
            "Number of params":10100000.0,
            "GFLOPs":1.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1023967,
            "title":"MobileOne: An Improved One millisecond Mobile Backbone",
            "url":"\/paper\/an-improved-one-millisecond-mobile-backbone",
            "published":"2022-06-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/an-improved-one-millisecond-mobile-backbone\/review\/?hl=105110"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29479,
        "rank":645,
        "Model":"LeViT-192",
        "mlmodel":{

        },
        "method_short":"LeViT-192",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-02",
        "metrics":{
            "Top 1 Accuracy":"80%",
            "Number of params":"10.4M",
            "GFLOPs":"0.624",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.0,
            "Number of params":10400000.0,
            "GFLOPs":0.624,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":774248,
            "title":"LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference",
            "url":"\/paper\/levit-a-vision-transformer-in-convnet-s",
            "published":"2021-04-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/levit-a-vision-transformer-in-convnet-s\/review\/?hl=29479"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":24933,
        "rank":646,
        "Model":"ResNet-101",
        "mlmodel":{

        },
        "method_short":"ResNet-101",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-27",
        "metrics":{
            "Top 1 Accuracy":"80%",
            "Number of params":"44.4M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.0,
            "Number of params":44400000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":739590,
            "title":"Bottleneck Transformers for Visual Recognition",
            "url":"\/paper\/bottleneck-transformers-for-visual",
            "published":"2021-01-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/bottleneck-transformers-for-visual\/review\/?hl=24933"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6135,
        "rank":647,
        "Model":"ResNet-200",
        "mlmodel":{

        },
        "method_short":"ResNet-200",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-03-16",
        "metrics":{
            "Top 1 Accuracy":"79.9%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.9,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":31943,
            "title":"Identity Mappings in Deep Residual Networks",
            "url":"\/paper\/identity-mappings-in-deep-residual-networks",
            "published":"2016-03-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/identity-mappings-in-deep-residual-networks\/review\/?hl=6135"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":10590,
        "rank":648,
        "Model":"RegNetY-8.0GF",
        "mlmodel":{

        },
        "method_short":"RegNetY-8.0GF",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-30",
        "metrics":{
            "Top 1 Accuracy":"79.9%",
            "Number of params":"39.2M",
            "GFLOPs":"8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.9,
            "Number of params":39200000.0,
            "GFLOPs":8.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":188625,
            "title":"Designing Network Design Spaces",
            "url":"\/paper\/designing-network-design-spaces",
            "published":"2020-03-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/designing-network-design-spaces\/review\/?hl=10590"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":34310,
        "rank":649,
        "Model":"ViT-B\/16-SAM",
        "mlmodel":{

        },
        "method_short":"ViT-B\/16-SAM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-03",
        "metrics":{
            "Top 1 Accuracy":"79.9%",
            "Number of params":"87M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.9,
            "Number of params":87000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":810994,
            "title":"When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations",
            "url":"\/paper\/when-vision-transformers-outperform-resnets",
            "published":"2021-06-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/when-vision-transformers-outperform-resnets\/review\/?hl=34310"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":71924,
        "rank":650,
        "Model":"TransBoost-ResNet101",
        "mlmodel":{

        },
        "method_short":"TransBoost-ResNet101",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-26",
        "metrics":{
            "Top 1 Accuracy":"79.86%",
            "Number of params":"44.55M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.86,
            "Number of params":44550000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1016638,
            "title":"TransBoost: Improving the Best ImageNet Performance using Deep Transduction",
            "url":"\/paper\/transboost-improving-the-best-imagenet",
            "published":"2022-05-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/transboost-improving-the-best-imagenet\/review\/?hl=71924"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":5139,
        "rank":651,
        "Model":"SKNet-101",
        "mlmodel":{

        },
        "method_short":"SKNet-101",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-03-15",
        "metrics":{
            "Top 1 Accuracy":"79.81%",
            "Number of params":"48.9M",
            "GFLOPs":"8.46",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.81,
            "Number of params":48900000.0,
            "GFLOPs":8.46,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":108643,
            "title":"Selective Kernel Networks",
            "url":"\/paper\/selective-kernel-networks",
            "published":"2019-03-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/selective-kernel-networks\/review\/?hl=5139"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":5976,
        "rank":652,
        "Model":"FixResNet-50 CutMix",
        "mlmodel":{

        },
        "method_short":"FixResNet-50 CutMix",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-14",
        "metrics":{
            "Top 1 Accuracy":"79.8%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.8,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":142997,
            "title":"Fixing the train-test resolution discrepancy",
            "url":"\/paper\/fixing-the-train-test-resolution-discrepancy",
            "published":"2019-06-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fixing-the-train-test-resolution-discrepancy\/review\/?hl=5976"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":9596,
        "rank":653,
        "Model":"CSPResNeXt-50 + Mish",
        "mlmodel":{

        },
        "method_short":"CSPResNeXt-50 + Mish",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-23",
        "metrics":{
            "Top 1 Accuracy":"79.8%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.8,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":151086,
            "title":"Mish: A Self Regularized Non-Monotonic Activation Function",
            "url":"\/paper\/mish-a-self-regularized-non-monotonic-neural",
            "published":"2019-08-23T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":99302,
        "rank":654,
        "Model":"kNN-CLIP",
        "mlmodel":{

        },
        "method_short":"kNN-CLIP",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-04-03",
        "metrics":{
            "Top 1 Accuracy":"79.8%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.8,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":988322,
            "title":"Revisiting a kNN-based Image Classification System with High-capacity Storage",
            "url":"\/paper\/revisiting-a-knn-based-image-classification",
            "published":"2022-04-03T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/revisiting-a-knn-based-image-classification\/review\/?hl=99302"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":397,
                "name":"Memory-Centric",
                "color":"#2771D3"
            },
            {
                "id":231,
                "name":"CLIP Pre-trained",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":101974,
        "rank":655,
        "Model":"CloFormer-XS",
        "mlmodel":{

        },
        "method_short":"CloFormer-XS",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-31",
        "metrics":{
            "Top 1 Accuracy":"79.8%",
            "Number of params":"7.2M",
            "GFLOPs":"1.1",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.8,
            "Number of params":7200000.0,
            "GFLOPs":1.1,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1184169,
            "title":"Rethinking Local Perception in Lightweight Vision Transformer",
            "url":"\/paper\/rethinking-local-perception-in-lightweight",
            "published":"2023-03-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-local-perception-in-lightweight\/review\/?hl=101974"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6185,
        "rank":656,
        "Model":"EfficientNet-B2",
        "mlmodel":{

        },
        "method_short":"EfficientNet-B2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-05-28",
        "metrics":{
            "Top 1 Accuracy":"79.8%",
            "Number of params":"9.2M",
            "GFLOPs":"1",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.8,
            "Number of params":9200000.0,
            "GFLOPs":1.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":117456,
            "title":"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
            "url":"\/paper\/efficientnet-rethinking-model-scaling-for",
            "published":"2019-05-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/efficientnet-rethinking-model-scaling-for\/review\/?hl=6185"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60739,
        "rank":657,
        "Model":"GC ViT-XXT",
        "mlmodel":{

        },
        "method_short":"GC ViT-XXT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-20",
        "metrics":{
            "Top 1 Accuracy":"79.8%",
            "Number of params":"12M",
            "GFLOPs":"2.1",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.8,
            "Number of params":12000000.0,
            "GFLOPs":2.1,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1029616,
            "title":"Global Context Vision Transformers",
            "url":"\/paper\/global-context-vision-transformers",
            "published":"2022-06-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/global-context-vision-transformers\/review\/?hl=60739"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":9800,
        "rank":658,
        "Model":"CSPResNeXt-50 (Mish+Aug)",
        "mlmodel":{

        },
        "method_short":"CSPResNeXt-50 ",
        "method_details":"Mish+Aug",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-27",
        "metrics":{
            "Top 1 Accuracy":"79.8%",
            "Number of params":"20.5M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.8,
            "Number of params":20500000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":174547,
            "title":"CSPNet: A New Backbone that can Enhance Learning Capability of CNN",
            "url":"\/paper\/cspnet-a-new-backbone-that-can-enhance",
            "published":"2019-11-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/cspnet-a-new-backbone-that-can-enhance\/review\/?hl=9800"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":98880,
        "rank":659,
        "Model":"DAFT-full",
        "mlmodel":{

        },
        "method_short":"DAFT-full",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-29",
        "metrics":{
            "Top 1 Accuracy":"79.8%",
            "Number of params":"22.6M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.8,
            "Number of params":22600000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":884638,
            "title":"A Dot Product Attention Free Transformer",
            "url":"\/paper\/a-dot-product-attention-free-transformer",
            "published":"2021-09-29T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":34134,
        "rank":660,
        "Model":"DVT (T2T-ViT-10)",
        "mlmodel":{

        },
        "method_short":"DVT ",
        "method_details":"T2T-ViT-10",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-31",
        "metrics":{
            "Top 1 Accuracy":"79.74%",
            "Number of params":null,
            "GFLOPs":"0.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.74,
            "Number of params":null,
            "GFLOPs":0.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":808027,
            "title":"Not All Images are Worth 16x16 Words: Dynamic Transformers for Efficient Image Recognition",
            "url":"\/paper\/not-all-images-are-worth-16x16-words-dynamic",
            "published":"2021-05-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/not-all-images-are-worth-16x16-words-dynamic\/review\/?hl=34134"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60733,
        "rank":661,
        "Model":"NASViT-A1",
        "mlmodel":{

        },
        "method_short":"NASViT-A1",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-29",
        "metrics":{
            "Top 1 Accuracy":"79.7%",
            "Number of params":null,
            "GFLOPs":"0.309",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.7,
            "Number of params":null,
            "GFLOPs":0.309,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":882976,
            "title":"NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training",
            "url":"\/paper\/nasvit-neural-architecture-search-for",
            "published":"2021-09-29T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":70350,
        "rank":662,
        "Model":"GPaCo (ResNet-50)",
        "mlmodel":{

        },
        "method_short":"GPaCo ",
        "method_details":"ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-26",
        "metrics":{
            "Top 1 Accuracy":"79.7%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.7,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1080045,
            "title":"Generalized Parametric Contrastive Learning",
            "url":"\/paper\/generalized-parametric-contrastive-learning",
            "published":"2022-09-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/generalized-parametric-contrastive-learning\/review\/?hl=70350"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":31818,
        "rank":663,
        "Model":"ResMLP-36",
        "mlmodel":{

        },
        "method_short":"ResMLP-36",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-07",
        "metrics":{
            "Top 1 Accuracy":"79.7%",
            "Number of params":"45M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.7,
            "Number of params":45000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":795412,
            "title":"ResMLP: Feedforward networks for image classification with data-efficient training",
            "url":"\/paper\/resmlp-feedforward-networks-for-image",
            "published":"2021-05-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/resmlp-feedforward-networks-for-image\/review\/?hl=31818"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":18,
                "name":"MLP",
                "color":"#ffae00"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":22083,
        "rank":664,
        "Model":"Grafit (ResNet-50)",
        "mlmodel":{

        },
        "method_short":"Grafit ",
        "method_details":"ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-11-25",
        "metrics":{
            "Top 1 Accuracy":"79.6%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.6,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":237887,
            "title":"Grafit: Learning fine-grained image representations with coarse labels",
            "url":"\/paper\/grafit-learning-fine-grained-image",
            "published":"2020-11-25T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/grafit-learning-fine-grained-image\/review\/?hl=22083"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29488,
        "rank":665,
        "Model":"LeViT-128",
        "mlmodel":{

        },
        "method_short":"LeViT-128",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-02",
        "metrics":{
            "Top 1 Accuracy":"79.6%",
            "Number of params":"8.8M",
            "GFLOPs":"0.376",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.6,
            "Number of params":8800000.0,
            "GFLOPs":0.376,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":774248,
            "title":"LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference",
            "url":"\/paper\/levit-a-vision-transformer-in-convnet-s",
            "published":"2021-04-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/levit-a-vision-transformer-in-convnet-s\/review\/?hl=29488"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":61464,
        "rank":666,
        "Model":"ResT-Small",
        "mlmodel":{

        },
        "method_short":"ResT-Small",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-28",
        "metrics":{
            "Top 1 Accuracy":"79.6%",
            "Number of params":"13.66M",
            "GFLOPs":"1.9",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.6,
            "Number of params":13660000.0,
            "GFLOPs":1.9,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":807341,
            "title":"ResT: An Efficient Transformer for Visual Recognition",
            "url":"\/paper\/rest-an-efficient-transformer-for-visual",
            "published":"2021-05-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rest-an-efficient-transformer-for-visual\/review\/?hl=61464"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":49039,
        "rank":667,
        "Model":"ReXNet_1.3",
        "mlmodel":{

        },
        "method_short":"ReXNet_1.3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-07-02",
        "metrics":{
            "Top 1 Accuracy":"79.5%",
            "Number of params":"7.6M",
            "GFLOPs":"0.66",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.5,
            "Number of params":7600000.0,
            "GFLOPs":0.66,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":206716,
            "title":"Rethinking Channel Dimensions for Efficient Model Design",
            "url":"\/paper\/rexnet-diminishing-representational",
            "published":"2020-07-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rexnet-diminishing-representational\/review\/?hl=49039"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":37528,
        "rank":668,
        "Model":"WideNet-L",
        "mlmodel":{

        },
        "method_short":"WideNet-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-25",
        "metrics":{
            "Top 1 Accuracy":"79.49%",
            "Number of params":"40M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.49,
            "Number of params":40000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":841171,
            "title":"Go Wider Instead of Deeper",
            "url":"\/paper\/go-wider-instead-of-deeper",
            "published":"2021-07-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/go-wider-instead-of-deeper\/review\/?hl=37528"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":45002,
        "rank":669,
        "Model":"ResNet-50 (SAMix)",
        "mlmodel":{

        },
        "method_short":"ResNet-50 ",
        "method_details":"SAMix",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-30",
        "metrics":{
            "Top 1 Accuracy":"79.41%",
            "Number of params":"25.6M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.41,
            "Number of params":25600000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":923048,
            "title":"Boosting Discriminative Visual Representation Learning with Scenario-Agnostic Mixup",
            "url":"\/paper\/boosting-discriminative-visual-representation",
            "published":"2021-11-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/boosting-discriminative-visual-representation\/review\/?hl=45002"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":57964,
        "rank":670,
        "Model":"AlphaNet-A3",
        "mlmodel":{

        },
        "method_short":"AlphaNet-A3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-16",
        "metrics":{
            "Top 1 Accuracy":"79.4%",
            "Number of params":null,
            "GFLOPs":"0.357",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.4,
            "Number of params":null,
            "GFLOPs":0.357,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":745792,
            "title":"AlphaNet: Improved Training of Supernets with Alpha-Divergence",
            "url":"\/paper\/alphanet-improved-training-of-supernet-with",
            "published":"2021-02-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/alphanet-improved-training-of-supernet-with\/review\/?hl=57964"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":4619,
        "rank":671,
        "Model":"MultiGrain R50-AA-500",
        "mlmodel":{

        },
        "method_short":"MultiGrain R50-AA-500",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-02-14",
        "metrics":{
            "Top 1 Accuracy":"79.4%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.4,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":105902,
            "title":"MultiGrain: a unified image embedding for classes and instances",
            "url":"\/paper\/multigrain-a-unified-image-embedding-for",
            "published":"2019-02-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multigrain-a-unified-image-embedding-for\/review\/?hl=4619"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":22143,
        "rank":672,
        "Model":"ResNet-50 (Adversarial Autoaugment)",
        "mlmodel":{

        },
        "method_short":"ResNet-50 ",
        "method_details":"Adversarial Autoaugment",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-12-24",
        "metrics":{
            "Top 1 Accuracy":"79.4%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.4,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":178148,
            "title":"Adversarial AutoAugment",
            "url":"\/paper\/adversarial-autoaugment-1",
            "published":"2019-12-24T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/adversarial-autoaugment-1\/review\/?hl=22143"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":31817,
        "rank":673,
        "Model":"ResMLP-24",
        "mlmodel":{

        },
        "method_short":"ResMLP-24",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-07",
        "metrics":{
            "Top 1 Accuracy":"79.4%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.4,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":795412,
            "title":"ResMLP: Feedforward networks for image classification with data-efficient training",
            "url":"\/paper\/resmlp-feedforward-networks-for-image",
            "published":"2021-05-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/resmlp-feedforward-networks-for-image\/review\/?hl=31817"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":57374,
        "rank":674,
        "Model":"EdgeNeXt-S",
        "mlmodel":{

        },
        "method_short":"EdgeNeXt-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-21",
        "metrics":{
            "Top 1 Accuracy":"79.4%",
            "Number of params":"5.6M",
            "GFLOPs":"2.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.4,
            "Number of params":5600000.0,
            "GFLOPs":2.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1029687,
            "title":"EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications",
            "url":"\/paper\/edgenext-efficiently-amalgamated-cnn",
            "published":"2022-06-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/edgenext-efficiently-amalgamated-cnn\/review\/?hl=57374"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":252,
                "name":"CrossCovarianceAttention",
                "color":"#2771D3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60596,
        "rank":675,
        "Model":"TinyNet (GhostNet-A)",
        "mlmodel":{

        },
        "method_short":"TinyNet ",
        "method_details":"GhostNet-A",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-10-28",
        "metrics":{
            "Top 1 Accuracy":"79.4%",
            "Number of params":"11.9M",
            "GFLOPs":"0.591",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.4,
            "Number of params":11900000.0,
            "GFLOPs":0.591,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":231372,
            "title":"Model Rubik's Cube: Twisting Resolution, Depth and Width for TinyNets",
            "url":"\/paper\/model-rubik-s-cube-twisting-resolution-depth",
            "published":"2020-10-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/model-rubik-s-cube-twisting-resolution-depth\/review\/?hl=60596"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":105102,
        "rank":676,
        "Model":"MobileOne-S4",
        "mlmodel":{

        },
        "method_short":"MobileOne-S4",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-08",
        "metrics":{
            "Top 1 Accuracy":"79.4%",
            "Number of params":"14.8M",
            "GFLOPs":"2.9",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.4,
            "Number of params":14800000.0,
            "GFLOPs":2.9,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1023967,
            "title":"MobileOne: An Improved One millisecond Mobile Backbone",
            "url":"\/paper\/an-improved-one-millisecond-mobile-backbone",
            "published":"2022-06-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/an-improved-one-millisecond-mobile-backbone\/review\/?hl=105102"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":10591,
        "rank":677,
        "Model":"RegNetY-4.0GF",
        "mlmodel":{

        },
        "method_short":"RegNetY-4.0GF",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-30",
        "metrics":{
            "Top 1 Accuracy":"79.4%",
            "Number of params":"20.6M",
            "GFLOPs":"4",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.4,
            "Number of params":20600000.0,
            "GFLOPs":4.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":188625,
            "title":"Designing Network Design Spaces",
            "url":"\/paper\/designing-network-design-spaces",
            "published":"2020-03-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/designing-network-design-spaces\/review\/?hl=10591"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":24932,
        "rank":678,
        "Model":"SENet-50",
        "mlmodel":{

        },
        "method_short":"SENet-50",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-27",
        "metrics":{
            "Top 1 Accuracy":"79.4%",
            "Number of params":"28.02M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.4,
            "Number of params":28020000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":739590,
            "title":"Bottleneck Transformers for Visual Recognition",
            "url":"\/paper\/bottleneck-transformers-for-visual",
            "published":"2021-01-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/bottleneck-transformers-for-visual\/review\/?hl=24932"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":5195,
        "rank":679,
        "Model":"ScaleNet-152",
        "mlmodel":{

        },
        "method_short":"ScaleNet-152",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-20",
        "metrics":{
            "Top 1 Accuracy":"79.38%",
            "Number of params":null,
            "GFLOPs":"11.2",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.38,
            "Number of params":null,
            "GFLOPs":11.2,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":112304,
            "title":"Data-Driven Neuron Allocation for Scale Aggregation Networks",
            "url":"\/paper\/190409460",
            "published":"2019-04-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/190409460\/review\/?hl=5195"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":11214,
        "rank":680,
        "Model":"LIP-ResNet-101",
        "mlmodel":{

        },
        "method_short":"LIP-ResNet-101",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-12",
        "metrics":{
            "Top 1 Accuracy":"79.33%",
            "Number of params":"42.9M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.33,
            "Number of params":42900000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":149671,
            "title":"LIP: Local Importance-based Pooling",
            "url":"\/paper\/lip-local-importance-based-pooling",
            "published":"2019-08-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lip-local-importance-based-pooling\/review\/?hl=11214"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":73070,
        "rank":681,
        "Model":"MobileViTv3-S",
        "mlmodel":{

        },
        "method_short":"MobileViTv3-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-30",
        "metrics":{
            "Top 1 Accuracy":"79.3%",
            "Number of params":"5.8M",
            "GFLOPs":"1.841",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.3,
            "Number of params":5800000.0,
            "GFLOPs":1.841,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1084483,
            "title":"MobileViTv3: Mobile-Friendly Vision Transformer with Simple and Effective Fusion of Local, Global and Input Features",
            "url":"\/paper\/mobilevitv3-mobile-friendly-vision",
            "published":"2022-09-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mobilevitv3-mobile-friendly-vision\/review\/?hl=73070"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":160,
                "name":"CNN+Transformer",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":27940,
        "rank":682,
        "Model":"RedNet-152",
        "mlmodel":{

        },
        "method_short":"RedNet-152",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-10",
        "metrics":{
            "Top 1 Accuracy":"79.3%",
            "Number of params":"34M",
            "GFLOPs":"6.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.3,
            "Number of params":34000000.0,
            "GFLOPs":6.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":752660,
            "title":"Involution: Inverting the Inherence of Convolution for Visual Recognition",
            "url":"\/paper\/involution-inverting-the-inherence-of",
            "published":"2021-03-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/involution-inverting-the-inherence-of\/review\/?hl=27940"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":44996,
        "rank":683,
        "Model":"ResNet-50 (AutoMix)",
        "mlmodel":{

        },
        "method_short":"ResNet-50 ",
        "method_details":"AutoMix",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-24",
        "metrics":{
            "Top 1 Accuracy":"79.25%",
            "Number of params":"25.6M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.25,
            "Number of params":25600000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":756879,
            "title":"AutoMix: Unveiling the Power of Mixup for Stronger Classifiers",
            "url":"\/paper\/automix-unveiling-the-power-of-mixup",
            "published":"2021-03-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/automix-unveiling-the-power-of-mixup\/review\/?hl=44996"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":40514,
        "rank":684,
        "Model":"PS-KD (ResNet-152 + CutMix)",
        "mlmodel":{

        },
        "method_short":"PS-KD ",
        "method_details":"ResNet-152 + CutMix",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-22",
        "metrics":{
            "Top 1 Accuracy":"79.24%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.24,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":204960,
            "title":"Self-Knowledge Distillation with Progressive Refinement of Targets",
            "url":"\/paper\/self-knowledge-distillation-a-simple-way-for",
            "published":"2020-06-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/self-knowledge-distillation-a-simple-way-for\/review\/?hl=40514"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":11215,
        "rank":685,
        "Model":"ResNet-101 (JFT-300M Finetuning)",
        "mlmodel":{

        },
        "method_short":"ResNet-101 ",
        "method_details":"JFT-300M Finetuning",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-07-10",
        "metrics":{
            "Top 1 Accuracy":"79.2%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.2,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":19305,
            "title":"Revisiting Unreasonable Effectiveness of Data in Deep Learning Era",
            "url":"\/paper\/revisiting-unreasonable-effectiveness-of-data",
            "published":"2017-07-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/revisiting-unreasonable-effectiveness-of-data\/review\/?hl=11215"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":33619,
        "rank":686,
        "Model":"RVT-Ti",
        "mlmodel":{

        },
        "method_short":"RVT-Ti*",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-17",
        "metrics":{
            "Top 1 Accuracy":"79.2%",
            "Number of params":"10.9M",
            "GFLOPs":"1.3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.2,
            "Number of params":10900000.0,
            "GFLOPs":1.3,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":800173,
            "title":"Towards Robust Vision Transformer",
            "url":"\/paper\/rethinking-the-design-principles-of-robust",
            "published":"2021-05-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-the-design-principles-of-robust\/review\/?hl=33619"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":17400,
        "rank":687,
        "Model":"Multiscale DEQ (MDEQ-XL)",
        "mlmodel":{

        },
        "method_short":"Multiscale DEQ ",
        "method_details":"MDEQ-XL",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-15",
        "metrics":{
            "Top 1 Accuracy":"79.2%",
            "Number of params":"81M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.2,
            "Number of params":81000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":202827,
            "title":"Multiscale Deep Equilibrium Models",
            "url":"\/paper\/multiscale-deep-equilibrium-models",
            "published":"2020-06-15T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":97060,
        "rank":688,
        "Model":"SimpleNetV1-5m-correct-labels",
        "mlmodel":{

        },
        "method_short":"SimpleNetV1-5m-correct-labels",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-08-22",
        "metrics":{
            "Top 1 Accuracy":"79.12",
            "Number of params":"5.7M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.12,
            "Number of params":5700000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":10271,
            "title":"Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures",
            "url":"\/paper\/lets-keep-it-simple-using-simple",
            "published":"2016-08-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lets-keep-it-simple-using-simple\/review\/?hl=97060"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":57963,
        "rank":689,
        "Model":"AlphaNet-A2",
        "mlmodel":{

        },
        "method_short":"AlphaNet-A2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-16",
        "metrics":{
            "Top 1 Accuracy":"79.1%",
            "Number of params":null,
            "GFLOPs":"0.317",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.1,
            "Number of params":null,
            "GFLOPs":0.317,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":745792,
            "title":"AlphaNet: Improved Training of Supernets with Alpha-Divergence",
            "url":"\/paper\/alphanet-improved-training-of-supernet-with",
            "published":"2021-02-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/alphanet-improved-training-of-supernet-with\/review\/?hl=57963"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":4812,
        "rank":690,
        "Model":"AA-ResNet-152",
        "mlmodel":{

        },
        "method_short":"AA-ResNet-152",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-22",
        "metrics":{
            "Top 1 Accuracy":"79.1%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.1,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":112308,
            "title":"Attention Augmented Convolutional Networks",
            "url":"\/paper\/190409925",
            "published":"2019-04-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/190409925\/review\/?hl=4812"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":5975,
        "rank":691,
        "Model":"FixResNet-50",
        "mlmodel":{

        },
        "method_short":"FixResNet-50",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-14",
        "metrics":{
            "Top 1 Accuracy":"79.1%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.1,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":142997,
            "title":"Fixing the train-test resolution discrepancy",
            "url":"\/paper\/fixing-the-train-test-resolution-discrepancy",
            "published":"2019-06-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fixing-the-train-test-resolution-discrepancy\/review\/?hl=5975"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":109076,
        "rank":692,
        "Model":"Diffusion Classifier",
        "mlmodel":{

        },
        "method_short":"Diffusion Classifier",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-28",
        "metrics":{
            "Top 1 Accuracy":"79.1%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.1,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1181860,
            "title":"Your Diffusion Model is Secretly a Zero-Shot Classifier",
            "url":"\/paper\/your-diffusion-model-is-secretly-a-zero-shot",
            "published":"2023-03-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/your-diffusion-model-is-secretly-a-zero-shot\/review\/?hl=109076"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60262,
        "rank":693,
        "Model":"TinyViT-5M",
        "mlmodel":{

        },
        "method_short":"TinyViT-5M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-21",
        "metrics":{
            "Top 1 Accuracy":"79.1%",
            "Number of params":"5.4M",
            "GFLOPs":"1.3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.1,
            "Number of params":5400000.0,
            "GFLOPs":1.3,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1047551,
            "title":"TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
            "url":"\/paper\/tinyvit-fast-pretraining-distillation-for",
            "published":"2022-07-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/tinyvit-fast-pretraining-distillation-for\/review\/?hl=60262"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":105109,
        "rank":694,
        "Model":"MobileOne-S2 (distill)",
        "mlmodel":{

        },
        "method_short":"MobileOne-S2 ",
        "method_details":"distill",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-08",
        "metrics":{
            "Top 1 Accuracy":"79.1%",
            "Number of params":"7.8M",
            "GFLOPs":"1.3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.1,
            "Number of params":7800000.0,
            "GFLOPs":1.3,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1023967,
            "title":"MobileOne: An Improved One millisecond Mobile Backbone",
            "url":"\/paper\/an-improved-one-millisecond-mobile-backbone",
            "published":"2022-06-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/an-improved-one-millisecond-mobile-backbone\/review\/?hl=105109"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29408,
        "rank":695,
        "Model":"PiT-XS",
        "mlmodel":{

        },
        "method_short":"PiT-XS",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-30",
        "metrics":{
            "Top 1 Accuracy":"79.1%",
            "Number of params":"10.6M",
            "GFLOPs":"1.4",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.1,
            "Number of params":10600000.0,
            "GFLOPs":1.4,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":771780,
            "title":"Rethinking Spatial Dimensions of Vision Transformers",
            "url":"\/paper\/rethinking-spatial-dimensions-of-vision",
            "published":"2021-03-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-spatial-dimensions-of-vision\/review\/?hl=29408"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":48305,
        "rank":696,
        "Model":"UniNet-B0",
        "mlmodel":{

        },
        "method_short":"UniNet-B0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-08",
        "metrics":{
            "Top 1 Accuracy":"79.1%",
            "Number of params":"11.9M",
            "GFLOPs":"0.56",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.1,
            "Number of params":11900000.0,
            "GFLOPs":0.56,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":881795,
            "title":"UniNet: Unified Architecture Search with Convolution, Transformer, and MLP",
            "url":"\/paper\/uninet-unified-architecture-search-with",
            "published":"2021-10-08T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/uninet-unified-architecture-search-with\/review\/?hl=48305"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":27939,
        "rank":697,
        "Model":"RedNet-101",
        "mlmodel":{

        },
        "method_short":"RedNet-101",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-10",
        "metrics":{
            "Top 1 Accuracy":"79.1%",
            "Number of params":"25.6M",
            "GFLOPs":"4.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.1,
            "Number of params":25600000.0,
            "GFLOPs":4.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":752660,
            "title":"Involution: Inverting the Inherence of Convolution for Visual Recognition",
            "url":"\/paper\/involution-inverting-the-inherence-of",
            "published":"2021-03-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/involution-inverting-the-inherence-of\/review\/?hl=27939"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":11216,
        "rank":698,
        "Model":"ResNet-50 (UDA)",
        "mlmodel":{

        },
        "method_short":"ResNet-50 ",
        "method_details":"UDA",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-29",
        "metrics":{
            "Top 1 Accuracy":"79.04%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.04,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":112981,
            "title":"Unsupervised Data Augmentation for Consistency Training",
            "url":"\/paper\/unsupervised-data-augmentation-1",
            "published":"2019-04-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/unsupervised-data-augmentation-1\/review\/?hl=11216"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":5196,
        "rank":699,
        "Model":"ScaleNet-101",
        "mlmodel":{

        },
        "method_short":"ScaleNet-101",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-20",
        "metrics":{
            "Top 1 Accuracy":"79.03%",
            "Number of params":null,
            "GFLOPs":"7.5",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.03,
            "Number of params":null,
            "GFLOPs":7.5,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":112304,
            "title":"Data-Driven Neuron Allocation for Scale Aggregation Networks",
            "url":"\/paper\/190409460",
            "published":"2019-04-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/190409460\/review\/?hl=5196"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":71923,
        "rank":700,
        "Model":"TransBoost-ResNet50",
        "mlmodel":{

        },
        "method_short":"TransBoost-ResNet50",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-26",
        "metrics":{
            "Top 1 Accuracy":"79.03%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.03,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1016638,
            "title":"TransBoost: Improving the Best ImageNet Performance using Deep Transduction",
            "url":"\/paper\/transboost-improving-the-best-imagenet",
            "published":"2022-05-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/transboost-improving-the-best-imagenet\/review\/?hl=71923"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":54152,
        "rank":701,
        "Model":"Co-ResNet-152",
        "mlmodel":{

        },
        "method_short":"Co-ResNet-152",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-08-17",
        "metrics":{
            "Top 1 Accuracy":"79.03%",
            "Number of params":"60M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.03,
            "Number of params":60000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":852792,
            "title":"Contextual Convolutional Neural Networks",
            "url":"\/paper\/contextual-convolutional-neural-networks",
            "published":"2021-08-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/contextual-convolutional-neural-networks\/review\/?hl=54152"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":18194,
        "rank":702,
        "Model":"MobileNetV3_large_x1_0_ssld",
        "mlmodel":{

        },
        "method_short":"MobileNetV3_large_x1_0_ssld",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-18",
        "metrics":{
            "Top 1 Accuracy":"79.0%",
            "Number of params":"5.47M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.0,
            "Number of params":5470000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":203194,
            "title":"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset",
            "url":"\/paper\/semi-supervised-recognition-under-a-noisy-and",
            "published":"2020-06-18T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":64202,
        "rank":703,
        "Model":"RevBiFPN-S2",
        "mlmodel":{

        },
        "method_short":"RevBiFPN-S2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-28",
        "metrics":{
            "Top 1 Accuracy":"79%",
            "Number of params":"10.6M",
            "GFLOPs":"1.37",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.0,
            "Number of params":10600000.0,
            "GFLOPs":1.37,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1034397,
            "title":"RevBiFPN: The Fully Reversible Bidirectional Feature Pyramid Network",
            "url":"\/paper\/revbifpn-the-fully-reversible-bidirectional",
            "published":"2022-06-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/revbifpn-the-fully-reversible-bidirectional\/review\/?hl=64202"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":283,
                "name":"Reversible",
                "color":"#3227d3"
            },
            {
                "id":13,
                "name":"FPN",
                "color":"#009481"
            },
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":39228,
        "rank":704,
        "Model":"ConvMLP-M",
        "mlmodel":{

        },
        "method_short":"ConvMLP-M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-09",
        "metrics":{
            "Top 1 Accuracy":"79%",
            "Number of params":"17.4M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.0,
            "Number of params":17400000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":864188,
            "title":"ConvMLP: Hierarchical Convolutional MLPs for Vision",
            "url":"\/paper\/convmlp-hierarchical-convolutional-mlps-for",
            "published":"2021-09-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/convmlp-hierarchical-convolutional-mlps-for\/review\/?hl=39228"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":2055,
        "rank":705,
        "Model":"Xception",
        "mlmodel":{

        },
        "method_short":"Xception",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-10-07",
        "metrics":{
            "Top 1 Accuracy":"79%",
            "Number of params":"22.855952M",
            "GFLOPs":null,
            "Hardware Burden":"87G",
            "Operations per network pass":"0.838G",
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.0,
            "Number of params":22855952.0,
            "GFLOPs":null,
            "Hardware Burden":87.0,
            "Operations per network pass":0.838,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":24504,
            "title":"Xception: Deep Learning with Depthwise Separable Convolutions",
            "url":"\/paper\/xception-deep-learning-with-depthwise",
            "published":"2016-10-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/xception-deep-learning-with-depthwise\/review\/?hl=2055"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":23972,
        "rank":706,
        "Model":"SpineNet-143",
        "mlmodel":{

        },
        "method_short":"SpineNet-143",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-12-10",
        "metrics":{
            "Top 1 Accuracy":"79%",
            "Number of params":"60.5M",
            "GFLOPs":"9.1",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.0,
            "Number of params":60500000.0,
            "GFLOPs":9.1,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":176141,
            "title":"SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization",
            "url":"\/paper\/spinenet-learning-scale-permuted-backbone-for",
            "published":"2019-12-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/spinenet-learning-scale-permuted-backbone-for\/review\/?hl=23972"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":34315,
        "rank":707,
        "Model":"Mixer-B\/8-SAM",
        "mlmodel":{

        },
        "method_short":"Mixer-B\/8-SAM",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-03",
        "metrics":{
            "Top 1 Accuracy":"79%",
            "Number of params":"64M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":79.0,
            "Number of params":64000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":810994,
            "title":"When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations",
            "url":"\/paper\/when-vision-transformers-outperform-resnets",
            "published":"2021-06-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/when-vision-transformers-outperform-resnets\/review\/?hl=34315"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":8971,
        "rank":708,
        "Model":"InceptionV3 (FRN layer)",
        "mlmodel":{

        },
        "method_short":"InceptionV3 ",
        "method_details":"FRN layer",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-21",
        "metrics":{
            "Top 1 Accuracy":"78.95%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.95,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":174026,
            "title":"Filter Response Normalization Layer: Eliminating Batch Dependence in the Training of Deep Neural Networks",
            "url":"\/paper\/filter-response-normalization-layer",
            "published":"2019-11-21T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":4166,
        "rank":709,
        "Model":"ResNet-152 + SWA",
        "mlmodel":{

        },
        "method_short":"ResNet-152 + SWA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-03-14",
        "metrics":{
            "Top 1 Accuracy":"78.94%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.94,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":8278,
            "title":"Averaging Weights Leads to Wider Optima and Better Generalization",
            "url":"\/paper\/averaging-weights-leads-to-wider-optima-and",
            "published":"2018-03-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/averaging-weights-leads-to-wider-optima-and\/review\/?hl=4166"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":8264,
        "rank":710,
        "Model":"ECA-Net (ResNet-152)",
        "mlmodel":{

        },
        "method_short":"ECA-Net ",
        "method_details":"ResNet-152",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-10-08",
        "metrics":{
            "Top 1 Accuracy":"78.92%",
            "Number of params":"57.40M",
            "GFLOPs":"10.83",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.92,
            "Number of params":57400000.0,
            "GFLOPs":10.83,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":157542,
            "title":"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks",
            "url":"\/paper\/eca-net-efficient-channel-attention-for-deep",
            "published":"2019-10-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/eca-net-efficient-channel-attention-for-deep\/review\/?hl=8264"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":57962,
        "rank":711,
        "Model":"AlphaNet-A1",
        "mlmodel":{

        },
        "method_short":"AlphaNet-A1",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-16",
        "metrics":{
            "Top 1 Accuracy":"78.9%",
            "Number of params":null,
            "GFLOPs":"0.279",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.9,
            "Number of params":null,
            "GFLOPs":0.279,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":745792,
            "title":"AlphaNet: Improved Training of Supernets with Alpha-Divergence",
            "url":"\/paper\/alphanet-improved-training-of-supernet-with",
            "published":"2021-02-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/alphanet-improved-training-of-supernet-with\/review\/?hl=57962"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6228,
        "rank":712,
        "Model":"MixNet-L",
        "mlmodel":{

        },
        "method_short":"MixNet-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-07-22",
        "metrics":{
            "Top 1 Accuracy":"78.9%",
            "Number of params":"7.3M",
            "GFLOPs":"0.565",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.9,
            "Number of params":7300000.0,
            "GFLOPs":0.565,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":146640,
            "title":"MixConv: Mixed Depthwise Convolutional Kernels",
            "url":"\/paper\/mixnet-mixed-depthwise-convolutional-kernels",
            "published":"2019-07-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mixnet-mixed-depthwise-convolutional-kernels\/review\/?hl=6228"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":28490,
        "rank":713,
        "Model":"CeiT-T (384 finetune res)",
        "mlmodel":{

        },
        "method_short":"CeiT-T ",
        "method_details":"384 finetune res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-22",
        "metrics":{
            "Top 1 Accuracy":"78.8%",
            "Number of params":null,
            "GFLOPs":"3.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.8,
            "Number of params":null,
            "GFLOPs":3.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755729,
            "title":"Incorporating Convolution Designs into Visual Transformers",
            "url":"\/paper\/incorporating-convolution-designs-into-visual",
            "published":"2021-03-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/incorporating-convolution-designs-into-visual\/review\/?hl=28490"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":11217,
        "rank":714,
        "Model":"NoisyStudent (EfficientNet-B0)",
        "mlmodel":{

        },
        "method_short":"NoisyStudent ",
        "method_details":"EfficientNet-B0",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-11",
        "metrics":{
            "Top 1 Accuracy":"78.8%",
            "Number of params":"5.3M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.8,
            "Number of params":5300000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":170047,
            "title":"Self-training with Noisy Student improves ImageNet classification",
            "url":"\/paper\/self-training-with-noisy-student-improves",
            "published":"2019-11-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/self-training-with-noisy-student-improves\/review\/?hl=11217"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":104,
                "name":"JFT-300M",
                "color":"#c4bc00"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6186,
        "rank":715,
        "Model":"EfficientNet-B1",
        "mlmodel":{

        },
        "method_short":"EfficientNet-B1",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-05-28",
        "metrics":{
            "Top 1 Accuracy":"78.8%",
            "Number of params":"7.8M",
            "GFLOPs":"0.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.8,
            "Number of params":7800000.0,
            "GFLOPs":0.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":117456,
            "title":"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
            "url":"\/paper\/efficientnet-rethinking-model-scaling-for",
            "published":"2019-05-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/efficientnet-rethinking-model-scaling-for\/review\/?hl=6186"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":24931,
        "rank":716,
        "Model":"ResNet-50",
        "mlmodel":{

        },
        "method_short":"ResNet-50",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-27",
        "metrics":{
            "Top 1 Accuracy":"78.8%",
            "Number of params":"25.5M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.8,
            "Number of params":25500000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":739590,
            "title":"Bottleneck Transformers for Visual Recognition",
            "url":"\/paper\/bottleneck-transformers-for-visual",
            "published":"2021-01-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/bottleneck-transformers-for-visual\/review\/?hl=24931"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":5137,
        "rank":717,
        "Model":"SGE-ResNet101",
        "mlmodel":{

        },
        "method_short":"SGE-ResNet101",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-05-23",
        "metrics":{
            "Top 1 Accuracy":"78.798%",
            "Number of params":"44.55M",
            "GFLOPs":"7.858",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.798,
            "Number of params":44550000.0,
            "GFLOPs":7.858,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":116805,
            "title":"Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks",
            "url":"\/paper\/spatial-group-wise-enhance-improving-semantic",
            "published":"2019-05-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/spatial-group-wise-enhance-improving-semantic\/review\/?hl=5137"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":24704,
        "rank":718,
        "Model":"RepVGG-B2",
        "mlmodel":{

        },
        "method_short":"RepVGG-B2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-11",
        "metrics":{
            "Top 1 Accuracy":"78.78%",
            "Number of params":"80.31M",
            "GFLOPs":"18.4",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.78,
            "Number of params":80310000.0,
            "GFLOPs":18.4,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":734898,
            "title":"RepVGG: Making VGG-style ConvNets Great Again",
            "url":"\/paper\/repvgg-making-vgg-style-convnets-great-again",
            "published":"2021-01-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/repvgg-making-vgg-style-convnets-great-again\/review\/?hl=24704"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":19216,
        "rank":719,
        "Model":"ResNet-50",
        "mlmodel":{

        },
        "method_short":"ResNet-50",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-09-15",
        "metrics":{
            "Top 1 Accuracy":"78.76%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.76,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":217984,
            "title":"Puzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup",
            "url":"\/paper\/puzzle-mix-exploiting-saliency-and-local-1",
            "published":"2020-09-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/puzzle-mix-exploiting-saliency-and-local-1\/review\/?hl=19216"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":79148,
        "rank":720,
        "Model":"SAMix+DM (ResNet-50 RSB A3)",
        "mlmodel":{

        },
        "method_short":"SAMix+DM ",
        "method_details":"ResNet-50 RSB A3",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "Top 1 Accuracy":"78.75%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.75,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":24593,
        "rank":721,
        "Model":"ResNet-50",
        "mlmodel":{

        },
        "method_short":"ResNet-50",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-05",
        "metrics":{
            "Top 1 Accuracy":"78.7%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.7,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":734166,
            "title":"AutoDropout: Learning Dropout Patterns to Regularize Deep Networks",
            "url":"\/paper\/autodropout-learning-dropout-patterns-to",
            "published":"2021-01-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/autodropout-learning-dropout-patterns-to\/review\/?hl=24593"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":44128,
        "rank":722,
        "Model":"SReT-LT (Fast Knowledge Distillation)",
        "mlmodel":{

        },
        "method_short":"SReT-LT ",
        "method_details":"Fast Knowledge Distillation",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-02",
        "metrics":{
            "Top 1 Accuracy":"78.7%",
            "Number of params":"5M",
            "GFLOPs":"1.2",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.7,
            "Number of params":5000000.0,
            "GFLOPs":1.2,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":924618,
            "title":"A Fast Knowledge Distillation Framework for Visual Recognition",
            "url":"\/paper\/a-fast-knowledge-distillation-framework-for",
            "published":"2021-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/a-fast-knowledge-distillation-framework-for\/review\/?hl=44128"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":36910,
        "rank":723,
        "Model":"PVTv2-B1",
        "mlmodel":{

        },
        "method_short":"PVTv2-B1",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-25",
        "metrics":{
            "Top 1 Accuracy":"78.7%",
            "Number of params":"13.1M",
            "GFLOPs":"2.1",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.7,
            "Number of params":13100000.0,
            "GFLOPs":2.1,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":825108,
            "title":"PVT v2: Improved Baselines with Pyramid Vision Transformer",
            "url":"\/paper\/pvtv2-improved-baselines-with-pyramid-vision",
            "published":"2021-06-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/pvtv2-improved-baselines-with-pyramid-vision\/review\/?hl=36910"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":8265,
        "rank":724,
        "Model":"ECA-Net (ResNet-101)",
        "mlmodel":{

        },
        "method_short":"ECA-Net ",
        "method_details":"ResNet-101",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-10-08",
        "metrics":{
            "Top 1 Accuracy":"78.65%",
            "Number of params":"42.49M",
            "GFLOPs":"7.35",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.65,
            "Number of params":42490000.0,
            "GFLOPs":7.35,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":157542,
            "title":"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks",
            "url":"\/paper\/eca-net-efficient-channel-attention-for-deep",
            "published":"2019-10-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/eca-net-efficient-channel-attention-for-deep\/review\/?hl=8265"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":73073,
        "rank":725,
        "Model":"MobileViTv3-1.0",
        "mlmodel":{

        },
        "method_short":"MobileViTv3-1.0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-30",
        "metrics":{
            "Top 1 Accuracy":"78.64%",
            "Number of params":null,
            "GFLOPs":"1.876",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.64,
            "Number of params":null,
            "GFLOPs":1.876,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1084483,
            "title":"MobileViTv3: Mobile-Friendly Vision Transformer with Simple and Effective Fusion of Local, Global and Input Features",
            "url":"\/paper\/mobilevitv3-mobile-friendly-vision",
            "published":"2022-09-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mobilevitv3-mobile-friendly-vision\/review\/?hl=73073"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":160,
                "name":"CNN+Transformer",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":49676,
        "rank":726,
        "Model":"EdgeFormer-S",
        "mlmodel":{

        },
        "method_short":"EdgeFormer-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-08",
        "metrics":{
            "Top 1 Accuracy":"78.63%",
            "Number of params":"5M",
            "GFLOPs":"3.48",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.63,
            "Number of params":5000000.0,
            "GFLOPs":3.48,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":973514,
            "title":"ParC-Net: Position Aware Circular Convolution with Merits from ConvNets and Transformer",
            "url":"\/paper\/edgeformer-improving-light-weight-convnets-by",
            "published":"2022-03-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/edgeformer-improving-light-weight-convnets-by\/review\/?hl=49676"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":79147,
        "rank":727,
        "Model":"AutoMix+DM (ResNet-50 RSB A3)",
        "mlmodel":{

        },
        "method_short":"AutoMix+DM ",
        "method_details":"ResNet-50 RSB A3",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "Top 1 Accuracy":"78.62%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.62,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":112825,
        "rank":728,
        "Model":"UniRepLKNet-F",
        "mlmodel":{

        },
        "method_short":"UniRepLKNet-F",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-27",
        "metrics":{
            "Top 1 Accuracy":"78.6%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.6,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1327838,
            "title":"UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition",
            "url":"\/paper\/unireplknet-a-universal-perception-large",
            "published":"2023-11-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/unireplknet-a-universal-perception-large\/review\/?hl=112825"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":71926,
        "rank":729,
        "Model":"TransBoost-EfficientNetB0",
        "mlmodel":{

        },
        "method_short":"TransBoost-EfficientNetB0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-26",
        "metrics":{
            "Top 1 Accuracy":"78.60%",
            "Number of params":"5.29M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.6,
            "Number of params":5290000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1016638,
            "title":"TransBoost: Improving the Best ImageNet Performance using Deep Transduction",
            "url":"\/paper\/transboost-improving-the-best-imagenet",
            "published":"2022-05-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/transboost-improving-the-best-imagenet\/review\/?hl=71926"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":30931,
        "rank":730,
        "Model":"Visformer-Ti",
        "mlmodel":{

        },
        "method_short":"Visformer-Ti",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-26",
        "metrics":{
            "Top 1 Accuracy":"78.6%",
            "Number of params":"10.3M",
            "GFLOPs":"1.3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.6,
            "Number of params":10300000.0,
            "GFLOPs":1.3,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":788363,
            "title":"Visformer: The Vision-friendly Transformer",
            "url":"\/paper\/visformer-the-vision-friendly-transformer",
            "published":"2021-04-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/visformer-the-vision-friendly-transformer\/review\/?hl=30931"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60764,
        "rank":731,
        "Model":"ResMLP-12 (distilled, class-MLP)",
        "mlmodel":{

        },
        "method_short":"ResMLP-12 ",
        "method_details":"distilled, class-MLP",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-07",
        "metrics":{
            "Top 1 Accuracy":"78.6%",
            "Number of params":"17.7M",
            "GFLOPs":"3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.6,
            "Number of params":17700000.0,
            "GFLOPs":3.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":795412,
            "title":"ResMLP: Feedforward networks for image classification with data-efficient training",
            "url":"\/paper\/resmlp-feedforward-networks-for-image",
            "published":"2021-05-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/resmlp-feedforward-networks-for-image\/review\/?hl=60764"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":31690,
        "rank":732,
        "Model":"RepMLP-Res50",
        "mlmodel":{

        },
        "method_short":"RepMLP-Res50",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-05",
        "metrics":{
            "Top 1 Accuracy":"78.60%",
            "Number of params":"52.77M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.6,
            "Number of params":52770000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":794048,
            "title":"RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition",
            "url":"\/paper\/repmlp-re-parameterizing-convolutions-into",
            "published":"2021-05-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/repmlp-re-parameterizing-convolutions-into\/review\/?hl=31690"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":11218,
        "rank":733,
        "Model":"Res2Net-50-299",
        "mlmodel":{

        },
        "method_short":"Res2Net-50-299",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-02",
        "metrics":{
            "Top 1 Accuracy":"78.59%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.59,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":110193,
            "title":"Res2Net: A New Multi-scale Backbone Architecture",
            "url":"\/paper\/res2net-a-new-multi-scale-backbone",
            "published":"2019-04-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/res2net-a-new-multi-scale-backbone\/review\/?hl=11218"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60765,
        "rank":734,
        "Model":"ResNet-152",
        "mlmodel":{

        },
        "method_short":"ResNet-152",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2015-12-10",
        "metrics":{
            "Top 1 Accuracy":"78.57%",
            "Number of params":null,
            "GFLOPs":"11.3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.57,
            "Number of params":null,
            "GFLOPs":11.3,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":37118,
            "title":"Deep Residual Learning for Image Recognition",
            "url":"\/paper\/deep-residual-learning-for-image-recognition",
            "published":"2015-12-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deep-residual-learning-for-image-recognition\/review\/?hl=60765"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":9707,
        "rank":735,
        "Model":"ResNet-50-DW (Deformable Kernels)",
        "mlmodel":{

        },
        "method_short":"ResNet-50-DW ",
        "method_details":"Deformable Kernels",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-10-07",
        "metrics":{
            "Top 1 Accuracy":"78.5%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.5,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":157224,
            "title":"Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation",
            "url":"\/paper\/deformable-kernels-adapting-effective",
            "published":"2019-10-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deformable-kernels-adapting-effective\/review\/?hl=9707"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":58455,
        "rank":736,
        "Model":"HRFormer-T",
        "mlmodel":{

        },
        "method_short":"HRFormer-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-18",
        "metrics":{
            "Top 1 Accuracy":"78.5%",
            "Number of params":"8.0M",
            "GFLOPs":"1.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.5,
            "Number of params":8000000.0,
            "GFLOPs":1.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":890071,
            "title":"HRFormer: High-Resolution Transformer for Dense Prediction",
            "url":"\/paper\/hrformer-high-resolution-transformer-for",
            "published":"2021-10-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/hrformer-high-resolution-transformer-for\/review\/?hl=58455"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":21626,
        "rank":737,
        "Model":"HCGNet-B",
        "mlmodel":{

        },
        "method_short":"HCGNet-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-26",
        "metrics":{
            "Top 1 Accuracy":"78.5%",
            "Number of params":"12.9M",
            "GFLOPs":"2.0",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.5,
            "Number of params":12900000.0,
            "GFLOPs":2.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":151373,
            "title":"Gated Convolutional Networks with Hybrid Connectivity for Image Classification",
            "url":"\/paper\/gated-convolutional-networks-with-hybrid",
            "published":"2019-08-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/gated-convolutional-networks-with-hybrid\/review\/?hl=21626"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":24703,
        "rank":738,
        "Model":"RepVGG-B2g4",
        "mlmodel":{

        },
        "method_short":"RepVGG-B2g4",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-11",
        "metrics":{
            "Top 1 Accuracy":"78.5%",
            "Number of params":"55.77M",
            "GFLOPs":"11.3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.5,
            "Number of params":55770000.0,
            "GFLOPs":11.3,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":734898,
            "title":"RepVGG: Making VGG-style ConvNets Great Again",
            "url":"\/paper\/repvgg-making-vgg-style-convnets-great-again",
            "published":"2021-01-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/repvgg-making-vgg-style-convnets-great-again\/review\/?hl=24703"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":34133,
        "rank":739,
        "Model":"DVT (T2T-ViT-7)",
        "mlmodel":{

        },
        "method_short":"DVT ",
        "method_details":"T2T-ViT-7",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-31",
        "metrics":{
            "Top 1 Accuracy":"78.48%",
            "Number of params":null,
            "GFLOPs":"0.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.48,
            "Number of params":null,
            "GFLOPs":0.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":808027,
            "title":"Not All Images are Worth 16x16 Words: Dynamic Transformers for Efficient Image Recognition",
            "url":"\/paper\/not-all-images-are-worth-16x16-words-dynamic",
            "published":"2021-05-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/not-all-images-are-worth-16x16-words-dynamic\/review\/?hl=34133"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":5103,
        "rank":740,
        "Model":"SRM-ResNet-101",
        "mlmodel":{

        },
        "method_short":"SRM-ResNet-101",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-03-26",
        "metrics":{
            "Top 1 Accuracy":"78.47%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.47,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":109531,
            "title":"SRM : A Style-based Recalibration Module for Convolutional Neural Networks",
            "url":"\/paper\/srm-a-style-based-recalibration-module-for",
            "published":"2019-03-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/srm-a-style-based-recalibration-module-for\/review\/?hl=5103"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":4164,
        "rank":741,
        "Model":"DenseNet-161 + SWA",
        "mlmodel":{

        },
        "method_short":"DenseNet-161 + SWA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-03-14",
        "metrics":{
            "Top 1 Accuracy":"78.44%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.44,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":8278,
            "title":"Averaging Weights Leads to Wider Optima and Better Generalization",
            "url":"\/paper\/averaging-weights-leads-to-wider-optima-and",
            "published":"2018-03-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/averaging-weights-leads-to-wider-optima-and\/review\/?hl=4164"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[
            {
                "url":"https:\/\/github.com\/facebookresearch\/AugLy\/blob\/main\/examples\/imagenet\/evaluate_robustness_imagenet_densenet161.ipynb"
            }
        ]
    },
    {
        "table_id":116,
        "row_id":108037,
        "rank":742,
        "Model":"CoaT-Ti",
        "mlmodel":{

        },
        "method_short":"CoaT-Ti",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-18",
        "metrics":{
            "Top 1 Accuracy":"78.42%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.42,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1265182,
            "title":"Which Transformer to Favor: A Comparative Analysis of Efficiency in Vision Transformers",
            "url":"\/paper\/which-transformer-to-favor-a-comparative",
            "published":"2023-08-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/which-transformer-to-favor-a-comparative\/review\/?hl=108037"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60745,
        "rank":743,
        "Model":"FBNetV5-AC-CLS",
        "mlmodel":{

        },
        "method_short":"FBNetV5-AC-CLS",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-19",
        "metrics":{
            "Top 1 Accuracy":"78.4%",
            "Number of params":null,
            "GFLOPs":"0.280",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.4,
            "Number of params":null,
            "GFLOPs":0.28,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":912986,
            "title":"FBNetV5: Neural Architecture Search for Multiple Tasks in One Run",
            "url":"\/paper\/fbnetv5-neural-architecture-search-for",
            "published":"2021-11-19T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/fbnetv5-neural-architecture-search-for\/review\/?hl=60745"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":5191,
        "rank":744,
        "Model":"ResNet-50 (CutMix)",
        "mlmodel":{

        },
        "method_short":"ResNet-50 ",
        "method_details":"CutMix",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-05-13",
        "metrics":{
            "Top 1 Accuracy":"78.4%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.4,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":114316,
            "title":"CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features",
            "url":"\/paper\/cutmix-regularization-strategy-to-train",
            "published":"2019-05-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/cutmix-regularization-strategy-to-train\/review\/?hl=5191"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":49040,
        "rank":745,
        "Model":"ReXNet_1.0-relabel",
        "mlmodel":{

        },
        "method_short":"ReXNet_1.0-relabel",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-13",
        "metrics":{
            "Top 1 Accuracy":"78.4%",
            "Number of params":"4.8M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.4,
            "Number of params":4800000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":735765,
            "title":"Re-labeling ImageNet: from Single to Multi-Labels, from Global to Localized Labels",
            "url":"\/paper\/re-labeling-imagenet-from-single-to-multi",
            "published":"2021-01-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/re-labeling-imagenet-from-single-to-multi\/review\/?hl=49040"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":45281,
        "rank":746,
        "Model":"MobileViT-S",
        "mlmodel":{

        },
        "method_short":"MobileViT-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-05",
        "metrics":{
            "Top 1 Accuracy":"78.4%",
            "Number of params":"5.6M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.4,
            "Number of params":5600000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":879564,
            "title":"MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer",
            "url":"\/paper\/mobilevit-light-weight-general-purpose-and",
            "published":"2021-10-05T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":27938,
        "rank":747,
        "Model":"RedNet-50",
        "mlmodel":{

        },
        "method_short":"RedNet-50",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-10",
        "metrics":{
            "Top 1 Accuracy":"78.4%",
            "Number of params":"15.5M",
            "GFLOPs":"2.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.4,
            "Number of params":15500000.0,
            "GFLOPs":2.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":752660,
            "title":"Involution: Inverting the Inherence of Convolution for Visual Recognition",
            "url":"\/paper\/involution-inverting-the-inherence-of",
            "published":"2021-03-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/involution-inverting-the-inherence-of\/review\/?hl=27938"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":79145,
        "rank":748,
        "Model":"ResNet-50 (SAMix+DM)",
        "mlmodel":{

        },
        "method_short":"ResNet-50 ",
        "method_details":"SAMix+DM",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "Top 1 Accuracy":"78.36%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.36,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":30496,
        "rank":749,
        "Model":"ResNet-50 + DropBlock (0.9 kp, 0.1 label smoothing)",
        "mlmodel":{

        },
        "method_short":"ResNet-50 + DropBlock ",
        "method_details":"0.9 kp, 0.1 label smoothing",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-10-30",
        "metrics":{
            "Top 1 Accuracy":"78.35%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.35,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":60685,
            "title":"DropBlock: A regularization method for convolutional networks",
            "url":"\/paper\/dropblock-a-regularization-method-for",
            "published":"2018-10-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dropblock-a-regularization-method-for\/review\/?hl=30496"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":108034,
        "rank":750,
        "Model":"Poly-SA-ViT-S",
        "mlmodel":{

        },
        "method_short":"Poly-SA-ViT-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-18",
        "metrics":{
            "Top 1 Accuracy":"78.34%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.34,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1265182,
            "title":"Which Transformer to Favor: A Comparative Analysis of Efficiency in Vision Transformers",
            "url":"\/paper\/which-transformer-to-favor-a-comparative",
            "published":"2023-08-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/which-transformer-to-favor-a-comparative\/review\/?hl=108034"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":9302,
        "rank":751,
        "Model":"EfficientNet-B0 (CondConv)",
        "mlmodel":{

        },
        "method_short":"EfficientNet-B0 ",
        "method_details":"CondConv",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-10",
        "metrics":{
            "Top 1 Accuracy":"78.3%",
            "Number of params":null,
            "GFLOPs":"0.826",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.3,
            "Number of params":null,
            "GFLOPs":0.826,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":111267,
            "title":"CondConv: Conditionally Parameterized Convolutions for Efficient Inference",
            "url":"\/paper\/soft-conditional-computation",
            "published":"2019-04-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/soft-conditional-computation\/review\/?hl=9302"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6137,
        "rank":752,
        "Model":"ResNet-101",
        "mlmodel":{

        },
        "method_short":"ResNet-101",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2015-12-10",
        "metrics":{
            "Top 1 Accuracy":"78.25%",
            "Number of params":"40M",
            "GFLOPs":"7.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.25,
            "Number of params":40000000.0,
            "GFLOPs":7.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":37118,
            "title":"Deep Residual Learning for Image Recognition",
            "url":"\/paper\/deep-residual-learning-for-image-recognition",
            "published":"2015-12-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deep-residual-learning-for-image-recognition\/review\/?hl=6137"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60732,
        "rank":753,
        "Model":"NASViT-A0",
        "mlmodel":{

        },
        "method_short":"NASViT-A0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-29",
        "metrics":{
            "Top 1 Accuracy":"78.2%",
            "Number of params":null,
            "GFLOPs":"0.208",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.2,
            "Number of params":null,
            "GFLOPs":0.208,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":882976,
            "title":"NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training",
            "url":"\/paper\/nasvit-neural-architecture-search-for",
            "published":"2021-09-29T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":4618,
        "rank":754,
        "Model":"MultiGrain R50-AA-224",
        "mlmodel":{

        },
        "method_short":"MultiGrain R50-AA-224",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-02-14",
        "metrics":{
            "Top 1 Accuracy":"78.2%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.2,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":105902,
            "title":"MultiGrain: a unified image embedding for classes and instances",
            "url":"\/paper\/multigrain-a-unified-image-embedding-for",
            "published":"2019-02-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multigrain-a-unified-image-embedding-for\/review\/?hl=4618"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":77454,
        "rank":755,
        "Model":"Pyramid ViG-Ti",
        "mlmodel":{

        },
        "method_short":"Pyramid ViG-Ti",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-01",
        "metrics":{
            "Top 1 Accuracy":"78.2%",
            "Number of params":"10.7M",
            "GFLOPs":"1.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.2,
            "Number of params":10700000.0,
            "GFLOPs":1.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1019895,
            "title":"Vision GNN: An Image is Worth Graph of Nodes",
            "url":"\/paper\/vision-gnn-an-image-is-worth-graph-of-nodes",
            "published":"2022-06-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vision-gnn-an-image-is-worth-graph-of-nodes\/review\/?hl=77454"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29954,
        "rank":756,
        "Model":"LocalViT-PVT",
        "mlmodel":{

        },
        "method_short":"LocalViT-PVT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-12",
        "metrics":{
            "Top 1 Accuracy":"78.2%",
            "Number of params":"13.5M",
            "GFLOPs":"4.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.2,
            "Number of params":13500000.0,
            "GFLOPs":4.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":778779,
            "title":"LocalViT: Bringing Locality to Vision Transformers",
            "url":"\/paper\/localvit-bringing-locality-to-vision",
            "published":"2021-04-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/localvit-bringing-locality-to-vision\/review\/?hl=29954"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":79144,
        "rank":757,
        "Model":"ResNet-50 (AutoMix+DM)",
        "mlmodel":{

        },
        "method_short":"ResNet-50 ",
        "method_details":"AutoMix+DM",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "Top 1 Accuracy":"78.15%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.15,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":79146,
        "rank":758,
        "Model":"PuzzleMix+DM (ResNet-50 RSB A3)",
        "mlmodel":{

        },
        "method_short":"PuzzleMix+DM ",
        "method_details":"ResNet-50 RSB A3",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "Top 1 Accuracy":"78.15%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.15,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":11220,
        "rank":759,
        "Model":"ResNet-50 (LIP Bottleneck-256)",
        "mlmodel":{

        },
        "method_short":"ResNet-50 ",
        "method_details":"LIP Bottleneck-256",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-12",
        "metrics":{
            "Top 1 Accuracy":"78.15%",
            "Number of params":"25.8M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.15,
            "Number of params":25800000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":149671,
            "title":"LIP: Local Importance-based Pooling",
            "url":"\/paper\/lip-local-importance-based-pooling",
            "published":"2019-08-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lip-local-importance-based-pooling\/review\/?hl=11220"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6134,
        "rank":760,
        "Model":"WRN-50-2-bottleneck",
        "mlmodel":{

        },
        "method_short":"WRN-50-2-bottleneck",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-05-23",
        "metrics":{
            "Top 1 Accuracy":"78.1%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.1,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":21610,
            "title":"Wide Residual Networks",
            "url":"\/paper\/wide-residual-networks",
            "published":"2016-05-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/wide-residual-networks\/review\/?hl=6134"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":105103,
        "rank":761,
        "Model":"MobileOne-S3",
        "mlmodel":{

        },
        "method_short":"MobileOne-S3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-08",
        "metrics":{
            "Top 1 Accuracy":"78.1%",
            "Number of params":"10.1M",
            "GFLOPs":"1.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.1,
            "Number of params":10100000.0,
            "GFLOPs":1.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1023967,
            "title":"MobileOne: An Improved One millisecond Mobile Backbone",
            "url":"\/paper\/an-improved-one-millisecond-mobile-backbone",
            "published":"2022-06-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/an-improved-one-millisecond-mobile-backbone\/review\/?hl=105103"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":40186,
        "rank":762,
        "Model":"ResNet50 (A3)",
        "mlmodel":{

        },
        "method_short":"ResNet50 ",
        "method_details":"A3",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-01",
        "metrics":{
            "Top 1 Accuracy":"78.1%",
            "Number of params":"25M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.1,
            "Number of params":25000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":877343,
            "title":"ResNet strikes back: An improved training procedure in timm",
            "url":"\/paper\/resnet-strikes-back-an-improved-training",
            "published":"2021-10-01T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":57960,
        "rank":763,
        "Model":"ZenNet-400M-SE",
        "mlmodel":{

        },
        "method_short":"ZenNet-400M-SE",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-01",
        "metrics":{
            "Top 1 Accuracy":"78%",
            "Number of params":"5.7M",
            "GFLOPs":"0.820",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.0,
            "Number of params":5700000.0,
            "GFLOPs":0.82,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":740994,
            "title":"Zen-NAS: A Zero-Shot NAS for High-Performance Deep Image Recognition",
            "url":"\/paper\/zen-nas-a-zero-shot-nas-for-high-performance",
            "published":"2021-02-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/zen-nas-a-zero-shot-nas-for-high-performance\/review\/?hl=57960"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":10592,
        "rank":764,
        "Model":"RegNetY-1.6GF",
        "mlmodel":{

        },
        "method_short":"RegNetY-1.6GF",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-30",
        "metrics":{
            "Top 1 Accuracy":"78%",
            "Number of params":"11.2M",
            "GFLOPs":"1.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.0,
            "Number of params":11200000.0,
            "GFLOPs":1.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":188625,
            "title":"Designing Network Design Spaces",
            "url":"\/paper\/designing-network-design-spaces",
            "published":"2020-03-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/designing-network-design-spaces\/review\/?hl=10592"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":28357,
        "rank":765,
        "Model":"HVT-S-1",
        "mlmodel":{

        },
        "method_short":"HVT-S-1",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-19",
        "metrics":{
            "Top 1 Accuracy":"78.00%",
            "Number of params":"21.74M",
            "GFLOPs":"2.4",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.0,
            "Number of params":21740000.0,
            "GFLOPs":2.4,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755458,
            "title":"Scalable Vision Transformers with Hierarchical Pooling",
            "url":"\/paper\/scalable-visual-transformers-with",
            "published":"2021-03-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scalable-visual-transformers-with\/review\/?hl=28357"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60626,
        "rank":766,
        "Model":"Perceiver (FF)",
        "mlmodel":{

        },
        "method_short":"Perceiver ",
        "method_details":"FF",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-04",
        "metrics":{
            "Top 1 Accuracy":"78%",
            "Number of params":"44.9M",
            "GFLOPs":"707.2",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.0,
            "Number of params":44900000.0,
            "GFLOPs":707.2,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":750724,
            "title":"Perceiver: General Perception with Iterative Attention",
            "url":"\/paper\/perceiver-general-perception-with-iterative",
            "published":"2021-03-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/perceiver-general-perception-with-iterative\/review\/?hl=60626"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":49035,
        "rank":767,
        "Model":"ReXNet_1.0",
        "mlmodel":{

        },
        "method_short":"ReXNet_1.0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-07-02",
        "metrics":{
            "Top 1 Accuracy":"77.9%",
            "Number of params":"4.8M",
            "GFLOPs":"0.40",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.9,
            "Number of params":4800000.0,
            "GFLOPs":0.4,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":206716,
            "title":"Rethinking Channel Dimensions for Efficient Model Design",
            "url":"\/paper\/rexnet-diminishing-representational",
            "published":"2020-07-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rexnet-diminishing-representational\/review\/?hl=49035"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":37,
                "name":"",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":35598,
        "rank":768,
        "Model":"ViTAE-6M",
        "mlmodel":{

        },
        "method_short":"ViTAE-6M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-07",
        "metrics":{
            "Top 1 Accuracy":"77.9%",
            "Number of params":"6.5M",
            "GFLOPs":"4",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.9,
            "Number of params":6500000.0,
            "GFLOPs":4.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":812332,
            "title":"ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias",
            "url":"\/paper\/vitae-vision-transformer-advanced-by",
            "published":"2021-06-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vitae-vision-transformer-advanced-by\/review\/?hl=35598"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6153,
        "rank":769,
        "Model":"DenseNet-264",
        "mlmodel":{

        },
        "method_short":"DenseNet-264",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-08-25",
        "metrics":{
            "Top 1 Accuracy":"77.85%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.85,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":11275,
            "title":"Densely Connected Convolutional Networks",
            "url":"\/paper\/densely-connected-convolutional-networks",
            "published":"2016-08-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/densely-connected-convolutional-networks\/review\/?hl=6153"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":57961,
        "rank":770,
        "Model":"AlphaNet-A0",
        "mlmodel":{

        },
        "method_short":"AlphaNet-A0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-16",
        "metrics":{
            "Top 1 Accuracy":"77.8%",
            "Number of params":null,
            "GFLOPs":"0.203",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.8,
            "Number of params":null,
            "GFLOPs":0.203,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":745792,
            "title":"AlphaNet: Improved Training of Supernets with Alpha-Divergence",
            "url":"\/paper\/alphanet-improved-training-of-supernet-with",
            "published":"2021-02-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/alphanet-improved-training-of-supernet-with\/review\/?hl=57961"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60600,
        "rank":771,
        "Model":"ScaleNet-50",
        "mlmodel":{

        },
        "method_short":"ScaleNet-50",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-20",
        "metrics":{
            "Top 1 Accuracy":"77.8%",
            "Number of params":null,
            "GFLOPs":"3.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.8,
            "Number of params":null,
            "GFLOPs":3.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":112304,
            "title":"Data-Driven Neuron Allocation for Scale Aggregation Networks",
            "url":"\/paper\/190409460",
            "published":"2019-04-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/190409460\/review\/?hl=60600"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60641,
        "rank":772,
        "Model":"ResMLP-S12",
        "mlmodel":{

        },
        "method_short":"ResMLP-S12",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-07",
        "metrics":{
            "Top 1 Accuracy":"77.8%",
            "Number of params":"15.4M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.8,
            "Number of params":15400000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":795412,
            "title":"ResMLP: Feedforward networks for image classification with data-efficient training",
            "url":"\/paper\/resmlp-feedforward-networks-for-image",
            "published":"2021-05-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/resmlp-feedforward-networks-for-image\/review\/?hl=60641"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":79143,
        "rank":773,
        "Model":"ResNet-50 (PuzzleMix+DM)",
        "mlmodel":{

        },
        "method_short":"ResNet-50 ",
        "method_details":"PuzzleMix+DM",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "Top 1 Accuracy":"77.71%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.71,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60595,
        "rank":774,
        "Model":"TinyNet-A + RA",
        "mlmodel":{

        },
        "method_short":"TinyNet-A + RA",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-10-28",
        "metrics":{
            "Top 1 Accuracy":"77.7%",
            "Number of params":"5.1M",
            "GFLOPs":"0.339",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.7,
            "Number of params":5100000.0,
            "GFLOPs":0.339,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":231372,
            "title":"Model Rubik's Cube: Twisting Resolution, Depth and Width for TinyNets",
            "url":"\/paper\/model-rubik-s-cube-twisting-resolution-depth",
            "published":"2020-10-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/model-rubik-s-cube-twisting-resolution-depth\/review\/?hl=60595"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":9310,
        "rank":775,
        "Model":"ResNet-50 (Fast AA)",
        "mlmodel":{

        },
        "method_short":"ResNet-50 ",
        "method_details":"Fast AA",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-05-01",
        "metrics":{
            "Top 1 Accuracy":"77.6%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.6,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":113336,
            "title":"Fast AutoAugment",
            "url":"\/paper\/fast-autoaugment",
            "published":"2019-05-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fast-autoaugment\/review\/?hl=9310"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60636,
        "rank":776,
        "Model":"SReT-T",
        "mlmodel":{

        },
        "method_short":"SReT-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-09",
        "metrics":{
            "Top 1 Accuracy":"77.6%",
            "Number of params":"4.8M",
            "GFLOPs":"1.1",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.6,
            "Number of params":4800000.0,
            "GFLOPs":1.1,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":907412,
            "title":"Sliced Recursive Transformer",
            "url":"\/paper\/sliced-recursive-transformer-1",
            "published":"2021-11-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/sliced-recursive-transformer-1\/review\/?hl=60636"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":27937,
        "rank":777,
        "Model":"RedNet-38",
        "mlmodel":{

        },
        "method_short":"RedNet-38",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-10",
        "metrics":{
            "Top 1 Accuracy":"77.6%",
            "Number of params":"12.4M",
            "GFLOPs":"2.2",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.6,
            "Number of params":12400000.0,
            "GFLOPs":2.2,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":752660,
            "title":"Involution: Inverting the Inherence of Convolution for Visual Recognition",
            "url":"\/paper\/involution-inverting-the-inherence-of",
            "published":"2021-03-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/involution-inverting-the-inherence-of\/review\/?hl=27937"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60598,
        "rank":778,
        "Model":"SGE-ResNet50",
        "mlmodel":{

        },
        "method_short":"SGE-ResNet50",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-05-23",
        "metrics":{
            "Top 1 Accuracy":"77.584%",
            "Number of params":"25.56M",
            "GFLOPs":"4.127",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.584,
            "Number of params":25560000.0,
            "GFLOPs":4.127,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":116805,
            "title":"Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks",
            "url":"\/paper\/spatial-group-wise-enhance-improving-semantic",
            "published":"2019-05-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/spatial-group-wise-enhance-improving-semantic\/review\/?hl=60598"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":37527,
        "rank":779,
        "Model":"WideNet-B",
        "mlmodel":{

        },
        "method_short":"WideNet-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-25",
        "metrics":{
            "Top 1 Accuracy":"77.54%",
            "Number of params":"29M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.54,
            "Number of params":29000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":841171,
            "title":"Go Wider Instead of Deeper",
            "url":"\/paper\/go-wider-instead-of-deeper",
            "published":"2021-07-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/go-wider-instead-of-deeper\/review\/?hl=37527"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":24594,
        "rank":780,
        "Model":"EfficientNet-B0",
        "mlmodel":{

        },
        "method_short":"EfficientNet-B0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-05",
        "metrics":{
            "Top 1 Accuracy":"77.5%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.5,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":734166,
            "title":"AutoDropout: Learning Dropout Patterns to Regularize Deep Networks",
            "url":"\/paper\/autodropout-learning-dropout-patterns-to",
            "published":"2021-01-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/autodropout-learning-dropout-patterns-to\/review\/?hl=24594"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":11221,
        "rank":781,
        "Model":"ACNet (ResNet-50)",
        "mlmodel":{

        },
        "method_short":"ACNet ",
        "method_details":"ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-07",
        "metrics":{
            "Top 1 Accuracy":"77.5%",
            "Number of params":"29.38M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.5,
            "Number of params":29380000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":110903,
            "title":"Adaptively Connected Neural Networks",
            "url":"\/paper\/adaptively-connected-neural-networks",
            "published":"2019-04-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/adaptively-connected-neural-networks\/review\/?hl=11221"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":8266,
        "rank":782,
        "Model":"ECA-Net (ResNet-50)",
        "mlmodel":{

        },
        "method_short":"ECA-Net ",
        "method_details":"ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-10-08",
        "metrics":{
            "Top 1 Accuracy":"77.48%",
            "Number of params":"24.37M",
            "GFLOPs":"3.86",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.48,
            "Number of params":24370000.0,
            "GFLOPs":3.86,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":157542,
            "title":"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks",
            "url":"\/paper\/eca-net-efficient-channel-attention-for-deep",
            "published":"2019-10-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/eca-net-efficient-channel-attention-for-deep\/review\/?hl=8266"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6154,
        "rank":783,
        "Model":"DenseNet-201",
        "mlmodel":{

        },
        "method_short":"DenseNet-201",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-08-25",
        "metrics":{
            "Top 1 Accuracy":"77.42%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.42,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":11275,
            "title":"Densely Connected Convolutional Networks",
            "url":"\/paper\/densely-connected-convolutional-networks",
            "published":"2016-08-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/densely-connected-convolutional-networks\/review\/?hl=6154"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":105104,
        "rank":784,
        "Model":"MobileOne-S2",
        "mlmodel":{

        },
        "method_short":"MobileOne-S2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-08",
        "metrics":{
            "Top 1 Accuracy":"77.4%",
            "Number of params":"7.8",
            "GFLOPs":"1.3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.4,
            "Number of params":7.8,
            "GFLOPs":1.3,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1023967,
            "title":"MobileOne: An Improved One millisecond Mobile Backbone",
            "url":"\/paper\/an-improved-one-millisecond-mobile-backbone",
            "published":"2022-06-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/an-improved-one-millisecond-mobile-backbone\/review\/?hl=105104"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":105108,
        "rank":785,
        "Model":"MobileOne-S1 (distill)",
        "mlmodel":{

        },
        "method_short":"MobileOne-S1 ",
        "method_details":"distill",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-08",
        "metrics":{
            "Top 1 Accuracy":"77.4%",
            "Number of params":"4.8M",
            "GFLOPs":"0.825",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.4,
            "Number of params":4800000.0,
            "GFLOPs":0.825,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1023967,
            "title":"MobileOne: An Improved One millisecond Mobile Backbone",
            "url":"\/paper\/an-improved-one-millisecond-mobile-backbone",
            "published":"2022-06-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/an-improved-one-millisecond-mobile-backbone\/review\/?hl=105108"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":87268,
        "rank":786,
        "Model":"R-Mix (ResNet-50)",
        "mlmodel":{

        },
        "method_short":"R-Mix ",
        "method_details":"ResNet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-09",
        "metrics":{
            "Top 1 Accuracy":"77.39%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.39,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1126215,
            "title":"Expeditious Saliency-guided Mix-up through Random Gradient Thresholding",
            "url":"\/paper\/expeditious-saliency-guided-mix-up-through",
            "published":"2022-12-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/expeditious-saliency-guided-mix-up-through\/review\/?hl=87268"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":8970,
        "rank":787,
        "Model":"ResnetV2 50 (FRN layer)",
        "mlmodel":{

        },
        "method_short":"ResnetV2 50 ",
        "method_details":"FRN layer",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-21",
        "metrics":{
            "Top 1 Accuracy":"77.21%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.21,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":174026,
            "title":"Filter Response Normalization Layer: Eliminating Batch Dependence in the Training of Deep Neural Networks",
            "url":"\/paper\/filter-response-normalization-layer",
            "published":"2019-11-21T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60744,
        "rank":788,
        "Model":"FBNetV5-AR-CLS",
        "mlmodel":{

        },
        "method_short":"FBNetV5-AR-CLS",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-19",
        "metrics":{
            "Top 1 Accuracy":"77.2%",
            "Number of params":null,
            "GFLOPs":"0.215",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.2,
            "Number of params":null,
            "GFLOPs":0.215,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":912986,
            "title":"FBNetV5: Neural Architecture Search for Multiple Tasks in One Run",
            "url":"\/paper\/fbnetv5-neural-architecture-search-for",
            "published":"2021-11-19T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/fbnetv5-neural-architecture-search-for\/review\/?hl=60744"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":99768,
        "rank":789,
        "Model":"MogaNet-XT (256res)",
        "mlmodel":{

        },
        "method_short":"MogaNet-XT ",
        "method_details":"256res",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-07",
        "metrics":{
            "Top 1 Accuracy":"77.2%",
            "Number of params":"3M",
            "GFLOPs":"1.04",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.2,
            "Number of params":3000000.0,
            "GFLOPs":1.04,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1106694,
            "title":"Efficient Multi-order Gated Aggregation Network",
            "url":"\/paper\/efficient-multi-order-gated-aggregation",
            "published":"2022-11-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/efficient-multi-order-gated-aggregation\/review\/?hl=99768"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":49044,
        "rank":790,
        "Model":"ReXNet_0.9",
        "mlmodel":{

        },
        "method_short":"ReXNet_0.9",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-07-02",
        "metrics":{
            "Top 1 Accuracy":"77.2%",
            "Number of params":"4.1M",
            "GFLOPs":"0.35",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.2,
            "Number of params":4100000.0,
            "GFLOPs":0.35,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":206716,
            "title":"Rethinking Channel Dimensions for Efficient Model Design",
            "url":"\/paper\/rexnet-diminishing-representational",
            "published":"2020-07-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rexnet-diminishing-representational\/review\/?hl=49044"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":27348,
        "rank":791,
        "Model":"Prodpoly",
        "mlmodel":{

        },
        "method_short":"Prodpoly",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-20",
        "metrics":{
            "Top 1 Accuracy":"77.17%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.17,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":205264,
            "title":"Deep Polynomial Neural Networks",
            "url":"\/paper\/deep-polynomial-neural-networks",
            "published":"2020-06-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deep-polynomial-neural-networks\/review\/?hl=27348"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6226,
        "rank":792,
        "Model":"ResNet-50-D",
        "mlmodel":{

        },
        "method_short":"ResNet-50-D",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-12-04",
        "metrics":{
            "Top 1 Accuracy":"77.16%",
            "Number of params":"25M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.16,
            "Number of params":25000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":64092,
            "title":"Bag of Tricks for Image Classification with Convolutional Neural Networks",
            "url":"\/paper\/bag-of-tricks-for-image-classification-with",
            "published":"2018-12-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/bag-of-tricks-for-image-classification-with\/review\/?hl=6226"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":24242,
        "rank":793,
        "Model":"Inception v3",
        "mlmodel":{

        },
        "method_short":"Inception v3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-03-22",
        "metrics":{
            "Top 1 Accuracy":"77.12%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.12,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":7743,
            "title":"What do Deep Networks Like to See?",
            "url":"\/paper\/what-do-deep-networks-like-to-see",
            "published":"2018-03-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/what-do-deep-networks-like-to-see\/review\/?hl=24242"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":75997,
        "rank":794,
        "Model":"MKD ViT-T",
        "mlmodel":{

        },
        "method_short":"MKD ViT-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-16",
        "metrics":{
            "Top 1 Accuracy":"77.1%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.1,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":962993,
            "title":"Meta Knowledge Distillation",
            "url":"\/paper\/meta-knowledge-distillation",
            "published":"2022-02-16T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/meta-knowledge-distillation\/review\/?hl=75997"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":10461,
        "rank":795,
        "Model":"GreedyNAS-A",
        "mlmodel":{

        },
        "method_short":"GreedyNAS-A",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-25",
        "metrics":{
            "Top 1 Accuracy":"77.1%",
            "Number of params":"6.5M",
            "GFLOPs":"0.366",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.1,
            "Number of params":6500000.0,
            "GFLOPs":0.366,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":188284,
            "title":"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet",
            "url":"\/paper\/greedynas-towards-fast-one-shot-nas-with",
            "published":"2020-03-25T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/greedynas-towards-fast-one-shot-nas-with\/review\/?hl=10461"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60583,
        "rank":796,
        "Model":"SkipblockNet-L",
        "mlmodel":{

        },
        "method_short":"SkipblockNet-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-23",
        "metrics":{
            "Top 1 Accuracy":"77.1%",
            "Number of params":"7.1M",
            "GFLOPs":"0.364",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.1,
            "Number of params":7100000.0,
            "GFLOPs":0.364,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":840600,
            "title":"Bias Loss for Mobile Neural Networks",
            "url":"\/paper\/bias-loss-for-mobile-neural-networks",
            "published":"2021-07-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/bias-loss-for-mobile-neural-networks\/review\/?hl=60583"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":24314,
        "rank":797,
        "Model":"SSAL-Resnet50",
        "mlmodel":{

        },
        "method_short":"SSAL-Resnet50",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-01-07",
        "metrics":{
            "Top 1 Accuracy":"77.0%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.0,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":734609,
            "title":"Contextual Classification Using Self-Supervised Auxiliary Models for Deep Neural Networks",
            "url":"\/paper\/contextual-classification-using-self",
            "published":"2021-01-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/contextual-classification-using-self\/review\/?hl=24314"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":112824,
        "rank":798,
        "Model":"UniRepLKNet-A",
        "mlmodel":{

        },
        "method_short":"UniRepLKNet-A",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-11-27",
        "metrics":{
            "Top 1 Accuracy":"77%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.0,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1327838,
            "title":"UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition",
            "url":"\/paper\/unireplknet-a-universal-perception-large",
            "published":"2023-11-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/unireplknet-a-universal-perception-large\/review\/?hl=112824"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":101973,
        "rank":799,
        "Model":"CloFormer-XXS",
        "mlmodel":{

        },
        "method_short":"CloFormer-XXS",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-31",
        "metrics":{
            "Top 1 Accuracy":"77%",
            "Number of params":"4.2M",
            "GFLOPs":"0.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.0,
            "Number of params":4200000.0,
            "GFLOPs":0.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1184169,
            "title":"Rethinking Local Perception in Lightweight Vision Transformer",
            "url":"\/paper\/rethinking-local-perception-in-lightweight",
            "published":"2023-03-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-local-perception-in-lightweight\/review\/?hl=101973"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6229,
        "rank":800,
        "Model":"MixNet-M",
        "mlmodel":{

        },
        "method_short":"MixNet-M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-07-22",
        "metrics":{
            "Top 1 Accuracy":"77%",
            "Number of params":"5.0M",
            "GFLOPs":"0.360",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.0,
            "Number of params":5000000.0,
            "GFLOPs":0.36,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":146640,
            "title":"MixConv: Mixed Depthwise Convolutional Kernels",
            "url":"\/paper\/mixnet-mixed-depthwise-convolutional-kernels",
            "published":"2019-07-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mixnet-mixed-depthwise-convolutional-kernels\/review\/?hl=6229"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6364,
        "rank":801,
        "Model":"SCARLET-A",
        "mlmodel":{

        },
        "method_short":"SCARLET-A",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-16",
        "metrics":{
            "Top 1 Accuracy":"76.9%",
            "Number of params":"6.7M",
            "GFLOPs":"0.730",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.9,
            "Number of params":6700000.0,
            "GFLOPs":0.73,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":150158,
            "title":"SCARLET-NAS: Bridging the Gap between Stability and Scalability in Weight-sharing Neural Architecture Search",
            "url":"\/paper\/scarletnas-bridging-the-gap-between",
            "published":"2019-08-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scarletnas-bridging-the-gap-between\/review\/?hl=6364"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":71927,
        "rank":802,
        "Model":"TransBoost-MobileNetV3-L",
        "mlmodel":{

        },
        "method_short":"TransBoost-MobileNetV3-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-26",
        "metrics":{
            "Top 1 Accuracy":"76.81%",
            "Number of params":"5.48M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.81,
            "Number of params":5480000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1016638,
            "title":"TransBoost: Improving the Best ImageNet Performance using Deep Transduction",
            "url":"\/paper\/transboost-improving-the-best-imagenet",
            "published":"2022-05-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/transboost-improving-the-best-imagenet\/review\/?hl=71927"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":36766,
        "rank":803,
        "Model":"ViTAE-T-Stage",
        "mlmodel":{

        },
        "method_short":"ViTAE-T-Stage",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-07",
        "metrics":{
            "Top 1 Accuracy":"76.8%",
            "Number of params":"4.8M",
            "GFLOPs":"4.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.8,
            "Number of params":4800000.0,
            "GFLOPs":4.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":812332,
            "title":"ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias",
            "url":"\/paper\/vitae-vision-transformer-advanced-by",
            "published":"2021-06-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vitae-vision-transformer-advanced-by\/review\/?hl=36766"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":10462,
        "rank":804,
        "Model":"GreedyNAS-B",
        "mlmodel":{

        },
        "method_short":"GreedyNAS-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-25",
        "metrics":{
            "Top 1 Accuracy":"76.8%",
            "Number of params":"5.2M",
            "GFLOPs":"0.324",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.8,
            "Number of params":5200000.0,
            "GFLOPs":0.324,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":188284,
            "title":"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet",
            "url":"\/paper\/greedynas-towards-fast-one-shot-nas-with",
            "published":"2020-03-25T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/greedynas-towards-fast-one-shot-nas-with\/review\/?hl=10462"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":39227,
        "rank":805,
        "Model":"ConvMLP-S",
        "mlmodel":{

        },
        "method_short":"ConvMLP-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-09-09",
        "metrics":{
            "Top 1 Accuracy":"76.8",
            "Number of params":"9M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.8,
            "Number of params":9000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":864188,
            "title":"ConvMLP: Hierarchical Convolutional MLPs for Vision",
            "url":"\/paper\/convmlp-hierarchical-convolutional-mlps-for",
            "published":"2021-09-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/convmlp-hierarchical-convolutional-mlps-for\/review\/?hl=39227"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":22520,
        "rank":806,
        "Model":"Perona Malik (Perona and Malik, 1990)",
        "mlmodel":{

        },
        "method_short":"Perona Malik ",
        "method_details":"Perona and Malik, 1990",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-11-03",
        "metrics":{
            "Top 1 Accuracy":"76.71%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.71,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":232518,
            "title":"Learning Visual Representations for Transfer Learning by Suppressing Texture",
            "url":"\/paper\/learning-visual-representations-for-transfer-1",
            "published":"2020-11-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learning-visual-representations-for-transfer-1\/review\/?hl=22520"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":108978,
        "rank":807,
        "Model":"PVT-T (+MixPro)",
        "mlmodel":{

        },
        "method_short":"PVT-T ",
        "method_details":"+MixPro",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-24",
        "metrics":{
            "Top 1 Accuracy":"76.7%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.7,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1195795,
            "title":"MixPro: Data Augmentation with MaskMix and Progressive Attention Labeling for Vision Transformer",
            "url":"\/paper\/mixpro-data-augmentation-with-maskmix-and",
            "published":"2023-04-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mixpro-data-augmentation-with-maskmix-and\/review\/?hl=108978"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":73069,
        "rank":808,
        "Model":"MobileViTv3-XS",
        "mlmodel":{

        },
        "method_short":"MobileViTv3-XS",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-30",
        "metrics":{
            "Top 1 Accuracy":"76.7%",
            "Number of params":"2.5M",
            "GFLOPs":"0.927",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.7,
            "Number of params":2500000.0,
            "GFLOPs":0.927,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1084483,
            "title":"MobileViTv3: Mobile-Friendly Vision Transformer with Simple and Effective Fusion of Local, Global and Input Features",
            "url":"\/paper\/mobilevitv3-mobile-friendly-vision",
            "published":"2022-09-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mobilevitv3-mobile-friendly-vision\/review\/?hl=73069"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":160,
                "name":"CNN+Transformer",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6233,
        "rank":809,
        "Model":"MnasNet-A3",
        "mlmodel":{

        },
        "method_short":"MnasNet-A3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-07-31",
        "metrics":{
            "Top 1 Accuracy":"76.7%",
            "Number of params":"5.2M",
            "GFLOPs":"0.806",
            "Hardware Burden":null,
            "Operations per network pass":"0.0403G",
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.7,
            "Number of params":5200000.0,
            "GFLOPs":0.806,
            "Hardware Burden":null,
            "Operations per network pass":0.0403,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":53964,
            "title":"MnasNet: Platform-Aware Neural Architecture Search for Mobile",
            "url":"\/paper\/mnasnet-platform-aware-neural-architecture",
            "published":"2018-07-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mnasnet-platform-aware-neural-architecture\/review\/?hl=6233"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60642,
        "rank":810,
        "Model":"ViL-Tiny-RPB",
        "mlmodel":{

        },
        "method_short":"ViL-Tiny-RPB",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-29",
        "metrics":{
            "Top 1 Accuracy":"76.7%",
            "Number of params":"6.7M",
            "GFLOPs":"1.3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.7,
            "Number of params":6700000.0,
            "GFLOPs":1.3,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":758437,
            "title":"Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding",
            "url":"\/paper\/2103-15358",
            "published":"2021-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/2103-15358\/review\/?hl=60642"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":28359,
        "rank":811,
        "Model":"ConViT-Ti+",
        "mlmodel":{

        },
        "method_short":"ConViT-Ti+",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-19",
        "metrics":{
            "Top 1 Accuracy":"76.7%",
            "Number of params":"10M",
            "GFLOPs":"2",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.7,
            "Number of params":10000000.0,
            "GFLOPs":2.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755434,
            "title":"ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases",
            "url":"\/paper\/convit-improving-vision-transformers-with",
            "published":"2021-03-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/convit-improving-vision-transformers-with\/review\/?hl=28359"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":71922,
        "rank":812,
        "Model":"TransBoost-ResNet34",
        "mlmodel":{

        },
        "method_short":"TransBoost-ResNet34",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-26",
        "metrics":{
            "Top 1 Accuracy":"76.70%",
            "Number of params":"21.8M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.7,
            "Number of params":21800000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1016638,
            "title":"TransBoost: Improving the Best ImageNet Performance using Deep Transduction",
            "url":"\/paper\/transboost-improving-the-best-imagenet",
            "published":"2022-05-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/transboost-improving-the-best-imagenet\/review\/?hl=71922"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":11222,
        "rank":813,
        "Model":"LIP-DenseNet-BC-121",
        "mlmodel":{

        },
        "method_short":"LIP-DenseNet-BC-121",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-12",
        "metrics":{
            "Top 1 Accuracy":"76.64%",
            "Number of params":"8.7M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.64,
            "Number of params":8700000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":149671,
            "title":"LIP: Local Importance-based Pooling",
            "url":"\/paper\/lip-local-importance-based-pooling",
            "published":"2019-08-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lip-local-importance-based-pooling\/review\/?hl=11222"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":34277,
        "rank":814,
        "Model":"ResNet-50 (X-volution, stage3)",
        "mlmodel":{

        },
        "method_short":"ResNet-50 ",
        "method_details":"X-volution, stage3",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-04",
        "metrics":{
            "Top 1 Accuracy":"76.6%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.6,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":811671,
            "title":"X-volution: On the unification of convolution and self-attention",
            "url":"\/paper\/x-volution-on-the-unification-of-convolution",
            "published":"2021-06-04T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/x-volution-on-the-unification-of-convolution\/review\/?hl=34277"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":16116,
        "rank":815,
        "Model":"MUXNet-l",
        "mlmodel":{

        },
        "method_short":"MUXNet-l",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-31",
        "metrics":{
            "Top 1 Accuracy":"76.6%",
            "Number of params":"4.0M",
            "GFLOPs":"0.636",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.6,
            "Number of params":4000000.0,
            "GFLOPs":0.636,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":188988,
            "title":"MUXConv: Information Multiplexing in Convolutional Neural Networks",
            "url":"\/paper\/muxconv-information-multiplexing-in",
            "published":"2020-03-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/muxconv-information-multiplexing-in\/review\/?hl=16116"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":23826,
        "rank":816,
        "Model":"DeiT-B",
        "mlmodel":{

        },
        "method_short":"DeiT-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-12-23",
        "metrics":{
            "Top 1 Accuracy":"76.6%",
            "Number of params":"5M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.6,
            "Number of params":5000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":731001,
            "title":"Training data-efficient image transformers & distillation through attention",
            "url":"\/paper\/training-data-efficient-image-transformers",
            "published":"2020-12-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/training-data-efficient-image-transformers\/review\/?hl=23826"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":73072,
        "rank":817,
        "Model":"MobileViTv3-0.75",
        "mlmodel":{

        },
        "method_short":"MobileViTv3-0.75",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-30",
        "metrics":{
            "Top 1 Accuracy":"76.55%",
            "Number of params":"3M",
            "GFLOPs":"1.064",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.55,
            "Number of params":3000000.0,
            "GFLOPs":1.064,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1084483,
            "title":"MobileViTv3: Mobile-Friendly Vision Transformer with Simple and Effective Fusion of Local, Global and Input Features",
            "url":"\/paper\/mobilevitv3-mobile-friendly-vision",
            "published":"2022-09-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mobilevitv3-mobile-friendly-vision\/review\/?hl=73072"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":31822,
        "rank":818,
        "Model":"Mixer-B\/16",
        "mlmodel":{

        },
        "method_short":"Mixer-B\/16",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-04",
        "metrics":{
            "Top 1 Accuracy":"76.44%",
            "Number of params":"46M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.44,
            "Number of params":46000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":793349,
            "title":"MLP-Mixer: An all-MLP Architecture for Vision",
            "url":"\/paper\/mlp-mixer-an-all-mlp-architecture-for-vision",
            "published":"2021-05-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mlp-mixer-an-all-mlp-architecture-for-vision\/review\/?hl=31822"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":28365,
        "rank":819,
        "Model":"Perceiver",
        "mlmodel":{

        },
        "method_short":"Perceiver",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-04",
        "metrics":{
            "Top 1 Accuracy":"76.4%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.4,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":750724,
            "title":"Perceiver: General Perception with Iterative Attention",
            "url":"\/paper\/perceiver-general-perception-with-iterative",
            "published":"2021-03-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/perceiver-general-perception-with-iterative\/review\/?hl=28365"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":28486,
        "rank":820,
        "Model":"CeiT-T",
        "mlmodel":{

        },
        "method_short":"CeiT-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-22",
        "metrics":{
            "Top 1 Accuracy":"76.4%",
            "Number of params":"6.4M",
            "GFLOPs":"1.2",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.4,
            "Number of params":6400000.0,
            "GFLOPs":1.2,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755729,
            "title":"Incorporating Convolution Designs into Visual Transformers",
            "url":"\/paper\/incorporating-convolution-designs-into-visual",
            "published":"2021-03-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/incorporating-convolution-designs-into-visual\/review\/?hl=28486"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":45003,
        "rank":821,
        "Model":"ResNet-34 (SAMix)",
        "mlmodel":{

        },
        "method_short":"ResNet-34 ",
        "method_details":"SAMix",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-30",
        "metrics":{
            "Top 1 Accuracy":"76.35%",
            "Number of params":"21.8M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.35,
            "Number of params":21800000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":923048,
            "title":"Boosting Discriminative Visual Representation Learning with Scenario-Agnostic Mixup",
            "url":"\/paper\/boosting-discriminative-visual-representation",
            "published":"2021-11-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/boosting-discriminative-visual-representation\/review\/?hl=45003"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6187,
        "rank":822,
        "Model":"EfficientNet-B0",
        "mlmodel":{

        },
        "method_short":"EfficientNet-B0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-05-28",
        "metrics":{
            "Top 1 Accuracy":"76.3%",
            "Number of params":"5.3M",
            "GFLOPs":"0.39",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.3,
            "Number of params":5300000.0,
            "GFLOPs":0.39,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":117456,
            "title":"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
            "url":"\/paper\/efficientnet-rethinking-model-scaling-for",
            "published":"2019-05-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/efficientnet-rethinking-model-scaling-for\/review\/?hl=6187"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":5,
                "name":"EfficientNet",
                "color":"#05A300"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":10593,
        "rank":823,
        "Model":"RegNetY-800MF",
        "mlmodel":{

        },
        "method_short":"RegNetY-800MF",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-30",
        "metrics":{
            "Top 1 Accuracy":"76.3%",
            "Number of params":"6.3M",
            "GFLOPs":"0.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.3,
            "Number of params":6300000.0,
            "GFLOPs":0.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":188625,
            "title":"Designing Network Design Spaces",
            "url":"\/paper\/designing-network-design-spaces",
            "published":"2020-03-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/designing-network-design-spaces\/review\/?hl=10593"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6365,
        "rank":824,
        "Model":"SCARLET-B",
        "mlmodel":{

        },
        "method_short":"SCARLET-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-16",
        "metrics":{
            "Top 1 Accuracy":"76.3%",
            "Number of params":"6.5M",
            "GFLOPs":"0.658",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.3,
            "Number of params":6500000.0,
            "GFLOPs":0.658,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":150158,
            "title":"SCARLET-NAS: Bridging the Gap between Stability and Scalability in Weight-sharing Neural Architecture Search",
            "url":"\/paper\/scarletnas-bridging-the-gap-between",
            "published":"2019-08-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scarletnas-bridging-the-gap-between\/review\/?hl=6365"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":37536,
        "rank":825,
        "Model":"GLiT-Tinys",
        "mlmodel":{

        },
        "method_short":"GLiT-Tinys",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-07",
        "metrics":{
            "Top 1 Accuracy":"76.3%",
            "Number of params":"7.2M",
            "GFLOPs":"1.4",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.3,
            "Number of params":7200000.0,
            "GFLOPs":1.4,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":831816,
            "title":"GLiT: Neural Architecture Search for Global and Local Image Transformer",
            "url":"\/paper\/glit-neural-architecture-search-for-global",
            "published":"2021-07-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/glit-neural-architecture-search-for-global\/review\/?hl=37536"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6155,
        "rank":826,
        "Model":"DenseNet-169",
        "mlmodel":{

        },
        "method_short":"DenseNet-169",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-08-25",
        "metrics":{
            "Top 1 Accuracy":"76.2%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.2,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":11275,
            "title":"Densely Connected Convolutional Networks",
            "url":"\/paper\/densely-connected-convolutional-networks",
            "published":"2016-08-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/densely-connected-convolutional-networks\/review\/?hl=6155"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":10463,
        "rank":827,
        "Model":"GreedyNAS-C",
        "mlmodel":{

        },
        "method_short":"GreedyNAS-C",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-25",
        "metrics":{
            "Top 1 Accuracy":"76.2%",
            "Number of params":"4.7M",
            "GFLOPs":"0.284",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.2,
            "Number of params":4700000.0,
            "GFLOPs":0.284,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":188284,
            "title":"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet",
            "url":"\/paper\/greedynas-towards-fast-one-shot-nas-with",
            "published":"2020-03-25T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/greedynas-towards-fast-one-shot-nas-with\/review\/?hl=10463"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":42225,
        "rank":828,
        "Model":"SkipblockNet-M",
        "mlmodel":{

        },
        "method_short":"SkipblockNet-M",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-23",
        "metrics":{
            "Top 1 Accuracy":"76.2%",
            "Number of params":"5.5M",
            "GFLOPs":"0.246",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.2,
            "Number of params":5500000.0,
            "GFLOPs":0.246,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":840600,
            "title":"Bias Loss for Mobile Neural Networks",
            "url":"\/paper\/bias-loss-for-mobile-neural-networks",
            "published":"2021-07-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/bias-loss-for-mobile-neural-networks\/review\/?hl=42225"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":58506,
        "rank":829,
        "Model":"ELP (naive ResNet50)",
        "mlmodel":{

        },
        "method_short":"ELP ",
        "method_details":"naive ResNet50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-29",
        "metrics":{
            "Top 1 Accuracy":"76.13",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.13,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1021878,
            "title":"A Simple Episodic Linear Probe Improves Visual Recognition in the Wild",
            "url":"\/paper\/a-simple-episodic-linear-probe-improves",
            "published":"2022-01-01T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":44997,
        "rank":830,
        "Model":"ResNet-34 (AutoMix)",
        "mlmodel":{

        },
        "method_short":"ResNet-34 ",
        "method_details":"AutoMix",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-24",
        "metrics":{
            "Top 1 Accuracy":"76.1%",
            "Number of params":"21.8M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.1,
            "Number of params":21800000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":756879,
            "title":"AutoMix: Unveiling the Power of Mixup for Stronger Classifiers",
            "url":"\/paper\/automix-unveiling-the-power-of-mixup",
            "published":"2021-03-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/automix-unveiling-the-power-of-mixup\/review\/?hl=44997"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":25985,
        "rank":831,
        "Model":"ResNet-50 MLPerf v0.7 - 2512 steps",
        "mlmodel":{

        },
        "method_short":"ResNet-50 MLPerf v0.7 - 2512 steps",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-12",
        "metrics":{
            "Top 1 Accuracy":"75.92%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.92,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":745083,
            "title":"A Large Batch Optimizer Reality Check: Traditional, Generic Optimizers Suffice Across Batch Sizes",
            "url":"\/paper\/a-large-batch-optimizer-reality-check",
            "published":"2021-02-12T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/a-large-batch-optimizer-reality-check\/review\/?hl=25985"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":5652,
        "rank":832,
        "Model":"DenseNAS-A",
        "mlmodel":{

        },
        "method_short":"DenseNAS-A",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-23",
        "metrics":{
            "Top 1 Accuracy":"75.9%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.9,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":143691,
            "title":"Densely Connected Search Space for More Flexible Neural Architecture Search",
            "url":"\/paper\/densely-connected-search-space-for-more",
            "published":"2019-06-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/densely-connected-search-space-for-more\/review\/?hl=5652"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":105105,
        "rank":833,
        "Model":"MobileOne-S1",
        "mlmodel":{

        },
        "method_short":"MobileOne-S1",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-08",
        "metrics":{
            "Top 1 Accuracy":"75.9",
            "Number of params":"4.8M",
            "GFLOPs":"0.825",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.9,
            "Number of params":4800000.0,
            "GFLOPs":0.825,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1023967,
            "title":"MobileOne: An Improved One millisecond Mobile Backbone",
            "url":"\/paper\/an-improved-one-millisecond-mobile-backbone",
            "published":"2022-06-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/an-improved-one-millisecond-mobile-backbone\/review\/?hl=105105"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":7939,
        "rank":834,
        "Model":"MoGA-A",
        "mlmodel":{

        },
        "method_short":"MoGA-A",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-04",
        "metrics":{
            "Top 1 Accuracy":"75.9%",
            "Number of params":"5.1M",
            "GFLOPs":"0.608",
            "Hardware Burden":null,
            "Operations per network pass":"0.0304G",
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.9,
            "Number of params":5100000.0,
            "GFLOPs":0.608,
            "Hardware Burden":null,
            "Operations per network pass":0.0304,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":149130,
            "title":"MoGA: Searching Beyond MobileNetV3",
            "url":"\/paper\/moga-searching-beyond-mobilenetv3",
            "published":"2019-08-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/moga-searching-beyond-mobilenetv3\/review\/?hl=7939"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":64201,
        "rank":835,
        "Model":"RevBiFPN-S1",
        "mlmodel":{

        },
        "method_short":"RevBiFPN-S1",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-28",
        "metrics":{
            "Top 1 Accuracy":"75.9%",
            "Number of params":"5.11M",
            "GFLOPs":"0.62",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.9,
            "Number of params":5110000.0,
            "GFLOPs":0.62,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1034397,
            "title":"RevBiFPN: The Fully Reversible Bidirectional Feature Pyramid Network",
            "url":"\/paper\/revbifpn-the-fully-reversible-bidirectional",
            "published":"2022-06-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/revbifpn-the-fully-reversible-bidirectional\/review\/?hl=64201"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":283,
                "name":"Reversible",
                "color":"#3227d3"
            },
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":13,
                "name":"FPN",
                "color":"#009481"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29953,
        "rank":836,
        "Model":"LocalViT-TNT",
        "mlmodel":{

        },
        "method_short":"LocalViT-TNT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-12",
        "metrics":{
            "Top 1 Accuracy":"75.9%",
            "Number of params":"6.3M",
            "GFLOPs":"1.4",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.9,
            "Number of params":6300000.0,
            "GFLOPs":1.4,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":778779,
            "title":"LocalViT: Bringing Locality to Vision Transformers",
            "url":"\/paper\/localvit-bringing-locality-to-vision",
            "published":"2021-04-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/localvit-bringing-locality-to-vision\/review\/?hl=29953"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":86759,
        "rank":837,
        "Model":"SALG-ST",
        "mlmodel":{

        },
        "method_short":"SALG-ST",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-27",
        "metrics":{
            "Top 1 Accuracy":"75.9%",
            "Number of params":"6.5M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.9,
            "Number of params":6500000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1118828,
            "title":"Semantic-Aware Local-Global Vision Transformer",
            "url":"\/paper\/semantic-aware-local-global-vision",
            "published":"2022-11-27T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/semantic-aware-local-global-vision\/review\/?hl=86759"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":27936,
        "rank":838,
        "Model":"RedNet-26",
        "mlmodel":{

        },
        "method_short":"RedNet-26",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-10",
        "metrics":{
            "Top 1 Accuracy":"75.9%",
            "Number of params":"9.2M",
            "GFLOPs":"1.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.9,
            "Number of params":9200000.0,
            "GFLOPs":1.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":752660,
            "title":"Involution: Inverting the Inherence of Convolution for Visual Recognition",
            "url":"\/paper\/involution-inverting-the-inherence-of",
            "published":"2021-03-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/involution-inverting-the-inherence-of\/review\/?hl=27936"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6139,
        "rank":839,
        "Model":"FractalNet-34",
        "mlmodel":{

        },
        "method_short":"FractalNet-34",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-05-24",
        "metrics":{
            "Top 1 Accuracy":"75.88%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.88,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":22396,
            "title":"FractalNet: Ultra-Deep Neural Networks without Residuals",
            "url":"\/paper\/fractalnet-ultra-deep-neural-networks-without",
            "published":"2016-05-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fractalnet-ultra-deep-neural-networks-without\/review\/?hl=6139"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6230,
        "rank":840,
        "Model":"MixNet-S",
        "mlmodel":{

        },
        "method_short":"MixNet-S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-07-22",
        "metrics":{
            "Top 1 Accuracy":"75.8%",
            "Number of params":"4.1M",
            "GFLOPs":"0.256",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.8,
            "Number of params":4100000.0,
            "GFLOPs":0.256,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":146640,
            "title":"MixConv: Mixed Depthwise Convolutional Kernels",
            "url":"\/paper\/mixnet-mixed-depthwise-convolutional-kernels",
            "published":"2019-07-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mixnet-mixed-depthwise-convolutional-kernels\/review\/?hl=6230"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":5940,
        "rank":841,
        "Model":"CoordConv ResNet-50",
        "mlmodel":{

        },
        "method_short":"CoordConv ResNet-50",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-07-09",
        "metrics":{
            "Top 1 Accuracy":"75.74%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.74,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":52147,
            "title":"An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution",
            "url":"\/paper\/an-intriguing-failing-of-convolutional-neural",
            "published":"2018-07-09T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29497,
        "rank":842,
        "Model":"LeViT-128S",
        "mlmodel":{

        },
        "method_short":"LeViT-128S",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-02",
        "metrics":{
            "Top 1 Accuracy":"75.7%",
            "Number of params":"4.7M",
            "GFLOPs":"0.288",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.7,
            "Number of params":4700000.0,
            "GFLOPs":0.288,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":774248,
            "title":"LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference",
            "url":"\/paper\/levit-a-vision-transformer-in-convnet-s",
            "published":"2021-04-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/levit-a-vision-transformer-in-convnet-s\/review\/?hl=29497"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":57968,
        "rank":843,
        "Model":"GhostNet \u00d71.3",
        "mlmodel":{

        },
        "method_short":"GhostNet \u00d71.3",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-27",
        "metrics":{
            "Top 1 Accuracy":"75.7%",
            "Number of params":"7.3M",
            "GFLOPs":"0.226",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.7,
            "Number of params":7300000.0,
            "GFLOPs":0.226,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":174553,
            "title":"GhostNet: More Features from Cheap Operations",
            "url":"\/paper\/ghostnet-more-features-from-cheap-operations",
            "published":"2019-11-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ghostnet-more-features-from-cheap-operations\/review\/?hl=57968"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":4856,
        "rank":844,
        "Model":"LR-Net-26",
        "mlmodel":{

        },
        "method_short":"LR-Net-26",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-25",
        "metrics":{
            "Top 1 Accuracy":"75.7%",
            "Number of params":"14.7M",
            "GFLOPs":"2.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.7,
            "Number of params":14700000.0,
            "GFLOPs":2.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":112703,
            "title":"Local Relation Networks for Image Recognition",
            "url":"\/paper\/190411491",
            "published":"2019-04-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/190411491\/review\/?hl=4856"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":105114,
        "rank":845,
        "Model":"Mixer-S16 + STD",
        "mlmodel":{

        },
        "method_short":"Mixer-S16 + STD",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-23",
        "metrics":{
            "Top 1 Accuracy":"75.7%",
            "Number of params":"22.2M",
            "GFLOPs":"4.3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.7,
            "Number of params":22200000.0,
            "GFLOPs":4.3,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1230624,
            "title":"Spatial-Channel Token Distillation for Vision MLPs",
            "url":"\/paper\/spatial-channel-token-distillation-for-vision",
            "published":"2022-07-23T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":97063,
        "rank":846,
        "Model":"SimpleNetV1-small-075-correct-labels",
        "mlmodel":{

        },
        "method_short":"SimpleNetV1-small-075-correct-labels",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-08-22",
        "metrics":{
            "Top 1 Accuracy":"75.66",
            "Number of params":"3M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.66,
            "Number of params":3000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":10271,
            "title":"Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures",
            "url":"\/paper\/lets-keep-it-simple-using-simple",
            "published":"2016-08-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lets-keep-it-simple-using-simple\/review\/?hl=97063"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6232,
        "rank":847,
        "Model":"MnasNet-A2",
        "mlmodel":{

        },
        "method_short":"MnasNet-A2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-07-31",
        "metrics":{
            "Top 1 Accuracy":"75.6%",
            "Number of params":"4.8M",
            "GFLOPs":"0.680",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.6,
            "Number of params":4800000.0,
            "GFLOPs":0.68,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":53964,
            "title":"MnasNet: Platform-Aware Neural Architecture Search for Mobile",
            "url":"\/paper\/mnasnet-platform-aware-neural-architecture",
            "published":"2018-07-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mnasnet-platform-aware-neural-architecture\/review\/?hl=6232"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6366,
        "rank":848,
        "Model":"SCARLET-C",
        "mlmodel":{

        },
        "method_short":"SCARLET-C",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-08-16",
        "metrics":{
            "Top 1 Accuracy":"75.6%",
            "Number of params":"6M",
            "GFLOPs":"0.560",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.6,
            "Number of params":6000000.0,
            "GFLOPs":0.56,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":150158,
            "title":"SCARLET-NAS: Bridging the Gap between Stability and Scalability in Weight-sharing Neural Architecture Search",
            "url":"\/paper\/scarletnas-bridging-the-gap-between",
            "published":"2019-08-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scarletnas-bridging-the-gap-between\/review\/?hl=6366"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":31363,
        "rank":849,
        "Model":"PAWS (ResNet-50, 10% labels)",
        "mlmodel":{

        },
        "method_short":"PAWS ",
        "method_details":"ResNet-50, 10% labels",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-28",
        "metrics":{
            "Top 1 Accuracy":"75.5%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.5,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":788977,
            "title":"Semi-Supervised Learning of Visual Features by Non-Parametrically Predicting View Assignments with Support Samples",
            "url":"\/paper\/semi-supervised-learning-of-visual-features",
            "published":"2021-04-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/semi-supervised-learning-of-visual-features\/review\/?hl=31363"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":10594,
        "rank":850,
        "Model":"RegNetY-600MF",
        "mlmodel":{

        },
        "method_short":"RegNetY-600MF",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-30",
        "metrics":{
            "Top 1 Accuracy":"75.5%",
            "Number of params":"6.1M",
            "GFLOPs":"0.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.5,
            "Number of params":6100000.0,
            "GFLOPs":0.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":188625,
            "title":"Designing Network Design Spaces",
            "url":"\/paper\/designing-network-design-spaces",
            "published":"2020-03-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/designing-network-design-spaces\/review\/?hl=10594"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":5679,
        "rank":851,
        "Model":"ShuffleNet V2",
        "mlmodel":{

        },
        "method_short":"ShuffleNet V2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-07-30",
        "metrics":{
            "Top 1 Accuracy":"75.4%",
            "Number of params":null,
            "GFLOPs":"0.597",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.4,
            "Number of params":null,
            "GFLOPs":0.597,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":53827,
            "title":"ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design",
            "url":"\/paper\/shufflenet-v2-practical-guidelines-for",
            "published":"2018-07-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/shufflenet-v2-practical-guidelines-for\/review\/?hl=5679"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":61461,
        "rank":852,
        "Model":"VAN-B0",
        "mlmodel":{

        },
        "method_short":"VAN-B0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-20",
        "metrics":{
            "Top 1 Accuracy":"75.4%",
            "Number of params":"4.1M",
            "GFLOPs":"0.9",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.4,
            "Number of params":4100000.0,
            "GFLOPs":0.9,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":964888,
            "title":"Visual Attention Network",
            "url":"\/paper\/visual-attention-network",
            "published":"2022-02-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/visual-attention-network\/review\/?hl=61461"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60688,
        "rank":853,
        "Model":"AsymmNet-Large \u00d71.0",
        "mlmodel":{

        },
        "method_short":"AsymmNet-Large \u00d71.0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-15",
        "metrics":{
            "Top 1 Accuracy":"75.4%",
            "Number of params":"5.99M",
            "GFLOPs":"0.4338",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.4,
            "Number of params":5990000.0,
            "GFLOPs":0.4338,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":783606,
            "title":"AsymmNet: Towards ultralight convolution neural networks using asymmetrical bottlenecks",
            "url":"\/paper\/asymmnet-towards-ultralight-convolution",
            "published":"2021-04-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/asymmnet-towards-ultralight-convolution\/review\/?hl=60688"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":57957,
        "rank":854,
        "Model":"FairNAS-A",
        "mlmodel":{

        },
        "method_short":"FairNAS-A",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-07-03",
        "metrics":{
            "Top 1 Accuracy":"75.34%",
            "Number of params":"4.6M",
            "GFLOPs":"0.776",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.34,
            "Number of params":4600000.0,
            "GFLOPs":0.776,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":144724,
            "title":"FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search",
            "url":"\/paper\/fairnas-rethinking-evaluation-fairness-of",
            "published":"2019-07-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fairnas-rethinking-evaluation-fairness-of\/review\/?hl=57957"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":35597,
        "rank":855,
        "Model":"ViTAE-T",
        "mlmodel":{

        },
        "method_short":"ViTAE-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-07",
        "metrics":{
            "Top 1 Accuracy":"75.3%",
            "Number of params":null,
            "GFLOPs":"3.0",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.3,
            "Number of params":null,
            "GFLOPs":3.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":812332,
            "title":"ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias",
            "url":"\/paper\/vitae-vision-transformer-advanced-by",
            "published":"2021-06-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vitae-vision-transformer-advanced-by\/review\/?hl=35597"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":16115,
        "rank":856,
        "Model":"MUXNet-m",
        "mlmodel":{

        },
        "method_short":"MUXNet-m",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-31",
        "metrics":{
            "Top 1 Accuracy":"75.3%",
            "Number of params":"3.4M",
            "GFLOPs":"0.436",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.3,
            "Number of params":3400000.0,
            "GFLOPs":0.436,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":188988,
            "title":"MUXConv: Information Multiplexing in Convolutional Neural Networks",
            "url":"\/paper\/muxconv-information-multiplexing-in",
            "published":"2020-03-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/muxconv-information-multiplexing-in\/review\/?hl=16115"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6136,
        "rank":857,
        "Model":"ResNet-50",
        "mlmodel":{

        },
        "method_short":"ResNet-50",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2015-12-10",
        "metrics":{
            "Top 1 Accuracy":"75.3%",
            "Number of params":"25M",
            "GFLOPs":"3.8",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.3,
            "Number of params":25000000.0,
            "GFLOPs":3.8,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":37118,
            "title":"Deep Residual Learning for Image Recognition",
            "url":"\/paper\/deep-residual-learning-for-image-recognition",
            "published":"2015-12-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/deep-residual-learning-for-image-recognition\/review\/?hl=6136"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6231,
        "rank":858,
        "Model":"MnasNet-A1",
        "mlmodel":{

        },
        "method_short":"MnasNet-A1",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-07-31",
        "metrics":{
            "Top 1 Accuracy":"75.2%",
            "Number of params":"3.9M",
            "GFLOPs":"0.624",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.2,
            "Number of params":3900000.0,
            "GFLOPs":0.624,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":53964,
            "title":"MnasNet: Platform-Aware Neural Architecture Search for Mobile",
            "url":"\/paper\/mnasnet-platform-aware-neural-architecture",
            "published":"2018-07-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mnasnet-platform-aware-neural-architecture\/review\/?hl=6231"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6235,
        "rank":859,
        "Model":"MobileNet V3-Large 1.0",
        "mlmodel":{

        },
        "method_short":"MobileNet V3-Large 1.0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-05-06",
        "metrics":{
            "Top 1 Accuracy":"75.2%",
            "Number of params":"5.4M",
            "GFLOPs":"0.438",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.2,
            "Number of params":5400000.0,
            "GFLOPs":0.438,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":113867,
            "title":"Searching for MobileNetV3",
            "url":"\/paper\/searching-for-mobilenetv3",
            "published":"2019-05-06T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/searching-for-mobilenetv3\/review\/?hl=6235"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[
            {
                "url":"https:\/\/github.com\/facebookresearch\/AugLy\/blob\/main\/examples\/imagenet\/evaluate_robustness_imagenet_mobilenetv3_large_100.ipynb"
            }
        ]
    },
    {
        "table_id":116,
        "row_id":5472,
        "rank":860,
        "Model":"DiCENet",
        "mlmodel":{

        },
        "method_short":"DiCENet",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-06-08",
        "metrics":{
            "Top 1 Accuracy":"75.1%",
            "Number of params":null,
            "GFLOPs":"0.553",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.1,
            "Number of params":null,
            "GFLOPs":0.553,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":142131,
            "title":"DiCENet: Dimension-wise Convolutions for Efficient Networks",
            "url":"\/paper\/dicenet-dimension-wise-convolutions-for",
            "published":"2019-06-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dicenet-dimension-wise-convolutions-for\/review\/?hl=5472"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":5010,
        "rank":861,
        "Model":"MultiGrain NASNet-A-Mobile (350px)",
        "mlmodel":{

        },
        "method_short":"MultiGrain NASNet-A-Mobile ",
        "method_details":"350px",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-02-14",
        "metrics":{
            "Top 1 Accuracy":"75.1%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.1,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":105902,
            "title":"MultiGrain: a unified image embedding for classes and instances",
            "url":"\/paper\/multigrain-a-unified-image-embedding-for",
            "published":"2019-02-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/multigrain-a-unified-image-embedding-for\/review\/?hl=5010"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":57958,
        "rank":862,
        "Model":"FairNAS-B",
        "mlmodel":{

        },
        "method_short":"FairNAS-B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-07-03",
        "metrics":{
            "Top 1 Accuracy":"75.10%",
            "Number of params":"4.5M",
            "GFLOPs":"0.690",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.1,
            "Number of params":4500000.0,
            "GFLOPs":0.69,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":144724,
            "title":"FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search",
            "url":"\/paper\/fairnas-rethinking-evaluation-fairness-of",
            "published":"2019-07-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fairnas-rethinking-evaluation-fairness-of\/review\/?hl=57958"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":34276,
        "rank":863,
        "Model":"ResNet-34 (X-volution, stage3)",
        "mlmodel":{

        },
        "method_short":"ResNet-34 ",
        "method_details":"X-volution, stage3",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-04",
        "metrics":{
            "Top 1 Accuracy":"75%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.0,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":811671,
            "title":"X-volution: On the unification of convolution and self-attention",
            "url":"\/paper\/x-volution-on-the-unification-of-convolution",
            "published":"2021-06-04T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/x-volution-on-the-unification-of-convolution\/review\/?hl=34276"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60592,
        "rank":864,
        "Model":"Ghost-ResNet-50 (s=2)",
        "mlmodel":{

        },
        "method_short":"Ghost-ResNet-50 ",
        "method_details":"s=2",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-27",
        "metrics":{
            "Top 1 Accuracy":"75%",
            "Number of params":"13M",
            "GFLOPs":"2.2",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":75.0,
            "Number of params":13000000.0,
            "GFLOPs":2.2,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":174553,
            "title":"GhostNet: More Features from Cheap Operations",
            "url":"\/paper\/ghostnet-more-features-from-cheap-operations",
            "published":"2019-11-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ghostnet-more-features-from-cheap-operations\/review\/?hl=60592"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6156,
        "rank":865,
        "Model":"DenseNet-121",
        "mlmodel":{

        },
        "method_short":"DenseNet-121",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-08-25",
        "metrics":{
            "Top 1 Accuracy":"74.98%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":74.98,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":11275,
            "title":"Densely Connected Convolutional Networks",
            "url":"\/paper\/densely-connected-convolutional-networks",
            "published":"2016-08-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/densely-connected-convolutional-networks\/review\/?hl=6156"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6236,
        "rank":866,
        "Model":"Single-Path NAS",
        "mlmodel":{

        },
        "method_short":"Single-Path NAS",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-05",
        "metrics":{
            "Top 1 Accuracy":"74.96%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":74.96,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":110700,
            "title":"Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours",
            "url":"\/paper\/single-path-nas-designing-hardware-efficient",
            "published":"2019-04-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/single-path-nas-designing-hardware-efficient\/review\/?hl=6236"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":95555,
        "rank":867,
        "Model":"WaveMix-192\/16 (level 3)",
        "mlmodel":{

        },
        "method_short":"WaveMix-192\/16 ",
        "method_details":"level 3",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-28",
        "metrics":{
            "Top 1 Accuracy":"74.93%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":74.93,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1017845,
            "title":"WaveMix: A Resource-efficient Neural Network for Image Analysis",
            "url":"\/paper\/wavemix-lite-a-resource-efficient-neural",
            "published":"2022-05-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/wavemix-lite-a-resource-efficient-neural\/review\/?hl=95555"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":31685,
        "rank":868,
        "Model":"FF",
        "mlmodel":{

        },
        "method_short":"FF",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-06",
        "metrics":{
            "Top 1 Accuracy":"74.9",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":74.9,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":794637,
            "title":"Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet",
            "url":"\/paper\/do-you-even-need-attention-a-stack-of-feed",
            "published":"2021-05-06T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6225,
        "rank":869,
        "Model":"FBNet-C",
        "mlmodel":{

        },
        "method_short":"FBNet-C",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-12-09",
        "metrics":{
            "Top 1 Accuracy":"74.9%",
            "Number of params":"5.5M",
            "GFLOPs":"0.375",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":74.9,
            "Number of params":5500000.0,
            "GFLOPs":0.375,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":64847,
            "title":"FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search",
            "url":"\/paper\/fbnet-hardware-aware-efficient-convnet-design",
            "published":"2018-12-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fbnet-hardware-aware-efficient-convnet-design\/review\/?hl=6225"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":5482,
        "rank":870,
        "Model":"ESPNetv2",
        "mlmodel":{

        },
        "method_short":"ESPNetv2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-11-28",
        "metrics":{
            "Top 1 Accuracy":"74.9%",
            "Number of params":"5.9M",
            "GFLOPs":"0.602",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":74.9,
            "Number of params":5900000.0,
            "GFLOPs":0.602,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":63634,
            "title":"ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network",
            "url":"\/paper\/espnetv2-a-light-weight-power-efficient-and",
            "published":"2018-11-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/espnetv2-a-light-weight-power-efficient-and\/review\/?hl=5482"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60768,
        "rank":871,
        "Model":"MobileViT-XS",
        "mlmodel":{

        },
        "method_short":"MobileViT-XS",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-10-05",
        "metrics":{
            "Top 1 Accuracy":"74.8%",
            "Number of params":"2.3M",
            "GFLOPs":"0.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":74.8,
            "Number of params":2300000.0,
            "GFLOPs":0.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":879564,
            "title":"MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer",
            "url":"\/paper\/mobilevit-light-weight-general-purpose-and",
            "published":"2021-10-05T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29950,
        "rank":872,
        "Model":"LocalViT-T",
        "mlmodel":{

        },
        "method_short":"LocalViT-T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-12",
        "metrics":{
            "Top 1 Accuracy":"74.8%",
            "Number of params":"5.9M",
            "GFLOPs":"1.3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":74.8,
            "Number of params":5900000.0,
            "GFLOPs":1.3,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":778779,
            "title":"LocalViT: Bringing Locality to Vision Transformers",
            "url":"\/paper\/localvit-bringing-locality-to-vision",
            "published":"2021-04-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/localvit-bringing-locality-to-vision\/review\/?hl=29950"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60603,
        "rank":873,
        "Model":"RandWire-WS (small)",
        "mlmodel":{

        },
        "method_short":"RandWire-WS ",
        "method_details":"small",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-04-02",
        "metrics":{
            "Top 1 Accuracy":"74.7%",
            "Number of params":"5.6M",
            "GFLOPs":"0.583",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":74.7,
            "Number of params":5600000.0,
            "GFLOPs":0.583,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":110191,
            "title":"Exploring Randomly Wired Neural Networks for Image Recognition",
            "url":"\/paper\/exploring-randomly-wired-neural-networks-for",
            "published":"2019-04-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/exploring-randomly-wired-neural-networks-for\/review\/?hl=60603"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":37553,
        "rank":874,
        "Model":"AutoFormer-tiny",
        "mlmodel":{

        },
        "method_short":"AutoFormer-tiny",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-01",
        "metrics":{
            "Top 1 Accuracy":"74.7%",
            "Number of params":"5.7M",
            "GFLOPs":"1.3",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":74.7,
            "Number of params":5700000.0,
            "GFLOPs":1.3,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":828740,
            "title":"AutoFormer: Searching Transformers for Visual Recognition",
            "url":"\/paper\/autoformer-searching-transformers-for-visual",
            "published":"2021-07-01T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/autoformer-searching-transformers-for-visual\/review\/?hl=37553"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":5387,
        "rank":875,
        "Model":"MobileNetV2 (1.4)",
        "mlmodel":{

        },
        "method_short":"MobileNetV2 ",
        "method_details":"1.4",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-01-13",
        "metrics":{
            "Top 1 Accuracy":"74.7%",
            "Number of params":"6.9M",
            "GFLOPs":"1.170",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":74.7,
            "Number of params":6900000.0,
            "GFLOPs":1.17,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":6891,
            "title":"MobileNetV2: Inverted Residuals and Linear Bottlenecks",
            "url":"\/paper\/mobilenetv2-inverted-residuals-and-linear",
            "published":"2018-01-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mobilenetv2-inverted-residuals-and-linear\/review\/?hl=5387"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[
            {
                "url":"https:\/\/github.com\/facebookresearch\/AugLy\/blob\/main\/examples\/imagenet\/evaluate_robustness_imagenet_mobilenet_v2.ipynb"
            }
        ]
    },
    {
        "table_id":116,
        "row_id":57959,
        "rank":876,
        "Model":"FairNAS-C",
        "mlmodel":{

        },
        "method_short":"FairNAS-C",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-07-03",
        "metrics":{
            "Top 1 Accuracy":"74.69%",
            "Number of params":"4.4M",
            "GFLOPs":"0.642",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":74.69,
            "Number of params":4400000.0,
            "GFLOPs":0.642,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":144724,
            "title":"FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search",
            "url":"\/paper\/fairnas-rethinking-evaluation-fairness-of",
            "published":"2019-07-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/fairnas-rethinking-evaluation-fairness-of\/review\/?hl=57959"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":49045,
        "rank":877,
        "Model":"ReXNet_0.6",
        "mlmodel":{

        },
        "method_short":"ReXNet_0.6",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-07-02",
        "metrics":{
            "Top 1 Accuracy":"74.6%",
            "Number of params":"2.7M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":74.6,
            "Number of params":2700000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":206716,
            "title":"Rethinking Channel Dimensions for Efficient Model Design",
            "url":"\/paper\/rexnet-diminishing-representational",
            "published":"2020-07-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rexnet-diminishing-representational\/review\/?hl=49045"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":11223,
        "rank":878,
        "Model":"Proxyless",
        "mlmodel":{

        },
        "method_short":"Proxyless",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-12-02",
        "metrics":{
            "Top 1 Accuracy":"74.6%",
            "Number of params":"4.0M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":74.6,
            "Number of params":4000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":64252,
            "title":"ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware",
            "url":"\/paper\/proxylessnas-direct-neural-architecture",
            "published":"2018-12-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/proxylessnas-direct-neural-architecture\/review\/?hl=11223"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29407,
        "rank":879,
        "Model":"PiT-Ti",
        "mlmodel":{

        },
        "method_short":"PiT-Ti",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-30",
        "metrics":{
            "Top 1 Accuracy":"74.6%",
            "Number of params":"4.9M",
            "GFLOPs":"0.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":74.6,
            "Number of params":4900000.0,
            "GFLOPs":0.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":771780,
            "title":"Rethinking Spatial Dimensions of Vision Transformers",
            "url":"\/paper\/rethinking-spatial-dimensions-of-vision",
            "published":"2021-03-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-spatial-dimensions-of-vision\/review\/?hl=29407"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":19544,
        "rank":880,
        "Model":"DY-MobileNetV2 \u00d71.0",
        "mlmodel":{

        },
        "method_short":"DY-MobileNetV2 \u00d71.0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-12-07",
        "metrics":{
            "Top 1 Accuracy":"74.4%",
            "Number of params":"11.1M",
            "GFLOPs":"0,626",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":74.4,
            "Number of params":11100000.0,
            "GFLOPs":0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":175971,
            "title":"Dynamic Convolution: Attention over Convolution Kernels",
            "url":"\/paper\/dynamic-convolution-attention-over",
            "published":"2019-12-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dynamic-convolution-attention-over\/review\/?hl=19544"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":97057,
        "rank":881,
        "Model":"SimpleNetV1-9m",
        "mlmodel":{

        },
        "method_short":"SimpleNetV1-9m",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-08-22",
        "metrics":{
            "Top 1 Accuracy":"74.17",
            "Number of params":"9.5M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":74.17,
            "Number of params":9500000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":10271,
            "title":"Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures",
            "url":"\/paper\/lets-keep-it-simple-using-simple",
            "published":"2016-08-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lets-keep-it-simple-using-simple\/review\/?hl=97057"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60585,
        "rank":882,
        "Model":"RegNetY-400MF",
        "mlmodel":{

        },
        "method_short":"RegNetY-400MF",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-30",
        "metrics":{
            "Top 1 Accuracy":"74.1%",
            "Number of params":"4.3M",
            "GFLOPs":"0.4",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":74.1,
            "Number of params":4300000.0,
            "GFLOPs":0.4,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":188625,
            "title":"Designing Network Design Spaces",
            "url":"\/paper\/designing-network-design-spaces",
            "published":"2020-03-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/designing-network-design-spaces\/review\/?hl=60585"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60593,
        "rank":883,
        "Model":"Ghost-ResNet-50 (s=4)",
        "mlmodel":{

        },
        "method_short":"Ghost-ResNet-50 ",
        "method_details":"s=4",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-27",
        "metrics":{
            "Top 1 Accuracy":"74.1%",
            "Number of params":"6.5M",
            "GFLOPs":"1.2",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":74.1,
            "Number of params":6500000.0,
            "GFLOPs":1.2,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":174553,
            "title":"GhostNet: More Features from Cheap Operations",
            "url":"\/paper\/ghostnet-more-features-from-cheap-operations",
            "published":"2019-11-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ghostnet-more-features-from-cheap-operations\/review\/?hl=60593"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60635,
        "rank":884,
        "Model":"SReT-ExT",
        "mlmodel":{

        },
        "method_short":"SReT-ExT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-09",
        "metrics":{
            "Top 1 Accuracy":"74.0%",
            "Number of params":"4M",
            "GFLOPs":"0.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":74.0,
            "Number of params":4000000.0,
            "GFLOPs":0.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":907412,
            "title":"Sliced Recursive Transformer",
            "url":"\/paper\/sliced-recursive-transformer-1",
            "published":"2021-11-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/sliced-recursive-transformer-1\/review\/?hl=60635"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":57969,
        "rank":885,
        "Model":"GhostNet \u00d71.0",
        "mlmodel":{

        },
        "method_short":"GhostNet \u00d71.0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-27",
        "metrics":{
            "Top 1 Accuracy":"73.9%",
            "Number of params":"5.2M",
            "GFLOPs":"0.141",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":73.9,
            "Number of params":5200000.0,
            "GFLOPs":0.141,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":174553,
            "title":"GhostNet: More Features from Cheap Operations",
            "url":"\/paper\/ghostnet-more-features-from-cheap-operations",
            "published":"2019-11-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ghostnet-more-features-from-cheap-operations\/review\/?hl=57969"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":108976,
        "rank":886,
        "Model":"DeiT-T (+MixPro)",
        "mlmodel":{

        },
        "method_short":"DeiT-T ",
        "method_details":"+MixPro",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-24",
        "metrics":{
            "Top 1 Accuracy":"73.8%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":73.8,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1195795,
            "title":"MixPro: Data Augmentation with MaskMix and Progressive Attention Labeling for Vision Transformer",
            "url":"\/paper\/mixpro-data-augmentation-with-maskmix-and",
            "published":"2023-04-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mixpro-data-augmentation-with-maskmix-and\/review\/?hl=108976"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":37853,
        "rank":887,
        "Model":"DeiT-Ti with iRPE-K",
        "mlmodel":{

        },
        "method_short":"DeiT-Ti with iRPE-K",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-07-29",
        "metrics":{
            "Top 1 Accuracy":"73.7%",
            "Number of params":"6M",
            "GFLOPs":"2.568",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":73.7,
            "Number of params":6000000.0,
            "GFLOPs":2.568,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":843509,
            "title":"Rethinking and Improving Relative Position Encoding for Vision Transformer",
            "url":"\/paper\/rethinking-and-improving-relative-position",
            "published":"2021-07-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/rethinking-and-improving-relative-position\/review\/?hl=37853"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":71921,
        "rank":888,
        "Model":"TransBoost-ResNet18",
        "mlmodel":{

        },
        "method_short":"TransBoost-ResNet18",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-26",
        "metrics":{
            "Top 1 Accuracy":"73.36%",
            "Number of params":"11.69M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":73.36,
            "Number of params":11690000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1016638,
            "title":"TransBoost: Improving the Best ImageNet Performance using Deep Transduction",
            "url":"\/paper\/transboost-improving-the-best-imagenet",
            "published":"2022-05-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/transboost-improving-the-best-imagenet\/review\/?hl=71921"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":9044,
        "rank":889,
        "Model":"Wide ResNet-50 (edge-popup)",
        "mlmodel":{

        },
        "method_short":"Wide ResNet-50 ",
        "method_details":"edge-popup",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-29",
        "metrics":{
            "Top 1 Accuracy":"73.3%",
            "Number of params":"20.6M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":73.3,
            "Number of params":20600000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":174702,
            "title":"What's Hidden in a Randomly Weighted Neural Network?",
            "url":"\/paper\/whats-hidden-in-a-randomly-weighted-neural",
            "published":"2019-11-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/whats-hidden-in-a-randomly-weighted-neural\/review\/?hl=9044"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":44127,
        "rank":890,
        "Model":"ResNet-18 (MEAL V2)",
        "mlmodel":{

        },
        "method_short":"ResNet-18 ",
        "method_details":"MEAL V2",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-09-17",
        "metrics":{
            "Top 1 Accuracy":"73.19%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":73.19,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":218270,
            "title":"MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks",
            "url":"\/paper\/meal-v2-boosting-vanilla-resnet-50-to-80-top",
            "published":"2020-09-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/meal-v2-boosting-vanilla-resnet-50-to-80-top\/review\/?hl=44127"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":28358,
        "rank":891,
        "Model":"ConViT-Ti",
        "mlmodel":{

        },
        "method_short":"ConViT-Ti",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-19",
        "metrics":{
            "Top 1 Accuracy":"73.1%",
            "Number of params":"6M",
            "GFLOPs":"1",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":73.1,
            "Number of params":6000000.0,
            "GFLOPs":1.0,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755434,
            "title":"ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases",
            "url":"\/paper\/convit-improving-vision-transformers-with",
            "published":"2021-03-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/convit-improving-vision-transformers-with\/review\/?hl=28358"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":64200,
        "rank":892,
        "Model":"RevBiFPN-S0",
        "mlmodel":{

        },
        "method_short":"RevBiFPN-S0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-28",
        "metrics":{
            "Top 1 Accuracy":"72.8%",
            "Number of params":"3.42M",
            "GFLOPs":"0.31",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":72.8,
            "Number of params":3420000.0,
            "GFLOPs":0.31,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1034397,
            "title":"RevBiFPN: The Fully Reversible Bidirectional Feature Pyramid Network",
            "url":"\/paper\/revbifpn-the-fully-reversible-bidirectional",
            "published":"2022-06-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/revbifpn-the-fully-reversible-bidirectional\/review\/?hl=64200"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":283,
                "name":"Reversible",
                "color":"#3227d3"
            },
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":13,
                "name":"FPN",
                "color":"#009481"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":19545,
        "rank":893,
        "Model":"DY-MobileNetV2 \u00d70.75",
        "mlmodel":{

        },
        "method_short":"DY-MobileNetV2 \u00d70.75",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-12-07",
        "metrics":{
            "Top 1 Accuracy":"72.8%",
            "Number of params":"7M",
            "GFLOPs":"0.435",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":72.8,
            "Number of params":7000000.0,
            "GFLOPs":0.435,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":175971,
            "title":"Dynamic Convolution: Attention over Convolution Kernels",
            "url":"\/paper\/dynamic-convolution-attention-over",
            "published":"2019-12-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dynamic-convolution-attention-over\/review\/?hl=19545"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":19549,
        "rank":894,
        "Model":"DY-ResNet-18",
        "mlmodel":{

        },
        "method_short":"DY-ResNet-18",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-12-07",
        "metrics":{
            "Top 1 Accuracy":"72.7%",
            "Number of params":"42.7M",
            "GFLOPs":"3.7",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":72.7,
            "Number of params":42700000.0,
            "GFLOPs":3.7,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":175971,
            "title":"Dynamic Convolution: Attention over Convolution Kernels",
            "url":"\/paper\/dynamic-convolution-attention-over",
            "published":"2019-12-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dynamic-convolution-attention-over\/review\/?hl=19549"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":8263,
        "rank":895,
        "Model":"ECA-Net (MobileNetV2)",
        "mlmodel":{

        },
        "method_short":"ECA-Net ",
        "method_details":"MobileNetV2",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-10-08",
        "metrics":{
            "Top 1 Accuracy":"72.56%",
            "Number of params":"3.34M",
            "GFLOPs":"0.320",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":72.56,
            "Number of params":3340000.0,
            "GFLOPs":0.32,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":157542,
            "title":"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks",
            "url":"\/paper\/eca-net-efficient-channel-attention-for-deep",
            "published":"2019-10-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/eca-net-efficient-channel-attention-for-deep\/review\/?hl=8263"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":6033,
        "rank":896,
        "Model":"MobileNet-224 (CGD)",
        "mlmodel":{

        },
        "method_short":"MobileNet-224 ",
        "method_details":"CGD",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-07-23",
        "metrics":{
            "Top 1 Accuracy":"72.56%",
            "Number of params":"4.26M",
            "GFLOPs":"1.198",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":72.56,
            "Number of params":4260000.0,
            "GFLOPs":1.198,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":146618,
            "title":"Compact Global Descriptor for Neural Networks",
            "url":"\/paper\/compact-global-descriptor-for-neural-networks",
            "published":"2019-07-23T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/compact-global-descriptor-for-neural-networks\/review\/?hl=6033"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":105107,
        "rank":897,
        "Model":"MobileOne-S0 (distill)",
        "mlmodel":{

        },
        "method_short":"MobileOne-S0 ",
        "method_details":"distill",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-08",
        "metrics":{
            "Top 1 Accuracy":"72.5%",
            "Number of params":"2.1M",
            "GFLOPs":"0.275",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":72.5,
            "Number of params":2100000.0,
            "GFLOPs":0.275,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1023967,
            "title":"MobileOne: An Improved One millisecond Mobile Backbone",
            "url":"\/paper\/an-improved-one-millisecond-mobile-backbone",
            "published":"2022-06-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/an-improved-one-millisecond-mobile-backbone\/review\/?hl=105107"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":29952,
        "rank":898,
        "Model":"LocalViT-T2T",
        "mlmodel":{

        },
        "method_short":"LocalViT-T2T",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-12",
        "metrics":{
            "Top 1 Accuracy":"72.5%",
            "Number of params":"4.3M",
            "GFLOPs":"1.2",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":72.5,
            "Number of params":4300000.0,
            "GFLOPs":1.2,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":778779,
            "title":"LocalViT: Bringing Locality to Vision Transformers",
            "url":"\/paper\/localvit-bringing-locality-to-vision",
            "published":"2021-04-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/localvit-bringing-locality-to-vision\/review\/?hl=29952"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":73068,
        "rank":899,
        "Model":"MobileViTv3-0.5",
        "mlmodel":{

        },
        "method_short":"MobileViTv3-0.5",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-30",
        "metrics":{
            "Top 1 Accuracy":"72.33%",
            "Number of params":"1.4M",
            "GFLOPs":"0.481",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":72.33,
            "Number of params":1400000.0,
            "GFLOPs":0.481,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1084483,
            "title":"MobileViTv3: Mobile-Friendly Vision Transformer with Simple and Effective Fusion of Local, Global and Input Features",
            "url":"\/paper\/mobilevitv3-mobile-friendly-vision",
            "published":"2022-09-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mobilevitv3-mobile-friendly-vision\/review\/?hl=73068"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":160,
                "name":"CNN+Transformer",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":45004,
        "rank":900,
        "Model":"ResNet-18 (SAMix)",
        "mlmodel":{

        },
        "method_short":"ResNet-18 ",
        "method_details":"SAMix",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-30",
        "metrics":{
            "Top 1 Accuracy":"72.33%",
            "Number of params":"11.7M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":72.33,
            "Number of params":11700000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":923048,
            "title":"Boosting Discriminative Visual Representation Learning with Scenario-Agnostic Mixup",
            "url":"\/paper\/boosting-discriminative-visual-representation",
            "published":"2021-11-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/boosting-discriminative-visual-representation\/review\/?hl=45004"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":8686,
        "rank":901,
        "Model":"ResNet-50",
        "mlmodel":{

        },
        "method_short":"ResNet-50",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-10-09",
        "metrics":{
            "Top 1 Accuracy":"72.1%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":72.1,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":157871,
            "title":"On the adequacy of untuned warmup for adaptive optimization",
            "url":"\/paper\/on-the-adequacy-of-untuned-warmup-for",
            "published":"2019-10-09T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":44998,
        "rank":902,
        "Model":"ResNet-18 (AutoMix)",
        "mlmodel":{

        },
        "method_short":"ResNet-18 ",
        "method_details":"AutoMix",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-24",
        "metrics":{
            "Top 1 Accuracy":"72.05%",
            "Number of params":"11.7M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":72.05,
            "Number of params":11700000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":756879,
            "title":"AutoMix: Unveiling the Power of Mixup for Stronger Classifiers",
            "url":"\/paper\/automix-unveiling-the-power-of-mixup",
            "published":"2021-03-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/automix-unveiling-the-power-of-mixup\/review\/?hl=44998"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60597,
        "rank":903,
        "Model":"MobileNetV2",
        "mlmodel":{

        },
        "method_short":"MobileNetV2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2018-01-13",
        "metrics":{
            "Top 1 Accuracy":"72%",
            "Number of params":"3.4M",
            "GFLOPs":"0.600",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":72.0,
            "Number of params":3400000.0,
            "GFLOPs":0.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":6891,
            "title":"MobileNetV2: Inverted Residuals and Linear Bottlenecks",
            "url":"\/paper\/mobilenetv2-inverted-residuals-and-linear",
            "published":"2018-01-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mobilenetv2-inverted-residuals-and-linear\/review\/?hl=60597"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":64242,
        "rank":904,
        "Model":"Ours",
        "mlmodel":{

        },
        "method_short":"Ours",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-09-10",
        "metrics":{
            "Top 1 Accuracy":"71.97%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":71.97,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":217427,
            "title":"QuantNet: Learning to Quantize by Learning within Fully Differentiable Framework",
            "url":"\/paper\/quantnet-learning-to-quantize-by-learning",
            "published":"2020-09-10T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/quantnet-learning-to-quantize-by-learning\/review\/?hl=64242"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":97056,
        "rank":905,
        "Model":"SimpleNetV1-5m",
        "mlmodel":{

        },
        "method_short":"SimpleNetV1-5m",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-08-22",
        "metrics":{
            "Top 1 Accuracy":"71.94",
            "Number of params":"5.7M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":71.94,
            "Number of params":5700000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":10271,
            "title":"Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures",
            "url":"\/paper\/lets-keep-it-simple-using-simple",
            "published":"2016-08-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lets-keep-it-simple-using-simple\/review\/?hl=97056"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":22004,
        "rank":906,
        "Model":"ResNet-18 (PAD-L2 w\/ ResNet-34 teacher)",
        "mlmodel":{

        },
        "method_short":"ResNet-18 ",
        "method_details":"PAD-L2 w\/ ResNet-34 teacher",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-11-25",
        "metrics":{
            "Top 1 Accuracy":"71.71%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":71.71,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":237343,
            "title":"torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation",
            "url":"\/paper\/torchdistill-a-modular-configuration-driven",
            "published":"2020-11-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/torchdistill-a-modular-configuration-driven\/review\/?hl=22004"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":16114,
        "rank":907,
        "Model":"MUXNet-s",
        "mlmodel":{

        },
        "method_short":"MUXNet-s",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-31",
        "metrics":{
            "Top 1 Accuracy":"71.6%",
            "Number of params":"2.4M",
            "GFLOPs":"0.234",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":71.6,
            "Number of params":2400000.0,
            "GFLOPs":0.234,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":188988,
            "title":"MUXConv: Information Multiplexing in Convolutional Neural Networks",
            "url":"\/paper\/muxconv-information-multiplexing-in",
            "published":"2020-03-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/muxconv-information-multiplexing-in\/review\/?hl=16114"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":64250,
        "rank":908,
        "Model":"PDC",
        "mlmodel":{

        },
        "method_short":"PDC",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-16",
        "metrics":{
            "Top 1 Accuracy":"71.6%",
            "Number of params":"11.51M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":71.6,
            "Number of params":11510000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":783593,
            "title":"Augmenting Deep Classifiers with Polynomial Neural Networks",
            "url":"\/paper\/polynomial-networks-in-deep-classifiers",
            "published":"2021-04-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/polynomial-networks-in-deep-classifiers\/review\/?hl=64250"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":21999,
        "rank":909,
        "Model":"ResNet-18 (FT w\/ ResNet-34 teacher)",
        "mlmodel":{

        },
        "method_short":"ResNet-18 ",
        "method_details":"FT w\/ ResNet-34 teacher",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-11-25",
        "metrics":{
            "Top 1 Accuracy":"71.56%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":71.56,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":237343,
            "title":"torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation",
            "url":"\/paper\/torchdistill-a-modular-configuration-driven",
            "published":"2020-11-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/torchdistill-a-modular-configuration-driven\/review\/?hl=21999"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":108035,
        "rank":910,
        "Model":"EfficientFormer-V2-S0",
        "mlmodel":{

        },
        "method_short":"EfficientFormer-V2-S0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-18",
        "metrics":{
            "Top 1 Accuracy":"71.53%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":71.53,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1265182,
            "title":"Which Transformer to Favor: A Comparative Analysis of Efficiency in Vision Transformers",
            "url":"\/paper\/which-transformer-to-favor-a-comparative",
            "published":"2023-08-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/which-transformer-to-favor-a-comparative\/review\/?hl=108035"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":105106,
        "rank":911,
        "Model":"MobileOne-S0",
        "mlmodel":{

        },
        "method_short":"MobileOne-S0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-08",
        "metrics":{
            "Top 1 Accuracy":"71.4%",
            "Number of params":"2.1M",
            "GFLOPs":"0.275",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":71.4,
            "Number of params":2100000.0,
            "GFLOPs":0.275,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1023967,
            "title":"MobileOne: An Improved One millisecond Mobile Backbone",
            "url":"\/paper\/an-improved-one-millisecond-mobile-backbone",
            "published":"2022-06-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/an-improved-one-millisecond-mobile-backbone\/review\/?hl=105106"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":21997,
        "rank":912,
        "Model":"ResNet-18 (KD w\/ ResNet-34 teacher)",
        "mlmodel":{

        },
        "method_short":"ResNet-18 ",
        "method_details":"KD w\/ ResNet-34 teacher",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-11-25",
        "metrics":{
            "Top 1 Accuracy":"71.37%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":71.37,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":237343,
            "title":"torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation",
            "url":"\/paper\/torchdistill-a-modular-configuration-driven",
            "published":"2020-11-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/torchdistill-a-modular-configuration-driven\/review\/?hl=21997"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":103900,
        "rank":913,
        "Model":"Dspike (VGG-16)",
        "mlmodel":{

        },
        "method_short":"Dspike ",
        "method_details":"VGG-16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-01",
        "metrics":{
            "Top 1 Accuracy":"71.24",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":71.24,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":914188,
            "title":"Differentiable Spike: Rethinking Gradient-Descent for Training Spiking Neural Networks",
            "url":"\/paper\/differentiable-spike-rethinking-gradient",
            "published":"2021-12-01T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":446,
                "name":"SNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":57375,
        "rank":914,
        "Model":"EdgeNeXt-XXS",
        "mlmodel":{

        },
        "method_short":"EdgeNeXt-XXS",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-06-21",
        "metrics":{
            "Top 1 Accuracy":"71.2%",
            "Number of params":"1.3M",
            "GFLOPs":"0.522",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":71.2,
            "Number of params":1300000.0,
            "GFLOPs":0.522,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1029687,
            "title":"EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications",
            "url":"\/paper\/edgenext-efficiently-amalgamated-cnn",
            "published":"2022-06-21T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/edgenext-efficiently-amalgamated-cnn\/review\/?hl=57375"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":252,
                "name":"CrossCovarianceAttention",
                "color":"#2771D3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":22003,
        "rank":915,
        "Model":"ResNet-18 (L2 w\/ ResNet-34 teacher)",
        "mlmodel":{

        },
        "method_short":"ResNet-18 ",
        "method_details":"L2 w\/ ResNet-34 teacher",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-11-25",
        "metrics":{
            "Top 1 Accuracy":"71.08%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":71.08,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":237343,
            "title":"torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation",
            "url":"\/paper\/torchdistill-a-modular-configuration-driven",
            "published":"2020-11-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/torchdistill-a-modular-configuration-driven\/review\/?hl=22003"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":73071,
        "rank":916,
        "Model":"MobileViTv3-XXS",
        "mlmodel":{

        },
        "method_short":"MobileViTv3-XXS",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-30",
        "metrics":{
            "Top 1 Accuracy":"70.98%",
            "Number of params":"1.2M",
            "GFLOPs":"0.289",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":70.98,
            "Number of params":1200000.0,
            "GFLOPs":0.289,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1084483,
            "title":"MobileViTv3: Mobile-Friendly Vision Transformer with Simple and Effective Fusion of Local, Global and Input Features",
            "url":"\/paper\/mobilevitv3-mobile-friendly-vision",
            "published":"2022-09-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mobilevitv3-mobile-friendly-vision\/review\/?hl=73071"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":160,
                "name":"CNN+Transformer",
                "color":"#2771D3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":22000,
        "rank":917,
        "Model":"ResNet-18 (CRD w\/ ResNet-34 teacher)",
        "mlmodel":{

        },
        "method_short":"ResNet-18 ",
        "method_details":"CRD w\/ ResNet-34 teacher",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-11-25",
        "metrics":{
            "Top 1 Accuracy":"70.93%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":70.93,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":237343,
            "title":"torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation",
            "url":"\/paper\/torchdistill-a-modular-configuration-driven",
            "published":"2020-11-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/torchdistill-a-modular-configuration-driven\/review\/?hl=22000"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":2058,
        "rank":918,
        "Model":"ShuffleNet",
        "mlmodel":{

        },
        "method_short":"ShuffleNet",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-07-04",
        "metrics":{
            "Top 1 Accuracy":"70.9%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":70.9,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":13326,
            "title":"ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices",
            "url":"\/paper\/shufflenet-an-extremely-efficient",
            "published":"2017-07-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/shufflenet-an-extremely-efficient\/review\/?hl=2058"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":106197,
        "rank":919,
        "Model":"MobileNet-224 \u00d71.25",
        "mlmodel":{

        },
        "method_short":"MobileNet-224 \u00d71.25",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-04-17",
        "metrics":{
            "Top 1 Accuracy":"70.6%",
            "Number of params":null,
            "GFLOPs":"1.138",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":70.6,
            "Number of params":null,
            "GFLOPs":1.138,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":24007,
            "title":"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications",
            "url":"\/paper\/mobilenets-efficient-convolutional-neural",
            "published":"2017-04-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mobilenets-efficient-convolutional-neural\/review\/?hl=106197"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":103879,
        "rank":920,
        "Model":"PSN (SEW ResNet-34)",
        "mlmodel":{

        },
        "method_short":"PSN ",
        "method_details":"SEW ResNet-34",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "Top 1 Accuracy":"70.54",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":70.54,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":446,
                "name":"SNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":22018,
        "rank":921,
        "Model":"ResNet-18 (tf-KD w\/ ResNet-18 teacher)",
        "mlmodel":{

        },
        "method_short":"ResNet-18 ",
        "method_details":"tf-KD w\/ ResNet-18 teacher",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-11-25",
        "metrics":{
            "Top 1 Accuracy":"70.52%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":70.52,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":237343,
            "title":"torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation",
            "url":"\/paper\/torchdistill-a-modular-configuration-driven",
            "published":"2020-11-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/torchdistill-a-modular-configuration-driven\/review\/?hl=22018"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":36909,
        "rank":922,
        "Model":"PVTv2-B0",
        "mlmodel":{

        },
        "method_short":"PVTv2-B0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-25",
        "metrics":{
            "Top 1 Accuracy":"70.5%",
            "Number of params":"3.4M",
            "GFLOPs":"0.6",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":70.5,
            "Number of params":3400000.0,
            "GFLOPs":0.6,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":825108,
            "title":"PVT v2: Improved Baselines with Pyramid Vision Transformer",
            "url":"\/paper\/pvtv2-improved-baselines-with-pyramid-vision",
            "published":"2021-06-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/pvtv2-improved-baselines-with-pyramid-vision\/review\/?hl=36909"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":22002,
        "rank":923,
        "Model":"ResNet-18 (SSKD w\/ ResNet-34 teacher)",
        "mlmodel":{

        },
        "method_short":"ResNet-18 ",
        "method_details":"SSKD w\/ ResNet-34 teacher",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-11-25",
        "metrics":{
            "Top 1 Accuracy":"70.09%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":70.09,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":237343,
            "title":"torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation",
            "url":"\/paper\/torchdistill-a-modular-configuration-driven",
            "published":"2020-11-25T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/torchdistill-a-modular-configuration-driven\/review\/?hl=22002"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":19548,
        "rank":924,
        "Model":"DY-MobileNetV3-Small",
        "mlmodel":{

        },
        "method_short":"DY-MobileNetV3-Small",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-12-07",
        "metrics":{
            "Top 1 Accuracy":"69.7%",
            "Number of params":"4.8M",
            "GFLOPs":"0.137",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":69.7,
            "Number of params":4800000.0,
            "GFLOPs":0.137,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":175971,
            "title":"Dynamic Convolution: Attention over Convolution Kernels",
            "url":"\/paper\/dynamic-convolution-attention-over",
            "published":"2019-12-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dynamic-convolution-attention-over\/review\/?hl=19548"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":28356,
        "rank":925,
        "Model":"HVT-Ti-1",
        "mlmodel":{

        },
        "method_short":"HVT-Ti-1",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-19",
        "metrics":{
            "Top 1 Accuracy":"69.64%",
            "Number of params":"5.74M",
            "GFLOPs":"0.64",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":69.64,
            "Number of params":5740000.0,
            "GFLOPs":0.64,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":755458,
            "title":"Scalable Vision Transformers with Hierarchical Pooling",
            "url":"\/paper\/scalable-visual-transformers-with",
            "published":"2021-03-19T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scalable-visual-transformers-with\/review\/?hl=28356"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":19546,
        "rank":926,
        "Model":"DY-MobileNetV2 \u00d70.5",
        "mlmodel":{

        },
        "method_short":"DY-MobileNetV2 \u00d70.5",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-12-07",
        "metrics":{
            "Top 1 Accuracy":"69.4%",
            "Number of params":"4M",
            "GFLOPs":"0.203",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":69.4,
            "Number of params":4000000.0,
            "GFLOPs":0.203,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":175971,
            "title":"Dynamic Convolution: Attention over Convolution Kernels",
            "url":"\/paper\/dynamic-convolution-attention-over",
            "published":"2019-12-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dynamic-convolution-attention-over\/review\/?hl=19546"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60690,
        "rank":927,
        "Model":"AsymmNet-Large \u00d70.5",
        "mlmodel":{

        },
        "method_short":"AsymmNet-Large \u00d70.5",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-15",
        "metrics":{
            "Top 1 Accuracy":"69.2%",
            "Number of params":"2.8M",
            "GFLOPs":"0.1344",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":69.2,
            "Number of params":2800000.0,
            "GFLOPs":0.1344,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":783606,
            "title":"AsymmNet: Towards ultralight convolution neural networks using asymmetrical bottlenecks",
            "url":"\/paper\/asymmnet-towards-ultralight-convolution",
            "published":"2021-04-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/asymmnet-towards-ultralight-convolution\/review\/?hl=60690"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":97062,
        "rank":928,
        "Model":"SimpleNetV1-small-05-correct-labels",
        "mlmodel":{

        },
        "method_short":"SimpleNetV1-small-05-correct-labels",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-08-22",
        "metrics":{
            "Top 1 Accuracy":"69.11",
            "Number of params":"1.5M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":69.11,
            "Number of params":1500000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":10271,
            "title":"Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures",
            "url":"\/paper\/lets-keep-it-simple-using-simple",
            "published":"2016-08-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lets-keep-it-simple-using-simple\/review\/?hl=97062"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":33776,
        "rank":929,
        "Model":"Heteroscedastic (InceptionResNet-v2)",
        "mlmodel":{

        },
        "method_short":"Heteroscedastic ",
        "method_details":"InceptionResNet-v2",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-19",
        "metrics":{
            "Top 1 Accuracy":"68.6%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":68.6,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":803513,
            "title":"Correlated Input-Dependent Label Noise in Large-Scale Image Classification",
            "url":"\/paper\/correlated-input-dependent-label-noise-in",
            "published":"2021-05-19T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/correlated-input-dependent-label-noise-in\/review\/?hl=33776"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60689,
        "rank":930,
        "Model":"AsymmNet-Small \u00d71.0",
        "mlmodel":{

        },
        "method_short":"AsymmNet-Small \u00d71.0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-15",
        "metrics":{
            "Top 1 Accuracy":"68.4%",
            "Number of params":"3.1M",
            "GFLOPs":"0.1154",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":68.4,
            "Number of params":3100000.0,
            "GFLOPs":0.1154,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":783606,
            "title":"AsymmNet: Towards ultralight convolution neural networks using asymmetrical bottlenecks",
            "url":"\/paper\/asymmnet-towards-ultralight-convolution",
            "published":"2021-04-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/asymmnet-towards-ultralight-convolution\/review\/?hl=60689"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":54265,
        "rank":931,
        "Model":"FireCaffe (GoogLeNet)",
        "mlmodel":{

        },
        "method_short":"FireCaffe ",
        "method_details":"GoogLeNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2015-10-31",
        "metrics":{
            "Top 1 Accuracy":"68.3%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":68.3,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":36686,
            "title":"FireCaffe: near-linear acceleration of deep neural network training on compute clusters",
            "url":"\/paper\/firecaffe-near-linear-acceleration-of-deep",
            "published":"2015-10-31T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/firecaffe-near-linear-acceleration-of-deep\/review\/?hl=54265"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":11224,
        "rank":932,
        "Model":"Graph-RISE (40M)",
        "mlmodel":{

        },
        "method_short":"Graph-RISE ",
        "method_details":"40M",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-02-14",
        "metrics":{
            "Top 1 Accuracy":"68.29%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":68.29,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":107335,
            "title":"Graph-RISE: Graph-Regularized Image Semantic Embedding",
            "url":"\/paper\/graph-rise-graph-regularized-image-semantic",
            "published":"2019-02-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/graph-rise-graph-regularized-image-semantic\/review\/?hl=11224"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":97059,
        "rank":933,
        "Model":"SimpleNetV1-small-075",
        "mlmodel":{

        },
        "method_short":"SimpleNetV1-small-075",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-08-22",
        "metrics":{
            "Top 1 Accuracy":"68.15",
            "Number of params":"3M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":68.15,
            "Number of params":3000000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":10271,
            "title":"Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures",
            "url":"\/paper\/lets-keep-it-simple-using-simple",
            "published":"2016-08-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lets-keep-it-simple-using-simple\/review\/?hl=97059"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":30305,
        "rank":934,
        "Model":"ReActNet-A (BN-Free)",
        "mlmodel":{

        },
        "method_short":"ReActNet-A ",
        "method_details":"BN-Free",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-16",
        "metrics":{
            "Top 1 Accuracy":"68.0%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":68.0,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":783568,
            "title":"\"BNN - BN = ?\": Training Binary Neural Networks without Batch Normalization",
            "url":"\/paper\/bnn-bn-training-binary-neural-networks",
            "published":"2021-04-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/bnn-bn-training-binary-neural-networks\/review\/?hl=30305"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":19550,
        "rank":935,
        "Model":"DY-ResNet-10",
        "mlmodel":{

        },
        "method_short":"DY-ResNet-10",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-12-07",
        "metrics":{
            "Top 1 Accuracy":"67.7%",
            "Number of params":"18.6M",
            "GFLOPs":"1.82",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":67.7,
            "Number of params":18600000.0,
            "GFLOPs":1.82,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":175971,
            "title":"Dynamic Convolution: Attention over Convolution Kernels",
            "url":"\/paper\/dynamic-convolution-attention-over",
            "published":"2019-12-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dynamic-convolution-attention-over\/review\/?hl=19550"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":71858,
        "rank":936,
        "Model":"WaveMixLite-256\/24",
        "mlmodel":{

        },
        "method_short":"WaveMixLite-256\/24",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-13",
        "metrics":{
            "Top 1 Accuracy":"67.7%",
            "Number of params":"32.4M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":67.7,
            "Number of params":32400000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1093361,
            "title":"WaveMix-Lite: A Resource-efficient Neural Network for Image Analysis",
            "url":"\/paper\/wavemix-lite-a-resource-efficient-neural-1",
            "published":"2022-10-13T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":103878,
        "rank":937,
        "Model":"PSN (SEW ResNet-18)",
        "mlmodel":{

        },
        "method_short":"PSN ",
        "method_details":"SEW ResNet-18",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "Top 1 Accuracy":"67.63",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":67.63,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":446,
                "name":"SNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":16113,
        "rank":938,
        "Model":"MUXNet-xs",
        "mlmodel":{

        },
        "method_short":"MUXNet-xs",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-03-31",
        "metrics":{
            "Top 1 Accuracy":"66.7%",
            "Number of params":"1.8M",
            "GFLOPs":"0.132",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":66.7,
            "Number of params":1800000.0,
            "GFLOPs":0.132,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":188988,
            "title":"MUXConv: Information Multiplexing in Convolutional Neural Networks",
            "url":"\/paper\/muxconv-information-multiplexing-in",
            "published":"2020-03-31T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/muxconv-information-multiplexing-in\/review\/?hl=16113"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":31362,
        "rank":939,
        "Model":"PAWS (ResNet-50, 1% labels)",
        "mlmodel":{

        },
        "method_short":"PAWS ",
        "method_details":"ResNet-50, 1% labels",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-28",
        "metrics":{
            "Top 1 Accuracy":"66.5%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":66.5,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":788977,
            "title":"Semi-Supervised Learning of Visual Features by Non-Parametrically Predicting View Assignments with Support Samples",
            "url":"\/paper\/semi-supervised-learning-of-visual-features",
            "published":"2021-04-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/semi-supervised-learning-of-visual-features\/review\/?hl=31362"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":60594,
        "rank":940,
        "Model":"GhostNet \u00d70.5",
        "mlmodel":{

        },
        "method_short":"GhostNet \u00d70.5",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-27",
        "metrics":{
            "Top 1 Accuracy":"66.2%",
            "Number of params":"2.6M",
            "GFLOPs":"0.042",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":66.2,
            "Number of params":2600000.0,
            "GFLOPs":0.042,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":174553,
            "title":"GhostNet: More Features from Cheap Operations",
            "url":"\/paper\/ghostnet-more-features-from-cheap-operations",
            "published":"2019-11-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/ghostnet-more-features-from-cheap-operations\/review\/?hl=60594"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":105308,
        "rank":941,
        "Model":"OTTT",
        "mlmodel":{

        },
        "method_short":"OTTT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-09",
        "metrics":{
            "Top 1 Accuracy":"65.15%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":65.15,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1089055,
            "title":"Online Training Through Time for Spiking Neural Networks",
            "url":"\/paper\/online-training-through-time-for-spiking",
            "published":"2022-10-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/online-training-through-time-for-spiking\/review\/?hl=105308"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":446,
                "name":"SNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":19547,
        "rank":942,
        "Model":"DY-MobileNetV2 \u00d70.35",
        "mlmodel":{

        },
        "method_short":"DY-MobileNetV2 \u00d70.35",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-12-07",
        "metrics":{
            "Top 1 Accuracy":"64.9%",
            "Number of params":"2.8M",
            "GFLOPs":"0.124",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":64.9,
            "Number of params":2800000.0,
            "GFLOPs":0.124,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":175971,
            "title":"Dynamic Convolution: Attention over Convolution Kernels",
            "url":"\/paper\/dynamic-convolution-attention-over",
            "published":"2019-12-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/dynamic-convolution-attention-over\/review\/?hl=19547"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":11225,
        "rank":943,
        "Model":"BBG (ResNet-34)",
        "mlmodel":{

        },
        "method_short":"BBG ",
        "method_details":"ResNet-34",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-09-26",
        "metrics":{
            "Top 1 Accuracy":"62.6%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":62.6,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":156101,
            "title":"Balanced Binary Neural Networks with Gated Residual",
            "url":"\/paper\/balanced-binary-neural-networks-with-gated",
            "published":"2019-09-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/balanced-binary-neural-networks-with-gated\/review\/?hl=11225"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":97058,
        "rank":944,
        "Model":"SimpleNetV1-small-05",
        "mlmodel":{

        },
        "method_short":"SimpleNetV1-small-05",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2016-08-22",
        "metrics":{
            "Top 1 Accuracy":"61.52",
            "Number of params":"1.5M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":61.52,
            "Number of params":1500000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":10271,
            "title":"Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures",
            "url":"\/paper\/lets-keep-it-simple-using-simple",
            "published":"2016-08-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lets-keep-it-simple-using-simple\/review\/?hl=97058"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":11226,
        "rank":945,
        "Model":"BBG (ResNet-18)",
        "mlmodel":{

        },
        "method_short":"BBG ",
        "method_details":"ResNet-18",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-09-26",
        "metrics":{
            "Top 1 Accuracy":"59.4%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":59.4,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":156101,
            "title":"Balanced Binary Neural Networks with Gated Residual",
            "url":"\/paper\/balanced-binary-neural-networks-with-gated",
            "published":"2019-09-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/balanced-binary-neural-networks-with-gated\/review\/?hl=11226"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":54264,
        "rank":946,
        "Model":"FireCaffe (AlexNet)",
        "mlmodel":{

        },
        "method_short":"FireCaffe ",
        "method_details":"AlexNet",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2015-10-31",
        "metrics":{
            "Top 1 Accuracy":"58.9%",
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":58.9,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":36686,
            "title":"FireCaffe: near-linear acceleration of deep neural network training on compute clusters",
            "url":"\/paper\/firecaffe-near-linear-acceleration-of-deep",
            "published":"2015-10-31T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/firecaffe-near-linear-acceleration-of-deep\/review\/?hl=54264"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":75970,
        "rank":947,
        "Model":"ViT-B\/16",
        "mlmodel":{

        },
        "method_short":"ViT-B\/16",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-10-22",
        "metrics":{
            "Top 1 Accuracy":null,
            "Number of params":null,
            "GFLOPs":"33.03",
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":null,
            "Number of params":null,
            "GFLOPs":33.03,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":229828,
            "title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
            "url":"\/paper\/an-image-is-worth-16x16-words-transformers-1",
            "published":"2020-10-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/an-image-is-worth-16x16-words-transformers-1\/review\/?hl=75970"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":113828,
        "rank":948,
        "Model":"GAC-SNN MS-ResNet-34",
        "mlmodel":{

        },
        "method_short":"GAC-SNN MS-ResNet-34",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-08-12",
        "metrics":{
            "Top 1 Accuracy":null,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":"70.42",
            "energy consumption":"0.0338"
        },
        "raw_metrics":{
            "Top 1 Accuracy":null,
            "Number of params":null,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":70.42,
            "energy consumption":0.0338
        },
        "uses_additional_data":false,
        "paper":{
            "id":1263133,
            "title":"Gated Attention Coding for Training High-performance and Efficient Spiking Neural Networks",
            "url":"\/paper\/gated-attention-coding-for-training-high",
            "published":"2023-08-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/gated-attention-coding-for-training-high\/review\/?hl=113828"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":86905,
        "rank":949,
        "Model":"EVA (EVA-CLIP)",
        "mlmodel":{

        },
        "method_short":"EVA ",
        "method_details":"EVA-CLIP",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-14",
        "metrics":{
            "Top 1 Accuracy":null,
            "Number of params":"1B",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":null,
            "Number of params":0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1110592,
            "title":"EVA: Exploring the Limits of Masked Visual Representation Learning at Scale",
            "url":"\/paper\/eva-exploring-the-limits-of-masked-visual",
            "published":"2022-11-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/eva-exploring-the-limits-of-masked-visual\/review\/?hl=86905"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":61431,
        "rank":950,
        "Model":"MobileNet-224 \u00d71.0",
        "mlmodel":{

        },
        "method_short":"MobileNet-224 \u00d71.0",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2017-04-17",
        "metrics":{
            "Top 1 Accuracy":null,
            "Number of params":"4.2M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":null,
            "Number of params":4200000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":24007,
            "title":"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications",
            "url":"\/paper\/mobilenets-efficient-convolutional-neural",
            "published":"2017-04-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mobilenets-efficient-convolutional-neural\/review\/?hl=61431"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":116,
        "row_id":2057,
        "rank":951,
        "Model":"Inception V2",
        "mlmodel":{

        },
        "method_short":"Inception V2",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2015-02-11",
        "metrics":{
            "Top 1 Accuracy":null,
            "Number of params":"11.2M",
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":null,
            "Number of params":11200000.0,
            "GFLOPs":null,
            "Hardware Burden":null,
            "Operations per network pass":null,
            "Accuarcy":null,
            "energy consumption":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":41878,
            "title":"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
            "url":"\/paper\/batch-normalization-accelerating-deep-network",
            "published":"2015-02-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/batch-normalization-accelerating-deep-network\/review\/?hl=2057"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    }
]