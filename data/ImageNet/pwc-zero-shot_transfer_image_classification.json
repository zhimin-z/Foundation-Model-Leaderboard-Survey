[
    {
        "table_id":7189,
        "row_id":97298,
        "rank":1,
        "method":"BASIC (Lion)",
        "mlmodel":{

        },
        "Model":"BASIC ",
        "method_details":"Lion",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "Accuracy (Private)":"88.3",
            "Accuracy (Public)":null
        },
        "raw_metrics":{
            "Accuracy (Private)":88.3,
            "Accuracy (Public)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7189,
        "row_id":54007,
        "rank":2,
        "method":"CoCa",
        "mlmodel":{

        },
        "Model":"CoCa",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-04",
        "metrics":{
            "Accuracy (Private)":"86.3",
            "Accuracy (Public)":null
        },
        "raw_metrics":{
            "Accuracy (Private)":86.3,
            "Accuracy (Public)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1004211,
            "title":"CoCa: Contrastive Captioners are Image-Text Foundation Models",
            "url":"\/paper\/coca-contrastive-captioners-are-image-text",
            "published":"2022-05-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/coca-contrastive-captioners-are-image-text\/review\/?hl=54007"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7189,
        "row_id":97010,
        "rank":3,
        "method":"LiT-22B",
        "mlmodel":{

        },
        "Model":"LiT-22B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-10",
        "metrics":{
            "Accuracy (Private)":"85.9",
            "Accuracy (Public)":null
        },
        "raw_metrics":{
            "Accuracy (Private)":85.9,
            "Accuracy (Public)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1155994,
            "title":"Scaling Vision Transformers to 22 Billion Parameters",
            "url":"\/paper\/scaling-vision-transformers-to-22-billion",
            "published":"2023-02-10T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7189,
        "row_id":43079,
        "rank":4,
        "method":"BASIC",
        "mlmodel":{

        },
        "Model":"BASIC",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-19",
        "metrics":{
            "Accuracy (Private)":"85.7",
            "Accuracy (Public)":null
        },
        "raw_metrics":{
            "Accuracy (Private)":85.7,
            "Accuracy (Public)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":912981,
            "title":"Combined Scaling for Zero-shot Transfer Learning",
            "url":"\/paper\/combined-scaling-for-zero-shot-transfer",
            "published":"2021-11-19T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/combined-scaling-for-zero-shot-transfer\/review\/?hl=43079"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7189,
        "row_id":70428,
        "rank":5,
        "method":"LiT ViT-e",
        "mlmodel":{

        },
        "Model":"LiT ViT-e",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-14",
        "metrics":{
            "Accuracy (Private)":"85.4",
            "Accuracy (Public)":null
        },
        "raw_metrics":{
            "Accuracy (Private)":85.4,
            "Accuracy (Public)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1074922,
            "title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model",
            "url":"\/paper\/pali-a-jointly-scaled-multilingual-language",
            "published":"2022-09-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/pali-a-jointly-scaled-multilingual-language\/review\/?hl=70428"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7189,
        "row_id":42554,
        "rank":6,
        "method":"LiT-tuning",
        "mlmodel":{

        },
        "Model":"LiT-tuning",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-15",
        "metrics":{
            "Accuracy (Private)":"84.5",
            "Accuracy (Public)":"75.7"
        },
        "raw_metrics":{
            "Accuracy (Private)":84.5,
            "Accuracy (Public)":75.7
        },
        "uses_additional_data":true,
        "paper":{
            "id":909846,
            "title":"LiT: Zero-Shot Transfer with Locked-image text Tuning",
            "url":"\/paper\/lit-zero-shot-transfer-with-locked-image-text",
            "published":"2021-11-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lit-zero-shot-transfer-with-locked-image-text\/review\/?hl=42554"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7189,
        "row_id":113824,
        "rank":7,
        "method":"IMP-MoE-L",
        "mlmodel":{

        },
        "Model":"IMP-MoE-L",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-05-10",
        "metrics":{
            "Accuracy (Private)":"83.9",
            "Accuracy (Public)":null
        },
        "raw_metrics":{
            "Accuracy (Private)":83.9,
            "Accuracy (Public)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1205713,
            "title":"Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception",
            "url":"\/paper\/alternating-gradient-descent-and-mixture-of",
            "published":"2023-05-10T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/alternating-gradient-descent-and-mixture-of\/review\/?hl=113824"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7189,
        "row_id":43155,
        "rank":8,
        "method":"Florence-CoSwin-H (@384pix)",
        "mlmodel":{

        },
        "Model":"Florence-CoSwin-H ",
        "method_details":"@384pix",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-22",
        "metrics":{
            "Accuracy (Private)":"83.7",
            "Accuracy (Public)":null
        },
        "raw_metrics":{
            "Accuracy (Private)":83.7,
            "Accuracy (Public)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":914419,
            "title":"Florence: A New Foundation Model for Computer Vision",
            "url":"\/paper\/florence-a-new-foundation-model-for-computer",
            "published":"2021-11-22T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/florence-a-new-foundation-model-for-computer\/review\/?hl=43155"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7189,
        "row_id":100973,
        "rank":9,
        "method":"MAWS (ViT-2B)",
        "mlmodel":{

        },
        "Model":"MAWS ",
        "method_details":"ViT-2B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-23",
        "metrics":{
            "Accuracy (Private)":"82.1",
            "Accuracy (Public)":null
        },
        "raw_metrics":{
            "Accuracy (Private)":82.1,
            "Accuracy (Public)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1179156,
            "title":"The effectiveness of MAE pre-pretraining for billion-scale pretraining",
            "url":"\/paper\/the-effectiveness-of-mae-pre-pretraining-for",
            "published":"2023-03-23T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7189,
        "row_id":111663,
        "rank":10,
        "method":"EVA-CLIP-E\/14+",
        "mlmodel":{

        },
        "Model":"EVA-CLIP-E\/14+",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-27",
        "metrics":{
            "Accuracy (Private)":"82",
            "Accuracy (Public)":null
        },
        "raw_metrics":{
            "Accuracy (Private)":82.0,
            "Accuracy (Public)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1180718,
            "title":"EVA-CLIP: Improved Training Techniques for CLIP at Scale",
            "url":"\/paper\/eva-clip-improved-training-techniques-for",
            "published":"2023-03-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/eva-clip-improved-training-techniques-for\/review\/?hl=111663"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7189,
        "row_id":105069,
        "rank":11,
        "method":"CLIPA (ViT-H\/14-336px)",
        "mlmodel":{

        },
        "Model":"CLIPA ",
        "method_details":"ViT-H\/14-336px",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "Accuracy (Private)":"81.8",
            "Accuracy (Public)":null
        },
        "raw_metrics":{
            "Accuracy (Private)":81.8,
            "Accuracy (Public)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7189,
        "row_id":109662,
        "rank":12,
        "method":"MAWS (ViT-H)",
        "mlmodel":{

        },
        "Model":"MAWS ",
        "method_details":"ViT-H",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-23",
        "metrics":{
            "Accuracy (Private)":"81.1",
            "Accuracy (Public)":null
        },
        "raw_metrics":{
            "Accuracy (Private)":81.1,
            "Accuracy (Public)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1179156,
            "title":"The effectiveness of MAE pre-pretraining for billion-scale pretraining",
            "url":"\/paper\/the-effectiveness-of-mae-pre-pretraining-for",
            "published":"2023-03-23T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7189,
        "row_id":95496,
        "rank":13,
        "method":"REACT",
        "mlmodel":{

        },
        "Model":"REACT",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-17",
        "metrics":{
            "Accuracy (Private)":"78.5",
            "Accuracy (Public)":null
        },
        "raw_metrics":{
            "Accuracy (Private)":78.5,
            "Accuracy (Public)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1143500,
            "title":"Learning Customized Visual Models with Retrieval-Augmented Knowledge",
            "url":"\/paper\/learning-customized-visual-models-with",
            "published":"2023-01-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learning-customized-visual-models-with\/review\/?hl=95496"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7189,
        "row_id":42555,
        "rank":14,
        "method":"ALIGN",
        "mlmodel":{

        },
        "Model":"ALIGN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-11",
        "metrics":{
            "Accuracy (Private)":"76.4",
            "Accuracy (Public)":"-"
        },
        "raw_metrics":{
            "Accuracy (Private)":76.4,
            "Accuracy (Public)":0
        },
        "uses_additional_data":false,
        "paper":{
            "id":744362,
            "title":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision",
            "url":"\/paper\/scaling-up-visual-and-vision-language",
            "published":"2021-02-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-up-visual-and-vision-language\/review\/?hl=42555"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7189,
        "row_id":83153,
        "rank":15,
        "method":"CLIP\uff08ViT-L\/14-336px\uff09",
        "mlmodel":{

        },
        "Model":"CLIP\uff08ViT-L\/14-336px\uff09",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-26",
        "metrics":{
            "Accuracy (Private)":"76.2",
            "Accuracy (Public)":null
        },
        "raw_metrics":{
            "Accuracy (Private)":76.2,
            "Accuracy (Public)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":749733,
            "title":"Learning Transferable Visual Models From Natural Language Supervision",
            "url":"\/paper\/learning-transferable-visual-models-from",
            "published":"2021-02-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learning-transferable-visual-models-from\/review\/?hl=83153"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7189,
        "row_id":79788,
        "rank":16,
        "method":"AltCLIP",
        "mlmodel":{

        },
        "Model":"AltCLIP",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-12",
        "metrics":{
            "Accuracy (Private)":"74.5",
            "Accuracy (Public)":null
        },
        "raw_metrics":{
            "Accuracy (Private)":74.5,
            "Accuracy (Public)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1110840,
            "title":"AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities",
            "url":"\/paper\/altclip-altering-the-language-encoder-in-clip",
            "published":"2022-11-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/altclip-altering-the-language-encoder-in-clip\/review\/?hl=79788"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7189,
        "row_id":70422,
        "rank":17,
        "method":"PaLI",
        "mlmodel":{

        },
        "Model":"PaLI",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-14",
        "metrics":{
            "Accuracy (Private)":"72.11",
            "Accuracy (Public)":null
        },
        "raw_metrics":{
            "Accuracy (Private)":72.11,
            "Accuracy (Public)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1074922,
            "title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model",
            "url":"\/paper\/pali-a-jointly-scaled-multilingual-language",
            "published":"2022-09-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/pali-a-jointly-scaled-multilingual-language\/review\/?hl=70422"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7189,
        "row_id":109092,
        "rank":18,
        "method":"Diffusion Classifier (zero-shot)",
        "mlmodel":{

        },
        "Model":"Diffusion Classifier ",
        "method_details":"zero-shot",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-28",
        "metrics":{
            "Accuracy (Private)":"61.4",
            "Accuracy (Public)":null
        },
        "raw_metrics":{
            "Accuracy (Private)":61.4,
            "Accuracy (Public)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1181860,
            "title":"Your Diffusion Model is Secretly a Zero-Shot Classifier",
            "url":"\/paper\/your-diffusion-model-is-secretly-a-zero-shot",
            "published":"2023-03-28T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/your-diffusion-model-is-secretly-a-zero-shot\/review\/?hl=109092"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7189,
        "row_id":45053,
        "rank":19,
        "method":"CLIP (ResNet50)",
        "mlmodel":{

        },
        "Model":"CLIP ",
        "method_details":"ResNet50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-26",
        "metrics":{
            "Accuracy (Private)":"59.6",
            "Accuracy (Public)":null
        },
        "raw_metrics":{
            "Accuracy (Private)":59.6,
            "Accuracy (Public)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":749733,
            "title":"Learning Transferable Visual Models From Natural Language Supervision",
            "url":"\/paper\/learning-transferable-visual-models-from",
            "published":"2021-02-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learning-transferable-visual-models-from\/review\/?hl=45053"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7189,
        "row_id":110774,
        "rank":20,
        "method":"CWCL",
        "mlmodel":{

        },
        "Model":"CWCL",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "Accuracy (Private)":null,
            "Accuracy (Public)":"76.5"
        },
        "raw_metrics":{
            "Accuracy (Private)":null,
            "Accuracy (Public)":76.5
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7189,
        "row_id":27637,
        "rank":21,
        "method":"CLIP",
        "mlmodel":{

        },
        "Model":"CLIP",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-26",
        "metrics":{
            "Accuracy (Private)":null,
            "Accuracy (Public)":" 31.3"
        },
        "raw_metrics":{
            "Accuracy (Private)":null,
            "Accuracy (Public)":31.3
        },
        "uses_additional_data":false,
        "paper":{
            "id":749733,
            "title":"Learning Transferable Visual Models From Natural Language Supervision",
            "url":"\/paper\/learning-transferable-visual-models-from",
            "published":"2021-02-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learning-transferable-visual-models-from\/review\/?hl=27637"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]