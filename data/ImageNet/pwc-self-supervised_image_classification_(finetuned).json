[
    {
        "table_id":7112,
        "row_id":101344,
        "rank":1,
        "method":"DINOv2 (ViT-g\/14, 448)",
        "mlmodel":{

        },
        "method_short":"DINOv2 ",
        "method_details":"ViT-g\/14, 448",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-14",
        "metrics":{
            "Top 1 Accuracy":"88.9%",
            "Number of Params":"1100M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.9,
            "Number of Params":1100000000.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":1191620,
            "title":"DINOv2: Learning Robust Visual Features without Supervision",
            "url":"\/paper\/dinov2-learning-robust-visual-features",
            "published":"2023-04-14T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":103623,
        "rank":2,
        "method":"PercMAE (ViT-L, dVAE)",
        "mlmodel":{

        },
        "method_short":"PercMAE ",
        "method_details":"ViT-L, dVAE",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-30",
        "metrics":{
            "Top 1 Accuracy":"88.6%",
            "Number of Params":"307M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.6,
            "Number of Params":307000000.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":1136076,
            "title":"Improving Visual Representation Learning through Perceptual Understanding",
            "url":"\/paper\/improving-visual-representation-learning",
            "published":"2022-12-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improving-visual-representation-learning\/review\/?hl=103623"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":101343,
        "rank":3,
        "method":"DINOv2 (ViT-g\/14)",
        "mlmodel":{

        },
        "method_short":"DINOv2 ",
        "method_details":"ViT-g\/14",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-04-14",
        "metrics":{
            "Top 1 Accuracy":"88.5%",
            "Number of Params":"1100M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.5,
            "Number of Params":1100000000.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":1191620,
            "title":"DINOv2: Learning Robust Visual Features without Supervision",
            "url":"\/paper\/dinov2-learning-robust-visual-features",
            "published":"2023-04-14T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":79500,
        "rank":4,
        "method":"PeCo(ViT-H\/14, 448)",
        "mlmodel":{

        },
        "method_short":"PeCo",
        "method_details":"ViT-H\/14, 448",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-24",
        "metrics":{
            "Top 1 Accuracy":"88.3%",
            "Number of Params":"632M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.3,
            "Number of Params":632000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":916192,
            "title":"PeCo: Perceptual Codebook for BERT Pre-training of Vision Transformers",
            "url":"\/paper\/peco-perceptual-codebook-for-bert-pre",
            "published":"2021-11-24T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/peco-perceptual-codebook-for-bert-pre\/review\/?hl=79500"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":103621,
        "rank":5,
        "method":"PercMAE (ViT-L)",
        "mlmodel":{

        },
        "method_short":"PercMAE ",
        "method_details":"ViT-L",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-12-30",
        "metrics":{
            "Top 1 Accuracy":"88.1%",
            "Number of Params":"307M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.1,
            "Number of Params":307000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1136076,
            "title":"Improving Visual Representation Learning through Perceptual Understanding",
            "url":"\/paper\/improving-visual-representation-learning",
            "published":"2022-12-30T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/improving-visual-representation-learning\/review\/?hl=103621"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":76350,
        "rank":6,
        "method":"dBOT (ViT-H\/14)",
        "mlmodel":{

        },
        "method_short":"dBOT ",
        "method_details":"ViT-H\/14",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-08",
        "metrics":{
            "Top 1 Accuracy":"88.0%",
            "Number of Params":"632M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":88.0,
            "Number of Params":632000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1071401,
            "title":"Exploring Target Representations for Masked Autoencoders",
            "url":"\/paper\/exploring-target-representations-for-masked",
            "published":"2022-09-08T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/exploring-target-representations-for-masked\/review\/?hl=76350"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":44698,
        "rank":7,
        "method":"MAE (ViT-H\/14, 448)",
        "mlmodel":{

        },
        "method_short":"MAE ",
        "method_details":"ViT-H\/14, 448",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-11",
        "metrics":{
            "Top 1 Accuracy":"87.8%",
            "Number of Params":"632M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.8,
            "Number of Params":632000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":908690,
            "title":"Masked Autoencoders Are Scalable Vision Learners",
            "url":"\/paper\/masked-autoencoders-are-scalable-vision",
            "published":"2021-11-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-autoencoders-are-scalable-vision\/review\/?hl=44698"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":45185,
        "rank":8,
        "method":"iBOT(ViT-L\/16, 512)",
        "mlmodel":{

        },
        "method_short":"iBOT",
        "method_details":"ViT-L\/16, 512",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-15",
        "metrics":{
            "Top 1 Accuracy":"87.8%",
            "Number of Params":"307M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.8,
            "Number of Params":307000000.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":909858,
            "title":"iBOT: Image BERT Pre-Training with Online Tokenizer",
            "url":"\/paper\/ibot-image-bert-pre-training-with-online",
            "published":"2021-11-15T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":229,
                "name":"ImageNet-22k unpretrained",
                "color":"#2771D3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":105528,
        "rank":9,
        "method":"MAE + AugSub finetune (ViT-H\/14)",
        "mlmodel":{

        },
        "method_short":"MAE + AugSub finetune ",
        "method_details":"ViT-H\/14",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-06-20",
        "metrics":{
            "Top 1 Accuracy":"87.2%",
            "Number of Params":"632M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.2,
            "Number of Params":632000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1231534,
            "title":"Augmenting Sub-model to Improve Main Model",
            "url":"\/paper\/augmenting-sub-model-to-improve-main-model",
            "published":"2023-06-20T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":45347,
        "rank":10,
        "method":"SimMIM (SwinV2-H, 512)",
        "mlmodel":{

        },
        "method_short":"SimMIM ",
        "method_details":"SwinV2-H, 512",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-18",
        "metrics":{
            "Top 1 Accuracy":"87.1%",
            "Number of Params":"658M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":87.1,
            "Number of Params":658000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":912368,
            "title":"SimMIM: A Simple Framework for Masked Image Modeling",
            "url":"\/paper\/simmim-a-simple-framework-for-masked-image",
            "published":"2021-11-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/simmim-a-simple-framework-for-masked-image\/review\/?hl=45347"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":45291,
        "rank":11,
        "method":"MAE (ViT-H\/14)",
        "mlmodel":{

        },
        "method_short":"MAE ",
        "method_details":"ViT-H\/14",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-11",
        "metrics":{
            "Top 1 Accuracy":"86.9%",
            "Number of Params":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.9,
            "Number of Params":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":908690,
            "title":"Masked Autoencoders Are Scalable Vision Learners",
            "url":"\/paper\/masked-autoencoders-are-scalable-vision",
            "published":"2021-11-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-autoencoders-are-scalable-vision\/review\/?hl=45291"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":44135,
        "rank":12,
        "method":"iBOT(ViT-L\/16)",
        "mlmodel":{

        },
        "method_short":"iBOT",
        "method_details":"ViT-L\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-15",
        "metrics":{
            "Top 1 Accuracy":"86.6%",
            "Number of Params":"307M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.6,
            "Number of Params":307000000.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":909858,
            "title":"iBOT: Image BERT Pre-Training with Online Tokenizer",
            "url":"\/paper\/ibot-image-bert-pre-training-with-online",
            "published":"2021-11-15T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":86807,
        "rank":13,
        "method":"TEC_MAE (ViT-L\/16, 224)",
        "mlmodel":{

        },
        "method_short":"TEC_MAE ",
        "method_details":"ViT-L\/16, 224",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-10-20",
        "metrics":{
            "Top 1 Accuracy":"86.5%",
            "Number of Params":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.5,
            "Number of Params":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1097111,
            "title":"Towards Sustainable Self-supervised Learning",
            "url":"\/paper\/towards-sustainable-self-supervised-learning",
            "published":"2022-10-20T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/towards-sustainable-self-supervised-learning\/review\/?hl=86807"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":27571,
        "rank":14,
        "method":"BEiT-L (ViT)",
        "mlmodel":{

        },
        "method_short":"BEiT-L ",
        "method_details":"ViT",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-15",
        "metrics":{
            "Top 1 Accuracy":"86.3%",
            "Number of Params":"307M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.3,
            "Number of Params":307000000.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":818838,
            "title":"BEiT: BERT Pre-Training of Image Transformers",
            "url":"\/paper\/beit-bert-pre-training-of-image-transformers",
            "published":"2021-06-15T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":57172,
        "rank":15,
        "method":"CAE (ViT-L\/16)",
        "mlmodel":{

        },
        "method_short":"CAE ",
        "method_details":"ViT-L\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-07",
        "metrics":{
            "Top 1 Accuracy":"86.3%",
            "Number of Params":"307M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.3,
            "Number of Params":307000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":956722,
            "title":"Context Autoencoder for Self-Supervised Representation Learning",
            "url":"\/paper\/context-autoencoder-for-self-supervised",
            "published":"2022-02-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/context-autoencoder-for-self-supervised\/review\/?hl=57172"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":113323,
        "rank":16,
        "method":"MIRL (ViT-B-48)",
        "mlmodel":{

        },
        "method_short":"MIRL ",
        "method_details":"ViT-B-48",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-09-22",
        "metrics":{
            "Top 1 Accuracy":"86.2%",
            "Number of Params":"341M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.2,
            "Number of Params":341000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1333554,
            "title":"Masked Image Residual Learning for Scaling Deeper Vision Transformers",
            "url":"\/paper\/masked-image-residual-learning-for-scaling-1",
            "published":"2023-09-25T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":105527,
        "rank":17,
        "method":"MAE + AugSub finetune (ViT-L\/16)",
        "mlmodel":{

        },
        "method_short":"MAE + AugSub finetune ",
        "method_details":"ViT-L\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-06-20",
        "metrics":{
            "Top 1 Accuracy":"86.1%",
            "Number of Params":"304M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.1,
            "Number of Params":304000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1231534,
            "title":"Augmenting Sub-model to Improve Main Model",
            "url":"\/paper\/augmenting-sub-model-to-improve-main-model",
            "published":"2023-06-20T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":88859,
        "rank":18,
        "method":"SparK (ConvNeXt-Large, 384)",
        "mlmodel":{

        },
        "method_short":"SparK ",
        "method_details":"ConvNeXt-Large, 384",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-09",
        "metrics":{
            "Top 1 Accuracy":"86.0%",
            "Number of Params":"198M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":86.0,
            "Number of Params":198000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1139501,
            "title":"Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling",
            "url":"\/paper\/designing-bert-for-convolutional-networks",
            "published":"2023-01-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/designing-bert-for-convolutional-networks\/review\/?hl=88859"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":212,
                "name":"ConvNeXt",
                "color":"#77bb41"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":79501,
        "rank":19,
        "method":"BootMAE(ViT-L)",
        "mlmodel":{

        },
        "method_short":"BootMAE",
        "method_details":"ViT-L",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-07-14",
        "metrics":{
            "Top 1 Accuracy":"85.9%",
            "Number of Params":"307M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.9,
            "Number of Params":307000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1043808,
            "title":"Bootstrapped Masked Autoencoders for Vision BERT Pretraining",
            "url":"\/paper\/bootstrapped-masked-autoencoders-for-vision",
            "published":"2022-07-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/bootstrapped-masked-autoencoders-for-vision\/review\/?hl=79501"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":47950,
        "rank":20,
        "method":"SEER (Regnet10B)",
        "mlmodel":{

        },
        "method_short":"SEER ",
        "method_details":"Regnet10B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-16",
        "metrics":{
            "Top 1 Accuracy":"85.8%",
            "Number of Params":"10000M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.8,
            "Number of Params":10000000000.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":963673,
            "title":"Vision Models Are More Robust And Fair When Pretrained On Uncurated Images Without Supervision",
            "url":"\/paper\/vision-models-are-more-robust-and-fair-when",
            "published":"2022-02-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/vision-models-are-more-robust-and-fair-when\/review\/?hl=47950"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":108,
                "name":"RegNet",
                "color":"#2771D3"
            },
            {
                "id":107,
                "name":"IG-1B",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":44633,
        "rank":21,
        "method":"MaskFeat (ViT-L)",
        "mlmodel":{

        },
        "method_short":"MaskFeat ",
        "method_details":"ViT-L",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-12-16",
        "metrics":{
            "Top 1 Accuracy":"85.7%",
            "Number of Params":"307M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.7,
            "Number of Params":307000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":932132,
            "title":"Masked Feature Prediction for Self-Supervised Visual Pre-Training",
            "url":"\/paper\/masked-feature-prediction-for-self-supervised",
            "published":"2021-12-16T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/masked-feature-prediction-for-self-supervised\/review\/?hl=44633"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":47712,
        "rank":22,
        "method":"OFA (Large)",
        "mlmodel":{

        },
        "method_short":"OFA ",
        "method_details":"Large",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-02-07",
        "metrics":{
            "Top 1 Accuracy":"85.6%",
            "Number of Params":"473M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.6,
            "Number of Params":473000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":956719,
            "title":"OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework",
            "url":"\/paper\/unifying-architectures-tasks-and-modalities",
            "published":"2022-02-07T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/unifying-architectures-tasks-and-modalities\/review\/?hl=47712"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":88858,
        "rank":23,
        "method":"SparK (ConvNeXt-Large)",
        "mlmodel":{

        },
        "method_short":"SparK ",
        "method_details":"ConvNeXt-Large",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-09",
        "metrics":{
            "Top 1 Accuracy":"85.4%",
            "Number of Params":"198M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.4,
            "Number of Params":198000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1139501,
            "title":"Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling",
            "url":"\/paper\/designing-bert-for-convolutional-networks",
            "published":"2023-01-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/designing-bert-for-convolutional-networks\/review\/?hl=88858"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":52790,
        "rank":24,
        "method":"SimMIM (Swin-L)",
        "mlmodel":{

        },
        "method_short":"SimMIM ",
        "method_details":"Swin-L",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-18",
        "metrics":{
            "Top 1 Accuracy":"85.4%",
            "Number of Params":"197M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.4,
            "Number of Params":197000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":912368,
            "title":"SimMIM: A Simple Framework for Masked Image Modeling",
            "url":"\/paper\/simmim-a-simple-framework-for-masked-image",
            "published":"2021-11-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/simmim-a-simple-framework-for-masked-image\/review\/?hl=52790"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":50522,
        "rank":25,
        "method":"Mugs (ViT-L\/16)",
        "mlmodel":{

        },
        "method_short":"Mugs ",
        "method_details":"ViT-L\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-27",
        "metrics":{
            "Top 1 Accuracy":"85.2%",
            "Number of Params":"307M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":85.2,
            "Number of Params":307000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":984107,
            "title":"Mugs: A Multi-Granular Self-Supervised Learning Framework",
            "url":"\/paper\/mugs-a-multi-granular-self-supervised",
            "published":"2022-03-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mugs-a-multi-granular-self-supervised\/review\/?hl=50522"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":45186,
        "rank":26,
        "method":"iBOT (ViT-L\/16)",
        "mlmodel":{

        },
        "method_short":"iBOT ",
        "method_details":"ViT-L\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-15",
        "metrics":{
            "Top 1 Accuracy":"84.8%",
            "Number of Params":"307M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.8,
            "Number of Params":307000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":909858,
            "title":"iBOT: Image BERT Pre-Training with Online Tokenizer",
            "url":"\/paper\/ibot-image-bert-pre-training-with-online",
            "published":"2021-11-15T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":113324,
        "rank":27,
        "method":"MIRL (ViT-S-54)",
        "mlmodel":{

        },
        "method_short":"MIRL ",
        "method_details":"ViT-S-54",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-09-22",
        "metrics":{
            "Top 1 Accuracy":"84.8%",
            "Number of Params":"96M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.8,
            "Number of Params":96000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1333554,
            "title":"Masked Image Residual Learning for Scaling Deeper Vision Transformers",
            "url":"\/paper\/masked-image-residual-learning-for-scaling-1",
            "published":"2023-09-25T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":88848,
        "rank":28,
        "method":"ConvNeXt-Base (SparK pre-training)",
        "mlmodel":{

        },
        "method_short":"ConvNeXt-Base ",
        "method_details":"SparK pre-training",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-09",
        "metrics":{
            "Top 1 Accuracy":"84.8%",
            "Number of Params":"89M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.8,
            "Number of Params":89000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1139501,
            "title":"Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling",
            "url":"\/paper\/designing-bert-for-convolutional-networks",
            "published":"2023-01-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/designing-bert-for-convolutional-networks\/review\/?hl=88848"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":212,
                "name":"ConvNeXt",
                "color":"#77bb41"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":36908,
        "rank":29,
        "method":"BEiT-B (ViT)",
        "mlmodel":{

        },
        "method_short":"BEiT-B ",
        "method_details":"ViT",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-15",
        "metrics":{
            "Top 1 Accuracy":"84.6%",
            "Number of Params":"86M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.6,
            "Number of Params":86000000.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":818838,
            "title":"BEiT: BERT Pre-Training of Image Transformers",
            "url":"\/paper\/beit-bert-pre-training-of-image-transformers",
            "published":"2021-06-15T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":79005,
        "rank":30,
        "method":"A2MIM+ (ViT-B)",
        "mlmodel":{

        },
        "method_short":"A2MIM+ ",
        "method_details":"ViT-B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-27",
        "metrics":{
            "Top 1 Accuracy":"84.5%",
            "Number of Params":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.5,
            "Number of Params":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1017241,
            "title":"Architecture-Agnostic Masked Image Modeling -- From ViT back to CNN",
            "url":"\/paper\/architecture-agnostic-masked-image-modeling",
            "published":"2022-05-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/architecture-agnostic-masked-image-modeling\/review\/?hl=79005"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":44141,
        "rank":31,
        "method":"iBOT (ViT-B\/16)",
        "mlmodel":{

        },
        "method_short":"iBOT ",
        "method_details":"ViT-B\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-15",
        "metrics":{
            "Top 1 Accuracy":"84.4%",
            "Number of Params":"85M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.4,
            "Number of Params":85000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":909858,
            "title":"iBOT: Image BERT Pre-Training with Online Tokenizer",
            "url":"\/paper\/ibot-image-bert-pre-training-with-online",
            "published":"2021-11-15T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":50521,
        "rank":32,
        "method":"Mugs (ViT-B\/16)",
        "mlmodel":{

        },
        "method_short":"Mugs ",
        "method_details":"ViT-B\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-27",
        "metrics":{
            "Top 1 Accuracy":"84.3%",
            "Number of Params":"85M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.3,
            "Number of Params":85000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":984107,
            "title":"Mugs: A Multi-Granular Self-Supervised Learning Framework",
            "url":"\/paper\/mugs-a-multi-granular-self-supervised",
            "published":"2022-03-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mugs-a-multi-granular-self-supervised\/review\/?hl=50521"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":39934,
        "rank":33,
        "method":"SEER (RegNetY-256GF)",
        "mlmodel":{

        },
        "method_short":"SEER ",
        "method_details":"RegNetY-256GF",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-02",
        "metrics":{
            "Top 1 Accuracy":"84.2%",
            "Number of Params":"1.3B"
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.2,
            "Number of Params":1.3
        },
        "uses_additional_data":true,
        "paper":{
            "id":750519,
            "title":"Self-supervised Pretraining of Visual Features in the Wild",
            "url":"\/paper\/self-supervised-pretraining-of-visual",
            "published":"2021-03-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/self-supervised-pretraining-of-visual\/review\/?hl=39934"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":107,
                "name":"IG-1B",
                "color":"#2771D3"
            },
            {
                "id":108,
                "name":"RegNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":79003,
        "rank":34,
        "method":"A2MIM (ViT-B)",
        "mlmodel":{

        },
        "method_short":"A2MIM ",
        "method_details":"ViT-B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-27",
        "metrics":{
            "Top 1 Accuracy":"84.2%",
            "Number of Params":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.2,
            "Number of Params":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1017241,
            "title":"Architecture-Agnostic Masked Image Modeling -- From ViT back to CNN",
            "url":"\/paper\/architecture-agnostic-masked-image-modeling",
            "published":"2022-05-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/architecture-agnostic-masked-image-modeling\/review\/?hl=79003"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":38626,
        "rank":35,
        "method":"MoCo v3 (ViT-L\/16)",
        "mlmodel":{

        },
        "method_short":"MoCo v3 ",
        "method_details":"ViT-L\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-05",
        "metrics":{
            "Top 1 Accuracy":"84.1%",
            "Number of Params":"304M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.1,
            "Number of Params":304000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":774854,
            "title":"An Empirical Study of Training Self-Supervised Vision Transformers",
            "url":"\/paper\/an-empirical-study-of-training-self",
            "published":"2021-04-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/an-empirical-study-of-training-self\/review\/?hl=38626"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":60782,
        "rank":36,
        "method":"mc-BEiT (ViT-B\/16)",
        "mlmodel":{

        },
        "method_short":"mc-BEiT ",
        "method_details":"ViT-B\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-29",
        "metrics":{
            "Top 1 Accuracy":"84.1%",
            "Number of Params":"86M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.1,
            "Number of Params":86000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":985369,
            "title":"mc-BEiT: Multi-choice Discretization for Image BERT Pre-training",
            "url":"\/paper\/mc-beit-multi-choice-discretization-for-image",
            "published":"2022-03-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mc-beit-multi-choice-discretization-for-image\/review\/?hl=60782"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":88847,
        "rank":37,
        "method":"ConvNeXt-Small (SparK pre-training)",
        "mlmodel":{

        },
        "method_short":"ConvNeXt-Small ",
        "method_details":"SparK pre-training",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-09",
        "metrics":{
            "Top 1 Accuracy":"84.1%",
            "Number of Params":"50M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.1,
            "Number of Params":50000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1139501,
            "title":"Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling",
            "url":"\/paper\/designing-bert-for-convolutional-networks",
            "published":"2023-01-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/designing-bert-for-convolutional-networks\/review\/?hl=88847"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":212,
                "name":"ConvNeXt",
                "color":"#77bb41"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":52791,
        "rank":38,
        "method":"SimMIM (Swin-B)",
        "mlmodel":{

        },
        "method_short":"SimMIM ",
        "method_details":"Swin-B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-18",
        "metrics":{
            "Top 1 Accuracy":"84.0%",
            "Number of Params":"88M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.0,
            "Number of Params":88000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":912368,
            "title":"SimMIM: A Simple Framework for Masked Image Modeling",
            "url":"\/paper\/simmim-a-simple-framework-for-masked-image",
            "published":"2021-11-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/simmim-a-simple-framework-for-masked-image\/review\/?hl=52791"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":44140,
        "rank":39,
        "method":"iBOT (ViT-B\/16)",
        "mlmodel":{

        },
        "method_short":"iBOT ",
        "method_details":"ViT-B\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-15",
        "metrics":{
            "Top 1 Accuracy":"84.0%",
            "Number of Params":"85M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":84.0,
            "Number of Params":85000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":909858,
            "title":"iBOT: Image BERT Pre-Training with Online Tokenizer",
            "url":"\/paper\/ibot-image-bert-pre-training-with-online",
            "published":"2021-11-15T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":88622,
        "rank":40,
        "method":"EsViT (Swin-B)",
        "mlmodel":{

        },
        "method_short":"EsViT ",
        "method_details":"Swin-B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-17",
        "metrics":{
            "Top 1 Accuracy":"83.9%",
            "Number of Params":"87M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.9,
            "Number of Params":87000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":821124,
            "title":"Efficient Self-supervised Vision Transformers for Representation Learning",
            "url":"\/paper\/efficient-self-supervised-vision-transformers",
            "published":"2021-06-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/efficient-self-supervised-vision-transformers\/review\/?hl=88622"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":105526,
        "rank":41,
        "method":"MAE + AugSub finetune (ViT-B\/16)",
        "mlmodel":{

        },
        "method_short":"MAE + AugSub finetune ",
        "method_details":"ViT-B\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-06-20",
        "metrics":{
            "Top 1 Accuracy":"83.9%",
            "Number of Params":"87M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.9,
            "Number of Params":87000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1231534,
            "title":"Augmenting Sub-model to Improve Main Model",
            "url":"\/paper\/augmenting-sub-model-to-improve-main-model",
            "published":"2023-06-20T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":27570,
        "rank":42,
        "method":"SEER (RegNetY-128GF)",
        "mlmodel":{

        },
        "method_short":"SEER ",
        "method_details":"RegNetY-128GF",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-03-02",
        "metrics":{
            "Top 1 Accuracy":"83.8%",
            "Number of Params":"693M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.8,
            "Number of Params":693000000.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":750519,
            "title":"Self-supervised Pretraining of Visual Features in the Wild",
            "url":"\/paper\/self-supervised-pretraining-of-visual",
            "published":"2021-03-02T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/self-supervised-pretraining-of-visual\/review\/?hl=27570"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":107,
                "name":"IG-1B",
                "color":"#2771D3"
            },
            {
                "id":108,
                "name":"RegNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":52792,
        "rank":43,
        "method":"SimMIM (ViT-B\/16)",
        "mlmodel":{

        },
        "method_short":"SimMIM ",
        "method_details":"ViT-B\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-18",
        "metrics":{
            "Top 1 Accuracy":"83.8%",
            "Number of Params":"85M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.8,
            "Number of Params":85000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":912368,
            "title":"SimMIM: A Simple Framework for Masked Image Modeling",
            "url":"\/paper\/simmim-a-simple-framework-for-masked-image",
            "published":"2021-11-18T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/simmim-a-simple-framework-for-masked-image\/review\/?hl=52792"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":38625,
        "rank":44,
        "method":"MoCo v3 (ViT-B\/16)",
        "mlmodel":{

        },
        "method_short":"MoCo v3 ",
        "method_details":"ViT-B\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-05",
        "metrics":{
            "Top 1 Accuracy":"83.2%",
            "Number of Params":"86M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.2,
            "Number of Params":86000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":774854,
            "title":"An Empirical Study of Training Self-Supervised Vision Transformers",
            "url":"\/paper\/an-empirical-study-of-training-self",
            "published":"2021-04-05T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/an-empirical-study-of-training-self\/review\/?hl=38625"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":54341,
        "rank":45,
        "method":"DAMA (ViT-B\/16)",
        "mlmodel":{

        },
        "method_short":"DAMA ",
        "method_details":"ViT-B\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-10",
        "metrics":{
            "Top 1 Accuracy":"83.2%",
            "Number of Params":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.2,
            "Number of Params":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1007661,
            "title":"Multiplexed Immunofluorescence Brain Image Analysis Using Self-Supervised Dual-Loss Adaptive Masked Autoencoder",
            "url":"\/paper\/student-collaboration-improves-self",
            "published":"2022-05-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/student-collaboration-improves-self\/review\/?hl=54341"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":239,
                "name":"Adaptive Masking",
                "color":"#828487"
            },
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":27574,
        "rank":46,
        "method":"SimCLRv2 (ResNet-152, 3\u00d7+SK)",
        "mlmodel":{

        },
        "method_short":"SimCLRv2 ",
        "method_details":"ResNet-152, 3\u00d7+SK",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-17",
        "metrics":{
            "Top 1 Accuracy":"83.1%",
            "Number of Params":"795M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.1,
            "Number of Params":795000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":202875,
            "title":"Big Self-Supervised Models are Strong Semi-Supervised Learners",
            "url":"\/paper\/big-self-supervised-models-are-strong-semi",
            "published":"2020-06-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/big-self-supervised-models-are-strong-semi\/review\/?hl=27574"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":88846,
        "rank":47,
        "method":"ResNet-200 (SparK pre-training)",
        "mlmodel":{

        },
        "method_short":"ResNet-200 ",
        "method_details":"SparK pre-training",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-09",
        "metrics":{
            "Top 1 Accuracy":"83.1%",
            "Number of Params":"65M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":83.1,
            "Number of Params":65000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1139501,
            "title":"Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling",
            "url":"\/paper\/designing-bert-for-convolutional-networks",
            "published":"2023-01-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/designing-bert-for-convolutional-networks\/review\/?hl=88846"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            },
            {
                "id":203,
                "name":"ResNet-200",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":36560,
        "rank":48,
        "method":"DINO (ViT-B\/16)",
        "mlmodel":{

        },
        "method_short":"DINO ",
        "method_details":"ViT-B\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-04-29",
        "metrics":{
            "Top 1 Accuracy":"82.8%",
            "Number of Params":"85M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.8,
            "Number of Params":85000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":789002,
            "title":"Emerging Properties in Self-Supervised Vision Transformers",
            "url":"\/paper\/emerging-properties-in-self-supervised-vision",
            "published":"2021-04-29T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/emerging-properties-in-self-supervised-vision\/review\/?hl=36560"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":4,
                "name":"Transformer",
                "color":"#0037CC"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":88845,
        "rank":49,
        "method":"ResNet-152 (SparK pre-training)",
        "mlmodel":{

        },
        "method_short":"ResNet-152 ",
        "method_details":"SparK pre-training",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-09",
        "metrics":{
            "Top 1 Accuracy":"82.7%",
            "Number of Params":"60M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.7,
            "Number of Params":60000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1139501,
            "title":"Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling",
            "url":"\/paper\/designing-bert-for-convolutional-networks",
            "published":"2023-01-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/designing-bert-for-convolutional-networks\/review\/?hl=88845"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            },
            {
                "id":81,
                "name":"ResNet-152",
                "color":"#598ac9"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":50520,
        "rank":50,
        "method":"Mugs (ViT-S\/16)",
        "mlmodel":{

        },
        "method_short":"Mugs ",
        "method_details":"ViT-S\/16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-03-27",
        "metrics":{
            "Top 1 Accuracy":"82.6%",
            "Number of Params":"21M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.6,
            "Number of Params":21000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":984107,
            "title":"Mugs: A Multi-Granular Self-Supervised Learning Framework",
            "url":"\/paper\/mugs-a-multi-granular-self-supervised",
            "published":"2022-03-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/mugs-a-multi-granular-self-supervised\/review\/?hl=50520"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":79004,
        "rank":51,
        "method":"A2MIM+ (ViT-S)",
        "mlmodel":{

        },
        "method_short":"A2MIM+ ",
        "method_details":"ViT-S",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-27",
        "metrics":{
            "Top 1 Accuracy":"82.4%",
            "Number of Params":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.4,
            "Number of Params":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1017241,
            "title":"Architecture-Agnostic Masked Image Modeling -- From ViT back to CNN",
            "url":"\/paper\/architecture-agnostic-masked-image-modeling",
            "published":"2022-05-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/architecture-agnostic-masked-image-modeling\/review\/?hl=79004"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":88844,
        "rank":52,
        "method":"ResNet-101 (SparK pre-training)",
        "mlmodel":{

        },
        "method_short":"ResNet-101 ",
        "method_details":"SparK pre-training",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-09",
        "metrics":{
            "Top 1 Accuracy":"82.2%",
            "Number of Params":"44M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.2,
            "Number of Params":44000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1139501,
            "title":"Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling",
            "url":"\/paper\/designing-bert-for-convolutional-networks",
            "published":"2023-01-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/designing-bert-for-convolutional-networks\/review\/?hl=88844"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            },
            {
                "id":54,
                "name":"ResNet-101",
                "color":"#cc1e1e"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":79002,
        "rank":53,
        "method":"A2MIM (ViT-S)",
        "mlmodel":{

        },
        "method_short":"A2MIM ",
        "method_details":"ViT-S",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-27",
        "metrics":{
            "Top 1 Accuracy":"82.2%",
            "Number of Params":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.2,
            "Number of Params":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1017241,
            "title":"Architecture-Agnostic Masked Image Modeling -- From ViT back to CNN",
            "url":"\/paper\/architecture-agnostic-masked-image-modeling",
            "published":"2022-05-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/architecture-agnostic-masked-image-modeling\/review\/?hl=79002"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":27573,
        "rank":54,
        "method":"SwAV (ResNeXt-101-32x16d)",
        "mlmodel":{

        },
        "method_short":"SwAV ",
        "method_details":"ResNeXt-101-32x16d",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-17",
        "metrics":{
            "Top 1 Accuracy":"82.0%",
            "Number of Params":"193M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.0,
            "Number of Params":193000000.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":202916,
            "title":"Unsupervised Learning of Visual Features by Contrasting Cluster Assignments",
            "url":"\/paper\/unsupervised-learning-of-visual-features-by",
            "published":"2020-06-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/unsupervised-learning-of-visual-features-by\/review\/?hl=27573"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":6,
                "name":"ResNeXt",
                "color":"#86960b"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            },
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":107,
                "name":"IG-1B",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":88843,
        "rank":55,
        "method":"ResNet-50 (SparK pre-training)",
        "mlmodel":{

        },
        "method_short":"ResNet-50 ",
        "method_details":"SparK pre-training",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-01-09",
        "metrics":{
            "Top 1 Accuracy":"80.6%",
            "Number of Params":"26M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.6,
            "Number of Params":26000000.0
        },
        "uses_additional_data":false,
        "paper":{
            "id":1139501,
            "title":"Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling",
            "url":"\/paper\/designing-bert-for-convolutional-networks",
            "published":"2023-01-09T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/designing-bert-for-convolutional-networks\/review\/?hl=88843"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":171,
                "name":"ImageNet-1k only",
                "color":"#ae27d3"
            },
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            },
            {
                "id":33,
                "name":"ResNet-50",
                "color":"#c17b9b"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":79001,
        "rank":56,
        "method":"A2MIM+ (ResNet-50 RSB-A2)",
        "mlmodel":{

        },
        "method_short":"A2MIM+ ",
        "method_details":"ResNet-50 RSB-A2",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-27",
        "metrics":{
            "Top 1 Accuracy":"80.5%",
            "Number of Params":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.5,
            "Number of Params":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1017241,
            "title":"Architecture-Agnostic Masked Image Modeling -- From ViT back to CNN",
            "url":"\/paper\/architecture-agnostic-masked-image-modeling",
            "published":"2022-05-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/architecture-agnostic-masked-image-modeling\/review\/?hl=79001"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            },
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":78999,
        "rank":57,
        "method":"A2MIM (ResNet-50 RSB-A2)",
        "mlmodel":{

        },
        "method_short":"A2MIM ",
        "method_details":"ResNet-50 RSB-A2",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-27",
        "metrics":{
            "Top 1 Accuracy":"80.4%",
            "Number of Params":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":80.4,
            "Number of Params":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1017241,
            "title":"Architecture-Agnostic Masked Image Modeling -- From ViT back to CNN",
            "url":"\/paper\/architecture-agnostic-masked-image-modeling",
            "published":"2022-05-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/architecture-agnostic-masked-image-modeling\/review\/?hl=78999"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            },
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":79000,
        "rank":58,
        "method":"A2MIM+ (ResNet-50 RSB-A3)",
        "mlmodel":{

        },
        "method_short":"A2MIM+ ",
        "method_details":"ResNet-50 RSB-A3",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-27",
        "metrics":{
            "Top 1 Accuracy":"78.9%",
            "Number of Params":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.9,
            "Number of Params":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1017241,
            "title":"Architecture-Agnostic Masked Image Modeling -- From ViT back to CNN",
            "url":"\/paper\/architecture-agnostic-masked-image-modeling",
            "published":"2022-05-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/architecture-agnostic-masked-image-modeling\/review\/?hl=79000"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            },
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":78998,
        "rank":59,
        "method":"A2MIM (ResNet-50 RSB-A3)",
        "mlmodel":{

        },
        "method_short":"A2MIM ",
        "method_details":"ResNet-50 RSB-A3",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-27",
        "metrics":{
            "Top 1 Accuracy":"78.8%",
            "Number of Params":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.8,
            "Number of Params":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1017241,
            "title":"Architecture-Agnostic Masked Image Modeling -- From ViT back to CNN",
            "url":"\/paper\/architecture-agnostic-masked-image-modeling",
            "published":"2022-05-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/architecture-agnostic-masked-image-modeling\/review\/?hl=78998"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            },
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":46868,
        "rank":60,
        "method":"DnC (Resnet-50)",
        "mlmodel":{

        },
        "method_short":"DnC ",
        "method_details":"Resnet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-05-17",
        "metrics":{
            "Top 1 Accuracy":"78.2%",
            "Number of Params":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.2,
            "Number of Params":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":800164,
            "title":"Divide and Contrast: Self-supervised Learning from Uncurated Data",
            "url":"\/paper\/divide-and-contrast-self-supervised-learning",
            "published":"2021-05-17T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            },
            {
                "id":33,
                "name":"ResNet-50",
                "color":"#c17b9b"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":39932,
        "rank":61,
        "method":"SwAV (Resnet-50)",
        "mlmodel":{

        },
        "method_short":"SwAV ",
        "method_details":"Resnet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-06-17",
        "metrics":{
            "Top 1 Accuracy":"77.8%",
            "Number of Params":"182M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.8,
            "Number of Params":182000000.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":202916,
            "title":"Unsupervised Learning of Visual Features by Contrasting Cluster Assignments",
            "url":"\/paper\/unsupervised-learning-of-visual-features-by",
            "published":"2020-06-17T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/unsupervised-learning-of-visual-features-by\/review\/?hl=39932"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":33,
                "name":"ResNet-50",
                "color":"#c17b9b"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            },
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":107,
                "name":"IG-1B",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":39899,
        "rank":62,
        "method":"MoCo (Resnet-50)",
        "mlmodel":{

        },
        "method_short":"MoCo ",
        "method_details":"Resnet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-13",
        "metrics":{
            "Top 1 Accuracy":"77.3%",
            "Number of Params":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.3,
            "Number of Params":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":172144,
            "title":"Momentum Contrast for Unsupervised Visual Representation Learning",
            "url":"\/paper\/momentum-contrast-for-unsupervised-visual",
            "published":"2019-11-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/momentum-contrast-for-unsupervised-visual\/review\/?hl=39899"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":33,
                "name":"ResNet-50",
                "color":"#c17b9b"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            },
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":107,
                "name":"IG-1B",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":39933,
        "rank":63,
        "method":"SimCLR (Resnet-50)",
        "mlmodel":{

        },
        "method_short":"SimCLR ",
        "method_details":"Resnet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2020-02-13",
        "metrics":{
            "Top 1 Accuracy":"77.2%",
            "Number of Params":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.2,
            "Number of Params":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":183167,
            "title":"A Simple Framework for Contrastive Learning of Visual Representations",
            "url":"\/paper\/a-simple-framework-for-contrastive-learning",
            "published":"2020-02-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/a-simple-framework-for-contrastive-learning\/review\/?hl=39933"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":33,
                "name":"ResNet-50",
                "color":"#c17b9b"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            },
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":107,
                "name":"IG-1B",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":39931,
        "rank":64,
        "method":"MoCo (Resnet-50)",
        "mlmodel":{

        },
        "method_short":"MoCo ",
        "method_details":"Resnet-50",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-11-13",
        "metrics":{
            "Top 1 Accuracy":"77.0%",
            "Number of Params":null
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.0,
            "Number of Params":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":172144,
            "title":"Momentum Contrast for Unsupervised Visual Representation Learning",
            "url":"\/paper\/momentum-contrast-for-unsupervised-visual",
            "published":"2019-11-13T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/momentum-contrast-for-unsupervised-visual\/review\/?hl=39931"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            },
            {
                "id":3,
                "name":"ResNet",
                "color":"#2771D3"
            },
            {
                "id":33,
                "name":"ResNet-50",
                "color":"#c17b9b"
            }
        ],
        "reports":[

        ]
    },
    {
        "table_id":7112,
        "row_id":27572,
        "rank":65,
        "method":"DeeperCluster (VGG16)",
        "mlmodel":{

        },
        "method_short":"DeeperCluster ",
        "method_details":"VGG16",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2019-05-03",
        "metrics":{
            "Top 1 Accuracy":"74.9%",
            "Number of Params":"138M"
        },
        "raw_metrics":{
            "Top 1 Accuracy":74.9,
            "Number of Params":138000000.0
        },
        "uses_additional_data":true,
        "paper":{
            "id":113519,
            "title":"Unsupervised Pre-Training of Image Features on Non-Curated Data",
            "url":"\/paper\/leveraging-large-scale-uncurated-data-for",
            "published":"2019-05-03T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/leveraging-large-scale-uncurated-data-for\/review\/?hl=27572"
        },
        "external_source_url":null,
        "tags":[
            {
                "id":16,
                "name":"VGG",
                "color":"#9e3dff"
            },
            {
                "id":106,
                "name":"YFCC100M",
                "color":"#2771D3"
            },
            {
                "id":17,
                "name":"CNN",
                "color":"#2771D3"
            }
        ],
        "reports":[

        ]
    }
]