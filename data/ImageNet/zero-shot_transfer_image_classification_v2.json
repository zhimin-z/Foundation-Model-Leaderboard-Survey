[
    {
        "table_id":13782,
        "row_id":97332,
        "rank":1,
        "method":"BASIC (Lion)",
        "mlmodel":{

        },
        "Model":"BASIC ",
        "method_details":"Lion",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":null,
        "metrics":{
            "Accuracy (Private)":"81.2",
            "Accuracy (Public)":null
        },
        "raw_metrics":{
            "Accuracy (Private)":81.2,
            "Accuracy (Public)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":null,
            "title":null,
            "url":null,
            "published":null,
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":13782,
        "row_id":97011,
        "rank":2,
        "method":"LiT-22B",
        "mlmodel":{

        },
        "Model":"LiT-22B",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-02-10",
        "metrics":{
            "Accuracy (Private)":"80.9",
            "Accuracy (Public)":null
        },
        "raw_metrics":{
            "Accuracy (Private)":80.9,
            "Accuracy (Public)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1155994,
            "title":"Scaling Vision Transformers to 22 Billion Parameters",
            "url":"\/paper\/scaling-vision-transformers-to-22-billion",
            "published":"2023-02-10T00:00:00.000000",
            "code":false,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":13782,
        "row_id":54010,
        "rank":3,
        "method":"CoCa",
        "mlmodel":{

        },
        "Model":"CoCa",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-05-04",
        "metrics":{
            "Accuracy (Private)":"80.7",
            "Accuracy (Public)":null
        },
        "raw_metrics":{
            "Accuracy (Private)":80.7,
            "Accuracy (Public)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1004211,
            "title":"CoCa: Contrastive Captioners are Image-Text Foundation Models",
            "url":"\/paper\/coca-contrastive-captioners-are-image-text",
            "published":"2022-05-04T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/coca-contrastive-captioners-are-image-text\/review\/?hl=54010"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":13782,
        "row_id":43081,
        "rank":4,
        "method":"BASIC",
        "mlmodel":{

        },
        "Model":"BASIC",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-19",
        "metrics":{
            "Accuracy (Private)":"80.6",
            "Accuracy (Public)":null
        },
        "raw_metrics":{
            "Accuracy (Private)":80.6,
            "Accuracy (Public)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":912981,
            "title":"Combined Scaling for Zero-shot Transfer Learning",
            "url":"\/paper\/combined-scaling-for-zero-shot-transfer",
            "published":"2021-11-19T00:00:00.000000",
            "code":false,
            "review_url":"\/paper\/combined-scaling-for-zero-shot-transfer\/review\/?hl=43081"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":13782,
        "row_id":70429,
        "rank":5,
        "method":"LiT ViT-e",
        "mlmodel":{

        },
        "Model":"LiT ViT-e",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-14",
        "metrics":{
            "Accuracy (Private)":"80.6",
            "Accuracy (Public)":null
        },
        "raw_metrics":{
            "Accuracy (Private)":80.6,
            "Accuracy (Public)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1074922,
            "title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model",
            "url":"\/paper\/pali-a-jointly-scaled-multilingual-language",
            "published":"2022-09-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/pali-a-jointly-scaled-multilingual-language\/review\/?hl=70429"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":13782,
        "row_id":42557,
        "rank":6,
        "method":"LiT-tuning",
        "mlmodel":{

        },
        "Model":"LiT-tuning",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-11-15",
        "metrics":{
            "Accuracy (Private)":"78.7",
            "Accuracy (Public)":" 66.6"
        },
        "raw_metrics":{
            "Accuracy (Private)":78.7,
            "Accuracy (Public)":66.6
        },
        "uses_additional_data":false,
        "paper":{
            "id":909846,
            "title":"LiT: Zero-Shot Transfer with Locked-image text Tuning",
            "url":"\/paper\/lit-zero-shot-transfer-with-locked-image-text",
            "published":"2021-11-15T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/lit-zero-shot-transfer-with-locked-image-text\/review\/?hl=42557"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":13782,
        "row_id":111669,
        "rank":7,
        "method":"EVA-CLIP-E\/14+",
        "mlmodel":{

        },
        "Model":"EVA-CLIP-E\/14+",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-27",
        "metrics":{
            "Accuracy (Private)":"75.7",
            "Accuracy (Public)":null
        },
        "raw_metrics":{
            "Accuracy (Private)":75.7,
            "Accuracy (Public)":null
        },
        "uses_additional_data":true,
        "paper":{
            "id":1180718,
            "title":"EVA-CLIP: Improved Training Techniques for CLIP at Scale",
            "url":"\/paper\/eva-clip-improved-training-techniques-for",
            "published":"2023-03-27T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/eva-clip-improved-training-techniques-for\/review\/?hl=111669"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":13782,
        "row_id":42558,
        "rank":8,
        "method":"ALIGN",
        "mlmodel":{

        },
        "Model":"ALIGN",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-11",
        "metrics":{
            "Accuracy (Private)":" 70.1",
            "Accuracy (Public)":"-"
        },
        "raw_metrics":{
            "Accuracy (Private)":70.1,
            "Accuracy (Public)":0
        },
        "uses_additional_data":false,
        "paper":{
            "id":744362,
            "title":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision",
            "url":"\/paper\/scaling-up-visual-and-vision-language",
            "published":"2021-02-11T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-up-visual-and-vision-language\/review\/?hl=42558"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":13782,
        "row_id":42559,
        "rank":9,
        "method":"CLIP",
        "mlmodel":{

        },
        "Model":"CLIP",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-02-26",
        "metrics":{
            "Accuracy (Private)":"70.1",
            "Accuracy (Public)":"-"
        },
        "raw_metrics":{
            "Accuracy (Private)":70.1,
            "Accuracy (Public)":0
        },
        "uses_additional_data":false,
        "paper":{
            "id":749733,
            "title":"Learning Transferable Visual Models From Natural Language Supervision",
            "url":"\/paper\/learning-transferable-visual-models-from",
            "published":"2021-02-26T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/learning-transferable-visual-models-from\/review\/?hl=42559"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":13782,
        "row_id":79795,
        "rank":10,
        "method":"AltCLIP",
        "mlmodel":{

        },
        "Model":"AltCLIP",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-11-12",
        "metrics":{
            "Accuracy (Private)":"68.1",
            "Accuracy (Public)":null
        },
        "raw_metrics":{
            "Accuracy (Private)":68.1,
            "Accuracy (Public)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1110840,
            "title":"AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities",
            "url":"\/paper\/altclip-altering-the-language-encoder-in-clip",
            "published":"2022-11-12T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/altclip-altering-the-language-encoder-in-clip\/review\/?hl=79795"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":13782,
        "row_id":70426,
        "rank":11,
        "method":"PaLI",
        "mlmodel":{

        },
        "Model":"PaLI",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2022-09-14",
        "metrics":{
            "Accuracy (Private)":"64.46",
            "Accuracy (Public)":null
        },
        "raw_metrics":{
            "Accuracy (Private)":64.46,
            "Accuracy (Public)":null
        },
        "uses_additional_data":false,
        "paper":{
            "id":1074922,
            "title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model",
            "url":"\/paper\/pali-a-jointly-scaled-multilingual-language",
            "published":"2022-09-14T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/pali-a-jointly-scaled-multilingual-language\/review\/?hl=70426"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]