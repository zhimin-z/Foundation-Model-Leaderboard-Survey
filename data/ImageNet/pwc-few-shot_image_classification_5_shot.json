[
    {
        "table_id":9956,
        "row_id":34964,
        "rank":1,
        "Model":"ViT-MoE-15B (Every-2)",
        "mlmodel":{

        },
        "method_short":"ViT-MoE-15B ",
        "method_details":"Every-2",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-10",
        "metrics":{
            "Top 1 Accuracy":"82.78"
        },
        "raw_metrics":{
            "Top 1 Accuracy":82.78
        },
        "uses_additional_data":false,
        "paper":{
            "id":816109,
            "title":"Scaling Vision with Sparse Mixture of Experts",
            "url":"\/paper\/scaling-vision-with-sparse-mixture-of-experts",
            "published":"2021-06-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-vision-with-sparse-mixture-of-experts\/review\/?hl=34964"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":9956,
        "row_id":100968,
        "rank":2,
        "Model":"MAWS (ViT-2B)",
        "mlmodel":{

        },
        "method_short":"MAWS ",
        "method_details":"ViT-2B",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2023-03-23",
        "metrics":{
            "Top 1 Accuracy":"81.5"
        },
        "raw_metrics":{
            "Top 1 Accuracy":81.5
        },
        "uses_additional_data":true,
        "paper":{
            "id":1179156,
            "title":"The effectiveness of MAE pre-pretraining for billion-scale pretraining",
            "url":"\/paper\/the-effectiveness-of-mae-pre-pretraining-for",
            "published":"2023-03-23T00:00:00.000000",
            "code":true,
            "review_url":null
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":9956,
        "row_id":34960,
        "rank":3,
        "Model":"V-MoE-H\/14 (Every-2)",
        "mlmodel":{

        },
        "method_short":"V-MoE-H\/14 ",
        "method_details":"Every-2",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-10",
        "metrics":{
            "Top 1 Accuracy":"78.21"
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.21
        },
        "uses_additional_data":false,
        "paper":{
            "id":816109,
            "title":"Scaling Vision with Sparse Mixture of Experts",
            "url":"\/paper\/scaling-vision-with-sparse-mixture-of-experts",
            "published":"2021-06-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-vision-with-sparse-mixture-of-experts\/review\/?hl=34960"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":9956,
        "row_id":34955,
        "rank":4,
        "Model":"V-MoE-H\/14 (Last-5)",
        "mlmodel":{

        },
        "method_short":"V-MoE-H\/14 ",
        "method_details":"Last-5",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-10",
        "metrics":{
            "Top 1 Accuracy":"78.08"
        },
        "raw_metrics":{
            "Top 1 Accuracy":78.08
        },
        "uses_additional_data":false,
        "paper":{
            "id":816109,
            "title":"Scaling Vision with Sparse Mixture of Experts",
            "url":"\/paper\/scaling-vision-with-sparse-mixture-of-experts",
            "published":"2021-06-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-vision-with-sparse-mixture-of-experts\/review\/?hl=34955"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":9956,
        "row_id":34950,
        "rank":5,
        "Model":"V-MoE-L\/16 (Every-2)",
        "mlmodel":{

        },
        "method_short":"V-MoE-L\/16 ",
        "method_details":"Every-2",
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-10",
        "metrics":{
            "Top 1 Accuracy":"77.1"
        },
        "raw_metrics":{
            "Top 1 Accuracy":77.1
        },
        "uses_additional_data":false,
        "paper":{
            "id":816109,
            "title":"Scaling Vision with Sparse Mixture of Experts",
            "url":"\/paper\/scaling-vision-with-sparse-mixture-of-experts",
            "published":"2021-06-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-vision-with-sparse-mixture-of-experts\/review\/?hl=34950"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    },
    {
        "table_id":9956,
        "row_id":34945,
        "rank":6,
        "Model":"VIT-H\/14",
        "mlmodel":{

        },
        "method_short":"VIT-H\/14",
        "method_details":null,
        "mlmodel_short":null,
        "mlmodeldetails":null,
        "evaluation_date":"2021-06-10",
        "metrics":{
            "Top 1 Accuracy":"76.95"
        },
        "raw_metrics":{
            "Top 1 Accuracy":76.95
        },
        "uses_additional_data":false,
        "paper":{
            "id":816109,
            "title":"Scaling Vision with Sparse Mixture of Experts",
            "url":"\/paper\/scaling-vision-with-sparse-mixture-of-experts",
            "published":"2021-06-10T00:00:00.000000",
            "code":true,
            "review_url":"\/paper\/scaling-vision-with-sparse-mixture-of-experts\/review\/?hl=34945"
        },
        "external_source_url":null,
        "tags":[

        ],
        "reports":[

        ]
    }
]