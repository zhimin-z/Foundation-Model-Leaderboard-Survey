Leaderboard,Benchmark
AgentBench,"['Mind2Web', 'ALFWorld', 'WebShop']"
AlpacaEval (v2),"['Koala', 'OASST1', 'Vicuna', 'Self-Instruct', 'HH-RLHF']"
BBH,"['Adjective Order', 'Sorting Words', 'Sports Understanding', 'Sequences', 'Ruin a Name with One Edit', 'Salient Translation Error Detection', 'Tracking Shuffled Objects', 'Multistep Arithmetic', 'Logical Deduction', 'Disambiguation QA', 'Dyck Languages', 'Causal Judgment', 'Tables of Penguins', 'Boolean Expressions', 'Formal Fallacies and Syllogisms with Negation', 'Reasoning about Colored Objects', 'SNARKS', 'Object Counting', 'Geometric Shapes', 'Date Understanding', 'Navigation', 'MovieLens', 'Web of Lies']"
BEIR,"['MSMARCO', 'BioASQ', 'Climate-FEVER', 'NFCorpus', 'Signal-1M', 'SciFact', 'CQADupStack (gis)', 'FiQA (2018)', 'Robust04', 'DBpedia', 'HotpotQA', 'ArguAna', 'SciDocs', 'Touche (2020)', 'QQP', 'FEVER', 'NQ', 'TREC-News', 'TREC-COVID']"
Big Code Models Leaderboard,"['MultiPL-E', 'HumanEval']"
BIG-Bench,"['SGD', 'Disfl-QA', 'Forecasting Subquestions', 'State Tracking in Chess', 'Color', 'Self-awareness', 'Simple Text Editing', 'Implicatures', 'Simple multiple choice arithmetic (JSON)', 'Simple arithmetic with multiple targets (JSON)', 'Simple arithmetic', 'characterRelations', 'ParsiNLU (rc)', 'GEM', 'Sports Understanding', 'Spelling Bee', 'GRE Reading Comprehension', 'Key Value Maps', 'Modified Arithmetic', 'Salient Translation Error Detection', 'CRT', 'Self Evaluation of Tutoring', 'Sudoku', 'YesNoBlackWhite Game', 'Which Wiki Edit', 'Tables of Penguins', 'Boolean Expressions', 'Codenames', 'Formal Fallacies and Syllogisms with Negation', 'TimeDial', 'CoQA', 'ASCII Word Recognition', 'English to Russian Proverbs', 'COPA (qa)', 'Subject-Verb Agreement', 'Geometric Shapes', 'Wikimedia', 'ASCII MNIST', 'UnQover', 'Strange Stories', 'Hindu Mythology Trivia', 'Understanding Grammar of Unseen Words', 'Hinglish Toxicity Prediction', 'MNLI (ipa)', 'Periodic Elements', 'Keyword Sentence Transformation', 'High Low Guessing Game', 'Emoji Movie', 'MultiEmo', 'Social Support', 'Taboo', 'CIFAR-10', 'Indic Cause and Effect', 'Sorting Words', 'Cornell Movie-Dialogs Corpus', 'WinoWhy', 'Sequences', 'BBQ-Lite', 'Intersection Points', 'Misconceptions (ru)', 'Tracking Shuffled Objects', 'Multistep Arithmetic', 'Simple arithmetic with subtasks (JSON)', 'COM2SENSE', 'Disambiguation QA', 'SQuAD (pp)', 'Odd One Out', 'StrategyQA', 'HHH', 'Cause and Effect', 'Common Morpheme', 'Logical Arguments', 'Gender Sensitivity Test (English)', 'Identify Math Theorems', 'MNLI (transliteration)', 'Goal-Step Inference', 'Python Program Synthesis', 'Fact-Checking', 'Word Problems on Sets and Graphs', 'Intent Recognition', 'Sequential Order', 'Persian Idioms', 'Navigation', 'Wikidata', 'Linguistic Puzzles', 'Sufficient Information', 'Long Input Contexts', 'Repeat Copy Logic', 'Muslim-Violence Bias', 'Similarities Test for Abstraction', 'SIT', 'RiddleSense', 'Root Finding, Optimization and Games', 'BBQ-Lite (Json)', 'Wino-X (German)', 'Minute Mysteries QA', 'ISNotes', 'Data Wrangling', 'ARC (The Abstraction and Reasoning Corpus)', 'The Essential, the Excessive, and the Extraneous', 'Irony Identification', 'Verb Tense', 'Unnatural In-Context Learning', 'What is the Tao', 'Code Description', 'Hindi Question Answering', 'Training on the test set', 'Adjective Order', 'Swahili-English Paremiologic Competence', 'Scientific Press Release', 'CS Algorithms', 'Physical Intuition', 'Phrase Relatedness', 'Metaphor Boolean', 'KPWr', 'Matrix Shapes', 'RoFT', 'Self Evaluation Courtroom', 'Authorship Verification', 'Unit Interpretation', 'Misconceptions', 'General Knowledge', 'Logical Deduction', 'Figure of Speech Detection', 'MathQA', 'Kanji ASCII Art', 'Dyck Languages', 'Mathematical Induction', 'Logic Grid Puzzles', 'LTI LangID Corpus', 'Topical-Chat', 'Empirical Judgments', 'PARSINLU (qa)', 'SIQA', 'TalkDown', 'Simple Ethical Questions', 'Object Counting', 'SNARKS', 'Kannada Riddles', 'Analytic Entailment', 'SQuADShifts', 'Twenty Questions', 'Unit Conversion', 'Conlang Translation Problems', 'Estimating Risk of Suicide', 'Date Understanding', 'Word Unscrambling', 'Physics Multiple Choice', 'TellMeWhy', 'Spider', 'Python Programming', 'Factuality', 'Physics Questions', 'Swedish to German proverbs', 'VitaminC', 'Checkmate In One Move', 'Metaphor Understanding', 'Identifying Anachronisms', 'Judging Moral Permissibility', 'Presuppositions as NLI', 'Medical Questions in Russian', 'SParC', 'Linguistic Mappings', 'Entailed Polarity', 'Diverse Metrics for Social Biases in Language Models', 'ePiC', 'List Functions', 'Protein Interaction Sites', 'Reordering', 'Known Unknowns', 'Text Navigation Game', 'Conceptual Combinations', 'Dynamic Counting', 'TruthfulQA', 'Automatic Debugging', 'Alignment of Simplicity Priors for Turing (Complete Concept Learning)', 'Understanding Fables', 'Ruin a Name with One Edit', 'Discovery', 'Cryptonite', 'Autoclassification', 'Language Games', 'Human Organs and Senses', 'Transforming German Sentences to Gender (Inclusive Forms)', 'Causal Judgment', 'Gender Sensitivity Test (zh)', 'Cycled Letters', 'Simple arithmetic (JSON)', 'Cryobiology Spanish', 'CRASS', 'Analogical Similarity', 'Entailed Polarity in Hindi', 'Informal and Formal Fallacies', 'Natural Instructions', 'Reasoning about Colored Objects', 'Rhyming', 'Arithmetic', 'Social Bias from Sentence Probability', 'SNLI', 'Novel Concepts', 'CoDA', 'Truthful QA', 'Crash Blossoms', 'Dark Humor Detection', 'EmoTag1200', 'Identify Odd Metaphor', 'English Proverbs', 'MovieLens', 'Fantasy Reasoning', 'Sentence Ambiguity', 'NQ', 'Operators', 'Shakespeare Dialogue', 'Web of Lies']"
Chatbot Arena Leaderboard,"['MMLU', 'Chatbot Arena Conversations', 'MT-Bench']"
ChEF,"['ScienceQA', 'Flickr30K', 'VOC (2012)', 'MSCOCO', 'FSC147', 'MME', 'CIFAR-10', 'SEED-Bench', 'Omnibenchmark', 'MMBench']"
CLUE,"['OCNLI', 'CLUE Diagnostics', 'TNEWS', 'CMNLI', 'iFLYTEK', 'CMRC (2018)', 'DRCD', 'ChID', 'WSC (CLUE)', 'CSL', 'AFQMC', 'C3']"
CMB,"['CMB-Exam', 'CMB-Clin']"
CMTEB,"['Multi-CPR (ecom)', 'cMedQA (v2) (reranking)', 'OCNLI', 'DuReader (retrieval)', 'Waimai', 'BQ', 'THUCNews (s2s)', 'mMARCO (retrieval)', 'cMedQA (v2) (retrieval)', 'Multi-CPR (video)', 'CMNLI', 'ATEC', 'T2Ranking (reranking)', 'CSL (p2p)', 'LCQMC', 'cCOVID-News', 'THUCNews (p2p)', 'PAWS-X', 'T2Ranking (retrieval)', 'JDReview', 'TNEWS', 'QBQTC', 'Multi-CPR (medical)', 'cMedQA (reranking)', 'STS-B (zh)', 'OnlineShopping', 'CSL (s2s)', 'iFLYTEK', 'AFQMC', 'mMARCO (reranking)']"
Colossal-AI,"['MMLU', 'C-Eval', 'CMMLU', 'AGIEval', 'GAOKAO-Bench']"
CoQA,"['Project Gutenberg', 'CNN DM', 'WritingPrompts', 'RACE', 'Wikipedia', 'MCTest', 'SciQ']"
EvalPlus,"['HumanEval', 'MBPP']"
FacTool,"['FactPrompts', 'HumanEval', 'RoSE', 'GSM8K', 'Self-Instruct']"
FewCLUE,"['OCNLI', 'TNEWS', 'iFLYTEK', 'EPRSTMT', 'ChID', 'CSLDCP', 'WSC (CLUE)', 'CSL', 'BUSTM']"
FlagEval,"['COCO-Stuff', 'OCNLI', 'C-SEM', 'MMLU', 'Places', 'LibriSpeech', 'FGVC-Aircraft', 'ADE20K', 'TDIUC', 'BUSTM', 'Stanford Cars', 'TruthfulQA', 'CUB', 'CMMLU', 'Flowers102', 'KITTI Eigen split', 'SOP', 'UCF101', 'DTD', 'IEMOCAP', 'VQA (v2)', 'IMDB', 'TNEWS', 'Flickr30K', 'HumanEval', 'RAFT', 'ChID', 'CelebA-HQ', 'WSC (CLUE)', 'MSRVTT', 'CSL', 'iNaturalist (2018)', 'KeSpeech', 'CLCC', 'VQA-CP', 'Food-101', 'EPRSTMT', 'MSCOCO', 'Cityscapes', 'AISHELL-1', 'GAOKAO-Bench (2023)', 'NYU-Depth', 'ImageNet-1K', 'BoolQ']"
GENIE,"['XSUM', 'ARC-DA (2018)', 'a-NLG', 'WMT (2021) (de-en)', 'WMT (2019) (de-en)']"
HEIM,"['MSCOCO (es)', 'MSCOCO (zh)', 'Magazine Cover Photos', 'CUB', 'MSCOCO (fidelity)', 'PartiPrompts (reasoning categories)', 'CSP', 'Relational Understanding', 'DrawBench (knowledge categories)', 'Winoground', 'MSCOCO (base)', 'MSCOCO (robustness - typos)', 'DrawBench (image quality categories)', 'PartiPrompts (knowledge categories)', 'Demographic Stereotypes', 'PartiPrompts (image quality categories)', 'MSCOCO (art styles)', 'PaintSkills', 'DrawBench (reasoning categories)', 'Mental Disorders', 'MSCOCO (fairness - AAVE dialect)', 'MSCOCO (efficiency)', 'Landing Pages', 'MSCOCO (fairness - gender)', 'Dailydall.e', 'MSCOCO (hi)', 'MSCOCO', 'Historical Figures', 'I2P', 'Logos']"
HellaSwag Leaderboard,"['HellaSwag', 'ActivityNet Captions', 'WikiHow']"
HELM Classic,"['MSMARCO', 'MMLU', 'Civil Comments', 'EntityMatching', 'OpenbookQA', 'Synthetic efficiency', 'LegalBench', 'BBQ', 'LSAT', 'QuAC', 'Copyright (text)', 'TruthfulQA', 'Copyright (code)', 'Disinformation (reiteration)', 'RealToxicityPrompts', 'MATH (chain-of-thought)', 'BOLD', 'Synthetic Reasoning (natural)', 'XSUM', 'LegalSupport', 'DataImputation', 'Dyck Languages', 'bAbI', 'MATH', 'NarrativeQA', 'BLiMP', 'IMDB', 'TwitterAAE', 'ICE', 'CNN DM', 'Synthetic Reasoning (symbolic)', 'Disinformation (wedging)', 'HumanEval', 'RAFT', 'WikiFact', 'TwitterAAE (white)', 'WMT (2014)', 'EurLexSum', 'The Pile', 'GSM8K', 'MedQA', 'Billsum', 'TwitterAAE (aa)', 'Numerical reasoning', 'MSMARCO (v2)', 'NQ (open-book)', 'NQ (closed-book)', 'APPS', 'MultiLexSum', 'HellaSwag', 'BoolQ']"
HELM Lite,"['MMLU', 'NQ (open-book)', 'OpenbookQA', 'NQ (closed-book)', 'LegalBench', 'WMT (2014)', 'MATH', 'GSM8K', 'MedQA', 'NarrativeQA']"
HHEM Leaderboard,['CNN DM']
InstructEval,"['BBH', 'MMLU', 'HumanEval', 'DROP', 'IMPACT', 'HHH', 'CRASS']"
InterCode,"['MBPP', 'Spider', 'SWE-bench', 'NL2Bash', 'InterCode-CTF']"
KoLA,"['ETA', 'COPEN (CPJ)', 'ETU', 'DocRED', 'KoRC', 'MAVEN', 'MAVEN-ERE', 'High-Frequency Knowledge', 'Low-Frequency Knowledge', 'KQA Pro', 'MuSiQue', 'COPEN (CSJ)', 'HotpotQA', '2WikiMQA', 'Encyclopedic', 'FewNERD', 'COPEN (CiC)', 'ETM', 'ETC']"
L-Eval,"['TopicRet', 'CUAD', 'MultiDoc2Dial', 'Qasper', 'QMSum', 'SPACE', 'SFiction', 'QuALITY', 'NarrativeQA', 'SummScreen', 'BigPatent', 'LONGFQA', 'Coursera', 'TOEFL-QA', 'GSM8K', 'CodeU', 'Multi-News', 'NQ', 'GovReport', 'OPENREVIEW']"
LAiW Leaderboard,"['CAIL (2018)', 'CAIL (2020)', 'Criminal-S', 'MSJudge', 'CrimeKgAssitant', 'CAIL (2021)', 'MLMN', 'CAIL (2019)', 'AC-NLG', 'CFM', 'CJRC', 'JEC-QA']"
Large Language Model Leaderboard,"['OCNLI', 'BBH', 'MMLU', 'RACE', 'OpenbookQA', 'PIQA', 'SummEdits', 'BUSTM', 'COPA', 'CMMLU', 'CSQA', 'TheoremQA', 'CMNLI', 'MBPP', 'SQuAD (v2)', 'WSC', 'XSUM', 'Winogender Diagnostic', 'DRCD', 'DROP', 'MATH', 'Xiezhi', 'Broad-Coverage Diagnostic', 'StoryCloze', 'MultiRC', 'GAOKAO-Bench', 'Flores-101', 'GAOKAO-Bench (2023)', 'TNEWS', 'SIQA', 'ARC', 'HumanEval', 'ChID', 'LCSTS', 'CSL', 'CMRC', 'GSM8K', 'WiC', 'AGIEval', 'CB', 'LAMBADA', 'C3', 'ReCoRD', 'EPRSTMT', 'RTE', 'WinoGrande', 'TyDiQA', 'AFQMC', 'NQ', 'C-Eval', 'HellaSwag', 'BoolQ', 'TriviaQA']"
LLM Benchmarker Suite,"['MMLU', 'HumanEval', 'OpenbookQA', 'WinoGrande', 'QuAC', 'NQ', 'GSM8K', 'HellaSwag', 'AGIEval', 'BoolQ', 'TriviaQA']"
LLM-Leaderboard,"['MMLU', 'HumanEval', 'Chatbot Arena Conversations', 'WinoGrande', 'HellaSwag', 'LAMBADA', 'TriviaQA']"
LongBench,"['LCC', 'Qasper', 'TREC', 'QMSum', 'MultiFieldQA (zh)', 'RepoBench-P', 'MultiFieldQA', 'NarrativeQA', 'MuSiQue', 'HotpotQA', 'LSHTC', 'SAMSum', 'DuReader', '2WikiMQA', 'PassageRetrieval (zh)', 'VCSUM', 'PassageRetrieval', 'PassageCount', 'Multi-News', 'GovReport', 'TriviaQA']"
LVLM-eHub,"['CTW', 'IIIT5K', 'Total-Text', 'Whoops', 'Meta-World', 'VisDial', 'ImageNetVC', 'CIFAR-10', 'SNLI-VE', 'IC (2015)', 'Flowers102', 'VCR-MCI', 'GQA', 'HOST', 'Minecraft', 'OCR-VQA', 'NoCaps', 'SROIE', 'WOST', 'MSCOCO (random)', 'Pets37', 'VSR', 'Virtual Home', 'COCO-Text', 'Franka Kitchen', 'Flickr30K', 'MSCOCO (OC)', 'SVT', 'IC (2013)', 'SVTP', 'MSCOCO (MCI)', 'STVQA', 'VCR', 'MSCOCO (adversarial)', 'WordArt', 'IconQA', 'VCR-OC', 'ScienceQA IMG', 'VizWiz', 'FUNSD', 'DocVQA', 'MSCOCO (popular)', 'TextVQA', 'CUTE80', 'ImageNet-1K', 'OK-VQA']"
MedBench,"['MedHG', 'DBMHG', 'DrugCA', 'CHIP-CDN', 'MedHC', 'DDx-basic', 'SMDoc', 'CHIP-CTC', 'MedDG', 'MedMC', 'MedTreat', 'IMCS-MRG (v2)', 'SafetyBench', 'DDx-advanced', 'MedSpeQA', 'CMeEE', 'Med-Exam', 'CMB-Clin', 'CMeIE', 'CHIP-CDEE']"
MMBench,"['ARAS', 'Places', 'ScienceQA', 'KonIQ-10k', 'LLaVA-Bench', 'VSR', 'W3C School', 'COCO Captions', 'CLEVR', 'TextVQA', 'Internet', 'PISC']"
MMLU-by-task Leaderboard,"['MMLU', 'ARC', 'HellaSwag']"
MOCHA,"['Quoref', 'SIQA', 'CosmosQA', 'DROP', 'MCScript', 'NarrativeQA']"
MTEB,"['DKhate', 'CQADupStack (mathematica)', 'CQADupStack (gaming)', 'NFCorpus', 'ScaLA (nn)', 'B77', 'BIOSSES', 'ScaLA (sv)', 'SciFact', 'bioRxiv (p2p)', 'STS (2014)', 'CQADupStack (unix)', 'HotpotQA (pl)', 'FiQA (2018)', 'Amazon Polarity', 'LinkSO', 'MTOP (domain)', 'DBpedia', 'arXiv (p2p)', 'SweRec', 'arXiv (s2s)', 'STS (2016)', 'PAC', 'Blurbs (s2s)', 'Nordic Language Identification', 'SweFAQ (v2)', 'Climate-FEVER', 'Civil Comments (tc)', 'Allegro Reviews', 'ScaLA (nb)', 'Bornholmsk', 'SciDocs (pl)', 'STS (2017)', 'CQADupStack (stats)', 'CQADupStack (programmers)', 'CARER', 'Polish CDSCorpus (relatedness)', 'PPC', 'DBpedia (pl)', 'DanishPoliticalComments', 'ArguAna', 'SciDocs', 'Blurbs (p2p)', 'MSMARCO (pl)', 'SICK (relatedness) (pl)', 'SummEval', 'Tatoeba', 'Reddit', 'QQP', 'MTOP (intent)', 'NoReC', 'MSMARCO (v2)', 'ArguAna (pl)', 'medRxiv (p2p)', '20 Newsgroups', 'CBD', 'TREC-COVID', 'MSMARCO', 'Reddit (p2p)', 'CQADupStack (tex)', 'LCC', 'BUCC', 'Stack Exchange', 'DaLAJ', 'STS (2013)', 'MASSIVE (intent)', 'SICK (pl)', 'Stack Exchange (p2p)', 'CQADupStack (wordpress)', 'Polish CDSCorpus', 'NQ (pl)', 'AskUbuntu', 'HotpotQA', 'Touche (2020)', 'medRxiv (s2s)', 'TwitterSemEval (2015)', 'FEVER', 'ScaLA (da)', 'LanguageNet', 'CQADupStack (webmasters)', 'QQP (pl)', 'FiQA (2018) (pl)', 'SciFact (pl)', 'STS (2022)', 'NPSC', 'MIND', 'STS (2015)', 'PolEmo (v2) (in)', 'SprintDuplicateQuestions', 'Tweet Sentiment Extraction', '10kGNAD (s2s)', '8TAGS', 'CQADupStack (gis)', 'SciDocs (rr)', 'PolEmo (v2) (out)', 'STS-B', 'MARC', 'IMDB', 'CQADupStack (android)', 'NFCorpus (pl)', '10kGNAD (p2p)', 'CQADupStack (english)', 'AMCD', 'PSC', 'SICK (relatedness)', 'STS (2012)', 'NQ', 'Angry Tweets', 'CQADupStack (physics)', 'MASSIVE (scenario)', 'bioRxiv (s2s)']"
Multi-modal Modal Leaderboard,"['MM-Vet', 'MMBench (test) (zh)', 'SEED-Bench (img)', 'MME (normalized)', 'MathVista (minitest)', 'CCBench', 'MMMU (val)', 'MMBench (test)', 'HallusionBench']"
MVBench,"['Charades-STA', 'MovieNet', 'MiT', 'NTU RGB+D', 'STAR', 'FunQA', 'PAXION', 'CLEVRER', 'TVQA', 'VLN-CE', 'Perception Test']"
Open Ko-LLM Leaderboard,"['CommonGen', 'ARC (Korean)', 'HellaSwag (Korean)', 'MMLU (Korean)', 'TruthfulQA (Korean)']"
Open LLM Leaderboard,"['MMLU', 'ARC', 'WinoGrande', 'TruthfulQA', 'GSM8K', 'HellaSwag']"
Open Multilingual LLM Evaluation Leaderboard,"['MMLU', 'ARC', 'TruthfulQA', 'HellaSwag']"
OpenEval (text),"['Self-awareness', 'SNLI (zh)', 'Guilt Law', 'BiPaR', 'CooridinateAI', 'CMMLU', 'One-box Tendency', 'CMNLI', 'COLD', 'TGEA', 'M3KE', 'GAOKAO-Bench', 'CORGI-PM', 'SQuAD (zh)', 'CBBQ', 'CommonMT', 'TUMCC', 'ChID', 'Corrigible', 'WSC (CLUE)', 'CDIAL-BIAS', 'WGlaw', 'C3', 'Myopia Reward', 'CAIL (2018)', 'OL-CC', 'SWSR', 'Power-seeking', 'WPLC', 'TOCP']"
PromptBench,"['GLUE', 'MMLU', 'NumerSense', 'QASC', 'SQuAD (v2)', 'MultiUN', 'BIG-Bench', 'MATH', 'GSM8K', 'IWSLT (2017)', 'CSQA']"
Q-Bench,"['LIVE-itw', 'LLVisionQA', 'KonIQ-10k', 'CGIQA-6K', 'LIVE-FB LSVQ', 'KADID-10K', 'SPAQ', 'LLDescribe', 'AGIQA-3K']"
RAFT,"['TweetEval (hate)', 'ADE Corpus (v2)', 'TC', 'ToS', 'OSE', 'SOT', 'B77', 'NIS', 'TAI', 'SRI', 'Over']"
ReCoRD,"['CNN DM', 'Internet Archive']"
ReForm-Eval,"['IIIT5K', 'Whoops', 'MSCOCO (MOS)', 'VizWiz (yesno)', 'ImageNetVC', 'CIFAR-10', 'VisDial', 'SNLI-VE', 'IC (2015)', 'TextOCR', 'MEDIC (dts)', 'Flowers102', 'ViQuAE', 'GQA', 'COCO-Text (Grounded)', 'ScienceQA', 'OCR-VQA', 'RefCOCO (res)', 'NoCaps', 'SROIE', 'POIE', 'TextCaps', 'Winoground', 'Pets37', 'VSR', 'MOCHEG', 'VQA (v2)', 'CLEVR', 'A-OKVQA', 'COCO-Text', 'VizWiz (singleChoice)', 'TDIUC (color)', 'MSCOCO (GOI)', 'TextOCR (Grounded)', 'Flickr30K', 'WikiHow', 'TDIUC (detection)', 'TDIUC (position)', 'MSCOCO (OC)', 'MSCOCO (MCI)', 'MP3D', 'TDIUC (scene)', 'TDIUC (counting)', 'TDIUC (sport)', 'WordArt', 'IC (2015) (Grounded)', 'FUNSD', 'DocVQA', 'TextVQA', 'CUTE80', 'TDIUC (utility)', 'ImageNet-1K', 'OK-VQA']"
RoleEval,"['RoleEval-Chinese', 'RoleEval-Global']"
ScandEval,"['SUC (v3)', 'ScaLA (da)', 'NoReC', 'ScandiQA (sv)', 'ScandiQA (da)', 'NorNE (nb)', 'DaNE', 'SweRec', 'ScaLA (nn)', 'ScandiQA (no)', 'Angry Tweets', 'ScaLA (nb)', 'ScaLA (sv)', 'NorNE (nn)']"
SCROLLS,"['SummScreen', 'QuALITY', 'ContractNLI', 'QMSum', 'Qasper', 'GovReport', 'NarrativeQA']"
SummEdits,"['Spotify Podcast', 'ECTSum', 'TinyShakespeare', 'SciTLDR', 'Sales Email', 'SAMSum', 'Google News', 'Sales Call', 'QMSum', 'Billsum']"
SuperCLUE,"['CLOSE Set', 'OPEN Set', 'CArena']"
SuperGLUE,"['ReCoRD', 'WiC', 'Broad-Coverage Diagnostic', 'WSC', 'Winogender Diagnostic', 'RTE', 'COPA', 'CB', 'MultiRC', 'BoolQ']"
SuperLim (v2),"['Swedish ABSAbank-Imm (v1.1)', 'Winogender Diagnostic (sv) (v2)', 'SweSAT Synonyms (v1.1)', 'SweDN', 'Winograd (sv) (v2)', 'Swedish analogy (v2)', 'SweWiC (v2)', 'SweFAQ (v2)', 'SuperSim (v2)', 'GLUE Diagnostic (sv)', 'STS-B (sv) (v2)', 'DaLAJ-GED-SuperLim (v2)', 'MNLI (sv)', 'Argumentation sentences']"
Video-Bench Leaderboard,"['MVQA', 'MSRVTT-QA', 'SQA3D', 'TGIF-QA', 'MOT', 'MSVD-QA', 'ActivityNet-QA', 'DLE', 'TVQA', 'YouCook2', 'DDM', 'NBAQA', 'UCF-Crime']"
VLM-Eval,"['HMDB51', 'UCF101', 'Kinetics-400', 'ActivityNet', 'MSRVTT', 'TGIF', 'MSVD']"
