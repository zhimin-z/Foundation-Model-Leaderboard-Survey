Leaderboard,Benchmark
AgentBench,"['ALFWorld', 'WebShop', 'Mind2Web']"
AlpacaEval (v2),"['HH-RLHF', 'Vicuna', 'OASST1', 'Koala', 'Self-Instruct']"
BBH,"['Boolean Expressions', 'Ruin a Name with One Edit', 'Disambiguation QA', 'Salient Translation Error Detection', 'Date Understanding', 'Sports Understanding', 'Tables of Penguins', 'Dyck Languages', 'Sorting Words', 'SNARKS', 'Navigation', 'Tracking Shuffled Objects', 'Causal Judgment', 'Web of Lies', 'Object Counting', 'Reasoning about Colored Objects', 'Adjective Order', 'Geometric Shapes', 'Multistep Arithmetic', 'Formal Fallacies and Syllogisms with Negation', 'Logical Deduction', 'MovieLens', 'Sequences']"
BEIR,"['Climate-FEVER', 'SciFact', 'MSMARCO', 'CQADupStack (gis)', 'NFCorpus', 'BioASQ', 'FiQA (2018)', 'Signal-1M', 'DBpedia', 'ArguAna', 'NQ', 'QQP', 'FEVER', 'SciDocs', 'Touche (2020)', 'Robust04', 'TREC-COVID', 'TREC-News', 'HotpotQA']"
Big Code Models Leaderboard,"['MultiPL-E', 'HumanEval']"
BIG-Bench,"['Text Navigation Game', 'Cryobiology Spanish', 'TimeDial', 'Boolean Expressions', 'GRE Reading Comprehension', 'Ruin a Name with One Edit', 'Topical-Chat', 'Odd One Out', 'Strange Stories', 'Arithmetic', 'Entailed Polarity', 'Date Understanding', 'State Tracking in Chess', 'Transforming German Sentences to Gender (Inclusive Forms)', 'MNLI (transliteration)', 'Sports Understanding', 'Natural Instructions', 'Persian Idioms', 'MultiEmo', 'BBQ-Lite (Json)', 'Phrase Relatedness', 'Simple arithmetic with subtasks (JSON)', 'Sorting Words', 'Analogical Similarity', 'Misconceptions', 'HHH', 'Factuality', 'Tracking Shuffled Objects', 'Logic Grid Puzzles', 'What is the Tao', 'VitaminC', 'Color', 'RoFT', 'Causal Judgment', 'CoDA', 'Adjective Order', 'Fact-Checking', 'ASCII Word Recognition', 'General Knowledge', 'Presuppositions as NLI', 'Formal Fallacies and Syllogisms with Negation', 'Scientific Press Release', 'Intersection Points', 'Kannada Riddles', 'Forecasting Subquestions', 'MovieLens', 'SGD', 'Known Unknowns', 'Subject-Verb Agreement', 'Logical Arguments', 'Sequences', 'Swahili-English Paremiologic Competence', 'Cryptonite', 'ePiC', 'Periodic Elements', 'Physical Intuition', 'Python Program Synthesis', 'Emoji Movie', 'SParC', 'The Essential, the Excessive, and the Extraneous', 'MNLI (ipa)', 'Wikimedia', 'Understanding Fables', 'GEM', 'COPA (qa)', 'Medical Questions in Russian', 'Dyck Languages', 'MathQA', 'List Functions', 'Simple arithmetic', 'WinoWhy', 'ParsiNLU (rc)', 'Crash Blossoms', 'Empirical Judgments', 'Goal-Step Inference', 'Navigation', 'English to Russian Proverbs', 'Python Programming', 'Human Organs and Senses', 'Entailed Polarity in Hindi', 'Analytic Entailment', 'Word Problems on Sets and Graphs', 'Codenames', 'Web of Lies', 'Muslim-Violence Bias', 'LTI LangID Corpus', 'Novel Concepts', 'ISNotes', 'BBQ-Lite', 'Geometric Shapes', 'Unit Conversion', 'Physics Questions', 'Long Input Contexts', 'Verb Tense', 'High Low Guessing Game', 'RiddleSense', 'Logical Deduction', 'CRASS', 'SNLI', 'Self Evaluation Courtroom', 'TruthfulQA', 'Wikidata', 'Training on the test set', 'Sequential Order', 'Gender Sensitivity Test (English)', 'Swedish to German proverbs', 'ARC (The Abstraction and Reasoning Corpus)', 'Truthful QA', 'CIFAR-10', 'Disambiguation QA', 'Root Finding, Optimization and Games', 'Intent Recognition', 'SQuAD (pp)', 'Which Wiki Edit', 'Sufficient Information', 'Sudoku', 'Disfl-QA', 'Estimating Risk of Suicide', 'Data Wrangling', 'Tables of Penguins', 'TalkDown', 'Understanding Grammar of Unseen Words', 'UnQover', 'Informal and Formal Fallacies', 'Modified Arithmetic', 'Identify Math Theorems', 'COM2SENSE', 'YesNoBlackWhite Game', 'KPWr', 'Physics Multiple Choice', 'English Proverbs', 'Self Evaluation of Tutoring', 'Diverse Metrics for Social Biases in Language Models', 'Simple arithmetic (JSON)', 'Code Description', 'Hindu Mythology Trivia', 'Common Morpheme', 'CoQA', 'Linguistic Mappings', 'Judging Moral Permissibility', 'Wino-X (German)', 'Hinglish Toxicity Prediction', 'Checkmate In One Move', 'characterRelations', 'CRT', 'Hindi Question Answering', 'Automatic Debugging', 'Alignment of Simplicity Priors for Turing (Complete Concept Learning)', 'Unnatural In-Context Learning', 'Word Unscrambling', 'Dynamic Counting', 'Simple multiple choice arithmetic (JSON)', 'Cycled Letters', 'Irony Identification', 'StrategyQA', 'Mathematical Induction', 'Protein Interaction Sites', 'Spelling Bee', 'Metaphor Boolean', 'SQuADShifts', 'EmoTag1200', 'Dark Humor Detection', 'CS Algorithms', 'Implicatures', 'Kanji ASCII Art', 'Matrix Shapes', 'Salient Translation Error Detection', 'Shakespeare Dialogue', 'ASCII MNIST', 'Conlang Translation Problems', 'Linguistic Puzzles', 'PARSINLU (qa)', 'Unit Interpretation', 'Spider', 'Rhyming', 'Language Games', 'Operators', 'Taboo', 'Conceptual Combinations', 'Keyword Sentence Transformation', 'Cornell Movie-Dialogs Corpus', 'SNARKS', 'Self-awareness', 'SIQA', 'Misconceptions (ru)', 'Autoclassification', 'Repeat Copy Logic', 'Similarities Test for Abstraction', 'Reordering', 'Social Bias from Sentence Probability', 'Cause and Effect', 'NQ', 'Key Value Maps', 'Object Counting', 'Gender Sensitivity Test (zh)', 'Minute Mysteries QA', 'Social Support', 'Reasoning about Colored Objects', 'Figure of Speech Detection', 'Sentence Ambiguity', 'Metaphor Understanding', 'Multistep Arithmetic', 'Indic Cause and Effect', 'Identifying Anachronisms', 'Discovery', 'Simple Ethical Questions', 'Simple arithmetic with multiple targets (JSON)', 'Authorship Verification', 'SIT', 'Simple Text Editing', 'TellMeWhy', 'Fantasy Reasoning', 'Twenty Questions', 'Identify Odd Metaphor']"
Chatbot Arena Leaderboard,"['MT-Bench', 'MMLU', 'Chatbot Arena Conversations']"
ChEF,"['VOC (2012)', 'FSC147', 'MMBench', 'MSCOCO', 'Flickr30K', 'CIFAR-10', 'Omnibenchmark', 'ScienceQA', 'SEED-Bench', 'MME']"
CLUE,"['CMNLI', 'CLUE Diagnostics', 'DRCD', 'TNEWS', 'iFLYTEK', 'AFQMC', 'ChID', 'OCNLI', 'C3', 'CMRC (2018)', 'CSL', 'WSC (CLUE)']"
CMB,"['CMB-Exam', 'CMB-Clin']"
CMTEB,"['CMNLI', 'T2Ranking (reranking)', 'CSL (s2s)', 'Multi-CPR (medical)', 'TNEWS', 'cMedQA (v2) (retrieval)', 'LCQMC', 'iFLYTEK', 'Multi-CPR (video)', 'CSL (p2p)', 'BQ', 'mMARCO (reranking)', 'mMARCO (retrieval)', 'AFQMC', 'cCOVID-News', 'Multi-CPR (ecom)', 'DuReader (retrieval)', 'cMedQA (reranking)', 'JDReview', 'ATEC', 'Waimai', 'THUCNews (s2s)', 'OnlineShopping', 'PAWS-X', 'STS-B (zh)', 'QBQTC', 'OCNLI', 'cMedQA (v2) (reranking)', 'T2Ranking (retrieval)', 'THUCNews (p2p)']"
Colossal-AI,"['C-Eval', 'AGIEval', 'GAOKAO-Bench', 'CMMLU', 'MMLU']"
CoQA,"['WritingPrompts', 'Project Gutenberg', 'Wikipedia', 'SciQ', 'CNN DM', 'RACE', 'MCTest']"
EvalPlus,"['HumanEval', 'MBPP']"
FacTool,"['GSM8K', 'HumanEval', 'FactPrompts', 'RoSE', 'Self-Instruct']"
FewCLUE,"['BUSTM', 'CSLDCP', 'TNEWS', 'iFLYTEK', 'ChID', 'OCNLI', 'WSC (CLUE)', 'CSL', 'EPRSTMT']"
FlagEval,"['BUSTM', 'ADE20K', 'FGVC-Aircraft', 'DTD', 'TNEWS', 'RAFT', 'COCO-Stuff', 'C-SEM', 'iNaturalist (2018)', 'KeSpeech', 'Flowers102', 'Food-101', 'ImageNet-1K', 'Cityscapes', 'CMMLU', 'MMLU', 'IEMOCAP', 'NYU-Depth', 'EPRSTMT', 'IMDB', 'TruthfulQA', 'VQA-CP', 'KITTI Eigen split', 'VQA (v2)', 'Flickr30K', 'AISHELL-1', 'CLCC', 'GAOKAO-Bench (2023)', 'Stanford Cars', 'MSCOCO', 'CelebA-HQ', 'BoolQ', 'Places', 'SOP', 'MSRVTT', 'CUB', 'ChID', 'OCNLI', 'TDIUC', 'HumanEval', 'LibriSpeech', 'CSL', 'UCF101', 'WSC (CLUE)']"
GENIE,"['ARC-DA (2018)', 'a-NLG', 'WMT (2019) (de-en)', 'XSUM', 'WMT (2021) (de-en)']"
GPT4All,"['ARC', 'BoolQ', 'WinoGrande', 'HellaSwag', 'PIQA', 'OpenbookQA']"
HEIM,"['DrawBench (image quality categories)', 'Winoground', 'Demographic Stereotypes', 'DrawBench (knowledge categories)', 'MSCOCO (base)', 'Dailydall.e', 'Relational Understanding', 'MSCOCO (art styles)', 'MSCOCO (fairness - gender)', 'MSCOCO (robustness - typos)', 'MSCOCO (fairness - AAVE dialect)', 'Historical Figures', 'CSP', 'Landing Pages', 'PartiPrompts (reasoning categories)', 'PaintSkills', 'Magazine Cover Photos', 'I2P', 'MSCOCO (fidelity)', 'Logos', 'Mental Disorders', 'MSCOCO (zh)', 'PartiPrompts (knowledge categories)', 'MSCOCO (es)', 'MSCOCO (hi)', 'MSCOCO (efficiency)', 'MSCOCO', 'CUB', 'PartiPrompts (image quality categories)', 'DrawBench (reasoning categories)']"
HellaSwag Leaderboard,"['ActivityNet Captions', 'WikiHow', 'HellaSwag']"
HELM Classic,"['BLiMP', 'Synthetic Reasoning (symbolic)', 'Synthetic efficiency', 'MSMARCO', 'bAbI', 'LegalSupport', 'HellaSwag', 'WMT (2014)', 'NQ (open-book)', 'RAFT', 'Synthetic Reasoning (natural)', 'MSMARCO (v2)', 'BBQ', 'MATH', 'Disinformation (wedging)', 'Dyck Languages', 'MedQA', 'Billsum', 'RealToxicityPrompts', 'OpenbookQA', 'LegalBench', 'NQ (closed-book)', 'CNN DM', 'QuAC', 'The Pile', 'MMLU', 'WikiFact', 'DataImputation', 'MATH (chain-of-thought)', 'IMDB', 'GSM8K', 'LSAT', 'APPS', 'Disinformation (reiteration)', 'ICE', 'TwitterAAE (white)', 'BOLD', 'MultiLexSum', 'BoolQ', 'Numerical reasoning', 'TwitterAAE (aa)', 'EurLexSum', 'Copyright (text)', 'HumanEval', 'Civil Comments', 'TruthfulQA', 'NarrativeQA', 'Copyright (code)', 'XSUM', 'TwitterAAE', 'EntityMatching']"
HELM Lite,"['MATH', 'GSM8K', 'MedQA', 'WMT (2014)', 'OpenbookQA', 'LegalBench', 'NQ (closed-book)', 'NQ (open-book)', 'NarrativeQA', 'MMLU']"
HHEM Leaderboard,['CNN DM']
InstructEval,"['HHH', 'DROP', 'CRASS', 'IMPACT', 'HumanEval', 'MMLU', 'BBH']"
InterCode,"['NL2Bash', 'InterCode-CTF', 'SWE-bench', 'Spider', 'MBPP']"
KoLA,"['2WikiMQA', 'MuSiQue', 'COPEN (CiC)', 'ETC', 'COPEN (CSJ)', 'MAVEN-ERE', 'KQA Pro', 'Encyclopedic', 'High-Frequency Knowledge', 'FewNERD', 'KoRC', 'MAVEN', 'ETM', 'DocRED', 'ETA', 'COPEN (CPJ)', 'Low-Frequency Knowledge', 'ETU', 'HotpotQA']"
L-Eval,"['BigPatent', 'GovReport', 'OPENREVIEW', 'SFiction', 'LONGFQA', 'SPACE', 'CodeU', 'TopicRet', 'QMSum', 'TOEFL-QA', 'GSM8K', 'NQ', 'Multi-News', 'SummScreen', 'Coursera', 'CUAD', 'Qasper', 'MultiDoc2Dial', 'NarrativeQA', 'QuALITY']"
LAiW Leaderboard,"['CFM', 'CrimeKgAssitant', 'CAIL (2021)', 'MLMN', 'CJRC', 'CAIL (2019)', 'CAIL (2020)', 'AC-NLG', 'CAIL (2018)', 'Criminal-S', 'MSJudge', 'JEC-QA']"
LLM Benchmarker Suite,"['TriviaQA', 'GSM8K', 'BoolQ', 'WinoGrande', 'NQ', 'HellaSwag', 'AGIEval', 'OpenbookQA', 'HumanEval', 'QuAC', 'MMLU']"
LLM-Leaderboard,"['TriviaQA', 'Chatbot Arena Conversations', 'LAMBADA', 'WinoGrande', 'HellaSwag', 'HumanEval', 'MMLU']"
LongBench,"['2WikiMQA', 'TriviaQA', 'GovReport', 'RepoBench-P', 'MuSiQue', 'PassageRetrieval (zh)', 'PassageCount', 'LSHTC', 'TREC', 'DuReader', 'QMSum', 'LCC', 'PassageRetrieval', 'Multi-News', 'VCSUM', 'Qasper', 'MultiFieldQA (zh)', 'SAMSum', 'MultiFieldQA', 'NarrativeQA', 'HotpotQA']"
LVLM-eHub,"['IconQA', 'VCR-OC', 'Minecraft', 'VSR', 'SROIE', 'CIFAR-10', 'CUTE80', 'VizWiz', 'SVTP', 'MSCOCO (OC)', 'CTW', 'Franka Kitchen', 'TextVQA', 'ScienceQA IMG', 'MSCOCO (random)', 'IC (2013)', 'Flowers102', 'Pets37', 'OK-VQA', 'SVT', 'WOST', 'VCR-MCI', 'FUNSD', 'ImageNet-1K', 'STVQA', 'SNLI-VE', 'Whoops', 'Virtual Home', 'Flickr30K', 'ImageNetVC', 'VisDial', 'Meta-World', 'MSCOCO (popular)', 'MSCOCO (MCI)', 'HOST', 'NoCaps', 'VCR', 'IIIT5K', 'GQA', 'OCR-VQA', 'COCO-Text', 'MSCOCO (adversarial)', 'WordArt', 'Total-Text', 'DocVQA', 'IC (2015)']"
MedBench,"['DBMHG', 'DDx-basic', 'MedSpeQA', 'DrugCA', 'MedDG', 'CMB-Clin', 'CHIP-CDN', 'CHIP-CDEE', 'MedHG', 'MedTreat', 'DDx-advanced', 'MedHC', 'SMDoc', 'MedMC', 'SafetyBench', 'IMCS-MRG (v2)', 'CMeEE', 'Med-Exam', 'CMeIE', 'CHIP-CTC']"
MMBench,"['VSR', 'CLEVR', 'Places', 'ARAS', 'COCO Captions', 'ScienceQA', 'PISC', 'Internet', 'TextVQA', 'LLaVA-Bench', 'KonIQ-10k', 'W3C School']"
MMLU-by-task Leaderboard,"['MMLU', 'HellaSwag', 'ARC']"
MOCHA,"['MCScript', 'DROP', 'SIQA', 'CosmosQA', 'NarrativeQA', 'Quoref']"
MTEB,"['SciFact', 'NoReC', 'Bornholmsk', 'Blurbs (p2p)', 'HotpotQA (pl)', 'NFCorpus', 'PAC', 'arXiv (s2s)', 'bioRxiv (p2p)', 'STS (2013)', 'Polish CDSCorpus (relatedness)', 'SciDocs (pl)', 'DKhate', 'SICK (relatedness) (pl)', 'SICK (relatedness)', 'NPSC', '8TAGS', 'FiQA (2018) (pl)', 'ScaLA (sv)', 'ScaLA (da)', 'FEVER', 'QQP', '10kGNAD (p2p)', 'SciDocs', 'MTOP (intent)', 'MASSIVE (scenario)', 'NFCorpus (pl)', 'STS (2017)', 'STS (2016)', 'Nordic Language Identification', 'PPC', 'HotpotQA', 'Tweet Sentiment Extraction', 'Reddit', 'Stack Exchange (p2p)', 'MARC', 'STS (2014)', 'STS (2015)', 'Civil Comments (tc)', 'Blurbs (s2s)', 'STS (2012)', 'CQADupStack (wordpress)', 'CQADupStack (unix)', 'FiQA (2018)', 'CQADupStack (programmers)', 'SummEval', 'CQADupStack (english)', 'ScaLA (nn)', 'Tatoeba', 'LCC', 'ArguAna', 'STS-B', 'DanishPoliticalComments', 'Polish CDSCorpus', '20 Newsgroups', 'SICK (pl)', 'ArguAna (pl)', 'CARER', 'B77', 'CQADupStack (webmasters)', '10kGNAD (s2s)', 'MSMARCO', 'Stack Exchange', 'CQADupStack (gis)', 'DBpedia (pl)', 'LanguageNet', 'BIOSSES', 'MSMARCO (v2)', 'SciFact (pl)', 'TwitterSemEval (2015)', 'MIND', 'BUCC', 'CBD', 'Amazon Polarity', 'IMDB', 'SprintDuplicateQuestions', 'AskUbuntu', 'STS (2022)', 'CQADupStack (physics)', 'DBpedia', 'PolEmo (v2) (out)', 'AMCD', 'arXiv (p2p)', 'Angry Tweets', 'TREC-COVID', 'DaLAJ', 'Reddit (p2p)', 'CQADupStack (gaming)', 'SweFAQ (v2)', 'Climate-FEVER', 'bioRxiv (s2s)', 'PolEmo (v2) (in)', 'Allegro Reviews', 'CQADupStack (mathematica)', 'NQ (pl)', 'QQP (pl)', 'MASSIVE (intent)', 'SweRec', 'NQ', 'CQADupStack (android)', 'LinkSO', 'CQADupStack (tex)', 'PSC', 'Touche (2020)', 'MTOP (domain)', 'medRxiv (p2p)', 'medRxiv (s2s)', 'CQADupStack (stats)', 'MSMARCO (pl)', 'SciDocs (rr)', 'ScaLA (nb)']"
Multi-modal Modal Leaderboard,"['CCBench', 'MMBench (test) (zh)', 'MMBench (test)', 'MM-Vet', 'MME (normalized)', 'MathVista (minitest)', 'MMMU (val)', 'HallusionBench', 'SEED-Bench (img)']"
MVBench,"['MovieNet', 'CLEVRER', 'Charades-STA', 'VLN-CE', 'FunQA', 'STAR', 'NTU RGB+D', 'TVQA', 'MiT', 'Perception Test', 'PAXION']"
OCRBench,"['IAM', 'SROIE', 'SVTP', 'InfographicVQA', 'TextVQA', 'Non-Semantic Text', 'SCUT-CTW1500', 'ESTVQA (en)', 'ChartQA (Hum.)', 'Semantic Text', 'SVT', 'WOST', 'IC15', 'ORAND-CAR-2014', 'FUNSD', 'STVQA', 'ReCTS', 'ChartQA (Aug.)', 'HOST', 'IIIT5K', 'OCR-VQA', 'COCO-Text', 'ESTVQA (zh)', 'WordArt', 'IC13', 'Total-Text', 'DocVQA', 'CT80', 'POIE', 'HME100K']"
Open Ko-LLM Leaderboard,"['ARC (Korean)', 'TruthfulQA (Korean)', 'MMLU (Korean)', 'CommonGen', 'HellaSwag (Korean)']"
Open LLM Leaderboard,"['GSM8K', 'ARC', 'WinoGrande', 'HellaSwag', 'TruthfulQA', 'MMLU']"
Open Multilingual LLM Evaluation Leaderboard,"['TruthfulQA', 'MMLU', 'HellaSwag', 'ARC']"
OpenCompass LLM Leaderboard (v2),"['BUSTM', 'CMNLI', 'CSQA', 'TriviaQA', 'ARC', 'C-Eval', 'LAMBADA', 'TNEWS', 'HellaSwag', 'GAOKAO-Bench', 'RACE', 'ReCoRD', 'TheoremQA', 'SQuAD (v2)', 'Broad-Coverage Diagnostic', 'CMRC', 'SummEdits', 'MATH', 'AFQMC', 'OpenbookQA', 'CMMLU', 'SIQA', 'MMLU', 'EPRSTMT', 'BBH', 'GSM8K', 'WinoGrande', 'NQ', 'TyDiQA', 'AGIEval', 'C3', 'Flores-101', 'Winogender Diagnostic', 'RTE', 'GAOKAO-Bench (2023)', 'StoryCloze', 'BoolQ', 'DRCD', 'LCSTS', 'WSC', 'MultiRC', 'PIQA', 'WiC', 'ChID', 'OCNLI', 'DROP', 'HumanEval', 'Xiezhi', 'CSL', 'COPA', 'CB', 'XSUM', 'MBPP']"
OpenEval (text),"['CDIAL-BIAS', 'CMNLI', 'SWSR', 'WGlaw', 'Myopia Reward', 'COLD', 'GAOKAO-Bench', 'CooridinateAI', 'Self-awareness', 'Power-seeking', 'CMMLU', 'CORGI-PM', 'Corrigible', 'CommonMT', 'C3', 'M3KE', 'TUMCC', 'WPLC', 'SQuAD (zh)', 'TOCP', 'SNLI (zh)', 'CBBQ', 'TGEA', 'OL-CC', 'ChID', 'One-box Tendency', 'CAIL (2018)', 'BiPaR', 'Guilt Law', 'WSC (CLUE)']"
PromptBench,"['NumerSense', 'CSQA', 'MATH', 'GSM8K', 'GLUE', 'MultiUN', 'BIG-Bench', 'QASC', 'MMLU', 'IWSLT (2017)', 'SQuAD (v2)']"
Q-Bench,"['SPAQ', 'AGIQA-3K', 'LLDescribe', 'LIVE-itw', 'CGIQA-6K', 'LIVE-FB LSVQ', 'KADID-10K', 'LLVisionQA', 'KonIQ-10k']"
RAFT,"['TAI', 'ADE Corpus (v2)', 'B77', 'TweetEval (hate)', 'Over', 'SOT', 'TC', 'NIS', 'SRI', 'OSE', 'ToS']"
ReCoRD,"['CNN DM', 'Internet Archive']"
ReForm-Eval,"['Winoground', 'VSR', 'CLEVR', 'TDIUC (utility)', 'ViQuAE', 'SROIE', 'CIFAR-10', 'CUTE80', 'TDIUC (scene)', 'IC (2015) (Grounded)', 'MSCOCO (OC)', 'TextVQA', 'MSCOCO (GOI)', 'VizWiz (yesno)', 'TextOCR', 'Flowers102', 'Pets37', 'RefCOCO (res)', 'OK-VQA', 'ScienceQA', 'FUNSD', 'ImageNet-1K', 'SNLI-VE', 'TDIUC (color)', 'Whoops', 'MOCHEG', 'MEDIC (dts)', 'WikiHow', 'VQA (v2)', 'MSCOCO (MOS)', 'Flickr30K', 'ImageNetVC', 'VisDial', 'A-OKVQA', 'TDIUC (detection)', 'COCO-Text (Grounded)', 'MSCOCO (MCI)', 'TDIUC (counting)', 'NoCaps', 'GQA', 'IIIT5K', 'VizWiz (singleChoice)', 'OCR-VQA', 'MP3D', 'COCO-Text', 'TextOCR (Grounded)', 'TDIUC (sport)', 'TextCaps', 'TDIUC (position)', 'WordArt', 'DocVQA', 'POIE', 'IC (2015)']"
RoleEval,"['RoleEval-Chinese', 'RoleEval-Global']"
ScandEval,"['ScandiQA (da)', 'NoReC', 'ScaLA (nn)', 'ScaLA (sv)', 'ScandiQA (sv)', 'NorNE (nb)', 'DaNE', 'SweRec', 'ScaLA (da)', 'SUC (v3)', 'ScandiQA (no)', 'ScaLA (nb)', 'NorNE (nn)', 'Angry Tweets']"
SCROLLS,"['GovReport', 'Qasper', 'ContractNLI', 'NarrativeQA', 'QMSum', 'SummScreen', 'QuALITY']"
SummEdits,"['SciTLDR', 'Sales Call', 'Billsum', 'Google News', 'Sales Email', 'ECTSum', 'SAMSum', 'TinyShakespeare', 'Spotify Podcast', 'QMSum']"
SuperCLUE,"['OPEN Set', 'CLOSE Set', 'CArena']"
SuperGLUE,"['BoolQ', 'WSC', 'RTE', 'WiC', 'Winogender Diagnostic', 'ReCoRD', 'COPA', 'CB', 'MultiRC', 'Broad-Coverage Diagnostic']"
SuperLim (v2),"['SweDN', 'Argumentation sentences', 'Winogender Diagnostic (sv) (v2)', 'STS-B (sv) (v2)', 'Winograd (sv) (v2)', 'SweSAT Synonyms (v1.1)', 'Swedish ABSAbank-Imm (v1.1)', 'Swedish analogy (v2)', 'MNLI (sv)', 'SweWiC (v2)', 'DaLAJ-GED-SuperLim (v2)', 'SuperSim (v2)', 'GLUE Diagnostic (sv)', 'SweFAQ (v2)']"
UHGEval,['XinhuaHallucinations']
Video-Bench Leaderboard,"['ActivityNet-QA', 'NBAQA', 'UCF-Crime', 'DLE', 'DDM', 'TVQA', 'MVQA', 'TGIF-QA', 'YouCook2', 'SQA3D', 'MOT', 'MSRVTT-QA', 'MSVD-QA']"
VLM-Eval,"['MSVD', 'HMDB51', 'MSRVTT', 'Kinetics-400', 'TGIF', 'UCF101', 'ActivityNet']"
YALL,"['AGIEval', 'TruthfulQA', 'BIG-Bench', 'GPT4All']"
