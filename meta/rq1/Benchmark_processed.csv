Leaderboard name,Benchmarks
AgentBench,"['Mind2Web', 'ALFWorld', 'WebShop']"
AlpacaEval (v2),"['Koala', 'HH-RLHF', 'OASST1', 'Vicuna', 'Self-Instruct']"
BBH,"['MovieLens', 'SNARKS', 'Sorting Words', 'Tracking Shuffled Objects', 'Salient Translation Error Detection', 'Dyck Languages', 'Tables of Penguins', 'Causal Judgment', 'Disambiguation QA', 'Logical Deduction', 'Object Counting', 'Sports Understanding', 'Ruin a Name with One Edit', 'Geometric Shapes', 'Reasoning about Colored Objects', 'Web of Lies', 'Boolean Expressions', 'Formal Fallacies and Syllogisms with Negation', 'Navigation', 'Multistep Arithmetic', 'Sequences', 'Adjective Order', 'Date Understanding']"
BEIR,"['QQP', 'Climate-FEVER', 'NFCorpus', 'SciDocs', 'Touche (2020)', 'MSMARCO', 'Signal-1M', 'ArguAna', 'NQ', 'FEVER', 'FiQA (2018)', 'SciFact', 'TREC-News', 'DBpedia', 'BioASQ', 'HotpotQA', 'TREC-COVID', 'CQADupStack (gis)', 'Robust04']"
Big Code Models Leaderboard,"['MultiPL-E', 'HumanEval']"
BIG-Bench,"['Emoji Movie', 'Metaphor Boolean', 'Data Wrangling', 'Identify Odd Metaphor', 'Hindu Mythology Trivia', 'Transforming German Sentences to Gender (Inclusive Forms)', 'SNARKS', 'SIT', 'Spelling Bee', 'COM2SENSE', 'Figure of Speech Detection', 'Forecasting Subquestions', 'Self Evaluation Courtroom', 'Self Evaluation of Tutoring', 'Mathematical Induction', 'Cryptonite', 'Dyck Languages', 'Understanding Fables', 'CoDA', 'CS Algorithms', 'Simple arithmetic with subtasks (JSON)', 'Which Wiki Edit', 'Goal-Step Inference', 'Truthful QA', 'Simple arithmetic (JSON)', 'Matrix Shapes', 'Identifying Anachronisms', 'Medical Questions in Russian', 'Python Programming', 'Swahili-English Paremiologic Competence', 'StrategyQA', 'Twenty Questions', 'Presuppositions as NLI', 'Irony Identification', 'Automatic Debugging', 'Discovery', 'SParC', 'Misconceptions', 'English to Russian Proverbs', 'LTI LangID Corpus', 'Checkmate In One Move', 'Entailed Polarity', 'Wikidata', 'ARC (The Abstraction and Reasoning Corpus)', 'Phrase Relatedness', 'Cycled Letters', 'The Essential, the Excessive, and the Extraneous', 'Boolean Expressions', 'Similarities Test for Abstraction', 'Autoclassification', 'RoFT', 'Cornell Movie-Dialogs Corpus', 'Linguistic Puzzles', 'General Knowledge', 'Subject-Verb Agreement', 'Common Morpheme', 'Sequences', 'Misconceptions (ru)', 'SQuAD (pp)', 'Code Description', 'Text Navigation Game', 'MovieLens', 'GRE Reading Comprehension', 'SGD', 'ASCII MNIST', 'Verb Tense', 'Gender Sensitivity Test (English)', 'Dark Humor Detection', 'UnQover', 'Physics Multiple Choice', 'Logic Grid Puzzles', 'HHH', 'Sufficient Information', 'Entailed Polarity in Hindi', 'EmoTag1200', 'Social Bias from Sentence Probability', 'Natural Instructions', 'Understanding Grammar of Unseen Words', 'ParsiNLU (rc)', 'Disfl-QA', 'BBQ-Lite', 'Conceptual Combinations', 'Tables of Penguins', 'Physical Intuition', 'Causal Judgment', 'Cause and Effect', 'Authorship Verification', 'MNLI (transliteration)', 'English Proverbs', 'Persian Idioms', 'Intersection Points', 'Operators', 'Indic Cause and Effect', 'Swedish to German proverbs', 'Dynamic Counting', 'Intent Recognition', 'Factuality', 'Alignment of Simplicity Priors for Turing (Complete Concept Learning)', 'Ruin a Name with One Edit', 'Gender Sensitivity Test (zh)', 'Muslim-Violence Bias', 'Novel Concepts', 'Protein Interaction Sites', 'ASCII Word Recognition', 'Minute Mysteries QA', 'Web of Lies', 'CRT', 'Wino-X (German)', 'Navigation', 'Multistep Arithmetic', 'Linguistic Mappings', 'Language Games', 'Repeat Copy Logic', 'Date Understanding', 'Analogical Similarity', 'Scientific Press Release', 'Python Program Synthesis', 'CIFAR-10', 'Informal and Formal Fallacies', 'MultiEmo', 'Kannada Riddles', 'Word Unscrambling', 'TimeDial', 'Salient Translation Error Detection', 'SIQA', 'Word Problems on Sets and Graphs', 'Conlang Translation Problems', 'Keyword Sentence Transformation', 'Unnatural In-Context Learning', 'KPWr', 'Known Unknowns', 'TruthfulQA', 'VitaminC', 'Social Support', 'TellMeWhy', 'RiddleSense', 'Empirical Judgments', 'Disambiguation QA', 'Judging Moral Permissibility', 'Logical Deduction', 'Physics Questions', 'Object Counting', 'Cryobiology Spanish', 'BBQ-Lite (Json)', 'Simple arithmetic', 'Sports Understanding', 'Unit Interpretation', 'Geometric Shapes', 'Reasoning about Colored Objects', 'Hindi Question Answering', 'ISNotes', 'Implicatures', 'GEM', 'ePiC', 'What is the Tao', 'Human Organs and Senses', 'Arithmetic', 'High Low Guessing Game', 'Hinglish Toxicity Prediction', 'Root Finding, Optimization and Games', 'Simple Ethical Questions', 'Spider', 'Codenames', 'Odd One Out', 'Key Value Maps', 'Fantasy Reasoning', 'MathQA', 'CoQA', 'Shakespeare Dialogue', 'Topical-Chat', 'Sorting Words', 'Tracking Shuffled Objects', 'Metaphor Understanding', 'TalkDown', 'State Tracking in Chess', 'Diverse Metrics for Social Biases in Language Models', 'Fact-Checking', 'List Functions', 'Periodic Elements', 'PARSINLU (qa)', 'Sudoku', 'Unit Conversion', 'NQ', 'COPA (qa)', 'Sentence Ambiguity', 'characterRelations', 'Strange Stories', 'Taboo', 'Color', 'Long Input Contexts', 'SNLI', 'Sequential Order', 'Identify Math Theorems', 'YesNoBlackWhite Game', 'Modified Arithmetic', 'Rhyming', 'SQuADShifts', 'Estimating Risk of Suicide', 'Simple arithmetic with multiple targets (JSON)', 'Crash Blossoms', 'Simple multiple choice arithmetic (JSON)', 'Simple Text Editing', 'Logical Arguments', 'CRASS', 'WinoWhy', 'Self-awareness', 'Training on the test set', 'Analytic Entailment', 'Formal Fallacies and Syllogisms with Negation', 'Reordering', 'Kanji ASCII Art', 'Wikimedia', 'MNLI (ipa)', 'Adjective Order']"
Chatbot Arena Leaderboard,"['MMLU', 'MT-Bench', 'Chatbot Arena Conversations']"
ChEF,"['Omnibenchmark', 'VOC (2012)', 'Flickr30K', 'MSCOCO', 'MME', 'CIFAR-10', 'FSC147', 'SEED-Bench', 'ScienceQA', 'MMBench']"
CLUE,"['TNEWS', 'CMNLI', 'CSL', 'iFLYTEK', 'C3', 'CMRC (2018)', 'CLUE Diagnostics', 'AFQMC', 'DRCD', 'WSC (CLUE)', 'OCNLI', 'ChID']"
CMB,"['CMB-Clin', 'CMB-Exam']"
CMTEB,"['cMedQA (v2) (reranking)', 'cMedQA (v2) (retrieval)', 'Multi-CPR (ecom)', 'STS-B (zh)', 'iFLYTEK', 'Multi-CPR (video)', 'AFQMC', 'CSL (p2p)', 'PAWS-X', 'QBQTC', 'DuReader (retrieval)', 'mMARCO (reranking)', 'mMARCO (retrieval)', 'CSL (s2s)', 'Multi-CPR (medical)', 'BQ', 'cMedQA (reranking)', 'CMNLI', 'OCNLI', 'ATEC', 'THUCNews (s2s)', 'JDReview', 'OnlineShopping', 'TNEWS', 'THUCNews (p2p)', 'LCQMC', 'T2Ranking (reranking)', 'cCOVID-News', 'Waimai', 'T2Ranking (retrieval)']"
Colossal-AI,"['AGIEval', 'MMLU', 'CMMLU', 'C-Eval', 'GAOKAO-Bench']"
CoQA,"['Project Gutenberg', 'WritingPrompts', 'MCTest', 'RACE', 'Wikipedia', 'SciQ', 'CNN DM']"
EvalPlus,"['HumanEval', 'MBPP']"
FacTool,"['RoSE', 'HumanEval', 'FactPrompts', 'GSM8K', 'Self-Instruct']"
FewCLUE,"['TNEWS', 'CSLDCP', 'CSL', 'iFLYTEK', 'BUSTM', 'WSC (CLUE)', 'OCNLI', 'EPRSTMT', 'ChID']"
FlagEval,"['IEMOCAP', 'CSL', 'BUSTM', 'Places', 'Cityscapes', 'BoolQ', 'MMLU', 'KeSpeech', 'WSC (CLUE)', 'CelebA-HQ', 'AISHELL-1', 'Food-101', 'Stanford Cars', 'UCF101', 'TruthfulQA', 'VQA-CP', 'RAFT', 'CMMLU', 'VQA (v2)', 'NYU-Depth', 'EPRSTMT', 'iNaturalist (2018)', 'COCO-Stuff', 'HumanEval', 'ImageNet-1K', 'MSRVTT', 'OCNLI', 'IMDB', 'FGVC-Aircraft', 'DTD', 'SOP', 'GAOKAO-Bench (2023)', 'TNEWS', 'KITTI Eigen split', 'Flickr30K', 'MSCOCO', 'LibriSpeech', 'C-SEM', 'CLCC', 'Flowers102', 'TDIUC', 'CUB', 'ADE20K', 'ChID']"
GENIE,"['a-NLG', 'XSUM', 'ARC-DA (2018)', 'WMT (2021) (de-en)', 'WMT (2019) (de-en)']"
GPT4All,"['OpenbookQA', 'WinoGrande', 'BoolQ', 'HellaSwag', 'PIQA', 'ARC']"
HEIM,"['Winoground', 'DrawBench (reasoning categories)', 'MSCOCO (hi)', 'Magazine Cover Photos', 'Relational Understanding', 'MSCOCO (efficiency)', 'MSCOCO (fairness - AAVE dialect)', 'MSCOCO (base)', 'Mental Disorders', 'Landing Pages', 'MSCOCO (robustness - typos)', 'PartiPrompts (knowledge categories)', 'I2P', 'DrawBench (image quality categories)', 'Demographic Stereotypes', 'MSCOCO (es)', 'MSCOCO (art styles)', 'MSCOCO (zh)', 'PartiPrompts (image quality categories)', 'PaintSkills', 'Historical Figures', 'MSCOCO (fidelity)', 'MSCOCO', 'CSP', 'MSCOCO (fairness - gender)', 'Logos', 'PartiPrompts (reasoning categories)', 'Dailydall.e', 'CUB', 'DrawBench (knowledge categories)']"
HellaSwag Leaderboard,"['WikiHow', 'HellaSwag', 'ActivityNet Captions']"
HELM Classic,"['TwitterAAE', 'NQ (closed-book)', 'LSAT', 'OpenbookQA', 'WikiFact', 'Billsum', 'BoolQ', 'HellaSwag', 'MMLU', 'Disinformation (reiteration)', 'MSMARCO (v2)', 'MATH (chain-of-thought)', 'BLiMP', 'LegalSupport', 'EntityMatching', 'TwitterAAE (white)', 'CNN DM', 'Copyright (text)', 'Dyck Languages', 'LegalBench', 'MSMARCO', 'TwitterAAE (aa)', 'TruthfulQA', 'MultiLexSum', 'Synthetic efficiency', 'RAFT', 'ICE', 'WMT (2014)', 'BBQ', 'Synthetic Reasoning (natural)', 'BOLD', 'HumanEval', 'NQ (open-book)', 'Numerical reasoning', 'NarrativeQA', 'Disinformation (wedging)', 'EurLexSum', 'IMDB', 'DataImputation', 'Synthetic Reasoning (symbolic)', 'XSUM', 'MATH', 'Civil Comments', 'Copyright (code)', 'bAbI', 'The Pile', 'QuAC', 'GSM8K', 'RealToxicityPrompts', 'APPS', 'MedQA']"
HELM Lite,"['LegalBench', 'MATH', 'NQ (closed-book)', 'OpenbookQA', 'NQ (open-book)', 'MMLU', 'NarrativeQA', 'GSM8K', 'WMT (2014)', 'MedQA']"
HHEM Leaderboard,['CNN DM']
InstructEval,"['CRASS', 'HumanEval', 'MMLU', 'BBH', 'HHH', 'IMPACT', 'DROP']"
InterCode,"['Spider', 'SWE-bench', 'MBPP', 'InterCode-CTF', 'NL2Bash']"
KoLA,"['KQA Pro', 'FewNERD', 'ETU', 'MuSiQue', 'ETM', '2WikiMQA', 'Low-Frequency Knowledge', 'Encyclopedic', 'ETC', 'COPEN (CiC)', 'COPEN (CSJ)', 'MAVEN-ERE', 'DocRED', 'COPEN (CPJ)', 'High-Frequency Knowledge', 'HotpotQA', 'MAVEN', 'KoRC', 'ETA']"
L-Eval,"['QuALITY', 'CUAD', 'LONGFQA', 'TOEFL-QA', 'CodeU', 'TopicRet', 'Coursera', 'QMSum', 'NQ', 'Multi-News', 'SPACE', 'SFiction', 'Qasper', 'BigPatent', 'OPENREVIEW', 'NarrativeQA', 'SummScreen', 'MultiDoc2Dial', 'GSM8K', 'GovReport']"
LAiW Leaderboard,"['CAIL (2021)', 'CAIL (2019)', 'MSJudge', 'MLMN', 'CFM', 'CJRC', 'Criminal-S', 'CrimeKgAssitant', 'AC-NLG', 'CAIL (2020)', 'CAIL (2018)', 'JEC-QA']"
LLM Benchmarker Suite,"['HumanEval', 'AGIEval', 'OpenbookQA', 'WinoGrande', 'BoolQ', 'HellaSwag', 'MMLU', 'TriviaQA', 'NQ', 'QuAC', 'GSM8K']"
LLM-Leaderboard,"['HumanEval', 'WinoGrande', 'HellaSwag', 'MMLU', 'TriviaQA', 'LAMBADA', 'Chatbot Arena Conversations']"
LongBench,"['PassageCount', 'MuSiQue', '2WikiMQA', 'RepoBench-P', 'LSHTC', 'QMSum', 'Multi-News', 'VCSUM', 'DuReader', 'Qasper', 'MultiFieldQA (zh)', 'NarrativeQA', 'TriviaQA', 'PassageRetrieval', 'HotpotQA', 'SAMSum', 'MultiFieldQA', 'LCC', 'PassageRetrieval (zh)', 'TREC', 'GovReport']"
LVLM-eHub,"['Pets37', 'Virtual Home', 'VSR', 'TextVQA', 'CIFAR-10', 'OK-VQA', 'VCR', 'CTW', 'DocVQA', 'MSCOCO (popular)', 'MSCOCO (adversarial)', 'HOST', 'Meta-World', 'Franka Kitchen', 'SROIE', 'Whoops', 'GQA', 'ScienceQA IMG', 'VisDial', 'ImageNetVC', 'Total-Text', 'MSCOCO (random)', 'MSCOCO (OC)', 'VCR-MCI', 'VCR-OC', 'ImageNet-1K', 'FUNSD', 'IIIT5K', 'STVQA', 'WOST', 'NoCaps', 'Minecraft', 'SVT', 'MSCOCO (MCI)', 'IC (2013)', 'COCO-Text', 'OCR-VQA', 'SNLI-VE', 'Flickr30K', 'IconQA', 'IC (2015)', 'SVTP', 'Flowers102', 'WordArt', 'CUTE80', 'VizWiz']"
MedBench,"['MedMC', 'CHIP-CTC', 'DDx-advanced', 'MedSpeQA', 'SafetyBench', 'SMDoc', 'CHIP-CDN', 'CHIP-CDEE', 'DrugCA', 'MedHG', 'CMeIE', 'DDx-basic', 'CMB-Clin', 'MedTreat', 'DBMHG', 'CMeEE', 'MedDG', 'Med-Exam', 'MedHC', 'IMCS-MRG (v2)']"
MMBench,"['ScienceQA', 'VSR', 'TextVQA', 'Places', 'CLEVR', 'Internet', 'ARAS', 'PISC', 'KonIQ-10k', 'W3C School', 'COCO Captions', 'LLaVA-Bench']"
MMLU-by-task Leaderboard,"['MMLU', 'ARC', 'HellaSwag']"
MOCHA,"['CosmosQA', 'NarrativeQA', 'MCScript', 'Quoref', 'SIQA', 'DROP']"
MTEB,"['STS (2013)', 'Blurbs (p2p)', 'QQP', 'Climate-FEVER', 'SciDocs', 'B77', 'STS (2015)', 'BIOSSES', 'ScaLA (da)', 'MSMARCO', 'SciFact (pl)', 'ArguAna', 'Blurbs (s2s)', 'FEVER', 'PAC', 'AMCD', 'SICK (relatedness)', 'bioRxiv (p2p)', 'CBD', 'STS (2022)', 'DBpedia', 'QQP (pl)', 'NFCorpus (pl)', 'Reddit (p2p)', 'LCC', 'Angry Tweets', '10kGNAD (s2s)', 'CQADupStack (gaming)', 'CQADupStack (stats)', '20 Newsgroups', 'ScaLA (nn)', 'MIND', 'PolEmo (v2) (in)', 'arXiv (s2s)', 'MSMARCO (v2)', 'STS-B', 'TwitterSemEval (2015)', 'Touche (2020)', 'SciDocs (rr)', '10kGNAD (p2p)', 'Allegro Reviews', 'STS (2017)', 'STS (2016)', 'Tweet Sentiment Extraction', 'Stack Exchange (p2p)', 'HotpotQA (pl)', 'bioRxiv (s2s)', 'CQADupStack (android)', 'STS (2012)', 'CARER', 'HotpotQA', 'FiQA (2018) (pl)', 'SICK (pl)', 'NPSC', 'ScaLA (nb)', 'CQADupStack (wordpress)', 'SummEval', 'NFCorpus', 'MASSIVE (scenario)', 'DanishPoliticalComments', 'PPC', 'Tatoeba', 'MSMARCO (pl)', 'AskUbuntu', '8TAGS', 'arXiv (p2p)', 'PSC', 'CQADupStack (mathematica)', 'FiQA (2018)', 'Reddit', 'DBpedia (pl)', 'CQADupStack (programmers)', 'SciFact', 'IMDB', 'MARC', 'PolEmo (v2) (out)', 'Polish CDSCorpus', 'SweRec', 'MTOP (domain)', 'MASSIVE (intent)', 'NQ (pl)', 'CQADupStack (gis)', 'Nordic Language Identification', 'medRxiv (p2p)', 'STS (2014)', 'CQADupStack (webmasters)', 'CQADupStack (tex)', 'LanguageNet', 'CQADupStack (unix)', 'LinkSO', 'medRxiv (s2s)', 'DaLAJ', 'ScaLA (sv)', 'SICK (relatedness) (pl)', 'ArguAna (pl)', 'SciDocs (pl)', 'Bornholmsk', 'SweFAQ (v2)', 'NQ', 'Stack Exchange', 'Amazon Polarity', 'BUCC', 'Polish CDSCorpus (relatedness)', 'CQADupStack (physics)', 'CQADupStack (english)', 'NoReC', 'MTOP (intent)', 'DKhate', 'TREC-COVID', 'SprintDuplicateQuestions', 'Civil Comments (tc)']"
Multi-modal Modal Leaderboard,"['CCBench', 'MM-Vet', 'SEED-Bench (img)', 'HallusionBench', 'MMBench (test)', 'MathVista (minitest)', 'MME (normalized)', 'MMMU (val)', 'MMBench (test) (zh)']"
MVBench,"['Charades-STA', 'PAXION', 'VLN-CE', 'FunQA', 'NTU RGB+D', 'MiT', 'STAR', 'Perception Test', 'TVQA', 'CLEVRER', 'MovieNet']"
OCRBench,"['ReCTS', 'TextVQA', 'ESTVQA (en)', 'SCUT-CTW1500', 'DocVQA', 'HOST', 'ORAND-CAR-2014', 'SROIE', 'IC15', 'ChartQA (Aug.)', 'ESTVQA (zh)', 'Total-Text', 'Non-Semantic Text', 'IC13', 'InfographicVQA', 'FUNSD', 'IIIT5K', 'ChartQA (Hum.)', 'STVQA', 'WOST', 'SVT', 'IAM', 'Semantic Text', 'COCO-Text', 'OCR-VQA', 'POIE', 'SVTP', 'CT80', 'HME100K', 'WordArt']"
Open Ko-LLM Leaderboard,"['MMLU (ko)', 'ARC (ko)', 'CommonGen', 'TruthfulQA (ko)', 'HellaSwag (ko)']"
Open LLM Leaderboard,"['TruthfulQA', 'WinoGrande', 'HellaSwag', 'MMLU', 'GSM8K', 'ARC']"
Open Multilingual LLM Evaluation Leaderboard,"['MMLU', 'TruthfulQA', 'ARC', 'HellaSwag']"
OpenCompass LLM Leaderboard (v2),"['LCSTS', 'CSL', 'MBPP', 'BUSTM', 'OpenbookQA', 'RTE', 'BoolQ', 'HellaSwag', 'MMLU', 'AFQMC', 'WiC', 'GAOKAO-Bench', 'CSQA', 'LAMBADA', 'SIQA', 'Broad-Coverage Diagnostic', 'AGIEval', 'StoryCloze', 'PIQA', 'BBH', 'CMMLU', 'NQ', 'Xiezhi', 'EPRSTMT', 'MultiRC', 'COPA', 'CMNLI', 'HumanEval', 'RACE', 'WSC', 'Winogender Diagnostic', 'ReCoRD', 'TriviaQA', 'CB', 'OCNLI', 'ARC', 'GAOKAO-Bench (2023)', 'TNEWS', 'DROP', 'TyDiQA', 'SQuAD (v2)', 'XSUM', 'MATH', 'Flores-101', 'WinoGrande', 'CMRC', 'SummEdits', 'DRCD', 'C-Eval', 'GSM8K', 'C3', 'TheoremQA', 'ChID']"
OpenEval (text),"['WPLC', 'GAOKAO-Bench', 'WSC (CLUE)', 'SQuAD (zh)', 'Power-seeking', 'COLD', 'CommonMT', 'Corrigible', 'CMMLU', 'OL-CC', 'WGlaw', 'One-box Tendency', 'CAIL (2018)', 'CDIAL-BIAS', 'CMNLI', 'CooridinateAI', 'Guilt Law', 'CORGI-PM', 'M3KE', 'CBBQ', 'TOCP', 'TUMCC', 'TGEA', 'Self-awareness', 'BiPaR', 'SNLI (zh)', 'SWSR', 'Myopia Reward', 'C3', 'ChID']"
PromptBench,"['MultiUN', 'QASC', 'SQuAD (v2)', 'GSM8K', 'IWSLT (2017)', 'MATH', 'GLUE', 'NumerSense', 'MMLU', 'BIG-Bench', 'CSQA']"
Q-Bench,"['LIVE-itw', 'LLVisionQA', 'SPAQ', 'AGIQA-3K', 'KonIQ-10k', 'KADID-10K', 'CGIQA-6K', 'LLDescribe', 'LIVE-FB LSVQ']"
RAFT,"['Over', 'TweetEval (hate)', 'TC', 'TAI', 'NIS', 'SOT', 'ToS', 'B77', 'SRI', 'OSE', 'ADE Corpus (v2)']"
ReCoRD,"['Internet Archive', 'CNN DM']"
ReForm-Eval,"['Winoground', 'Pets37', 'TDIUC (position)', 'VSR', 'TextVQA', 'CIFAR-10', 'CLEVR', 'TDIUC (scene)', 'IC (2015) (Grounded)', 'OK-VQA', 'TDIUC (color)', 'ViQuAE', 'TDIUC (detection)', 'WikiHow', 'TextOCR', 'A-OKVQA', 'DocVQA', 'TextOCR (Grounded)', 'Whoops', 'SROIE', 'TDIUC (utility)', 'GQA', 'TDIUC (sport)', 'VQA (v2)', 'COCO-Text (Grounded)', 'CUTE80', 'MOCHEG', 'VisDial', 'MSCOCO (OC)', 'ImageNet-1K', 'TextCaps', 'FUNSD', 'TDIUC (counting)', 'MSCOCO (MOS)', 'IIIT5K', 'NoCaps', 'MSCOCO (MCI)', 'COCO-Text', 'POIE', 'OCR-VQA', 'SNLI-VE', 'Flickr30K', 'MSCOCO (GOI)', 'IC (2015)', 'VizWiz (singleChoice)', 'WordArt', 'Flowers102', 'VizWiz (yesno)', 'ImageNetVC', 'RefCOCO (res)', 'MEDIC (dts)', 'MP3D', 'ScienceQA']"
RoleEval,"['RoleEval-Chinese', 'RoleEval-Global']"
ScandEval,"['ScaLA (nb)', 'DaNE', 'ScaLA (nn)', 'ScaLA (da)', 'ScandiQA (da)', 'ScandiQA (no)', 'NoReC', 'SUC (v3)', 'SweRec', 'ScaLA (sv)', 'Angry Tweets', 'ScandiQA (sv)', 'NorNE (nn)', 'NorNE (nb)']"
SCROLLS,"['Qasper', 'ContractNLI', 'QuALITY', 'QMSum', 'NarrativeQA', 'SummScreen', 'GovReport']"
SummEdits,"['SciTLDR', 'SAMSum', 'Sales Email', 'Billsum', 'Sales Call', 'QMSum', 'Spotify Podcast', 'Google News', 'ECTSum', 'TinyShakespeare']"
SuperCLUE,"['OPEN Set', 'CLOSE Set', 'CArena']"
SuperGLUE,"['Broad-Coverage Diagnostic', 'COPA', 'RTE', 'BoolQ', 'ReCoRD', 'WSC', 'WiC', 'CB', 'Winogender Diagnostic', 'MultiRC']"
SuperLim (v2),"['DaLAJ-GED-SuperLim (v2)', 'Swedish ABSAbank-Imm (v1.1)', 'SweSAT Synonyms (v1.1)', 'SweWiC (v2)', 'GLUE Diagnostic (sv)', 'SweDN', 'SweFAQ (v2)', 'Swedish analogy (v2)', 'Winograd (sv) (v2)', 'MNLI (sv)', 'STS-B (sv) (v2)', 'Winogender Diagnostic (sv) (v2)', 'SuperSim (v2)', 'Argumentation sentences']"
UHGEval,['XinhuaHallucinations']
Video-Bench Leaderboard,"['MSVD-QA', 'MVQA', 'DLE', 'UCF-Crime', 'TGIF-QA', 'DDM', 'YouCook2', 'ActivityNet-QA', 'MSRVTT-QA', 'MOT', 'SQA3D', 'TVQA', 'NBAQA']"
VLM-Eval,"['ActivityNet', 'UCF101', 'Kinetics-400', 'HMDB51', 'MSVD', 'TGIF', 'MSRVTT']"
YALL,"['AGIEval', 'TruthfulQA', 'GPT4All', 'BIG-Bench']"
