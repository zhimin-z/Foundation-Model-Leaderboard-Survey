Leaderboard,Benchmark
AgentBench,"['Mind2Web', 'ALFWorld', 'WebShop']"
AlpacaEval (v2),"['OASST1', 'Self-Instruct', 'Koala', 'HH-RLHF', 'Vicuna']"
BBH,"['Salient Translation Error Detection', 'Adjective Order', 'Multistep Arithmetic', 'Disambiguation QA', 'Formal Fallacies and Syllogisms with Negation', 'Causal Judgment', 'Web of Lies', 'Boolean Expressions', 'Dyck Languages', 'MovieLens', 'Sequences', 'Sorting Words', 'Object Counting', 'Reasoning about Colored Objects', 'SNARKS', 'Navigation', 'Tracking Shuffled Objects', 'Logical Deduction', 'Geometric Shapes', 'Sports Understanding', 'Tables of Penguins', 'Ruin a Name with One Edit', 'Date Understanding']"
BEIR,"['MSMARCO', 'BioASQ', 'DBpedia', 'Touche (2020)', 'Signal-1M', 'Robust04', 'ArguAna', 'Climate-FEVER', 'CQADupStack (gis)', 'SciFact', 'QQP', 'NQ', 'TREC-COVID', 'FEVER', 'TREC-News', 'SciDocs', 'HotpotQA', 'NFCorpus', 'FiQA (2018)']"
Big Code Models Leaderboard,"['MultiPL-E', 'HumanEval']"
BIG-Bench,"['Human Organs and Senses', 'Alignment of Simplicity Priors for Turing (Complete Concept Learning)', 'ePiC', 'TruthfulQA', 'Twenty Questions', 'Entailed Polarity', 'Checkmate In One Move', 'GEM', 'Adjective Order', 'Multistep Arithmetic', 'Language Games', 'RiddleSense', 'Figure of Speech Detection', 'Sufficient Information', 'Natural Instructions', 'What is the Tao', 'ASCII Word Recognition', 'Web of Lies', 'Phrase Relatedness', 'Hindu Mythology Trivia', 'Sequences', 'Diverse Metrics for Social Biases in Language Models', 'Understanding Fables', 'Spelling Bee', 'Linguistic Mappings', 'Misconceptions', 'Key Value Maps', 'Unit Interpretation', 'Arithmetic', 'Word Unscrambling', 'Truthful QA', 'COM2SENSE', 'Estimating Risk of Suicide', 'Forecasting Subquestions', 'SParC', 'Logical Deduction', 'Simple multiple choice arithmetic (JSON)', 'Swedish to German proverbs', 'Simple arithmetic (JSON)', 'Sports Understanding', 'Wikimedia', 'Python Programming', 'Kannada Riddles', 'Informal and Formal Fallacies', 'Operators', 'Simple Text Editing', 'Physics Questions', 'Linguistic Puzzles', 'Transforming German Sentences to Gender (Inclusive Forms)', 'Emoji Movie', 'CoQA', 'Date Understanding', 'Verb Tense', 'Conceptual Combinations', 'Metaphor Understanding', 'General Knowledge', 'State Tracking in Chess', 'CoDA', 'Codenames', 'Gender Sensitivity Test (zh)', 'Taboo', 'Similarities Test for Abstraction', 'SQuAD (pp)', 'Disambiguation QA', 'Data Wrangling', 'LTI LangID Corpus', 'TellMeWhy', 'Which Wiki Edit', 'CIFAR-10', 'Rhyming', 'Shakespeare Dialogue', 'Sudoku', 'VitaminC', 'Boolean Expressions', 'Spider', 'Kanji ASCII Art', 'CS Algorithms', 'Object Counting', 'Gender Sensitivity Test (English)', 'Presuppositions as NLI', 'Unit Conversion', 'WinoWhy', 'Muslim-Violence Bias', 'Judging Moral Permissibility', 'characterRelations', 'Disfl-QA', 'SNARKS', 'Subject-Verb Agreement', 'Training on the test set', 'SIQA', 'Sentence Ambiguity', 'Navigation', 'Tracking Shuffled Objects', 'Identify Odd Metaphor', 'Geometric Shapes', 'GRE Reading Comprehension', 'Intersection Points', 'MultiEmo', 'Code Description', 'Swahili-English Paremiologic Competence', 'Fact-Checking', 'Authorship Verification', 'SNLI', 'Goal-Step Inference', 'Cause and Effect', 'Reordering', 'Cryptonite', 'BBQ-Lite', 'Logical Arguments', 'Indic Cause and Effect', 'Implicatures', 'Salient Translation Error Detection', 'Social Support', 'YesNoBlackWhite Game', 'High Low Guessing Game', 'StrategyQA', 'Unnatural In-Context Learning', 'Discovery', 'Misconceptions (ru)', 'Cornell Movie-Dialogs Corpus', 'Word Problems on Sets and Graphs', 'Fantasy Reasoning', 'Empirical Judgments', 'Automatic Debugging', 'Matrix Shapes', 'Text Navigation Game', 'Color', 'Cryobiology Spanish', 'Intent Recognition', 'MovieLens', 'Sorting Words', 'English to Russian Proverbs', 'Medical Questions in Russian', 'Long Input Contexts', 'List Functions', 'CRASS', 'Self-awareness', 'Periodic Elements', 'Reasoning about Colored Objects', 'Strange Stories', 'TalkDown', 'MNLI (transliteration)', 'PARSINLU (qa)', 'SIT', 'EmoTag1200', 'Self Evaluation of Tutoring', 'RoFT', 'Common Morpheme', 'Novel Concepts', 'Self Evaluation Courtroom', 'Odd One Out', 'Repeat Copy Logic', 'Factuality', 'ARC (The Abstraction and Reasoning Corpus)', 'Logic Grid Puzzles', 'Minute Mysteries QA', 'Ruin a Name with One Edit', 'Autoclassification', 'CRT', 'Identifying Anachronisms', 'COPA (qa)', 'Scientific Press Release', 'UnQover', 'Simple Ethical Questions', 'Topical-Chat', 'Wino-X (German)', 'Social Bias from Sentence Probability', 'MNLI (ipa)', 'Simple arithmetic with multiple targets (JSON)', 'BBQ-Lite (Json)', 'KPWr', 'Known Unknowns', 'Mathematical Induction', 'Simple arithmetic with subtasks (JSON)', 'Analytic Entailment', 'The Essential, the Excessive, and the Extraneous', 'Formal Fallacies and Syllogisms with Negation', 'Causal Judgment', 'Entailed Polarity in Hindi', 'Understanding Grammar of Unseen Words', 'Root Finding, Optimization and Games', 'TimeDial', 'ISNotes', 'Crash Blossoms', 'Dyck Languages', 'HHH', 'SGD', 'Physical Intuition', 'Modified Arithmetic', 'Python Program Synthesis', 'Irony Identification', 'ASCII MNIST', 'Physics Multiple Choice', 'SQuADShifts', 'Dark Humor Detection', 'Sequential Order', 'Protein Interaction Sites', 'Wikidata', 'Hindi Question Answering', 'MathQA', 'ParsiNLU (rc)', 'Simple arithmetic', 'English Proverbs', 'NQ', 'Identify Math Theorems', 'Metaphor Boolean', 'Conlang Translation Problems', 'Hinglish Toxicity Prediction', 'Tables of Penguins', 'Persian Idioms', 'Cycled Letters', 'Keyword Sentence Transformation', 'Dynamic Counting', 'Analogical Similarity']"
Chatbot Arena Leaderboard,"['MT-Bench', 'MMLU', 'Chatbot Arena Conversations']"
ChEF,"['SEED-Bench', 'VOC (2012)', 'FSC147', 'CIFAR-10', 'Flickr30K', 'MME', 'Omnibenchmark', 'MMBench', 'ScienceQA', 'MSCOCO']"
CLUE,"['ChID', 'OCNLI', 'CLUE Diagnostics', 'DRCD', 'TNEWS', 'CMRC (2018)', 'CMNLI', 'C3', 'AFQMC', 'WSC (CLUE)', 'CSL', 'iFLYTEK']"
CMB,"['CMB-Clin', 'CMB-Exam']"
CMTEB,"['T2Ranking (reranking)', 'OCNLI', 'PAWS-X', 'cMedQA (v2) (retrieval)', 'JDReview', 'cMedQA (v2) (reranking)', 'T2Ranking (retrieval)', 'DuReader (retrieval)', 'Waimai', 'LCQMC', 'Multi-CPR (video)', 'OnlineShopping', 'cCOVID-News', 'mMARCO (retrieval)', 'THUCNews (p2p)', 'CSL (s2s)', 'TNEWS', 'CMNLI', 'ATEC', 'QBQTC', 'Multi-CPR (medical)', 'THUCNews (s2s)', 'Multi-CPR (ecom)', 'mMARCO (reranking)', 'cMedQA (reranking)', 'CSL (p2p)', 'AFQMC', 'BQ', 'STS-B (zh)', 'iFLYTEK']"
Colossal-AI,"['C-Eval', 'CMMLU', 'AGIEval', 'GAOKAO-Bench', 'MMLU']"
CoQA,"['Project Gutenberg', 'MCTest', 'Wikipedia', 'SciQ', 'RACE', 'WritingPrompts', 'CNN DM']"
EvalPlus,"['MBPP', 'HumanEval']"
FacTool,"['Self-Instruct', 'FactPrompts', 'HumanEval', 'RoSE', 'GSM8K']"
FewCLUE,"['ChID', 'OCNLI', 'BUSTM', 'TNEWS', 'WSC (CLUE)', 'CSL', 'EPRSTMT', 'iFLYTEK', 'CSLDCP']"
FlagEval,"['ChID', 'iNaturalist (2018)', 'OCNLI', 'IEMOCAP', 'TruthfulQA', 'SOP', 'CLCC', 'HumanEval', 'KeSpeech', 'VQA (v2)', 'WSC (CLUE)', 'KITTI Eigen split', 'CSL', 'IMDB', 'NYU-Depth', 'VQA-CP', 'BUSTM', 'ADE20K', 'UCF101', 'ImageNet-1K', 'Flowers102', 'CelebA-HQ', 'DTD', 'LibriSpeech', 'BoolQ', 'COCO-Stuff', 'Stanford Cars', 'RAFT', 'TNEWS', 'Food-101', 'TDIUC', 'MSCOCO', 'CUB', 'C-SEM', 'CMMLU', 'Flickr30K', 'MSRVTT', 'GAOKAO-Bench (2023)', 'AISHELL-1', 'Places', 'FGVC-Aircraft', 'MMLU', 'EPRSTMT', 'Cityscapes']"
GENIE,"['ARC-DA (2018)', 'WMT (2021) (de-en)', 'a-NLG', 'WMT (2019) (de-en)', 'XSUM']"
GPT4All,"['ARC', 'OpenbookQA', 'BoolQ', 'WinoGrande', 'PIQA', 'HellaSwag']"
HEIM,"['MSCOCO (base)', 'MSCOCO (zh)', 'PartiPrompts (image quality categories)', 'DrawBench (knowledge categories)', 'Historical Figures', 'Magazine Cover Photos', 'MSCOCO (art styles)', 'Logos', 'DrawBench (reasoning categories)', 'Landing Pages', 'Dailydall.e', 'MSCOCO (fairness - gender)', 'PaintSkills', 'MSCOCO (robustness - typos)', 'DrawBench (image quality categories)', 'Demographic Stereotypes', 'MSCOCO (hi)', 'MSCOCO (fidelity)', 'MSCOCO (fairness - AAVE dialect)', 'Relational Understanding', 'MSCOCO', 'MSCOCO (es)', 'CUB', 'PartiPrompts (knowledge categories)', 'PartiPrompts (reasoning categories)', 'CSP', 'Mental Disorders', 'MSCOCO (efficiency)', 'Winoground', 'I2P']"
HellaSwag Leaderboard,"['ActivityNet Captions', 'WikiHow', 'HellaSwag']"
HELM Classic,"['Copyright (text)', 'MATH (chain-of-thought)', 'TruthfulQA', 'LegalSupport', 'Civil Comments', 'HumanEval', 'MSMARCO', 'TwitterAAE (aa)', 'BLiMP', 'HellaSwag', 'Numerical reasoning', 'IMDB', 'RealToxicityPrompts', 'QuAC', 'Copyright (code)', 'LegalBench', 'Dyck Languages', 'Disinformation (reiteration)', 'Synthetic Reasoning (natural)', 'BoolQ', 'BOLD', 'TwitterAAE (white)', 'Synthetic efficiency', 'Synthetic Reasoning (symbolic)', 'RAFT', 'NQ (open-book)', 'ICE', 'APPS', 'The Pile', 'BBQ', 'GSM8K', 'MATH', 'WikiFact', 'CNN DM', 'Disinformation (wedging)', 'XSUM', 'bAbI', 'MultiLexSum', 'NQ (closed-book)', 'LSAT', 'OpenbookQA', 'MSMARCO (v2)', 'Billsum', 'TwitterAAE', 'WMT (2014)', 'EurLexSum', 'NarrativeQA', 'MedQA', 'DataImputation', 'EntityMatching', 'MMLU']"
HELM Lite,"['NQ (closed-book)', 'OpenbookQA', 'NQ (open-book)', 'WMT (2014)', 'LegalBench', 'NarrativeQA', 'MATH', 'GSM8K', 'MedQA', 'MMLU']"
HHEM Leaderboard,['CNN DM']
InstructEval,"['CRASS', 'IMPACT', 'HumanEval', 'BBH', 'DROP', 'HHH', 'MMLU']"
InterCode,"['SWE-bench', 'NL2Bash', 'InterCode-CTF', 'Spider', 'MBPP']"
KoLA,"['COPEN (CiC)', 'MAVEN', 'Low-Frequency Knowledge', 'KoRC', 'ETM', 'DocRED', 'ETU', 'High-Frequency Knowledge', 'COPEN (CPJ)', 'MuSiQue', 'ETA', 'KQA Pro', '2WikiMQA', 'MAVEN-ERE', 'ETC', 'FewNERD', 'HotpotQA', 'COPEN (CSJ)', 'Encyclopedic']"
L-Eval,"['QuALITY', 'SPACE', 'BigPatent', 'Coursera', 'TOEFL-QA', 'CUAD', 'SFiction', 'Multi-News', 'Qasper', 'TopicRet', 'MultiDoc2Dial', 'OPENREVIEW', 'QMSum', 'GovReport', 'GSM8K', 'LONGFQA', 'SummScreen', 'NQ', 'CodeU', 'NarrativeQA']"
LAiW Leaderboard,"['CAIL (2019)', 'CFM', 'CJRC', 'MLMN', 'AC-NLG', 'CrimeKgAssitant', 'CAIL (2018)', 'Criminal-S', 'CAIL (2020)', 'MSJudge', 'JEC-QA', 'CAIL (2021)']"
Large Language Model Leaderboard,"['ChID', 'ARC', 'OCNLI', 'WinoGrande', 'HumanEval', 'HellaSwag', 'CSL', 'TriviaQA', 'RTE', 'GAOKAO-Bench', 'BUSTM', 'AGIEval', 'DRCD', 'PIQA', 'DROP', 'Broad-Coverage Diagnostic', 'ReCoRD', 'MultiRC', 'StoryCloze', 'WiC', 'LAMBADA', 'BoolQ', 'TNEWS', 'CMNLI', 'C3', 'COPA', 'Winogender Diagnostic', 'SummEdits', 'MATH', 'SIQA', 'GSM8K', 'Xiezhi', 'CSQA', 'XSUM', 'NQ', 'C-Eval', 'CMMLU', 'OpenbookQA', 'SQuAD (v2)', 'Flores-101', 'LCSTS', 'WSC', 'TyDiQA', 'GAOKAO-Bench (2023)', 'BBH', 'CMRC', 'CB', 'TheoremQA', 'AFQMC', 'RACE', 'MMLU', 'EPRSTMT', 'MBPP']"
LLM Benchmarker Suite,"['NQ', 'OpenbookQA', 'BoolQ', 'WinoGrande', 'AGIEval', 'QuAC', 'HumanEval', 'GSM8K', 'HellaSwag', 'MMLU', 'TriviaQA']"
LLM-Leaderboard,"['Chatbot Arena Conversations', 'WinoGrande', 'HumanEval', 'TriviaQA', 'HellaSwag', 'MMLU', 'LAMBADA']"
LongBench,"['MultiFieldQA (zh)', 'RepoBench-P', 'TriviaQA', 'PassageRetrieval (zh)', 'TREC', 'DuReader', 'PassageRetrieval', 'Multi-News', 'LCC', 'MultiFieldQA', 'Qasper', 'PassageCount', 'VCSUM', 'QMSum', 'MuSiQue', 'GovReport', 'LSHTC', '2WikiMQA', 'SAMSum', 'NarrativeQA', 'HotpotQA']"
LVLM-eHub,"['ScienceQA IMG', 'FUNSD', 'MSCOCO (random)', 'OK-VQA', 'HOST', 'Franka Kitchen', 'CIFAR-10', 'VCR-MCI', 'GQA', 'ImageNet-1K', 'SVTP', 'Virtual Home', 'OCR-VQA', 'Flowers102', 'IIIT5K', 'VCR-OC', 'MSCOCO (OC)', 'IC (2013)', 'Total-Text', 'CUTE80', 'VSR', 'ImageNetVC', 'Pets37', 'VisDial', 'Whoops', 'IC (2015)', 'NoCaps', 'COCO-Text', 'WOST', 'DocVQA', 'TextVQA', 'CTW', 'STVQA', 'Meta-World', 'MSCOCO (popular)', 'MSCOCO (MCI)', 'VCR', 'SNLI-VE', 'SVT', 'IconQA', 'WordArt', 'Minecraft', 'Flickr30K', 'MSCOCO (adversarial)', 'SROIE', 'VizWiz']"
MedBench,"['SafetyBench', 'Med-Exam', 'CHIP-CDEE', 'SMDoc', 'MedHC', 'DrugCA', 'CHIP-CDN', 'MedDG', 'MedSpeQA', 'DBMHG', 'CMB-Clin', 'DDx-basic', 'IMCS-MRG (v2)', 'CMeIE', 'CHIP-CTC', 'CMeEE', 'DDx-advanced', 'MedHG', 'MedMC', 'MedTreat']"
MMBench,"['KonIQ-10k', 'ARAS', 'PISC', 'TextVQA', 'W3C School', 'COCO Captions', 'Places', 'ScienceQA', 'LLaVA-Bench', 'CLEVR', 'Internet', 'VSR']"
MMLU-by-task Leaderboard,"['ARC', 'MMLU', 'HellaSwag']"
MOCHA,"['DROP', 'CosmosQA', 'NarrativeQA', 'SIQA', 'MCScript', 'Quoref']"
MTEB,"['ScaLA (nb)', 'DBpedia', 'CQADupStack (webmasters)', 'STS (2014)', 'FiQA (2018) (pl)', 'AMCD', 'Stack Exchange', 'SciDocs (rr)', 'Touche (2020)', 'Allegro Reviews', 'Reddit (p2p)', 'SprintDuplicateQuestions', 'Nordic Language Identification', 'MASSIVE (scenario)', 'CQADupStack (android)', 'B77', 'STS (2022)', 'ScaLA (sv)', 'MASSIVE (intent)', 'Tatoeba', 'LinkSO', 'Polish CDSCorpus (relatedness)', 'arXiv (p2p)', 'MSMARCO (pl)', 'NQ (pl)', 'MTOP (intent)', 'CQADupStack (tex)', 'MTOP (domain)', 'ScaLA (nn)', 'ArguAna (pl)', 'TwitterSemEval (2015)', 'PPC', 'CQADupStack (stats)', 'SciDocs', 'CQADupStack (gaming)', 'FiQA (2018)', 'DaLAJ', 'CQADupStack (physics)', 'BIOSSES', 'SweFAQ (v2)', 'STS (2013)', 'MSMARCO', 'HotpotQA (pl)', 'QQP (pl)', 'STS (2017)', 'medRxiv (p2p)', 'Reddit', 'SICK (pl)', 'medRxiv (s2s)', 'ArguAna', 'CQADupStack (gis)', 'Blurbs (s2s)', 'STS (2016)', 'PSC', 'Stack Exchange (p2p)', 'CARER', 'Amazon Polarity', 'AskUbuntu', 'NoReC', 'FEVER', 'Tweet Sentiment Extraction', '10kGNAD (p2p)', 'NFCorpus', 'NFCorpus (pl)', 'LanguageNet', 'MIND', 'arXiv (s2s)', 'DanishPoliticalComments', 'SummEval', 'DKhate', 'bioRxiv (p2p)', 'LCC', 'SweRec', 'CBD', 'CQADupStack (wordpress)', 'QQP', 'SciDocs (pl)', 'DBpedia (pl)', 'MARC', '10kGNAD (s2s)', 'PolEmo (v2) (in)', 'Blurbs (p2p)', 'STS-B', 'HotpotQA', 'CQADupStack (mathematica)', 'STS (2012)', 'IMDB', 'PolEmo (v2) (out)', 'STS (2015)', 'SciFact (pl)', 'bioRxiv (s2s)', 'SICK (relatedness)', 'BUCC', 'Polish CDSCorpus', 'Civil Comments (tc)', 'Climate-FEVER', 'SciFact', 'Angry Tweets', 'CQADupStack (programmers)', 'SICK (relatedness) (pl)', 'NPSC', 'NQ', 'Bornholmsk', 'MSMARCO (v2)', 'TREC-COVID', 'CQADupStack (unix)', 'PAC', 'ScaLA (da)', '8TAGS', '20 Newsgroups', 'CQADupStack (english)']"
Multi-modal Modal Leaderboard,"['CCBench', 'MMMU (val)', 'MMBench (test) (zh)', 'MathVista (minitest)', 'HallusionBench', 'MMBench (test)', 'MM-Vet', 'SEED-Bench (img)', 'MME (normalized)']"
MVBench,"['TVQA', 'CLEVRER', 'MovieNet', 'PAXION', 'NTU RGB+D', 'STAR', 'VLN-CE', 'MiT', 'Perception Test', 'Charades-STA', 'FunQA']"
Open Ko-LLM Leaderboard,"['ARC (Korean)', 'TruthfulQA (Korean)', 'CommonGen', 'MMLU (Korean)', 'HellaSwag (Korean)']"
Open LLM Leaderboard,"['ARC', 'WinoGrande', 'TruthfulQA', 'GSM8K', 'HellaSwag', 'MMLU']"
Open Multilingual LLM Evaluation Leaderboard,"['ARC', 'TruthfulQA', 'MMLU', 'HellaSwag']"
OpenEval (text),"['SQuAD (zh)', 'ChID', 'Guilt Law', 'WPLC', 'BiPaR', 'WSC (CLUE)', 'SWSR', 'TOCP', 'OL-CC', 'M3KE', 'TUMCC', 'CAIL (2018)', 'CORGI-PM', 'CommonMT', 'Self-awareness', 'Power-seeking', 'CMNLI', 'C3', 'Corrigible', 'CBBQ', 'WGlaw', 'One-box Tendency', 'TGEA', 'CMMLU', 'Myopia Reward', 'SNLI (zh)', 'CDIAL-BIAS', 'COLD', 'CooridinateAI', 'GAOKAO-Bench']"
PromptBench,"['GLUE', 'MultiUN', 'CSQA', 'NumerSense', 'SQuAD (v2)', 'QASC', 'IWSLT (2017)', 'BIG-Bench', 'MATH', 'GSM8K', 'MMLU']"
Q-Bench,"['LIVE-FB LSVQ', 'LLDescribe', 'KADID-10K', 'KonIQ-10k', 'SPAQ', 'CGIQA-6K', 'AGIQA-3K', 'LIVE-itw', 'LLVisionQA']"
RAFT,"['TC', 'Over', 'NIS', 'ToS', 'ADE Corpus (v2)', 'TweetEval (hate)', 'SRI', 'OSE', 'B77', 'SOT', 'TAI']"
ReCoRD,"['Internet Archive', 'CNN DM']"
ReForm-Eval,"['ViQuAE', 'TextCaps', 'MP3D', 'FUNSD', 'OK-VQA', 'VQA (v2)', 'MEDIC (dts)', 'COCO-Text (Grounded)', 'A-OKVQA', 'CIFAR-10', 'TDIUC (color)', 'RefCOCO (res)', 'TDIUC (detection)', 'GQA', 'ImageNet-1K', 'TDIUC (counting)', 'POIE', 'OCR-VQA', 'ScienceQA', 'Flowers102', 'TextOCR', 'IIIT5K', 'CLEVR', 'MSCOCO (OC)', 'VSR', 'CUTE80', 'ImageNetVC', 'TDIUC (position)', 'Pets37', 'VisDial', 'MSCOCO (GOI)', 'Whoops', 'VizWiz (singleChoice)', 'IC (2015)', 'NoCaps', 'TDIUC (sport)', 'COCO-Text', 'TDIUC (utility)', 'TextVQA', 'DocVQA', 'WikiHow', 'TDIUC (scene)', 'IC (2015) (Grounded)', 'MSCOCO (MCI)', 'TextOCR (Grounded)', 'MSCOCO (MOS)', 'SNLI-VE', 'WordArt', 'Flickr30K', 'VizWiz (yesno)', 'SROIE', 'Winoground', 'MOCHEG']"
RoleEval,"['RoleEval-Chinese', 'RoleEval-Global']"
ScandEval,"['ScaLA (nn)', 'ScaLA (nb)', 'NorNE (nb)', 'ScandiQA (sv)', 'NorNE (nn)', 'NoReC', 'SweRec', 'SUC (v3)', 'Angry Tweets', 'ScandiQA (da)', 'ScandiQA (no)', 'ScaLA (sv)', 'ScaLA (da)', 'DaNE']"
SCROLLS,"['SummScreen', 'QMSum', 'GovReport', 'ContractNLI', 'NarrativeQA', 'QuALITY', 'Qasper']"
SummEdits,"['SAMSum', 'Sales Email', 'QMSum', 'Billsum', 'TinyShakespeare', 'SciTLDR', 'Google News', 'ECTSum', 'Sales Call', 'Spotify Podcast']"
SuperCLUE,"['CLOSE Set', 'OPEN Set', 'CArena']"
SuperGLUE,"['BoolQ', 'WSC', 'CB', 'COPA', 'Winogender Diagnostic', 'Broad-Coverage Diagnostic', 'ReCoRD', 'MultiRC', 'WiC', 'RTE']"
SuperLim (v2),"['SweSAT Synonyms (v1.1)', 'Swedish ABSAbank-Imm (v1.1)', 'SweFAQ (v2)', 'Argumentation sentences', 'Swedish analogy (v2)', 'SuperSim (v2)', 'STS-B (sv) (v2)', 'SweWiC (v2)', 'Winograd (sv) (v2)', 'GLUE Diagnostic (sv)', 'DaLAJ-GED-SuperLim (v2)', 'Winogender Diagnostic (sv) (v2)', 'SweDN', 'MNLI (sv)']"
Video-Bench Leaderboard,"['TVQA', 'ActivityNet-QA', 'UCF-Crime', 'MSRVTT-QA', 'DDM', 'SQA3D', 'NBAQA', 'MSVD-QA', 'TGIF-QA', 'DLE', 'MVQA', 'YouCook2', 'MOT']"
VLM-Eval,"['MSVD', 'HMDB51', 'TGIF', 'MSRVTT', 'UCF101', 'Kinetics-400', 'ActivityNet']"
YALL,"['BIG-Bench', 'GPT4All', 'TruthfulQA', 'AGIEval']"
