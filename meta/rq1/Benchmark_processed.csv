Leaderboard,Benchmark
AgentBench,"['Mind2Web', 'ALFWorld', 'WebShop']"
AlpacaEval (v2),"['HH-RLHF', 'OASST1', 'Self-Instruct', 'Vicuna', 'Koala']"
BBH,"['Formal Fallacies and Syllogisms with Negation', 'Causal Judgment', 'Salient Translation Error Detection', 'Logical Deduction', 'Web of Lies', 'Ruin a Name with One Edit', 'Multistep Arithmetic', 'Navigation', 'Geometric Shapes', 'Adjective Order', 'Date Understanding', 'Boolean Expressions', 'Sequences', 'Dyck Languages', 'Tracking Shuffled Objects', 'SNARKS', 'Object Counting', 'Disambiguation QA', 'Sports Understanding', 'Reasoning about Colored Objects', 'Sorting Words', 'Tables of Penguins', 'MovieLens']"
BEIR,"['Signal-1M', 'SciDocs', 'DBpedia', 'Touche (2020)', 'MSMARCO', 'BioASQ', 'NFCorpus', 'Robust04', 'CQADupStack (gis)', 'TREC-News', 'NQ', 'TREC-COVID', 'FiQA (2018)', 'Climate-FEVER', 'ArguAna', 'FEVER', 'QQP', 'SciFact', 'HotpotQA']"
Big Code Models Leaderboard,"['MultiPL-E', 'HumanEval']"
BIG-Bench,"['StrategyQA', 'Irony Identification', 'Wino-X (German)', 'Formal Fallacies and Syllogisms with Negation', 'Causal Judgment', 'Metaphor Boolean', 'KPWr', 'Misconceptions (ru)', 'Salient Translation Error Detection', 'Scientific Press Release', 'Hindi Question Answering', 'Kanji ASCII Art', 'Web of Lies', 'Ruin a Name with One Edit', 'Known Unknowns', 'Intent Recognition', 'Conlang Translation Problems', 'Logical Arguments', 'Multistep Arithmetic', 'Dark Humor Detection', 'GRE Reading Comprehension', 'Adjective Order', 'Authorship Verification', 'Social Bias from Sentence Probability', 'SGD', 'Social Support', 'CRASS', 'VitaminC', 'Keyword Sentence Transformation', 'Identifying Anachronisms', 'Periodic Elements', 'Transforming German Sentences to Gender (Inclusive Forms)', 'Minute Mysteries QA', 'Key Value Maps', 'ASCII Word Recognition', 'Strange Stories', 'High Low Guessing Game', 'Hinglish Toxicity Prediction', 'MultiEmo', 'SIT', 'Conceptual Combinations', 'Wikimedia', 'Sufficient Information', 'Sentence Ambiguity', 'Taboo', 'Disfl-QA', 'UnQover', 'Forecasting Subquestions', 'Reasoning about Colored Objects', 'Common Morpheme', 'Sorting Words', 'The Essential, the Excessive, and the Extraneous', 'Verb Tense', 'MovieLens', 'TimeDial', 'Wikidata', 'MathQA', 'Twenty Questions', 'Gender Sensitivity Test (zh)', 'Fact-Checking', 'TruthfulQA', 'HHH', 'Informal and Formal Fallacies', 'COM2SENSE', 'Unit Interpretation', 'Color', 'Judging Moral Permissibility', 'CoDA', 'Python Programming', 'Logic Grid Puzzles', 'Root Finding, Optimization and Games', 'TalkDown', 'Geometric Shapes', 'SNLI', 'Linguistic Puzzles', 'Cause and Effect', 'MNLI (transliteration)', 'ParsiNLU (rc)', 'ARC (The Abstraction and Reasoning Corpus)', 'Boolean Expressions', 'Human Organs and Senses', 'Mathematical Induction', 'SQuAD (pp)', 'Simple multiple choice arithmetic (JSON)', 'Physics Multiple Choice', 'Analogical Similarity', 'Alignment of Simplicity Priors for Turing (Complete Concept Learning)', 'Simple arithmetic with multiple targets (JSON)', 'MNLI (ipa)', 'NQ', 'Natural Instructions', 'Empirical Judgments', 'Rhyming', 'Tracking Shuffled Objects', 'Language Games', 'PARSINLU (qa)', 'Matrix Shapes', 'Physics Questions', 'Novel Concepts', 'Data Wrangling', 'Operators', 'Simple Ethical Questions', 'CRT', 'Word Unscrambling', 'CS Algorithms', 'Training on the test set', 'Factuality', 'Tables of Penguins', 'SQuADShifts', 'Which Wiki Edit', 'Repeat Copy Logic', 'Arithmetic', 'EmoTag1200', 'Sudoku', 'Self Evaluation of Tutoring', 'Spelling Bee', 'RoFT', 'Discovery', 'CIFAR-10', 'Odd One Out', 'Autoclassification', 'Goal-Step Inference', 'Python Program Synthesis', 'Presuppositions as NLI', 'Cycled Letters', 'Emoji Movie', 'ePiC', 'Medical Questions in Russian', 'Cryobiology Spanish', 'Cryptonite', 'BBQ-Lite', 'Implicatures', 'Shakespeare Dialogue', 'Gender Sensitivity Test (English)', 'Simple arithmetic (JSON)', 'Code Description', 'Intersection Points', 'Long Input Contexts', 'Indic Cause and Effect', 'Entailed Polarity', 'Muslim-Violence Bias', 'Estimating Risk of Suicide', 'Swedish to German proverbs', 'Dyck Languages', 'Physical Intuition', 'SNARKS', 'Object Counting', 'Misconceptions', 'Cornell Movie-Dialogs Corpus', 'Word Problems on Sets and Graphs', 'Kannada Riddles', 'ISNotes', 'Disambiguation QA', 'Self Evaluation Courtroom', 'Spider', 'Unnatural In-Context Learning', 'English Proverbs', 'Diverse Metrics for Social Biases in Language Models', 'Analytic Entailment', 'Identify Math Theorems', 'Truthful QA', 'Sports Understanding', 'RiddleSense', 'LTI LangID Corpus', 'Linguistic Mappings', 'BBQ-Lite (Json)', 'Swahili-English Paremiologic Competence', 'Understanding Grammar of Unseen Words', 'Codenames', 'Sequential Order', 'Reordering', 'Entailed Polarity in Hindi', 'List Functions', 'Simple Text Editing', 'Simple arithmetic with subtasks (JSON)', 'Subject-Verb Agreement', 'Logical Deduction', 'Protein Interaction Sites', 'Self-awareness', 'Identify Odd Metaphor', 'Dynamic Counting', 'Figure of Speech Detection', 'GEM', 'Metaphor Understanding', 'Navigation', 'Persian Idioms', 'COPA (qa)', 'SIQA', 'Date Understanding', 'Sequences', 'Crash Blossoms', 'Fantasy Reasoning', 'English to Russian Proverbs', 'SParC', 'Similarities Test for Abstraction', 'Hindu Mythology Trivia', 'Text Navigation Game', 'ASCII MNIST', 'TellMeWhy', 'Understanding Fables', 'Unit Conversion', 'WinoWhy', 'Modified Arithmetic', 'characterRelations', 'Automatic Debugging', 'State Tracking in Chess', 'Topical-Chat', 'What is the Tao', 'Simple arithmetic', 'Checkmate In One Move', 'YesNoBlackWhite Game', 'Phrase Relatedness', 'General Knowledge', 'CoQA']"
Chatbot Arena Leaderboard,"['MT-Bench', 'Chatbot Arena Conversations', 'MMLU']"
ChEF,"['MME', 'ScienceQA', 'VOC (2012)', 'FSC147', 'CIFAR-10', 'SEED-Bench', 'Flickr30K', 'Omnibenchmark', 'MMBench', 'MSCOCO']"
CLUE,"['CMNLI', 'ChID', 'DRCD', 'AFQMC', 'iFLYTEK', 'OCNLI', 'CMRC (2018)', 'WSC (CLUE)', 'CLUE Diagnostics', 'C3', 'TNEWS', 'CSL']"
CMB,"['CMB-Clin', 'CMB-Exam']"
CMTEB,"['cMedQA (v2) (reranking)', 'ATEC', 'BQ', 'CSL (s2s)', 'PAWS-X', 'Multi-CPR (medical)', 'OCNLI', 'THUCNews (s2s)', 'cMedQA (v2) (retrieval)', 'cMedQA (reranking)', 'Waimai', 'JDReview', 'T2Ranking (retrieval)', 'cCOVID-News', 'CMNLI', 'AFQMC', 'mMARCO (reranking)', 'CSL (p2p)', 'LCQMC', 'T2Ranking (reranking)', 'Multi-CPR (ecom)', 'mMARCO (retrieval)', 'TNEWS', 'OnlineShopping', 'QBQTC', 'iFLYTEK', 'DuReader (retrieval)', 'Multi-CPR (video)', 'THUCNews (p2p)', 'STS-B (zh)']"
Colossal-AI,"['CMMLU', 'C-Eval', 'AGIEval', 'GAOKAO-Bench', 'MMLU']"
EvalPlus,"['MBPP', 'HumanEval']"
FacTool,"['FactPrompts', 'RoSE', 'Self-Instruct', 'GSM8K', 'HumanEval']"
FewCLUE,"['BUSTM', 'ChID', 'iFLYTEK', 'OCNLI', 'WSC (CLUE)', 'EPRSTMT', 'TNEWS', 'CSL', 'CSLDCP']"
FlagEval,"['ADE20K', 'TruthfulQA', 'UCF101', 'Places', 'FGVC-Aircraft', 'WSC (CLUE)', 'Food-101', 'HumanEval', 'MMLU', 'LibriSpeech', 'VQA-CP', 'ChID', 'ImageNet-1K', 'Flowers102', 'GAOKAO-Bench (2023)', 'OCNLI', 'VQA (v2)', 'DTD', 'BoolQ', 'TDIUC', 'CelebA-HQ', 'CMMLU', 'NYU-Depth', 'SOP', 'iNaturalist (2018)', 'CUB', 'KITTI Eigen split', 'Flickr30K', 'MSRVTT', 'TNEWS', 'CSL', 'CLCC', 'IEMOCAP', 'BUSTM', 'Cityscapes', 'C-SEM', 'Stanford Cars', 'IMDB', 'AISHELL-1', 'KeSpeech', 'COCO-Stuff', 'EPRSTMT', 'RAFT', 'MSCOCO']"
GENIE,"['ARC-DA (2018)', 'a-NLG', 'WMT (2021) (de-en)', 'XSUM', 'WMT (2019) (de-en)']"
Hallucination Leaderboard,['CNN DM']
HEIM,"['Logos', 'Mental Disorders', 'Magazine Cover Photos', 'MSCOCO (efficiency)', 'Demographic Stereotypes', 'DrawBench (image quality categories)', 'MSCOCO (base)', 'Winoground', 'Relational Understanding', 'Landing Pages', 'MSCOCO (fairness - gender)', 'MSCOCO (robustness - typos)', 'PaintSkills', 'DrawBench (reasoning categories)', 'MSCOCO (fidelity)', 'MSCOCO (zh)', 'PartiPrompts (knowledge categories)', 'Dailydall.e', 'MSCOCO (es)', 'MSCOCO (art styles)', 'CSP', 'CUB', 'PartiPrompts (reasoning categories)', 'Historical Figures', 'I2P', 'PartiPrompts (image quality categories)', 'DrawBench (knowledge categories)', 'MSCOCO (hi)', 'MSCOCO (fairness - AAVE dialect)', 'MSCOCO']"
HellaSwag Leaderboard,"['HellaSwag', 'ActivityNet Captions', 'WikiHow']"
HELM Classic,"['bAbI', 'TwitterAAE', 'TruthfulQA', 'EntityMatching', 'WMT (2014)', 'Numerical reasoning', 'RealToxicityPrompts', 'MATH', 'MedQA', 'MATH (chain-of-thought)', 'APPS', 'BOLD', 'NQ (closed-book)', 'Copyright (code)', 'HumanEval', 'MMLU', 'Synthetic Reasoning (symbolic)', 'LSAT', 'MSMARCO', 'The Pile', 'Copyright (text)', 'LegalSupport', 'Synthetic Reasoning (natural)', 'LegalBench', 'BoolQ', 'Billsum', 'TwitterAAE (white)', 'Dyck Languages', 'OpenbookQA', 'ICE', 'DataImputation', 'MSMARCO (v2)', 'NQ (open-book)', 'XSUM', 'EurLexSum', 'TwitterAAE (aa)', 'BBQ', 'Disinformation (reiteration)', 'Civil Comments', 'Disinformation (wedging)', 'IMDB', 'Synthetic efficiency', 'WikiFact', 'HellaSwag', 'MultiLexSum', 'BLiMP', 'QuAC', 'RAFT', 'NarrativeQA', 'GSM8K', 'CNN DM']"
HELM Lite,"['OpenbookQA', 'WMT (2014)', 'MedQA', 'MATH', 'LegalBench', 'NQ (open-book)', 'NQ (closed-book)', 'NarrativeQA', 'GSM8K', 'MMLU']"
InstructEval,"['HHH', 'CRASS', 'DROP', 'BBH', 'IMPACT', 'HumanEval', 'MMLU']"
InterCode,"['NL2Bash', 'SWE-bench', 'Spider', 'InterCode-CTF', 'MBPP']"
KoLA,"['FewNERD', 'ETU', 'KoRC', 'COPEN (CPJ)', 'ETC', 'Encyclopedic', 'High-Frequency Knowledge', 'DocRED', 'COPEN (CiC)', 'COPEN (CSJ)', 'ETA', 'Low-Frequency Knowledge', 'MuSiQue', 'MAVEN-ERE', 'ETM', 'HotpotQA', '2WikiMQA', 'MAVEN', 'KQA Pro']"
L-Eval,"['Multi-News', 'BigPatent', 'QuALITY', 'SPACE', 'OPENREVIEW', 'CUAD', 'SummScreen', 'LONGFQA', 'TopicRet', 'NQ', 'TOEFL-QA', 'GovReport', 'Qasper', 'Coursera', 'QMSum', 'MultiDoc2Dial', 'SFiction', 'NarrativeQA', 'GSM8K', 'CodeU']"
LAiW Leaderboard,"['MSJudge', 'CAIL (2019)', 'CFM', 'CAIL (2021)', 'CAIL (2018)', 'AC-NLG', 'CAIL (2020)', 'Criminal-S', 'CJRC', 'MLMN', 'CrimeKgAssitant', 'JEC-QA']"
Large Language Model Leaderboard,"['WiC', 'StoryCloze', 'MultiRC', 'MATH', 'Xiezhi', 'AGIEval', 'HumanEval', 'MMLU', 'ChID', 'DRCD', 'TriviaQA', 'CSQA', 'SIQA', 'GAOKAO-Bench (2023)', 'OCNLI', 'Broad-Coverage Diagnostic', 'PIQA', 'COPA', 'ARC', 'BoolQ', 'ReCoRD', 'MBPP', 'CMRC', 'CMNLI', 'OpenbookQA', 'NQ', 'CMMLU', 'AFQMC', 'RTE', 'TheoremQA', 'SQuAD (v2)', 'DROP', 'BBH', 'Winogender Diagnostic', 'CSL', 'TNEWS', 'XSUM', 'BUSTM', 'Flores-101', 'RACE', 'CB', 'LAMBADA', 'C-Eval', 'TyDiQA', 'LCSTS', 'HellaSwag', 'WinoGrande', 'C3', 'EPRSTMT', 'WSC', 'SummEdits', 'GAOKAO-Bench', 'GSM8K']"
LLM Benchmarker Suite,"['NQ', 'OpenbookQA', 'TriviaQA', 'HellaSwag', 'AGIEval', 'WinoGrande', 'QuAC', 'BoolQ', 'GSM8K', 'HumanEval', 'MMLU']"
LLM-Leaderboard,"['TriviaQA', 'LAMBADA', 'HellaSwag', 'WinoGrande', 'Chatbot Arena Conversations', 'HumanEval', 'MMLU']"
LongBench,"['Multi-News', 'RepoBench-P', 'SAMSum', 'TriviaQA', 'VCSUM', 'PassageRetrieval (zh)', 'MultiFieldQA', 'PassageCount', 'GovReport', 'MultiFieldQA (zh)', 'Qasper', 'DuReader', 'QMSum', 'MuSiQue', 'TREC', 'LCC', 'PassageRetrieval', 'HotpotQA', 'LSHTC', '2WikiMQA', 'NarrativeQA']"
LVLM-eHub,"['OCR-VQA', 'Total-Text', 'WordArt', 'IC (2015)', 'VisDial', 'COCO-Text', 'CIFAR-10', 'Pets37', 'Franka Kitchen', 'SROIE', 'SNLI-VE', 'ImageNet-1K', 'Flowers102', 'Whoops', 'MSCOCO (OC)', 'Meta-World', 'VCR', 'STVQA', 'SVTP', 'MSCOCO (random)', 'IC (2013)', 'ScienceQA IMG', 'VCR-MCI', 'SVT', 'NoCaps', 'MSCOCO (MCI)', 'Flickr30K', 'DocVQA', 'VizWiz', 'ImageNetVC', 'Minecraft', 'Virtual Home', 'CUTE80', 'MSCOCO (popular)', 'HOST', 'CTW', 'FUNSD', 'GQA', 'MSCOCO (adversarial)', 'TextVQA', 'WOST', 'VCR-OC', 'IIIT5K', 'OK-VQA', 'IconQA', 'VSR']"
MedBench,"['MedHC', 'DrugCA', 'IMCS-MRG (v2)', 'DDx-advanced', 'MedSpeQA', 'CMB-Clin', 'MedHG', 'CMeEE', 'DDx-basic', 'CMeIE', 'CHIP-CDN', 'CHIP-CTC', 'MedTreat', 'MedDG', 'DBMHG', 'MedMC', 'Med-Exam', 'SMDoc', 'CHIP-CDEE', 'SafetyBench']"
MMBench,"['KonIQ-10k', 'ScienceQA', 'PISC', 'Places', 'TextVQA', 'LLaVA-Bench', 'COCO Captions', 'W3C School', 'CLEVR', 'Internet', 'ARAS', 'VSR']"
MMLU-by-task Leaderboard,"['ARC', 'HellaSwag', 'MMLU']"
MOCHA,"['Quoref', 'MCScript', 'SIQA', 'DROP', 'CosmosQA', 'NarrativeQA']"
MTEB,"['CQADupStack (mathematica)', 'CARER', 'MASSIVE (scenario)', 'Tatoeba', 'NoReC', 'DBpedia', 'CQADupStack (english)', 'STS (2022)', 'Tweet Sentiment Extraction', 'AMCD', 'SweFAQ (v2)', 'CQADupStack (gaming)', 'MARC', 'SciDocs (pl)', 'SweRec', 'TREC-COVID', 'FiQA (2018)', 'arXiv (s2s)', 'SciDocs (rr)', 'MSMARCO (v2)', 'NQ (pl)', '10kGNAD (s2s)', 'QQP', 'PPC', 'LCC', 'CQADupStack (unix)', 'STS (2014)', 'CQADupStack (stats)', 'ScaLA (da)', 'arXiv (p2p)', 'SciFact (pl)', 'MTOP (domain)', 'ArguAna (pl)', 'PolEmo (v2) (in)', 'BIOSSES', 'Polish CDSCorpus', 'SummEval', 'MASSIVE (intent)', 'CQADupStack (physics)', 'DKhate', 'CBD', 'BUCC', 'CQADupStack (wordpress)', 'TwitterSemEval (2015)', 'PAC', 'CQADupStack (gis)', 'NQ', 'STS (2015)', 'ArguAna', 'B77', 'medRxiv (s2s)', '10kGNAD (p2p)', 'FEVER', 'STS (2016)', 'MSMARCO (pl)', 'Angry Tweets', 'LinkSO', 'CQADupStack (webmasters)', 'PSC', 'IMDB', 'Bornholmsk', 'HotpotQA', 'DanishPoliticalComments', '8TAGS', 'STS (2017)', 'Blurbs (p2p)', 'Amazon Polarity', 'ScaLA (nb)', 'DBpedia (pl)', 'MSMARCO', 'CQADupStack (programmers)', 'SICK (relatedness)', 'NPSC', 'Blurbs (s2s)', 'SICK (pl)', '20 Newsgroups', 'DaLAJ', 'SICK (relatedness) (pl)', 'AskUbuntu', 'FiQA (2018) (pl)', 'HotpotQA (pl)', 'Nordic Language Identification', 'ScaLA (nn)', 'STS (2013)', 'bioRxiv (p2p)', 'CQADupStack (tex)', 'CQADupStack (android)', 'STS-B', 'NFCorpus (pl)', 'Stack Exchange', 'STS (2012)', 'Allegro Reviews', 'Reddit (p2p)', 'SciDocs', 'LanguageNet', 'MIND', 'QQP (pl)', 'Touche (2020)', 'Stack Exchange (p2p)', 'medRxiv (p2p)', 'NFCorpus', 'Polish CDSCorpus (relatedness)', 'Reddit', 'bioRxiv (s2s)', 'Civil Comments (tc)', 'Climate-FEVER', 'PolEmo (v2) (out)', 'ScaLA (sv)', 'SprintDuplicateQuestions', 'MTOP (intent)', 'SciFact']"
Multi-modal Modal Leaderboard,"['MMBench (test)', 'MMMU (val)', 'MathVista (minitest)', 'HallusionBench', 'MME (normalized)', 'SEED-Bench (img)', 'MMBench (test) (zh)', 'MM-Vet', 'CCBench']"
MVBench,"['Perception Test', 'NTU RGB+D', 'CLEVRER', 'MovieNet', 'MiT', 'STAR', 'FunQA', 'Charades-STA', 'VLN-CE', 'TVQA', 'PAXION']"
Open Ko-LLM Leaderboard,"['MMLU (Korean)', 'HellaSwag (Korean)', 'TruthfulQA (Korean)', 'CommonGen', 'ARC (Korean)']"
Open LLM Leaderboard,"['TruthfulQA', 'HellaSwag', 'WinoGrande', 'ARC', 'GSM8K', 'MMLU']"
Open Multilingual LLM Evaluation Leaderboard,"['ARC', 'HellaSwag', 'TruthfulQA', 'MMLU']"
OpenEval (text),"['One-box Tendency', 'CooridinateAI', 'Self-awareness', 'SNLI (zh)', 'WSC (CLUE)', 'SQuAD (zh)', 'ChID', 'WPLC', 'COLD', 'CMNLI', 'SWSR', 'Corrigible', 'Myopia Reward', 'TGEA', 'CMMLU', 'CDIAL-BIAS', 'CommonMT', 'TUMCC', 'CAIL (2018)', 'Guilt Law', 'CBBQ', 'CORGI-PM', 'BiPaR', 'TOCP', 'OL-CC', 'M3KE', 'WGlaw', 'C3', 'GAOKAO-Bench', 'Power-seeking']"
PromptBench,"['CSQA', 'MATH', 'QASC', 'SQuAD (v2)', 'IWSLT (2017)', 'GLUE', 'MultiUN', 'NumerSense', 'GSM8K', 'BIG-Bench', 'MMLU']"
Q-Bench,"['KonIQ-10k', 'LLVisionQA', 'CGIQA-6K', 'SPAQ', 'AGIQA-3K', 'KADID-10K', 'LIVE-itw', 'LIVE-FB LSVQ', 'LLDescribe']"
RAFT,"['TAI', 'ToS', 'SOT', 'OSE', 'SRI', 'B77', 'NIS', 'Over', 'TC', 'TweetEval (hate)', 'ADE Corpus (v2)']"
ReForm-Eval,"['OCR-VQA', 'ScienceQA', 'VisDial', 'WordArt', 'IC (2015)', 'COCO-Text', 'CIFAR-10', 'Pets37', 'VizWiz (yesno)', 'ViQuAE', 'Winoground', 'POIE', 'TDIUC (color)', 'TextOCR', 'TDIUC (counting)', 'SROIE', 'SNLI-VE', 'ImageNet-1K', 'Flowers102', 'Whoops', 'VQA (v2)', 'TDIUC (detection)', 'MSCOCO (OC)', 'TDIUC (sport)', 'MSCOCO (GOI)', 'IC (2015) (Grounded)', 'TextOCR (Grounded)', 'WikiHow', 'MSCOCO (MOS)', 'MEDIC (dts)', 'COCO-Text (Grounded)', 'Flickr30K', 'NoCaps', 'MSCOCO (MCI)', 'ImageNetVC', 'DocVQA', 'A-OKVQA', 'CUTE80', 'TextCaps', 'FUNSD', 'TDIUC (position)', 'GQA', 'MP3D', 'RefCOCO (res)', 'TextVQA', 'IIIT5K', 'VizWiz (singleChoice)', 'OK-VQA', 'CLEVR', 'TDIUC (utility)', 'TDIUC (scene)', 'MOCHEG', 'VSR']"
ScandEval,"['SUC (v3)', 'ScaLA (sv)', 'Angry Tweets', 'SweRec', 'ScandiQA (no)', 'ScaLA (da)', 'DaNE', 'NorNE (nn)', 'NorNE (nb)', 'NoReC', 'ScandiQA (sv)', 'ScandiQA (da)', 'ScaLA (nb)', 'ScaLA (nn)']"
SCROLLS,"['SummScreen', 'QuALITY', 'ContractNLI', 'GovReport', 'Qasper', 'NarrativeQA', 'QMSum']"
SummEdits,"['SAMSum', 'TinyShakespeare', 'Sales Call', 'Google News', 'Sales Email', 'Spotify Podcast', 'SciTLDR', 'ECTSum', 'Billsum', 'QMSum']"
SuperCLUE,"['CArena', 'OPEN Set', 'CLOSE Set']"
SuperGLUE,"['WiC', 'CB', 'RTE', 'MultiRC', 'WSC', 'Broad-Coverage Diagnostic', 'COPA', 'Winogender Diagnostic', 'ReCoRD', 'BoolQ']"
SuperLim (v2),"['Swedish ABSAbank-Imm (v1.1)', 'SweDN', 'STS-B (sv) (v2)', 'Winograd (sv) (v2)', 'SweWiC (v2)', 'MNLI (sv)', 'DaLAJ-GED-SuperLim (v2)', 'SweSAT Synonyms (v1.1)', 'Argumentation sentences', 'SuperSim (v2)', 'Swedish analogy (v2)', 'SweFAQ (v2)', 'Winogender Diagnostic (sv) (v2)', 'GLUE Diagnostic (sv)']"
Video-Bench Leaderboard,"['DLE', 'MOT', 'YouCook2', 'ActivityNet-QA', 'MSVD-QA', 'MSRVTT-QA', 'NBAQA', 'SQA3D', 'TGIF-QA', 'UCF-Crime', 'MVQA', 'DDM', 'TVQA']"
VLM-Eval,"['HMDB51', 'UCF101', 'Kinetics-400', 'TGIF', 'MSRVTT', 'MSVD', 'ActivityNet']"
