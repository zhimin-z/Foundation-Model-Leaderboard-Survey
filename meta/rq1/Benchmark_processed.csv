Leaderboard name,Benchmarks
AgentBench,"['ALFWorld', 'Mind2Web', 'WebShop']"
AlpacaEval (v2),"['OASST1', 'Koala', 'Vicuna', 'HH-RLHF', 'Self-Instruct']"
BBH,"['Adjective Order', 'Boolean Expressions', 'Sorting Words', 'Dyck Languages', 'Salient Translation Error Detection', 'Sports Understanding', 'Logical Deduction', 'Formal Fallacies and Syllogisms with Negation', 'Web of Lies', 'Reasoning about Colored Objects', 'Date Understanding', 'Tables of Penguins', 'MovieLens', 'Causal Judgment', 'Disambiguation QA', 'SNARKS', 'Sequences', 'Ruin a Name with One Edit', 'Multistep Arithmetic', 'Tracking Shuffled Objects', 'Geometric Shapes', 'Object Counting', 'Navigation']"
BEIR,"['MSMARCO', 'SciDocs', 'TREC-COVID', 'QQP', 'Signal-1M', 'Touche (2020)', 'SciFact', 'NFCorpus', 'Climate-FEVER', 'DBpedia', 'HotpotQA', 'TREC-News', 'CQADupStack (gis)', 'Robust04', 'ArguAna', 'NQ', 'BioASQ', 'FiQA (2018)', 'FEVER']"
Big Code Models Leaderboard,"['HumanEval', 'MultiPL-E']"
BIG-Bench,"['BBQ-Lite', 'Cause and Effect', 'Shakespeare Dialogue', 'Self-awareness', 'Misconceptions (ru)', 'Physics Multiple Choice', 'ePiC', 'BBQ-Lite (Json)', 'Alignment of Simplicity Priors for Turing (Complete Concept Learning)', 'Identify Math Theorems', 'Factuality', 'Cornell Movie-Dialogs Corpus', 'Unit Conversion', 'MathQA', 'Logic Grid Puzzles', 'Sorting Words', 'SIQA', 'Discovery', 'Dyck Languages', 'Salient Translation Error Detection', 'Simple arithmetic with multiple targets (JSON)', 'Estimating Risk of Suicide', 'Arithmetic', 'Phrase Relatedness', 'ASCII Word Recognition', 'Forecasting Subquestions', 'Linguistic Mappings', 'Entailed Polarity in Hindi', 'TellMeWhy', 'COM2SENSE', 'Reasoning about Colored Objects', 'Transforming German Sentences to Gender (Inclusive Forms)', 'Self Evaluation Courtroom', 'Emoji Movie', 'Root Finding, Optimization and Games', 'Human Organs and Senses', 'Medical Questions in Russian', 'CRT', 'Spelling Bee', 'Intersection Points', 'Topical-Chat', 'UnQover', 'RiddleSense', 'Word Problems on Sets and Graphs', 'CIFAR-10', 'COPA (qa)', 'characterRelations', 'Logical Arguments', 'Persian Idioms', 'CoQA', 'Ruin a Name with One Edit', 'SIT', 'NQ', 'Rhyming', 'Multistep Arithmetic', 'Metaphor Understanding', 'Twenty Questions', 'Irony Identification', 'Geometric Shapes', 'Minute Mysteries QA', 'Taboo', 'Periodic Elements', 'Boolean Expressions', 'Common Morpheme', 'Conceptual Combinations', 'Simple arithmetic with subtasks (JSON)', 'GRE Reading Comprehension', 'Metaphor Boolean', 'PARSINLU (qa)', 'Code Description', 'StrategyQA', 'Figure of Speech Detection', 'Sequential Order', 'Sufficient Information', 'Implicatures', 'Which Wiki Edit', 'Language Games', 'Logical Deduction', 'Operators', 'Sentence Ambiguity', 'Data Wrangling', 'Reordering', 'What is the Tao', 'Web of Lies', 'CS Algorithms', 'State Tracking in Chess', 'CoDA', 'Navigation', 'Kannada Riddles', 'MovieLens', 'Identify Odd Metaphor', 'Self Evaluation of Tutoring', 'Physics Questions', 'Diverse Metrics for Social Biases in Language Models', 'Presuppositions as NLI', 'Cycled Letters', 'Misconceptions', 'List Functions', 'TimeDial', 'ASCII MNIST', 'Strange Stories', 'English to Russian Proverbs', 'Fact-Checking', 'Automatic Debugging', 'Indic Cause and Effect', 'Natural Instructions', 'SQuADShifts', 'YesNoBlackWhite Game', 'KPWr', 'TruthfulQA', 'Cryobiology Spanish', 'Object Counting', 'RoFT', 'Text Navigation Game', 'High Low Guessing Game', 'Scientific Press Release', 'Simple multiple choice arithmetic (JSON)', 'Simple arithmetic', 'Swedish to German proverbs', 'Disfl-QA', 'Conlang Translation Problems', 'Unnatural In-Context Learning', 'Novel Concepts', 'Analogical Similarity', 'Hinglish Toxicity Prediction', 'Understanding Fables', 'VitaminC', 'Judging Moral Permissibility', 'Hindu Mythology Trivia', 'Date Understanding', 'Key Value Maps', 'Tables of Penguins', 'Dynamic Counting', 'Swahili-English Paremiologic Competence', 'Word Unscrambling', 'Identifying Anachronisms', 'HHH', 'Disambiguation QA', 'Mathematical Induction', 'WinoWhy', 'Crash Blossoms', 'Kanji ASCII Art', 'Dark Humor Detection', 'Authorship Verification', 'MultiEmo', 'Autoclassification', 'Linguistic Puzzles', 'Cryptonite', 'Muslim-Violence Bias', 'Understanding Grammar of Unseen Words', 'Sudoku', 'Social Support', 'Wikimedia', 'Known Unknowns', 'EmoTag1200', 'Gender Sensitivity Test (English)', 'Adjective Order', 'Hindi Question Answering', 'Empirical Judgments', 'Wino-X (German)', 'Long Input Contexts', 'Verb Tense', 'Similarities Test for Abstraction', 'SNLI', 'Gender Sensitivity Test (zh)', 'MNLI (transliteration)', 'Odd One Out', 'Keyword Sentence Transformation', 'Truthful QA', 'Intent Recognition', 'Sports Understanding', 'Matrix Shapes', 'LTI LangID Corpus', 'Formal Fallacies and Syllogisms with Negation', 'CRASS', 'Checkmate In One Move', 'General Knowledge', 'Modified Arithmetic', 'Entailed Polarity', 'Informal and Formal Fallacies', 'Simple Ethical Questions', 'Unit Interpretation', 'Causal Judgment', 'Analytic Entailment', 'Python Program Synthesis', 'Simple arithmetic (JSON)', 'Physical Intuition', 'Repeat Copy Logic', 'ARC (The Abstraction and Reasoning Corpus)', 'SNARKS', 'Training on the test set', 'ParsiNLU (rc)', 'TalkDown', 'The Essential, the Excessive, and the Extraneous', 'SGD', 'Sequences', 'Goal-Step Inference', 'English Proverbs', 'SParC', 'Fantasy Reasoning', 'Subject-Verb Agreement', 'Codenames', 'GEM', 'Tracking Shuffled Objects', 'Simple Text Editing', 'Protein Interaction Sites', 'ISNotes', 'SQuAD (pp)', 'Social Bias from Sentence Probability', 'Wikidata', 'MNLI (ipa)', 'Spider', 'Python Programming', 'Color']"
Chatbot Arena Leaderboard,"['Chatbot Arena Conversations', 'MT-Bench', 'MMLU']"
ChEF,"['Omnibenchmark', 'Flickr30K', 'MSCOCO', 'ScienceQA', 'MME', 'VOC (2012)', 'SEED-Bench', 'FSC147', 'CIFAR-10', 'MMBench']"
CLUE,"['DRCD', 'TNEWS', 'CMRC (2018)', 'CLUE Diagnostics', 'AFQMC', 'C3', 'iFLYTEK', 'WSC (CLUE)', 'CSL', 'ChID', 'CMNLI', 'OCNLI']"
CMB,"['CMB-Clin', 'CMB-Exam']"
CMTEB,"['CSL (s2s)', 'STS-B (zh)', 'DuReader (retrieval)', 'ATEC', 'OnlineShopping', 'mMARCO (reranking)', 'Waimai', 'Multi-CPR (video)', 'cCOVID-News', 'THUCNews (p2p)', 'T2Ranking (retrieval)', 'CMNLI', 'T2Ranking (reranking)', 'cMedQA (v2) (retrieval)', 'CSL (p2p)', 'TNEWS', 'AFQMC', 'PAWS-X', 'BQ', 'QBQTC', 'JDReview', 'mMARCO (retrieval)', 'LCQMC', 'Multi-CPR (medical)', 'cMedQA (v2) (reranking)', 'Multi-CPR (ecom)', 'THUCNews (s2s)', 'iFLYTEK', 'cMedQA (reranking)', 'OCNLI']"
Colossal-AI,"['C-Eval', 'CMMLU', 'GAOKAO-Bench', 'MMLU', 'AGIEval']"
CoQA,"['Project Gutenberg', 'RACE', 'Wikipedia', 'MCTest', 'SciQ', 'WritingPrompts', 'CNN DM']"
EQ-Bench Leaderboard,"['EQ-Bench (v2)', 'AGIEval', 'MMLU']"
EvalPlus,"['MBPP', 'HumanEval']"
Factuality Leaderboard,"['HumanEval', 'GSM8K', 'FactPrompts', 'Self-Instruct', 'RoSE']"
FewCLUE,"['TNEWS', 'CSLDCP', 'BUSTM', 'EPRSTMT', 'iFLYTEK', 'WSC (CLUE)', 'CSL', 'ChID', 'OCNLI']"
FlagEval,"['BoolQ', 'AISHELL-1', 'KeSpeech', 'BUSTM', 'C-SEM', 'EPRSTMT', 'iNaturalist (2018)', 'WSC (CLUE)', 'DTD', 'ChID', 'CLCC', 'Places', 'ADE20K', 'KITTI Eigen split', 'UCF101', 'CelebA-HQ', 'Stanford Cars', 'VQA (v2)', 'Flowers102', 'MSRVTT', 'SOP', 'ImageNet-1K', 'IMDB', 'TNEWS', 'MSCOCO', 'RAFT', 'LibriSpeech', 'NYU-Depth', 'TDIUC', 'COCO-Stuff', 'Cityscapes', 'Flickr30K', 'HumanEval', 'GAOKAO-Bench (2023)', 'CMMLU', 'FGVC-Aircraft', 'VQA-CP', 'TruthfulQA', 'IEMOCAP', 'CUB', 'MMLU', 'CSL', 'OCNLI', 'Food-101']"
GENIE,"['a-NLG', 'ARC-DA (2018)', 'WMT (2019) (de-en)', 'XSUM', 'WMT (2021) (de-en)']"
GPT4All,"['BoolQ', 'PIQA', 'HellaSwag', 'ARC', 'WinoGrande', 'OpenbookQA']"
HEIM,"['MSCOCO (fidelity)', 'PartiPrompts (knowledge categories)', 'MSCOCO (es)', 'MSCOCO (fairness - AAVE dialect)', 'DrawBench (reasoning categories)', 'DrawBench (knowledge categories)', 'MSCOCO (art styles)', 'MSCOCO (zh)', 'Historical Figures', 'Mental Disorders', 'I2P', 'Magazine Cover Photos', 'Dailydall.e', 'MSCOCO (hi)', 'PartiPrompts (reasoning categories)', 'MSCOCO (fairness - gender)', 'MSCOCO (base)', 'Landing Pages', 'MSCOCO', 'Logos', 'Demographic Stereotypes', 'Relational Understanding', 'MSCOCO (efficiency)', 'Winoground', 'PaintSkills', 'CUB', 'PartiPrompts (image quality categories)', 'CSP', 'DrawBench (image quality categories)', 'MSCOCO (robustness - typos)']"
HellaSwag Leaderboard,"['ActivityNet Captions', 'HellaSwag', 'WikiHow']"
HELM Classic,"['BoolQ', 'MSMARCO', 'BBQ', 'LegalSupport', 'NQ (open-book)', 'Synthetic efficiency', 'GSM8K', 'MATH (chain-of-thought)', 'bAbI', 'WMT (2014)', 'MultiLexSum', 'EntityMatching', 'CNN DM', 'Dyck Languages', 'Copyright (code)', 'LegalBench', 'The Pile', 'MedQA', 'MSMARCO (v2)', 'OpenbookQA', 'NarrativeQA', 'APPS', 'IMDB', 'Synthetic Reasoning (natural)', 'HellaSwag', 'LSAT', 'RAFT', 'TwitterAAE (white)', 'Civil Comments', 'Billsum', 'XSUM', 'BLiMP', 'ICE', 'QuAC', 'Copyright (text)', 'NQ (closed-book)', 'BOLD', 'WikiFact', 'HumanEval', 'DataImputation', 'EurLexSum', 'Disinformation (reiteration)', 'Disinformation (wedging)', 'TruthfulQA', 'TwitterAAE', 'Synthetic Reasoning (symbolic)', 'MMLU', 'RealToxicityPrompts', 'MATH', 'TwitterAAE (aa)', 'Numerical reasoning']"
HELM Lite,"['GSM8K', 'LegalBench', 'WMT (2014)', 'MedQA', 'MMLU', 'NQ (open-book)', 'MATH', 'NarrativeQA', 'NQ (closed-book)', 'OpenbookQA']"
HHEM Leaderboard,['CNN DM']
InstructEval,"['BBH', 'DROP', 'HumanEval', 'HHH', 'IMPACT', 'CRASS', 'MMLU']"
InterCode,"['MBPP', 'InterCode-CTF', 'SWE-bench', 'NL2Bash', 'Spider']"
KoLA,"['Encyclopedic', 'MAVEN-ERE', 'MAVEN', 'DocRED', 'COPEN (CPJ)', 'FewNERD', 'MuSiQue', 'KQA Pro', 'Low-Frequency Knowledge', 'ETC', 'ETA', '2WikiMQA', 'COPEN (CSJ)', 'HotpotQA', 'ETM', 'High-Frequency Knowledge', 'COPEN (CiC)', 'ETU', 'KoRC']"
L-Eval,"['SFiction', 'Qasper', 'GSM8K', 'SummScreen', 'BigPatent', 'Multi-News', 'QuALITY', 'OPENREVIEW', 'MultiDoc2Dial', 'CUAD', 'NarrativeQA', 'SPACE', 'Coursera', 'QMSum', 'LONGFQA', 'NQ', 'TOEFL-QA', 'TopicRet', 'CodeU', 'GovReport']"
LAiW Leaderboard,"['CrimeKgAssitant', 'CJRC', 'Criminal-S', 'JEC-QA', 'CFM', 'MSJudge', 'CAIL (2018)', 'CAIL (2020)', 'MLMN', 'CAIL (2021)', 'CAIL (2019)', 'AC-NLG']"
LLM Benchmarker Suite,"['BoolQ', 'HellaSwag', 'HumanEval', 'NQ', 'WinoGrande', 'GSM8K', 'OpenbookQA', 'MMLU', 'TriviaQA', 'AGIEval', 'QuAC']"
LLM-Leaderboard,"['Chatbot Arena Conversations', 'HellaSwag', 'HumanEval', 'WinoGrande', 'MMLU', 'TriviaQA', 'LAMBADA']"
LongBench,"['Qasper', 'MultiFieldQA (zh)', 'VCSUM', 'MuSiQue', 'SAMSum', 'Multi-News', 'MultiFieldQA', 'PassageRetrieval', 'PassageRetrieval (zh)', 'NarrativeQA', '2WikiMQA', 'HotpotQA', 'LSHTC', 'PassageCount', 'LCC', 'QMSum', 'DuReader', 'RepoBench-P', 'GovReport', 'TriviaQA', 'TREC']"
LVLM-eHub,"['SVTP', 'Whoops', 'VizWiz', 'SNLI-VE', 'FUNSD', 'MSCOCO (random)', 'IC (2013)', 'MSCOCO (OC)', 'IIIT5K', 'Minecraft', 'IconQA', 'VSR', 'HOST', 'SROIE', 'Virtual Home', 'MSCOCO (adversarial)', 'NoCaps', 'Flowers102', 'OK-VQA', 'Franka Kitchen', 'ImageNet-1K', 'COCO-Text', 'OCR-VQA', 'VCR-OC', 'CUTE80', 'SVT', 'MSCOCO (popular)', 'WOST', 'IC (2015)', 'ScienceQA IMG', 'CIFAR-10', 'DocVQA', 'Meta-World', 'STVQA', 'Flickr30K', 'VCR', 'CTW', 'Total-Text', 'VisDial', 'MSCOCO (MCI)', 'ImageNetVC', 'Pets37', 'GQA', 'TextVQA', 'VCR-MCI', 'WordArt']"
MedBench,"['SafetyBench', 'DDx-advanced', 'MedMC', 'Med-Exam', 'CHIP-CTC', 'CHIP-CDN', 'DBMHG', 'MedDG', 'CMeIE', 'SMDoc', 'MedHC', 'CMB-Clin', 'CMeEE', 'MedTreat', 'DrugCA', 'IMCS-MRG (v2)', 'MedSpeQA', 'DDx-basic', 'CHIP-CDEE', 'MedHG']"
MMBench,"['PISC', 'CLEVR', 'KonIQ-10k', 'VSR', 'W3C School', 'LLaVA-Bench', 'COCO Captions', 'ScienceQA', 'Internet', 'ARAS', 'TextVQA', 'Places']"
MMLU-by-task Leaderboard,"['HellaSwag', 'ARC', 'MMLU']"
MOCHA,"['SIQA', 'DROP', 'CosmosQA', 'Quoref', 'NarrativeQA', 'MCScript']"
MTEB,"['STS (2022)', 'DanishPoliticalComments', 'CQADupStack (physics)', 'LanguageNet', 'STS (2015)', 'Reddit', 'Allegro Reviews', 'Bornholmsk', 'Touche (2020)', 'Nordic Language Identification', 'ScaLA (da)', 'DBpedia', 'STS (2016)', 'NoReC', 'STS (2013)', 'CQADupStack (gis)', 'STS (2012)', 'ArguAna', 'NQ', 'CQADupStack (english)', 'CQADupStack (android)', 'FEVER', 'Angry Tweets', 'MSMARCO', 'SICK (relatedness) (pl)', 'MASSIVE (intent)', 'SICK (pl)', 'arXiv (p2p)', 'STS (2014)', 'TREC-COVID', 'MTOP (intent)', 'CBD', 'NFCorpus (pl)', 'SciDocs (rr)', 'ScaLA (nb)', 'MARC', 'Climate-FEVER', 'NFCorpus', 'DaLAJ', 'CQADupStack (mathematica)', '10kGNAD (s2s)', 'LCC', 'PolEmo (v2) (in)', 'SprintDuplicateQuestions', 'AMCD', 'NQ (pl)', 'BIOSSES', 'SweRec', 'NPSC', 'Blurbs (p2p)', 'AskUbuntu', 'PPC', 'Stack Exchange (p2p)', 'SciDocs (pl)', 'SciDocs', 'Reddit (p2p)', '20 Newsgroups', 'PAC', 'ScaLA (sv)', 'PSC', 'CQADupStack (gaming)', 'ArguAna (pl)', '10kGNAD (p2p)', 'QQP (pl)', 'Polish CDSCorpus (relatedness)', 'medRxiv (s2s)', 'B77', 'BUCC', 'arXiv (s2s)', 'HotpotQA', 'SciFact (pl)', 'Civil Comments (tc)', 'Tatoeba', 'MTOP (domain)', 'bioRxiv (p2p)', 'CQADupStack (stats)', 'CQADupStack (unix)', 'SweFAQ (v2)', 'Blurbs (s2s)', 'FiQA (2018)', 'STS-B', 'CQADupStack (tex)', 'bioRxiv (s2s)', '8TAGS', 'CQADupStack (webmasters)', 'CQADupStack (programmers)', 'MASSIVE (scenario)', 'SICK (relatedness)', 'LinkSO', 'PolEmo (v2) (out)', 'DKhate', 'SummEval', 'QQP', 'TwitterSemEval (2015)', 'Polish CDSCorpus', 'ScaLA (nn)', 'CARER', 'SciFact', 'MSMARCO (v2)', 'Tweet Sentiment Extraction', 'STS (2017)', 'IMDB', 'FiQA (2018) (pl)', 'medRxiv (p2p)', 'HotpotQA (pl)', 'MIND', 'CQADupStack (wordpress)', 'Amazon Polarity', 'DBpedia (pl)', 'MSMARCO (pl)', 'Stack Exchange']"
Multi-modal Model Leaderboard,"['MMMU (val)', 'CCBench', 'AI2D (test)', 'LLaVA-Bench (in-the-wild)', 'SEED-Bench (img)', 'MMBench (test) (zh)', 'MMBench (test)', 'MathVista (minitest)', 'MME (normalized)', 'MM-Vet', 'HallusionBench']"
MVBench,"['CLEVRER', 'FunQA', 'TVQA', 'Perception Test', 'NTU RGB+D', 'MiT', 'STAR', 'Charades-STA', 'MovieNet', 'PAXION', 'VLN-CE']"
OCRBench,"['SVTP', 'Semantic Text', 'FUNSD', 'ESTVQA (en)', 'ReCTS', 'IIIT5K', 'POIE', 'ESTVQA (zh)', 'HOST', 'ORAND-CAR-2014', 'SROIE', 'CT80', 'InfographicVQA', 'ChartQA (Aug.)', 'IC15', 'COCO-Text', 'OCR-VQA', 'SVT', 'WOST', 'SCUT-CTW1500', 'DocVQA', 'ChartQA (Hum.)', 'STVQA', 'IAM', 'Total-Text', 'IC13', 'Non-Semantic Text', 'HME100K', 'TextVQA', 'WordArt']"
Open Ko-LLM Leaderboard,"['MMLU (ko)', 'CommonGen', 'ARC (ko)', 'TruthfulQA (ko)', 'HellaSwag (ko)']"
Open LLM Leaderboard,"['HellaSwag', 'ARC', 'WinoGrande', 'GSM8K', 'TruthfulQA', 'MMLU']"
Open Multilingual LLM Evaluation Leaderboard,"['HellaSwag', 'TruthfulQA', 'ARC', 'MMLU']"
OpenCompass LLM Leaderboard (v2),"['BoolQ', 'DROP', 'EPRSTMT', 'CSQA', 'C3', 'GSM8K', 'GAOKAO-Bench', 'ARC (challenge)', 'DS-1000', 'ChID', 'HumanEval-X', 'SIQA', 'ARC (easy)', 'RACE (middle)', 'L-Eval', 'C-Eval', 'TyDiQA', 'ReCoRD', 'Broad-Coverage Diagnostic', 'CMNLI', 'LAMBADA', 'OpenbookQA', 'CIBench', 'OCNLI', 'HellaSwag', 'AFQMC', 'LCSTS', 'CreationBench', 'COPA', 'CSL', 'Flores-101', 'XSUM', 'MathBench', 'RACE (high)', 'LongBench', 'BBH', 'MBPP', 'PIQA', 'HumanEval', 'NQ', 'OCRBench', 'Winogender Diagnostic', 'CMMLU', 'WiC', 'WSC', 'MMLU', 'TriviaQA', 'MATH', 'RTE', 'T-Eval', 'AGIEval']"
OpenEval (text),"['SQuAD (zh)', 'Power-seeking', 'SNLI (zh)', 'C3', 'Self-awareness', 'SWSR', 'GAOKAO-Bench', 'WSC (CLUE)', 'COLD', 'CBBQ', 'ChID', 'OL-CC', 'M3KE', 'CMNLI', 'TOCP', 'Guilt Law', 'CommonMT', 'TUMCC', 'One-box Tendency', 'Corrigible', 'CAIL (2018)', 'WGlaw', 'CORGI-PM', 'WPLC', 'CooridinateAI', 'BiPaR', 'CDIAL-BIAS', 'CMMLU', 'Myopia Reward', 'TGEA']"
PromptBench,"['SQuAD (v2)', 'Boolean Expressions', 'CSQA', 'CoLA', 'GSM8K', 'MultiUN', 'QQP', 'Valid Parantheses', 'MNLI', 'WNLI', 'Date Understanding', 'NumerSense', 'QNLI', 'SST-2', 'LastLetterConcat', 'Tracking Shuffled Objects', 'IWSLT (2017)', 'MMLU', 'QASC', 'MATH', 'RTE', 'MRPC']"
Q-Bench,"['LLDescribe', 'CGIQA-6K', 'SPAQ', 'KonIQ-10k', 'AGIQA-3K', 'LIVE-FB LSVQ', 'LLVisionQA', 'KADID-10K', 'LIVE-itw']"
RAFT,"['ToS', 'Over', 'SRI', 'TweetEval (hate)', 'OSE', 'SOT', 'TAI', 'TC', 'B77', 'ADE Corpus (v2)', 'NIS']"
ReCoRD,"['CNN DM', 'Internet Archive']"
Red-Eval,"['HarmfulQA', 'DangerousQA']"
ReForm-Eval,"['Whoops', 'MSCOCO (GOI)', 'SNLI-VE', 'ScienceQA', 'FUNSD', 'MSCOCO (OC)', 'TextOCR (Grounded)', 'IIIT5K', 'POIE', 'TDIUC (detection)', 'TDIUC (sport)', 'VSR', 'CLEVR', 'SROIE', 'A-OKVQA', 'NoCaps', 'Flowers102', 'VQA (v2)', 'RefCOCO (res)', 'OK-VQA', 'VizWiz (singleChoice)', 'WikiHow', 'IC (2015) (Grounded)', 'ViQuAE', 'TDIUC (position)', 'ImageNet-1K', 'COCO-Text', 'TDIUC (utility)', 'OCR-VQA', 'TextCaps', 'MOCHEG', 'CUTE80', 'MEDIC (dts)', 'COCO-Text (Grounded)', 'VizWiz (yesno)', 'IC (2015)', 'MSCOCO (MOS)', 'MP3D', 'CIFAR-10', 'DocVQA', 'Winoground', 'TDIUC (counting)', 'Flickr30K', 'TDIUC (scene)', 'TextOCR', 'VisDial', 'MSCOCO (MCI)', 'ImageNetVC', 'Pets37', 'GQA', 'TextVQA', 'TDIUC (color)', 'WordArt']"
ScandEval,"['Angry Tweets', 'DaNE', 'ScaLA (nn)', 'ScandiQA (sv)', 'SweRec', 'SUC (v3)', 'NoReC', 'ScandiQA (no)', 'NorNE (nn)', 'ScaLA (da)', 'ScandiQA (da)', 'ScaLA (sv)', 'NorNE (nb)', 'ScaLA (nb)']"
SCROLLS,"['Qasper', 'QuALITY', 'SummScreen', 'GovReport', 'ContractNLI', 'QMSum', 'NarrativeQA']"
SST,"['SST-2', 'SST-5']"
SummEdits,"['Google News', 'ECTSum', 'Sales Call', 'Sales Email', 'Spotify Podcast', 'Billsum', 'TinyShakespeare', 'QMSum', 'SciTLDR', 'SAMSum']"
SuperCLUE,"['OPEN Set', 'CLOSE Set', 'CArena']"
SuperGLUE,"['BoolQ', 'CB', 'COPA', 'Winogender Diagnostic', 'Broad-Coverage Diagnostic', 'WiC', 'WSC', 'ReCoRD', 'MultiRC', 'RTE']"
SuperLim (v2),"['SweDN', 'SweFAQ (v2)', 'Winogender Diagnostic (sv) (v2)', 'Winograd (sv) (v2)', 'Swedish ABSAbank-Imm (v1.1)', 'GLUE Diagnostic (sv)', 'MNLI (sv)', 'Swedish analogy (v2)', 'STS-B (sv) (v2)', 'Argumentation sentences', 'SuperSim (v2)', 'SweSAT Synonyms (v1.1)', 'DaLAJ-GED-SuperLim (v2)', 'SweWiC (v2)']"
TrustLLM,"['SQuAD (v2)', 'CODAH', 'Privacy Awareness', 'AdvInstruction', 'AdvGLUE', 'ConfAIde', 'Do-Not-Answer', 'COVID-Fact', 'ToolE', 'XSTest', 'ETHICS', 'SciFact', 'StereoSet', 'HealthVer', 'Adult', 'Climate-FEVER', 'HotpotQA', 'DDXPlus', 'Social Chemistry 101', 'MoralChoice', 'AdversarialQA', 'WinoBias', 'Opinion pairs', 'LM-exp-sycophancy', 'TruthfulQA', 'Flipkart', 'Jailbraek Trigger', 'Enron Email', 'Misuse (additional)']"
UHGEval,['XinhuaHallucinations']
Video-Bench,"['ActivityNet-QA', 'DLE', 'TVQA', 'UCF-Crime', 'MVQA', 'MOT', 'TGIF-QA', 'NBAQA', 'DDM', 'YouCook2', 'SQA3D', 'MSRVTT-QA', 'MSVD-QA']"
ViP-Bench,"['PointQA (LookTwice)', 'VCR', 'Flickr30K', 'RefCOCOg', 'Visual Genome', 'Visual7W']"
VLM-Eval,"['ActivityNet', 'Kinetics-400', 'UCF101', 'TGIF', 'MSVD', 'MSRVTT', 'HMDB51']"
YALL,"['GPT4All', 'TruthfulQA', 'AGIEval', 'BIG-Bench']"
