Leaderboard,Benchmark
AgentBench,"['WebShop', 'Mind2Web', 'ALFWorld']"
AlpacaEval (v2),"['HH-RLHF', 'Vicuna', 'OASST1', 'Koala', 'Self-Instruct']"
BBH,"['Disambiguation QA', 'Dyck Languages', 'Ruin a Name with One Edit', 'Salient Translation Error Detection', 'Adjective Order', 'Formal Fallacies and Syllogisms with Negation', 'Logical Deduction', 'Navigation', 'Multistep Arithmetic', 'Date Understanding', 'MovieLens', 'Object Counting', 'Reasoning about Colored Objects', 'Sports Understanding', 'Tables of Penguins', 'Causal Judgment', 'Geometric Shapes', 'Tracking Shuffled Objects', 'Web of Lies', 'Sorting Words', 'Sequences', 'SNARKS', 'Boolean Expressions']"
BEIR,"['Robust04', 'MSMARCO', 'HotpotQA', 'QQP', 'FEVER', 'SciDocs', 'Touche (2020)', 'Signal-1M', 'CQADupStack (gis)', 'Climate-FEVER', 'NQ', 'ArguAna', 'FiQA (2018)', 'TREC-News', 'SciFact', 'TREC-COVID', 'BioASQ', 'NFCorpus', 'DBpedia']"
Big Code Models Leaderboard,"['MultiPL-E', 'HumanEval']"
BIG-Bench,"['Presuppositions as NLI', 'SNLI', 'Unnatural In-Context Learning', 'Modified Arithmetic', 'Diverse Metrics for Social Biases in Language Models', 'Logic Grid Puzzles', 'SParC', 'Formal Fallacies and Syllogisms with Negation', 'Spelling Bee', 'Authorship Verification', 'MNLI (transliteration)', 'Conlang Translation Problems', 'Logical Deduction', 'Verb Tense', 'Physics Multiple Choice', 'Simple arithmetic (JSON)', 'Natural Instructions', 'Rhyming', 'Object Counting', 'Self-awareness', 'Estimating Risk of Suicide', 'Matrix Shapes', 'Color', 'characterRelations', 'Identifying Anachronisms', 'ARC (The Abstraction and Reasoning Corpus)', 'ePiC', 'Sufficient Information', 'Wino-X (German)', 'Web of Lies', 'Simple multiple choice arithmetic (JSON)', 'Implicatures', 'Protein Interaction Sites', 'CRT', 'Human Organs and Senses', 'Goal-Step Inference', 'Self Evaluation of Tutoring', 'Novel Concepts', 'MultiEmo', 'Entailed Polarity in Hindi', 'Sorting Words', 'Cornell Movie-Dialogs Corpus', 'Identify Odd Metaphor', 'CRASS', 'PARSINLU (qa)', 'Hindi Question Answering', 'Simple arithmetic with subtasks (JSON)', 'CoDA', 'Social Bias from Sentence Probability', 'Unit Conversion', 'Emoji Movie', 'CoQA', 'Dyck Languages', 'Medical Questions in Russian', 'Python Programming', 'Fantasy Reasoning', 'SQuADShifts', 'Understanding Fables', 'Arithmetic', 'Salient Translation Error Detection', 'Adjective Order', 'Indic Cause and Effect', 'WinoWhy', 'LTI LangID Corpus', 'Simple Text Editing', 'Metaphor Understanding', 'Odd One Out', 'Empirical Judgments', 'Date Understanding', 'Intersection Points', 'ParsiNLU (rc)', 'MovieLens', 'SIQA', 'CS Algorithms', 'SGD', 'Reasoning about Colored Objects', 'StrategyQA', 'Intent Recognition', 'Swahili-English Paremiologic Competence', 'Discovery', 'GEM', 'Shakespeare Dialogue', 'MathQA', 'Sports Understanding', 'Phrase Relatedness', 'Checkmate In One Move', 'Informal and Formal Fallacies', 'Periodic Elements', 'Logical Arguments', 'EmoTag1200', 'Scientific Press Release', 'Sequential Order', 'Python Program Synthesis', 'COPA (qa)', 'Social Support', 'Similarities Test for Abstraction', 'YesNoBlackWhite Game', 'Cause and Effect', 'Self Evaluation Courtroom', 'Metaphor Boolean', 'Cryobiology Spanish', 'Training on the test set', 'Gender Sensitivity Test (zh)', 'Geometric Shapes', 'Disfl-QA', 'Conceptual Combinations', 'Disambiguation QA', 'Sudoku', 'Keyword Sentence Transformation', 'Muslim-Violence Bias', 'Ruin a Name with One Edit', 'Key Value Maps', 'Sentence Ambiguity', 'GRE Reading Comprehension', 'Language Games', 'Word Problems on Sets and Graphs', 'BBQ-Lite', 'Kanji ASCII Art', 'Fact-Checking', 'Codenames', 'Mathematical Induction', 'Multistep Arithmetic', 'Text Navigation Game', 'SQuAD (pp)', 'Linguistic Puzzles', 'Forecasting Subquestions', 'Simple arithmetic', 'Irony Identification', 'Simple arithmetic with multiple targets (JSON)', 'MNLI (ipa)', 'Spider', 'RoFT', 'Cycled Letters', 'NQ', 'Minute Mysteries QA', 'KPWr', 'COM2SENSE', 'RiddleSense', 'HHH', 'Wikidata', 'Root Finding, Optimization and Games', 'Which Wiki Edit', 'Sequences', 'Physics Questions', 'SNARKS', 'The Essential, the Excessive, and the Extraneous', 'Long Input Contexts', 'Boolean Expressions', 'Hinglish Toxicity Prediction', 'Transforming German Sentences to Gender (Inclusive Forms)', 'High Low Guessing Game', 'Factuality', 'Common Morpheme', 'Dark Humor Detection', 'Persian Idioms', 'Reordering', 'Linguistic Mappings', 'Topical-Chat', 'Subject-Verb Agreement', 'Taboo', 'TimeDial', 'List Functions', 'BBQ-Lite (Json)', 'Analytic Entailment', 'English Proverbs', 'Judging Moral Permissibility', 'ASCII MNIST', 'Misconceptions (ru)', 'Navigation', 'Repeat Copy Logic', 'Swedish to German proverbs', 'Twenty Questions', 'Cryptonite', 'Unit Interpretation', 'UnQover', 'Kannada Riddles', 'Dynamic Counting', 'Wikimedia', 'Simple Ethical Questions', 'State Tracking in Chess', 'CIFAR-10', 'General Knowledge', 'Known Unknowns', 'Alignment of Simplicity Priors for Turing (Complete Concept Learning)', 'What is the Tao', 'Tables of Penguins', 'Word Unscrambling', 'Causal Judgment', 'Gender Sensitivity Test (English)', 'ASCII Word Recognition', 'TalkDown', 'Tracking Shuffled Objects', 'VitaminC', 'Crash Blossoms', 'Identify Math Theorems', 'ISNotes', 'Physical Intuition', 'Entailed Polarity', 'TruthfulQA', 'Figure of Speech Detection', 'Data Wrangling', 'Misconceptions', 'Code Description', 'Understanding Grammar of Unseen Words', 'TellMeWhy', 'Operators', 'English to Russian Proverbs', 'Strange Stories', 'SIT', 'Automatic Debugging', 'Truthful QA', 'Analogical Similarity', 'Hindu Mythology Trivia', 'Autoclassification']"
Chatbot Arena Leaderboard,"['MMLU', 'MT-Bench', 'Chatbot Arena Conversations']"
ChEF,"['MME', 'SEED-Bench', 'MSCOCO', 'VOC (2012)', 'Flickr30K', 'MMBench', 'CIFAR-10', 'FSC147', 'Omnibenchmark', 'ScienceQA']"
CLUE,"['OCNLI', 'CMRC (2018)', 'C3', 'DRCD', 'ChID', 'CLUE Diagnostics', 'iFLYTEK', 'TNEWS', 'CSL', 'CMNLI', 'AFQMC', 'WSC (CLUE)']"
CMB,"['CMB-Clin', 'CMB-Exam']"
CMTEB,"['OnlineShopping', 'cCOVID-News', 'OCNLI', 'Multi-CPR (video)', 'iFLYTEK', 'Waimai', 'STS-B (zh)', 'CSL (s2s)', 'Multi-CPR (medical)', 'LCQMC', 'BQ', 'mMARCO (retrieval)', 'THUCNews (s2s)', 'JDReview', 'T2Ranking (retrieval)', 'DuReader (retrieval)', 'THUCNews (p2p)', 'cMedQA (v2) (retrieval)', 'T2Ranking (reranking)', 'PAWS-X', 'QBQTC', 'cMedQA (v2) (reranking)', 'Multi-CPR (ecom)', 'CSL (p2p)', 'mMARCO (reranking)', 'TNEWS', 'ATEC', 'cMedQA (reranking)', 'CMNLI', 'AFQMC']"
Coding LLMs Leaderboard,['CCEval']
Colossal-AI,"['GAOKAO-Bench', 'AGIEval', 'MMLU', 'C-Eval', 'CMMLU']"
CoQA,"['CNN DM', 'SciQ', 'Wikipedia', 'MCTest', 'RACE', 'WritingPrompts', 'Project Gutenberg']"
EvalPlus,"['MBPP', 'HumanEval']"
FacTool,"['RoSE', 'GSM8K', 'HumanEval', 'FactPrompts', 'Self-Instruct']"
FewCLUE,"['OCNLI', 'BUSTM', 'ChID', 'iFLYTEK', 'TNEWS', 'CSL', 'EPRSTMT', 'CSLDCP', 'WSC (CLUE)']"
FlagEval,"['COCO-Stuff', 'OCNLI', 'CUB', 'KeSpeech', 'MSRVTT', 'ADE20K', 'Flickr30K', 'UCF101', 'Places', 'AISHELL-1', 'SOP', 'IEMOCAP', 'IMDB', 'WSC (CLUE)', 'C-SEM', 'CLCC', 'Flowers102', 'BUSTM', 'TDIUC', 'ChID', 'Stanford Cars', 'Food-101', 'Cityscapes', 'LibriSpeech', 'RAFT', 'EPRSTMT', 'ImageNet-1K', 'DTD', 'NYU-Depth', 'iNaturalist (2018)', 'MSCOCO', 'GAOKAO-Bench (2023)', 'MMLU', 'CSL', 'TruthfulQA', 'CelebA-HQ', 'VQA-CP', 'KITTI Eigen split', 'HumanEval', 'FGVC-Aircraft', 'TNEWS', 'CMMLU', 'VQA (v2)', 'BoolQ']"
GENIE,"['WMT (2021) (de-en)', 'XSUM', 'a-NLG', 'WMT (2019) (de-en)', 'ARC-DA (2018)']"
GPT4All,"['PIQA', 'ARC', 'WinoGrande', 'OpenbookQA', 'HellaSwag', 'BoolQ']"
HEIM,"['CUB', 'Relational Understanding', 'Historical Figures', 'PartiPrompts (image quality categories)', 'MSCOCO (base)', 'DrawBench (image quality categories)', 'MSCOCO (fidelity)', 'MSCOCO (robustness - typos)', 'Dailydall.e', 'Mental Disorders', 'I2P', 'Winoground', 'MSCOCO (es)', 'Logos', 'MSCOCO (fairness - AAVE dialect)', 'MSCOCO (fairness - gender)', 'MSCOCO', 'Landing Pages', 'CSP', 'MSCOCO (art styles)', 'DrawBench (reasoning categories)', 'MSCOCO (zh)', 'Demographic Stereotypes', 'MSCOCO (efficiency)', 'MSCOCO (hi)', 'PaintSkills', 'PartiPrompts (reasoning categories)', 'PartiPrompts (knowledge categories)', 'Magazine Cover Photos', 'DrawBench (knowledge categories)']"
HellaSwag Leaderboard,"['ActivityNet Captions', 'HellaSwag', 'WikiHow']"
HELM Classic,"['LSAT', 'Dyck Languages', 'MATH', 'OpenbookQA', 'MSMARCO', 'XSUM', 'BLiMP', 'NQ (closed-book)', 'GSM8K', 'BBQ', 'ICE', 'DataImputation', 'TwitterAAE (aa)', 'NarrativeQA', 'NQ (open-book)', 'IMDB', 'LegalBench', 'Copyright (text)', 'Disinformation (wedging)', 'Civil Comments', 'RAFT', 'MSMARCO (v2)', 'Synthetic Reasoning (symbolic)', 'MultiLexSum', 'MedQA', 'Synthetic Reasoning (natural)', 'CNN DM', 'The Pile', 'TwitterAAE', 'WMT (2014)', 'Synthetic efficiency', 'HellaSwag', 'APPS', 'Disinformation (reiteration)', 'MMLU', 'QuAC', 'bAbI', 'EntityMatching', 'Numerical reasoning', 'TruthfulQA', 'LegalSupport', 'BOLD', 'Billsum', 'RealToxicityPrompts', 'HumanEval', 'MATH (chain-of-thought)', 'WikiFact', 'Copyright (code)', 'EurLexSum', 'BoolQ', 'TwitterAAE (white)']"
HELM Lite,"['MATH', 'OpenbookQA', 'GSM8K', 'LegalBench', 'WMT (2014)', 'NQ (closed-book)', 'NarrativeQA', 'MMLU', 'NQ (open-book)', 'MedQA']"
HHEM Leaderboard,['CNN DM']
InstructEval,"['HHH', 'HumanEval', 'CRASS', 'DROP', 'MMLU', 'BBH', 'IMPACT']"
InterCode,"['InterCode-CTF', 'Spider', 'SWE-bench', 'NL2Bash', 'MBPP']"
KoLA,"['DocRED', 'ETM', 'COPEN (CPJ)', 'MuSiQue', 'HotpotQA', 'MAVEN-ERE', 'KoRC', 'MAVEN', 'ETA', 'KQA Pro', 'FewNERD', 'ETU', 'High-Frequency Knowledge', 'COPEN (CSJ)', 'Encyclopedic', 'COPEN (CiC)', '2WikiMQA', 'Low-Frequency Knowledge', 'ETC']"
L-Eval,"['LONGFQA', 'GSM8K', 'SummScreen', 'SPACE', 'NarrativeQA', 'Coursera', 'TOEFL-QA', 'QMSum', 'MultiDoc2Dial', 'CodeU', 'Multi-News', 'Qasper', 'QuALITY', 'NQ', 'CUAD', 'SFiction', 'GovReport', 'BigPatent', 'OPENREVIEW', 'TopicRet']"
LAiW Leaderboard,"['CrimeKgAssitant', 'CJRC', 'CFM', 'MSJudge', 'CAIL (2021)', 'JEC-QA', 'CAIL (2019)', 'AC-NLG', 'Criminal-S', 'MLMN', 'CAIL (2018)', 'CAIL (2020)']"
LLM Benchmarker Suite,"['WinoGrande', 'TriviaQA', 'GSM8K', 'HumanEval', 'OpenbookQA', 'HellaSwag', 'AGIEval', 'MMLU', 'QuAC', 'BoolQ', 'NQ']"
LLM-Leaderboard,"['WinoGrande', 'TriviaQA', 'HumanEval', 'HellaSwag', 'MMLU', 'LAMBADA', 'Chatbot Arena Conversations']"
LongBench,"['MuSiQue', 'HotpotQA', 'VCSUM', 'NarrativeQA', 'QMSum', 'PassageRetrieval', 'TriviaQA', 'PassageCount', 'MultiFieldQA', 'RepoBench-P', 'MultiFieldQA (zh)', 'Multi-News', 'TREC', 'PassageRetrieval (zh)', 'Qasper', 'SAMSum', 'DuReader', 'LSHTC', 'LCC', '2WikiMQA', 'GovReport']"
LVLM-eHub,"['HOST', 'Meta-World', 'CUTE80', 'WOST', 'IC (2015)', 'GQA', 'Flickr30K', 'VCR-MCI', 'IC (2013)', 'ImageNetVC', 'SVTP', 'Total-Text', 'Flowers102', 'MSCOCO (MCI)', 'ScienceQA IMG', 'MSCOCO (random)', 'SVT', 'VCR', 'CIFAR-10', 'SROIE', 'VCR-OC', 'TextVQA', 'VisDial', 'DocVQA', 'Virtual Home', 'WordArt', 'MSCOCO (OC)', 'IIIT5K', 'VSR', 'MSCOCO (popular)', 'MSCOCO (adversarial)', 'COCO-Text', 'SNLI-VE', 'Pets37', 'IconQA', 'VizWiz', 'FUNSD', 'NoCaps', 'Whoops', 'OK-VQA', 'Franka Kitchen', 'STVQA', 'OCR-VQA', 'ImageNet-1K', 'Minecraft', 'CTW']"
MedBench,"['MedSpeQA', 'DDx-basic', 'CMeIE', 'DrugCA', 'DDx-advanced', 'MedHG', 'CMeEE', 'IMCS-MRG (v2)', 'MedTreat', 'CHIP-CTC', 'MedMC', 'MedDG', 'SMDoc', 'CHIP-CDN', 'CHIP-CDEE', 'SafetyBench', 'DBMHG', 'Med-Exam', 'CMB-Clin', 'MedHC']"
MMBench,"['CLEVR', 'LLaVA-Bench', 'COCO Captions', 'ARAS', 'Internet', 'Places', 'VSR', 'PISC', 'ScienceQA', 'KonIQ-10k', 'TextVQA', 'W3C School']"
MMLU-by-task Leaderboard,"['HellaSwag', 'ARC', 'MMLU']"
MOCHA,"['CosmosQA', 'SIQA', 'DROP', 'MCScript', 'NarrativeQA', 'Quoref']"
MTEB,"['AskUbuntu', 'CQADupStack (unix)', 'QQP (pl)', 'ScaLA (da)', 'SweFAQ (v2)', 'Stack Exchange (p2p)', 'Blurbs (s2s)', 'bioRxiv (s2s)', '20 Newsgroups', 'Tatoeba', 'PSC', 'STS (2017)', 'SICK (relatedness) (pl)', 'PolEmo (v2) (out)', 'Blurbs (p2p)', 'DBpedia (pl)', 'bioRxiv (p2p)', 'STS (2016)', 'SciFact', 'STS (2022)', 'Allegro Reviews', 'CQADupStack (mathematica)', 'MARC', 'SprintDuplicateQuestions', 'NFCorpus', 'HotpotQA (pl)', 'CQADupStack (webmasters)', 'Bornholmsk', 'arXiv (s2s)', 'PPC', 'MIND', 'MASSIVE (scenario)', 'SciDocs', 'Touche (2020)', 'SciDocs (pl)', 'MSMARCO (pl)', 'ScaLA (nn)', 'STS-B', 'ArguAna (pl)', 'SciDocs (rr)', 'NoReC', 'PAC', 'ArguAna', 'LCC', 'FiQA (2018)', 'AMCD', 'TREC-COVID', 'Nordic Language Identification', '10kGNAD (p2p)', 'ScaLA (sv)', 'Polish CDSCorpus', 'CBD', 'arXiv (p2p)', 'BIOSSES', 'MASSIVE (intent)', 'SICK (pl)', 'IMDB', 'DKhate', 'FEVER', 'NQ (pl)', '10kGNAD (s2s)', 'STS (2015)', 'MSMARCO (v2)', 'FiQA (2018) (pl)', 'STS (2013)', '8TAGS', 'MTOP (intent)', 'STS (2014)', 'DaLAJ', 'Tweet Sentiment Extraction', 'Climate-FEVER', 'NQ', 'DanishPoliticalComments', 'ScaLA (nb)', 'TwitterSemEval (2015)', 'LinkSO', 'Amazon Polarity', 'DBpedia', 'Angry Tweets', 'MSMARCO', 'CQADupStack (android)', 'HotpotQA', 'SciFact (pl)', 'CQADupStack (stats)', 'Polish CDSCorpus (relatedness)', 'QQP', 'BUCC', 'LanguageNet', 'CQADupStack (physics)', 'medRxiv (s2s)', 'B77', 'CQADupStack (wordpress)', 'CARER', 'Reddit (p2p)', 'CQADupStack (gis)', 'PolEmo (v2) (in)', 'SICK (relatedness)', 'MTOP (domain)', 'Civil Comments (tc)', 'SweRec', 'CQADupStack (tex)', 'STS (2012)', 'Reddit', 'CQADupStack (programmers)', 'NFCorpus (pl)', 'medRxiv (p2p)', 'SummEval', 'NPSC', 'Stack Exchange', 'CQADupStack (gaming)', 'CQADupStack (english)']"
Multi-modal Modal Leaderboard,"['MMMU (val)', 'MMBench (test) (zh)', 'MME (normalized)', 'HallusionBench', 'MathVista (minitest)', 'MM-Vet', 'CCBench', 'MMBench (test)', 'SEED-Bench (img)']"
MVBench,"['Perception Test', 'STAR', 'MovieNet', 'TVQA', 'CLEVRER', 'Charades-STA', 'VLN-CE', 'MiT', 'FunQA', 'NTU RGB+D', 'PAXION']"
Open Ko-LLM Leaderboard,"['TruthfulQA (Korean)', 'ARC (Korean)', 'MMLU (Korean)', 'HellaSwag (Korean)', 'CommonGen']"
Open LLM Leaderboard,"['ARC', 'WinoGrande', 'GSM8K', 'HellaSwag', 'MMLU', 'TruthfulQA']"
Open Multilingual LLM Evaluation Leaderboard,"['HellaSwag', 'ARC', 'TruthfulQA', 'MMLU']"
OpenCompass LLM Leaderboard (v2),"['MultiRC', 'OCNLI', 'MATH', 'OpenbookQA', 'GSM8K', 'XSUM', 'WinoGrande', 'DROP', 'WSC', 'CB', 'BBH', 'RTE', 'GAOKAO-Bench', 'ARC', 'BUSTM', 'TyDiQA', 'TriviaQA', 'ChID', 'Winogender Diagnostic', 'SIQA', 'AGIEval', 'CSQA', 'C-Eval', 'RACE', 'EPRSTMT', 'AFQMC', 'Flores-101', 'SummEdits', 'Xiezhi', 'SQuAD (v2)', 'HellaSwag', 'GAOKAO-Bench (2023)', 'MMLU', 'CSL', 'LCSTS', 'WiC', 'CMRC', 'MBPP', 'NQ', 'PIQA', 'TheoremQA', 'C3', 'HumanEval', 'DRCD', 'ReCoRD', 'Broad-Coverage Diagnostic', 'TNEWS', 'CMMLU', 'CMNLI', 'StoryCloze', 'BoolQ', 'COPA', 'LAMBADA']"
OpenEval (text),"['BiPaR', 'TUMCC', 'Corrigible', 'CAIL (2018)', 'WSC (CLUE)', 'GAOKAO-Bench', 'SNLI (zh)', 'CORGI-PM', 'ChID', 'CDIAL-BIAS', 'TGEA', 'TOCP', 'Self-awareness', 'One-box Tendency', 'CooridinateAI', 'OL-CC', 'CBBQ', 'COLD', 'SWSR', 'SQuAD (zh)', 'WPLC', 'Myopia Reward', 'M3KE', 'CommonMT', 'C3', 'Guilt Law', 'Power-seeking', 'CMMLU', 'CMNLI', 'WGlaw']"
PromptBench,"['MATH', 'GSM8K', 'MultiUN', 'CSQA', 'MMLU', 'GLUE', 'NumerSense', 'QASC', 'IWSLT (2017)', 'SQuAD (v2)', 'BIG-Bench']"
Q-Bench,"['LIVE-itw', 'KADID-10K', 'CGIQA-6K', 'LLDescribe', 'LLVisionQA', 'AGIQA-3K', 'SPAQ', 'KonIQ-10k', 'LIVE-FB LSVQ']"
RAFT,"['OSE', 'SRI', 'Over', 'B77', 'TAI', 'SOT', 'TweetEval (hate)', 'ADE Corpus (v2)', 'NIS', 'ToS', 'TC']"
ReCoRD,"['CNN DM', 'Internet Archive']"
ReForm-Eval,"['POIE', 'TDIUC (sport)', 'CUTE80', 'IC (2015)', 'GQA', 'Flickr30K', 'TDIUC (position)', 'TextCaps', 'MEDIC (dts)', 'MSCOCO (MOS)', 'ImageNetVC', 'Flowers102', 'MSCOCO (MCI)', 'RefCOCO (res)', 'CLEVR', 'TDIUC (counting)', 'WikiHow', 'TDIUC (detection)', 'TDIUC (color)', 'CIFAR-10', 'VizWiz (yesno)', 'ViQuAE', 'SROIE', 'TextVQA', 'Winoground', 'VQA (v2)', 'VisDial', 'DocVQA', 'VizWiz (singleChoice)', 'IC (2015) (Grounded)', 'TDIUC (scene)', 'MOCHEG', 'WordArt', 'IIIT5K', 'MSCOCO (OC)', 'VSR', 'COCO-Text', 'ScienceQA', 'SNLI-VE', 'Pets37', 'MP3D', 'FUNSD', 'NoCaps', 'A-OKVQA', 'TDIUC (utility)', 'Whoops', 'MSCOCO (GOI)', 'OK-VQA', 'OCR-VQA', 'COCO-Text (Grounded)', 'ImageNet-1K', 'TextOCR (Grounded)', 'TextOCR']"
RoleEval,"['RoleEval-Global', 'RoleEval-Chinese']"
ScandEval,"['ScandiQA (sv)', 'ScaLA (nn)', 'DaNE', 'SUC (v3)', 'ScaLA (nb)', 'ScaLA (da)', 'ScaLA (sv)', 'NoReC', 'ScandiQA (da)', 'SweRec', 'NorNE (nn)', 'ScandiQA (no)', 'NorNE (nb)', 'Angry Tweets']"
SCROLLS,"['QMSum', 'SummScreen', 'Qasper', 'ContractNLI', 'NarrativeQA', 'GovReport', 'QuALITY']"
SummEdits,"['QMSum', 'Billsum', 'Google News', 'SAMSum', 'ECTSum', 'TinyShakespeare', 'SciTLDR', 'Sales Call', 'Spotify Podcast', 'Sales Email']"
SuperCLUE,"['OPEN Set', 'CArena', 'CLOSE Set']"
SuperGLUE,"['MultiRC', 'WiC', 'ReCoRD', 'Winogender Diagnostic', 'WSC', 'Broad-Coverage Diagnostic', 'CB', 'RTE', 'BoolQ', 'COPA']"
SuperLim (v2),"['Winogender Diagnostic (sv) (v2)', 'Swedish analogy (v2)', 'SweWiC (v2)', 'Swedish ABSAbank-Imm (v1.1)', 'Argumentation sentences', 'MNLI (sv)', 'STS-B (sv) (v2)', 'Winograd (sv) (v2)', 'SweFAQ (v2)', 'DaLAJ-GED-SuperLim (v2)', 'SuperSim (v2)', 'SweSAT Synonyms (v1.1)', 'SweDN', 'GLUE Diagnostic (sv)']"
UHGEval,['XinhuaHallucinations']
Video-Bench Leaderboard,"['MVQA', 'YouCook2', 'TGIF-QA', 'ActivityNet-QA', 'MSVD-QA', 'DDM', 'MOT', 'SQA3D', 'NBAQA', 'TVQA', 'UCF-Crime', 'DLE', 'MSRVTT-QA']"
VLM-Eval,"['TGIF', 'MSRVTT', 'ActivityNet', 'UCF101', 'MSVD', 'Kinetics-400', 'HMDB51']"
YALL,"['GPT4All', 'TruthfulQA', 'AGIEval', 'BIG-Bench']"
