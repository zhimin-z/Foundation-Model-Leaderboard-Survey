Leaderboard,Benchmark
AgentBench,"['WebShop', 'ALFWorld', 'Mind2Web']"
AlpacaEval,"['Self-Instruct', 'HH-RLHF', 'OASST1', 'Koala', 'Vicuna']"
BBH,"['Sorting Words', 'Dyck Languages', 'Object Counting', 'Tables of Penguins', 'Web of Lies', 'Salient Translation Error Detection', 'Causal Judgment', 'Date Understanding', 'Multistep Arithmetic', 'Geometric Shapes', 'Navigation', 'Logical Deduction', 'SNARKS', 'Sports Understanding', 'Formal Fallacies and Syllogisms with Negation', 'Sequences', 'Tracking Shuffled Objects', 'Reasoning about Colored Objects', 'Disambiguation QA', 'MovieLens', 'Ruin a Name with One Edit', 'Adjective Order', 'Boolean Expressions']"
BEIR,"['ArguAna', 'CQADupStack (gis)', 'FEVER', 'FiQA (2018)', 'DBpedia', 'SciDocs', 'Climate-FEVER', 'Robust04', 'TREC-COVID', 'SciFact', 'QQP', 'MSMARCO', 'BioASQ', 'Signal-1M', 'NQ', 'Touche (2020)', 'NFCorpus', 'HotpotQA', 'TREC-News']"
Big Code Models Leaderboard,"['HumanEval', 'MultiPL-E']"
BIG-Bench,"['Self Evaluation of Tutoring', 'Cryobiology Spanish', 'Entailed Polarity', 'Dyck Languages', 'Transforming German Sentences to Gender (Inclusive Forms)', 'Simple multiple choice arithmetic (JSON)', 'COM2SENSE', 'Novel Concepts', 'Odd One Out', 'Emoji Movie', 'Understanding Fables', 'Arithmetic', 'Cryptonite', 'Empirical Judgments', 'Misconceptions (ru)', 'Logic Grid Puzzles', 'Judging Moral Permissibility', 'Similarities Test for Abstraction', 'Alignment of Simplicity Priors for Turing (Complete Concept Learning)', 'Identify Math Theorems', 'Taboo', 'Sudoku', 'ISNotes', 'Root Finding, Optimization and Games', 'Misconceptions', 'Logical Arguments', 'Persian Idioms', 'Swahili-English Paremiologic Competence', 'Unit Conversion', 'CRT', 'Understanding Grammar of Unseen Words', 'Spider', 'Color', 'Geometric Shapes', 'Protein Interaction Sites', 'Repeat Copy Logic', 'Sequential Order', 'MNLI (transliteration)', 'TimeDial', 'English to Russian Proverbs', 'characterRelations', 'High Low Guessing Game', 'Hinglish Toxicity Prediction', 'Language Games', 'Indic Cause and Effect', 'Wikidata', 'RoFT', 'Sports Understanding', 'Modified Arithmetic', 'Formal Fallacies and Syllogisms with Negation', 'Cornell Movie-Dialogs Corpus', 'Tracking Shuffled Objects', 'StrategyQA', 'The Essential, the Excessive, and the Extraneous', 'KPWr', 'Intersection Points', 'Gender Sensitivity Test (zh)', 'Rhyming', 'Word Unscrambling', 'TalkDown', 'Scientific Press Release', 'Implicatures', 'Kannada Riddles', 'Reordering', 'SGD', 'Known Unknowns', 'List Functions', 'Physics Multiple Choice', 'UnQover', 'Conceptual Combinations', 'ParsiNLU (rc)', 'BBQ-Lite', 'Tables of Penguins', 'SQuADShifts', 'Verb Tense', 'Autoclassification', 'Long Input Contexts', 'WinoWhy', 'Goal-Step Inference', 'Factuality', 'LTI LangID Corpus', 'SNLI', 'Date Understanding', 'EmoTag1200', 'Forecasting Subquestions', 'Metaphor Understanding', 'GEM', 'Phrase Relatedness', 'Hindi Question Answering', 'CoDA', 'PARSINLU (qa)', 'Simple arithmetic', 'Presuppositions as NLI', 'Figure of Speech Detection', 'Periodic Elements', 'Physical Intuition', 'Fact-Checking', 'Human Organs and Senses', 'Checkmate In One Move', 'Identifying Anachronisms', 'Shakespeare Dialogue', 'Ruin a Name with One Edit', 'Operators', 'Wino-X (German)', 'VitaminC', 'Sorting Words', 'Linguistic Mappings', 'Disfl-QA', 'Automatic Debugging', 'SIQA', 'MultiEmo', 'English Proverbs', 'Dark Humor Detection', 'Hindu Mythology Trivia', 'Simple arithmetic with multiple targets (JSON)', 'Web of Lies', 'Social Support', 'Dynamic Counting', 'Keyword Sentence Transformation', 'TellMeWhy', 'Simple arithmetic with subtasks (JSON)', 'Which Wiki Edit', 'ASCII MNIST', 'Causal Judgment', 'Strange Stories', 'Analogical Similarity', 'Analytic Entailment', 'BBQ-Lite (Json)', 'Multistep Arithmetic', 'Irony Identification', 'Discovery', 'YesNoBlackWhite Game', 'Navigation', 'Unit Interpretation', 'Cycled Letters', 'Key Value Maps', 'Identify Odd Metaphor', 'ePiC', 'Unnatural In-Context Learning', 'State Tracking in Chess', 'Simple Text Editing', 'Swedish to German proverbs', 'Sentence Ambiguity', 'Simple arithmetic (JSON)', 'Twenty Questions', 'CoQA', 'Physics Questions', 'Self-awareness', 'GRE Reading Comprehension', 'Mathematical Induction', 'Word Problems on Sets and Graphs', 'Estimating Risk of Suicide', 'Cause and Effect', 'Code Description', 'Fantasy Reasoning', 'Training on the test set', 'TruthfulQA', 'Gender Sensitivity Test (English)', 'Metaphor Boolean', 'Linguistic Puzzles', 'NQ', 'HHH', 'RiddleSense', 'SIT', 'Adjective Order', 'Common Morpheme', 'CIFAR-10', 'Boolean Expressions', 'Simple Ethical Questions', 'Natural Instructions', 'Conlang Translation Problems', 'General Knowledge', 'Python Program Synthesis', 'Minute Mysteries QA', 'Object Counting', 'Medical Questions in Russian', 'Informal and Formal Fallacies', 'Muslim-Violence Bias', 'Salient Translation Error Detection', 'CRASS', 'Matrix Shapes', 'ARC (The Abstraction and Reasoning Corpus)', 'CS Algorithms', 'MathQA', 'Wikimedia', 'Self Evaluation Courtroom', 'ASCII Word Recognition', 'SQuAD (pp)', 'Intent Recognition', 'SParC', 'Crash Blossoms', 'Spelling Bee', 'Sufficient Information', 'MNLI (ipa)', 'Logical Deduction', 'Codenames', 'SNARKS', 'Kanji ASCII Art', 'Data Wrangling', 'COPA (qa)', 'Diverse Metrics for Social Biases in Language Models', 'Sequences', 'Text Navigation Game', 'Social Bias from Sentence Probability', 'Disambiguation QA', 'MovieLens', 'Reasoning about Colored Objects', 'What is the Tao', 'Truthful QA', 'Topical-Chat', 'Entailed Polarity in Hindi', 'Authorship Verification', 'Python Programming', 'Subject-Verb Agreement']"
Chatbot Arena Leaderboad,"['Chatbot Arena Conversations', 'MMLU', 'MT-Bench']"
ChEF,"['ScienceQA', 'FSC147', 'MSCOCO', 'MME', 'VOC (2012)', 'MMBench', 'SEED-Bench', 'Omnibenchmark', 'Flickr30K', 'CIFAR-10']"
CLUE,"['OCNLI', 'TNEWS', 'C3', 'CLUE Diagnostics', 'AFQMC', 'ChID', 'CSL', 'iFLYTEK', 'CMRC (2018)', 'WSC (CLUE)', 'DRCD', 'CMNLI']"
CMB,"['CMB-Clin', 'CMB-Exam']"
CMTEB,"['T2Ranking (retrieval)', 'Waimai', 'CSL (s2s)', 'QBQTC', 'THUCNews (s2s)', 'STS-B (zh)', 'BQ', 'cMedQA (reranking)', 'OCNLI', 'PAWS-X', 'TNEWS', 'ATEC', 'mMARCO (retrieval)', 'AFQMC', 'Multi-CPR (video)', 'OnlineShopping', 'Multi-CPR (ecom)', 'JDReview', 'LCQMC', 'Multi-CPR (medical)', 'CSL (p2p)', 'iFLYTEK', 'cMedQA (v2) (retrieval)', 'DuReader (retrieval)', 'cCOVID-News', 'CMNLI', 'THUCNews (p2p)', 'mMARCO (reranking)', 'T2Ranking (reranking)', 'cMedQA (v2) (reranking)']"
Colossal-AI,"['AGIEval', 'MMLU', 'C-Eval', 'CMMLU', 'GAOKAO-Bench']"
EvalPlus,"['MBPP', 'HumanEval']"
FacTool,"['Self-Instruct', 'GSM8K', 'FactPrompts', 'RoSE', 'HumanEval']"
FewCLUE,"['OCNLI', 'BUSTM', 'TNEWS', 'ChID', 'CSL', 'iFLYTEK', 'CSLDCP', 'WSC (CLUE)', 'EPRSTMT']"
FlagEval,"['VQA (v2)', 'ImageNet-1K', 'Food-101', 'COCO-Stuff', 'AISHELL-1', 'IEMOCAP', 'Flickr30K', 'RAFT', 'HumanEval', 'OCNLI', 'BUSTM', 'TNEWS', 'Stanford Cars', 'DTD', 'CLCC', 'ChID', 'KITTI Eigen split', 'BoolQ', 'LibriSpeech', 'VQA-CP', 'FGVC-Aircraft', 'CelebA-HQ', 'C-SEM', 'GAOKAO-Bench (2023)', 'iNaturalist (2018)', 'CUB', 'Places', 'EPRSTMT', 'IMDB', 'TDIUC', 'MMLU', 'UCF101', 'TruthfulQA', 'MSCOCO', 'Flowers102', 'Cityscapes', 'SOP', 'CSL', 'WSC (CLUE)', 'ADE20K', 'KeSpeech', 'NYU-Depth', 'CMMLU', 'MSRVTT']"
GENIE,"['a-NLG', 'WMT (2019) (de-en)', 'ARC-DA (2018)', 'WMT (2021) (de-en)', 'XSUM']"
HEIM,"['Dailydall.e', 'PartiPrompts (knowledge categories)', 'Landing Pages', 'MSCOCO (zh)', 'Winoground', 'MSCOCO (efficiency)', 'I2P', 'MSCOCO (fairness - gender)', 'MSCOCO (fairness - AAVE dialect)', 'Demographic Stereotypes', 'MSCOCO (art styles)', 'CSP', 'Mental Disorders', 'Magazine Cover Photos', 'MSCOCO (base)', 'MSCOCO (fidelity)', 'PartiPrompts (reasoning categories)', 'PartiPrompts (image quality categories)', 'DrawBench (image quality categories)', 'Logos', 'CUB', 'PaintSkills', 'DrawBench (knowledge categories)', 'Historical Figures', 'DrawBench (reasoning categories)', 'MSCOCO (robustness - typos)', 'MSCOCO', 'MSCOCO (es)', 'Relational Understanding', 'MSCOCO (hi)']"
HellaSwag Leaderboard,"['HellaSwag', 'ActivityNet Captions', 'WikiHow']"
HELM Classic,"['Dyck Languages', 'bAbI', 'TwitterAAE (aa)', 'APPS', 'Synthetic Reasoning (symbolic)', 'LegalSupport', 'ICE', 'RealToxicityPrompts', 'MATH (chain-of-thought)', 'RAFT', 'HumanEval', 'Billsum', 'BoolQ', 'HellaSwag', 'BOLD', 'QuAC', 'EurLexSum', 'Disinformation (reiteration)', 'XSUM', 'WMT (2014)', 'NarrativeQA', 'EntityMatching', 'The Pile', 'Synthetic efficiency', 'DataImputation', 'Copyright (text)', 'TwitterAAE', 'NQ (open-book)', 'Civil Comments', 'Synthetic Reasoning (natural)', 'Disinformation (wedging)', 'MSMARCO', 'CNN DM', 'Copyright (code)', 'BLiMP', 'LegalBench', 'IMDB', 'TwitterAAE (white)', 'MedQA', 'MMLU', 'MSMARCO (v2)', 'NQ (closed-book)', 'TruthfulQA', 'Numerical reasoning', 'GSM8K', 'LSAT', 'BBQ', 'MultiLexSum', 'WikiFact', 'OpenbookQA', 'MATH']"
HELM Lite,"['NarrativeQA', 'MedQA', 'MMLU', 'NQ (closed-book)', 'NQ (open-book)', 'GSM8K', 'OpenbookQA', 'MATH', 'LegalBench', 'WMT (2014)']"
InstructEval,"['BBH', 'MMLU', 'IMPACT', 'HHH', 'DROP', 'CRASS', 'HumanEval']"
InterCode,"['NL2Bash', 'InterCode-CTF', 'SWE-bench', 'Spider', 'MBPP']"
KoLA,"['MAVEN-ERE', 'COPEN (CiC)', '2WikiMQA', 'ETU', 'Encyclopedic', 'FewNERD', 'COPEN (CPJ)', 'DocRED', 'High-Frequency Knowledge', 'ETM', 'COPEN (CSJ)', 'KQA Pro', 'ETC', 'Low-Frequency Knowledge', 'ETA', 'MuSiQue', 'KoRC', 'MAVEN', 'HotpotQA']"
L-Eval,"['SPACE', 'TopicRet', 'Qasper', 'TOEFL-QA', 'QMSum', 'SFiction', 'CUAD', 'MultiDoc2Dial', 'NarrativeQA', 'BigPatent', 'LONGFQA', 'OPENREVIEW', 'Multi-News', 'CodeU', 'GovReport', 'GSM8K', 'SummScreen', 'NQ', 'QuALITY', 'Coursera']"
LAiW Leaderboard,"['CAIL (2018)', 'AC-NLG', 'CAIL (2020)', 'JEC-QA', 'CAIL (2019)', 'CAIL (2021)', 'MLMN', 'CJRC', 'CFM', 'Criminal-S', 'CrimeKgAssitant', 'MSJudge']"
Large Language Model Leaderboard,"['AGIEval', 'RACE', 'PIQA', 'SIQA', 'SQuAD (v2)', 'MBPP', 'LCSTS', 'DROP', 'COPA', 'Flores-101', 'GAOKAO-Bench', 'HumanEval', 'Xiezhi', 'OCNLI', 'BUSTM', 'TNEWS', 'TheoremQA', 'Winogender Diagnostic', 'RTE', 'AFQMC', 'ChID', 'CSQA', 'CB', 'BoolQ', 'HellaSwag', 'C-Eval', 'Broad-Coverage Diagnostic', 'XSUM', 'BBH', 'LAMBADA', 'TriviaQA', 'SummEdits', 'DRCD', 'MultiRC', 'GAOKAO-Bench (2023)', 'EPRSTMT', 'TyDiQA', 'WSC', 'ReCoRD', 'MMLU', 'C3', 'CMRC', 'GSM8K', 'WiC', 'CSL', 'OpenbookQA', 'ARC', 'NQ', 'MATH', 'CMNLI', 'StoryCloze', 'CMMLU', 'WinoGrande']"
LLM Benchmarker Suite,"['AGIEval', 'MMLU', 'GSM8K', 'BoolQ', 'HellaSwag', 'OpenbookQA', 'NQ', 'QuAC', 'TriviaQA', 'HumanEval', 'WinoGrande']"
LLM-Leaderboard,"['MMLU', 'LAMBADA', 'HellaSwag', 'Chatbot Arena Conversations', 'TriviaQA', 'HumanEval', 'WinoGrande']"
LongBench,"['TREC', 'MultiFieldQA', 'Qasper', '2WikiMQA', 'LCC', 'PassageRetrieval', 'PassageCount', 'SAMSum', 'MultiFieldQA (zh)', 'QMSum', 'VCSUM', 'NarrativeQA', 'DuReader', 'Multi-News', 'PassageRetrieval (zh)', 'MuSiQue', 'GovReport', 'LSHTC', 'TriviaQA', 'HotpotQA', 'RepoBench-P']"
LVLM-eHub,"['OCR-VQA', 'ImageNet-1K', 'VCR-MCI', 'OK-VQA', 'Virtual Home', 'ScienceQA IMG', 'CTW', 'SVT', 'HOST', 'MSCOCO (MCI)', 'Flickr30K', 'SROIE', 'VCR-OC', 'FUNSD', 'MSCOCO (adversarial)', 'MSCOCO (OC)', 'MSCOCO (random)', 'SNLI-VE', 'WOST', 'DocVQA', 'WordArt', 'SVTP', 'Whoops', 'VisDial', 'IIIT5K', 'MSCOCO (popular)', 'Total-Text', 'Meta-World', 'VCR', 'CUTE80', 'Pets37', 'IconQA', 'VSR', 'IC (2015)', 'COCO-Text', 'NoCaps', 'TextVQA', 'VizWiz', 'Minecraft', 'Flowers102', 'Franka Kitchen', 'ImageNetVC', 'GQA', 'IC (2013)', 'STVQA', 'CIFAR-10']"
MedBench,"['DDx-advanced', 'MedSpeQA', 'MedHG', 'SMDoc', 'DrugCA', 'MedMC', 'DBMHG', 'MedDG', 'SafetyBench', 'CHIP-CDEE', 'CHIP-CTC', 'Med-Exam', 'CMB-Clin', 'DDx-basic', 'MedTreat', 'CHIP-CDN', 'MedHC', 'IMCS-MRG (v2)', 'CMeEE', 'CMeIE']"
MMBench,"['CLEVR', 'KonIQ-10k', 'ScienceQA', 'W3C School', 'VSR', 'LLaVA-Bench', 'COCO Captions', 'TextVQA', 'ARAS', 'Internet', 'Places', 'PISC']"
MMLU-by-task Leaderboard,"['HellaSwag', 'ARC', 'MMLU']"
MOCHA,"['NarrativeQA', 'Quoref', 'SIQA', 'MCScript', 'CosmosQA', 'DROP']"
MTEB,"['Stack Exchange', 'ScaLA (da)', 'Stack Exchange (p2p)', 'MTOP (intent)', 'DanishPoliticalComments', 'DBpedia', 'AskUbuntu', 'FiQA (2018) (pl)', 'SummEval', 'LanguageNet', 'Nordic Language Identification', 'BUCC', 'MASSIVE (intent)', 'SciFact', 'SICK (relatedness) (pl)', 'NFCorpus (pl)', 'bioRxiv (s2s)', '10kGNAD (p2p)', 'MSMARCO (pl)', 'Blurbs (p2p)', 'arXiv (s2s)', 'PPC', 'HotpotQA', 'STS-B', 'STS (2015)', 'CQADupStack (gis)', 'CQADupStack (gaming)', 'FEVER', 'QQP (pl)', 'STS (2022)', 'CQADupStack (programmers)', 'FiQA (2018)', 'ScaLA (sv)', 'PolEmo (v2) (out)', 'Civil Comments (tc)', 'STS (2012)', 'STS (2013)', 'SciFact (pl)', 'STS (2017)', 'CQADupStack (stats)', 'Angry Tweets', 'SICK (relatedness)', 'NPSC', 'HotpotQA (pl)', 'BIOSSES', 'TwitterSemEval (2015)', 'NoReC', 'Polish CDSCorpus', 'arXiv (p2p)', 'LCC', 'Reddit (p2p)', 'CQADupStack (unix)', '10kGNAD (s2s)', 'PSC', 'PAC', 'CQADupStack (android)', 'SciDocs', 'Bornholmsk', 'Climate-FEVER', 'SweFAQ (v2)', 'TREC-COVID', '20 Newsgroups', 'DBpedia (pl)', 'bioRxiv (p2p)', 'QQP', 'MSMARCO', 'MASSIVE (scenario)', 'CQADupStack (webmasters)', 'Allegro Reviews', 'SciDocs (rr)', 'CQADupStack (physics)', 'IMDB', 'CQADupStack (mathematica)', 'MARC', 'LinkSO', 'STS (2014)', 'NQ', 'NFCorpus', 'SICK (pl)', 'CQADupStack (tex)', 'ArguAna', 'medRxiv (s2s)', 'MIND', '8TAGS', 'ArguAna (pl)', 'CQADupStack (wordpress)', 'DKhate', 'ScaLA (nn)', 'Tatoeba', 'CBD', 'CARER', 'SprintDuplicateQuestions', 'B77', 'Polish CDSCorpus (relatedness)', 'medRxiv (p2p)', 'NQ (pl)', 'Reddit', 'CQADupStack (english)', 'AMCD', 'Tweet Sentiment Extraction', 'SweRec', 'MSMARCO (v2)', 'SciDocs (pl)', 'DaLAJ', 'ScaLA (nb)', 'STS (2016)', 'Touche (2020)', 'Amazon Polarity', 'Blurbs (s2s)', 'MTOP (domain)', 'PolEmo (v2) (in)']"
Multi-modal Modal Leaderboard,"['HallusionBench', 'MM-Vet', 'MathVista (minitest)', 'MMMU (val)', 'MMBench (test)', 'MMBench (test) (zh)', 'SEED-Bench (img)', 'CCBench', 'MME (normalized)']"
MVBench,"['VLN-CE', 'STAR', 'FunQA', 'NTU RGB+D', 'Charades-STA', 'Perception Test', 'CLEVRER', 'MiT', 'TVQA', 'PAXION', 'MovieNet']"
Open Ko-LLM Leaderboard,"['CommonGen', 'TruthfulQA (Korean)', 'ARC (Korean)', 'HellaSwag (Korean)', 'MMLU (Korean)']"
Open LLM Leaderboard,"['MMLU', 'TruthfulQA', 'GSM8K', 'HellaSwag', 'ARC', 'WinoGrande']"
Open Multilingual LLM Evaluation Leaderboard,"['HellaSwag', 'TruthfulQA', 'ARC', 'MMLU']"
OpenEval (text),"['OL-CC', 'Myopia Reward', 'SWSR', 'CDIAL-BIAS', 'GAOKAO-Bench', 'ChID', 'CBBQ', 'WPLC', 'TUMCC', 'Power-seeking', 'One-box Tendency', 'CAIL (2018)', 'SQuAD (zh)', 'CommonMT', 'M3KE', 'TGEA', 'Self-awareness', 'TOCP', 'C3', 'CooridinateAI', 'CORGI-PM', 'WSC (CLUE)', 'SNLI (zh)', 'WGlaw', 'Corrigible', 'BiPaR', 'CMNLI', 'CMMLU', 'Guilt Law', 'COLD']"
PromptBench,"['MMLU', 'GLUE', 'GSM8K', 'CSQA', 'SQuAD (v2)', 'MultiUN', 'NumerSense', 'QASC', 'MATH', 'BIG-Bench', 'IWSLT (2017)']"
Q-Bench,"['SPAQ', 'KonIQ-10k', 'LLDescribe', 'CGIQA-6K', 'LIVE-FB LSVQ', 'LLVisionQA', 'KADID-10K', 'AGIQA-3K', 'LIVE-itw']"
RAFT,"['TweetEval (hate)', 'NIS', 'SRI', 'SOT', 'ToS', 'TC', 'B77', 'ADE Corpus (v2)', 'OSE', 'Over', 'TAI']"
ReForm-Eval,"['OCR-VQA', 'ImageNet-1K', 'VQA (v2)', 'TextOCR (Grounded)', 'Winoground', 'OK-VQA', 'POIE', 'MSCOCO (MCI)', 'Flickr30K', 'SROIE', 'MEDIC (dts)', 'MP3D', 'FUNSD', 'MSCOCO (OC)', 'CLEVR', 'SNLI-VE', 'DocVQA', 'TDIUC (utility)', 'WordArt', 'TextCaps', 'Whoops', 'VisDial', 'COCO-Text (Grounded)', 'VizWiz (yesno)', 'IIIT5K', 'A-OKVQA', 'VizWiz (singleChoice)', 'TDIUC (counting)', 'MSCOCO (MOS)', 'RefCOCO (res)', 'VSR', 'Pets37', 'CUTE80', 'IC (2015)', 'COCO-Text', 'WikiHow', 'MOCHEG', 'TextOCR', 'TextVQA', 'NoCaps', 'TDIUC (scene)', 'TDIUC (position)', 'TDIUC (color)', 'ViQuAE', 'MSCOCO (GOI)', 'ScienceQA', 'Flowers102', 'IC (2015) (Grounded)', 'ImageNetVC', 'GQA', 'TDIUC (detection)', 'TDIUC (sport)', 'CIFAR-10']"
ScandEval,"['NorNE (nn)', 'ScandiQA (da)', 'SUC (v3)', 'ScaLA (da)', 'SweRec', 'ScaLA (nb)', 'ScandiQA (sv)', 'NorNE (nb)', 'Angry Tweets', 'ScaLA (sv)', 'ScaLA (nn)', 'ScandiQA (no)', 'DaNE', 'NoReC']"
SCROLLS,"['NarrativeQA', 'Qasper', 'GovReport', 'QMSum', 'SummScreen', 'ContractNLI', 'QuALITY']"
SummEdits,"['Google News', 'Spotify Podcast', 'TinyShakespeare', 'ECTSum', 'Billsum', 'QMSum', 'Sales Call', 'SAMSum', 'Sales Email', 'SciTLDR']"
SuperCLUE,"['CArena', 'CLOSE Set', 'OPEN Set']"
SuperGLUE,"['WSC', 'ReCoRD', 'Winogender Diagnostic', 'RTE', 'Broad-Coverage Diagnostic', 'CB', 'BoolQ', 'WiC', 'MultiRC', 'COPA']"
SuperLim (v2),"['SweSAT Synonyms (v1.1)', 'MNLI (sv)', 'DaLAJ-GED-SuperLim (v2)', 'SweDN', 'Winogender Diagnostic (sv) (v2)', 'Winograd (sv) (v2)', 'Argumentation sentences', 'SuperSim (v2)', 'Swedish ABSAbank-Imm (v1.1)', 'STS-B (sv) (v2)', 'SweFAQ (v2)', 'SweWiC (v2)', 'Swedish analogy (v2)', 'GLUE Diagnostic (sv)']"
Video-Bench Leaderboard,"['NBAQA', 'SQA3D', 'UCF-Crime', 'DLE', 'ActivityNet-QA', 'YouCook2', 'MOT', 'TGIF-QA', 'MSRVTT-QA', 'MVQA', 'DDM', 'TVQA', 'MSVD-QA']"
VLM-Eval,"['TGIF', 'UCF101', 'ActivityNet', 'Kinetics-400', 'HMDB51', 'MSVD', 'MSRVTT']"
