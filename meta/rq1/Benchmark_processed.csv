Leaderboard,Benchmark
AgentBench,"['Mind2Web', 'WebShop', 'ALFWorld']"
AlpacaEval (v2),"['OASST1', 'Self-Instruct', 'Koala', 'Vicuna', 'HH-RLHF']"
BBH,"['Disambiguation QA', 'Geometric Shapes', 'Boolean Expressions', 'Adjective Order', 'Ruin a Name with One Edit', 'Navigation', 'Logical Deduction', 'Causal Judgment', 'Tables of Penguins', 'Date Understanding', 'Formal Fallacies and Syllogisms with Negation', 'MovieLens', 'Multistep Arithmetic', 'Sports Understanding', 'Sequences', 'Object Counting', 'Reasoning about Colored Objects', 'Web of Lies', 'Sorting Words', 'SNARKS', 'Tracking Shuffled Objects', 'Salient Translation Error Detection', 'Dyck Languages']"
BEIR,"['NFCorpus', 'Climate-FEVER', 'TREC-News', 'SciFact', 'HotpotQA', 'Robust04', 'QQP', 'Touche (2020)', 'Signal-1M', 'NQ', 'FEVER', 'TREC-COVID', 'BioASQ', 'CQADupStack (gis)', 'MSMARCO', 'FiQA (2018)', 'DBpedia', 'ArguAna', 'SciDocs']"
Big Code Models Leaderboard,"['HumanEval', 'MultiPL-E']"
BIG-Bench,"['English to Russian Proverbs', 'Cause and Effect', 'MNLI (ipa)', 'Data Wrangling', 'Hinglish Toxicity Prediction', 'Disambiguation QA', 'Identify Math Theorems', 'Phrase Relatedness', 'Swahili-English Paremiologic Competence', 'Unit Conversion', 'Taboo', 'Python Programming', 'Implicatures', 'Physics Questions', 'BBQ-Lite (Json)', 'Factuality', 'Intersection Points', 'Authorship Verification', 'Discovery', 'Gender Sensitivity Test (zh)', 'Ruin a Name with One Edit', 'Simple arithmetic (JSON)', 'Estimating Risk of Suicide', 'Sentence Ambiguity', 'Minute Mysteries QA', 'Spelling Bee', 'GRE Reading Comprehension', 'Topical-Chat', 'Judging Moral Permissibility', 'TalkDown', 'Analogical Similarity', 'Wino-X (German)', 'Unnatural In-Context Learning', 'Truthful QA', 'Arithmetic', 'Language Games', 'CS Algorithms', 'COM2SENSE', 'CIFAR-10', 'Hindi Question Answering', 'Wikidata', 'Sports Understanding', 'Transforming German Sentences to Gender (Inclusive Forms)', 'Goal-Step Inference', 'Which Wiki Edit', 'Indic Cause and Effect', 'Simple multiple choice arithmetic (JSON)', 'Simple arithmetic', 'Misconceptions (ru)', 'Kanji ASCII Art', 'Spider', 'Cycled Letters', 'Web of Lies', 'YesNoBlackWhite Game', 'Medical Questions in Russian', 'Root Finding, Optimization and Games', 'Cornell Movie-Dialogs Corpus', 'LTI LangID Corpus', 'Dyck Languages', 'Analytic Entailment', 'Code Description', 'CoQA', 'EmoTag1200', 'Disfl-QA', 'Muslim-Violence Bias', 'Adjective Order', 'Physics Multiple Choice', 'Scientific Press Release', 'State Tracking in Chess', 'StrategyQA', 'Swedish to German proverbs', 'TellMeWhy', 'Text Navigation Game', 'Wikimedia', 'MultiEmo', 'Persian Idioms', 'Autoclassification', 'Navigation', 'VitaminC', 'Irony Identification', 'What is the Tao', 'The Essential, the Excessive, and the Extraneous', 'Tables of Penguins', 'Rhyming', 'BBQ-Lite', 'Training on the test set', 'Forecasting Subquestions', 'Color', 'Automatic Debugging', 'Date Understanding', 'SIT', 'Linguistic Puzzles', 'Identifying Anachronisms', 'RoFT', 'Multistep Arithmetic', 'Simple arithmetic with subtasks (JSON)', 'Logical Arguments', 'Alignment of Simplicity Priors for Turing (Complete Concept Learning)', 'Sequences', 'Twenty Questions', 'MathQA', 'Self-awareness', 'Long Input Contexts', 'Conlang Translation Problems', 'CoDA', 'SQuAD (pp)', 'Dynamic Counting', 'Linguistic Mappings', 'Verb Tense', 'Common Morpheme', 'SNARKS', 'Diverse Metrics for Social Biases in Language Models', 'Subject-Verb Agreement', 'WinoWhy', 'Self Evaluation of Tutoring', 'ParsiNLU (rc)', 'RiddleSense', 'Gender Sensitivity Test (English)', 'Python Program Synthesis', 'Understanding Grammar of Unseen Words', 'English Proverbs', 'Shakespeare Dialogue', 'characterRelations', 'Presuppositions as NLI', 'SIQA', 'SNLI', 'Repeat Copy Logic', 'Cryptonite', 'Logical Deduction', 'Sudoku', 'ARC (The Abstraction and Reasoning Corpus)', 'PARSINLU (qa)', 'Understanding Fables', 'Misconceptions', 'Operators', 'Novel Concepts', 'SQuADShifts', 'Natural Instructions', 'Fact-Checking', 'Formal Fallacies and Syllogisms with Negation', 'Sufficient Information', 'ePiC', 'MovieLens', 'Logic Grid Puzzles', 'Metaphor Boolean', 'ISNotes', 'Mathematical Induction', 'List Functions', 'Object Counting', 'Empirical Judgments', 'Sequential Order', 'TruthfulQA', 'Known Unknowns', 'Simple arithmetic with multiple targets (JSON)', 'Intent Recognition', 'ASCII MNIST', 'Figure of Speech Detection', 'TimeDial', 'Protein Interaction Sites', 'CRASS', 'GEM', 'Key Value Maps', 'Odd One Out', 'General Knowledge', 'Fantasy Reasoning', 'Keyword Sentence Transformation', 'Geometric Shapes', 'SGD', 'Similarities Test for Abstraction', 'ASCII Word Recognition', 'Simple Ethical Questions', 'Social Bias from Sentence Probability', 'Crash Blossoms', 'Entailed Polarity in Hindi', 'Boolean Expressions', 'Dark Humor Detection', 'COPA (qa)', 'UnQover', 'Hindu Mythology Trivia', 'MNLI (transliteration)', 'Reordering', 'Cryobiology Spanish', 'Conceptual Combinations', 'HHH', 'Word Unscrambling', 'Human Organs and Senses', 'Causal Judgment', 'Periodic Elements', 'SParC', 'Simple Text Editing', 'Social Support', 'Matrix Shapes', 'Checkmate In One Move', 'Informal and Formal Fallacies', 'Codenames', 'Strange Stories', 'Word Problems on Sets and Graphs', 'NQ', 'Physical Intuition', 'Unit Interpretation', 'Modified Arithmetic', 'Identify Odd Metaphor', 'Kannada Riddles', 'Reasoning about Colored Objects', 'CRT', 'KPWr', 'Sorting Words', 'Self Evaluation Courtroom', 'Tracking Shuffled Objects', 'Emoji Movie', 'Entailed Polarity', 'Salient Translation Error Detection', 'High Low Guessing Game', 'Metaphor Understanding']"
Chatbot Arena Leaderboard,"['MT-Bench', 'MMLU', 'Chatbot Arena Conversations']"
ChEF,"['MME', 'VOC (2012)', 'Flickr30K', 'MMBench', 'FSC147', 'Omnibenchmark', 'CIFAR-10', 'MSCOCO', 'ScienceQA', 'SEED-Bench']"
CLUE,"['OCNLI', 'TNEWS', 'CSL', 'WSC (CLUE)', 'DRCD', 'C3', 'iFLYTEK', 'CMRC (2018)', 'ChID', 'AFQMC', 'CMNLI', 'CLUE Diagnostics']"
CMB,"['CMB-Exam', 'CMB-Clin']"
CMTEB,"['Multi-CPR (medical)', 'cMedQA (v2) (retrieval)', 'cMedQA (v2) (reranking)', 'THUCNews (p2p)', 'CSL (s2s)', 'mMARCO (reranking)', 'JDReview', 'Waimai', 'STS-B (zh)', 'OCNLI', 'THUCNews (s2s)', 'Multi-CPR (ecom)', 'QBQTC', 'PAWS-X', 'cCOVID-News', 'iFLYTEK', 'cMedQA (reranking)', 'CSL (p2p)', 'CMNLI', 'ATEC', 'AFQMC', 'mMARCO (retrieval)', 'DuReader (retrieval)', 'OnlineShopping', 'T2Ranking (reranking)', 'T2Ranking (retrieval)', 'LCQMC', 'TNEWS', 'Multi-CPR (video)', 'BQ']"
Colossal-AI,"['CMMLU', 'GAOKAO-Bench', 'C-Eval', 'AGIEval', 'MMLU']"
CoQA,"['CNN DM', 'Wikipedia', 'RACE', 'MCTest', 'WritingPrompts', 'Project Gutenberg', 'SciQ']"
EvalPlus,"['HumanEval', 'MBPP']"
FacTool,"['HumanEval', 'Self-Instruct', 'FactPrompts', 'RoSE', 'GSM8K']"
FewCLUE,"['OCNLI', 'TNEWS', 'CSL', 'WSC (CLUE)', 'iFLYTEK', 'BUSTM', 'CSLDCP', 'ChID', 'EPRSTMT']"
FlagEval,"['CMMLU', 'Stanford Cars', 'iNaturalist (2018)', 'Flickr30K', 'Food-101', 'FGVC-Aircraft', 'IEMOCAP', 'SOP', 'VQA-CP', 'MMLU', 'ADE20K', 'KITTI Eigen split', 'VQA (v2)', 'OCNLI', 'GAOKAO-Bench (2023)', 'HumanEval', 'CSL', 'CUB', 'LibriSpeech', 'CelebA-HQ', 'MSRVTT', 'BUSTM', 'IMDB', 'AISHELL-1', 'UCF101', 'Places', 'MSCOCO', 'C-SEM', 'BoolQ', 'ChID', 'COCO-Stuff', 'EPRSTMT', 'ImageNet-1K', 'TruthfulQA', 'TNEWS', 'WSC (CLUE)', 'CLCC', 'Flowers102', 'NYU-Depth', 'Cityscapes', 'RAFT', 'TDIUC', 'DTD', 'KeSpeech']"
GENIE,"['ARC-DA (2018)', 'XSUM', 'WMT (2019) (de-en)', 'a-NLG', 'WMT (2021) (de-en)']"
HEIM,"['DrawBench (reasoning categories)', 'Historical Figures', 'Mental Disorders', 'MSCOCO (art styles)', 'MSCOCO (robustness - typos)', 'MSCOCO (fidelity)', 'CUB', 'Dailydall.e', 'MSCOCO (fairness - gender)', 'MSCOCO (efficiency)', 'Demographic Stereotypes', 'Winoground', 'I2P', 'Logos', 'MSCOCO (zh)', 'DrawBench (knowledge categories)', 'Landing Pages', 'MSCOCO (hi)', 'PartiPrompts (image quality categories)', 'CSP', 'MSCOCO (base)', 'Magazine Cover Photos', 'MSCOCO', 'DrawBench (image quality categories)', 'MSCOCO (fairness - AAVE dialect)', 'MSCOCO (es)', 'PartiPrompts (reasoning categories)', 'Relational Understanding', 'PartiPrompts (knowledge categories)', 'PaintSkills']"
HellaSwag Leaderboard,"['WikiHow', 'HellaSwag', 'ActivityNet Captions']"
HELM Classic,"['WikiFact', 'Disinformation (wedging)', 'LegalBench', 'Synthetic Reasoning (symbolic)', 'MMLU', 'TwitterAAE (white)', 'MultiLexSum', 'LegalSupport', 'RealToxicityPrompts', 'ICE', 'Copyright (code)', 'MATH', 'HumanEval', 'OpenbookQA', 'Billsum', 'TwitterAAE (aa)', 'IMDB', 'Disinformation (reiteration)', 'bAbI', 'QuAC', 'Numerical reasoning', 'NQ (open-book)', 'Synthetic Reasoning (natural)', 'BoolQ', 'Synthetic efficiency', 'DataImputation', 'APPS', 'XSUM', 'BBQ', 'BOLD', 'MedQA', 'HellaSwag', 'CNN DM', 'EurLexSum', 'Copyright (text)', 'TruthfulQA', 'LSAT', 'The Pile', 'MSMARCO', 'EntityMatching', 'RAFT', 'BLiMP', 'NQ (closed-book)', 'NarrativeQA', 'Dyck Languages', 'MATH (chain-of-thought)', 'TwitterAAE', 'GSM8K', 'WMT (2014)', 'MSMARCO (v2)', 'Civil Comments']"
HELM Lite,"['MATH', 'OpenbookQA', 'NQ (open-book)', 'LegalBench', 'MMLU', 'NQ (closed-book)', 'NarrativeQA', 'MedQA', 'GSM8K', 'WMT (2014)']"
HHEM Leaderboard,['CNN DM']
InstructEval,"['DROP', 'HumanEval', 'HHH', 'MMLU', 'BBH', 'CRASS', 'IMPACT']"
InterCode,"['Spider', 'SWE-bench', 'InterCode-CTF', 'MBPP', 'NL2Bash']"
KoLA,"['COPEN (CSJ)', 'ETU', 'MAVEN-ERE', 'MAVEN', 'HotpotQA', 'MuSiQue', 'ETM', 'FewNERD', 'ETC', 'KoRC', 'COPEN (CPJ)', 'COPEN (CiC)', 'ETA', 'DocRED', 'High-Frequency Knowledge', 'Low-Frequency Knowledge', 'Encyclopedic', 'KQA Pro', '2WikiMQA']"
L-Eval,"['Multi-News', 'SummScreen', 'LONGFQA', 'QuALITY', 'SPACE', 'TopicRet', 'MultiDoc2Dial', 'GovReport', 'SFiction', 'OPENREVIEW', 'QMSum', 'NQ', 'CUAD', 'BigPatent', 'TOEFL-QA', 'Coursera', 'CodeU', 'Qasper', 'NarrativeQA', 'GSM8K']"
LAiW Leaderboard,"['CFM', 'AC-NLG', 'CAIL (2020)', 'MLMN', 'CAIL (2021)', 'JEC-QA', 'CAIL (2018)', 'Criminal-S', 'MSJudge', 'CAIL (2019)', 'CrimeKgAssitant', 'CJRC']"
Large Language Model Leaderboard,"['CMMLU', 'MultiRC', 'MBPP', 'Flores-101', 'MMLU', 'RTE', 'PIQA', 'SIQA', 'OCNLI', 'GAOKAO-Bench', 'MATH', 'DROP', 'CSL', 'HumanEval', 'OpenbookQA', 'Broad-Coverage Diagnostic', 'COPA', 'GAOKAO-Bench (2023)', 'StoryCloze', 'TyDiQA', 'BUSTM', 'CB', 'Xiezhi', 'ARC', 'C-Eval', 'SQuAD (v2)', 'AGIEval', 'C3', 'ReCoRD', 'WiC', 'RACE', 'NQ', 'Winogender Diagnostic', 'BoolQ', 'LAMBADA', 'ChID', 'AFQMC', 'XSUM', 'CMNLI', 'BBH', 'EPRSTMT', 'HellaSwag', 'SummEdits', 'CMRC', 'TNEWS', 'DRCD', 'CSQA', 'LCSTS', 'WSC', 'TriviaQA', 'WinoGrande', 'GSM8K', 'TheoremQA']"
LLM Benchmarker Suite,"['HumanEval', 'OpenbookQA', 'AGIEval', 'NQ', 'TriviaQA', 'MMLU', 'BoolQ', 'WinoGrande', 'QuAC', 'HellaSwag', 'GSM8K']"
LLM-Leaderboard,"['HumanEval', 'TriviaQA', 'LAMBADA', 'MMLU', 'WinoGrande', 'HellaSwag', 'Chatbot Arena Conversations']"
LongBench,"['Multi-News', 'LCC', 'MultiFieldQA (zh)', 'TREC', 'HotpotQA', 'PassageRetrieval (zh)', 'RepoBench-P', 'MultiFieldQA', 'MuSiQue', 'GovReport', 'DuReader', 'QMSum', 'LSHTC', 'VCSUM', 'SAMSum', 'TriviaQA', 'Qasper', 'PassageRetrieval', 'NarrativeQA', 'PassageCount', '2WikiMQA']"
LVLM-eHub,"['ImageNetVC', 'IC (2013)', 'SVTP', 'MSCOCO (MCI)', 'Flickr30K', 'IconQA', 'TextVQA', 'MSCOCO (adversarial)', 'CTW', 'NoCaps', 'VSR', 'SNLI-VE', 'VCR-OC', 'FUNSD', 'SROIE', 'Meta-World', 'CUTE80', 'VCR-MCI', 'Pets37', 'Total-Text', 'VizWiz', 'DocVQA', 'STVQA', 'Minecraft', 'SVT', 'IC (2015)', 'Whoops', 'OK-VQA', 'GQA', 'MSCOCO (random)', 'Virtual Home', 'CIFAR-10', 'IIIT5K', 'Franka Kitchen', 'VisDial', 'MSCOCO (OC)', 'MSCOCO (popular)', 'OCR-VQA', 'ImageNet-1K', 'HOST', 'VCR', 'WordArt', 'Flowers102', 'WOST', 'COCO-Text', 'ScienceQA IMG']"
MedBench,"['DBMHG', 'CHIP-CDN', 'DDx-basic', 'MedHG', 'DrugCA', 'CMeEE', 'MedMC', 'Med-Exam', 'MedHC', 'CHIP-CTC', 'CMB-Clin', 'CMeIE', 'MedTreat', 'CHIP-CDEE', 'IMCS-MRG (v2)', 'SMDoc', 'MedSpeQA', 'SafetyBench', 'DDx-advanced', 'MedDG']"
MMBench,"['W3C School', 'TextVQA', 'Places', 'LLaVA-Bench', 'ScienceQA', 'ARAS', 'PISC', 'Internet', 'CLEVR', 'KonIQ-10k', 'COCO Captions', 'VSR']"
MMLU-by-task Leaderboard,"['ARC', 'HellaSwag', 'MMLU']"
MOCHA,"['SIQA', 'DROP', 'Quoref', 'CosmosQA', 'NarrativeQA', 'MCScript']"
MTEB,"['NQ (pl)', 'NPSC', 'ArguAna (pl)', 'HotpotQA (pl)', 'Allegro Reviews', 'IMDB', 'CQADupStack (unix)', 'DKhate', 'SummEval', 'TREC-COVID', 'B77', 'bioRxiv (p2p)', 'PSC', 'PPC', 'CQADupStack (gis)', 'CQADupStack (wordpress)', 'MIND', 'MSMARCO', 'AMCD', 'STS (2012)', 'Reddit (p2p)', 'ScaLA (nn)', 'medRxiv (p2p)', 'PAC', 'MSMARCO (v2)', 'SICK (relatedness)', 'CQADupStack (tex)', '10kGNAD (s2s)', 'CQADupStack (programmers)', 'Civil Comments (tc)', 'Stack Exchange (p2p)', 'CQADupStack (stats)', 'Polish CDSCorpus', 'MASSIVE (intent)', 'STS (2016)', 'DBpedia (pl)', 'PolEmo (v2) (out)', 'Angry Tweets', 'ScaLA (nb)', 'MASSIVE (scenario)', 'CBD', 'CQADupStack (english)', 'bioRxiv (s2s)', 'CQADupStack (gaming)', 'BUCC', 'CQADupStack (physics)', 'Reddit', 'MTOP (domain)', 'MARC', 'arXiv (p2p)', 'medRxiv (s2s)', 'Tatoeba', 'Blurbs (p2p)', 'LCC', 'AskUbuntu', 'Climate-FEVER', 'NFCorpus', 'TwitterSemEval (2015)', 'CQADupStack (webmasters)', 'SciDocs (pl)', 'SciFact', 'SweRec', 'NoReC', 'QQP (pl)', 'Nordic Language Identification', 'SprintDuplicateQuestions', 'MTOP (intent)', 'Tweet Sentiment Extraction', 'STS (2015)', 'STS (2013)', 'SICK (pl)', '10kGNAD (p2p)', 'STS (2022)', 'Stack Exchange', 'LinkSO', 'NFCorpus (pl)', 'FiQA (2018)', '20 Newsgroups', 'CARER', 'SICK (relatedness) (pl)', 'STS-B', 'STS (2014)', 'SciDocs (rr)', 'DaLAJ', 'STS (2017)', 'PolEmo (v2) (in)', 'MSMARCO (pl)', 'Polish CDSCorpus (relatedness)', 'SciFact (pl)', 'HotpotQA', '8TAGS', 'Bornholmsk', 'LanguageNet', 'QQP', 'ScaLA (da)', 'BIOSSES', 'Amazon Polarity', 'Touche (2020)', 'DanishPoliticalComments', 'Blurbs (s2s)', 'NQ', 'FEVER', 'FiQA (2018) (pl)', 'ScaLA (sv)', 'DBpedia', 'arXiv (s2s)', 'SweFAQ (v2)', 'ArguAna', 'CQADupStack (android)', 'SciDocs', 'CQADupStack (mathematica)']"
Multi-modal Modal Leaderboard,"['SEED-Bench (img)', 'HallusionBench', 'MME (normalized)', 'MMMU (val)', 'MMBench (test) (zh)', 'CCBench', 'MathVista (minitest)', 'MMBench (test)', 'MM-Vet']"
MVBench,"['MovieNet', 'PAXION', 'VLN-CE', 'CLEVRER', 'MiT', 'TVQA', 'FunQA', 'Perception Test', 'Charades-STA', 'STAR', 'NTU RGB+D']"
Open Ko-LLM Leaderboard,"['HellaSwag (Korean)', 'ARC (Korean)', 'CommonGen', 'TruthfulQA (Korean)', 'MMLU (Korean)']"
Open LLM Leaderboard,"['TruthfulQA', 'MMLU', 'WinoGrande', 'ARC', 'HellaSwag', 'GSM8K']"
Open Multilingual LLM Evaluation Leaderboard,"['ARC', 'HellaSwag', 'MMLU', 'TruthfulQA']"
OpenEval (text),"['CMMLU', 'SNLI (zh)', 'WGlaw', 'BiPaR', 'CORGI-PM', 'TOCP', 'CommonMT', 'TGEA', 'SQuAD (zh)', 'CooridinateAI', 'GAOKAO-Bench', 'OL-CC', 'CDIAL-BIAS', 'Power-seeking', 'SWSR', 'CBBQ', 'M3KE', 'C3', 'Corrigible', 'ChID', 'CMNLI', 'Self-awareness', 'One-box Tendency', 'WSC (CLUE)', 'Myopia Reward', 'TUMCC', 'Guilt Law', 'COLD', 'CAIL (2018)', 'WPLC']"
PromptBench,"['NumerSense', 'MATH', 'SQuAD (v2)', 'CSQA', 'MMLU', 'MultiUN', 'BIG-Bench', 'IWSLT (2017)', 'GSM8K', 'QASC', 'GLUE']"
Q-Bench,"['LLDescribe', 'AGIQA-3K', 'SPAQ', 'LIVE-itw', 'LLVisionQA', 'KonIQ-10k', 'CGIQA-6K', 'KADID-10K', 'LIVE-FB LSVQ']"
RAFT,"['SRI', 'ADE Corpus (v2)', 'TAI', 'SOT', 'Over', 'OSE', 'NIS', 'B77', 'TC', 'TweetEval (hate)', 'ToS']"
ReCoRD,"['CNN DM', 'Internet Archive']"
ReForm-Eval,"['ImageNetVC', 'MSCOCO (MCI)', 'Flickr30K', 'TextVQA', 'TDIUC (utility)', 'MEDIC (dts)', 'TextOCR (Grounded)', 'VizWiz (yesno)', 'TextCaps', 'TDIUC (scene)', 'TDIUC (counting)', 'NoCaps', 'VSR', 'VQA (v2)', 'SNLI-VE', 'FUNSD', 'IC (2015) (Grounded)', 'SROIE', 'CUTE80', 'TDIUC (detection)', 'VizWiz (singleChoice)', 'ScienceQA', 'MP3D', 'WikiHow', 'Pets37', 'Winoground', 'CLEVR', 'MOCHEG', 'DocVQA', 'OK-VQA', 'GQA', 'Whoops', 'IC (2015)', 'POIE', 'CIFAR-10', 'IIIT5K', 'VisDial', 'MSCOCO (OC)', 'TDIUC (sport)', 'RefCOCO (res)', 'MSCOCO (MOS)', 'TDIUC (position)', 'OCR-VQA', 'MSCOCO (GOI)', 'ImageNet-1K', 'TDIUC (color)', 'COCO-Text (Grounded)', 'WordArt', 'Flowers102', 'COCO-Text', 'ViQuAE', 'TextOCR', 'A-OKVQA']"
RoleEval,"['RoleEval-Chinese', 'RoleEval-Global']"
ScandEval,"['DaNE', 'ScandiQA (da)', 'NorNE (nn)', 'SweRec', 'NoReC', 'ScaLA (da)', 'ScaLA (nn)', 'Angry Tweets', 'NorNE (nb)', 'ScaLA (sv)', 'ScaLA (nb)', 'SUC (v3)', 'ScandiQA (sv)', 'ScandiQA (no)']"
SCROLLS,"['QMSum', 'SummScreen', 'Qasper', 'ContractNLI', 'NarrativeQA', 'GovReport', 'QuALITY']"
SummEdits,"['Sales Email', 'Google News', 'QMSum', 'SAMSum', 'ECTSum', 'Billsum', 'SciTLDR', 'TinyShakespeare', 'Spotify Podcast', 'Sales Call']"
SuperCLUE,"['OPEN Set', 'CLOSE Set', 'CArena']"
SuperGLUE,"['CB', 'MultiRC', 'Broad-Coverage Diagnostic', 'COPA', 'ReCoRD', 'WiC', 'WSC', 'Winogender Diagnostic', 'BoolQ', 'RTE']"
SuperLim (v2),"['SweWiC (v2)', 'GLUE Diagnostic (sv)', 'DaLAJ-GED-SuperLim (v2)', 'Winogender Diagnostic (sv) (v2)', 'Winograd (sv) (v2)', 'SuperSim (v2)', 'SweFAQ (v2)', 'STS-B (sv) (v2)', 'Swedish ABSAbank-Imm (v1.1)', 'Argumentation sentences', 'SweDN', 'SweSAT Synonyms (v1.1)', 'MNLI (sv)', 'Swedish analogy (v2)']"
Video-Bench Leaderboard,"['MVQA', 'MSVD-QA', 'YouCook2', 'NBAQA', 'TVQA', 'UCF-Crime', 'MSRVTT-QA', 'MOT', 'SQA3D', 'DDM', 'ActivityNet-QA', 'DLE', 'TGIF-QA']"
VLM-Eval,"['Kinetics-400', 'ActivityNet', 'TGIF', 'MSRVTT', 'UCF101', 'MSVD', 'HMDB51']"
