Leaderboard,N-shot,Metrics,Format
Chatbot Arena Leaderboad,5,EM,1F
Flageval,0,EM,3F
HELM,0,"EM,ECE (10-bin),EM (Robustness),EM (Fairness),Denoised inference time (s),# eval,# train,truncated,# prompt tokens,# output tokens,# trials","0F,1F,3F"
InstructEval,5,EM,1F
LLM Benchmarker Suite,0,?,1F
LLM-Leaderboard,"0,3,5,10","Accuracy,EM,ECE (10-bin),EM (Robustness),EM (Fairness),Denoised inference time (s),# eval,# train,truncated,# prompt tokens,# output tokens,# trials","0F,0F%,1F,2F,3F"
Open Ko-LLM Leaderboard,5,Accuracy,2F
Open LLM Leaderboard,5,Accuracy,2F
Open Multilingual LLM Evaluation Leaderboard,5,Accuracy,1F
OpenCompass,5,Accuracy,1F
PromptBench,3,PDR,"2F,+-"