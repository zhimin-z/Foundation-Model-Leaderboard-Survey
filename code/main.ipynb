{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jimmy/Documents/GitHub/LLM-Leaderboard-Integration/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from gradio_client import Client\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = Path(\"../data\")\n",
    "path_llm = path_data / \"llm\"\n",
    "path_lvlm = path_data / \"lvlm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://felixz-open-llm-leaderboard.hf.space/ ✔\n"
     ]
    }
   ],
   "source": [
    "client = Client(\"https://felixz-open-llm-leaderboard.hf.space/\")\n",
    "json_data = client.predict(\"\",\"\", api_name='/predict')\n",
    "\n",
    "with open(json_data, 'r') as file:\n",
    "    file_data = file.read()\n",
    "    data = json.loads(file_data)\n",
    "    df = pd.DataFrame(data['data'], columns=data['headers'])\n",
    "    df.drop(columns=['Model'], inplace=True)\n",
    "    df.to_json(path_llm / 'HuggingFace-Open-llm-leaderboard-20231116.json', orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path_llm / 'a.csv')\n",
    "df.to_json(path_llm / 'Sherlock.json', orient='records', indent=4)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Text-to-Text Generation', 'Text-to-Image Generation', 'Text-to-Code Generation', 'Text-to-Audio Generation', 'Text-to-Video Generation', 'Image-to-Text Generation', 'Code-to-Text Generation', 'Audio-to-Text Generation', 'Video-to-Text Generation', 'Image-to-Image Generation', 'Code-to-Code Generation', 'Audio-to-Audio Generation', 'Video-to-Video Generation',\n",
    "\n",
    "'Code Summarization', 'Code Review', 'Identifier Prediction', 'Defect Detection', 'Clone Detection', 'Code Classification', 'Code Reasoning', 'Document Translation', 'Log Parsing',\n",
    "\n",
    "# 'Image Captioning', 'Sign Language Recognition', 'Emotion Recognition', 'Video Processing', 'Digital Human', 'Multimodality'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [\n",
    "    \"Natural Language Understanding\",\n",
    "    \"Natural Language Inference\",\n",
    "    \"Question Answering\",\n",
    "    \"Summarization\",\n",
    "    \"Translation\",\n",
    "    \"Sentiment Analysis\",\n",
    "    \"Dialogue\",\n",
    "    \"Text Classification\",\n",
    "    \"Information Retrieval\",\n",
    "    \"Knowledge Completion\",\n",
    "    \"Relation Extraction\",\n",
    "    \"Ethics and Morality\",\n",
    "    \"Societal Bias\",\n",
    "    \"Toxicity\",\n",
    "    \"Fairness\",\n",
    "    \"Hallucination\",\n",
    "    \"Calibration\",\n",
    "    \"Efficiency\",\n",
    "    \"Sequence Tagging\",\n",
    "    \"Coreference Resolution\",\n",
    "    \"Fact Extraction\",\n",
    "    \"Multilinguality\",\n",
    "    \"Hate Detection\",\n",
    "    \"Recommendation\",\n",
    "    \"Healthcare\",\n",
    "    \"Education\",\n",
    "    \"Honestness\",\n",
    "    \"Helpfulness\",\n",
    "    \"Harmlessness\",\n",
    "    \"Robustness\",\n",
    "    \"Quality\",\n",
    "    \"Alignment\",\n",
    "    \"Aesthetics\",\n",
    "    \"Originality\",\n",
    "    \"Fidelity\",\n",
    "    \"Risk\",\n",
    "    \"Law\",\n",
    "    \"Mathematics\",\n",
    "    \"Social Science\",\n",
    "    \"Natural Science\",\n",
    "    \"Finance\",\n",
    "    \"Engineering\",\n",
    "    \"Truthfulness\",\n",
    "    \"Code Reasoning\",\n",
    "    \"Commonsense Reasoning\",\n",
    "    \"Knowledge Reasoning\",\n",
    "    \"Multi-hop Reasoning\",\n",
    "    \"Logical Reasoning\",\n",
    "    \"Arithmetic Reasoning\",\n",
    "    \"Symbolic Reasoning\",\n",
    "    \"Attribute Reasoning\",\n",
    "    \"Relation Reasoning\",\n",
    "    \"Tool Creation\",\n",
    "    \"Tool Manipulation\",\n",
    "    \"Robotic Tasks\",\n",
    "    \"Code Executor\",\n",
    "    \"Calculator\",\n",
    "    \"Search Engine\",\n",
    "    \"Online Shopping\",\n",
    "    \"Personality Testing\",\n",
    "    \"Crowd-sourcing Testing\",\n",
    "    \"Human-in-the-loop\",\n",
    "]\n",
    "\n",
    "labels2 = [\n",
    "    \"Visual Perception\",\n",
    "    \"Visual Reasoning\",\n",
    "    'Visual Knowledge Acquisition',\n",
    "    \"Visual Commonsense\",\n",
    "    \"Object Hallucination\",\n",
    "    \"Embodied Intelligence\",\n",
    "]\n",
    "\n",
    "# \"Open Book\",\n",
    "# \"Closed Book\",\n",
    "# 'Single Choice',\n",
    "# 'Multiple Choice',\n",
    "\n",
    "benchmark_lvlm_labels = {\n",
    "    'A-OKVQA': [\n",
    "        \"Perception\",\n",
    "        \"Existence\",\n",
    "        \"Position\",\n",
    "        \"Color\"\n",
    "    ],\n",
    "    'ALFWorld': [\n",
    "        \"Perception\",\n",
    "        \"Existence\",\n",
    "        \"Position\",\n",
    "        \"Color\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "benchmark_llm_labels = {\n",
    "    '2WikiMultihopQA': [\n",
    "        \"Question Answering\",\n",
    "        \"Multi-hop Reasoning\",\n",
    "        \"Information Retrieval\",\n",
    "        \"Knowledge Reasoning\",\n",
    "        \"Commonsense Reasoning\",\n",
    "        \"Natural Language Inference\",\n",
    "        \"Relation Extraction\"\n",
    "    ],\n",
    "    'A-OKVQA': [\n",
    "        \"Question Answering\",\n",
    "        \"Commonsense Reasoning\",\n",
    "        \"Knowledge Reasoning\",\n",
    "        \"Relation Reasoning\"\n",
    "    ],\n",
    "    'AFQMC': [\n",
    "        \"Natural Language Understanding\",\n",
    "        \"Text Classification\",\n",
    "        \"Natural Language Inference\",\n",
    "        \"Commonsense Reasoning\",\n",
    "        \"Finance\"\n",
    "    ],\n",
    "    'AGIEval': [\n",
    "        \"Natural Language Understanding\",\n",
    "        \"Question Answering\",\n",
    "        \"Text Classification\",\n",
    "        \"Knowledge Reasoning\",\n",
    "        \"Logical Reasoning\",\n",
    "        \"Arithmetic Reasoning\",\n",
    "        \"Multi-hop Reasoning\"\n",
    "    ],\n",
    "    'ALFWorld': [\n",
    "        \"Knowledge Reasoning\",\n",
    "        \"Relation Reasoning\",\n",
    "        \"Tool Manipulation\",\n",
    "        \"Robotic Tasks\",\n",
    "        \"Commonsense Reasoning\"\n",
    "    ],\n",
    "    'AlpacaEval': [\n",
    "        \"Natural Language Understanding\",\n",
    "        \"Dialogue\",\n",
    "        \"Text Classification\",\n",
    "        \"Information Retrieval\",\n",
    "        \"Knowledge Completion\",\n",
    "        \"Helpfulness\",\n",
    "        \"Robustness\",\n",
    "        \"Quality\"\n",
    "    ],\n",
    "    'Amazon Review': [\n",
    "        \"Sentiment Analysis\",\n",
    "        \"Text Classification\",\n",
    "        \"Information Retrieval\",\n",
    "        \"Recommendation\",\n",
    "        \"Summarization\",\n",
    "        \"Helpfulness\"\n",
    "    ],\n",
    "    'ANGO': [\n",
    "        \n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values with hyperlinks written to b.xlsx\n"
     ]
    }
   ],
   "source": [
    "from openpyxl import load_workbook, Workbook\n",
    "\n",
    "# Path to your Excel file and output file\n",
    "input_file = 'a.xlsx'\n",
    "output_file = 'b.xlsx'\n",
    "\n",
    "# Load the workbook and select the active worksheet\n",
    "wb = load_workbook(input_file)\n",
    "ws = wb.active\n",
    "\n",
    "# Dictionary to store unique values and their hyperlinks\n",
    "unique_values = {}\n",
    "\n",
    "# Loop through all cells in the worksheet\n",
    "for row in ws.iter_rows():\n",
    "    for cell in row:\n",
    "        # Check if cell has a value\n",
    "        if cell.value:\n",
    "            # Store value and hyperlink (if any)\n",
    "            unique_values[cell.value] = cell.hyperlink.target if cell.hyperlink else None\n",
    "\n",
    "# Sort the unique values\n",
    "sorted_unique_values = sorted(unique_values.items())\n",
    "\n",
    "# Create a new workbook and select the active worksheet\n",
    "new_wb = Workbook()\n",
    "new_ws = new_wb.active\n",
    "\n",
    "# Write unique values and hyperlinks to the new worksheet\n",
    "for i, (text, hyperlink) in enumerate(sorted_unique_values, start=1):\n",
    "    new_ws.cell(row=i, column=1, value=text)\n",
    "    if hyperlink:\n",
    "        new_ws.cell(row=i, column=1).hyperlink = hyperlink\n",
    "\n",
    "# Save the new workbook\n",
    "new_wb.save(output_file)\n",
    "\n",
    "print(f\"Unique values with hyperlinks written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_description_list = {\n",
    "    '2WikiMultihopQA': '2WikiMultihopQA is a dataset for comprehensive evaluation of reasoning steps in multi-hop question answering. The dataset contains questions that require multiple steps of reasoning to answer, and is based on Wikipedia articles. The dataset includes evidence information containing a reasoning path for multi-hop questions.',\n",
    "    'A-OKVQA': 'A-OKVQA is a visual question answering dataset that requires models to draw upon outside knowledge to answer questions. It is composed of approximately 25k questions that require a broad base of commonsense and world knowledge to answer. The questions in the dataset cannot be answered by simply querying a knowledge base, and instead require some form of commonsense reasoning. The dataset includes questions that require reasoning via a variety of knowledge types such as commonsense, world knowledge, and visual knowledge.',\n",
    "    'AFQMC': 'The Ant Financial Question Matching Corpus (AFQMC) is a dataset for semantic similarity in the Chinese language. It is used to evaluate the performance of natural language processing models on tasks such as question matching and paraphrase identification. The dataset contains 16,000 pairs of questions, each labeled with a similarity score ranging from 0 to 5.',\n",
    "    'AGIEval': 'AGIEval is a human-centric benchmark designed to evaluate the general abilities of foundation models in tasks pertinent to human cognition and problem-solving. It is derived from 20 official, public, and high-standard admission and qualification exams intended for general human test-takers, such as general college admission tests (e.g., Chinese College Entrance Exam (Gaokao) and American SAT), law school admission tests, math competitions, lawyer qualification tests, and national civil service exams.',\n",
    "    'ALFWorld': 'ALFWorld contains interactive TextWorld environments that parallel embodied worlds in the ALFRED dataset. The aligned environments allow agents to reason and learn high-level policies in an abstract space before solving embodied tasks through low-level actuation.',\n",
    "    'ANGO': 'ANGO-S1 is a dataset of Chinese analogical reasoning questions. It contains 1,000 questions that are categorized into 4 types: \"judgmental reasoning\", \"analogical reasoning\", \"logical relationship\", and \"logical relationship-correspondence\"',\n",
    "    'APPS': 'The APPS dataset is a collection of programming problems gathered from various open-access coding websites such as Codeforces, Kattis, and more. The dataset is intended to mimic how human programmers are evaluated by presenting coding problems in unrestricted natural language and assessing the accuracy of solutions.',\n",
    "    'ARC': 'The AI2’s Reasoning Challenge (ARC) dataset is a multiple-choice question-answering dataset that contains questions from science exams from grade 3 to grade 9. The dataset is split into two partitions: Easy and Challenge, where the latter partition contains more difficult questions that require reasoning. Most of the questions have 4 answer choices, and some have 5. The dataset consists of 7,787 science exam questions drawn from a variety of sources, including science questions provided under license by a research partner affiliated with AI2.',\n",
    "    'ARC-DA': 'ARC-DA is a dataset of 2,985 grade-school level, direct-answer (\"open response\", \"free form\") science questions derived from the ARC multiple-choice question set released as part of the AI2 Reasoning Challenge. The questions were filtered based on heuristic filters such as concreteness of the question and confidence in the correctness of the answers. The dataset was built by presenting each of the multiple-choice questions as a direct answer question to five crowdsourced workers to gather additional answers. The questions were then further filtered and modified by volunteers in-house. The dataset is one of the first direct-answer datasets of natural questions that often require reasoning, and where appropriate question decompositions are not evident from the questions themselves.',\n",
    "    'ActivityNet-QA': 'The ActivityNet-QA dataset consists of 58,000 human-annotated QA pairs on 5.8k complex web videos. The dataset provides a benchmark for testing the performance of VideoQA models on long-term spatio-temporal reasoning.',\n",
    "    'AlpacaEval': 'AlpacaEval is an automatic evaluator for instruction-following language models. It is a human-validated, high-quality, cheap, and fast dataset that is particularly useful for model development. The dataset is a simplification of AlpacaFarm’s evaluation set, where \"instructions\" and \"inputs\" are merged into one field, and reference outputs are longer. It is designed to evaluate instruction-following models (e.g., ChatGPT) and is validated against 20K human annotations. The dataset is useful if you have to run quick and cheap proxy for human evaluation of simple instruction-following tasks.',\n",
    "    'Amazon Review': 'Amazon Review Data (2018) is a dataset that contains reviews and ratings of products sold on Amazon. The dataset is available on Kaggle and includes full reviews and ratings for various products. The dataset is useful for sentiment analysis and natural language processing tasks. It includes reviews from four different merchandise categories: Books, DVDs, Electronics, and Kitchen and housewares.',\n",
    "    'AQUA-RAT': 'The AQUA-RAT (Algebra Question Answering with Rationales) Dataset is a collection of approximately 100,000 algebraic word problems with natural language rationales. Each problem is a JSON object consisting of four parts: question, options, rationale, and correct option. The dataset is used to train a program generation model that learns to generate the explanation while generating the program that solves the problem.',\n",
    "    'AR-LSAT': 'AR-LSAT dataset consists of analytical reasoning (AR) problems from the Law School Admission Test (LSAT) from 1991 to 2016. The dataset is used to study the challenge of analytical reasoning of text and to analyze what knowledge understanding and reasoning abilities are required to do well on this task.',\n",
    "    'ArguAna': 'ArguAna is a dataset in the BEIR (Benchmarking Information Retrieval) framework that contains textual data for entity-linking-retrieval and fact-checking-retrieval tasks.',\n",
    "    'BBQ': 'BBQ (Bias Benchmark for QA) is a dataset of question sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts. The dataset was introduced to evaluate model responses at two levels: (i) given an under-informative context, we test how strongly responses reflect social biases, and (ii) given an adequately informative context, we test whether the model’s biases override a correct answer choice.',\n",
    "    'BIG-Bench': 'BIG-bench is a collaborative benchmark that aims to evaluate and extrapolate the capabilities of large language models. It consists of over 200 tasks that are designed to test the models’ ability to perform a wide range of natural language processing tasks, such as question answering, summarization, and translation.',\n",
    "    'BLiMP': 'BLiMP stands for Benchmark of Linguistic Minimal Pairs. It is a dataset that consists of 67 sub-datasets, each containing 1000 minimal pairs that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics 123. The data is automatically generated according to expert-crafted grammars. The purpose of this dataset is to evaluate what language models know about major grammatical phenomena in English.',\n",
    "    'BOLD': 'The Bias in Open-ended Language Generation Dataset (BOLD) is a large-scale dataset that consists of 23,679 English text generation prompts for benchmarking biases across five domains: profession, gender, race, religion, and political ideology. The dataset was introduced to systematically study and benchmark social biases in open-ended language generation. It also proposes new automated metrics for toxicity, psycholinguistic norms, and text gender polarity to measure social biases in open-ended text generation from multiple angles.',\n",
    "    'BUSTM': 'BUSTM dataset is a part of the CLUE benchmark. It is a Chinese language dataset that contains short text pairs. The dataset is designed to evaluate the performance of models on dialogue short text matching tasks.',\n",
    "    'Belebele': 'The Belebele dataset is a massively multilingual reading comprehension dataset that spans 122 language variants. It is a multiple-choice machine reading comprehension (MRC) dataset that contains 900 questions per language variant. The dataset is designed to evaluate the performance of mono- and multi-lingual models in high-, medium-, and low-resource languages. Each question has four multiple-choice answers and is linked to a short passage from the FLORES-200 dataset. The human annotation procedure was carefully curated to create questions that discriminate between different levels of generalizable language comprehension and is reinforced by extensive quality checks.',\n",
    "    'BiPaR': 'BiPaR is a bilingual parallel novel-style machine reading comprehension (MRC) dataset that was developed to support multilingual and cross-lingual reading comprehension. It contains 3,667 bilingual parallel paragraphs from Chinese and English novels, from which 14,668 parallel question-answer pairs were constructed via crowdsourced workers following a strict quality control procedure. Each triple (Passage, Question, Answer) in BiPaR is written parallelly in two languages, making it different from existing reading comprehension datasets. The dataset offers good diversification in prefixes of questions, answer types, and relationships between questions and passages. BiPaR requires reading comprehension skills of coreference resolution, multi-sentence reasoning, and understanding of implicit causality, etc.',\n",
    "    'BigPatent': 'BIGPATENT is a large-scale dataset of.3 million records of U.S. patent documents along with human-written abstractive summaries. It was created to address the limitations of existing summarization datasets, which are mostly compiled from the news domain and have summaries with a flattened discourse structure. In contrast, BIGPATENT summaries contain a richer discourse structure with more recurring entities, salient content is evenly distributed in the input, and lesser and shorter extractive fragments are present in the summaries. The dataset is useful for training and evaluating systems that can understand an article’s global content structure as well as produce abstractive summaries with high compression ratio.',\n",
    "    'BioASQ-QA': 'The BioASQ-QA dataset is a manually curated corpus for biomedical question answering. It is composed of a question, human-annotated answers, and the relevant contexts (also called snippets). The dataset is part of the BioASQ (Biomedical Semantic Indexing and Question Answering) project, which aims to advance the state-of-the-art in biomedical semantic indexing and question answering. The dataset is designed to evaluate the performance of biomedical question answering systems. The dataset is composed of two phases: Phase A and Phase B. Phase A contains 3.7k questions, and the answers are either a document or a snippet. Phase B contains 3.7k questions, and the answers are either a factoid answer, a list of answers, or a yes/no answer.',\n",
    "    'BoolQ': 'The BoolQ dataset is a question answering dataset specifically designed for yes/no questions. It contains 15942 examples. These questions are naturally occurring, meaning they are generated in unprompted and unconstrained settings.',\n",
    "    'Boolean Expressions': 'The Boolean Expressions dataset is a part of the BIG-bench (Beyond the Imitation Game benchmark) project hosted on GitHub. This dataset is designed to test the ability of language models to understand and reason about Boolean expressions. The task involves evaluating Boolean expressions that are composed of the logical operators AND, OR, and NOT, as well as parentheses for grouping. The expressions are generated randomly, and the variables in the expressions can take on the values True or False.',\n",
    "    'Broadcoverage Diagnostics': 'The Broadcoverage Diagnostics dataset is a part of the GLUE benchmark. It is a manually-curated evaluation dataset for fine-grained analysis of system performance on a broad range of linguistic phenomena. This dataset evaluates sentence understanding through Natural Language Inference (NLI) problems.',\n",
    "    'C-Eval': 'The C-Eval benchmark dataset is a comprehensive Chinese evaluation suite designed to assess the advanced knowledge and reasoning abilities of foundation models. The dataset consists of 13,948 multiple-choice questions spanning 52 diverse disciplines and four difficulty levels13. The disciplines range from humanities to science and engineering. The questions are designed to test the model’s understanding of a wide range of topics and its ability to reason about them. In addition to the main C-Eval dataset, there is also a C-Eval Hard subset. This subset includes 8 challenging subjects in math, physics, and chemistry that require advanced reasoning abilities to solve',\n",
    "    'C-SEM': 'The C-SEM dataset is a benchmark dataset for evaluating the semantic understanding abilities of large Chinese language models. Addressing the lack of linguistically oriented benchmarks in this area, C-SEM introduces various levels and difficulties of evaluation data to scrutinize the potential flaws of current models. It focuses on how these models process semantics, drawing parallels to human language cognition.',\n",
    "    'C3': 'C3 is a free-form multiple-choice Chinese machine reading comprehension dataset that contains 13,369 documents and their associated 19,577 multiple-choice free-form questions. The dataset was collected from Chinese-as-a-second language examinations. The dataset is the first of its kind and is designed to test the ability of machine learning models to understand and answer questions based on free-form text in Chinese.',\n",
    "    'CAIL': '''The CAIL (Chinese AI and Law) dataset, specifically CAIL2018, is a pioneering large-scale Chinese legal dataset aimed at facilitating judgment prediction research. It contains over 2.6 million criminal cases, which were published by the Supreme People's Court of China. This makes it significantly larger than other datasets previously available in the field of legal judgment prediction. Its considerable size and focus on Chinese legal cases make it a unique and valuable resource for research in artificial intelligence applications within the legal domain, particularly in the context of the Chinese legal system''',\n",
    "    'CBBQ': 'The CBBQ dataset is a Chinese bias benchmark dataset that consists of over 100,000 questions jointly constructed by human experts and generative language models. The dataset covers stereotypes and societal biases in 14 social dimensions related to Chinese culture and values. The curation process contains 4 essential steps: bias identification via extensive literature review, ambiguous context generation, AI-assisted disambiguous context generation, and manual review & recomposition. The testing instances in the dataset are automatically derived from 3,000+ high-quality templates manually authored with stringent quality control.',\n",
    "    'CCBench': 'The CCBench dataset is a multi-modal benchmark specifically focused on the domain of Chinese culture. It was introduced as a new benchmark in 2023 and is an extension of the MMBench benchmark. The CCBench dataset is designed to evaluate multi-modal models, particularly in the context of their understanding and interpretation of aspects related to Chinese culture. While I was unable to find detailed information regarding the specific contents and structure of the dataset, it is clear that it serves as a specialized tool for assessing the performance of AI models in handling culturally-specific multimodal data, especially those pertaining to Chinese traditions and contexts',\n",
    "    'CDIAL-BIAS': 'The CDial-Bias Dataset is a unique resource specifically designed to measure social bias in dialog scenarios. It was created to address the subtleties and subjective nature of biased utterances in dialogues. The dataset, which is a part of a larger effort to identify social bias in dialogue systems, includes well-annotated training data that covers aspects like context-sensitivity, data-type, targeted group, and implied attitudes.',\n",
    "    'CG-Eval': 'The CG-Eval dataset is a comprehensive benchmark designed to evaluate the generation capabilities of large Chinese language models across various academic disciplines. It covers six major academic disciplines, including Science and Engineering, Humanities and Social Sciences, Mathematics, Medical Qualification Exam, Judicial Examination, and Certified Public Accountant Exam, encompassing 55 sub-disciplines within these categories. The dataset includes over 11,000 questions of various types, for which the models are expected to provide precise and pertinent answers. CG-Eval utilizes a composite scoring system, with standard reference answers for non-computational questions and a detailed evaluation of both the final results and the problem-solving process for computational questions.',\n",
    "    'CHID': '''The CHID dataset, standing for \"Chinese IDiom Dataset for Cloze Test,\" is a large-scale dataset specifically designed for cloze-style tests in Chinese. It consists of 581,000 passages that contain a total of 729,000 blanks, encompassing a wide range of domains. In these passages, idioms are deliberately replaced with blank symbols. The dataset challenges the ability of models to understand and predict idioms in context, a unique aspect of the Chinese language. To complete the test, a list of passages is provided, and the correct idioms must be selected from a set of given candidates with a fixed length. This dataset not only includes the passages and their associated blanks but also offers training data and answers, making it a comprehensive resource for enhancing cloze test-related research in Chinese language processing.''',\n",
    "    'CIFAR-10': 'The CIFAR-10 dataset (Canadian Institute For Advanced Research) is a collection of images that are commonly used to train machine learning and computer vision algorithms. It is one of the most widely used datasets for machine learning research. The CIFAR-10 dataset contains 60,000 32x32 color images in 10 different classes.',\n",
    "    'CJRC': '''The CJRC (Chinese Judicial Reading Comprehension) dataset is a substantial collection specifically designed for legal research in the Chinese language. It comprises approximately 10,000 documents and nearly 50,000 questions with answers. The documents included in the dataset are derived from judicial judgment documents. These questions are expertly annotated by law professionals, ensuring their relevance and accuracy in the context of legal studies. This dataset is particularly valuable for research in the field of reading comprehension technology, especially in its application to legal documents. It enables researchers to test and enhance machine reading comprehension capabilities, specifically tailored for the legal domain. The CJRC dataset is a significant resource for developing and evaluating AI models' ability to understand and process legal texts.''',\n",
    "    'CLEVR': 'The CLEVR dataset is a diagnostic dataset for compositional language and elementary visual reasoning. It is used to test a range of visual reasoning abilities and contains minimal biases. The dataset has detailed annotations describing the kind of reasoning each question requires. It is used to analyze a variety of modern visual reasoning systems, providing novel insights into their abilities and limitations. Additionally, it contains a Compositional Generalization Test (CoGenT) that tests the ability of models to recognize novel combinations of attributes at test-time.',\n",
    "    'CLEVRER': '''The CLEVRER dataset, short for CoLlision Events for Video REpresentation and Reasoning, is a diagnostic video dataset designed for the systematic evaluation of computational models on a variety of reasoning tasks. It was created with the intention of providing a controlled and well-annotated environment to facilitate the assessment of logic reasoning in both temporal and causal domains. CLEVRER is unique in that it focuses on simplicity and minimal biases in visual scenes and language, ensuring that the tasks are purely logic-based. This dataset includes four types of questions: descriptive (e.g., \"what color\"), explanatory (\"what's responsible for\"), predictive (\"what will happen next\"), and counterfactual reasoning. The goal of these questions is to probe the models' abilities in different aspects of reasoning, including description, explanation, prediction, and counterfactual analysis. The design and structure of CLEVRER make it a pioneering dataset for neuro-symbolic reasoning in the realm of artificial intelligence, providing a comprehensive tool for evaluating and enhancing computational models' reasoning capabilities​.''',\n",
    "    'CLUE diagnostics': '''The CLUE (Chinese Language Understanding Evaluation) diagnostics dataset is a comprehensive tool designed for evaluating and analyzing the performance of Chinese language models. This dataset is part of the broader CLUE benchmark, which is akin to the GLUE benchmark in English. It serves as a specialized resource for diagnosing the strengths and weaknesses of models in various aspects of language understanding. The CLUE diagnostics dataset includes a range of linguistic tasks and challenges, designed to thoroughly test different facets of language comprehension, such as syntax, semantics, context understanding, and more. By providing detailed performance metrics across these areas, the CLUE diagnostics dataset helps researchers and developers identify specific areas for improvement in Chinese language models, ultimately contributing to the advancement of natural language processing technologies in the Chinese context.''',\n",
    "    'CLUEWSC': 'The CLUEWSC (CLUE Winograd Scheme Challenge) 2020 dataset is a specialized subset of the broader CLUE (Chinese Language Understanding Evaluation) benchmark, which is designed to evaluate natural language understanding (NLU) in Chinese. While CLUE itself encompasses various NLU datasets spanning single-sentence and sentence-pair classification tasks, machine reading comprehension, and other tasks on Chinese text​​, CLUEWSC 2020 specifically focuses on the challenge of coreference resolution. In CLUEWSC 2020, the primary task is to determine which noun a given pronoun in a sentence refers to. The dataset consists of texts sourced from contemporary literature, which have been annotated by human experts. The format of the questions is centered around true or false discrimination, requiring the model to discern the correct referent of a pronoun within a sentence. This specific focus makes CLUEWSC 2020 an important tool for assessing the understanding of pronoun-noun relationships in Chinese language models, a crucial aspect of language understanding and processing​',\n",
    "    'CLiB': 'The CLiB dataset is a comprehensive leaderboard for evaluating the capabilities of large Chinese language models. The evaluation in CLiB covers multiple dimensions of model capabilities, including classification ability, information extraction ability, reading comprehension ability, and table question-answering ability.',\n",
    "    'CMMLU': '''The CMMLU (Chinese Massive Multitask Language Understanding) dataset is a comprehensive evaluation benchmark specifically designed to assess the knowledge and reasoning abilities of language models within the Chinese language and cultural context. It encompasses a wide array of subjects, covering 67 topics that range from elementary to advanced professional levels. This includes natural sciences requiring calculation and reasoning, humanities and social sciences requiring knowledge, as well as practical knowledge like Chinese driving rules.''',\n",
    "    'CMNLI': '''The CMNLI dataset, part of the Chinese Language Understanding Evaluation (CLUE) benchmark, is a Chinese version of the Natural Language Inference (NLI) task, focusing on assessing the capability of models to understand and infer relationships between sentences. It is composed of two parts: XNLI and MNLI, and includes a variety of data sources such as fiction, telephone, travel, government, and slate.''',\n",
    "    'CMRC2018': '''The CMRC 2018 (Chinese Machine Reading Comprehension 2018) dataset, introduced by Cui et al., is a specialized dataset designed for evaluating Chinese machine reading comprehension. It features a collection of nearly 20,000 questions that have been carefully annotated by human experts. A notable aspect of this dataset is the inclusion of a challenging set of questions that require reasoning over multiple clues to answer. This characteristic makes CMRC 2018 particularly valuable for assessing the depth and complexity of machine reading comprehension models, especially in their ability to handle nuanced and multifaceted queries in the Chinese language​.''',\n",
    "    'CNN/Daily Mail': '''The CNN/Daily Mail summarization dataset is a widely used resource for text summarization research. It consists of news articles from the CNN and Daily Mail websites, accompanied by human-generated abstractive summary bullets. These summaries are created in a unique format where questions are formulated with one of the entities hidden, and the system is expected to fill in the blank based on the corresponding news story. The dataset includes over 300,000 unique English-language news articles.''',\n",
    "    'COCO-Text': '''The COCO-Text dataset is a specialized resource for text detection and recognition within natural images. It's an extension of the MS COCO dataset, which is renowned for its complex everyday scene images. COCO-Text aims to advance the state-of-the-art in text detection and recognition by providing a large-scale dataset specifically focused on scene text. The dataset includes various types of images, such as those with non-text, legible text, and illegible text. The dataset is structured to support a variety of text-related tasks, including End-To-End Recognition, Cropped Word Recognition, and Text Localization.''',\n",
    "    'COLDataset': '''COLDataset is a dataset that facilitates Chinese offensive language detection and model evaluation. It contains 37,480 comments with binary offensive labels and covers diverse topics of race, gender, and region. The test set is annotated at a fine-grained level with four categories: attacking individuals, attacking groups, anti-bias, and other non-offensive.''',\n",
    "    'COPA': '''Choice of Plausible Alternatives (COPA) is a benchmark dataset that evaluates the ability of machine learning models to transfer commonsense reasoning across languages. Each question is composed of a premise and two alternatives, where the task is to select the alternative that more plausibly has a causal relation with the premise. The COPA evaluation provides researchers with a tool for assessing progress in open-domain commonsense causal reasoning.''',\n",
    "    'COPEN': '''The COPEN dataset is a conceptual knowledge probing benchmark designed to evaluate the conceptual understanding capabilities of Pre-trained Language Models (PLMs). it consists of three specific tasks: Conceptual Similarity Judgment (CSJ): This task involves presenting a query entity along with several candidate entities. The PLM's job is to select the candidate entity that is most conceptually similar to the query entity. Conceptual Property Judgment (CPJ): In this task, the PLM is given a statement that describes a property of a concept. The model must then determine whether this statement is true. Conceptualization in Contexts (CiC): This task requires a PLM to analyze a sentence that mentions an entity and is provided with several concept chains of the entity. The model must then choose the most appropriate concept for the entity based on the context of the sentence.''',\n",
    "    'CORGI-PM': '''CORGI-PM is a Chinese cOrpus foR Gender bIas Probing and Mitigation, which contains 32.9k sentences with high-quality labels derived by following an annotation scheme specifically developed for gender bias in the Chinese context.''',\n",
    "    'CQADupStack': '''CQADupStack is a benchmark dataset for community question-answering (cQA) research. It contains threads from twelve StackExchange subforums, annotated with duplicate question information and comes with pre-defined training, development, and test splits, both for retrieval and classification experiments.''',\n",
    "    'CRASS': '''The CRASS benchmarks aims to develope a new test suite for the evaluation of the performance of current large language models (LLMs) by utilizing so called questionized counterfactual conditionals (QCC). The data consists of so called PCTs (Premise-Counterfactual Tuples). They contrast a hypothetical situation using a counterfactual conditional against a base premise. In the fixed target mode the task of the model is to decide which answer is correct. In the open scoring mode the model has to give an answer to the PCT which in turn is judged by human annotators as correct or incorrect.''',\n",
    "    'CSL': '''CSL is a large-scale Chinese scientific literature dataset that contains the titles, abstracts, keywords, and academic fields of 396k papers.''',\n",
    "    'CSNLI': '''The Chinese-SNLI dataset is a large-scale Chinese nature language inference and semantic similarity calculation dataset. It was created by translating and partially correcting an English dataset, and it can help alleviate the lack of Chinese natural language inference and semantic similarity calculation datasets''',\n",
    "    'CUAD': '''The Contract Understanding Atticus Dataset (CUAD) is a dataset of legal contracts with rich expert annotations. It is a corpus of 13,000+ labels in 510 commercial legal contracts that have been manually labeled under the supervision of experienced lawyers to identify 41 types of legal clauses that are considered important in contact review in connection with a corporate transaction, including mergers & acquisitions, etc.''',\n",
    "    'Cute80': '''Cute80 dataset is a public dataset that can be used for text detection in images. It consists of 80 high-resolution images with 288 cropped text instances.''',\n",
    "    'CWQ': '''The ComplexWebQuestions (CWQ) dataset is a collection of complex questions that require reasoning over multiple web snippets. It contains a large set of complex questions in natural language, and can be used in multiple ways: 1) By interacting with a search engine, which is the focus of our paper; 2) As a reading comprehension task: we release 12,725,989 web snippets that are relevant for the questions, and were collected during the development of our model; 3) As a semantic parsing task: each question is paired with a SPARQL query that can be executed against Freebase to retrieve the answer.''',\n",
    "    'CUB-200-2011': '''The Caltech-UCSD Birds-200-2011 (CUB-200-2011) dataset is a collection of images of birds that is widely used for fine-grained visual categorization tasks. Each image has detailed annotations such as 1 subcategory label, 15 part locations, 312 binary attributes, and 1 bounding box. The dataset is suitable for object recognition or attribute recognition tasks.''',\n",
    "    'Charades-STA': '''The Charades-STA dataset is an enhanced version of the original Charades dataset, specifically designed for video captioning and temporal localization tasks. It includes 9,848 short video clips that depict a variety of daily indoor activities such as brushing teeth and cooking. This dataset stands out by incorporating sentence temporal annotations to the original Charades videos, effectively extending its utility for more complex language and video processing tasks. In Charades-STA, each video is accompanied by sentence-level temporal annotations, creating a rich source of data for training and testing models in accurately associating textual descriptions with specific moments in a video. The dataset contains 12,408 moment-sentence pairs in the training set and 3,720 pairs in the testing set. The format of these annotations typically includes the video name, the start and end times of the action within the video, and the corresponding sentence description. This structured approach allows for advanced applications in video analysis, such as the development of models capable of localizing specific activities in a video based on textual queries. Charades-STA's focus on daily indoor activities, combined with its detailed temporal and linguistic annotations, makes it an invaluable resource for research in video captioning, natural language processing, and temporal activity localization.''',\n",
    "    'Chatbot Arena Conversations': '''The Chatbot Arena Conversations dataset comprises 33,000 crowd-sourced conversations, each annotated with human preferences. These conversations were collected from the Chatbot Arena platform between April and June 2023, involving interactions with various chatbot models. Key features of the dataset include detailed information for each conversation, such as the names of the two AI models involved, the full text of the conversation, the user's vote, anonymized user ID, the detected language, moderation tags (including the OpenAI moderation API tag and an additional toxic tag), and a timestamp. Efforts were made to exclude conversations containing personally identifiable information, and the dataset includes flagged inappropriate conversations, which can be critical for researchers studying safety-related aspects of LLMs.''',\n",
    "    'ChineseSquad': '''The ChineseSquad dataset is a Chinese machine reading comprehension dataset created by translating and manually correcting the original Squad dataset into Chinese. This dataset was developed to address the limitations of existing Chinese reading comprehension datasets, which were either too small or too focused on specific domains.''',\n",
    "    'CivilComments': '''The CivilComments dataset is derived from the Civil Comments platform, a commenting plugin used on about 50 English-language news sites worldwide. The dataset is known for its use in the Jigsaw Unintended Bias in Toxicity Classification and features a large collection of comments, approximately 2 million, which are useful for understanding the nuances and challenges of online discourse. The dataset serves as a benchmark for toxic comment classification, helping to advance research in detecting toxic comments on social media, which is crucial for content moderation.''',\n",
    "    'Climate-FEVER': '''CLIMATE-FEVER is a publicly available dataset that aims to verify climate change-related claims. It consists of 1,535 real-world claims collected from the internet and each claim is accompanied by five manually annotated evidence sentences retrieved from the English Wikipedia that support, refute or do not give enough information to validate the claim. The dataset was created to facilitate and encourage work on improving algorithms for retrieving evidential support for climate-specific claims, addressing the underlying language understanding challenges, and ultimately help alleviate the impact of misinformation on climate change.''',\n",
    "    'CoLA': '''The Corpus of Linguistic Acceptability (CoLA) is a specialized dataset comprising 10,657 sentences extracted from 23 linguistics publications. These sentences have been expertly annotated for acceptability (grammaticality) by their original authors. The primary aim of CoLA is to provide a resource for training and evaluating natural language processing models, particularly in the area of grammatical acceptability judgments. The public version of CoLA includes 9,594 sentences designated for training and development purposes, while excluding 1,063 sentences that are part of a held-out test set. This dataset serves as a valuable tool for researchers in the field of computational linguistics, offering a rich set of examples to improve and assess the grammatical understanding capabilities of various language models​''',\n",
    "    'CoQA': '''CoQA is a large-scale dataset for building Conversational Question Answering systems. The dataset contains 127,000+ questions with answers collected from 8000+ conversations. Each conversation is collected by pairing two crowdworkers to chat about a passage in the form of questions and answers. The unique features of CoQA include 1) the questions are conversational; 2) the answers can be free-form text; 3) each answer also comes with an evidence subsequence highlighted in the passage; and 4) the passages are collected from seven diverse domains.''',\n",
    "    'CodeU': '''The CodeU dataset is a part of the L-Eval evaluation suite for long context language models. It is designed to test the capability of understanding long code by calling some functions defined in a lengthy codebase and inferring the final output of the program. The dataset mainly uses source code from Numpy and also includes a string processing codebase containing more than 100 functions that take a string as input. To prevent language models from answering the question based on their parametric knowledge, the original function names defined in Numpy are replaced with Op1, Op2..., OpN.''',\n",
    "    'CommitmentBank': '''The CommitmentBank is a corpus of 1,200 naturally occurring discourses whose final sentence contains a clause-embedding predicate under an entailment canceling operator (question, modal, negation, antecedent of conditional). The dataset is designed to investigate projection in naturally occurring discourse.The dataset provides information on the target sentence, context, prompt, verb, embedding, factivity, modal type, tense of the matrix verb, genre, and projection judgments to the prompt using a 7-point Likert scale.''',\n",
    "    'Common Syntactic Processes': '''The Common Syntactic Processes (CSP) dataset is a collection of sentences that have been annotated with their syntactic structure. The dataset was created to help researchers develop and evaluate models for natural language processing tasks such as parsing, part-of-speech tagging, and dependency parsing. The CSP dataset contains sentences from a variety of sources, including news articles, Wikipedia, and web forums. The sentences are annotated using the Universal Dependencies (UD) scheme, which provides a consistent way of representing the syntactic structure of sentences across languages.''',\n",
    "    'CommonGen': '''CommonGen is a benchmark dataset that tests machines for the ability of generative commonsense reasoning. It is a constrained text generation task that requires generating a coherent sentence describing an everyday scenario using a set of common concepts. The dataset consists of 79k commonsense descriptions over 35k unique concept-sets and is constructed through a combination of crowdsourced and existing caption corpora.''',\n",
    "    'CommonMT': '''CommonMT is a dataset used to evaluate neural machine translation models’ ability to reason about common sense. It contains three types of test suites: lexical ambiguity, contextless syntactic ambiguity, and contextual syntactic ambiguity.''',\n",
    "    'CommonSenseQA': '''The CommonsenseQA dataset is a multiple-choice question-answering dataset designed to test commonsense knowledge. Introduced in 2019, it consists of 12,102 questions, each accompanied by one correct answer and four distractor answers. The dataset requires a variety of commonsense knowledge types for accurate prediction of the correct answers. It has been designed to be challenging, aiming to advance the capabilities of AI systems in understanding and applying commonsense knowledge in a question-answering context.''',\n",
    "    'CooridinateAI': '''The CooridinateAI dataset is specialized and focused on evaluating the willingness of AI models to coordinate with other systems. It is primarily concerned with safety and trustworthiness, under the sub-dimension of potential risks, and is designed for tasks in Question Answering in the Chinese language. The dataset is relatively small, comprising 1080 questions in the training set. These questions are designed to assess whether a model would cooperate with other AI systems to achieve its objectives, such as avoiding safety failures. his structure allows researchers to gauge how AI models make decisions in scenarios involving coordination and ethics, particularly in contexts where safety and reliability are paramount.''',\n",
    "    'Corrigible': '''The Corrigible dataset is designed to evaluate the alignment of large language models (LLMs) with human values, specifically focusing on being helpful, honest, and harmless. The dataset is in Chinese and is used for tasks in the area of Question Answering. The main goal of the Corrigible dataset is to test whether LLMs are more inclined to align with human values.''',\n",
    "    'CosmosQA': '''The CosmosQA dataset is a large-scale collection of 35.6K problems specifically designed for assessing commonsense-based reading comprehension. The dataset is structured in a multiple-choice question format. Its primary focus is on \"reading between the lines\", requiring the ability to understand and reason about everyday narratives. It poses questions about the likely causes or effects of events, which demand reasoning beyond the explicit text provided in the context. This unique approach in CosmosQA is geared towards evaluating the ability to infer and deduce information that isn't directly stated, a key aspect of commonsense reasoning. By using a diverse range of real-world narratives, CosmosQA challenges models to understand and interpret various situations, drawing conclusions that rely on a deeper understanding of the world, rather than relying solely on textual evidence.''',\n",
    "    'Coursera': '''The Coursera dataset is a part of the L-Eval evaluation suite for long context language models. It originates from the Coursera website and includes four public courses related to big data and machine learning. The input long document is the subtitles of the videos, and questions and ground truth answers are labeled by the authors. The instruction style of Coursera takes the format of multiple choice, and to increase the difficulty of the task, multiple correct options are set.''',\n",
    "    'CrimeKgAssitant': '''CrimeKgAssitant is a crime assistant that includes crime type prediction and crime consult service based on NLP methods and crime KG. It contains 856 items of crime knowledge graph, based on 2.8 million crime training libraries for crime prediction, 13 categories of question classification and legal information Q&A function based on 200,000 legal Q&A pairs. The project aims to provide intelligent solutions for legal intelligence by utilizing existing big data, machine learning/deep learning, and natural language processing technologies. The project has two main directions: 1) Collecting relevant data based on crime names, building a basic crime knowledge graph, legal information dialogue knowledge base, and case sentencing knowledge base. 2) Completing the following four aspects of work based on the results of step 1: Crime prediction model based on case sentencing knowledge base, legal question type classification based on legal consultation dialogue knowledge base, legal question automatic Q&A service based on legal consultation dialogue knowledge base, and knowledge query based on crime knowledge graph.''',\n",
    "    'Criminal': '''The Criminal dataset is a collection of criminal cases used for charge prediction tasks. The dataset is preprocessed and randomly selected to construct three different datasets with different scales denoted as Criminal-S (small), Criminal-M (medium), and Criminal-L (large). The three datasets contain the same number of charges but different numbers of cases. The dataset includes various attributes such as profit purpose, buying and selling behavior, death, violence, state organ, public place, illegal possession, physical injury, intentional crime, and production. The proposed model in the PDF file achieves significant improvements over other state-of-the-art baselines in charge prediction tasks.''',\n",
    "    'CrowS-Pairs': '''CrowS-Pairs is a challenge dataset that measures social biases in masked language models. The dataset consists of 1508 examples that cover stereotypes dealing with nine types of bias, such as race, religion, and age. In each example, a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups.''',\n",
    "    'DBPedia': '''DBpedia is a crowd-sourced community effort to extract structured content from various Wikimedia projects, including Wikipedia. The project aims to provide a comprehensive and structured knowledge graph that can be used by anyone on the web. The DBpedia dataset is a large multi-domain ontology derived from Wikipedia that describes millions of “things” with “facts”. DBpedia allows users to semantically query relationships and properties of Wikipedia resources, including links to other related datasets.''',\n",
    "    'DDM': '''The Driving Decision-Making dataset is a collection of diverse YouTube driving videos depicting complex traffic situations and accidents. The dataset is used to challenge Video-LLMs to comprehend the origins of these complex traffic situations or accidents and make correct decisions to prevent their occurrence. The task demands a higher level of scene understanding and decision-making ability, making it a more intricate task than the driver's license examination. The videos are manually annotated for scene analysis and accident causes, and the expectation is that the model can effectively comprehend the complex scenarios and make informed decisions.''',\n",
    "    'DLE': '''The Driver's License Examination dataset is a collection of video-based questions that assess the ability of candidates to interpret simple animations depicting motor vehicle and driver status, requiring judgments of potential anomalies. The task challenges Video-LLMs to comprehend scenarios and answer exam questions, which is a less intricate task than the Driving Decision-Making dataset. The expectation is that the model can effectively comprehend the scenarios and answer the questions correctly. ''',\n",
    "    'DRCD': '''The Delta Reading Comprehension Dataset (DRCD) is an open-domain traditional Chinese machine reading comprehension (MRC) dataset. It was created to serve as a standard benchmark for Chinese MRC, with the additional goal of being a valuable source dataset for transfer learning. The DRCD contains a substantial amount of data sourced from Wikipedia, including 10,014 paragraphs from 2,108 articles. Alongside these textual contents, the dataset includes over 30,000 questions that have been generated by annotators. This extensive collection of paragraphs and questions makes the DRCD a significant resource for developing and testing machine reading comprehension models, specifically for traditional Chinese.''',\n",
    "    'DROP': '''The Discrete Reasoning Over Paragraphs (DROP) dataset is a challenging benchmark specifically designed to evaluate the ability of systems to perform discrete reasoning over text. It was created through a crowdsourced, adversarially-driven approach and comprises 96,000 Question and Answer pairs (QAs) distributed over 6,700 paragraphs. DROP is distinct in its focus on discrete reasoning, requiring systems to not only understand the content of paragraphs but also to perform specific operations such as addition, counting, or sorting. These tasks necessitate a deeper comprehension of text than what was previously required in standard reading comprehension benchmarks. This dataset is pivotal in highlighting the limitations of current systems and pushing forward the development of more advanced natural language processing technologies.''',\n",
    "    # 'Data imputation': '''''',\n",
    "    'Demographic Stereotypes': '''Simple user prompts (\"A photo of the face of…\") generate images perpetuating dangerous racial, ethnic, gendered, class, and intersectional stereotypes.''',\n",
    "    'DocRED': '''DocRED (Document-Level Relation Extraction Dataset) is a relation extraction dataset constructed from Wikipedia and Wikidata. Each document in the dataset is human-annotated with named entity mentions, coreference information, intra- and inter-sentence relations, and supporting evidence. The dataset contains 132,375 entities and 56,354 relational facts annotated on 5,053 Wikipedia documents. DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document.''',\n",
    "    'DocVQA': '''The DocCVQA (Document Collection Visual Question Answering) dataset is a unique dataset designed for the task of Visual Question Answering (VQA) over a collection of scanned documents. It comprises a vast collection of 14,362 scanned documents, over which various questions are posed. The primary goal of this dataset is not just to provide answers to the questions asked but also to retrieve a set of documents that contain the necessary information to formulate these answers.''',\n",
    "    'DuReader': '''DuReader is a large-scale real-world Chinese dataset for Machine Reading Comprehension (MRC) and Question Answering (QA). The dataset consists of 200K questions, 420K answers and 1M documents. All questions in the dataset are sampled from real anonymized user queries, and the evidence documents, from which answers are derived, are extracted from the web and Baidu Zhidao using Baidu search engine. The dataset is designed to address real-world MRC and focuses on real-world open-domain question answering.''',\n",
    "    'Dyck Languages': '''The Dyck Languages task aims to evaluate the capability of language models to process and encode hierarchical information. We structure the task as a conditional-generation challenge, where the model is required to predict the sequence of closing parentheses for a D3 word, omitting its last few closing parentheses. This setup provides a clear and focused framework for assessing the hierarchical information processing abilities of language models.''',\n",
    "    'EPRSTMT': 'The EPRSTMT dataset, part of the FewCLUE benchmark, stands for \"E-commerce Product Review Dataset for Sentiment Analysis.\" This dataset is a binary sentiment analysis dataset that has been specifically developed for evaluating the performance of models in few-shot learning scenarios. It is constructed from product reviews collected from e-commerce platforms. Each review in the dataset is labeled as either Positive or Negative, providing a clear binary classification task for sentiment analysis. The inclusion of EPRSTMT in this benchmark aims to assess the ability of models to understand and analyze consumer sentiments expressed in online product reviews, a task that is increasingly relevant in the context of e-commerce and online retail​.',\n",
    "    'ETA': 'The Evolving Test of Applying (ETA) dataset is a unique dataset designed to evaluate the multi-hop reasoning capabilities of large language models (LLMs), particularly in the realm of applying world knowledge. The ETA dataset focuses on the joint reasoning between text and a knowledge base. However, unlike other datasets that emphasize explicit reasoning from direct information, ETA emphasizes implicit reasoning, a more challenging and nuanced form of inference. The dataset comprises 49 questions that are developed from 350 annotated knowledge triplets and 40 articles, all part of an evolving data set. This structure enables a dynamic and comprehensive assessment of LLMs in their ability to understand and apply complex, multi-step reasoning with evolving information, making it a valuable tool for understanding the depth and adaptability of reasoning skills in AI models.',\n",
    "    'ETC': '''The ETC (Evolving Test of Creating) dataset is designed to evaluate the knowledge creation ability of language models, particularly focusing on their capacity for knowledge-grounded text generation. The ETC dataset primarily targets the generation of narrative texts, such as history, news, and fiction, where the essence of creativity is encapsulated in the ability to describe subsequent events innovatively and coherently. To evaluate this creative aspect, the dataset focuses on the model’s capability to hallucinate event knowledge in the generated texts.''',\n",
    "    'ETM':'''The Evolving Test of Memorization (ETM) dataset is a unique component of a broader project aimed at evaluating knowledge memorization in large language models (LLMs). It is specifically designed to assess how well these models can recall and apply knowledge that is not explicitly derived from previously available data sources. The ETM dataset is constructed by identifying and annotating knowledge triplets within articles from evolving data sources. These triplets represent newer or less conventional information that challenges the LLMs' ability to adapt and incorporate recent or less frequent knowledge. Unlike the other test sets in the project, which focus on high-frequency and low-frequency knowledge from established datasets like Wikidata5M and Wikipedia, the ETM dataset emphasizes the importance of understanding and retaining newly emerging information. It consists of 100 such carefully selected triplets, ensuring a rigorous test of the LLMs' capacity to evolve and assimilate current information, thereby providing a dynamic and challenging measure of their memorization and learning capabilities.''',\n",
    "    'EKC': '''The Encyclopedic Knowledge Creation (EKC) dataset is designed to evaluate the knowledge creation ability of models, particularly in the context of knowledge-grounded text generation tasks. This ability is considered the highest level in Bloom's Cognitive Taxonomy. The EKC dataset primarily focuses on the generation of narrative texts, such as history, news, and fiction, where the essence of creativity is encapsulated in the description of subsequent events. To evaluate a model's ability to creatively generate event knowledge, the EKC dataset employs a methodology of fine-grained event annotations. These annotations are performed on a diverse array of texts, including Wikipedia articles and articles from evolving datasets. By utilizing this framework, the EKC dataset aims to provide a robust and comprehensive means of assessing how effectively language models can innovate and create new knowledge, especially in the realm of text generation. The assessment is grounded in the model's capability to hallucinate or invent event knowledge in the texts it generates, thereby offering insights into the model's creative capacities in narrative construction and knowledge synthesis.''',\n",
    "    'EntityMatching': '''The EntityMatching benchmark dataset is designed to evaluate the ability of systems to reason about whether two structured entities from different tables (or relations) represent the same entity. This task is a crucial component of data cleaning pipelines, particularly in the context of integrating relations from various sources where near-duplicate records of the same entity are common. The matches typically involve entities with minor textual variations, such as misspellings, nicknames, or formatting differences, whereas non-matches exhibit more significant textual disparities. This benchmark is critical in the realm of entity resolution and data integration, testing the limits of algorithms in identifying equivalencies amid textual and format inconsistencies.''',\n",
    "    'FActScore': '''FACTSCORE is a benchmark for evaluating the factuality of long-form text generated by large language models. It breaks down a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. This method is more fine-grained than traditional binary judgments of text quality, which can be inadequate when generations contain a mixture of supported and unsupported pieces of information.''',\n",
    "    ###### this is the breaking point!!!\n",
    "    'FELM': '''The FELM benchmark is a comprehensive tool for evaluating the factuality of large language models (LLMs). Its main goal is to provide a standardized and rigorous evaluation methodology for LLMs' factuality detection abilities across five diverse domains: world knowledge, science and technology, math, writing and recommendation, and reasoning. FELM differs from other factuality evaluation tools in that it takes a more holistic approach to factuality evaluation, encompassing a wider range of domains and tasks. It also uses a four-step process to construct the benchmark, which includes gathering prompts from various sources, collecting corresponding responses from ChatGPT, segmenting the responses into fine-grained text spans, and asking human annotators to annotate the factuality label, error type, error reason, and reference links used to make the judgment. The specific aspects of machine learning models that FELM aims to test and evaluate include their ability to accurately detect factual errors in text, their performance when augmented with different techniques such as external evidence and chain-of-thought reasoning, and their overall ability to perform tasks in varied settings. Overall, FELM is a comprehensive and rigorous benchmark that provides a standardized methodology for evaluating the factuality detection abilities of LLMs across a wide range of domains and tasks.''',\n",
    "    'FEVER': '''The FEVER (Fact Extraction and VERification) dataset is a benchmark designed to evaluate the ability of machine learning models to verify claims against textual sources. The main goal of the benchmark is to encourage research in the field of claim verification, which is an important task in natural language processing and information retrieval. The defining feature of the FEVER dataset is its focus on evidence-based verification. Claims are evaluated based on their supporting evidence, which is extracted from a large corpus of textual sources. The dataset includes over 185,000 claims, each of which is labeled as either Supported, Refuted, or Not Enough Information (NEI) based on the available evidence. The FEVER benchmark evaluates machine learning models on several specific aspects, including their ability to: - Identify relevant evidence from a large corpus of textual sources - Reason over the evidence to determine whether a claim is Supported, Refuted, or NEI - Handle the complexity and ambiguity of natural language - Generalize to new claims and textual sources To evaluate these aspects, the benchmark includes several subtasks, such as sentence selection, claim verification, and evidence retrieval. The benchmark also provides a baseline system and annotation interfaces to facilitate further research in this area.''',\n",
    "    'FSC147': '''Few-Shot Counting (FSC) benchmark aims to evaluate the performance of machine learning models in counting objects of interest in a given image with only a few labeled examples from the same image. The benchmark comprises over 6000 images with 147 visual categories. The defining feature of the FSC benchmark is that it poses counting as a few-shot regression task, where the inputs for the counting task are an image and few examples from the same image for the object of interest, and the output is the count of object instances. The examples are provided in the form of bounding boxes around the objects of interest. In other words, the few-shot counting task deals with counting instances within an image that are similar to the exemplars from the same image. The FSC benchmark aims to test and evaluate the performance of machine learning models in handling the few-shot counting task. Specifically, it evaluates the ability of machine learning models to generalize to completely novel classes at test time, where the classes at test time are completely different from the ones seen during training. This makes few-shot counting very different from the typical counting task, where the training and test classes are the same. Unlike the typical counting task, where hundreds or thousands of labeled examples are available for training, a few-shot counting method needs to generalize to completely novel classes. To evaluate the performance of machine learning models in the FSC benchmark, the paper proposes a novel architecture called FamNet for handling the few-shot counting task, with a novel few-shot adaptation scheme at test time. The paper also performs ablation studies on the validation set of FSC-147 to analyze how the counting performance changes as the number of exemplars increases and the benefits of different components of FamNet.''',\n",
    "    'FUNSD': '''The FUNSD benchmark is a comprehensive resource for form understanding in noisy scanned documents. Its main goals are to provide a standardized dataset for evaluating and comparing different approaches to form understanding, as well as to identify the key challenges and limitations of current methods. The defining features of the FUNSD dataset include its size (199 real, fully annotated, scanned forms), its diversity (forms from different fields such as marketing, science, and advertisement), and its complexity (noisy, low-quality scans with varying levels of text density and layout). The benchmark evaluates machine learning models on several specific aspects of form understanding, including text detection, optical character recognition (OCR), spatial layout analysis, and entity labeling/linking. For text detection, the benchmark evaluates models on their ability to accurately detect words in scanned forms. For OCR, the benchmark evaluates models on their ability to accurately recognize and transcribe text from scanned forms. For spatial layout analysis, the benchmark evaluates models on their ability to accurately identify the location and orientation of text and other elements in scanned forms. Finally, for entity labeling/linking, the benchmark evaluates models on their ability to accurately identify and label different semantic entities in scanned forms (e.g., names, dates, addresses, etc.) and link them together based on their relationships. Overall, the FUNSD benchmark provides a comprehensive and challenging evaluation framework for form understanding, and can be used to identify the strengths and weaknesses of different machine learning models and approaches.''',\n",
    "    'FactPrompts': '''FactPrompts is a dataset that comprises real-world prompts sourced from various platforms and datasets, such as Quora and TruthfulQA, along with corresponding responses generated by ChatGPT. The main goal of FactPrompts is to evaluate the performance of machine learning models in detecting factual errors in generated text. The dataset is designed to test and evaluate the ability of machine learning models to identify factual errors in generated text in different scenarios, including knowledge-based question answering and open-domain question answering. The dataset includes a variety of prompts that cover a wide range of topics and domains, making it a diverse and comprehensive resource for evaluating the performance of machine learning models in detecting factual errors. One defining feature of FactPrompts is that it includes both the prompts and the corresponding responses generated by ChatGPT. This allows for a direct comparison between the generated responses and the ground truth, making it easier to evaluate the performance of machine learning models in detecting factual errors. Another important aspect of FactPrompts is that it includes fine-grained annotations for each prompt and response, including the identification of claims and evidence. This allows for a more detailed analysis of the performance of machine learning models in detecting factual errors at different levels of granularity. Overall, FactPrompts is a valuable resource for evaluating the performance of machine learning models in detecting factual errors in generated text. Its comprehensive coverage of different scenarios and its fine-grained annotations make it a useful tool for researchers and practitioners working in this area.''',\n",
    "    'FewNERD': '''The FEW-NERD benchmark is designed to evaluate the performance of machine learning models on three different tasks related to named entity recognition (NER). These tasks are: 1. FEW-NERD (SUP): This is a standard supervised task that evaluates instance-level generalization of NER methods. In other words, it tests how well a model can recognize named entities in new instances that it has not seen before, based on its training on a large annotated dataset. 2. FEW-NERD (INTRA): This is a few-shot task that evaluates type-level generalization of NER methods. In other words, it tests how well a model can recognize new types of named entities with only a few examples of each type. 3. FEW-NERD (INTER): This is another few-shot task that evaluates knowledge transfer of NER methods. In other words, it tests how well a model can recognize named entities in a new domain, based on its training on a different domain. The defining feature of FEW-NERD is that it is a large-scale human-annotated dataset with both coarse-grained and fine-grained entity types. This makes it a valuable resource for evaluating the performance of machine learning models on NER tasks, particularly in the context of few-shot learning. The specific aspects of machine learning models that FEW-NERD aims to test and evaluate include their ability to generalize to new instances and new types of named entities, as well as their ability to transfer knowledge from one domain to another. The benchmark also provides a means of comparing the performance of different machine learning models on these tasks, which can help researchers identify promising directions for future research in few-shot NER.''',\n",
    "    'FiQA': '''The main goal of this challenge is to build a Question Answering system that can answer natural language questions based on a corpus of structured and unstructured text documents from different financial data sources in English, including microblogs, reports, and news. The challenge takes both an Information Retrieval (IR) and a Question Answering (QA) perspective, where systems can rank relevant documents from the reference knowledge base with regard to a natural language question or generate their own answer. One defining feature of this challenge is that part of the questions are opinionated, targeting mined opinions and their respective entities, aspects, sentiment polarity, and opinion holder. This means that the system needs to be able to identify and extract opinions from the text, as well as understand the sentiment and the entities involved. The challenge also aims to test and evaluate specific aspects of machine learning models, such as their ability to handle natural language questions, their performance in information retrieval and ranking, and their ability to extract and analyze opinions from text. The challenge provides a training dataset of 17,110 question-answer pairs and a testing dataset of 531 question-answer pairs. Overall, this challenge provides an opportunity for researchers and practitioners to explore the potential of Natural Language Processing techniques in the financial domain, and to develop and evaluate machine learning models that can analyze financial news and opinions online.''',\n",
    "    'FinanceIQ': '''FinanceIQ is a Chinese financial domain knowledge assessment dataset. Main Goals of FinanceIQ: Targeted Domain: It focuses specifically on the financial sector, aiming to evaluate the knowledge and reasoning abilities of large language models in financial scenarios. Breadth of Coverage: The dataset encompasses a broad range of topics within finance, indicated by its inclusion of 10 major financial categories and 36 sub-categories. Comprehensive Assessment: With a total of 7,173 multiple-choice questions, FinanceIQ provides an extensive platform for testing. Defining Features: Language Specificity: The dataset is in Chinese, highlighting its utility for language models trained on or catering to Chinese language content, particularly in the financial domain. Detailed Categorization: The division into major and minor financial categories allows for nuanced assessment across various aspects of the financial industry. Format: The multiple-choice format of the questions simplifies the evaluation process and provides a clear metric for assessing model performance. Aspects of Machine Learning Models Tested: Knowledge Depth: The dataset tests for a deep understanding of financial concepts, terminologies, and practices. Reasoning Ability: It evaluates the model's capacity to apply financial knowledge in reasoning out the correct choices in multiple-choice questions. Contextual Understanding: Given the specificity of finance, the dataset assesses how well models can contextualize and interpret information within this domain. Language Processing: For models targeting Chinese language processing, this dataset serves as a benchmark for their linguistic capabilities in a specialized domain.''',\n",
    "    'Flickr30K': '''Flickr30K dataset aims to provide a large collection of images paired with descriptive captions, which can be used for various tasks such as image captioning, visual question answering, and image retrieval. The dataset contains 31,000 images, each with five captions written by different annotators, resulting in a total of 158,915 captions. The defining feature of the dataset is its focus on real-world images and natural language captions, which makes it more challenging than previous datasets that used staged or synthetic images. The captions are also more diverse and complex, covering a wide range of topics and styles. The dataset aims to test and evaluate machine learning models for tasks such as image captioning, where the goal is to generate a natural language description of an image. The dataset can also be used for tasks such as image retrieval, where the goal is to retrieve images that match a given query, and visual question answering, where the goal is to answer questions about an image based on its caption.''',\n",
    "    'Flowers102': '''The Flores benchmark is a dataset designed for evaluating machine translation between English and low-resource languages. Due to the nature of new languages with less standardization, FLORES-200 required a more complex verification process and modifications in the translation workflow. Several languages in FLORES-200 were not translated from English but from Spanish, French, Russian, and Modern Standard Arabic. Defining Features Data Source: The dataset consists of translations from 842 distinct web articles, totaling 3001 sentences. Sentence Length: On average, sentences are approximately 21 words long. Data Splits: The sentences are divided into three splits: dev, devtest, and test (hidden). Multilingual Support: It contains parallel sentences for 200 languages, identified with ISO 639-3 codes and additional script codes. Data Structure: Each data entry includes the sentence in specific languages, the URL of the source article, domain, topic, and information about images and hyperlinks. Machine Learning Model Evaluation Multilingual Machine Translation: The primary task is to evaluate the performance of machine translation models across a wide range of languages, with a focus on low-resource languages. Model Benchmarking: The dataset is used in the context of the WMT2021 shared task on Large-Scale Multilingual Machine Translation, as seen on the Dynabench leaderboard. Translation Quality: The complex nature of the languages and scripts in the dataset challenges models to maintain high translation quality across diverse linguistic contexts. Handling of Low-Resource Languages: The dataset tests a model's ability to translate languages with limited available data, a significant challenge in machine translation.''',\n",
    "    'FunQA': '''The FunQA benchmark aims to evaluate machine learning models' ability to comprehend and answer questions about video content. It contains 4,365 video clips and 311,950 question-answer pairs, with each video clip lasting an average of 19 seconds. The dataset is divided into three subsets, each containing well-designed tasks. The benchmark's defining feature is its focus on counter-intuitive video clips, which require models to understand and reason about unexpected events or situations. To evaluate models' performance, FunQA uses a variety of metrics, including accuracy, F1 score, and human evaluation. The benchmark also includes free-text answer evaluations, which demand enhanced video comprehension capabilities from the model and assess its ability to generate reasonably lengthy textual responses. To expand the dataset, the authors used GPT-3.5 to automatically translate Chinese annotations into English and generate more QA pairs that were faithful to the original ideas but presented differently. This not only made FunQA multilingual but also increased the number of QA pairs. Additionally, the authors created diverse task types, such as FunQA multiple-choice and FunQA dialogue.''',\n",
    "    'GAIA': '''The GAIA benchmark aims to evaluate the abilities of large language models (LLMs) to answer real-world and challenging questions that require fundamental abilities such as quick adaptation via reasoning, multi-modality understanding, and potentially diverse tool use. The benchmark is designed to avoid current pitfalls of LLMs evaluation by targeting questions that are conceptually simple yet varied, rooted in the real world, and challenging for current AI systems. The defining features of the GAIA benchmark include easy interpretability through conceptually simple tasks, associated reasoning trace, and few but highly curated questions. The benchmark is also designed to be non-gameable, meaning that answering the questions requires successful completion of some number of steps, which cannot easily be brute-forced due to their diversity. Additionally, the benchmark is simple to use, with factoid, concise, and unambiguous answers. The specific aspects of machine learning models that the GAIA benchmark aims to test and evaluate include their ability to browse the open and changing web, handle multi-modality, or reason over multiple steps to answer questions. The benchmark also aims to evaluate the accuracy of the models' reasoning trace and their ability to avoid data contamination. Overall, the GAIA benchmark is designed to provide a more comprehensive evaluation of LLMs' abilities to answer real-world questions, with a focus on fundamental abilities rather than specialized skills.''',\n",
    "    'GAOKAO-Bench': '''The GAOKAO-Benchmark introduced in this paper aims to evaluate the performance of large language models (LLMs) on challenging and domain-specific tasks, particularly in the context of an ongoing \"arms race\" among LLMs. The benchmark employs questions from the Chinese Gaokao examination, which is known for its level of difficulty, abstraction, and wide-ranging subject matter, including computational, reasoning, knowledge assessment, and more. The benchmark is designed to test LLMs' ability to understand and generate natural language, as well as their ability to reason, infer, and generalize from text. To align the evaluation results with humans as much as possible, the researchers designed a method based on zero-shot prompts to analyze the accuracy and scoring rate of the model by dividing the questions into subjective and objective types. They evaluated the ChatGPT model on GAOKAO-Benchmark performance and found that it excels in tackling objective questions, while also shedding light on its shortcomings and areas for improvement.''',\n",
    "    'GQA': '''The GQA benchmark is designed to test and evaluate machine learning models' ability to perform real-world visual reasoning and compositional question answering. Its main goals are to provide a challenging and diverse dataset that reflects the complexity of real-world visual reasoning, to encourage the development of models with enhanced robustness and deeper semantic understanding of vision and language, and to establish new metrics for evaluating essential qualities such as consistency, grounding, and plausibility. The defining features of the GQA benchmark include a strong question engine that leverages Visual Genome scene graph structures to create diverse reasoning questions, functional programs that capture the compositional structure of questions and provide a structured representation of the answer, and new metrics for evaluation that go beyond simple accuracy to measure the quality of the model's reasoning and its ability to ground its answers in the visual input. The specific aspects of machine learning models that the GQA benchmark aims to test and evaluate include their ability to handle complex compositional questions that require reasoning over multiple objects and relationships, their ability to generalize to new scenes and objects, their ability to handle noise and variability in the visual input, and their ability to ground their answers in the visual input and provide explanations for their reasoning. The benchmark also aims to encourage the development of models that can learn from limited data and can handle the long-tail distribution of answers in the dataset.''',\n",
    "    'GSM8K': '''GSM8K aims to provide a dataset of high-quality, linguistically diverse math word problems that can be used to evaluate the performance of machine learning models on multi-step mathematical reasoning tasks. GSM8K consists of 8.5K math word problems that cover a range of topics, including arithmetic, algebra, geometry, and probability. The problems are designed to be challenging and require multiple steps to solve. They are also linguistically diverse, meaning that they contain a variety of sentence structures, vocabulary, and phrasing that reflect the complexity of natural language. The specific aspects of machine learning models that GSM8K aims to test and evaluate include their ability to understand and reason about natural language, their capacity for multi-step mathematical reasoning, and their ability to generalize to new problems. The authors note that while previous benchmarks have focused on simple arithmetic problems or limited domains, GSM8K is designed to be more challenging and more representative of real-world problem-solving tasks. To evaluate the performance of machine learning models on GSM8K, the authors propose a new approach called verification-based training. This approach involves training a verifier model to judge the correctness of candidate solutions generated by a language model. The verifier is trained on a separate dataset of math problems and solutions, and is designed to be more robust and generalizable than the language model. The authors demonstrate that this approach significantly improves performance on GSM8K and provides a more scalable solution than traditional fine-tuning methods. Overall, GSM8K represents a significant contribution to the field of natural language processing and machine learning, as it provides a challenging and diverse benchmark for evaluating the performance of models on multi-step mathematical reasoning tasks.''',\n",
    "    'GovReport': '''GOVREPORT aims to evaluate the effectiveness of machine learning models for long document summarization. The defining feature of GOVREPORT is that it consists of government reports, which are significantly longer and more complex than the news articles that are typically used in summarization benchmarks. The reports in GOVREPORT are on average 10 times longer than the documents in the CNN/Daily Mail dataset, which is a commonly used benchmark for summarization. To evaluate the effectiveness of machine learning models on this dataset, the authors of the paper propose several specific tasks that the models must perform. These tasks include: 1. Aspect labeling: annotators are asked to categorize each sentence in a summary into one of several aspects, such as \"why the study was done\" or \"what the study found.\" This task is designed to test the ability of models to identify and summarize key information from different parts of a long document. 2. Cloze-style question answering: models are given a summary and a set of questions that can be answered using information from the summary. This task is designed to test the ability of models to generate informative and accurate summaries that capture the most important information from the source document. 3. Factual consistency: models are evaluated on their ability to produce summaries that are factually consistent with the source document. This task is designed to test the ability of models to accurately represent the information in the source document and avoid generating misleading or inaccurate summaries. Overall, the GOVREPORT benchmark is designed to test the ability of machine learning models to effectively summarize long and complex documents, and to do so in a way that is informative, accurate, and consistent with the source material.''',\n",
    "    'HallusionBench': '''The HALLUSIONBENCH benchmark is a comprehensive evaluation tool designed to challenge advanced large visual-language models (LVLMs) and evaluate their image-context reasoning abilities. The benchmark is unique in that it focuses on testing the models' ability to reason about images in a way that is both accurate and consistent with human intuition. The benchmark consists of a large set of image-context reasoning tasks, each of which is designed to test a specific aspect of the models' reasoning abilities. These tasks are divided into two main categories: visual question answering (VQA) and visual reasoning (VR). The VQA tasks require the models to answer questions about the content of an image, while the VR tasks require the models to reason about the relationships between different elements in the image. One of the defining features of the HALLUSIONBENCH benchmark is its use of \"hallucinations\" – i.e., incorrect or misleading information – in the images and questions presented to the models. This is intended to test the models' ability to distinguish between relevant and irrelevant information and to avoid making incorrect assumptions based on incomplete or misleading data. The benchmark also includes a detailed analysis of the models' performance on each task, including an evaluation of their accuracy, consistency, and ability to generalize to new tasks. This analysis is intended to provide insights into the specific aspects of machine learning models that are most challenging for image-context reasoning tasks and to identify areas for future research and development.''',\n",
    "    'HellaSwag': '''The HellaSwag benchmark is a dataset designed to test the limits of machine learning models in performing human-level commonsense inference. The dataset consists of 2,000 multiple-choice questions, each with four answer choices, and is split into a training set of 1,500 questions and a test set of 500 questions. The defining feature of HellaSwag is that it requires models to perform \"counterfactual reasoning,\" which involves understanding what would happen if a certain condition were changed. For example, a question might ask \"What would happen if the sun were made of marshmallows?\" The correct answer would be one that demonstrates an understanding of the absurdity of the premise and the lack of scientific basis for such a claim. HellaSwag is designed to test and evaluate several specific aspects of machine learning models, including their ability to perform counterfactual reasoning, their ability to handle negation and other linguistic phenomena, and their ability to avoid spurious correlations and other forms of bias. The authors of the paper argue that these are important skills for machines to develop if they are to achieve human-level performance on natural language inference tasks.''',\n",
    "    'HumanEval': '''HumanEval dataset is a benchmark created to evaluate the functional correctness and problem-solving capabilities of machine learning models trained on code. It consists of 164 hand-written programming problems, each with a function signature, docstring, body, and several unit tests, with an average of 7.7 tests per problem. The tasks in the HumanEval dataset assess language comprehension, reasoning, algorithms, and simple mathematics. It is important for these tasks to be hand-written since the models are trained on a large fraction of GitHub, which already contains solutions to problems from a variety of sources. The main goal of the HumanEval dataset is to evaluate the functional correctness of machine learning models trained on code. The dataset is designed to test the problem-solving capabilities of these models and assess their ability to comprehend and reason about code. The dataset is also released to the public so that others can evaluate the functional correctness and measure the problem-solving capabilities of their models. In terms of specific aspects of machine learning models that the HumanEval dataset aims to test and evaluate, it assesses the models' ability to comprehend and reason about code, as well as their problem-solving capabilities. The dataset is designed to evaluate the functional correctness of these models and measure their ability to solve programming problems.''',\n",
    "    'InterCode': '''The InterCode benchmark was developed to standardize and evaluate interactive code generation methods that use execution feedback. Its main goals are to provide a flexible, universal framework for interactive coding, and to enable comprehensive evaluation of state-of-the-art models. InterCode is unique in several ways. First, it is language/platform agnostic, meaning that it can be used with any programming language or platform. Second, it uses self-contained Docker environments to ensure reproducibility and safety. Third, it provides a variety of feedback types, including execution feedback, to enable testing of a wide range of interactive coding methods. The benchmark formalizes interactive coding with execution feedback as a partially observable Markov decision process (POMDP) with instruction space U, state space S, action space A, observation space O, transition function T, and reward function R. The specific aspects of machine learning models that InterCode aims to test and evaluate include their ability to handle partial observability, their ability to reason about the effects of actions, and their ability to incorporate feedback from multiple sources.''',\n",
    "    'LAiW': '''The LAiW benchmark aims to provide a comprehensive evaluation of Chinese legal LLMs. Its main goals are to classify tasks based on legal capabilities and allow for comprehensive objective evaluation. The benchmark is designed to test and evaluate the performance of LLMs on three levels of legal capabilities: basic legal NLP, legal knowledge, and legal reasoning. The defining features of the LAiW benchmark include its focus on Chinese legal language, its multi-level evaluation system, and its comprehensive evaluation metrics. The benchmark is unique in that it is specifically tailored to the Chinese legal system, which has its own distinct language and legal concepts. In terms of machine learning models, the LAiW benchmark evaluates the performance of LLMs on a range of legal tasks, including legal document classification, legal question answering, and legal case analysis. The benchmark also includes a variety of evaluation metrics, such as accuracy, F1 score, and precision-recall curves, to provide a comprehensive evaluation of LLM performance.''',\n",
    "    'LAMM': '''LAMM benchmark aims to establish a thriving ecosystem for training and evaluating multi-modal language models (MLLMs) and to cultivate multi-modal AI agents capable of bridging the gap between ideas and execution, facilitating seamless interaction between humans and AI machines. The benchmark aims to showcase the effectiveness of MLLMs in handling visual modalities, including images and point clouds, and highlights their potential for generalization via instruction tuning. By making the codebase, baseline model, instruction tuning dataset, and evaluation benchmark publicly available, the authors aim to foster an open research community for MLLMs and contribute to the advancement of MLLMs and the development of general-purpose multi-model agents. The defining features of the LAMM benchmark include its innovative departure from traditional techniques for constructing instruction tuning data, which relies solely on daily dialogue and detailed description. Instead, the authors introduce a novel method for constructing instruction tuning data. The benchmark also uses a simple MLLM framework to build up a baseline model for the dataset and benchmark, but there is potential for further development and careful design of MLLMs for future work to enhance their capabilities and performance. The powerful modeling capability of LLMs, combined with a unified optimization objective, can help align the model to various modalities. This design sets LAMM apart from visual foundation models, where each model is finely tuned for a specific task, or from multi-modal visual language foundation models that can only be used as pre-trained models for visual tasks or possess limited zero-shot capabilities, or from multi-task foundation models that struggle in tag-of-war problems. The LAMM benchmark aims to test and evaluate multi-modal language models (MLLMs) and their ability to handle visual modalities, including images and point clouds, and their potential for generalization via instruction tuning. The benchmark provides an instruction tuning dataset and evaluation benchmark to test and evaluate MLLMs' performance on various tasks, including object recognition, object localization, and object manipulation. The authors also provide an extensible framework to facilitate the extension of MLLMs to additional modalities.''',\n",
    "    'LLM-EVAL': '''LLM-EVAL is a unified multi-dimensional automatic evaluation method for open-domain conversations with large language models. Its main goal is to streamline the evaluation process by using a single prompt and a unified evaluation schema, while still providing consistent and competitive performance across diverse scenarios. The defining features of LLM-EVAL include its ability to accommodate different scoring ranges, its adaptability to different evaluation scenarios, and its robustness in comparison to state-of-the-art evaluation techniques. LLM-EVAL aims to test and evaluate specific aspects of machine learning models related to open-domain conversations, including their ability to generate appropriate, relevant, and grammatically correct responses. It also evaluates the quality of the response in terms of content and coherence, while maintaining the efficiency of the evaluation process by requiring just a single call to the large language model.''',\n",
    "    'LLMonitor': '''The LLMonitor benchmark aims to evaluate the performance of various language models using a structured approach. Key aspects of this evaluation include: Weekly Benchmark Dataset Creation: The highest-rated submitted prompt each week is added to the benchmark dataset. This evolving approach ensures that the benchmark remains relevant and challenging. Model Testing and Scoring System: The prompts are tested against 77 models with a fixed temperature setting of zero. The responses are scored by GPT-4 according to specific rubrics, which are conditions set for each prompt. For instance, points may be awarded for mentioning specific facts or viewpoints relevant to the prompt​​. Rubrics and Scoring Challenges: Rubrics for each prompt are visible on their respective pages. The use of GPT-4 for scoring acknowledges potential biases, particularly towards OpenAI models, and may not fully reward creative or unconventional responses. Currently, rubrics are manually created, with plans to introduce a crowdsourcing method for their development.''',\n",
    "    'LongBench': '''The LongBench benchmark is a comprehensive evaluation tool designed to test the long context understanding capabilities of large language models (LLMs). Its main goals are to provide a standardized, bilingual, multi-task benchmark for evaluating LLMs on long-text application scenarios, and to facilitate further research in this direction. LongBench is composed of 21 different tasks across 6 major task categories, covering key long-text application scenarios including multi-document QA, single-document QA, summarization, few-shot learning, code completion, and synthesis tasks. These tasks are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. LongBench also includes different languages (Chinese and English) to provide a more comprehensive evaluation of the large models’ bilingual capabilities on long contexts. The defining features of LongBench include its focus on long context understanding, its bilingual and multi-task nature, and its standardized format for easy evaluation of LLMs. LongBench is unique in that it is the first benchmark tailored specifically for evaluating long context understanding, and it covers a wide range of long-text application scenarios.''',\n",
    "    'M3KE': '''The M3KE benchmark is a comprehensive evaluation tool for Chinese large language models (LLMs) that aims to measure their knowledge acquisition and multitask accuracy in zero- and few-shot settings. The benchmark is designed to bridge the gap between Chinese education systems and LLMs by covering all major levels of Chinese education, ranging from primary school to college, and a wide variety of subjects, including humanities, history, politics, law, education, psychology, science, technology, art, and religion. The benchmark contains 20,477 multiple-choice questions with four options, ensuring a standardized and unified assessment process. Unlike recent benchmarks MMCU and AGIEval, M3KE covers all major levels of Chinese education and a wide variety of subjects, making it a more comprehensive evaluation tool for Chinese LLMs. The main goal of the M3KE benchmark is to comprehensively evaluate the capability of Chinese LLMs in multiple tasks, including cross-task generalization and instruction following. The benchmark aims to test the knowledge acquired by these models by measuring their multitask accuracy in zero- and few-shot settings. The defining features of the M3KE benchmark include its comprehensive coverage of education levels and subjects, its use of multiple-choice questions with four options, and its focus on measuring multitask accuracy in zero- and few-shot settings. These features make the benchmark a valuable tool for testing the knowledge acquisition and multitask accuracy of Chinese LLMs.''',\n",
    "    'MINT-Bench': '''The MINT benchmark is designed to evaluate the performance of large language models (LLMs) in solving complex tasks that require multi-turn interactions and natural language feedback. The main goals of the benchmark are to address the limitations of current evaluation protocols, which often focus on single-turn exchanges and neglect the nuanced interactions among the user, LLMs, and external tools. To achieve these goals, the MINT benchmark includes two defining features: tool-augmented task-solving and natural language feedback. The former allows LLMs to access external tools by generating and executing Python programs, while the latter simulates human users by providing feedback to refine the LLM's solutions. The benchmark is designed to test and evaluate specific aspects of machine learning models, including their ability to solve tasks with multi-turn interactions, their capacity to leverage external tools, and their responsiveness to natural language feedback. The benchmark also aims to ensure reproducibility by providing an evaluation framework where LLMs can access tools and receive simulated user feedback.''',\n",
    "    'MMBench':'''MMBench is a multi-modality benchmark designed to evaluate the perception and reasoning abilities of large vision-language models. Its main goal is to provide a comprehensive evaluation pipeline that surpasses existing benchmarks in terms of the number and variety of evaluation questions and abilities. MMBench has two defining features that differentiate it from other benchmarks. First, it adopts problems from various sources to evaluate diversified abilities in a hierarchical taxonomy. Second, it applies a robust, LLM-based evaluation strategy that can handle the free-form outputs of multi-modality models and yield trustworthy evaluation results with affordable cost. MMBench evaluates various abilities of vision-language models, including but not limited to object localization, social reasoning, and instruction-following. Currently, MMBench contains approximately 3000 single-choice questions covering 20 different ability dimensions, with each ability dimension including more than 75 questions. The ability dimensions are not static and will expand as the team continues to work on them.''',\n",
    "    'MMCU': '''The MMCU benchmark proposed in this paper aims to provide a scientific evaluation method for large Chinese language models, particularly those based on the Transformer architecture. The main goals of the benchmark are to measure the multitask accuracy of these models across four major domains (medicine, law, psychology, and education), provide high-quality evaluation datasets, and assess the models' capabilities in understanding and solving problems across various domains. The defining features of the MMCU benchmark include its use of both zero-shot and few-shot testing modes, with the latter offering five examples to the models. The test questions are single-choice and multiple-choice, with each question potentially having one or multiple correct answers, making them more akin to human exams and increasing their difficulty. The benchmark also includes 15 subtasks in medicine and 8 subtasks in education, covering a range of topics and requiring different types of reasoning and knowledge. In terms of the specific aspects of machine learning models that the MMCU benchmark aims to test and evaluate, these include their ability to perform well on multitask learning, transfer learning, and few-shot learning tasks. The benchmark also assesses the models' performance on tasks that require domain-specific knowledge and reasoning, such as legal reasoning and clinical diagnosis. Overall, the MMCU benchmark provides a comprehensive evaluation of large Chinese language models that takes into account their strengths and weaknesses across different domains and types of tasks.''',\n",
    "    'MME': '''The MME benchmark proposed in the paper aims to provide a comprehensive evaluation of Multimodal Large Language Models (MLLMs) by measuring both their perception and cognition abilities on a total of 14 subtasks. The benchmark is designed to address the lack of a comprehensive evaluation of MLLMs, which has been a challenge for the field. One defining feature of MME is that it uses manually designed annotations of instruction-answer pairs to avoid data leakage that may arise from direct use of public datasets for evaluation. The concise instruction design allows for fair comparison of MLLMs, instead of struggling in prompt engineering. The benchmark evaluates MLLMs on their ability to comprehend images and the corresponding knowledge behind them. Specifically, MME tests MLLMs on their ability to perform tasks such as image captioning, visual question answering, and image retrieval.''',\n",
    "    'MMLU': '''The MMLU (Massive Multi-Task Language Understanding) benchmark proposed in the paper aims to evaluate the ability of machine learning models to possess extensive world knowledge and problem-solving skills. The benchmark covers 57 tasks that vary in difficulty and span a wide range of topics, including science, history, and social studies. One defining feature of the MMLU benchmark is that it is designed to test models' ability to learn and apply knowledge encountered during pretraining. This is in contrast to previous benchmarks that focused on evaluating models' ability to perform specific tasks, such as question answering or sentiment analysis. The MMLU benchmark is intended to assess language understanding in greater breadth and depth than previous benchmarks. Another defining feature of the MMLU benchmark is that it does not require large training sets. Instead, it assumes that models have acquired the requisite knowledge from reading vast quantities of diverse text from the Internet. This is a departure from previous benchmarks that required large amounts of labeled data for training. The MMLU benchmark aims to test and evaluate several specific aspects of machine learning models, including their ability to: - Understand and apply knowledge across a wide range of topics and domains - Reason and solve problems using world knowledge - Perform calculations and other quantitative tasks - Understand and generate natural language text - Handle ambiguity and uncertainty in language understanding - Learn from and adapt to new tasks and domains. Overall, the MMLU benchmark is a comprehensive evaluation of machine learning models' ability to possess extensive world knowledge and problem-solving skills.''',\n",
    "    'MOCHA': '''MOCHA is a benchmark dataset for training and evaluating generative reading comprehension metrics. Its main goal is to address the limitations of existing generation metrics, which rely on token overlap and are agnostic to the nuances of reading comprehension. MOCHA aims to develop learned metrics that model the correctness of candidates using human annotation scores. MOCHA contains 40K human judgement scores on model outputs from six diverse question answering datasets and an additional set of minimal pairs for evaluation. The candidates come from a wide range of RC phenomena, such as commonsense reasoning and understanding narrative over movie scripts. MOCHA presents a challenging problem for developing accurate and robust generative reading comprehension metrics. To evaluate machine learning models, MOCHA uses a Learned Evaluation metric for Reading Comprehension (LERC) that mimics human judgement scores. LERC outperforms baseline metrics by 10 to 36 absolute Pearson points on held-out annotations. When evaluated on minimal pairs, LERC achieves 80% accuracy, outperforming baselines by 14 to 26 absolute percentage points while leaving significant room for improvement. In summary, MOCHA aims to improve generative reading comprehension metrics by developing learned metrics that model the correctness of candidates using human annotation scores. It contains a diverse set of RC phenomena and uses LERC to evaluate machine learning models.''',\n",
    "    'NaturalInstructions': '''The NATURAL INSTRUCTIONS benchmark is a dataset of 61 distinct tasks, their human-authored instructions, and 193k task instances (input-output pairs) . The main goal of this benchmark is to study the ability of machine learning models to learn new tasks by understanding the human-readable instructions that define them, and to evaluate their generalization performance across tasks . The defining feature of this benchmark is that it is based on natural language crowdsourcing instructions, which are often used to define tasks for human workers in crowdsourcing platforms. The instructions are curated from existing well-known datasets and mapped to a unified schema, providing training and evaluation data for learning from instructions . The benchmark aims to test and evaluate the ability of machine learning models to encode task-specific instructions and generate task output. Specifically, it evaluates the performance of generative pre-trained language models in encoding task-specific instructions and generating task output, and compares their performance to that of other machine learning models, such as supervised learning models and models that do not use instructions .''',\n",
    "    'Natural Questions': '''The Natural Questions benchmark is a comprehensive data set designed to help researchers improve their question answering systems. The paper outlines three main goals for the data set: 1. To provide large-scale end-to-end training data for the QA problem. 2. To provide a data set that drives research in natural language understanding. 3. To study human performance in providing QA annotations for naturally occurring questions. The data set is unique in several ways. First, it consists of real anonymized, aggregated queries issued to the Google search engine, making it a valuable resource for researchers interested in natural language processing. Second, the data set includes both long and short answers, as well as indications that there is no answer on the page. Finally, the data set is annotated using a 5-way annotation system, which allows for more nuanced evaluation of machine learning models. The paper also details several specific aspects of machine learning models that the Natural Questions benchmark aims to test and evaluate. These include: 1. The ability of models to understand natural language questions and identify relevant information in Wikipedia pages. 2. The ability of models to generate accurate long and short answers to questions. 3. The ability of models to identify when there is no answer to a question on a given page. 4. The ability of models to handle complex questions that require multiple pieces of information to answer.''',\n",
    "    'OpenEval': '''The OpenEval benchmark is a dataset designed for evaluating the functional correctness of Large Language Models (LLMs) in code generation tasks. It consists of 178 function-level programming problems sourced from open-source programming competition websites. Each problem includes a function signature, a documentation string, and a code body, with five test cases per problem. The documentation strings are generated using OpenAI's GPT-3.5 in a few-shot method and are manually reviewed for quality. To avoid test case leakage affecting model performance, test cases are excluded from the model input prompt​​.''',\n",
    "    'PIQA': '''The PIQA benchmark is a new dataset designed to evaluate and study physical commonsense understanding in natural language models . Its main goals are to provide insight into the limitations of current AI models in answering questions about the physical world, and to encourage further research into building language representations that capture details of the real world . The dataset consists of multiple-choice questions that require concrete physical reasoning, with two possible solutions and exactly one correct answer . The questions are derived from how-to instructions on instructables.com, which provide a scaffold for the annotators . The dataset is designed to be challenging for current state-of-the-art models, and to require a deep understanding of physical properties of the world around us . The defining feature of PIQA is its focus on physical commonsense understanding, which is the ability to reason about the physical world based on common sense knowledge . This is a challenging task for natural language models, as it requires not only linguistic understanding but also a deep understanding of the physical properties of objects and their interactions . Overall, the PIQA benchmark is a valuable resource for evaluating and studying physical commonsense understanding in natural language models.''',\n",
    "    'ProtoQA': '''The ProtoQA benchmark is a question answering dataset designed to test and evaluate the common sense reasoning capabilities of machine learning models. The paper introduces a large-scale QA dataset of 9.7k questions regarding common sense knowledge of prototypical situations with 7-8 labeled answer categories per question, and a corresponding evaluation set of 15,400 crowd-sourced human judgments over 154 unseen questions . ProtoQA differs from other datasets in three different ways : 1. ProtoQA focuses on prototypical situations. Humans can agree about the details and characteristics of a prototypical event or situation due to commonalities in their shared lived experiences, cultural norms, and expectations. This rough agreement extends beyond an agreement on a single top response, and that’s why the task and evaluation values diversity of answers. 2. The evaluation ProtoQA is a generative evaluation task where a model has to output a ranked list of answers, ideally covering all prototypical answers for a question. 3. ProtoQA has a large number of annotations for each example which makes the generation evaluation possible. In summary, ProtoQA aims to test and evaluate the common sense reasoning capabilities of machine learning models by presenting them with questions regarding prototypical situations and evaluating their ability to output a ranked list of diverse, prototypical answers.''',\n",
    "    \n",
    "}\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key = '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')\n",
    "\n",
    "def query_gpt4_turbo(prompt, model=\"gpt-4-1106-preview\"):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a fair and objective evaluator, skilled in assessing the relevance of a benchmark dataset for assessing various capabilities.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "benchmark_capability_list = []\n",
    "\n",
    "# Example usage\n",
    "capability_list = ['reasoning']\n",
    "for capability in capability_list:\n",
    "    prompt_base = f'Your task is to evaluate a benchmark dataset based on its relevance for assessing {capability}-related capabilities. After I provide a description of the dataset, rate its relevance on a Likert scale from 1 to 5, with 1 being the least relevant and 5 being the most relevant.'\n",
    "    for key, value in benchmark_description_list.items():\n",
    "        prompt = f'{prompt_base}\\nHere is the description of the {key} dataset:\\n{value}\\nThen give me the relevance in Likert scale only:'\n",
    "        result = query_gpt4_turbo(prompt)\n",
    "        # print(result)\n",
    "        benchmark_capability_list.append({\n",
    "            'Capability': capability,\n",
    "            'Benchmark': key,\n",
    "            'Relevance': int(result),\n",
    "        })\n",
    "        # break\n",
    "\n",
    "benchmark_capability_list = pd.DataFrame(benchmark_capability_list)\n",
    "\n",
    "for name, group in benchmark_capability_list.groupby('Capability'):\n",
    "    fig = go.Figure(go.Bar(\n",
    "        x=group['Relevance'],  # Values for the bar lengths\n",
    "        y=group['Benchmark'],  # Categories for each bar\n",
    "        orientation='h'  # Sets the bars to be horizontal\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f'Relevance of Benchmark Datasets for Assessing {name} Capabilities',\n",
    "        xaxis_title='Relevance score',\n",
    "        yaxis_title='Benchmark dataset',\n",
    "        yaxis_autorange='reversed'  # This line makes the bars go top-down\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_description_list = {\n",
    "    'AgentBench': '''AGENTBENCH is a comprehensive benchmark designed to evaluate Large Language Models (LLMs) as agents in multi-turn open-ended generation settings. The main goals of AGENTBENCH are to standardize the evaluation of LLMs as agents and to provide a practical testbed for their wide array of capabilities. AGENTBENCH defines eight distinct environments of three types based on real-world scenarios, offering a practical testbed for LLMs' capabilities. The three types of environments are: 1. Instruction Following: where the LLM is given a set of instructions to follow and must generate a response that satisfies the instructions. 2. Decision Making: where the LLM is given a set of options and must choose the best one based on a given context. 3. Open-Ended: where the LLM is given a context and must generate a response that is coherent and relevant to the context. The benchmark evaluates LLMs based on their performance in these environments, with a focus on their long-term reasoning, decision-making, and instruction-following abilities. The benchmark also aims to identify the reasons for failures in existing LLM agents and highlight directions for improvement, such as code training and higher-quality alignment data. To evaluate the LLMs, AGENTBENCH uses a set of metrics that measure the models' performance in terms of task completion, context size constraints, return format accuracy, action accuracy, and task limitations. The benchmark also compares the performance of different LLMs, including leading API-based commercial LLMs and OSS models, to uncover significant performance gaps and highlight areas for improvement. Overall, AGENTBENCH provides a comprehensive and standardized approach to evaluating LLMs as agents, with a focus on their long-term reasoning, decision-making, and instruction-following abilities.''',\n",
    "    'BigCode': '''The Big Code Models Leaderboard on Hugging Face is a platform designed to compare the performance of base multilingual code generation models. Defining Features Performance Metrics: Models are evaluated based on their performance on the HumanEval benchmark and MultiPL-E, measuring functional correctness in synthesizing programs from docstrings. Throughput Measurement: The leaderboard also measures model throughput (tokens per second) to compare inference speed. Diverse Programming Languages: It includes benchmarks for multiple programming languages. Evaluation Parameters: Models are evaluated using the big code-evaluation-harness with specific settings. Specific Aspects Tested and Evaluated Functional Correctness: Using the HumanEval benchmark, the leaderboard assesses the ability of models to synthesize correct programs from given docstrings. Multilingual Capabilities: Through the MultiPL-E benchmark, evaluates the performance across 18 programming languages. Inference Speed: By measuring throughput, the leaderboard evaluates how quickly models can process tokens. Memory Usage: Peak memory usage is also measured, providing insights into the efficiency of the models. Scoring and Rankings: Models are ranked based on the average pass@1 score over all languages and their win rate, which represents how often a model outperforms others. Overall, this leaderboard serves as a crucial tool for the AI community, offering a comprehensive and comparative view of the capabilities of various code generation models, especially in terms of multilingual support and functional correctness.''',\n",
    "    'CLUE': '''The CLUE benchmark is a large-scale Chinese Language Understanding Evaluation designed to test and evaluate the performance of machine learning models on a variety of sentence classification and machine reading comprehension tasks in Chinese. The benchmark is unique in that it covers a wide range of tasks, at different levels of difficulty, in different sizes and forms, all on original Chinese text. The main goals of the CLUE benchmark are to provide a standardized evaluation framework for Chinese NLU models, to encourage reproducibility and transparency in research, and to promote the development of high-quality Chinese NLU models. Some of the defining features of the CLUE benchmark include a diagnostic dataset that contains nine linguistic and logic phenomena, a leaderboard for users to submit their own results, and a toolkit named PyCLUE implemented in TensorFlow that supports mainstream pre-training models and a wide range of target tasks. The specific aspects of machine learning models that the CLUE benchmark aims to test and evaluate include model size, pre-training data, and whole word masking. The benchmark results show that larger models and models trained with more pre-training data tend to perform better, as do models that use whole word masking. The best performing models on the CLUE benchmark are RoBERTa-wwm-ext-large and ALBERT-xxlarge, which show advantages over other models particularly for machine reading tasks such as C3.''',\n",
    "    'HEIM': '''The Holistic Evaluation of Text-to-Image Models (HEIM) benchmark aims to provide a comprehensive evaluation of state-of-the-art text-to-image models across 12 different aspects, including text-image alignment, image quality, aesthetics, originality, reasoning, knowledge, bias, toxicity, fairness, robustness, multilinguality, and efficiency. HEIM is unique in that it combines both automated metrics and crowdsourced human evaluations to provide a more comprehensive understanding of model performance. The benchmark also includes a diverse collection of 62 scenarios, which are datasets of prompts, and 25 metrics, which are measurements used for assessing the quality of generated images specific to each aspect. The main goals of the HEIM benchmark are to provide a standardized evaluation framework for text-to-image models, offer holistic insights into model performance, and enable researchers, developers, and end-users to make informed decisions based on comparable assessments. Overall, the HEIM benchmark provides a quantitative understanding of the capabilities and risks of text-to-image models, and highlights the importance of evaluating models across multiple aspects to ensure their technological advancement and societal impact.''',\n",
    "    'HELM': '''HELM aims to provide a standardized and comprehensive evaluation of language models across a broad range of scenarios and metrics. The benchmark aims to test and evaluate language models in terms of their accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency. Additionally, HELM includes targeted evaluations of skills and risks, such as knowledge, reasoning, disinformation, and copyright. HELM is defined by its broad coverage, which includes 16 core scenarios and 21 new scenarios that have not been previously used in mainstream language model evaluation. The benchmark evaluates 30 language models under standardized conditions, ensuring that models can be directly compared across many scenarios and metrics. These models vary in terms of their public accessibility, with 10 being open, 17 being limited-access, and 3 being closed. The specific aspects of machine learning models that HELM aims to test and evaluate include their ability to accurately predict outcomes, their ability to handle different types of inputs and prompts, their ability to avoid bias and toxicity, and their efficiency in terms of speed and resource usage. HELM also aims to evaluate the skills and risks associated with language models, such as their ability to reason and their potential for spreading disinformation. Overall, HELM provides a comprehensive and standardized approach to evaluating language models, with the goal of guiding future language model development and providing ample opportunities for further analysis.''',\n",
    "    'InstructEval': '''The INSTRUCTEVAL benchmark suite is a comprehensive evaluation tool designed to assess the performance of instruction-tuned large language models (LLMs) across a range of general capabilities, usage scenarios, and human-centric behavior. Its main goals are to provide a more holistic evaluation of LLMs that goes beyond traditional benchmarks, and to identify areas where models may fall short in terms of problem-solving, writing ability, and alignment with human values. INSTRUCTEVAL consists of three main evaluation categories: problem-solving, writing, and alignment to human values. The problem-solving category includes multiple benchmarks that cover real-world exams on diverse topics, complex instructions, arithmetic, programming, and causality. The writing category includes prompts that test the models' ability to write informative, persuasive, and creative texts. The alignment to human values category includes tasks that assess the models' ability to generate text that is consistent with human values and ethical principles. The defining features of INSTRUCTEVAL include its focus on instruction-tuned LLMs, its use of multiple evaluation methods (including both objective and qualitative scoring), and its coverage of a wide range of general abilities and usage scenarios. The benchmark suite is designed to test models on tasks that require world knowledge, multi-hop reasoning, creativity, and more, and to provide a comprehensive evaluation of their performance across a range of domains.''',\n",
    "    'LawBench': '''LawBench is a comprehensive evaluation benchmark designed to assess the legal capabilities of Large Language Models (LLMs) from three cognitive levels: legal knowledge memorization, understanding, and application. Its main goal is to provide a precise assessment of LLMs' legal knowledge and their ability to perform legal-related tasks in the highly specialized, safe-critical legal domain. LawBench is unique in its design and features. It covers 20 diverse tasks that are divided into five task types: legal fact verification, legal question answering, legal text completion, legal text modification, and legal text generation. The tasks are meticulously crafted to have precise assessment of the LLMs’ legal capabilities. The benchmark evaluates LLMs' legal knowledge memorization by testing whether they can memorize needed legal concepts, articles, and facts. It evaluates legal knowledge understanding by testing whether LLMs can comprehend entities, events, and relationships within legal text. Finally, it evaluates legal knowledge application by testing whether LLMs can properly utilize their legal knowledge and make necessary reasoning steps to solve realistic legal tasks.''',\n",
    "    'LLM-Leaderboard': '''The LLM-Leaderboard is a community-driven initiative to establish a centralized leaderboard for Large Language Models (LLMs). Its main goals and features include: Centralized Leaderboard: It provides a central platform for comparing various LLMs. The leaderboard includes model names, publishers, and whether the models are open for local deployment and commercial use​​​​. Evaluation Benchmarks: Various benchmarks are used to evaluate the LLMs, including Chatbot Arena Elo, HellaSwag, HumanEval-Python, LAMBADA, MMLU, TriviaQA, and WinoGrande. These benchmarks test aspects like chatbot performance, commonsense natural language inference, programming problem-solving, text understanding, and world knowledge.''',\n",
    "    'LLM-Perf Leaderboard': '''The LM-Perf Leaderboard serves as a critical tool in the realm of machine learning, particularly focusing on the evaluation and benchmarking of Large Language Models (LLMs). Its primary goal is to benchmark the performance of these models in terms of latency, throughput, and memory across different hardware, backends, and optimization configurations using Optimum-Benchmark and Optimum flavors​​.''',\n",
    "    'Open Ko-LLM Leaderboard': '''The Open Ko-LLM Leaderboard is emphasizes high-quality data that reflects the characteristics and culture of the Korean language, thereby addressing the unique challenges and nuances of processing and understanding Korean text​. Defining Features Rapid Expansion and Participation: The leaderboard witnessed rapid growth, registering over 100 models within two weeks of its launch. This participation includes various companies, academic organizations, and individual researchers, indicating its wide-reaching impact and acceptance​​. Diverse Model Collection: It hosts a variety of famous Korean open-source models like 'Ko-Alpaca', 'KULLM (Cloud)', and 'Polyglot-Ko', showcasing a comprehensive range of Korean LLM performances within the industry​​. Continuous Development and Engagement: Future plans include monthly leaderboard rankings, open seminars, and collaborative efforts to further develop and refine the Korean LLM ecosystem​​.''',\n",
    "    'Open LLM Leaderboard': '''The Open LLM Leaderboard is designed to track, rank, and evaluate open Large Language Models (LLMs) and chatbots. Defining Features Benchmarks: The leaderboard evaluates models on seven key benchmarks using the Eleuther AI Language Model Evaluation Harness. This framework tests generative language models on a variety of evaluation tasks. Model Categories: It categorizes models into different types, such as pretrained, fine-tuned, instruction-tuned, and RL-tuned models. Quantization: The leaderboard includes information about model quantization, like 8-bit and 4-bit quantizations, which are important for understanding model efficiency and deployment capabilities. Community Engagement: Models can be flagged by the community for issues, ensuring the integrity and reliability of the leaderboard rankings. Additional Features Reproducibility: The leaderboard provides commands and methods to reproduce the results using the Eleuther AI Harness. Model Submission Guidelines: Guidelines for submitting models to the leaderboard, ensuring standardization and quality control. Detailed Results and Logs: Access to detailed numerical results and model input/output details for deeper analysis. Overall, the Open LLM Leaderboard is a comprehensive platform for evaluating and comparing large language models and chatbots. It emphasizes a variety of reasoning, knowledge, and inference tasks, providing a holistic view of model capabilities.''',\n",
    "    'Open Multilingual LLM Evaluation Leaderboard': '''The Open Multilingual LLM Evaluation Leaderboard at HuggingFace is a platform that tracks and ranks the performance of large language models (LLMs) developed for different languages. Defining Features Multilingual Focus: The current version of the leaderboard includes evaluation data for 29 languages, including Arabic, Chinese, French, German, Hindi, Russian, Spanish, and others. This multilingual aspect is significant as it expands the scope of LLMs beyond predominantly English-centric models. Open Participation: The leaderboard is open to both multilingual and language-specific LLMs. It encourages participation from a wide range of models, promoting diversity in AI development.''',\n",
    "    'OpenCompass': '''The OpenCompass Leaderboard is an open-source, efficient, and comprehensive evaluation suite and platform designed for large models. Defining Features: Multi-Type Model Support: OpenCompass can evaluate various model types, including HF models, API models, and custom open-source models. Comprehensive Capability Dimensions: The evaluation covers thorough dimensions, each backed by abundant datasets. Flexible Expansion: The platform supports the flexible and convenient addition of evaluation datasets and models. Diverse Evaluation Methods: As mentioned, it includes zero-shot, few-shot, and chain-of-thought evaluations. Defining Features: Multi-Type Model Support: OpenCompass can evaluate various model types, including HF models, API models, and custom open-source models. Comprehensive Capability Dimensions: The evaluation covers thorough dimensions, each backed by abundant datasets. Flexible Expansion: The platform supports the flexible and convenient addition of evaluation datasets and models. Diverse Evaluation Methods: It includes zero-shot, few-shot, and chain-of-thought evaluations. Specific Aspects of Machine Learning Models Tested: Large Language Model Evaluation: Evaluating capabilities based on five major dimensions: language, knowledge, reasoning, discipline, and comprehension. This is done using more than 50 datasets. Base Models and Chat Models: The leaderboard includes a variety of models, such as InternLM, LLaMA, ChatGPT, GPT-4, and many others, evaluated across different dimensions. Examination Datasets: The evaluation uses datasets like Middle School Exam, High School Exam, College Exam, and others for comprehensive testing. Capability Dimensions: These include language (word definition, idiom learning, translation), knowledge (question answering, commonsense QA), understanding (reading comprehension, content analysis), and reasoning (NLI, common sense, mathematics). Visual Language Model Benchmark: OpenCompass also conducts a progressive assessment from perception to reasoning, covering 20 finely-grained capabilities with approximately 3000 multiple-choice questions.''',\n",
    "    'OpenEval': '''OpenEval is a comprehensive, multidimensional, and open evaluation platform dedicated to assessing Chinese large-scale models. Specific Aspects of Machine Learning Models Tested and Evaluated: Knowledge Assessment: Evaluating the depth and accuracy of the knowledge encapsulated in large models. Capability Evaluation: Assessing the functional capabilities of models, such as understanding, reasoning, and language generation. Alignment and Safety Levels: Testing how well these models align with ethical, cultural, and safety standards, which is crucial in preventing biases and ensuring responsible AI deployment. Detailed Evaluation Reports: Providing comprehensive reports that detail the performance and characteristics of evaluated models, which can be crucial for developers and researchers.''',\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "orientation": "h",
         "type": "bar",
         "x": [
          5,
          5,
          3,
          5,
          5,
          4,
          5,
          5,
          4,
          5,
          4,
          3,
          4,
          5,
          3,
          4,
          4,
          4,
          4
         ],
         "y": [
          "2WikiMultihopQA",
          "A-OKVQA",
          "AFQMC",
          "AGIEval",
          "ALFWorld",
          "ANGO",
          "APPS",
          "ARC",
          "ARC-DA",
          "ActivityNet-QA",
          "AlpacaEval",
          "Amazon Review",
          "AQUA-RAT",
          "AR-LSAT",
          "ArguAna",
          "BBQ",
          "BIG-Bench",
          "BLiMP",
          "BOLD"
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Relevance of Benchmark Datasets for Assessing reasoning Capabilities"
        },
        "xaxis": {
         "title": {
          "text": "Relevance score"
         }
        },
        "yaxis": {
         "title": {
          "text": "Benchmark dataset"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    fig = go.Figure(go.Bar(\n",
    "        x=[int(r) for r in group['Relevance']],  # Values for the bar lengths\n",
    "        y=group['Benchmark'],  # Categories for each bar\n",
    "        orientation='h'  # Sets the bars to be horizontal\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f'Relevance of Benchmark Datasets for Assessing {name} Capabilities',\n",
    "        xaxis_title='Relevance score',\n",
    "        yaxis_title='Benchmark dataset',\n",
    "        # yaxis_autorange='reversed'  # This line makes the bars go top-down\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MMLU', 11),\n",
       " ('HellaSwag', 7),\n",
       " ('HumanEval', 6),\n",
       " ('BoolQ', 5),\n",
       " ('TruthfulQA', 4),\n",
       " ('ARC', 4),\n",
       " ('GSM8K', 4),\n",
       " ('DROP', 4),\n",
       " ('CHID', 3),\n",
       " ('NarrativeQA', 3),\n",
       " ('OpenbookQA', 3),\n",
       " ('WinoGrande', 3),\n",
       " ('TriviaQA', 3),\n",
       " ('Grounded', 3),\n",
       " ('IMDB', 2),\n",
       " ('RAFT', 2),\n",
       " ('CMMLU', 2),\n",
       " ('CSL', 2),\n",
       " ('CLUEWSC', 2),\n",
       " ('EPRSTMT', 2),\n",
       " ('OCNLI', 2),\n",
       " ('MS-COCO', 2),\n",
       " ('Winoground', 2),\n",
       " ('MATH', 2),\n",
       " ('NaturalQuestions', 2),\n",
       " ('QuAC', 2),\n",
       " ('BBH', 2),\n",
       " ('MBPP', 2),\n",
       " ('ScienceQA', 2),\n",
       " ('LAMBADA', 2),\n",
       " ('SIQA', 2),\n",
       " ('WiC', 2),\n",
       " ('GAOKAO-Bench', 2),\n",
       " ('AGIEval', 2),\n",
       " ('WSC', 2),\n",
       " ('C3', 2),\n",
       " ('CMNLI', 2),\n",
       " ('RTE', 2),\n",
       " ('ReCoRD', 2),\n",
       " ('IC15', 2),\n",
       " ('COCO-Text', 2),\n",
       " ('TextOCR', 2),\n",
       " ('ALFWorld', 1),\n",
       " ('WebShop', 1),\n",
       " ('Mind2Web', 1),\n",
       " ('MultiPL-E', 1),\n",
       " ('MT-Bench', 1),\n",
       " ('Chatbot', 1),\n",
       " ('Arena', 1),\n",
       " ('Conversations', 1),\n",
       " ('GAOKAO-Bench-2023', 1),\n",
       " ('LLSRC', 1),\n",
       " ('SLSRC', 1),\n",
       " ('SLPWC', 1),\n",
       " ('SLRFC', 1),\n",
       " ('TNEWS', 1),\n",
       " ('BUSTM', 1),\n",
       " ('gnad10', 1),\n",
       " ('Belebele', 1),\n",
       " ('XNLI', 1),\n",
       " ('Amazon', 1),\n",
       " ('Review', 1),\n",
       " ('Caltech-UCSD', 1),\n",
       " ('Birds-200-2011', 1),\n",
       " ('Common', 1),\n",
       " ('Syntactic', 1),\n",
       " ('Processes', 1),\n",
       " ('Demographic', 1),\n",
       " ('Stereotypes', 1),\n",
       " ('PaintSkills', 1),\n",
       " ('I2P', 1),\n",
       " ('Magazine', 1),\n",
       " ('Cover', 1),\n",
       " ('Photos', 1),\n",
       " ('Mental', 1),\n",
       " ('Disorders', 1),\n",
       " ('P2', 1),\n",
       " ('Relational', 1),\n",
       " ('Understanding', 1),\n",
       " (\"TIME's\", 1),\n",
       " ('most', 1),\n",
       " ('significant', 1),\n",
       " ('historical', 1),\n",
       " ('figures', 1),\n",
       " ('dailydall.e', 1),\n",
       " ('APPS', 1),\n",
       " ('BBQ', 1),\n",
       " ('BLiMP', 1),\n",
       " ('BOLD', 1),\n",
       " ('CNN/DailyMail', 1),\n",
       " ('CivilComments', 1),\n",
       " ('Data', 1),\n",
       " ('imputation', 1),\n",
       " ('Disinformation', 1),\n",
       " ('Dyck', 1),\n",
       " ('Entity', 1),\n",
       " ('matching', 1),\n",
       " ('ICE', 1),\n",
       " ('AR-LSAT', 1),\n",
       " ('LegalSupport', 1),\n",
       " ('MS', 1),\n",
       " ('MARCO', 1),\n",
       " ('RealToxicityPrompts', 1),\n",
       " ('LIME', 1),\n",
       " ('WikiFact', 1),\n",
       " ('XSUM', 1),\n",
       " ('bAbI', 1),\n",
       " ('Synthetic', 1),\n",
       " ('efficiency', 1),\n",
       " ('The', 1),\n",
       " ('Pile', 1),\n",
       " ('TwitterAAE', 1),\n",
       " ('CRASS', 1),\n",
       " ('IMPACT', 1),\n",
       " ('HHH', 1),\n",
       " ('Spider', 1),\n",
       " ('NL2Bash', 1),\n",
       " ('AR', 1),\n",
       " ('ER', 1),\n",
       " ('NER', 1),\n",
       " ('JS', 1),\n",
       " ('CR', 1),\n",
       " ('CFM', 1),\n",
       " ('SCM', 1),\n",
       " ('CJP', 1),\n",
       " ('CTP', 1),\n",
       " ('LQA', 1),\n",
       " ('JRG', 1),\n",
       " ('CU', 1),\n",
       " ('LC', 1),\n",
       " ('CIFAR-10', 1),\n",
       " ('Flickr30k', 1),\n",
       " ('VOC2012', 1),\n",
       " ('Omnibenchmark', 1),\n",
       " ('FSC147', 1),\n",
       " ('MMBench', 1),\n",
       " ('SEED-Bench', 1),\n",
       " ('Chatbot-Arena', 1),\n",
       " ('HotpotQA', 1),\n",
       " ('2WikiMultihopQA', 1),\n",
       " ('MuSiQue', 1),\n",
       " ('DuReader', 1),\n",
       " ('MultiFieldQA', 1),\n",
       " ('Qasper', 1),\n",
       " ('GovReport', 1),\n",
       " ('QMSum', 1),\n",
       " ('Multi-News', 1),\n",
       " ('VCSUM', 1),\n",
       " ('SAMSum', 1),\n",
       " ('TREC', 1),\n",
       " ('LSHTC', 1),\n",
       " ('PassageRetrieval', 1),\n",
       " ('PassageCount', 1),\n",
       " ('LCC', 1),\n",
       " ('RepoBench-P', 1),\n",
       " ('MCScript', 1),\n",
       " ('CosmosQA', 1),\n",
       " ('Quoref', 1),\n",
       " ('CommonGen', 1),\n",
       " ('C-Eval', 1),\n",
       " ('AFQMC', 1),\n",
       " ('TyDiQA', 1),\n",
       " ('Flores', 1),\n",
       " ('CommonSenseQA', 1),\n",
       " ('RACE', 1),\n",
       " ('LCSTS', 1),\n",
       " ('XSum', 1),\n",
       " ('AX-b', 1),\n",
       " ('AX-g', 1),\n",
       " ('PIQA', 1),\n",
       " ('M3KE', 1),\n",
       " ('TGEA', 1),\n",
       " ('OL-CC', 1),\n",
       " ('CSNLI', 1),\n",
       " ('ChineseSquad', 1),\n",
       " ('WPLC', 1),\n",
       " ('BiPaR', 1),\n",
       " ('CommonMT', 1),\n",
       " ('TOCP', 1),\n",
       " ('SWSR', 1),\n",
       " ('CORGI-PM', 1),\n",
       " ('CDIAL-BIAS', 1),\n",
       " ('COLD', 1),\n",
       " ('CBBQ', 1),\n",
       " ('TUMCC', 1),\n",
       " ('Cooridinate', 1),\n",
       " ('AI', 1),\n",
       " ('Corrigible', 1),\n",
       " ('Myopia', 1),\n",
       " ('Reward', 1),\n",
       " ('One-box', 1),\n",
       " ('Tendency', 1),\n",
       " ('Power-seeking', 1),\n",
       " ('Self-awareness', 1),\n",
       " ('CAIL判决预测数据集', 1),\n",
       " ('KQApro', 1),\n",
       " ('LC-quad2', 1),\n",
       " ('WQSP', 1),\n",
       " ('CWQ', 1),\n",
       " ('GrailQA', 1),\n",
       " ('GraphQ', 1),\n",
       " ('QALD-9', 1),\n",
       " ('MKQA', 1),\n",
       " ('FewNERD', 1),\n",
       " ('FewRel', 1),\n",
       " ('InstructIE', 1),\n",
       " ('MAVEN', 1),\n",
       " ('WikiEvents', 1),\n",
       " ('Flowers102', 1),\n",
       " ('CIFAR10', 1),\n",
       " ('ImageNet-1K', 1),\n",
       " ('Pets37', 1),\n",
       " ('VizWiz-yesno', 1),\n",
       " ('VizWiz-singleChoice', 1),\n",
       " ('TDIUC-Sport', 1),\n",
       " ('TDIUC-Scene', 1),\n",
       " ('MEDIC', 1),\n",
       " ('MSCOCO-MCI', 1),\n",
       " ('MSCOCO-GOI', 1),\n",
       " ('MSCOCO-MOS', 1),\n",
       " ('TDIUC-Color', 1),\n",
       " ('TDIUC-Utility', 1),\n",
       " ('TDIUC-Position', 1),\n",
       " ('TDIUC-Detection', 1),\n",
       " ('TDIUC-Counting', 1),\n",
       " ('RefCOCO', 1),\n",
       " ('MSCOCO-OC', 1),\n",
       " ('VQA', 1),\n",
       " ('v2', 1),\n",
       " ('GQA', 1),\n",
       " ('Whoops', 1),\n",
       " ('OK-VQA', 1),\n",
       " ('VizWiz', 1),\n",
       " ('ViQuAE', 1),\n",
       " ('K-ViQuAE', 1),\n",
       " ('A-OKVQA', 1),\n",
       " ('A-OKVQRA', 1),\n",
       " ('A-OKVQAR', 1),\n",
       " ('ImageNetVC', 1),\n",
       " ('CLEVR', 1),\n",
       " ('VSR', 1),\n",
       " ('MP3D', 1),\n",
       " ('VQA-MT', 1),\n",
       " ('VisDial', 1),\n",
       " ('MSCOCO-ITM', 1),\n",
       " ('MSCOCO-ITS', 1),\n",
       " ('WikiHow', 1),\n",
       " ('SNLI-VE', 1),\n",
       " ('MOCHEG', 1),\n",
       " ('CUTE80', 1),\n",
       " ('IIIT5K', 1),\n",
       " ('WordArt', 1),\n",
       " ('FUNSD', 1),\n",
       " ('POIE', 1),\n",
       " ('SROIE', 1),\n",
       " ('TextVQA', 1),\n",
       " ('DocVQA', 1),\n",
       " ('OCR-VQA', 1),\n",
       " ('MSCOCO', 1),\n",
       " ('TextCaps', 1),\n",
       " ('NoCaps', 1),\n",
       " ('Flickr30K', 1),\n",
       " ('CommitmentBank', 1),\n",
       " ('COPA', 1),\n",
       " ('MultiRC', 1),\n",
       " ('Broadcoverage', 1),\n",
       " ('Diagnostics', 1),\n",
       " ('Winogender', 1)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''ALFWorld\tWebShop\tMind2Web\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "MultiPL-E\tHumanEval\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "MMLU\tMT-Bench\tChatbot Arena Conversations\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "BoolQ\tMMLU\tTruthfulQA\tIMDB\tRAFT\tCMMLU\tGAOKAO-Bench-2023\tCSL\tCHID\tCLUEWSC\tLLSRC\tSLSRC\tSLPWC\tSLRFC\tEPRSTMT\tTNEWS\tOCNLI\tBUSTM\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "MMLU\tgnad10\tHellaSwag\tARC\tBelebele\tXNLI\tAmazon Review\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "Caltech-UCSD Birds-200-2011\tCommon Syntactic Processes\tDemographic Stereotypes\tPaintSkills\tI2P\tMS-COCO\tMagazine Cover Photos\tMental Disorders\tP2\tRelational Understanding\tTIME\tWinoground\tdailydall.e\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "APPS\tBBQ\tBLiMP\tBOLD\tBoolQ\tCNN/DailyMail\tCivilComments\tData imputation\tDisinformation\tDyck\tEntity matching\tGSM8K\tHellaSwag\tHumanEval\tICE\tIMDB\tAR-LSAT\tLegalSupport\tMATH\tMMLU\tMS MARCO\tNarrativeQA\tNaturalQuestions\tOpenbookQA\tQuAC\tRAFT\tRealToxicityPrompts\tLIME\tWikiFact\tXSUM\tbAbI\tSynthetic efficiency\tThe Pile\tTruthfulQA\tTwitterAAE\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "ARC\tHellaSwag\tMMLU\tTruthfulQA\tWinoGrande\tGSM8K\tDROP\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "MMLU\tBBH\tDROP\tCRASS\tHumanEval\tIMPACT\tHHH\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "MBPP\tSpider\tNL2Bash\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "AR\tER\tNER\tJS\tCR\tCFM\tSCM\tCJP\tCTP\tLQA\tJRG\tCU\tLC\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "CIFAR-10\tFlickr30k\tVOC2012\tOmnibenchmark\tFSC147\tScienceQA\tMMBench\tSEED-Bench\tMS-COCO\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "Chatbot-Arena\tHellaSwag\tHumanEval\tLAMBADA\tMMLU\tTriviaQA\tWinoGrande\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "HotpotQA\t2WikiMultihopQA\tMuSiQue\tDuReader\tMultiFieldQA\tNarrativeQA\tQasper\tGovReport\tQMSum\tMulti-News\tVCSUM\tTriviaQA\tSAMSum\tTREC\tLSHTC\tPassageRetrieval\tPassageCount\tLCC\tRepoBench-P\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "NarrativeQA\tMCScript\tCosmosQA\tSIQA\tDROP\tQuoref\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "HellaSwag\tMMLU\tARC\tTruthfulQA\tCommonGen\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "C-Eval\tMMLU\tARC\tCHID\tWiC\tGAOKAO-Bench\tCMMLU\tAGIEval\tAFQMC\tWSC\tTyDiQA\tFlores\tBoolQ\tCommonSenseQA\tC3\tRACE\tOpenbookQA\tCSL\tLCSTS\tXSum\tEPRSTMT\tLAMBADA\tCMNLI\tOCNLI\tAX-b\tAX-g\tRTE\tReCoRD\tHellaSwag\tPIQA\tSIQA\tMATH\tGSM8K\tDROP\tHumanEval\tMBPP\tBBH\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "M3KE\tMMLU\tGAOKAO-Bench\tTGEA\tOL-CC\tCSNLI\tCLUEWSC\tChineseSquad\tCHID\tWPLC\tBiPaR\tCommonMT\tC3\tCMNLI\tTOCP\tSWSR\tCORGI-PM\tCDIAL-BIAS\tCOLD\tCBBQ\tTUMCC\tCooridinate AI\tCorrigible\tMyopia Reward\tOne-box Tendency\tPower-seeking\tSelf-awareness\tCAIL判决预测数据集\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "KQApro\tLC-quad2\tWQSP\tCWQ\tGrailQA\tGraphQ\tQALD-9\tMKQA\tFewNERD\tFewRel\tInstructIE\tMAVEN\tWikiEvents\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "Flowers102\tCIFAR10\tImageNet-1K\tPets37\tVizWiz-yesno\tVizWiz-singleChoice\tTDIUC-Sport\tTDIUC-Scene\tMEDIC\tMSCOCO-MCI\tMSCOCO-GOI\tMSCOCO-MOS\tTDIUC-Color\tTDIUC-Utility\tTDIUC-Position\tTDIUC-Detection\tTDIUC-Counting\tRefCOCO\tMSCOCO-OC\tVQA v2\tGQA\tWhoops\tOK-VQA\tScienceQA\tVizWiz\tViQuAE\tK-ViQuAE\tA-OKVQA\tA-OKVQRA\tA-OKVQAR\tImageNetVC\tCLEVR\tVSR\tMP3D\tVQA-MT\tVisDial\tMSCOCO-ITM\tMSCOCO-ITS\tWikiHow\tWinoground\tSNLI-VE\tMOCHEG\tGrounded IC15\tIC15\tGrounded COCO-Text\tCOCO-Text\tGrounded TextOCR\tTextOCR\tCUTE80\tIIIT5K\tWordArt\tFUNSD\tPOIE\tSROIE\tTextVQA\tDocVQA\tOCR-VQA\tMSCOCO\tTextCaps\tNoCaps\tFlickr30K\n",
    "BoolQ\tCommitmentBank\tCOPA\tMultiRC\tReCoRD\tRTE\tWiC\tWSC\tBroadcoverage Diagnostics\tWinogender\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "MMLU\tTriviaQA\tNaturalQuestions\tGSM8K\tHumanEval\tAGIEval\tBoolQ\tHellaSwag\tOpenbookQA\tQuAC\tWinoGrande'''\n",
    "\n",
    "from collections import defaultdict\n",
    "df = defaultdict(int)\n",
    "for items in text.split('\\n'):\n",
    "    for item in items.split():\n",
    "        df[item] += 1\n",
    "\n",
    "sorted(df.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = '''10-bin expected calibration error\n",
    "1-bin expected calibration error\n",
    "Max prob\n",
    "10-bin expected calibration error\n",
    "1-bin expected calibration error (after Platt scaling)\n",
    "10-bin Expected Calibration Error (after Platt scaling)\n",
    "Platt Scaling Coefficient\n",
    "Platt Scaling Intercept\n",
    "Selective coverage-accuracy area\n",
    "Accuracy at 10% coverage\n",
    "Stereotypical associations (race, profession)\n",
    "Stereotypical associations (gender, profession)\n",
    "Demographic representation (race)\n",
    "Demographic representation (gender)\n",
    "Toxic fraction\n",
    "Denoised inference runtime (s)\n",
    "Estimated training emissions (kg CO2)\n",
    "Estimated training energy cost (MWh)\n",
    "Observed inference runtime (s)\n",
    "Idealized inference runtime (s)\n",
    "Denoised inference runtime (s)\n",
    "# trials\n",
    "# prompt tokens\n",
    "# output tokens\n",
    "# eval\n",
    "# train\n",
    "truncated\n",
    "SummaC\n",
    "QAFactEval\n",
    "Coverage\n",
    "Density\n",
    "Compression\n",
    "BERTScore (F1)\n",
    "HumanEval-faithfulness\n",
    "HumanEval-relevance\n",
    "HumanEval-coherence\n",
    "Avg. # tests passed\n",
    "Strict correctness\n",
    "BBQ (ambiguous)\n",
    "BBQ (unambiguous)\n",
    "Longest common prefix length\n",
    "Edit distance (Levenshtein)\n",
    "Edit similarity (Levenshtein)\n",
    "Self-BLEU\n",
    "Entropy (Monte Carlo)\n",
    "Macro-F1\n",
    "Micro-F1\n",
    "Chinese iBLEU\n",
    "Chinese Top-1 Accuracy\n",
    "Chinese ROUGE-2 score\n",
    "Chinese BLEU-1 score\n",
    "CLEVA Math Exact Match\n",
    "Chinese BLEU-1 score\n",
    "Chinese BLEU-1 score'''\n",
    "\n",
    "# s = set()\n",
    "df = pd.DataFrame()\n",
    "for scenario in metrics.split('\\n'):\n",
    "    # scenario_ = scenario.split('(')[0].strip()\n",
    "    s = {\n",
    "        'Name': scenario\n",
    "    }\n",
    "    df = pd.concat([df, pd.DataFrame([s])], ignore_index=True)\n",
    "    # s.add(scenario)\n",
    "# len(df)\n",
    "df.to_csv('metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenarios = '''BoolQ\n",
    "# NarrativeQA\n",
    "# NaturalQuestions (closed-book)\n",
    "# NaturalQuestions (open-book)\n",
    "# QuAC (Question Answering in Context)\n",
    "# HellaSwag\n",
    "# OpenbookQA\n",
    "# TruthfulQA\n",
    "# MMLU (Massive Multitask Language Understanding)\n",
    "# MS MARCO (regular track)\n",
    "# MS MARCO (TREC track)\n",
    "# CNN/DailyMail\n",
    "# XSUM\n",
    "# IMDB\n",
    "# CivilComments\n",
    "# RAFT (Real-world Annotated Few-Shot)\n",
    "# ICE (International Corpus of English)\n",
    "# The Pile\n",
    "# TwitterAAE\n",
    "# BLiMP (The Benchmark of Linguistic Minimal Pairs for English)\n",
    "# NaturalQuestions (closed-book)\n",
    "# HellaSwag\n",
    "# OpenbookQA\n",
    "# TruthfulQA\n",
    "# MMLU (Massive Multitask Language Understanding)\n",
    "# WikiFact\n",
    "# bAbI\n",
    "# Dyck\n",
    "# Synthetic reasoning (abstract symbols)\n",
    "# Synthetic reasoning (natural language)\n",
    "# GSM8K (Grade school math word problems)\n",
    "# MATH\n",
    "# MATH (chain-of-thoughts)\n",
    "# APPS (Code)\n",
    "# HumanEval (Code)\n",
    "# LegalSupport\n",
    "# LSAT\n",
    "# Data imputation\n",
    "# Entity matching\n",
    "# Copyright (text)\n",
    "# Copyright (code)\n",
    "# Disinformation (reiteration)\n",
    "# Disinformation (wedging)\n",
    "# BBQ (Bias Benchmark for Question Answering)\n",
    "# BOLD (Bias in Open-Ended Language Generation Dataset)\n",
    "# RealToxicityPrompts\n",
    "# Synthetic efficiency\n",
    "# MMLU (Massive Multitask Language Understanding)\n",
    "# IMDB\n",
    "# RAFT (Real-world Annotated Few-Shot)\n",
    "# CivilComments\n",
    "# NaturalQuestions (open-book)\n",
    "# CNN/DailyMail\n",
    "# IMDB\n",
    "# CivilComments\n",
    "# HellaSwag\n",
    "# OpenbookQA\n",
    "# TruthfulQA\n",
    "# MMLU (Massive Multitask Language Understanding)\n",
    "# BLiMP (The Benchmark of Linguistic Minimal Pairs for English)\n",
    "# LegalSupport\n",
    "# LSAT\n",
    "# BBQ (Bias Benchmark for Question Answering)\n",
    "# NaturalQuestions (open-book)\n",
    "# CNN/DailyMail\n",
    "# IMDB\n",
    "# CivilComments\n",
    "# BoolQ\n",
    "# IMDB'''\n",
    "# s = set()\n",
    "# for scenario in scenarios.split('\\n'):\n",
    "#     scenario_ = scenario.split('(')[0].strip()\n",
    "#     s.add(scenario_)\n",
    "# # len(s)\n",
    "# s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenarios = '''MS-COCO (base)\n",
    "# Caltech-UCSD Birds-200-2011\n",
    "# DrawBench (image quality categories)\n",
    "# DrawBench (reasoning categories)\n",
    "# DrawBench (knowledge categories)\n",
    "# PartiPrompts (image quality categories)\n",
    "# PartiPrompts (reasoning categories)\n",
    "# PartiPrompts (knowledge categories)\n",
    "# MS-COCO (base)\n",
    "# DrawBench (image quality categories)\n",
    "# PartiPrompts (image quality categories)\n",
    "# MS-COCO (base)\n",
    "# MS-COCO (Art styles)\n",
    "# dailydall.e\n",
    "# Logos\n",
    "# Landing Page\n",
    "# Magazine Cover Photos\n",
    "# dailydall.e\n",
    "# Logos\n",
    "# Landing Page\n",
    "# Magazine Cover Photos\n",
    "# Common Syntactic Processes\n",
    "# DrawBench (reasoning categories)\n",
    "# PartiPrompts (reasoning categories)\n",
    "# Relational Understanding\n",
    "# Detection (PaintSkills)\n",
    "# Winoground\n",
    "# TIME\n",
    "# DrawBench (knowledge categories)\n",
    "# PartiPrompts (knowledge categories)\n",
    "# Demographic Stereotypes\n",
    "# Mental Disorders\n",
    "# Inappropriate Image Prompts (I2P)\n",
    "# MS-COCO (fairness - AAVE dialect)\n",
    "# MS-COCO (fairness - gender)\n",
    "# MS-COCO (robustness)\n",
    "# MS-COCO (Chinese)\n",
    "# MS-COCO (Hindi)\n",
    "# MS-COCO (Spanish)\n",
    "# MS-COCO Fidelity\n",
    "# MS-COCO Efficiency\n",
    "# MS-COCO (Art styles)'''\n",
    "# s = set()\n",
    "# for scenario in scenarios.split('\\n'):\n",
    "#     scenario = scenario.split('(')[0].strip()\n",
    "#     s.add(scenario)\n",
    "# s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1223"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "sum = 0\n",
    "\n",
    "for file in glob.glob(str(path_llm / \"*.json\")):\n",
    "    if 'imagenet' not in file.lower():\n",
    "        continue\n",
    "    df = pd.read_json(file)\n",
    "    sum += len(df)\n",
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('/Users/jimmy/Downloads/LMExamQA.json')\n",
    "df['domain'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ChatGLM2-6B</th>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPT-4</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bloom-7b1</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama-7B</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baichuan2-13B-Chat</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ChatGPT</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen-7B-Chat</th>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ImageLLM</th>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen-14B-Chat</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>InternLM-7B</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    count\n",
       "model                    \n",
       "ChatGLM2-6B            41\n",
       "GPT-4                  34\n",
       "bloom-7b1              33\n",
       "llama-7B               31\n",
       "Baichuan2-13B-Chat     31\n",
       "ChatGPT                30\n",
       "Qwen-7B-Chat           27\n",
       "ImageLLM               27\n",
       "Qwen-14B-Chat          25\n",
       "InternLM-7B            22"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_files = path_llm.glob('*.json')\n",
    "\n",
    "# Function to read a JSON file into a DataFrame\n",
    "def read_json_file(file_path):\n",
    "    return pd.read_json(file_path)\n",
    "\n",
    "# Function to find a column that contains 'model' in its name\n",
    "def find_model_column(df):\n",
    "    for col in df.columns:\n",
    "        if 'model' in col.lower():\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "dataframes = []\n",
    "# Read each file and store in a list of DataFrames\n",
    "for file in json_files:\n",
    "    if 'HELM' in file.name:\n",
    "        continue\n",
    "    dataframes.append(read_json_file(file))\n",
    "\n",
    "# Extract and count models in each DataFrame\n",
    "model_counts = []\n",
    "for i, df in enumerate(dataframes):\n",
    "    model_col = find_model_column(df)\n",
    "    if (model_col is not None) and (type(df[model_col][0]) == str):\n",
    "        count = df[model_col].fillna('').apply(lambda x: x.split('\\n')[0]).value_counts().reset_index()\n",
    "        count.columns = ['model', 'count']\n",
    "        model_counts.append(count)\n",
    "\n",
    "combined_df = pd.concat(model_counts, ignore_index=True)\n",
    "combined_df.groupby('model').sum().sort_values('count', ascending=False).head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
