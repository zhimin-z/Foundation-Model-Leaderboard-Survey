{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jimmy/Documents/GitHub/LLM-Leaderboard-Integration/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from gradio_client import Client\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = Path(\"../data\")\n",
    "path_llm = path_data / \"llm\"\n",
    "path_lvlm = path_data / \"lvlm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://felixz-open-llm-leaderboard.hf.space/ ✔\n"
     ]
    }
   ],
   "source": [
    "client = Client(\"https://felixz-open-llm-leaderboard.hf.space/\")\n",
    "json_data = client.predict(\"\",\"\", api_name='/predict')\n",
    "\n",
    "with open(json_data, 'r') as file:\n",
    "    file_data = file.read()\n",
    "    data = json.loads(file_data)\n",
    "    df = pd.DataFrame(data['data'], columns=data['headers'])\n",
    "    df.drop(columns=['Model'], inplace=True)\n",
    "    df.to_json(path_llm / 'HuggingFace-Open-llm-leaderboard-20231116.json', orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path_llm / 'a.csv')\n",
    "df.to_json(path_llm / 'Sherlock.json', orient='records', indent=4)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Text-to-Text Generation', 'Text-to-Image Generation', 'Text-to-Code Generation', 'Text-to-Audio Generation', 'Text-to-Video Generation', 'Image-to-Text Generation', 'Code-to-Text Generation', 'Audio-to-Text Generation', 'Video-to-Text Generation', 'Image-to-Image Generation', 'Code-to-Code Generation', 'Audio-to-Audio Generation', 'Video-to-Video Generation',\n",
    "\n",
    "'Code Summarization', 'Code Review', 'Identifier Prediction', 'Defect Detection', 'Clone Detection', 'Code Classification', 'Code Reasoning', 'Document Translation', 'Log Parsing',\n",
    "\n",
    "# 'Image Captioning', 'Sign Language Recognition', 'Emotion Recognition', 'Video Processing', 'Digital Human', 'Multimodality'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [\n",
    "    \"Natural Language Understanding\",\n",
    "    \"Natural Language Inference\",\n",
    "    \"Question Answering\",\n",
    "    \"Summarization\",\n",
    "    \"Translation\",\n",
    "    \"Sentiment Analysis\",\n",
    "    \"Dialogue\",\n",
    "    \"Text Classification\",\n",
    "    \"Information Retrieval\",\n",
    "    \"Knowledge Completion\",\n",
    "    \"Relation Extraction\",\n",
    "    \"Ethics and Morality\",\n",
    "    \"Societal Bias\",\n",
    "    \"Toxicity\",\n",
    "    \"Fairness\",\n",
    "    \"Hallucination\",\n",
    "    \"Calibration\",\n",
    "    \"Efficiency\",\n",
    "    \"Sequence Tagging\",\n",
    "    \"Coreference Resolution\",\n",
    "    \"Fact Extraction\",\n",
    "    \"Multilinguality\",\n",
    "    \"Hate Detection\",\n",
    "    \"Recommendation\",\n",
    "    \"Healthcare\",\n",
    "    \"Education\",\n",
    "    \"Honestness\",\n",
    "    \"Helpfulness\",\n",
    "    \"Harmlessness\",\n",
    "    \"Robustness\",\n",
    "    \"Quality\",\n",
    "    \"Alignment\",\n",
    "    \"Aesthetics\",\n",
    "    \"Originality\",\n",
    "    \"Fidelity\",\n",
    "    \"Risk\",\n",
    "    \"Law\",\n",
    "    \"Mathematics\",\n",
    "    \"Social Science\",\n",
    "    \"Natural Science\",\n",
    "    \"Finance\",\n",
    "    \"Engineering\",\n",
    "    \"Truthfulness\",\n",
    "    \"Code Reasoning\",\n",
    "    \"Commonsense Reasoning\",\n",
    "    \"Knowledge Reasoning\",\n",
    "    \"Multi-hop Reasoning\",\n",
    "    \"Logical Reasoning\",\n",
    "    \"Arithmetic Reasoning\",\n",
    "    \"Symbolic Reasoning\",\n",
    "    \"Attribute Reasoning\",\n",
    "    \"Relation Reasoning\",\n",
    "    \"Tool Creation\",\n",
    "    \"Tool Manipulation\",\n",
    "    \"Robotic Tasks\",\n",
    "    \"Code Executor\",\n",
    "    \"Calculator\",\n",
    "    \"Search Engine\",\n",
    "    \"Online Shopping\",\n",
    "    \"Personality Testing\",\n",
    "    \"Crowd-sourcing Testing\",\n",
    "    \"Human-in-the-loop\",\n",
    "]\n",
    "\n",
    "labels2 = [\n",
    "    \"Visual Perception\",\n",
    "    \"Visual Reasoning\",\n",
    "    'Visual Knowledge Acquisition',\n",
    "    \"Visual Commonsense\",\n",
    "    \"Object Hallucination\",\n",
    "    \"Embodied Intelligence\",\n",
    "]\n",
    "\n",
    "# \"Open Book\",\n",
    "# \"Closed Book\",\n",
    "# 'Single Choice',\n",
    "# 'Multiple Choice',\n",
    "\n",
    "benchmark_lvlm_labels = {\n",
    "    'A-OKVQA': [\n",
    "        \"Perception\",\n",
    "        \"Existence\",\n",
    "        \"Position\",\n",
    "        \"Color\"\n",
    "    ],\n",
    "    'ALFWorld': [\n",
    "        \"Perception\",\n",
    "        \"Existence\",\n",
    "        \"Position\",\n",
    "        \"Color\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "benchmark_llm_labels = {\n",
    "    '2WikiMultihopQA': [\n",
    "        \"Question Answering\",\n",
    "        \"Multi-hop Reasoning\",\n",
    "        \"Information Retrieval\",\n",
    "        \"Knowledge Reasoning\",\n",
    "        \"Commonsense Reasoning\",\n",
    "        \"Natural Language Inference\",\n",
    "        \"Relation Extraction\"\n",
    "    ],\n",
    "    'A-OKVQA': [\n",
    "        \"Question Answering\",\n",
    "        \"Commonsense Reasoning\",\n",
    "        \"Knowledge Reasoning\",\n",
    "        \"Relation Reasoning\"\n",
    "    ],\n",
    "    'AFQMC': [\n",
    "        \"Natural Language Understanding\",\n",
    "        \"Text Classification\",\n",
    "        \"Natural Language Inference\",\n",
    "        \"Commonsense Reasoning\",\n",
    "        \"Finance\"\n",
    "    ],\n",
    "    'AGIEval': [\n",
    "        \"Natural Language Understanding\",\n",
    "        \"Question Answering\",\n",
    "        \"Text Classification\",\n",
    "        \"Knowledge Reasoning\",\n",
    "        \"Logical Reasoning\",\n",
    "        \"Arithmetic Reasoning\",\n",
    "        \"Multi-hop Reasoning\"\n",
    "    ],\n",
    "    'ALFWorld': [\n",
    "        \"Knowledge Reasoning\",\n",
    "        \"Relation Reasoning\",\n",
    "        \"Tool Manipulation\",\n",
    "        \"Robotic Tasks\",\n",
    "        \"Commonsense Reasoning\"\n",
    "    ],\n",
    "    'AlpacaEval': [\n",
    "        \"Natural Language Understanding\",\n",
    "        \"Dialogue\",\n",
    "        \"Text Classification\",\n",
    "        \"Information Retrieval\",\n",
    "        \"Knowledge Completion\",\n",
    "        \"Helpfulness\",\n",
    "        \"Robustness\",\n",
    "        \"Quality\"\n",
    "    ],\n",
    "    'Amazon Review': [\n",
    "        \"Sentiment Analysis\",\n",
    "        \"Text Classification\",\n",
    "        \"Information Retrieval\",\n",
    "        \"Recommendation\",\n",
    "        \"Summarization\",\n",
    "        \"Helpfulness\"\n",
    "    ],\n",
    "    'ANGO': [\n",
    "        \n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values with hyperlinks written to b.xlsx\n"
     ]
    }
   ],
   "source": [
    "from openpyxl import load_workbook, Workbook\n",
    "\n",
    "# Path to your Excel file and output file\n",
    "input_file = 'a.xlsx'\n",
    "output_file = 'b.xlsx'\n",
    "\n",
    "# Load the workbook and select the active worksheet\n",
    "wb = load_workbook(input_file)\n",
    "ws = wb.active\n",
    "\n",
    "# Dictionary to store unique values and their hyperlinks\n",
    "unique_values = {}\n",
    "\n",
    "# Loop through all cells in the worksheet\n",
    "for row in ws.iter_rows():\n",
    "    for cell in row:\n",
    "        # Check if cell has a value\n",
    "        if cell.value:\n",
    "            # Store value and hyperlink (if any)\n",
    "            unique_values[cell.value] = cell.hyperlink.target if cell.hyperlink else None\n",
    "\n",
    "# Create a new workbook and select the active worksheet\n",
    "new_wb = Workbook()\n",
    "new_ws = new_wb.active\n",
    "\n",
    "# Write unique values and hyperlinks to the new worksheet\n",
    "for i, (text, hyperlink) in enumerate(unique_values.items(), start=1):\n",
    "    new_ws.cell(row=i, column=1, value=text)\n",
    "    if hyperlink:\n",
    "        new_ws.cell(row=i, column=1).hyperlink = hyperlink\n",
    "\n",
    "# Save the new workbook\n",
    "new_wb.save(output_file)\n",
    "\n",
    "print(f\"Unique values with hyperlinks written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_links = {\n",
    "    'ALFWorld': 'https://github.com/alfworld/alfworld',\n",
    "    'BoolQ': 'https://github.com/google-research-datasets/boolean-questions',\n",
    "    'Chatbot Arena Conversations': 'https://huggingface.co/datasets/lmsys/chatbot_arena_conversations',\n",
    "    'CMMLU': 'https://github.com/haonan-li/CMMLU',\n",
    "    'GAOKAO-Bench-2023': 'https://github.com/OpenLMLab/GAOKAO-Bench-2023',\n",
    "    'HumanEval': 'https://github.com/openai/human-eval',\n",
    "    'IMDB': 'https://huggingface.co/datasets/imdb',\n",
    "    'Mind2Web': 'https://github.com/OSU-NLP-Group/Mind2Web',\n",
    "    'MMLU': 'https://github.com/hendrycks/test',\n",
    "    'MT-Bench': 'https://paperswithcode.com/dataset/mt-bench',\n",
    "    'MultiPL-E': 'https://github.com/nuprl/multipl-e',\n",
    "    'RAFT': 'https://huggingface.co/datasets/ought/raft',\n",
    "    'TruthfulQA': 'https://github.com/sylinrl/TruthfulQA',\n",
    "    'WebShop': 'https://github.com/princeton-nlp/webshop',  \n",
    "}\n",
    "\n",
    "benchmark_tags = {\n",
    "    'ALFWorld': 'Aligning Text and Embodied Environments for Interactive Learning',\n",
    "    'BoolQ': 'https://github.com/google-research-datasets/boolean-questions',\n",
    "    'Chatbot Arena Conversations': 'https://huggingface.co/datasets/lmsys/chatbot_arena_conversations',\n",
    "    'CMMLU': 'https://github.com/haonan-li/CMMLU',\n",
    "    'GAOKAO-Bench-2023': 'https://github.com/OpenLMLab/GAOKAO-Bench-2023',\n",
    "    'HumanEval': 'https://github.com/openai/human-eval',\n",
    "    'IMDB': 'https://huggingface.co/datasets/imdb',\n",
    "    'Mind2Web': 'https://github.com/OSU-NLP-Group/Mind2Web',\n",
    "    'MMLU': 'https://github.com/hendrycks/test',\n",
    "    'MT-Bench': 'https://paperswithcode.com/dataset/mt-bench',\n",
    "    'MultiPL-E': 'https://github.com/nuprl/multipl-e',\n",
    "    'RAFT': 'https://huggingface.co/datasets/ought/raft',\n",
    "    'TruthfulQA': 'https://github.com/sylinrl/TruthfulQA',\n",
    "    'WebShop': 'https://github.com/princeton-nlp/webshop',  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MMLU', 11),\n",
       " ('HellaSwag', 7),\n",
       " ('HumanEval', 6),\n",
       " ('BoolQ', 5),\n",
       " ('TruthfulQA', 4),\n",
       " ('ARC', 4),\n",
       " ('GSM8K', 4),\n",
       " ('DROP', 4),\n",
       " ('CHID', 3),\n",
       " ('NarrativeQA', 3),\n",
       " ('OpenbookQA', 3),\n",
       " ('WinoGrande', 3),\n",
       " ('TriviaQA', 3),\n",
       " ('Grounded', 3),\n",
       " ('IMDB', 2),\n",
       " ('RAFT', 2),\n",
       " ('CMMLU', 2),\n",
       " ('CSL', 2),\n",
       " ('CLUEWSC', 2),\n",
       " ('EPRSTMT', 2),\n",
       " ('OCNLI', 2),\n",
       " ('MS-COCO', 2),\n",
       " ('Winoground', 2),\n",
       " ('MATH', 2),\n",
       " ('NaturalQuestions', 2),\n",
       " ('QuAC', 2),\n",
       " ('BBH', 2),\n",
       " ('MBPP', 2),\n",
       " ('ScienceQA', 2),\n",
       " ('LAMBADA', 2),\n",
       " ('SIQA', 2),\n",
       " ('WiC', 2),\n",
       " ('GAOKAO-Bench', 2),\n",
       " ('AGIEval', 2),\n",
       " ('WSC', 2),\n",
       " ('C3', 2),\n",
       " ('CMNLI', 2),\n",
       " ('RTE', 2),\n",
       " ('ReCoRD', 2),\n",
       " ('IC15', 2),\n",
       " ('COCO-Text', 2),\n",
       " ('TextOCR', 2),\n",
       " ('ALFWorld', 1),\n",
       " ('WebShop', 1),\n",
       " ('Mind2Web', 1),\n",
       " ('MultiPL-E', 1),\n",
       " ('MT-Bench', 1),\n",
       " ('Chatbot', 1),\n",
       " ('Arena', 1),\n",
       " ('Conversations', 1),\n",
       " ('GAOKAO-Bench-2023', 1),\n",
       " ('LLSRC', 1),\n",
       " ('SLSRC', 1),\n",
       " ('SLPWC', 1),\n",
       " ('SLRFC', 1),\n",
       " ('TNEWS', 1),\n",
       " ('BUSTM', 1),\n",
       " ('gnad10', 1),\n",
       " ('Belebele', 1),\n",
       " ('XNLI', 1),\n",
       " ('Amazon', 1),\n",
       " ('Review', 1),\n",
       " ('Caltech-UCSD', 1),\n",
       " ('Birds-200-2011', 1),\n",
       " ('Common', 1),\n",
       " ('Syntactic', 1),\n",
       " ('Processes', 1),\n",
       " ('Demographic', 1),\n",
       " ('Stereotypes', 1),\n",
       " ('PaintSkills', 1),\n",
       " ('I2P', 1),\n",
       " ('Magazine', 1),\n",
       " ('Cover', 1),\n",
       " ('Photos', 1),\n",
       " ('Mental', 1),\n",
       " ('Disorders', 1),\n",
       " ('P2', 1),\n",
       " ('Relational', 1),\n",
       " ('Understanding', 1),\n",
       " (\"TIME's\", 1),\n",
       " ('most', 1),\n",
       " ('significant', 1),\n",
       " ('historical', 1),\n",
       " ('figures', 1),\n",
       " ('dailydall.e', 1),\n",
       " ('APPS', 1),\n",
       " ('BBQ', 1),\n",
       " ('BLiMP', 1),\n",
       " ('BOLD', 1),\n",
       " ('CNN/DailyMail', 1),\n",
       " ('CivilComments', 1),\n",
       " ('Data', 1),\n",
       " ('imputation', 1),\n",
       " ('Disinformation', 1),\n",
       " ('Dyck', 1),\n",
       " ('Entity', 1),\n",
       " ('matching', 1),\n",
       " ('ICE', 1),\n",
       " ('AR-LSAT', 1),\n",
       " ('LegalSupport', 1),\n",
       " ('MS', 1),\n",
       " ('MARCO', 1),\n",
       " ('RealToxicityPrompts', 1),\n",
       " ('LIME', 1),\n",
       " ('WikiFact', 1),\n",
       " ('XSUM', 1),\n",
       " ('bAbI', 1),\n",
       " ('Synthetic', 1),\n",
       " ('efficiency', 1),\n",
       " ('The', 1),\n",
       " ('Pile', 1),\n",
       " ('TwitterAAE', 1),\n",
       " ('CRASS', 1),\n",
       " ('IMPACT', 1),\n",
       " ('HHH', 1),\n",
       " ('Spider', 1),\n",
       " ('NL2Bash', 1),\n",
       " ('AR', 1),\n",
       " ('ER', 1),\n",
       " ('NER', 1),\n",
       " ('JS', 1),\n",
       " ('CR', 1),\n",
       " ('CFM', 1),\n",
       " ('SCM', 1),\n",
       " ('CJP', 1),\n",
       " ('CTP', 1),\n",
       " ('LQA', 1),\n",
       " ('JRG', 1),\n",
       " ('CU', 1),\n",
       " ('LC', 1),\n",
       " ('CIFAR-10', 1),\n",
       " ('Flickr30k', 1),\n",
       " ('VOC2012', 1),\n",
       " ('Omnibenchmark', 1),\n",
       " ('FSC147', 1),\n",
       " ('MMBench', 1),\n",
       " ('SEED-Bench', 1),\n",
       " ('Chatbot-Arena', 1),\n",
       " ('HotpotQA', 1),\n",
       " ('2WikiMultihopQA', 1),\n",
       " ('MuSiQue', 1),\n",
       " ('DuReader', 1),\n",
       " ('MultiFieldQA', 1),\n",
       " ('Qasper', 1),\n",
       " ('GovReport', 1),\n",
       " ('QMSum', 1),\n",
       " ('Multi-News', 1),\n",
       " ('VCSUM', 1),\n",
       " ('SAMSum', 1),\n",
       " ('TREC', 1),\n",
       " ('LSHTC', 1),\n",
       " ('PassageRetrieval', 1),\n",
       " ('PassageCount', 1),\n",
       " ('LCC', 1),\n",
       " ('RepoBench-P', 1),\n",
       " ('MCScript', 1),\n",
       " ('CosmosQA', 1),\n",
       " ('Quoref', 1),\n",
       " ('CommonGen', 1),\n",
       " ('C-Eval', 1),\n",
       " ('AFQMC', 1),\n",
       " ('TyDiQA', 1),\n",
       " ('Flores', 1),\n",
       " ('CommonSenseQA', 1),\n",
       " ('RACE', 1),\n",
       " ('LCSTS', 1),\n",
       " ('XSum', 1),\n",
       " ('AX-b', 1),\n",
       " ('AX-g', 1),\n",
       " ('PIQA', 1),\n",
       " ('M3KE', 1),\n",
       " ('TGEA', 1),\n",
       " ('OL-CC', 1),\n",
       " ('CSNLI', 1),\n",
       " ('ChineseSquad', 1),\n",
       " ('WPLC', 1),\n",
       " ('BiPaR', 1),\n",
       " ('CommonMT', 1),\n",
       " ('TOCP', 1),\n",
       " ('SWSR', 1),\n",
       " ('CORGI-PM', 1),\n",
       " ('CDIAL-BIAS', 1),\n",
       " ('COLD', 1),\n",
       " ('CBBQ', 1),\n",
       " ('TUMCC', 1),\n",
       " ('Cooridinate', 1),\n",
       " ('AI', 1),\n",
       " ('Corrigible', 1),\n",
       " ('Myopia', 1),\n",
       " ('Reward', 1),\n",
       " ('One-box', 1),\n",
       " ('Tendency', 1),\n",
       " ('Power-seeking', 1),\n",
       " ('Self-awareness', 1),\n",
       " ('CAIL判决预测数据集', 1),\n",
       " ('KQApro', 1),\n",
       " ('LC-quad2', 1),\n",
       " ('WQSP', 1),\n",
       " ('CWQ', 1),\n",
       " ('GrailQA', 1),\n",
       " ('GraphQ', 1),\n",
       " ('QALD-9', 1),\n",
       " ('MKQA', 1),\n",
       " ('FewNERD', 1),\n",
       " ('FewRel', 1),\n",
       " ('InstructIE', 1),\n",
       " ('MAVEN', 1),\n",
       " ('WikiEvents', 1),\n",
       " ('Flowers102', 1),\n",
       " ('CIFAR10', 1),\n",
       " ('ImageNet-1K', 1),\n",
       " ('Pets37', 1),\n",
       " ('VizWiz-yesno', 1),\n",
       " ('VizWiz-singleChoice', 1),\n",
       " ('TDIUC-Sport', 1),\n",
       " ('TDIUC-Scene', 1),\n",
       " ('MEDIC', 1),\n",
       " ('MSCOCO-MCI', 1),\n",
       " ('MSCOCO-GOI', 1),\n",
       " ('MSCOCO-MOS', 1),\n",
       " ('TDIUC-Color', 1),\n",
       " ('TDIUC-Utility', 1),\n",
       " ('TDIUC-Position', 1),\n",
       " ('TDIUC-Detection', 1),\n",
       " ('TDIUC-Counting', 1),\n",
       " ('RefCOCO', 1),\n",
       " ('MSCOCO-OC', 1),\n",
       " ('VQA', 1),\n",
       " ('v2', 1),\n",
       " ('GQA', 1),\n",
       " ('Whoops', 1),\n",
       " ('OK-VQA', 1),\n",
       " ('VizWiz', 1),\n",
       " ('ViQuAE', 1),\n",
       " ('K-ViQuAE', 1),\n",
       " ('A-OKVQA', 1),\n",
       " ('A-OKVQRA', 1),\n",
       " ('A-OKVQAR', 1),\n",
       " ('ImageNetVC', 1),\n",
       " ('CLEVR', 1),\n",
       " ('VSR', 1),\n",
       " ('MP3D', 1),\n",
       " ('VQA-MT', 1),\n",
       " ('VisDial', 1),\n",
       " ('MSCOCO-ITM', 1),\n",
       " ('MSCOCO-ITS', 1),\n",
       " ('WikiHow', 1),\n",
       " ('SNLI-VE', 1),\n",
       " ('MOCHEG', 1),\n",
       " ('CUTE80', 1),\n",
       " ('IIIT5K', 1),\n",
       " ('WordArt', 1),\n",
       " ('FUNSD', 1),\n",
       " ('POIE', 1),\n",
       " ('SROIE', 1),\n",
       " ('TextVQA', 1),\n",
       " ('DocVQA', 1),\n",
       " ('OCR-VQA', 1),\n",
       " ('MSCOCO', 1),\n",
       " ('TextCaps', 1),\n",
       " ('NoCaps', 1),\n",
       " ('Flickr30K', 1),\n",
       " ('CommitmentBank', 1),\n",
       " ('COPA', 1),\n",
       " ('MultiRC', 1),\n",
       " ('Broadcoverage', 1),\n",
       " ('Diagnostics', 1),\n",
       " ('Winogender', 1)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''ALFWorld\tWebShop\tMind2Web\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "MultiPL-E\tHumanEval\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "MMLU\tMT-Bench\tChatbot Arena Conversations\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "BoolQ\tMMLU\tTruthfulQA\tIMDB\tRAFT\tCMMLU\tGAOKAO-Bench-2023\tCSL\tCHID\tCLUEWSC\tLLSRC\tSLSRC\tSLPWC\tSLRFC\tEPRSTMT\tTNEWS\tOCNLI\tBUSTM\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "MMLU\tgnad10\tHellaSwag\tARC\tBelebele\tXNLI\tAmazon Review\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "Caltech-UCSD Birds-200-2011\tCommon Syntactic Processes\tDemographic Stereotypes\tPaintSkills\tI2P\tMS-COCO\tMagazine Cover Photos\tMental Disorders\tP2\tRelational Understanding\tTIME\tWinoground\tdailydall.e\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "APPS\tBBQ\tBLiMP\tBOLD\tBoolQ\tCNN/DailyMail\tCivilComments\tData imputation\tDisinformation\tDyck\tEntity matching\tGSM8K\tHellaSwag\tHumanEval\tICE\tIMDB\tAR-LSAT\tLegalSupport\tMATH\tMMLU\tMS MARCO\tNarrativeQA\tNaturalQuestions\tOpenbookQA\tQuAC\tRAFT\tRealToxicityPrompts\tLIME\tWikiFact\tXSUM\tbAbI\tSynthetic efficiency\tThe Pile\tTruthfulQA\tTwitterAAE\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "ARC\tHellaSwag\tMMLU\tTruthfulQA\tWinoGrande\tGSM8K\tDROP\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "MMLU\tBBH\tDROP\tCRASS\tHumanEval\tIMPACT\tHHH\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "MBPP\tSpider\tNL2Bash\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "AR\tER\tNER\tJS\tCR\tCFM\tSCM\tCJP\tCTP\tLQA\tJRG\tCU\tLC\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "CIFAR-10\tFlickr30k\tVOC2012\tOmnibenchmark\tFSC147\tScienceQA\tMMBench\tSEED-Bench\tMS-COCO\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "Chatbot-Arena\tHellaSwag\tHumanEval\tLAMBADA\tMMLU\tTriviaQA\tWinoGrande\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "HotpotQA\t2WikiMultihopQA\tMuSiQue\tDuReader\tMultiFieldQA\tNarrativeQA\tQasper\tGovReport\tQMSum\tMulti-News\tVCSUM\tTriviaQA\tSAMSum\tTREC\tLSHTC\tPassageRetrieval\tPassageCount\tLCC\tRepoBench-P\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "NarrativeQA\tMCScript\tCosmosQA\tSIQA\tDROP\tQuoref\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "HellaSwag\tMMLU\tARC\tTruthfulQA\tCommonGen\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "C-Eval\tMMLU\tARC\tCHID\tWiC\tGAOKAO-Bench\tCMMLU\tAGIEval\tAFQMC\tWSC\tTyDiQA\tFlores\tBoolQ\tCommonSenseQA\tC3\tRACE\tOpenbookQA\tCSL\tLCSTS\tXSum\tEPRSTMT\tLAMBADA\tCMNLI\tOCNLI\tAX-b\tAX-g\tRTE\tReCoRD\tHellaSwag\tPIQA\tSIQA\tMATH\tGSM8K\tDROP\tHumanEval\tMBPP\tBBH\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "M3KE\tMMLU\tGAOKAO-Bench\tTGEA\tOL-CC\tCSNLI\tCLUEWSC\tChineseSquad\tCHID\tWPLC\tBiPaR\tCommonMT\tC3\tCMNLI\tTOCP\tSWSR\tCORGI-PM\tCDIAL-BIAS\tCOLD\tCBBQ\tTUMCC\tCooridinate AI\tCorrigible\tMyopia Reward\tOne-box Tendency\tPower-seeking\tSelf-awareness\tCAIL判决预测数据集\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "KQApro\tLC-quad2\tWQSP\tCWQ\tGrailQA\tGraphQ\tQALD-9\tMKQA\tFewNERD\tFewRel\tInstructIE\tMAVEN\tWikiEvents\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "Flowers102\tCIFAR10\tImageNet-1K\tPets37\tVizWiz-yesno\tVizWiz-singleChoice\tTDIUC-Sport\tTDIUC-Scene\tMEDIC\tMSCOCO-MCI\tMSCOCO-GOI\tMSCOCO-MOS\tTDIUC-Color\tTDIUC-Utility\tTDIUC-Position\tTDIUC-Detection\tTDIUC-Counting\tRefCOCO\tMSCOCO-OC\tVQA v2\tGQA\tWhoops\tOK-VQA\tScienceQA\tVizWiz\tViQuAE\tK-ViQuAE\tA-OKVQA\tA-OKVQRA\tA-OKVQAR\tImageNetVC\tCLEVR\tVSR\tMP3D\tVQA-MT\tVisDial\tMSCOCO-ITM\tMSCOCO-ITS\tWikiHow\tWinoground\tSNLI-VE\tMOCHEG\tGrounded IC15\tIC15\tGrounded COCO-Text\tCOCO-Text\tGrounded TextOCR\tTextOCR\tCUTE80\tIIIT5K\tWordArt\tFUNSD\tPOIE\tSROIE\tTextVQA\tDocVQA\tOCR-VQA\tMSCOCO\tTextCaps\tNoCaps\tFlickr30K\n",
    "BoolQ\tCommitmentBank\tCOPA\tMultiRC\tReCoRD\tRTE\tWiC\tWSC\tBroadcoverage Diagnostics\tWinogender\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "MMLU\tTriviaQA\tNaturalQuestions\tGSM8K\tHumanEval\tAGIEval\tBoolQ\tHellaSwag\tOpenbookQA\tQuAC\tWinoGrande'''\n",
    "\n",
    "from collections import defaultdict\n",
    "df = defaultdict(int)\n",
    "for items in text.split('\\n'):\n",
    "    for item in items.split():\n",
    "        df[item] += 1\n",
    "\n",
    "sorted(df.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = '''10-bin expected calibration error\n",
    "1-bin expected calibration error\n",
    "Max prob\n",
    "10-bin expected calibration error\n",
    "1-bin expected calibration error (after Platt scaling)\n",
    "10-bin Expected Calibration Error (after Platt scaling)\n",
    "Platt Scaling Coefficient\n",
    "Platt Scaling Intercept\n",
    "Selective coverage-accuracy area\n",
    "Accuracy at 10% coverage\n",
    "Stereotypical associations (race, profession)\n",
    "Stereotypical associations (gender, profession)\n",
    "Demographic representation (race)\n",
    "Demographic representation (gender)\n",
    "Toxic fraction\n",
    "Denoised inference runtime (s)\n",
    "Estimated training emissions (kg CO2)\n",
    "Estimated training energy cost (MWh)\n",
    "Observed inference runtime (s)\n",
    "Idealized inference runtime (s)\n",
    "Denoised inference runtime (s)\n",
    "# trials\n",
    "# prompt tokens\n",
    "# output tokens\n",
    "# eval\n",
    "# train\n",
    "truncated\n",
    "SummaC\n",
    "QAFactEval\n",
    "Coverage\n",
    "Density\n",
    "Compression\n",
    "BERTScore (F1)\n",
    "HumanEval-faithfulness\n",
    "HumanEval-relevance\n",
    "HumanEval-coherence\n",
    "Avg. # tests passed\n",
    "Strict correctness\n",
    "BBQ (ambiguous)\n",
    "BBQ (unambiguous)\n",
    "Longest common prefix length\n",
    "Edit distance (Levenshtein)\n",
    "Edit similarity (Levenshtein)\n",
    "Self-BLEU\n",
    "Entropy (Monte Carlo)\n",
    "Macro-F1\n",
    "Micro-F1\n",
    "Chinese iBLEU\n",
    "Chinese Top-1 Accuracy\n",
    "Chinese ROUGE-2 score\n",
    "Chinese BLEU-1 score\n",
    "CLEVA Math Exact Match\n",
    "Chinese BLEU-1 score\n",
    "Chinese BLEU-1 score'''\n",
    "\n",
    "# s = set()\n",
    "df = pd.DataFrame()\n",
    "for scenario in metrics.split('\\n'):\n",
    "    # scenario_ = scenario.split('(')[0].strip()\n",
    "    s = {\n",
    "        'Name': scenario\n",
    "    }\n",
    "    df = pd.concat([df, pd.DataFrame([s])], ignore_index=True)\n",
    "    # s.add(scenario)\n",
    "# len(df)\n",
    "df.to_csv('metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenarios = '''BoolQ\n",
    "# NarrativeQA\n",
    "# NaturalQuestions (closed-book)\n",
    "# NaturalQuestions (open-book)\n",
    "# QuAC (Question Answering in Context)\n",
    "# HellaSwag\n",
    "# OpenbookQA\n",
    "# TruthfulQA\n",
    "# MMLU (Massive Multitask Language Understanding)\n",
    "# MS MARCO (regular track)\n",
    "# MS MARCO (TREC track)\n",
    "# CNN/DailyMail\n",
    "# XSUM\n",
    "# IMDB\n",
    "# CivilComments\n",
    "# RAFT (Real-world Annotated Few-Shot)\n",
    "# ICE (International Corpus of English)\n",
    "# The Pile\n",
    "# TwitterAAE\n",
    "# BLiMP (The Benchmark of Linguistic Minimal Pairs for English)\n",
    "# NaturalQuestions (closed-book)\n",
    "# HellaSwag\n",
    "# OpenbookQA\n",
    "# TruthfulQA\n",
    "# MMLU (Massive Multitask Language Understanding)\n",
    "# WikiFact\n",
    "# bAbI\n",
    "# Dyck\n",
    "# Synthetic reasoning (abstract symbols)\n",
    "# Synthetic reasoning (natural language)\n",
    "# GSM8K (Grade school math word problems)\n",
    "# MATH\n",
    "# MATH (chain-of-thoughts)\n",
    "# APPS (Code)\n",
    "# HumanEval (Code)\n",
    "# LegalSupport\n",
    "# LSAT\n",
    "# Data imputation\n",
    "# Entity matching\n",
    "# Copyright (text)\n",
    "# Copyright (code)\n",
    "# Disinformation (reiteration)\n",
    "# Disinformation (wedging)\n",
    "# BBQ (Bias Benchmark for Question Answering)\n",
    "# BOLD (Bias in Open-Ended Language Generation Dataset)\n",
    "# RealToxicityPrompts\n",
    "# Synthetic efficiency\n",
    "# MMLU (Massive Multitask Language Understanding)\n",
    "# IMDB\n",
    "# RAFT (Real-world Annotated Few-Shot)\n",
    "# CivilComments\n",
    "# NaturalQuestions (open-book)\n",
    "# CNN/DailyMail\n",
    "# IMDB\n",
    "# CivilComments\n",
    "# HellaSwag\n",
    "# OpenbookQA\n",
    "# TruthfulQA\n",
    "# MMLU (Massive Multitask Language Understanding)\n",
    "# BLiMP (The Benchmark of Linguistic Minimal Pairs for English)\n",
    "# LegalSupport\n",
    "# LSAT\n",
    "# BBQ (Bias Benchmark for Question Answering)\n",
    "# NaturalQuestions (open-book)\n",
    "# CNN/DailyMail\n",
    "# IMDB\n",
    "# CivilComments\n",
    "# BoolQ\n",
    "# IMDB'''\n",
    "# s = set()\n",
    "# for scenario in scenarios.split('\\n'):\n",
    "#     scenario_ = scenario.split('(')[0].strip()\n",
    "#     s.add(scenario_)\n",
    "# # len(s)\n",
    "# s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenarios = '''MS-COCO (base)\n",
    "# Caltech-UCSD Birds-200-2011\n",
    "# DrawBench (image quality categories)\n",
    "# DrawBench (reasoning categories)\n",
    "# DrawBench (knowledge categories)\n",
    "# PartiPrompts (image quality categories)\n",
    "# PartiPrompts (reasoning categories)\n",
    "# PartiPrompts (knowledge categories)\n",
    "# MS-COCO (base)\n",
    "# DrawBench (image quality categories)\n",
    "# PartiPrompts (image quality categories)\n",
    "# MS-COCO (base)\n",
    "# MS-COCO (Art styles)\n",
    "# dailydall.e\n",
    "# Logos\n",
    "# Landing Page\n",
    "# Magazine Cover Photos\n",
    "# dailydall.e\n",
    "# Logos\n",
    "# Landing Page\n",
    "# Magazine Cover Photos\n",
    "# Common Syntactic Processes\n",
    "# DrawBench (reasoning categories)\n",
    "# PartiPrompts (reasoning categories)\n",
    "# Relational Understanding\n",
    "# Detection (PaintSkills)\n",
    "# Winoground\n",
    "# TIME\n",
    "# DrawBench (knowledge categories)\n",
    "# PartiPrompts (knowledge categories)\n",
    "# Demographic Stereotypes\n",
    "# Mental Disorders\n",
    "# Inappropriate Image Prompts (I2P)\n",
    "# MS-COCO (fairness - AAVE dialect)\n",
    "# MS-COCO (fairness - gender)\n",
    "# MS-COCO (robustness)\n",
    "# MS-COCO (Chinese)\n",
    "# MS-COCO (Hindi)\n",
    "# MS-COCO (Spanish)\n",
    "# MS-COCO Fidelity\n",
    "# MS-COCO Efficiency\n",
    "# MS-COCO (Art styles)'''\n",
    "# s = set()\n",
    "# for scenario in scenarios.split('\\n'):\n",
    "#     scenario = scenario.split('(')[0].strip()\n",
    "#     s.add(scenario)\n",
    "# s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1223"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "sum = 0\n",
    "\n",
    "for file in glob.glob(str(path_llm / \"*.json\")):\n",
    "    if 'imagenet' not in file.lower():\n",
    "        continue\n",
    "    df = pd.read_json(file)\n",
    "    sum += len(df)\n",
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('/Users/jimmy/Downloads/LMExamQA.json')\n",
    "df['domain'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ChatGLM2-6B</th>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPT-4</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bloom-7b1</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama-7B</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baichuan2-13B-Chat</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ChatGPT</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen-7B-Chat</th>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ImageLLM</th>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen-14B-Chat</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>InternLM-7B</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    count\n",
       "model                    \n",
       "ChatGLM2-6B            41\n",
       "GPT-4                  34\n",
       "bloom-7b1              33\n",
       "llama-7B               31\n",
       "Baichuan2-13B-Chat     31\n",
       "ChatGPT                30\n",
       "Qwen-7B-Chat           27\n",
       "ImageLLM               27\n",
       "Qwen-14B-Chat          25\n",
       "InternLM-7B            22"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_files = path_llm.glob('*.json')\n",
    "\n",
    "# Function to read a JSON file into a DataFrame\n",
    "def read_json_file(file_path):\n",
    "    return pd.read_json(file_path)\n",
    "\n",
    "# Function to find a column that contains 'model' in its name\n",
    "def find_model_column(df):\n",
    "    for col in df.columns:\n",
    "        if 'model' in col.lower():\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "dataframes = []\n",
    "# Read each file and store in a list of DataFrames\n",
    "for file in json_files:\n",
    "    if 'HELM' in file.name:\n",
    "        continue\n",
    "    dataframes.append(read_json_file(file))\n",
    "\n",
    "# Extract and count models in each DataFrame\n",
    "model_counts = []\n",
    "for i, df in enumerate(dataframes):\n",
    "    model_col = find_model_column(df)\n",
    "    if (model_col is not None) and (type(df[model_col][0]) == str):\n",
    "        count = df[model_col].fillna('').apply(lambda x: x.split('\\n')[0]).value_counts().reset_index()\n",
    "        count.columns = ['model', 'count']\n",
    "        model_counts.append(count)\n",
    "\n",
    "combined_df = pd.concat(model_counts, ignore_index=True)\n",
    "combined_df.groupby('model').sum().sort_values('count', ascending=False).head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
